{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Odibi Documentation","text":"<p>Welcome to the Odibi documentation. Odibi is a declarative data engineering framework designed to make data pipelines transparent, self-documenting, and easy to maintain.</p>"},{"location":"#how-to-use-this-documentation","title":"\ud83d\udcda How to use this documentation","text":"<p>We follow the Di\u00e1taxis framework, organizing documentation by user needs:</p>"},{"location":"#1-tutorials-learning-oriented","title":"1. \ud83c\udfc1 Tutorials (Learning-Oriented)","text":"<p>Start here if you are new. Step-by-step lessons that take you from \"Zero\" to \"Running Pipeline\". *   Getting Started: Your first 10 minutes with Odibi. *   Spark Engine: Running Odibi on Spark clusters.</p>"},{"location":"#2-how-to-guides-task-oriented","title":"2. \ud83d\udcd8 How-To Guides (Task-Oriented)","text":"<p>Read these when you need to solve a specific problem. Practical recipes for common tasks. *   Best Practices: Project organization, naming, performance, and more. *   Writing Custom Transformations: How to write Python logic for your pipeline. *   Using the CLI: Running, stress-testing, and debugging. *   Production Deployment: Moving from laptop to cloud. *   Performance Tuning: Optimize for speed and scale. *   WSL Setup: The definitive guide for Windows users.</p>"},{"location":"#3-reference-information-oriented","title":"3. \u2699\ufe0f Reference (Information-Oriented)","text":"<p>Look here for technical specs and syntax. *   Configuration Reference: Complete guide to <code>odibi.yaml</code>. *   Cheatsheet: Quick lookup for commands and syntax. *   Supported Formats: CSV, Parquet, Delta, JSON details.</p>"},{"location":"#4-explanation-understanding-oriented","title":"4. \ud83e\udde0 Explanation (Understanding-Oriented)","text":"<p>Read these to understand the \"Why\" and \"How\". *   Architecture: How Odibi works under the hood. *   Case Studies: Real-world patterns and examples.</p>"},{"location":"#quick-links","title":"\ud83d\ude80 Quick Links","text":"<ul> <li>Repository: GitHub</li> <li>Issues: Report a Bug</li> <li>PyPI: View Package</li> </ul> <p>This documentation is versioned with the Odibi framework. Last updated: December 2025.</p>"},{"location":"AUDIT_FINDINGS/","title":"Implementation Quality Audit - Odibi Pipeline Framework","text":"<p>Date: December 3, 2025 Audit Thread: https://ampcode.com/threads/T-e994f339-852c-4546-a642-5b02a984b896</p>"},{"location":"AUDIT_FINDINGS/#summary","title":"Summary","text":"<p>Found 17 implementation issues across the Odibi codebase. These are categorized by priority and include specific file locations, problem descriptions, impact assessments, and suggested fixes.</p>"},{"location":"AUDIT_FINDINGS/#high-priority-performance-critical","title":"\ud83d\udd34 HIGH PRIORITY (Performance Critical)","text":""},{"location":"AUDIT_FINDINGS/#issue-1-redundant-dataframe-count-operations","title":"Issue #1: Redundant DataFrame Count Operations","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 2098-2108, 2167-2172, 2228-2256 Problem Multiple <code>.count()</code> calls on same DataFrame trigger redundant Spark jobs Impact Performance - each <code>.count()</code> is an expensive distributed action <p>Current Code:</p> <pre><code>initial_count = df.count()\ndf = df.filter(...)\ndeleted_count += initial_count - df.count()  # Second count on filtered df\n</code></pre> <p>Suggested Fix: Cache DataFrame before counting, or use single aggregation to get both values:</p> <pre><code>df.cache()\ninitial_count = df.count()\n# ... filter operations\nfinal_count = df.count()\ndf.unpersist()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-2-repeated-delta-table-reads-in-catalog-methods","title":"Issue #2: Repeated Delta Table Reads in Catalog Methods","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 507-634 Problem Methods <code>get_registered_pipeline</code>, <code>get_registered_nodes</code>, <code>get_all_registered_pipelines</code>, <code>get_all_registered_nodes</code> each read the same Delta table separately Impact Performance - redundant I/O when called in sequence during auto-registration <p>Affected Methods: - <code>get_registered_pipeline()</code> - reads <code>meta_pipelines</code> - <code>get_registered_nodes()</code> - reads <code>meta_nodes</code> - <code>get_all_registered_pipelines()</code> - reads <code>meta_pipelines</code> - <code>get_all_registered_nodes()</code> - reads <code>meta_nodes</code></p> <p>Suggested Fix: Add caching layer or batch read method that fetches all needed data in one read:</p> <pre><code>def _get_cached_pipelines(self) -&gt; Dict[str, Dict]:\n    if self._pipeline_cache is None:\n        self._pipeline_cache = self._read_all_pipelines()\n    return self._pipeline_cache\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-3-expensive-count-in-validation-loop","title":"Issue #3: Expensive .count() in Validation Loop","text":"Attribute Details File <code>odibi/validation/engine.py</code> Lines 250, 269, 282, 305, 323, 337 Problem Individual <code>.count()</code> for each column validation rule Impact O(n) Spark jobs where n = number of validation columns/rules <p>Current Pattern:</p> <pre><code>for col in validation_config.no_nulls:\n    null_count = df.filter(F.col(col).isNull()).count()  # One job per column\n</code></pre> <p>Suggested Fix: Aggregate all null counts in single <code>.agg()</code> call:</p> <pre><code>null_counts = df.select([\n    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n    for c in validation_config.no_nulls\n]).collect()[0].asDict()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-4-individual-register_pipelineregister_node-methods-still-perform-single-writes","title":"Issue #4: Individual register_pipeline/register_node Methods Still Perform Single Writes","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 816-1063 Problem While <code>register_pipelines_batch</code> and <code>register_nodes_batch</code> exist, the individual methods still perform single writes Impact Callers using individual methods still suffer N individual Delta writes <p>Suggested Fix: Deprecate individual methods or have them delegate to batch internally:</p> <pre><code>def register_pipeline(self, pipeline_config, ...):\n    \"\"\"Deprecated: Use register_pipelines_batch for better performance.\"\"\"\n    warnings.warn(\"Use register_pipelines_batch instead\", DeprecationWarning)\n    return self.register_pipelines_batch([self._prepare_pipeline_record(pipeline_config)])\n</code></pre>"},{"location":"AUDIT_FINDINGS/#medium-priority-reliability-maintainability","title":"\ud83d\udfe1 MEDIUM PRIORITY (Reliability &amp; Maintainability)","text":""},{"location":"AUDIT_FINDINGS/#issue-5-silent-exception-handling-suppresses-failures","title":"Issue #5: Silent Exception Handling Suppresses Failures","text":"Attribute Details Files Multiple (see below) Problem Exception handlers log warnings but don't re-raise, hiding failures Impact Reliability - failures go unnoticed, debugging is harder <p>Locations: | File | Lines | Pattern | |------|-------|---------| | <code>odibi/story/generator.py</code> | 186-188 | <code>except Exception: pass</code> | | <code>odibi/lineage.py</code> | 66-68 | Silently disables OpenLineage | | <code>odibi/lineage.py</code> | 118-120 | Returns fake UUID on failure | | <code>odibi/lineage.py</code> | 157-158 | Silently ignores emit failure | | <code>odibi/utils/extensions.py</code> | 27-28 | Only logs warning | | <code>odibi/plugins.py</code> | 64-65, 67-68 | Only logs error |</p> <p>Suggested Fix: At minimum, log at ERROR level with stack trace. Consider raising wrapped exceptions:</p> <pre><code>except Exception as e:\n    logger.error(f\"Operation failed: {e}\", exc_info=True)\n    # Either re-raise or set a flag for caller to check\n    self._last_error = e\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-6-inconsistent-selfspark-vs-selfengine-checks","title":"Issue #6: Inconsistent self.spark vs self.engine Checks","text":"Attribute Details File <code>odibi/catalog.py</code> Lines Throughout (139, 145, 244, 282, 320, 337, 507, 516, 544, 555, etc.) Problem Code uses two patterns interchangeably without clear reasoning Impact Maintainability - inconsistent control flow makes code hard to understand <p>Current Patterns:</p> <pre><code># Pattern 1\nif self.spark:\n    # Spark logic\nelif self.engine:\n    # Engine logic\n\n# Pattern 2\nif self.spark:\n    # Spark logic\nelif self.engine and self.engine.name == \"pandas\":\n    # Pandas-specific logic\n</code></pre> <p>Suggested Fix: Standardize on single pattern using computed properties:</p> <pre><code>@property\ndef is_spark_mode(self) -&gt; bool:\n    return self.spark is not None\n\n@property\ndef is_pandas_mode(self) -&gt; bool:\n    return self.engine is not None and self.engine.name == \"pandas\"\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-7-missing-connection-closecleanup","title":"Issue #7: Missing Connection Close/Cleanup","text":"Attribute Details File <code>odibi/connections/azure_sql.py</code> Lines 573-589 (close method exists but never called) Problem <code>AzureSQL.close()</code> method exists but is never called in Pipeline/PipelineManager lifecycle Impact Resource leak - SQLAlchemy connection pools may not be properly disposed <p>Suggested Fix: Add cleanup in Pipeline/PipelineManager using context manager pattern:</p> <pre><code>class Pipeline:\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._cleanup_connections()\n\n    def _cleanup_connections(self):\n        for conn in self.connections.values():\n            if hasattr(conn, 'close'):\n                conn.close()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-8-uncached-row-count-fallback-in-schema-capture-phase","title":"Issue #8: Uncached Row Count Fallback in Schema Capture Phase","text":"Attribute Details File <code>odibi/node.py</code> Lines 227-231 Problem When read phase is skipped, row count is computed via expensive fallback Impact Performance - unnecessary Spark action <p>Current Code:</p> <pre><code>rows_in = (\n    self._read_row_count\n    if self._read_row_count is not None\n    else self._count_rows(input_df)  # Expensive fallback\n)\n</code></pre> <p>Suggested Fix: Make counting optional or ensure <code>_read_row_count</code> is always populated:</p> <pre><code>rows_in = self._read_row_count  # May be None, and that's OK\nif rows_in is None and self.performance_config.track_row_counts:\n    rows_in = self._count_rows(input_df)\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-9-dict-access-without-consistent-get-fallback","title":"Issue #9: Dict Access Without Consistent .get() Fallback","text":"Attribute Details File <code>odibi/node.py</code> Lines Various (inconsistent pattern) Problem Some connection lookups use <code>.get()</code>, others use direct access Impact Inconsistent - some missing connections raise KeyError, others return None <p>Inconsistent Patterns:</p> <pre><code># Safe pattern (used sometimes)\nconnection = self.connections.get(read_config.connection)\n\n# Unsafe pattern (used elsewhere)\nconnection = self.connections[write_config.connection]  # KeyError if missing\n</code></pre> <p>Suggested Fix: Use <code>.get()</code> consistently and check for None explicitly:</p> <pre><code>connection = self.connections.get(write_config.connection)\nif connection is None:\n    raise ValueError(f\"Connection '{write_config.connection}' not found\")\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-10-repeated-hash-calculation-pattern","title":"Issue #10: Repeated Hash Calculation Pattern","text":"Attribute Details Files <code>odibi/pipeline.py</code>, <code>odibi/catalog.py</code>, <code>odibi/node.py</code> Lines pipeline.py:400-413, catalog.py:840-848, node.py:737-745 Problem Same hash calculation pattern duplicated in multiple places Impact Code duplication, inconsistency risk <p>Duplicated Pattern:</p> <pre><code>if hasattr(config, \"model_dump\"):\n    dump = config.model_dump(mode=\"json\", exclude={\"description\", \"tags\", \"log_level\"})\nelse:\n    dump = config.dict(exclude={\"description\", \"tags\", \"log_level\"})\ndump_str = json.dumps(dump, sort_keys=True)\nversion_hash = hashlib.md5(dump_str.encode(\"utf-8\")).hexdigest()\n</code></pre> <p>Suggested Fix: Extract to shared utility function:</p> <pre><code># odibi/utils/hashing.py\ndef calculate_config_hash(config, exclude: Set[str] = None) -&gt; str:\n    exclude = exclude or {\"description\", \"tags\", \"log_level\"}\n    dump = config.model_dump(mode=\"json\", exclude=exclude) if hasattr(config, \"model_dump\") else config.dict(exclude=exclude)\n    return hashlib.md5(json.dumps(dump, sort_keys=True).encode()).hexdigest()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#low-priority-minor-improvements","title":"\ud83d\udfe2 LOW PRIORITY (Minor Improvements)","text":""},{"location":"AUDIT_FINDINGS/#issue-11-import-inside-loopmethod","title":"Issue #11: Import Inside Loop/Method","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 268, 508, 545, 662, 751, 871, 1000, 1085, 1160, etc. Problem <code>from pyspark.sql import functions as F</code> imported inside methods repeatedly Impact Minor performance overhead from repeated imports <p>Suggested Fix: Import once at module level with try/except guard:</p> <pre><code>try:\n    from pyspark.sql import functions as F\n    from pyspark.sql import SparkSession\n    SPARK_AVAILABLE = True\nexcept ImportError:\n    F = None\n    SparkSession = None\n    SPARK_AVAILABLE = False\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-12-duplicate-logging-context-creation","title":"Issue #12: Duplicate Logging Context Creation","text":"Attribute Details File <code>odibi/node.py</code> Lines 170-174 and 761-764 Problem <code>create_logging_context()</code> called in both <code>NodeExecutor.execute()</code> and <code>Node.execute()</code> Impact Creates redundant context objects <p>Suggested Fix: Pass context from outer to inner method:</p> <pre><code>def execute(self, ctx: Optional[LoggingContext] = None) -&gt; NodeResult:\n    ctx = ctx or create_logging_context(...)\n    result = self.executor.execute(self.config, ctx=ctx)\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-13-get_delta_history-collects-all-history","title":"Issue #13: get_delta_history Collects All History","text":"Attribute Details File <code>odibi/engine/spark_engine.py</code> Lines 806 Problem When limit is None, <code>delta_table.history()</code> returns all versions then <code>.collect()</code> brings everything to driver Impact Memory issue for tables with long history <p>Current Code:</p> <pre><code>history_df = delta_table.history(limit) if limit else delta_table.history()\nhistory = [row.asDict() for row in history_df.collect()]\n</code></pre> <p>Suggested Fix: Always require/default a reasonable limit:</p> <pre><code>DEFAULT_HISTORY_LIMIT = 100\n\ndef get_delta_history(self, connection, path, limit: int = DEFAULT_HISTORY_LIMIT):\n    history_df = delta_table.history(limit)\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-14-optional-get-without-default","title":"Issue #14: Optional .get() Without Default","text":"Attribute Details Files Various Problem Some <code>.get()</code> calls use proper defaults, others don't Impact Potential NoneType errors <p>Examples:</p> <pre><code># Good\nr.get(\"rows_processed\", 0)\nr.get(\"duration_ms\", 0)\n\n# Potentially problematic\nconfig.get(\"some_key\")  # Returns None if missing\n</code></pre> <p>Suggested Fix: Audit all <code>.get()</code> calls to ensure appropriate defaults are provided.</p>"},{"location":"AUDIT_FINDINGS/#issue-15-thread-unsafe-caching-pattern","title":"Issue #15: Thread-Unsafe Caching Pattern","text":"Attribute Details File <code>odibi/connections/azure_adls.py</code> Lines 77, 200-314 Problem <code>self._cached_key</code> used without locking in <code>get_storage_key()</code> Impact Potential race condition in parallel execution <p>Suggested Fix: Use <code>threading.Lock</code> or <code>functools.lru_cache</code>:</p> <pre><code>import threading\n\nclass AzureADLS:\n    def __init__(self, ...):\n        self._cache_lock = threading.Lock()\n        self._cached_key = None\n\n    def get_storage_key(self):\n        with self._cache_lock:\n            if self._cached_key is None:\n                self._cached_key = self._fetch_key()\n            return self._cached_key\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-16-hardcoded-magic-numbers","title":"Issue #16: Hardcoded Magic Numbers","text":"Attribute Details Files Various Problem Hardcoded values like <code>168</code> (hours), <code>1000</code> (chunk size), <code>30</code> (timeout) Impact Maintainability - hard to adjust settings <p>Examples: - <code>168</code> hours vacuum retention - <code>1000</code> chunk size for SQL writes - <code>30</code> second timeout - <code>10</code> max sample rows</p> <p>Suggested Fix: Move to configuration or named constants:</p> <pre><code># odibi/constants.py\nDEFAULT_VACUUM_RETENTION_HOURS = 168\nDEFAULT_SQL_CHUNK_SIZE = 1000\nDEFAULT_CONNECTION_TIMEOUT = 30\nDEFAULT_MAX_SAMPLE_ROWS = 10\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-17-empty-dataframe-check-uses-isempty","title":"Issue #17: Empty DataFrame Check Uses .isEmpty()","text":"Attribute Details File <code>odibi/engine/spark_engine.py</code> Lines 597 Problem <code>df.isEmpty()</code> triggers Spark action Impact Performance cost for simple empty check <p>Suggested Fix: Use more efficient check:</p> <pre><code># Instead of\nif df.isEmpty():\n\n# Use\nif df.limit(1).count() == 0:\n# Or\nif len(df.head(1)) == 0:\n</code></pre>"},{"location":"AUDIT_FINDINGS/#recommended-priority-order","title":"Recommended Priority Order","text":"<ol> <li>Issues #1, #2, #3 (batching/performance) - Highest ROI</li> <li>Issue #7 (connection cleanup) - Reliability</li> <li>Issues #5, #6 (consistency) - Maintainability</li> <li>Issue #10 (DRY principle) - Code quality</li> <li>Other issues as time permits</li> </ol>"},{"location":"AUDIT_FINDINGS/#related-previous-fixes","title":"Related Previous Fixes","text":"<p>The audit found that some issues were already addressed: - \u2705 19 individual Delta writes \u2192 batched (fixed) - \u2705 18 individual log_run writes \u2192 <code>log_runs_batch()</code> (fixed) - \u2705 Row count caching in read phase (partially fixed)</p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/","title":"Cross-Pipeline Dependencies Plan","text":""},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#overview","title":"Overview","text":"<p>Enable pipelines to reference outputs from other pipelines using a <code>$pipeline.node</code> syntax, resolved via Odibi's internal catalog. This supports the medallion architecture pattern (bronze \u2192 silver \u2192 gold) where silver nodes depend on bronze outputs.</p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#features","title":"Features","text":""},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#1-odibi-catalog-node-outputs-registry","title":"1. Odibi Catalog: Node Outputs Registry","text":"<p>New table: <code>meta_outputs</code></p> <p>Stores output metadata for every node that has a <code>write</code> block.</p> <pre><code>Schema:\n- pipeline_name: STRING (pipeline identifier)\n- node_name: STRING (node identifier)\n- output_type: STRING (\"external_table\" | \"managed_table\")\n- connection_name: STRING (nullable, for external tables)\n- path: STRING (nullable, storage path)\n- format: STRING (delta, parquet, etc.)\n- table_name: STRING (nullable, registered table name)\n- last_run: TIMESTAMP\n- row_count: LONG (nullable)\n- updated_at: TIMESTAMP\n</code></pre> <p>Primary key: <code>(pipeline_name, node_name)</code></p> <p>Location: <code>{base_path}/meta_outputs</code></p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#2-batch-registration-performance-critical","title":"2. Batch Registration (Performance Critical)","text":"<p>Problem: Per-node catalog writes caused 40+ second overhead in Databricks (17 nodes \u00d7 ~2-3s each).</p> <p>Solution: Collect metadata in-memory during execution, single MERGE at pipeline end.</p> <pre><code># In CatalogManager\ndef register_outputs_batch(self, records: List[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Batch registers/upserts multiple node outputs to meta_outputs.\n    Uses MERGE INTO for efficient upsert.\n\n    Args:\n        records: List of dicts with keys: pipeline_name, node_name,\n                 output_type, connection_name, path, format, table_name,\n                 last_run, row_count\n    \"\"\"\n    # Same pattern as register_pipelines_batch / register_nodes_batch\n    # Single MERGE INTO delta.`{target_path}`\n</code></pre> <p>Pipeline execution flow:</p> <pre><code># In Pipeline.run()\noutput_records = []\n\nfor node in execution_order:\n    result = execute_node(node)\n\n    if node.has_write:\n        output_records.append({\n            \"pipeline_name\": self.config.pipeline,\n            \"node_name\": node.name,\n            \"output_type\": \"external_table\" if node.write.path else \"managed_table\",\n            \"connection_name\": node.write.connection,\n            \"path\": node.write.path,\n            \"format\": node.write.format,\n            \"table_name\": node.write.register_table or node.write.table,\n            \"last_run\": datetime.now(timezone.utc),\n            \"row_count\": result.row_count,\n        })\n\n# Single batch write at end\nif output_records:\n    self.catalog_manager.register_outputs_batch(output_records)\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#3-cross-pipeline-reference-syntax-pipelinenode","title":"3. Cross-Pipeline Reference Syntax (<code>$pipeline.node</code>)","text":"<p>Usage in silver pipeline:</p> <pre><code>pipelines:\n  - pipeline: transform_silver\n    layer: silver\n    nodes:\n      - name: enriched_downtime\n        inputs:\n          events: $read_bronze.opsvisdata_ShiftDowntimeEventsview\n          calendar: $read_bronze.opsvisdata_vw_calender\n          plant: $read_bronze.opsvisdata_vw_dim_plantprocess\n        transform:\n          - operation: join\n            left: events\n            right: calendar\n            on: [DateId]\n          - operation: join\n            right: plant\n            on: [P_ID]\n        write:\n          connection: goat_prod\n          format: delta\n          path: \"silver/OEE/enriched_downtime\"\n</code></pre> <p>Resolution logic:</p> <pre><code>def resolve_input_reference(ref: str, catalog: CatalogManager) -&gt; Dict:\n    \"\"\"\n    Resolves $pipeline.node to read configuration.\n\n    Args:\n        ref: Reference string like \"$read_bronze.opsvisdata_vw_calender\"\n        catalog: CatalogManager instance\n\n    Returns:\n        Dict with keys: connection, path, format, table (for engine.read())\n    \"\"\"\n    if not ref.startswith(\"$\"):\n        raise ValueError(f\"Invalid reference: {ref}\")\n\n    parts = ref[1:].split(\".\", 1)  # Remove $ and split\n    if len(parts) != 2:\n        raise ValueError(f\"Invalid reference format: {ref}. Expected $pipeline.node\")\n\n    pipeline_name, node_name = parts\n\n    # Query meta_outputs\n    output = catalog.get_node_output(pipeline_name, node_name)\n\n    if output is None:\n        raise ValueError(\n            f\"No output found for {ref}. \"\n            f\"Ensure pipeline '{pipeline_name}' has run and node '{node_name}' has a write block.\"\n        )\n\n    if output[\"output_type\"] == \"managed_table\":\n        return {\n            \"table\": output[\"table_name\"],\n            \"format\": output[\"format\"],\n        }\n    else:  # external_table\n        return {\n            \"connection\": output[\"connection_name\"],\n            \"path\": output[\"path\"],\n            \"format\": output[\"format\"],\n        }\n</code></pre> <p>CatalogManager method:</p> <pre><code>def get_node_output(self, pipeline_name: str, node_name: str) -&gt; Optional[Dict]:\n    \"\"\"\n    Retrieves output metadata for a specific node.\n\n    Returns:\n        Dict with output metadata or None if not found.\n    \"\"\"\n    # Query meta_outputs table\n    # Use caching for performance (similar to _pipelines_cache pattern)\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#4-multi-input-nodes-inputs-block","title":"4. Multi-Input Nodes (<code>inputs:</code> block)","text":"<p>Config schema:</p> <pre><code>inputs:\n  # Reference to another pipeline's node output\n  events: $read_bronze.opsvisdata_ShiftDowntimeEventsview\n\n  # Or explicit read config (for non-dependent reads)\n  calendar:\n    connection: goat_prod\n    path: \"bronze/OEE/vw_calender\"\n    format: delta\n</code></pre> <p>Execution logic:</p> <pre><code>def execute_node_with_inputs(node: NodeConfig, engine: BaseEngine, catalog: CatalogManager):\n    \"\"\"\n    Executes a node that has an inputs block.\n    \"\"\"\n    dataframes = {}\n\n    for name, ref in node.inputs.items():\n        if isinstance(ref, str) and ref.startswith(\"$\"):\n            # Cross-pipeline reference\n            read_config = resolve_input_reference(ref, catalog)\n            dataframes[name] = engine.read(**read_config)\n        elif isinstance(ref, dict):\n            # Explicit read config\n            dataframes[name] = engine.read(**ref)\n        else:\n            raise ValueError(f\"Invalid input format for '{name}': {ref}\")\n\n    # Execute transforms with named dataframes\n    result = execute_transforms(node.transforms, dataframes)\n\n    return result\n</code></pre> <p>Transform execution with named inputs:</p> <pre><code>def execute_transforms(transforms: List[TransformConfig], dataframes: Dict[str, DataFrame]):\n    \"\"\"\n    Executes transforms using named dataframes.\n\n    First transform references input names directly.\n    Subsequent transforms can reference \"_result\" for chaining.\n    \"\"\"\n    result = None\n\n    for transform in transforms:\n        if transform.operation == \"join\":\n            left_df = dataframes.get(transform.left) or result\n            right_df = dataframes.get(transform.right)\n            result = left_df.join(right_df, on=transform.on, how=transform.how or \"inner\")\n        # ... other operations\n\n    return result\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#5-config-schema-updates","title":"5. Config Schema Updates","text":"<p>NodeConfig additions:</p> <pre><code>class NodeConfig(BaseModel):\n    name: str\n    description: Optional[str] = None\n\n    # Existing\n    read: Optional[ReadConfig] = None\n    depends_on: Optional[List[str]] = None\n\n    # New: multi-input support\n    inputs: Optional[Dict[str, Union[str, ReadConfig]]] = None\n\n    transform: Optional[List[TransformConfig]] = None\n    write: Optional[WriteConfig] = None\n\n    @validator(\"inputs\", \"read\")\n    def validate_input_source(cls, v, values):\n        # Node must have either 'read' or 'inputs', not both\n        if values.get(\"read\") and values.get(\"inputs\"):\n            raise ValueError(\"Node cannot have both 'read' and 'inputs'\")\n        return v\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#engine-compatibility","title":"Engine Compatibility","text":"Feature Spark Pandas Polars <code>meta_outputs</code> writes \u2705 MERGE \u2705 upsert \u2705 upsert <code>$pipeline.node</code> (path-based) \u2705 \u2705 \u2705 <code>$pipeline.node</code> (managed table) \u2705 \u274c \u274c <code>inputs:</code> block \u2705 \u2705 \u2705 <p>Best practice: Always use <code>path:</code> in write config for cross-engine compatibility.</p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#implementation-order","title":"Implementation Order","text":""},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-1-catalog-extension-complete","title":"Phase 1: Catalog Extension \u2705 COMPLETE","text":"<ol> <li>\u2705 Add <code>meta_outputs</code> table schema to CatalogManager</li> <li>\u2705 Add <code>_get_schema_meta_outputs()</code> method</li> <li>\u2705 Add <code>register_outputs_batch()</code> method (MERGE pattern)</li> <li>\u2705 Add <code>get_node_output()</code> query method with caching</li> <li>\u2705 Update <code>bootstrap()</code> to create meta_outputs table</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-2-pipeline-integration-complete","title":"Phase 2: Pipeline Integration \u2705 COMPLETE","text":"<ol> <li>\u2705 Collect output metadata during node execution</li> <li>\u2705 Batch write to catalog at pipeline end</li> <li>\u2705 Add error handling for catalog write failures</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-3-reference-resolution-complete","title":"Phase 3: Reference Resolution \u2705 COMPLETE","text":"<ol> <li>\u2705 Add <code>resolve_input_reference()</code> function (odibi/references.py)</li> <li>\u2705 Validate references at pipeline load time (fail fast)</li> <li>\u2705 Add clear error messages for missing dependencies</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-4-multi-input-nodes-complete","title":"Phase 4: Multi-Input Nodes \u2705 COMPLETE","text":"<ol> <li>\u2705 Add <code>inputs</code> field to NodeConfig</li> <li>\u2705 Update node executor to handle inputs block (<code>_execute_inputs_phase()</code>)</li> <li>\u2705 Update transform executor for named dataframes</li> <li>\u2705 Add validation: node has either <code>read</code> or <code>inputs</code>, not both</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-5-testing-complete","title":"Phase 5: Testing \u2705 COMPLETE","text":"<ol> <li>\u2705 Unit tests for <code>register_outputs_batch()</code></li> <li>\u2705 Unit tests for <code>resolve_input_reference()</code></li> <li>\u2705 Integration test: bronze pipeline writes, silver reads via <code>$ref</code></li> <li>\u2705 Performance test: verify single batch write (no per-node overhead)</li> </ol> <p>Implementation completed: All 26 tests passing in <code>tests/unit/test_cross_pipeline_dependencies.py</code></p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#example-full-bronze-silver-flow","title":"Example: Full Bronze \u2192 Silver Flow","text":"<p>Bronze pipeline (<code>read_bronze.yaml</code>):</p> <pre><code>pipeline: read_bronze\nlayer: bronze\nnodes:\n  - name: opsvisdata_ShiftDowntimeEventsview\n    read:\n      connection: opsvisdata\n      format: sql\n      table: OEE.ShiftDowntimeEventsview\n    write:\n      connection: goat_prod\n      format: delta\n      path: \"bronze/OEE/shift_downtime_events\"\n      register_table: test.shift_downtime_events\n      add_metadata: true\n\n  - name: opsvisdata_vw_calender\n    read:\n      connection: opsvisdata\n      format: sql\n      table: OEE.vw_calender\n    write:\n      connection: goat_prod\n      format: delta\n      path: \"bronze/OEE/vw_calender\"\n      register_table: test.vw_calender\n</code></pre> <p>After bronze runs, <code>meta_outputs</code> contains:</p> <pre><code>| pipeline_name | node_name                           | connection_name | path                           | format | table_name                  |\n|---------------|-------------------------------------|-----------------|--------------------------------|--------|-----------------------------|\n| read_bronze   | opsvisdata_ShiftDowntimeEventsview  | goat_prod       | bronze/OEE/shift_downtime_events | delta  | test.shift_downtime_events |\n| read_bronze   | opsvisdata_vw_calender              | goat_prod       | bronze/OEE/vw_calender         | delta  | test.vw_calender            |\n</code></pre> <p>Silver pipeline (<code>transform_silver.yaml</code>):</p> <pre><code>pipeline: transform_silver\nlayer: silver\nnodes:\n  - name: enriched_events\n    inputs:\n      events: $read_bronze.opsvisdata_ShiftDowntimeEventsview\n      calendar: $read_bronze.opsvisdata_vw_calender\n    transform:\n      - operation: join\n        left: events\n        right: calendar\n        on: [DateId]\n    write:\n      connection: goat_prod\n      format: delta\n      path: \"silver/OEE/enriched_events\"\n</code></pre> <p>Resolution:</p> <pre><code>$read_bronze.opsvisdata_ShiftDowntimeEventsview\n  \u2192 catalog lookup \u2192 {connection: goat_prod, path: bronze/OEE/shift_downtime_events, format: delta}\n  \u2192 engine.read(connection=goat_prod, path=\"bronze/OEE/shift_downtime_events\", format=\"delta\")\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Batch writes only \u2014 never write to catalog per-node</li> <li>Cache catalog queries \u2014 <code>get_node_output()</code> should cache results</li> <li>Validate early \u2014 resolve all <code>$references</code> at pipeline load, not execution</li> <li>Fail fast \u2014 clear errors if referenced pipeline/node hasn't run</li> </ol>"},{"location":"DOCUMENTATION_CAMPAIGN/","title":"Odibi Documentation Campaign","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#goal","title":"Goal","text":"<p>Make all documentation accurate, consistent, and professional - matching the quality of the codebase (958 tests passing).</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#context","title":"Context","text":"<p>Current State: 155+ markdown files across the project with significant gaps: - Broken links and missing files in mkdocs.yml - Duplicate files (yaml_schema.md in two locations) - Archived work mixed with user-facing docs - Root tracking files (GAPS.md, STABILITY_CAMPAIGN.md) outdated - GitHub issues #7-31 document specific gaps</p> <p>Success Criteria: - [x] <code>mkdocs build</code> runs without errors - [x] All nav links resolve to existing files - [x] No duplicate or broken files - [x] Root tracking files reflect current state - [x] GitHub documentation issues addressed or closed</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-1-cleanup-deletemove","title":"Phase 1: Cleanup (Delete/Move)","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#files-to-delete","title":"Files to Delete","text":"File Reason <code>docs/yaml_schema.md</code> Contains Python traceback error (3 lines)"},{"location":"DOCUMENTATION_CAMPAIGN/#files-to-move-to-_archive","title":"Files to Move to <code>_archive/</code>","text":"Source Destination <code>docs/archive/</code> (16 files) <code>_archive/docs/</code> <code>odibi/agents/docs/</code> (22 files) <code>_archive/agents/docs/</code> <code>odibi/agents/ROADMAP.md</code> <code>_archive/agents/</code> <code>odibi/agents/AUDIT_FIX_PLAN.md</code> <code>_archive/agents/</code>"},{"location":"DOCUMENTATION_CAMPAIGN/#duplicates-to-resolve","title":"Duplicates to Resolve","text":"File 1 File 2 Action <code>docs/WSL_SETUP.md</code> <code>docs/guides/wsl_setup.md</code> Delete <code>docs/WSL_SETUP.md</code>, keep guides version"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-2-fix-mkdocsyml","title":"Phase 2: Fix mkdocs.yml","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#broken-nav-links","title":"Broken Nav Links","text":"Current (broken) Fix <code>docs/patterns/hwm_pattern_guide.md</code> Change to <code>docs/patterns/incremental_stateful.md</code> <code>docs/guides/contributing.md</code> Change to <code>CONTRIBUTING.md</code> (root file)"},{"location":"DOCUMENTATION_CAMPAIGN/#missing-patterns-add-to-nav","title":"Missing Patterns (add to nav)","text":"<pre><code># Add under Patterns section:\n- SCD2 Pattern: docs/patterns/scd2.md\n- Incremental Stateful: docs/patterns/incremental_stateful.md\n- Smart Read: docs/patterns/smart_read.md\n- Skip If Unchanged: docs/patterns/skip_if_unchanged.md\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#missing-features-section-add-to-nav","title":"Missing Features Section (add to nav)","text":"<pre><code>- Features:\n    - Engines: docs/features/engines.md\n    - Pipelines: docs/features/pipelines.md\n    - Connections: docs/features/connections.md\n    - CLI: docs/features/cli.md\n    - Validation: docs/features/quality_gates.md\n    - Quarantine: docs/features/quarantine.md\n    - Alerting: docs/features/alerting.md\n    - Stories: docs/features/stories.md\n    - Catalog: docs/features/catalog.md\n    - Lineage: docs/features/lineage.md\n    - Transformers: docs/features/transformers.md\n    - Patterns: docs/features/patterns.md\n    - Schema Tracking: docs/features/schema_tracking.md\n    - Diagnostics: docs/features/diagnostics.md\n    - Orchestration: docs/features/orchestration.md\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-3-update-root-tracking-files","title":"Phase 3: Update Root Tracking Files","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#gapsmd","title":"GAPS.md","text":"<p>Verify each gap against current code:</p> Gap Status to Verify GAP-001: datetime.utcnow deprecation Check if fixed GAP-002: Pydantic .dict() Check if fixed GAP-003: Pandas FutureWarning Check if fixed GAP-004: Polars API deprecation Check if fixed GAP-005: GitHub Events dataset Check if exists GAP-006-010 Verify current state <p>Update checkboxes in \"Roadmap Recommendations\" section.</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#stability_campaignmd","title":"STABILITY_CAMPAIGN.md","text":"<p>Update success criteria checkboxes: - [x] All tests passing (958 on Windows) - [x] BUGS.md addressed (24 fixed) - [ ] Review remaining items</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#readmemd","title":"README.md","text":"<p>Fix broken documentation links:</p> Current (broken) Fix <code>docs/guides/cli_master.md</code> <code>docs/guides/cli_master_guide.md</code> <code>docs/guides/writing-transformations.md</code> <code>docs/guides/writing_transformations.md</code>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-4-audit-reference-docs","title":"Phase 4: Audit Reference Docs","text":"<p>Verify against actual code implementation:</p> File Verify Against <code>docs/reference/yaml_schema.md</code> <code>odibi/config.py</code> Pydantic models <code>docs/reference/cheatsheet.md</code> CLI commands, YAML options <code>docs/reference/configuration.md</code> <code>odibi/config.py</code> <code>docs/reference/PARITY_TABLE.md</code> Actual engine implementations"},{"location":"DOCUMENTATION_CAMPAIGN/#yaml_schemamd-regeneration","title":"yaml_schema.md Regeneration","text":"<p>Per AGENTS.md, regenerate from Pydantic models:</p> <pre><code>python odibi/introspect.py\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-5-audit-guides","title":"Phase 5: Audit Guides","text":"<p>For each guide, verify accuracy:</p> Guide Key Checks <code>cli_master_guide.md</code> All commands exist and work <code>writing_transformations.md</code> Registration process accurate <code>production_deployment.md</code> Polars mentioned? Engine options correct? <code>performance_tuning.md</code> Polars optimization section? <code>secrets.md</code> Implementation details accurate? <code>MIGRATION_GUIDE.md</code> V3 references still valid? <code>best_practices.md</code> Validation best practices included?"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-6-audit-feature-docs","title":"Phase 6: Audit Feature Docs","text":"<p>For each feature doc, verify: 1. Feature exists in code 2. Configuration examples work 3. No references to non-existent features</p> Feature Doc Priority Issue # <code>engines.md</code> HIGH #13 (Polars missing) <code>quarantine.md</code> HIGH #23 (Integration unclear) <code>quality_gates.md</code> MEDIUM #11 (Error handling) <code>schema_tracking.md</code> MEDIUM #31 (Not documented) <code>cli.md</code> MEDIUM #14 (Commands may not exist)"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-7-address-github-issues","title":"Phase 7: Address GitHub Issues","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#documentation-issues-to-closeupdate","title":"Documentation Issues to Close/Update","text":"Issue Title Action #7 Configuration.md Incomplete Audit and update #8 Getting Started Missing Validation Add example #9 README.md Commands Outdated Verify commands #10 FK Validation Integration Unclear Update docs/validation/fk.md #11 Validation Error Handling Not Documented Add to feature doc #13 Polars Engine Missing from Docs Add to engines.md #14 CLI Guide References Non-Existent Commands Audit CLI #15 Version Mismatch Between Docs Standardize versions #16 Secrets Management Missing Details Update secrets.md #17 Patterns Doc References Non-Existent File Fix reference #18 Engine Parity Table Incomplete Update PARITY_TABLE.md #19 Architecture Doc May Be Outdated Audit architecture.md #20 Date Dimension Config Not Documented Add to pattern doc #21 Custom SQL Transformation Unclear Update guide #23 Quarantine Not Integrated with Validation Guide Cross-link docs #24 Performance Guide Missing Polars Add Polars section #25 Writing Transformations Missing Registration Add registration #26 Migration Guide V3 Outdated Update or note #27 Best Practices Missing Validation Add section #28 Production Deployment References Polars Verify/add #29 CLI Commands May Not Match Audit cheatsheet #30 Cross-Env Secrets Not Documented Add to secrets.md #31 Schema Tracking Not Documented Create/update doc"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-8-polish","title":"Phase 8: Polish","text":"<ol> <li>Consistent formatting across all docs</li> <li>Cross-links between related docs</li> <li>Navigation - ensure logical flow</li> <li>Build test - <code>mkdocs build</code> succeeds</li> <li>Visual review - <code>mkdocs serve</code> and check</li> </ol>"},{"location":"DOCUMENTATION_CAMPAIGN/#files-summary","title":"Files Summary","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#keep-user-facing","title":"Keep (User-Facing)","text":"<pre><code>docs/\n\u251c\u2500\u2500 tutorials/          # 14 files - Getting started, dimensional modeling\n\u251c\u2500\u2500 guides/             # 15 files - How-to guides\n\u251c\u2500\u2500 reference/          # 9 files - API, schema, cheatsheet\n\u251c\u2500\u2500 patterns/           # 13 files - Pattern documentation\n\u251c\u2500\u2500 features/           # 15 files - Feature documentation\n\u251c\u2500\u2500 semantics/          # 4 files - Semantic layer\n\u251c\u2500\u2500 validation/         # 1 file - FK validation\n\u251c\u2500\u2500 explanation/        # 2 files - Architecture, case studies\n\u2514\u2500\u2500 README.md           # Docs index\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#delete","title":"Delete","text":"<pre><code>docs/yaml_schema.md     # Broken (contains traceback)\ndocs/WSL_SETUP.md       # Duplicate of guides/wsl_setup.md\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#move-to-_archive","title":"Move to _archive/","text":"<pre><code>docs/archive/           # 16 historical gap analysis files\nodibi/agents/docs/      # 22 archived agent docs\nodibi/agents/*.md       # Agent planning docs\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#keep-but-update","title":"Keep but Update","text":"<pre><code>GAPS.md                 # Update status of each gap\nSTABILITY_CAMPAIGN.md   # Check off completed items\nREADME.md               # Fix broken links\nBUGS.md                 # Keep as-is (accurate)\nCHANGELOG.md            # Keep as-is (accurate)\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#estimated-effort","title":"Estimated Effort","text":"Phase Effort Files 1. Cleanup 15 min ~40 2. mkdocs.yml 15 min 1 3. Root files 30 min 3 4. Reference audit 1 hr 5 5. Guides audit 1.5 hr 15 6. Features audit 1 hr 15 7. GitHub issues 2 hr 25 issues 8. Polish 30 min All <p>Total: ~7 hours</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#success-metrics","title":"Success Metrics","text":"Metric Before After mkdocs build errors Unknown 0 Broken nav links 2 0 Duplicate files 2 0 GitHub doc issues open 25 0 Pattern docs in nav 8 13 Feature docs in nav 0 15"},{"location":"DOCUMENTATION_QUALITY_AUDIT/","title":"Documentation Quality Audit","text":"<p>Audit Date: December 21, 2025 Scope: Post-Reference Enrichment Campaign Review Audited Files: - <code>docs/reference/yaml_schema.md</code> - <code>docs/tutorials/getting_started.md</code> - <code>docs/guides/</code> (16 guides) - <code>docs/features/quality_gates.md</code>, <code>docs/features/quarantine.md</code></p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#executive-summary","title":"Executive Summary","text":"Criterion Score Status 1. Beginner Test 4/5 \u2705 Good 2. Example Quality 4/5 \u2705 Good 3. Navigation (See Also) 3/5 \u26a0\ufe0f Needs Work 4. Terminology Consistency 3/5 \u26a0\ufe0f Needs Work 5. Coverage (Gaps) 4/5 \u2705 Good <p>Overall Score: 3.6/5 (Good, with key improvement areas)</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#1-beginner-test-score-45","title":"1. Beginner Test (Score: 4/5)","text":"<p>Question: Can someone new to data engineering understand Contracts vs Validation vs Quality Gates?</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#strengths","title":"Strengths","text":"<p>\u2705 Excellent comparison table in <code>yaml_schema.md:1387-1394</code>:</p> <pre><code>| Feature | When it Runs | On Failure | Use Case |\n|---------|--------------|------------|----------|\n| Contracts | Before transform | Always fails | Input data quality |\n| Validation | After transform | Configurable | Output data quality |\n| Quality Gates | After validation | Configurable | Pipeline-level thresholds |\n| Quarantine | With validation | Routes bad rows | Capture invalid records |\n</code></pre> <p>\u2705 Getting started tutorial (<code>getting_started.md</code>) progressively introduces concepts with working examples</p> <p>\u2705 \"Beginner Note\" callouts in yaml_schema.md (e.g., lines 3366-3369 for DimensionPattern) explain concepts in plain language</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#issues-found","title":"Issues Found","text":"Issue File:Line Severity Recommended Fix Missing validation README <code>docs/validation/README.md</code> Medium Create overview page linking fk.md and explaining validation concepts hierarchy Contracts introduced without definition <code>getting_started.md:173-175</code> Low Add 1-sentence definition before \"Contracts validate data before processing\" No visual diagram of validation flow <code>yaml_schema.md</code> Contracts section Medium Add Mermaid flowchart showing Contracts \u2192 Transform \u2192 Validation \u2192 Gate \u2192 Write"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#2-example-quality-score-45","title":"2. Example Quality (Score: 4/5)","text":"<p>Question: Are YAML examples realistic, complete, and copy-pasteable?</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#strengths_1","title":"Strengths","text":"<p>\u2705 Business Problem + Recipe format in yaml_schema.md is excellent: - \"The Time Traveler\" recipe (line 628) - \"The Indestructible Pipeline\" pattern (line 910) - \"CDC Without CDC\" guide (line 829)</p> <p>\u2705 Complete end-to-end examples with connections, pipelines, and writes</p> <p>\u2705 Scenario variations (Scenario 1, 2, 3...) show different use cases</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#issues-found_1","title":"Issues Found","text":"Issue File:Line Severity Recommended Fix Incomplete node example <code>yaml_schema.md:74-75</code> Medium Replace <code>...</code> with actual node config Example uses undefined connection <code>yaml_schema.md:163</code> \"s3_landing\" Low Add comment that connection must be defined in project Missing <code>...</code> explanation <code>yaml_schema.md:38</code> Low Add note: \"# ... (pipelines section follows)\" Regex pattern unescaped <code>yaml_schema.md:945</code> Medium Escape backslashes for YAML: <code>\"^[\\\\w\\\\.-]+@...\"</code> Quarantine example missing <code>format</code> <code>features/quarantine.md:162-165</code> Low Add <code>format: delta</code> or <code>format: parquet</code> to write block"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#excellent-examples-to-preserve","title":"Excellent Examples to Preserve","text":"<ul> <li><code>getting_started.md:151-170</code> - Complete inline validation config</li> <li><code>yaml_schema.md:3397-3410</code> - DimensionPattern full example</li> <li><code>yaml_schema.md:3521-3544</code> - FactPattern with all options</li> <li><code>guides/dimensional_modeling_guide.md:405-426</code> - Target declarative config</li> </ul>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#3-navigation-see-also-links-score-35","title":"3. Navigation / See Also Links (Score: 3/5)","text":"<p>Question: Do \"See Also\" links help users find related content?</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#strengths_2","title":"Strengths","text":"<p>\u2705 28 \"See Also\" references found across docs \u2705 Cross-references between yaml_schema.md and features/ are present \u2705 guides/writing_transformations.md:294-298 has excellent related links</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#issues-found_2","title":"Issues Found","text":"Issue File:Line Severity Recommended Fix Broken link <code>yaml_schema.md:907</code> <code>../features/quality_gates.md</code> High Validate link resolves correctly Broken link <code>getting_started.md:214</code> <code>../validation/README.md</code> High Create file or change link to <code>../features/quality_gates.md</code> Self-referential link <code>yaml_schema.md:787</code> \"See Also: Transformer Catalog \u2192 #nodeconfig\" Medium Should link to specific transformer section, not NodeConfig Missing pattern cross-links <code>yaml_schema.md:3347-3600</code> Data Patterns section Medium Add \"See Also: [patterns/fact.md]\" after FactPattern No See Also in <code>guides/best_practices.md</code> Low Add links to related guides Inconsistent anchor format <code>yaml_schema.md</code> Low Some use <code>#gateconfig</code>, some use <code>#contracts-data-quality-gates</code>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#missing-navigation","title":"Missing Navigation","text":"<p>The following configs in <code>yaml_schema.md</code> have NO \"See Also\" or \"When to Use\": - <code>StoryConfig</code> (line 46) - <code>LineageConfig</code> (line 1686) - <code>RetryConfig</code> (line 1791) - <code>LoggingConfig</code> (line 1741) - <code>EnvironmentConfig</code> (check for existence)</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#4-terminology-consistency-score-35","title":"4. Terminology Consistency (Score: 3/5)","text":"<p>Question: Is terminology consistent across docs?</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#inconsistencies-found","title":"Inconsistencies Found","text":"Term Variant Files Using It Recommended Standard \"contracts\" vs \"tests\" vs \"checks\" Mixed usage Use \"contracts\" for pre-transform, \"tests\" for validation tests, \"checks\" only informally \"validation tests\" vs \"validation rules\" <code>getting_started.md:149</code> uses \"rules\", <code>yaml_schema.md</code> uses \"tests\" Standardize on \"tests\" \"Quality Gates\" vs \"Gates\" vs \"gate\" Mixed case and singular/plural Use \"Quality Gates\" (title case, plural) in prose \"quarantine\" vs \"Quarantine Tables\" <code>quarantine.md</code> title vs inline usage Use \"quarantine\" (lowercase) for action, \"Quarantine Table\" for the table \"transformer\" vs \"Transformer\" vs \"pattern\" <code>yaml_schema.md:130-142</code> explains difference but usage is mixed Document that <code>transformer:</code> (YAML key) refers to catalog functions, <code>pattern:</code> is for dimensional patterns"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#specific-issues","title":"Specific Issues","text":"Issue File:Line Recommended Fix \"data quality checks\" used where \"validation tests\" meant <code>getting_started.md:145</code> Change to \"validation tests\" \"contracts\" called \"Pre-condition contracts (Circuit Breakers)\" <code>yaml_schema.md:393</code> Simplify: just \"Contracts\" - Circuit Breaker metaphor is confusing \"on_failure\" vs \"on_fail\" API uses both Audit codebase and standardize (prefer <code>on_fail</code>)"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#5-gaps-missing-coverage-score-45","title":"5. Gaps / Missing Coverage (Score: 4/5)","text":"<p>Question: Are there configs with no examples or unclear descriptions?</p>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#configs-missing-examples","title":"Configs Missing Examples","text":"Config File:Line Priority <code>CustomConnectionConfig</code> yaml_schema.md (not found in sample) Medium - add example <code>streaming: true</code> on NodeConfig yaml_schema.md:383 High - add streaming node example <code>log_level</code> per-node yaml_schema.md:390 Low - add one-liner <code>cache: true</code> yaml_schema.md:389 Medium - explain when useful"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#configs-with-unclear-descriptions","title":"Configs with Unclear Descriptions","text":"Config Issue Recommended Fix <code>vars</code> (yaml_schema.md:52) Description says \"for substitution\" but no example of <code>${vars.env}</code> Add example showing variable resolution <code>semantic</code> (yaml_schema.md:58) Description is long but lacks inline example Add minimal inline example <code>story</code> (yaml_schema.md:46) \"Story generation configuration (mandatory)\" - no details Link to features/stories.md <code>system</code> (yaml_schema.md:47) \"System Catalog configuration (mandatory)\" - minimal explanation Link to features/catalog.md"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#tutorial-gaps","title":"Tutorial Gaps","text":"Gap Location Priority No tutorial for Spark engine tutorials/ Medium No tutorial for Azure connections tutorials/ Medium Getting started doesn't show SCD2 or dimensions getting_started.md Low (advanced topic)"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#prioritized-recommendations","title":"Prioritized Recommendations","text":""},{"location":"DOCUMENTATION_QUALITY_AUDIT/#p0-fix-immediately-broken","title":"P0 - Fix Immediately (Broken)","text":"<ol> <li>Create <code>docs/validation/README.md</code> - currently 404'd from getting_started.md</li> <li>Fix broken link in getting_started.md:214 to validation README</li> </ol>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#p1-high-impact-this-week","title":"P1 - High Impact (This Week)","text":"<ol> <li>Add validation flow diagram (Mermaid) to yaml_schema.md Contracts section</li> <li>Complete incomplete example at yaml_schema.md:74-75</li> <li>Standardize terminology - create a glossary or add to AGENTS.md</li> </ol>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#p2-medium-impact-this-sprint","title":"P2 - Medium Impact (This Sprint)","text":"<ol> <li>Add \"See Also\" to Data Patterns section linking to pattern guides</li> <li>Add examples for: <code>streaming</code>, <code>cache</code>, <code>vars</code></li> <li>Add \"When to Use\" to: <code>StoryConfig</code>, <code>LineageConfig</code>, <code>RetryConfig</code></li> </ol>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#p3-polish-backlog","title":"P3 - Polish (Backlog)","text":"<ol> <li>Escape regex patterns in YAML examples</li> <li>Add <code>format:</code> to quarantine.md example</li> <li>Consistent anchor formatting</li> </ol>"},{"location":"DOCUMENTATION_QUALITY_AUDIT/#metrics-for-future-audits","title":"Metrics for Future Audits","text":"<p>Track these metrics monthly:</p> Metric Current Target Configs with \"When to Use\" ~60% 100% Configs with \"See Also\" ~40% 80% Broken links 2 0 Examples that compile/validate Unknown 100% <p>Audit conducted by: Documentation Quality Review Next audit recommended: After P0/P1 items complete</p>"},{"location":"FEATURE_PARITY_AUDIT/","title":"Odibi Feature Parity Audit Report","text":"<p>Generated: 2024 Scope: Config vs Implementation, Engine Parity, CLI vs API, Docs vs Code</p>"},{"location":"FEATURE_PARITY_AUDIT/#executive-summary","title":"Executive Summary","text":"<p>This audit identifies feature parity gaps across the Odibi framework. Issues are categorized by severity:</p> Severity Count Description \ud83d\udd34 Critical 3 Documented but not implemented, or silently ignored \ud83d\udfe1 Medium 8 Missing in one engine/mode but present in another \ud83d\udfe2 Low 4 Minor inconsistencies or missing documentation"},{"location":"FEATURE_PARITY_AUDIT/#critical-issues","title":"\ud83d\udd34 Critical Issues","text":""},{"location":"FEATURE_PARITY_AUDIT/#1-cli-missing-tag-flag-documented-but-not-implemented","title":"1. CLI Missing <code>--tag</code> Flag (Documented but Not Implemented)","text":"<p>Location: <code>odibi/cli/main.py</code> (run command)</p> <p>Issue: The documentation and config reference describe tag-based execution (<code>odibi run --tag daily</code>), but the <code>--tag</code> flag is not implemented in the CLI.</p> <p>Evidence: - Config: <code>NodeConfig.tags</code> field exists (config.py#L2030) - Docs: \"Run only this with <code>odibi run --tag daily</code>\" (yaml_schema.md#L127) - CLI: No <code>--tag</code> argument in cli/main.py</p> <p>Impact: Users cannot run pipelines by tag despite documentation saying they can.</p> <p>Fix: Add <code>--tag</code> argument to the run parser and filter nodes in <code>Pipeline.run()</code>.</p>"},{"location":"FEATURE_PARITY_AUDIT/#2-cli-missing-node-and-pipeline-flags-for-run-command","title":"2. CLI Missing <code>--node</code> and <code>--pipeline</code> Flags for Run Command","text":"<p>Location: <code>odibi/cli/main.py</code> (run command)</p> <p>Issue: Orchestration templates (<code>odibi/orchestration/airflow.py</code>, <code>odibi/orchestration/dagster.py</code>) generate commands like:</p> <pre><code>odibi run --pipeline {{ pipeline_name }} --node {{ node.name }}\n</code></pre> <p>But these flags do not exist in the CLI.</p> <p>Evidence: - airflow.py#L39: <code>--pipeline</code> and <code>--node</code> usage - dagster.py#L61: <code>--pipeline</code> and <code>--node</code> usage - CLI main.py: No such flags exist</p> <p>Impact: Generated Airflow/Dagster DAGs will fail to execute.</p> <p>Fix: Add <code>--pipeline</code> and <code>--node</code> arguments to the run command.</p>"},{"location":"FEATURE_PARITY_AUDIT/#3-materialized-config-option-has-no-effect","title":"3. <code>materialized</code> Config Option Has No Effect","text":"<p>Location: <code>odibi/config.py</code> (NodeConfig), <code>odibi/node.py</code></p> <p>Issue: The <code>materialized</code> field (<code>table</code>, <code>view</code>, <code>incremental</code>) is defined in config but never read or used in node execution.</p> <p>Evidence: - Config: config.py#L2018 defines <code>materialized</code> field - Node.py: No references to <code>config.materialized</code> in execution logic - Docs: Referenced in yaml_schema.md#L338</p> <p>Impact: Users can set this option but it's silently ignored.</p> <p>Fix: Either implement materialization strategies or remove/deprecate the field.</p>"},{"location":"FEATURE_PARITY_AUDIT/#medium-issues","title":"\ud83d\udfe1 Medium Issues","text":""},{"location":"FEATURE_PARITY_AUDIT/#4-polars-engine-not-exposed-in-config-enum","title":"4. Polars Engine Not Exposed in Config Enum","text":"<p>Location: <code>odibi/config.py</code>, <code>odibi/enums.py</code></p> <p>Issue: <code>PolarsEngine</code> exists and is registered, but <code>EngineType</code> enum only has <code>SPARK</code> and <code>PANDAS</code>.</p> <p>Evidence: - polars_engine.py: Full implementation exists - engine/registry.py#L25: Registered as \"polars\" - config.py#L14-L18: Only SPARK/PANDAS</p> <p>Impact: Users cannot set <code>engine: polars</code> in YAML config.</p> <p>Fix: Add <code>POLARS = \"polars\"</code> to <code>EngineType</code> enum.</p>"},{"location":"FEATURE_PARITY_AUDIT/#5-streaming-write-not-implemented-read-only-fixed","title":"5. ~~Streaming Write Not Implemented (Read Only)~~ \u2705 FIXED","text":"<p>Location: <code>odibi/engine/spark_engine.py</code>, <code>odibi/config.py</code></p> <p>Status: \u2705 Implemented</p> <p>Implementation: - Added <code>StreamingWriteConfig</code> and <code>TriggerConfig</code> models in <code>config.py</code> - Added <code>streaming</code> field to <code>WriteConfig</code> - Implemented <code>_write_streaming()</code> method in <code>SparkEngine</code> - Supports all trigger types: <code>processing_time</code>, <code>once</code>, <code>available_now</code>, <code>continuous</code> - Supports all output modes: <code>append</code>, <code>update</code>, <code>complete</code></p> <p>Usage:</p> <pre><code>write:\n  connection: delta_lake\n  format: delta\n  table: events_stream\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events\"\n    trigger:\n      processing_time: \"10 seconds\"\n</code></pre>"},{"location":"FEATURE_PARITY_AUDIT/#6-partition_by-not-applied-for-delta-merge-operations","title":"6. <code>partition_by</code> Not Applied for Delta Merge Operations","text":"<p>Location: <code>odibi/engine/pandas_engine.py</code>, <code>odibi/engine/spark_engine.py</code></p> <p>Issue: For <code>upsert</code>/<code>append_once</code> modes with Delta tables, <code>partition_by</code> is extracted but applied after the merge logic completes. The merge itself doesn't partition-optimize.</p> <p>Evidence: - Spark: spark_engine.py#L800-L805 - partition_by applied post-merge - Pandas: Delta merge at pandas_engine.py#L773-L810 doesn't use partition_by</p> <p>Impact: Performance not optimal for partitioned upsert operations.</p>"},{"location":"FEATURE_PARITY_AUDIT/#7-pandasengine-missing-as_of_timestamp-time-travel","title":"7. PandasEngine Missing <code>as_of_timestamp</code> Time Travel","text":"<p>Location: <code>odibi/engine/pandas_engine.py</code></p> <p>Issue: <code>as_of_timestamp</code> is accepted but only <code>as_of_version</code> (versionAsOf) is implemented for Delta time travel.</p> <p>Evidence: - Method signature accepts both: pandas_engine.py#L187 - Only version is used: pandas_engine.py#L246-L247 - SparkEngine supports both: spark_engine.py#L443-L448</p> <p>Impact: Timestamp-based time travel silently ignored in Pandas engine.</p>"},{"location":"FEATURE_PARITY_AUDIT/#8-polarsengine-missing-several-abstract-methods","title":"8. PolarsEngine Missing Several Abstract Methods","text":"<p>Location: <code>odibi/engine/polars_engine.py</code></p> <p>Issue: PolarsEngine doesn't implement all abstract methods from <code>Engine</code> base class: - <code>harmonize_schema()</code> - Not implemented - <code>anonymize()</code> - Not implemented - <code>add_write_metadata()</code> - Not implemented - <code>table_exists()</code> - Partial (file only) - <code>maintain_table()</code> - Not implemented</p> <p>Impact: Polars pipelines will fail if these features are used.</p>"},{"location":"FEATURE_PARITY_AUDIT/#9-first_run_query-only-works-with-target-table-check","title":"9. <code>first_run_query</code> Only Works with Target Table Check","text":"<p>Location: <code>odibi/node.py</code> (NodeExecutor._execute_read_phase)</p> <p>Issue: <code>first_run_query</code> is only applied when <code>write</code> config exists and target table doesn't exist. If a node has <code>read</code> + <code>transform</code> but no <code>write</code>, the query is never used.</p> <p>Evidence: node.py#L344-L352</p>"},{"location":"FEATURE_PARITY_AUDIT/#10-delete_detection-config-never-read","title":"10. <code>delete_detection</code> Config Never Read","text":"<p>Location: <code>odibi/config.py</code>, <code>odibi/node.py</code></p> <p>Issue: <code>DeleteDetectionConfig</code> is fully defined with validation, but the node executor never reads or applies delete detection logic.</p> <p>Evidence: - Config: config.py#L160-L293 - full implementation - Transformer exists: transformers/delete_detection.py - Not wired: Must be used as explicit transform step, not automatic</p> <p>Clarification: This works if user explicitly adds <code>operation: detect_deletes</code> in transform steps. The config examples show this is the intended pattern.</p>"},{"location":"FEATURE_PARITY_AUDIT/#11-environments-config-not-validated-or-applied","title":"11. <code>environments</code> Config Not Validated or Applied","text":"<p>Location: <code>odibi/config.py</code> (ProjectConfig)</p> <p>Issue: The <code>environments</code> field is defined but has a validator that does nothing:</p> <pre><code>@model_validator(mode=\"after\")\ndef check_environments_not_implemented(self):\n    # Implemented in Phase 3\n    return self\n</code></pre> <p>Evidence: config.py#L980-L984</p> <p>Impact: Users can define environments but they're silently ignored.</p>"},{"location":"FEATURE_PARITY_AUDIT/#low-issues","title":"\ud83d\udfe2 Low Issues","text":""},{"location":"FEATURE_PARITY_AUDIT/#12-docs-reference-non-existent-commands","title":"12. Docs Reference Non-Existent Commands","text":"<p>Location: <code>docs/guides/cli_master_guide.md</code></p> <p>Issue: Docs reference <code>odibi init-vscode</code> but this command doesn't exist in CLI.</p> <p>Evidence: cli_master_guide.md#L56-L59</p>"},{"location":"FEATURE_PARITY_AUDIT/#13-retrybackoff-types-inconsistent","title":"13. <code>retry.backoff</code> Types Inconsistent","text":"<p>Location: <code>odibi/config.py</code>, <code>odibi/node.py</code></p> <p>Issue: Config defines <code>BackoffStrategy</code> enum with values <code>exponential</code>, <code>linear</code>, <code>constant</code>, but node retry logic only handles <code>exponential</code> and <code>linear</code>.</p> <p>Evidence: - Config: config.py#L730-L733 - includes <code>CONSTANT</code> - Node: node.py#L963-L968 - no <code>constant</code> handling</p>"},{"location":"FEATURE_PARITY_AUDIT/#14-prepost-sql-not-documented-in-cli-docs","title":"14. Pre/Post SQL Not Documented in CLI Docs","text":"<p>Location: <code>docs/features/cli.md</code></p> <p>Issue: <code>pre_sql</code> and <code>post_sql</code> node options are implemented but not mentioned in CLI documentation.</p>"},{"location":"FEATURE_PARITY_AUDIT/#15-validation_mode-on-connections-never-used","title":"15. <code>validation_mode</code> on Connections Never Used","text":"<p>Location: <code>odibi/config.py</code> (BaseConnectionConfig)</p> <p>Issue: All connection configs inherit <code>validation_mode: ValidationMode</code> but this field is never read during pipeline execution.</p>"},{"location":"FEATURE_PARITY_AUDIT/#write-mode-support-matrix","title":"Write Mode Support Matrix","text":"Mode PandasEngine SparkEngine Notes <code>overwrite</code> \u2705 \u2705 Both support all formats <code>append</code> \u2705 \u2705 Both support all formats <code>upsert</code> \u2705 Delta \u2705 Delta Both require <code>keys</code> option <code>append_once</code> \u2705 Delta \u2705 Delta Both require <code>keys</code> option <code>partition_by</code> \u2705 \u2705 Applied via options dict <code>merge_keys</code> \u274c \u274c Use <code>keys</code> in options instead"},{"location":"FEATURE_PARITY_AUDIT/#cli-vs-api-feature-matrix","title":"CLI vs API Feature Matrix","text":"Feature CLI Python API Notes Run pipeline \u2705 \u2705 Dry run \u2705 \u2705 Resume \u2705 \u2705 Parallel \u2705 \u2705 Filter by tag \u274c \u274c Not implemented Filter by node \u274c \u274c Not implemented Filter by pipeline \u274c \u2705 CLI doesn't support On-error override \u2705 \u2705"},{"location":"FEATURE_PARITY_AUDIT/#recommendations","title":"Recommendations","text":""},{"location":"FEATURE_PARITY_AUDIT/#high-priority","title":"High Priority","text":"<ol> <li>Add <code>--tag</code>, <code>--node</code>, <code>--pipeline</code> to CLI run command - Breaks orchestration exports</li> <li>Implement or remove <code>materialized</code> config - Silent no-op confuses users</li> <li>Add POLARS to EngineType enum - Blocks Polars adoption</li> </ol>"},{"location":"FEATURE_PARITY_AUDIT/#medium-priority","title":"Medium Priority","text":"<ol> <li>Implement streaming write support in SparkEngine</li> <li>Add <code>as_of_timestamp</code> support to PandasEngine</li> <li>Complete PolarsEngine abstract method implementations</li> </ol>"},{"location":"FEATURE_PARITY_AUDIT/#low-priority","title":"Low Priority","text":"<ol> <li>Add <code>constant</code> backoff handling</li> <li>Document pre_sql/post_sql in CLI docs</li> <li>Remove or implement <code>validation_mode</code> on connections</li> <li>Remove or implement <code>environments</code> config</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/","title":"Odibi Logging Audit Report","text":"<p>Date: 2025-12-01 Status: Complete Test Results: 655 passed, 0 failed, 20 skipped</p>"},{"location":"LOGGING_AUDIT_REPORT/#executive-summary","title":"Executive Summary","text":"<p>This audit enhanced the Odibi framework with comprehensive, structured logging across all major modules. The goal was to eliminate silent failures, provide better observability, and enable effective debugging in production environments.</p>"},{"location":"LOGGING_AUDIT_REPORT/#key-achievements","title":"Key Achievements","text":"<ul> <li>Created new <code>LoggingContext</code> infrastructure for context-aware logging</li> <li>Added structured logging with timing, row counts, and schema tracking</li> <li>Implemented secret redaction to prevent credential leaks</li> <li>Enhanced ~25 modules with detailed operation logging</li> <li>Fixed all silent failure patterns identified</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#new-infrastructure","title":"New Infrastructure","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibiutilslogging_contextpy","title":"<code>odibi/utils/logging_context.py</code>","text":"<p>A new module providing context-based logging with automatic correlation:</p> Component Description <code>OperationType</code> Enum categorizing operations (READ, WRITE, TRANSFORM, VALIDATE, etc.) <code>OperationMetrics</code> Dataclass tracking timing, row counts, schema changes, memory <code>StructuredLogger</code> Base logger supporting human-readable and JSON output with secret redaction <code>LoggingContext</code> Context-aware wrapper with pipeline/node/engine tracking <p>Key Features: - Automatic context injection (pipeline_id, node_id, engine) - Operation timing with millisecond precision - Row count tracking with delta calculations - Schema change detection (columns added/removed/type changes) - Secret redaction for connection strings and credentials</p> <p>Helper Methods: - <code>log_file_io()</code> - File read/write operations - <code>log_spark_metrics()</code> - Spark-specific metrics (partitions, broadcasts) - <code>log_pandas_metrics()</code> - Pandas-specific metrics (memory, dtypes) - <code>log_validation_result()</code> - Validation pass/fail with details - <code>log_graph_operation()</code> - Dependency graph operations - <code>log_connection()</code> - Connection lifecycle events - <code>log_schema_change()</code> - Schema transformation tracking - <code>log_row_count_change()</code> - Row count deltas with warnings</p>"},{"location":"LOGGING_AUDIT_REPORT/#enhanced-modules","title":"Enhanced Modules","text":""},{"location":"LOGGING_AUDIT_REPORT/#core-pipeline-components","title":"Core Pipeline Components","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibigraphpy","title":"<code>odibi/graph.py</code>","text":"<ul> <li>Added: Node count, edge count, layer count logging</li> <li>Added: Cycle detection with node names in error messages</li> <li>Added: Resolution phase timing</li> <li>Fixed: Silent failures in dependency resolution</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibinodepy","title":"<code>odibi/node.py</code>","text":"<ul> <li>Added: Per-phase logging (source_read, transform, validation, sink_write)</li> <li>Added: Row count tracking through execution phases</li> <li>Added: Schema change logging between transformations</li> <li>Added: Timing metrics for each phase</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibipipelinepy","title":"<code>odibi/pipeline.py</code>","text":"<ul> <li>Added: Pipeline start/end logging with run IDs</li> <li>Added: Layer-by-layer execution progress</li> <li>Added: Parallel node execution tracking</li> <li>Added: Summary metrics (total nodes, duration, failures)</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#configuration-utilities","title":"Configuration &amp; Utilities","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibiutilsconfig_loaderpy","title":"<code>odibi/utils/config_loader.py</code>","text":"<ul> <li>Added: YAML file loading with path logging</li> <li>Added: Environment variable substitution tracking</li> <li>Added: Missing env var warnings with variable names</li> <li>Added: Schema validation result logging</li> <li>Fixed: Silent env var substitution failures</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#validation-engine","title":"Validation Engine","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibivalidationenginepy","title":"<code>odibi/validation/engine.py</code>","text":"<ul> <li>Added: Rule-by-rule validation logging</li> <li>Added: Pass/fail counts per rule</li> <li>Added: Failure row details (configurable limit)</li> <li>Added: Timing for expensive validations</li> <li>Fixed: Silent validation failures now logged with details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#engine-layer","title":"Engine Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibienginepandas_enginepy","title":"<code>odibi/engine/pandas_engine.py</code>","text":"<ul> <li>Added: File I/O logging with format, path, row counts</li> <li>Added: Memory usage warnings (&gt;1GB threshold)</li> <li>Added: SQL execution logging with query previews</li> <li>Added: Chunk processing progress for large files</li> <li>Fixed: Silent read failures now include file path and error details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibienginespark_enginepy","title":"<code>odibi/engine/spark_engine.py</code>","text":"<ul> <li>Added: Partition count logging for reads/writes</li> <li>Added: JDBC connection logging (redacted credentials)</li> <li>Added: Delta Lake operation metrics (merge, optimize, vacuum)</li> <li>Added: Streaming batch progress logging</li> <li>Added: Broadcast join size tracking</li> <li>Fixed: Silent JDBC failures now include connection details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#connection-layer","title":"Connection Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionsfactorypy","title":"<code>odibi/connections/factory.py</code>","text":"<ul> <li>Added: Connection type resolution logging</li> <li>Added: Connection creation/reuse tracking</li> <li>Added: Failed connection attempts with error details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionslocalpy","title":"<code>odibi/connections/local.py</code>","text":"<ul> <li>Added: File system path resolution logging</li> <li>Added: Directory creation logging</li> <li>Added: File existence checks with paths</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionsazure_adlspy","title":"<code>odibi/connections/azure_adls.py</code>","text":"<ul> <li>Added: Storage account connection logging</li> <li>Added: Container/path resolution logging</li> <li>Added: Credential type detection (SAS, OAuth, Key)</li> <li>Added: Secret redaction for connection strings</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionsazure_sqlpy","title":"<code>odibi/connections/azure_sql.py</code>","text":"<ul> <li>Added: Server/database connection logging</li> <li>Added: Authentication method logging (SQL Auth, AAD)</li> <li>Added: JDBC/ODBC driver selection logging</li> <li>Added: Query execution timing</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#transformer-layer","title":"Transformer Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersrelationalpy","title":"<code>odibi/transformers/relational.py</code>","text":"<ul> <li>Added: Join operation logging with key columns</li> <li>Added: Row count before/after joins</li> <li>Added: Filter operation logging with predicate info</li> <li>Added: Aggregation logging with group-by columns</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersadvancedpy","title":"<code>odibi/transformers/advanced.py</code>","text":"<ul> <li>Added: Window function logging</li> <li>Added: Pivot/unpivot operation tracking</li> <li>Added: Complex expression evaluation logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersscdpy","title":"<code>odibi/transformers/scd.py</code>","text":"<ul> <li>Added: SCD Type 1/2 operation logging</li> <li>Added: Row version tracking</li> <li>Added: Effective date range logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersmerge_transformerpy","title":"<code>odibi/transformers/merge_transformer.py</code>","text":"<ul> <li>Added: Merge key logging</li> <li>Added: Insert/update/delete counts</li> <li>Added: Conflict resolution logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersvalidationpy","title":"<code>odibi/transformers/validation.py</code>","text":"<ul> <li>Added: Validation rule application logging</li> <li>Added: Quarantine row tracking</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformerssql_corepy","title":"<code>odibi/transformers/sql_core.py</code>","text":"<ul> <li>Added: SQL generation logging</li> <li>Added: Query execution timing</li> <li>Added: Result set size logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#pattern-layer","title":"Pattern Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibipatternsbasepy","title":"<code>odibi/patterns/base.py</code>","text":"<ul> <li>Added: Pattern instantiation logging</li> <li>Added: Configuration validation logging</li> <li>Added: Execution phase logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibipatternsscd2py","title":"<code>odibi/patterns/scd2.py</code>","text":"<ul> <li>Added: Slowly changing dimension processing logging</li> <li>Added: New/changed/expired row counts</li> <li>Added: Surrogate key generation logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibipatternsmergepy","title":"<code>odibi/patterns/merge.py</code>","text":"<ul> <li>Added: Merge pattern execution logging</li> <li>Added: Match condition logging</li> <li>Added: Update/insert clause logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#storydocumentation-layer","title":"Story/Documentation Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibistorygeneratorpy","title":"<code>odibi/story/generator.py</code>","text":"<ul> <li>Added: Story generation logging</li> <li>Added: Template rendering logging</li> <li>Added: Output file creation logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibistoryrendererspy","title":"<code>odibi/story/renderers.py</code>","text":"<ul> <li>Added: Renderer selection logging</li> <li>Added: Format conversion logging</li> <li>Added: Asset embedding logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibistorymetadatapy","title":"<code>odibi/story/metadata.py</code>","text":"<ul> <li>Added: Metadata extraction logging</li> <li>Added: Schema inference logging</li> <li>Added: Lineage tracking logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#silent-failure-patterns-fixed","title":"Silent Failure Patterns Fixed","text":"Module Issue Fix <code>config_loader</code> Missing env vars silently returned empty string Now logs warning with variable name <code>pandas_engine</code> File read errors lost path context Error now includes full path and format <code>spark_engine</code> JDBC failures had no connection context Errors include server/database (redacted creds) <code>validation/engine</code> Failed rules returned False silently Now logs rule name, failure count, sample failures <code>graph</code> Cycle detection raised generic error Now includes node names in cycle <code>connections/factory</code> Unknown connection type silent Now logs attempted type and available types <code>transformers/*</code> Transform failures lost input context Now log input row count, schema before failure"},{"location":"LOGGING_AUDIT_REPORT/#log-output-examples","title":"Log Output Examples","text":""},{"location":"LOGGING_AUDIT_REPORT/#human-readable-mode","title":"Human-Readable Mode","text":"<pre><code>[15:42:33] Starting READ: orders.parquet (pipeline_id=etl_daily, node_id=load_orders)\n[15:42:34] Completed READ: orders.parquet (elapsed_ms=1234.56, rows=50000, partitions=8)\n[15:42:34] [DEBUG] Schema change in transform (columns_before=12, columns_after=15, columns_added=['calculated_total', 'tax_amount', 'discount'])\n[15:42:35] [WARN] Validation failed: not_null_check (rule=not_null_check, passed=False, failures=['customer_id is null: 23 rows'])\n</code></pre>"},{"location":"LOGGING_AUDIT_REPORT/#jsonstructured-mode","title":"JSON/Structured Mode","text":"<pre><code>{\"timestamp\": \"2025-12-01T15:42:33.123Z\", \"level\": \"INFO\", \"message\": \"Starting READ: orders.parquet\", \"pipeline_id\": \"etl_daily\", \"node_id\": \"load_orders\", \"operation\": \"read\"}\n{\"timestamp\": \"2025-12-01T15:42:34.456Z\", \"level\": \"INFO\", \"message\": \"Completed READ: orders.parquet\", \"pipeline_id\": \"etl_daily\", \"node_id\": \"load_orders\", \"operation\": \"read\", \"elapsed_ms\": 1234.56, \"rows\": 50000, \"partitions\": 8}\n</code></pre>"},{"location":"LOGGING_AUDIT_REPORT/#recommendations","title":"Recommendations","text":""},{"location":"LOGGING_AUDIT_REPORT/#short-term","title":"Short-term","text":"<ol> <li>Add sampling for large failure sets - Currently logs up to 10 failure examples; consider making configurable</li> <li>Add correlation IDs - Pass run_id through all operations for distributed tracing</li> <li>Memory threshold configuration - Make the 1GB warning threshold configurable</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/#medium-term","title":"Medium-term","text":"<ol> <li>Metrics export - Add Prometheus/StatsD exporters for <code>OperationMetrics</code></li> <li>Log aggregation - Consider structured logging to ELK/Splunk/Datadog</li> <li>Performance dashboards - Build Grafana dashboards from structured logs</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/#long-term","title":"Long-term","text":"<ol> <li>OpenTelemetry integration - Add spans for distributed tracing</li> <li>Automatic alerting - Trigger alerts on validation failure rates</li> <li>Log-based debugging - Replay failed operations from logs</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/#files-modified","title":"Files Modified","text":"<pre><code>odibi/utils/logging_context.py  (NEW)\nodibi/utils/logging.py\nodibi/utils/__init__.py\nodibi/utils/config_loader.py\nodibi/graph.py\nodibi/node.py\nodibi/pipeline.py\nodibi/validation/engine.py\nodibi/engine/pandas_engine.py\nodibi/engine/spark_engine.py\nodibi/connections/factory.py\nodibi/connections/local.py\nodibi/connections/azure_adls.py\nodibi/connections/azure_sql.py\nodibi/transformers/relational.py\nodibi/transformers/advanced.py\nodibi/transformers/scd.py\nodibi/transformers/merge_transformer.py\nodibi/transformers/validation.py\nodibi/transformers/sql_core.py\nodibi/patterns/base.py\nodibi/patterns/scd2.py\nodibi/patterns/merge.py\nodibi/story/generator.py\nodibi/story/renderers.py\nodibi/story/metadata.py\n</code></pre>"},{"location":"LOGGING_AUDIT_REPORT/#verification","title":"Verification","text":"<p>All tests pass after the logging enhancements:</p> <pre><code>pytest tests/ -x -q --tb=short\n# Result: 655 passed, 20 skipped\n</code></pre> <p>Lint checks pass:</p> <pre><code>ruff check .\nruff format .\n# Result: All checks passed\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/","title":"GitHub Issues Tracker","text":""},{"location":"NEW_GITHUB_ISSUES/#completed-issues","title":"Completed Issues \u2705","text":"<p>These issues have been resolved and can be closed if opened.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-cli-missing-tag-flag-completed","title":"Issue: CLI missing <code>--tag</code> flag \u2705 COMPLETED","text":"<p>Labels: <code>bug</code>, <code>cli</code></p> <p>The <code>--tag</code> flag is now implemented in <code>odibi/cli/main.py:79</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-cli-missing-node-and-pipeline-flags-completed","title":"Issue: CLI missing <code>--node</code> and <code>--pipeline</code> flags \u2705 COMPLETED","text":"<p>Labels: <code>bug</code>, <code>cli</code>, <code>orchestration</code></p> <p>Both flags now exist in <code>odibi/cli/main.py:83,88</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-materialized-config-option-has-no-effect-completed","title":"Issue: <code>materialized</code> config option has no effect \u2705 COMPLETED","text":"<p>Labels: <code>bug</code>, <code>config</code></p> <p>Implemented in <code>odibi/node.py:199-1743</code> - supports <code>view</code> and <code>incremental</code> strategies.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-add-polars-to-enginetype-enum-completed","title":"Issue: Add <code>POLARS</code> to <code>EngineType</code> enum \u2705 COMPLETED","text":"<p>Labels: <code>enhancement</code>, <code>engine</code></p> <p>Added in <code>odibi/config.py:19</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-pandasengine-missing-as_of_timestamp-time-travel-completed","title":"Issue: PandasEngine missing <code>as_of_timestamp</code> time travel \u2705 COMPLETED","text":"<p>Labels: <code>bug</code>, <code>engine-parity</code></p> <p>Implemented in <code>odibi/engine/pandas_engine.py:281-283</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-polarsengine-missing-abstract-methods-completed","title":"Issue: PolarsEngine missing abstract methods \u2705 COMPLETED","text":"<p>Labels: <code>enhancement</code>, <code>engine-parity</code></p> <p><code>harmonize_schema</code> and <code>anonymize</code> now implemented in <code>polars_engine.py</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#issue-validation-engine-performance-optimizations-completed","title":"Issue: Validation engine performance optimizations \u2705 COMPLETED","text":"<p>Labels: <code>enhancement</code>, <code>performance</code></p> <p>Fixed in validation engine overhaul: - Pandas UNIQUE double scan - Pandas ACCEPTED_VALUES memory waste - Polars eager collection (now lazy) - Added all missing Polars contract types - Quarantine memory optimization (removed O(N\u00d7tests) lists) - Added <code>fail_fast</code> and <code>cache_df</code> config options - Added quarantine <code>max_rows</code> and <code>sample_fraction</code></p>"},{"location":"NEW_GITHUB_ISSUES/#open-issues","title":"Open Issues","text":""},{"location":"NEW_GITHUB_ISSUES/#issue-environments-config-not-implemented","title":"Issue: <code>environments</code> config not implemented","text":"<p>Labels: <code>enhancement</code>, <code>config</code>, <code>phase-3</code></p> <p>Priority: Low</p> <p>Body:</p> <pre><code>The `environments` field is defined in `ProjectConfig` but does nothing.\n\n## Current State\n\nThe validator explicitly notes this is deferred to Phase 3:\n\n```python\n@model_validator(mode=\"after\")\ndef check_environments_not_implemented(self):\n    # Implemented in Phase 3\n    return self\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior","title":"Expected Behavior","text":"<p>Users should be able to define environment-specific overrides:</p> <pre><code>environments:\n  dev:\n    connections:\n      bronze:\n        base_path: ./dev_data\n  prod:\n    connections:\n      bronze:\n        base_path: abfss://bronze@prod.dfs.core.windows.net\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>[ ] Environment switching via CLI flag or env var</li> <li>[ ] Config merging (base + environment overrides)</li> <li>[ ] Tests for environment resolution</li> </ul> <pre><code>\n---\n\n### Issue: Retry logic not wired to execution\n\n**Labels:** `bug`, `reliability`, `priority-high`\n\n**Priority:** High (Quick Win)\n\n**Body:**\n\n</code></pre> <p><code>RetryConfig</code> exists in config.py but is not connected to actual node execution.</p>"},{"location":"NEW_GITHUB_ISSUES/#evidence","title":"Evidence","text":"<ul> <li><code>odibi/config.py</code> defines <code>RetryConfig</code> with <code>max_attempts</code>, <code>backoff</code> strategy</li> <li><code>odibi/node.py</code> does not use retry config during execution</li> <li>Transient Azure failures (throttling, network) cause immediate pipeline failure</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_1","title":"Expected Behavior","text":"<p>Nodes should retry on transient failures with configurable backoff:</p> <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_1","title":"Acceptance Criteria","text":"<ul> <li>[ ] Wire RetryConfig to node execution</li> <li>[ ] Implement exponential/linear/constant backoff</li> <li>[ ] Retry only on transient errors (network, throttling), not data errors</li> <li>[ ] Log retry attempts with context</li> <li>[ ] Tests for retry behavior</li> </ul> <pre><code>\n---\n\n### Issue: Idempotency for append mode writes\n\n**Labels:** `enhancement`, `reliability`, `priority-high`\n\n**Priority:** High\n\n**Body:**\n\n</code></pre> <p>Re-running the same batch in append mode can create duplicate records.</p>"},{"location":"NEW_GITHUB_ISSUES/#problem","title":"Problem","text":"<p>If a pipeline fails after node 3 writes but before completion: 1. User re-runs pipeline 2. Nodes 1-3 re-execute and append again 3. Data is duplicated</p>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_2","title":"Expected Behavior","text":"<p>Append writes should be idempotent - re-running with same data should not duplicate.</p>"},{"location":"NEW_GITHUB_ISSUES/#proposed-solutions","title":"Proposed Solutions","text":"<p>Option A: Batch ID deduplication - Add <code>_batch_id</code> column on write - Delete existing batch before append - Simple but requires DELETE capability</p> <p>Option B: Content hash check - Hash input data, skip if already written - Works with append-only systems</p> <p>Option C: Write-ahead log - Track which batches completed - Skip already-completed nodes on re-run</p>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_2","title":"Acceptance Criteria","text":"<ul> <li>[ ] Design decision on approach</li> <li>[ ] Implementation for chosen approach</li> <li>[ ] Works with Delta Lake append</li> <li>[ ] Tests for idempotent re-runs</li> </ul> <pre><code>\n---\n\n### Issue: Rollback on partial pipeline failure\n\n**Labels:** `enhancement`, `reliability`, `priority-medium`\n\n**Priority:** Medium\n\n**Body:**\n\n</code></pre> <p>If node 3/5 fails, nodes 1-2 have already written data but pipeline is marked \"failed\".</p>"},{"location":"NEW_GITHUB_ISSUES/#problem_1","title":"Problem","text":"<p>No automatic cleanup of partial writes. User must manually identify and fix.</p>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_3","title":"Expected Behavior","text":"<p>Options: 1. Transaction-like rollback - Undo writes from failed run 2. Checkpoint resume - Resume from last successful node 3. Dry-run mode - Validate all nodes before writing any</p>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_3","title":"Acceptance Criteria","text":"<ul> <li>[ ] Design decision on approach</li> <li>[ ] Implementation</li> <li>[ ] Clear error message showing what was written before failure</li> <li>[ ] Recovery documentation</li> </ul> <pre><code>\n---\n\n### Issue: Dead letter queue for unparseable records\n\n**Labels:** `enhancement`, `data-quality`, `priority-low`\n\n**Priority:** Low\n\n**Body:**\n\n</code></pre> <p>Records that fail parsing (malformed JSON, encoding errors) have no destination.</p>"},{"location":"NEW_GITHUB_ISSUES/#current-state","title":"Current State","text":"<ul> <li>Quarantine handles validation failures (records that parse but fail contracts)</li> <li>Parsing failures cause node failure with no recovery</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_4","title":"Expected Behavior","text":"<p>Unparseable records should go to a dead letter destination for investigation.</p>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_4","title":"Acceptance Criteria","text":"<ul> <li>[ ] DLQ config option in read config</li> <li>[ ] Capture parsing errors with context</li> <li>[ ] Continue processing valid records</li> </ul> <pre><code>\n---\n\n### Issue: Metrics export (Prometheus/Azure Monitor)\n\n**Labels:** `enhancement`, `observability`, `priority-medium`\n\n**Priority:** Medium\n\n**Body:**\n\n</code></pre> <p>No way to export pipeline metrics to monitoring systems.</p>"},{"location":"NEW_GITHUB_ISSUES/#current-state_1","title":"Current State","text":"<ul> <li>Stories capture execution details (great for debugging)</li> <li>No push to Prometheus, StatsD, Azure Monitor, etc.</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_5","title":"Expected Behavior","text":"<pre><code>observability:\n  metrics:\n    type: prometheus  # or azure_monitor, statsd\n    endpoint: http://pushgateway:9091\n    labels:\n      environment: prod\n</code></pre> <p>Metrics to export: - <code>odibi_pipeline_duration_seconds</code> - <code>odibi_node_duration_seconds</code> - <code>odibi_rows_processed_total</code> - <code>odibi_validation_failures_total</code> - <code>odibi_quarantine_rows_total</code></p>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_5","title":"Acceptance Criteria","text":"<ul> <li>[ ] Prometheus pushgateway support</li> <li>[ ] Azure Monitor support (for your use case)</li> <li>[ ] Configurable labels/dimensions</li> </ul> <pre><code>\n---\n\n### Issue: Distributed tracing (OpenTelemetry)\n\n**Labels:** `enhancement`, `observability`, `priority-low`\n\n**Priority:** Low\n\n**Body:**\n\n</code></pre> <p>No distributed tracing for cross-node debugging.</p>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_6","title":"Expected Behavior","text":"<pre><code>observability:\n  tracing:\n    enabled: true\n    exporter: otlp\n    endpoint: http://jaeger:4317\n</code></pre> <p>Each node execution creates a span with: - Parent: pipeline span - Attributes: node name, row count, duration - Events: validation results, errors</p>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_6","title":"Acceptance Criteria","text":"<ul> <li>[ ] OpenTelemetry SDK integration</li> <li>[ ] Span per pipeline and node</li> <li>[ ] Trace context propagation</li> </ul> <pre><code>\n---\n\n### Issue: Real Spark integration tests\n\n**Labels:** `testing`, `priority-medium`\n\n**Priority:** Medium\n\n**Body:**\n\n</code></pre> <p>Most Spark tests use mocks, not real SparkSession.</p>"},{"location":"NEW_GITHUB_ISSUES/#current-state_2","title":"Current State","text":"<ul> <li><code>test_spark_real.py</code> exists but limited</li> <li>Many Spark code paths untested with real execution</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_7","title":"Acceptance Criteria","text":"<ul> <li>[ ] Integration tests with real SparkSession (WSL/Linux)</li> <li>[ ] Test Delta Lake operations end-to-end</li> <li>[ ] Test Spark-specific features (partitioning, caching)</li> <li>[ ] CI setup for Spark tests</li> </ul> <pre><code>\n---\n\n### Issue: Load/stress testing suite\n\n**Labels:** `testing`, `performance`, `priority-low`\n\n**Priority:** Low\n\n**Body:**\n\n</code></pre> <p>No benchmarks for large datasets (10M+ rows).</p>"},{"location":"NEW_GITHUB_ISSUES/#current-state_3","title":"Current State","text":"<ul> <li><code>tests/benchmarks/</code> exists with basic benchmarks</li> <li>No stress tests for production-scale data</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_8","title":"Acceptance Criteria","text":"<ul> <li>[ ] Benchmark with 1M, 10M, 100M rows</li> <li>[ ] Memory profiling</li> <li>[ ] Identify performance regressions</li> </ul> <pre><code>\n---\n\n### Issue: Streaming/CDC support\n\n**Labels:** `enhancement`, `future`, `priority-low`\n\n**Priority:** Low (Future)\n\n**Body:**\n\n</code></pre> <p>No support for real-time streaming or Change Data Capture.</p>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior-future","title":"Expected Behavior (Future)","text":"<pre><code>read:\n  connection: eventhub\n  stream: true\n  watermark_column: event_time\n  watermark_delay: \"10 minutes\"\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#note","title":"Note","text":"<p>This is a significant feature. Park for future consideration.</p> <pre><code>\n---\n\n### Issue: Data profiling / statistics\n\n**Labels:** `enhancement`, `data-quality`, `priority-low`\n\n**Priority:** Low\n\n**Body:**\n\n</code></pre> <p>Only basic stats captured. No histograms, distributions, or anomaly detection.</p>"},{"location":"NEW_GITHUB_ISSUES/#expected-behavior_7","title":"Expected Behavior","text":"<pre><code>profiling:\n  enabled: true\n  include:\n    - histograms\n    - null_percentage\n    - unique_count\n    - min_max\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#acceptance-criteria_9","title":"Acceptance Criteria","text":"<ul> <li>[ ] Extended profiling in story metadata</li> <li>[ ] Optional (can be expensive for large data) ```</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#summary","title":"Summary","text":"Issue Priority Status <code>--tag</code> flag - \u2705 Completed <code>--node</code>/<code>--pipeline</code> flags - \u2705 Completed <code>materialized</code> config - \u2705 Completed POLARS enum - \u2705 Completed PandasEngine timestamp time travel - \u2705 Completed PolarsEngine methods - \u2705 Completed Validation performance - \u2705 Completed <code>environments</code> config Low \ud83d\udd32 Open Retry wiring High \ud83d\udd32 Open Idempotency for append High \ud83d\udd32 Open Rollback on failure Medium \ud83d\udd32 Open Dead letter queue Low \ud83d\udd32 Open Metrics export Medium \ud83d\udd32 Open Distributed tracing Low \ud83d\udd32 Open Real Spark tests Medium \ud83d\udd32 Open Load/stress tests Low \ud83d\udd32 Open Streaming/CDC Low \ud83d\udd32 Future Data profiling Low \ud83d\udd32 Open"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/","title":"odibi Open-Source Launch Plan","text":"<p>A detailed guide for open-sourcing odibi and building community adoption.</p>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-1-pre-launch-prep-1-2-weeks","title":"Phase 1: Pre-Launch Prep (1-2 weeks)","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#11-code-cleanup","title":"1.1 Code Cleanup","text":"<ul> <li>[ ] Remove any hardcoded secrets, internal paths, company-specific references</li> <li>[ ] Audit for sensitive data in git history (consider squashing if needed)</li> <li>[ ] Ensure all credentials use environment variables</li> <li>[ ] Add <code>.env.example</code> with placeholder values</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#12-documentation-critical","title":"1.2 Documentation (Critical)","text":"<ul> <li>[ ] README.md \u2014 the first impression</li> <li>What is odibi? (one sentence)</li> <li>Key features (bullet points)</li> <li>Quick install: <code>pip install odibi</code></li> <li>5-minute getting started example</li> <li>Architecture diagram (use mermaid)</li> <li>Link to full docs</li> <li>[ ] docs/ folder or docs site (mkdocs, docusaurus, or just markdown)</li> <li>Installation guide</li> <li>Core concepts (connections, pipelines, nodes, layers)</li> <li>Bronze patterns (rolling_window, stateful, skip_if_unchanged, cloudFiles)</li> <li>Silver patterns (deduplicate, detect_deletes)</li> <li>Configuration reference (all YAML options)</li> <li>Examples for each pattern</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#13-examples","title":"1.3 Examples","text":"<ul> <li>[ ] <code>examples/</code> folder with working pipelines:</li> <li><code>examples/quickstart/</code> \u2014 minimal working example</li> <li><code>examples/bronze_sql_to_delta/</code> \u2014 SQL source to Delta</li> <li><code>examples/bronze_adls_cloudfiles/</code> \u2014 streaming file ingestion</li> <li><code>examples/silver_dedup_deletes/</code> \u2014 silver layer patterns</li> <li>[ ] Each example should be copy-paste runnable</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#14-project-hygiene","title":"1.4 Project Hygiene","text":"<ul> <li>[ ] <code>LICENSE</code> file (Apache 2.0 recommended)</li> <li>[ ] <code>CONTRIBUTING.md</code> \u2014 how to contribute</li> <li>[ ] <code>CODE_OF_CONDUCT.md</code> \u2014 standard community guidelines</li> <li>[ ] <code>CHANGELOG.md</code> \u2014 version history</li> <li>[ ] <code>.github/</code> folder:</li> <li>Issue templates (bug report, feature request)</li> <li>PR template</li> <li>GitHub Actions for CI (run tests on PR)</li> <li>[ ] Ensure <code>pytest</code> tests pass</li> <li>[ ] Add badges to README (build status, license, Python version)</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#15-packaging","title":"1.5 Packaging","text":"<ul> <li>[ ] Verify <code>pyproject.toml</code> is correct</li> <li>[ ] Test <code>pip install .</code> from fresh environment</li> <li>[ ] Publish to PyPI (or TestPyPI first): <code>pip install odibi</code></li> <li>[ ] Add install extras: <code>pip install odibi[azure]</code>, <code>pip install odibi[spark]</code></li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-2-launch-day-1-day","title":"Phase 2: Launch Day (1 day)","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#21-github","title":"2.1 GitHub","text":"<ul> <li>[ ] Make repo public</li> <li>[ ] Add topics/tags: <code>data-engineering</code>, <code>etl</code>, <code>medallion-architecture</code>, <code>delta-lake</code>, <code>spark</code>, <code>azure</code></li> <li>[ ] Pin the repo to your profile</li> <li>[ ] Star it yourself</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#22-announcement-posts","title":"2.2 Announcement Posts","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#hacker-news-show-hn","title":"Hacker News (Show HN)","text":"<pre><code>Show HN: odibi \u2013 Open-source medallion architecture framework for data engineering\n\nI built odibi to simplify bronze/silver/gold pipelines without vendor lock-in.\nIt's YAML-driven, supports Spark and Pandas, and has built-in patterns for\nincremental loads, deduplication, and CDC-like delete detection.\n\n- Runs on any Spark (Databricks, EMR, Synapse, local)\n- No vendor lock-in\n- Built-in patterns: rolling_window, skip_if_unchanged, detect_deletes\n\nGitHub: [link]\n\nI've been using it in production for OEE analytics. Looking for feedback!\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#reddit-rdataengineering-rpython-rapachespark","title":"Reddit (r/dataengineering, r/python, r/apachespark)","text":"<ul> <li>Similar post, slightly more casual</li> <li>Engage with comments, answer questions</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#twitterx","title":"Twitter/X","text":"<pre><code>Shipped odibi \u2014 an open-source framework for medallion architecture pipelines.\n\n\u2705 YAML-driven bronze \u2192 silver \u2192 gold\n\u2705 Runs on Spark or Pandas\n\u2705 No Databricks lock-in\n\u2705 Built-in CDC-like delete detection\n\nBuilt it for my own production workloads. Now it's yours.\n\nGitHub: [link]\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#linkedin","title":"LinkedIn","text":"<ul> <li>More professional angle</li> <li>\"After building data pipelines at [X], I open-sourced the framework I wish I had...\"</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#23-dev-communities","title":"2.3 Dev Communities","text":"<ul> <li>[ ] Post in Discord/Slack communities (Data Engineering, dbt, Databricks community)</li> <li>[ ] Consider a blog post on dev.to or Medium</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-3-post-launch-growth-ongoing","title":"Phase 3: Post-Launch Growth (Ongoing)","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#31-content-marketing-1-2-postsmonth","title":"3.1 Content Marketing (1-2 posts/month)","text":"<ul> <li>\"How to build CDC without CDC using odibi\"</li> <li>\"Medallion architecture on AWS EMR with odibi\"</li> <li>\"Why I stopped using Delta Live Tables\"</li> <li>\"Rolling window vs stateful incremental: when to use each\"</li> <li>Tutorial videos on YouTube</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#32-community-building","title":"3.2 Community Building","text":"<ul> <li>[ ] Respond to every GitHub issue within 24 hours</li> <li>[ ] Add a Discussions tab on GitHub for Q&amp;A</li> <li>[ ] Consider a Discord server once you have 10+ active users</li> <li>[ ] Celebrate contributors (shoutouts, contributor badges)</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#33-expand-connectors","title":"3.3 Expand Connectors","text":"<ul> <li>[ ] AWS: S3, Redshift, Glue Catalog</li> <li>[ ] GCP: GCS, BigQuery</li> <li>[ ] Snowflake</li> <li>[ ] Polars engine (once stable)</li> <li>[ ] DuckDB (for local/lightweight)</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#34-add-a-cli","title":"3.4 Add a CLI","text":"<pre><code>odibi init        # scaffold a new project\nodibi validate    # validate YAML config\nodibi run         # execute pipeline\nodibi plan        # dry-run, show what would execute\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-4-monetization-via-consulting","title":"Phase 4: Monetization via Consulting","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#41-services-page","title":"4.1 Services Page","text":"<p>Add to your docs site or a simple landing page:</p> <pre><code># odibi Services\n\nodibi is free and open-source. Need help?\n\n**Implementation Support**\n- Pipeline design and setup\n- Migration from legacy ETL\n- Performance tuning\n\n**Training**\n- Team workshops on medallion architecture\n- Best practices for incremental ingestion\n\n**Support Contracts**\n- Priority issue resolution\n- SLA-backed support\n\nContact: your@email.com\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#42-pricing-starting-point","title":"4.2 Pricing (Starting Point)","text":"Service Price Range Consulting $150-250/hour Implementation package $5-15k for full setup Support contract $1-3k/month"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#43-lead-generation","title":"4.3 Lead Generation","text":"<ul> <li>Add \"Need help? Contact me\" to README</li> <li>Blog posts with CTA at the end</li> <li>Engage in communities, offer free advice, build trust</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#priority-timeline","title":"Priority Timeline","text":"Week Focus 1 Code cleanup, README, LICENSE, tests passing 2 Docs site, examples, PyPI publish 3 Launch: GitHub public, HN, Reddit, Twitter 4+ Content, community, expand connectors, consulting"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ul> <li>GitHub stars</li> <li>PyPI downloads</li> <li>GitHub issues/PRs (engagement)</li> <li>Docs site traffic</li> <li>Consulting inquiries</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#resources","title":"Resources","text":"<ul> <li>Choose a License \u2014 Apache 2.0 recommended</li> <li>Keep a Changelog</li> <li>Semantic Versioning</li> <li>GitHub Actions for Python</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/","title":"Reference Enrichment Campaign","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#goal","title":"Goal","text":"<p>Make the auto-generated <code>yaml_schema.md</code> reference beginner-friendly, complete, and cross-linked to relevant guides/tutorials.</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#context","title":"Context","text":"<p>Current State: - <code>docs/reference/yaml_schema.md</code> is auto-generated from Pydantic docstrings via <code>odibi/introspect.py</code> - Source of truth: docstrings in <code>odibi/config.py</code> - Currently 3400+ lines with good structure but inconsistent depth - Missing: beginner context, cross-references, \"when to use\" guidance</p> <p>Key Pain Points Identified: - Validation vs Contracts vs Quality Gates - confusing overlap - Tests/Contracts terminology inconsistent - Some configs have great examples, others are bare - No links to relevant tutorials/guides - Assumes reader knows SCD2, Delta, HWM, etc.</p> <p>Success Criteria: - [ ] Every major config has a \"When to Use\" one-liner - [ ] Every major config has a \"See Also\" linking to relevant guide/tutorial - [ ] Validation/Contracts/Quality Gates clearly distinguished - [ ] All pattern configs have realistic YAML examples - [ ] <code>mkdocs build</code> passes - [ ] <code>python odibi/introspect.py</code> regenerates successfully - [ ] <code>ruff check . &amp;&amp; ruff format .</code> passes - [ ] All tests pass</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-1-clarify-validation-architecture-high-priority","title":"Phase 1: Clarify Validation Architecture (HIGH PRIORITY)","text":"<p>The current docs are confusing about: - Contracts (pre-transform checks) - Validation (post-transform checks) - Quality Gates (pipeline-level thresholds) - Quarantine (where bad rows go)</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#11-update-section_intros-in-introspectpy","title":"1.1 Update SECTION_INTROS in introspect.py","text":"<p>File: <code>odibi/introspect.py</code> \u2192 <code>SECTION_INTROS[\"Contract\"]</code></p> <p>Add clear disambiguation:</p> <pre><code>SECTION_INTROS[\"Contract\"] = \"\"\"\n### Contracts (Pre-Transform Checks)\n\nContracts are **fail-fast data quality checks** that run on input data **before** transformation.\nThey always halt execution on failure - use them to prevent bad data from entering the pipeline.\n\n**Contracts vs Validation vs Quality Gates:**\n\n| Feature | When it Runs | On Failure | Use Case |\n|---------|--------------|------------|----------|\n| **Contracts** | Before transform | Always fails | Input data quality (not-null, unique keys) |\n| **Validation** | After transform | Configurable (fail/warn/quarantine) | Output data quality (ranges, formats) |\n| **Quality Gates** | After validation | Configurable (abort/warn) | Pipeline-level thresholds (pass rate, row counts) |\n| **Quarantine** | With validation | Routes bad rows | Capture invalid records for review |\n\n**See Also:**\n- [Validation Guide](../features/quality_gates.md) - Full validation configuration\n- [Quarantine Guide](../features/quarantine.md) - Quarantine setup and review\n- [Getting Started: Validation](../tutorials/getting_started.md#add-data-validation)\n\n**Example:**\n```yaml\n- name: \"process_orders\"\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read:\n    source: raw_orders\n</code></pre> <p>\"\"\"</p> <pre><code>\n### 1.2 Update Contract Config Docstrings\n\n**File:** `odibi/config.py`\n\nFor each test type, add:\n1. One-liner \"when to use\"\n2. See Also reference\n\n**NotNullTest:**\n```python\nclass NotNullTest(BaseModel):\n    \"\"\"Ensures specified columns contain no NULL values.\n\n    **When to Use:** Primary keys, required fields, foreign keys that must resolve.\n\n    **See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n    ```yaml\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id, created_at]\n    ```\n    \"\"\"\n</code></pre> <p>UniqueTest:</p> <pre><code>class UniqueTest(BaseModel):\n    \"\"\"Ensures specified columns (or combination) contain unique values.\n\n    **When to Use:** Primary keys, natural keys, deduplication verification.\n\n    **See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n    ```yaml\n    contracts:\n      - type: unique\n        columns: [order_id]  # Single column\n      # OR composite key:\n      - type: unique\n        columns: [customer_id, order_date]  # Composite uniqueness\n    ```\n    \"\"\"\n</code></pre> <p>FreshnessContract:</p> <pre><code>class FreshnessContract(BaseModel):\n    \"\"\"Validates that data is not stale.\n\n    **When to Use:** Source systems that should update regularly, SLA monitoring.\n\n    **See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n    ```yaml\n    contracts:\n      - type: freshness\n        column: updated_at\n        max_age: \"24h\"  # Fail if no data newer than 24 hours\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-2-enrich-core-configs-medium-priority","title":"Phase 2: Enrich Core Configs (MEDIUM PRIORITY)","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#21-readconfig","title":"2.1 ReadConfig","text":"<p>File: <code>odibi/config.py</code> \u2192 <code>ReadConfig</code> docstring</p> <p>Add:</p> <pre><code>class ReadConfig(BaseModel):\n    \"\"\"Configuration for reading data into a node.\n\n    **When to Use:** First node in a pipeline, or any node that reads from storage.\n\n    **Key Concepts:**\n    - `connection`: References a named connection from `connections:` section\n    - `format`: File format (csv, parquet, delta, json, avro)\n    - `incremental`: Enable incremental loading (only new data)\n\n    **See Also:**\n    - [Smart Read Pattern](../patterns/smart_read.md) - Automatic full/incremental switching\n    - [Incremental Loading](../patterns/incremental_stateful.md) - HWM-based loading\n    - [Supported Formats](./supported_formats.md) - All format options\n\n    **Examples:**\n\n    Basic Read:\n    ```yaml\n    read:\n      connection: bronze\n      format: parquet\n      path: raw/customers/\n    ```\n\n    Incremental Read (Stateful HWM):\n    ```yaml\n    read:\n      connection: bronze\n      format: delta\n      table: raw_orders\n      incremental:\n        type: stateful\n        column: updated_at\n    ```\n\n    First Run Override:\n    ```yaml\n    read:\n      connection: source_db\n      query: \"SELECT * FROM orders\"\n      first_run_query: \"SELECT * FROM orders WHERE order_date &gt;= '2023-01-01'\"\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#22-writeconfig","title":"2.2 WriteConfig","text":"<p>File: <code>odibi/config.py</code> \u2192 <code>WriteConfig</code> docstring</p> <p>Add:</p> <pre><code>class WriteConfig(BaseModel):\n    \"\"\"Configuration for writing data from a node.\n\n    **When to Use:** Any node that persists data to storage.\n\n    **Key Concepts:**\n    - `mode`: How to handle existing data (overwrite, append, upsert)\n    - `keys`: Required for upsert mode - columns that identify unique records\n    - `partition_by`: Columns to partition output by (improves query performance)\n\n    **See Also:**\n    - [Merge/Upsert Pattern](../patterns/merge_upsert.md) - Delta merge operations\n    - [Performance Tuning](../guides/performance_tuning.md) - Partitioning strategies\n\n    **Examples:**\n\n    Simple Overwrite:\n    ```yaml\n    write:\n      connection: silver\n      format: parquet\n      path: cleaned/customers/\n      mode: overwrite\n    ```\n\n    Delta Upsert (Merge):\n    ```yaml\n    write:\n      connection: gold\n      format: delta\n      table: dim_customer\n      mode: upsert\n      keys: [customer_id]  # Match on this column\n    ```\n\n    Partitioned Write:\n    ```yaml\n    write:\n      connection: gold\n      format: delta\n      table: fact_orders\n      mode: append\n      partition_by: [order_date]\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#23-transformconfig","title":"2.3 TransformConfig","text":"<p>File: <code>odibi/config.py</code> \u2192 <code>TransformConfig</code> docstring</p> <p>Add:</p> <pre><code>class TransformConfig(BaseModel):\n    \"\"\"Configuration for transformation steps within a node.\n\n    **When to Use:** Custom business logic, data cleaning, SQL transformations.\n\n    **Key Concepts:**\n    - `steps`: Ordered list of operations (SQL, functions, or both)\n    - Each step receives `df` (the DataFrame from previous step)\n    - Steps execute in order: step1 \u2192 step2 \u2192 step3\n\n    **See Also:**\n    - [Writing Transformations](../guides/writing_transformations.md) - Custom functions\n    - [Transformer Catalog](#transformer-catalog) - Built-in functions\n\n    **Transformer vs Transform:**\n    - `transformer`: Single heavy operation (scd2, merge, deduplicate)\n    - `transform.steps`: Chain of lighter operations\n\n    **Examples:**\n\n    SQL-Only:\n    ```yaml\n    transform:\n      steps:\n        - sql: \"SELECT *, UPPER(name) as name_upper FROM df\"\n        - sql: \"SELECT * FROM df WHERE status = 'active'\"\n    ```\n\n    Function Steps:\n    ```yaml\n    transform:\n      steps:\n        - function: clean_text\n          params:\n            columns: [email, name]\n            case: lower\n        - function: fill_nulls\n          params:\n            defaults: {status: 'unknown', region: 'US'}\n    ```\n\n    Mixed SQL + Functions:\n    ```yaml\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE order_total &gt; 0\"\n        - function: derive_columns\n          params:\n            expressions:\n              profit: \"revenue - cost\"\n              margin: \"profit / revenue * 100\"\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-3-enrich-pattern-configs-medium-priority","title":"Phase 3: Enrich Pattern Configs (MEDIUM PRIORITY)","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#31-dimensionpattern","title":"3.1 DimensionPattern","text":"<p>Add beginner context to existing docstring:</p> <pre><code>\"\"\"Build complete dimension tables with surrogate keys and SCD support.\n\n**When to Use:**\n- Building dimension tables from source systems (customers, products, locations)\n- Need surrogate keys for star schema joins\n- Need to track historical changes (SCD Type 2)\n\n**Beginner Note:**\nDimensions are the \"who, what, where, when\" of your data warehouse.\nA customer dimension has customer_id (natural key) and customer_sk (surrogate key).\nFact tables join to dimensions via surrogate keys.\n\n**See Also:**\n- [Dimension Tutorial](../tutorials/dimensional_modeling/02_dimension_pattern.md)\n- [SCD2 Pattern](../patterns/scd2.md) - Slowly Changing Dimensions\n- [Fact Pattern](#factpattern) - Build facts that reference dimensions\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#32-factpattern","title":"3.2 FactPattern","text":"<p>Add:</p> <pre><code>\"\"\"Build fact tables with automatic surrogate key lookups from dimensions.\n\n**When to Use:**\n- Building fact tables from transactional data (orders, events, transactions)\n- Need to look up surrogate keys from dimension tables\n- Need to handle orphan records (missing dimension matches)\n\n**Beginner Note:**\nFacts are the \"how much, how many\" of your data warehouse.\nAn orders fact has measures (quantity, revenue) and dimension keys (customer_sk, product_sk).\nThe pattern automatically looks up SKs from dimensions.\n\n**See Also:**\n- [Fact Tutorial](../tutorials/dimensional_modeling/04_fact_pattern.md)\n- [Dimension Pattern](#dimensionpattern) - Build dimensions first\n- [FK Validation](../validation/fk.md) - Validate referential integrity\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#33-aggregationpattern","title":"3.3 AggregationPattern","text":"<p>Add:</p> <pre><code>\"\"\"Declarative aggregation with GROUP BY and optional incremental merge.\n\n**When to Use:**\n- Building summary/aggregate tables (daily sales, monthly metrics)\n- Need incremental aggregation (update existing aggregates)\n- Gold layer reporting tables\n\n**Beginner Note:**\nAggregations summarize facts at a higher grain.\nExample: daily_sales aggregates orders by date with SUM(revenue).\n\n**See Also:**\n- [Aggregation Tutorial](../tutorials/dimensional_modeling/05_aggregation_pattern.md)\n- [Windowed Reprocess](../patterns/windowed_reprocess.md) - Rolling aggregations\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-4-enrich-connection-configs-low-priority","title":"Phase 4: Enrich Connection Configs (LOW PRIORITY)","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#41-add-when-to-use-to-each-connection","title":"4.1 Add \"When to Use\" to Each Connection","text":"<p>LocalConnectionConfig:</p> <pre><code>\"\"\"Local filesystem connection.\n\n**When to Use:** Development, testing, small datasets, local processing.\n\n**See Also:** [Azure Setup](../guides/setup_azure.md) for cloud alternatives.\n\"\"\"\n</code></pre> <p>DeltaConnectionConfig:</p> <pre><code>\"\"\"Delta Lake connection for ACID-compliant data lakes.\n\n**When to Use:**\n- Production data lakes on Azure/AWS/GCP\n- Need time travel, ACID transactions, schema evolution\n- Upsert/merge operations\n\n**See Also:**\n- [Supported Formats](./supported_formats.md#delta) - Delta-specific options\n- [Merge Pattern](../patterns/merge_upsert.md) - Delta merge operations\n\"\"\"\n</code></pre> <p>AzureBlobConnectionConfig:</p> <pre><code>\"\"\"Azure Blob Storage / ADLS Gen2 connection.\n\n**When to Use:** Azure-based data lakes, landing zones, raw data storage.\n\n**See Also:**\n- [Azure Setup Guide](../guides/setup_azure.md) - Full Azure configuration\n- [Secrets Management](../guides/secrets.md) - Secure credential handling\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-5-fix-cross-references-in-introspectpy","title":"Phase 5: Fix Cross-References in introspect.py","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#51-add-see-also-links-generator","title":"5.1 Add See Also Links Generator","text":"<p>File: <code>odibi/introspect.py</code></p> <p>Add a mapping for auto-generating \"See Also\" sections:</p> <pre><code>SEE_ALSO_LINKS = {\n    \"ReadConfig\": [\n        (\"Smart Read Pattern\", \"../patterns/smart_read.md\"),\n        (\"Incremental Loading\", \"../patterns/incremental_stateful.md\"),\n    ],\n    \"WriteConfig\": [\n        (\"Merge/Upsert Pattern\", \"../patterns/merge_upsert.md\"),\n        (\"Performance Tuning\", \"../guides/performance_tuning.md\"),\n    ],\n    \"ValidationConfig\": [\n        (\"Validation Guide\", \"../features/quality_gates.md\"),\n        (\"Quarantine\", \"../features/quarantine.md\"),\n    ],\n    \"QuarantineConfig\": [\n        (\"Quarantine Guide\", \"../features/quarantine.md\"),\n        (\"Validation\", \"../features/quality_gates.md\"),\n    ],\n    \"GateConfig\": [\n        (\"Quality Gates\", \"../features/quality_gates.md\"),\n    ],\n    # Add more as needed\n}\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-6-regenerate-and-verify","title":"Phase 6: Regenerate and Verify","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#61-regenerate-yaml_schemamd","title":"6.1 Regenerate yaml_schema.md","text":"<pre><code>python odibi/introspect.py\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#62-verify-build","title":"6.2 Verify Build","text":"<pre><code>python -m mkdocs build\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#63-lint-and-format","title":"6.3 Lint and Format","text":"<pre><code>ruff check . --fix\nruff format .\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#64-run-tests","title":"6.4 Run Tests","text":"<pre><code>pytest tests/ -v\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#65-commit-and-push","title":"6.5 Commit and Push","text":"<pre><code>git add .\ngit commit -m \"docs: Reference enrichment campaign - beginner-friendly yaml_schema\"\ngit push\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#files-to-modify","title":"Files to Modify","text":"File Changes <code>odibi/config.py</code> Update docstrings for ~25 config classes <code>odibi/introspect.py</code> Update SECTION_INTROS, add SEE_ALSO_LINKS <code>docs/reference/yaml_schema.md</code> Auto-regenerated"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#config-classes-to-enrich","title":"Config Classes to Enrich","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#critical-must-have","title":"Critical (Must Have)","text":"<ul> <li>[ ] <code>NotNullTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>UniqueTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>FreshnessContract</code> - Add when-to-use, see-also</li> <li>[ ] <code>RowCountTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>RangeTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>AcceptedValuesTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>RegexMatchTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>VolumeDropTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>SchemaContract</code> - Add when-to-use, see-also</li> <li>[ ] <code>DistributionContract</code> - Add when-to-use, see-also</li> <li>[ ] <code>ValidationConfig</code> - Add disambiguation, see-also</li> <li>[ ] <code>QuarantineConfig</code> - Add when-to-use, see-also</li> <li>[ ] <code>GateConfig</code> - Add when-to-use, see-also</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#important-should-have","title":"Important (Should Have)","text":"<ul> <li>[ ] <code>ReadConfig</code> - Add full examples, see-also</li> <li>[ ] <code>WriteConfig</code> - Add full examples, see-also</li> <li>[ ] <code>TransformConfig</code> - Add examples, see-also</li> <li>[ ] <code>IncrementalConfig</code> - Add when-to-use, examples</li> <li>[ ] <code>DimensionPattern</code> - Add beginner context</li> <li>[ ] <code>FactPattern</code> - Add beginner context</li> <li>[ ] <code>AggregationPattern</code> - Add beginner context</li> <li>[ ] <code>DateDimensionPattern</code> - Add when-to-use</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#nice-to-have","title":"Nice to Have","text":"<ul> <li>[ ] <code>LocalConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>DeltaConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>AzureBlobConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>SQLServerConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>HttpConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>RetryConfig</code> - Add when-to-use</li> <li>[ ] <code>AlertConfig</code> - Add when-to-use</li> <li>[ ] <code>PerformanceConfig</code> - Add see-also</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#estimated-effort","title":"Estimated Effort","text":"Phase Focus Time 1 Validation/Contracts/Gates disambiguation 1 hour 2 Core configs (Read/Write/Transform) 1.5 hours 3 Pattern configs 1 hour 4 Connection configs 30 min 5 Cross-references in introspect.py 30 min 6 Regenerate, test, push 30 min <p>Total: ~5 hours</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#validation-checklist","title":"Validation Checklist","text":"<p>Before marking complete:</p> <ul> <li>[ ] <code>python odibi/introspect.py</code> runs without errors</li> <li>[ ] <code>docs/reference/yaml_schema.md</code> is regenerated</li> <li>[ ] <code>python -m mkdocs build</code> passes</li> <li>[ ] <code>ruff check .</code> passes</li> <li>[ ] <code>ruff format .</code> passes (no changes)</li> <li>[ ] <code>pytest tests/</code> passes</li> <li>[ ] Git commit and push successful</li> </ul>"},{"location":"ROADMAP/","title":"Odibi Phase 3 Plan: The \"Rich Docs\" Strategy","text":"<p>We are pivoting from a separate Cookbook to an Embedded Knowledge strategy. The goal is to make <code>docs/api.md</code> a single, high-value artifact that serves as both Reference and Guide.</p> <p>We will achieve this by embedding \"Gold Mine\" recipes directly into the Python code docstrings.</p>"},{"location":"ROADMAP/#1-the-strategy-code-as-documentation","title":"1. The Strategy: \"Code as Documentation\"","text":"<p>We will rewrite the Pydantic model docstrings in <code>odibi/config.py</code> and <code>odibi/transformers/*.py</code> to include: 1.  The Business Problem: Why do I need this? 2.  The Recipe: A copy-pasteable YAML block. 3.  The \"Why\": Brief explanation of the mechanics.</p>"},{"location":"ROADMAP/#2-targeted-upgrades","title":"2. Targeted Upgrades","text":"<p>We will enhance the following models with extensive examples:</p>"},{"location":"ROADMAP/#a-scd2params-in-odibitransformersscdpy","title":"A. <code>SCD2Params</code> (in <code>odibi/transformers/scd.py</code>)","text":"<ul> <li>Theme: \"The Time Machine\"</li> <li>Content: Explain how to track history, handle effective dates, and close open records.</li> <li>Recipe: A full YAML snippet showing keys, tracking columns, and flags.</li> </ul>"},{"location":"ROADMAP/#b-writeconfig-in-odibiconfigpy","title":"B. <code>WriteConfig</code> (in <code>odibi/config.py</code>)","text":"<ul> <li>Theme: \"Big Data Performance\"</li> <li>Content: Explain when to use <code>partition_by</code> (filtering) vs <code>zorder_by</code> (skipping).</li> <li>Recipe: A \"Lakehouse Optimized\" configuration.</li> </ul>"},{"location":"ROADMAP/#c-validationconfig-in-odibiconfigpy","title":"C. <code>ValidationConfig</code> (in <code>odibi/config.py</code>)","text":"<ul> <li>Theme: \"The Indestructible Pipeline\"</li> <li>Content: Explain blocking vs. warning severity.</li> <li>Recipe: A \"Quality Gate\" configuration preventing bad data from reaching Gold.</li> </ul>"},{"location":"ROADMAP/#d-mergeparams-in-odibitransformersmerge_transformerpy","title":"D. <code>MergeParams</code> (in <code>odibi/transformers/merge_transformer.py</code>)","text":"<ul> <li>Theme: \"GDPR &amp; Compliance\"</li> <li>Content: Explain <code>delete_match</code> strategy.</li> <li>Recipe: A \"Right to be Forgotten\" pipeline snippet.</li> </ul>"},{"location":"ROADMAP/#3-execution-steps","title":"3. Execution Steps","text":"<ol> <li>[x] Edit <code>odibi/transformers/scd.py</code>: Replace the placeholder docstring with the full \"Time Machine\" guide.</li> <li>[x] Edit <code>odibi/config.py</code>: update <code>WriteConfig</code>, <code>ValidationConfig</code>, and <code>NodeConfig</code> with rich examples.</li> <li>[x] Run <code>python odibi/introspect.py</code>: Generate the new <code>api.md</code>.</li> <li>[x] Review: Confirm <code>api.md</code> looks like a \"Gold Mine\" without needing a separate cookbook.</li> </ol>"},{"location":"ROADMAP/#4-extended-improvements-completed","title":"4. Extended Improvements (Completed)","text":"<p>Based on feedback, we went further to improve usability:</p> <ul> <li>Smart Node Scenarios: Added concrete examples for \"Standard ETL\", \"Heavy Lifter\", and \"Tagged Runner\" in <code>NodeConfig</code>.</li> <li>Universal Reader: Added \"Time Traveler\" and \"Streaming\" recipes to <code>ReadConfig</code>.</li> <li>Transformer Catalog: Added a searchable list of available transformers directly in <code>NodeConfig</code> docs.</li> <li>Concept Clarity: Explicitly explained \"Transformer (App) vs. Transform Steps (Script)\" and \"Chaining Operations\".</li> <li>Navigation: Added \"Back to Catalog\" links for better UX.</li> <li>\"Kitchen Sink\" Scenario: Demonstrated <code>read</code> -&gt; <code>transformer</code> -&gt; <code>transform</code> -&gt; <code>write</code> in a single node to prove composability.</li> </ul>"},{"location":"catalog_audit_report/","title":"Catalog Implementation Audit Report","text":"<p>Date: December 2024 Scope: Comprehensive audit of the Odibi System Catalog implementation</p>"},{"location":"catalog_audit_report/#executive-summary","title":"Executive Summary","text":"<p>A comprehensive audit of the catalog implementation revealed a critical path mismatch bug that causes silent failures in the stateful incremental mode. Additionally, 52 instances of silent exception handling were found that mask errors.</p>"},{"location":"catalog_audit_report/#critical-issues-found","title":"Critical Issues Found","text":"Severity Issue Location Impact \ud83d\udd34 Critical Path naming mismatch <code>state/__init__.py:475-476</code> HWM state silently fails \ud83d\udd34 Critical Node fallback uses wrong paths <code>node.py:1457-1458</code> State operations fail \ud83d\udfe1 Medium Silent exception handling Multiple files (52 instances) Errors hidden from users"},{"location":"catalog_audit_report/#1-path-consistency-audit","title":"1. Path Consistency Audit","text":""},{"location":"catalog_audit_report/#11-catalog-manager-paths-correct-source-of-truth","title":"1.1 Catalog Manager Paths (Correct Source of Truth)","text":"<p>File: <code>odibi/catalog.py</code> (lines 89-99)</p> <pre><code>self.tables = {\n    \"meta_tables\": f\"{self.base_path}/meta_tables\",\n    \"meta_runs\": f\"{self.base_path}/meta_runs\",\n    \"meta_patterns\": f\"{self.base_path}/meta_patterns\",\n    \"meta_metrics\": f\"{self.base_path}/meta_metrics\",\n    \"meta_state\": f\"{self.base_path}/meta_state\",      # \u2705 Correct: meta_state\n    \"meta_pipelines\": f\"{self.base_path}/meta_pipelines\",\n    \"meta_nodes\": f\"{self.base_path}/meta_nodes\",\n    \"meta_schemas\": f\"{self.base_path}/meta_schemas\",\n    \"meta_lineage\": f\"{self.base_path}/meta_lineage\",\n}\n</code></pre>"},{"location":"catalog_audit_report/#12-state-backend-paths-bug-wrong-names","title":"1.2 State Backend Paths (BUG: Wrong Names)","text":"<p>File: <code>odibi/state/__init__.py</code> (lines 475-476)</p> <pre><code>meta_state_path = f\"{base_uri}/state\"    # \u274c WRONG: should be /meta_state\nmeta_runs_path = f\"{base_uri}/runs\"      # \u274c WRONG: should be /meta_runs\n</code></pre>"},{"location":"catalog_audit_report/#13-node-fallback-paths-bug-wrong-names","title":"1.3 Node Fallback Paths (BUG: Wrong Names)","text":"<p>File: <code>odibi/node.py</code> (lines 1457-1458)</p> <pre><code>backend = CatalogStateBackend(\n    spark_session=spark_session,\n    meta_state_path=\".odibi/system/state\",   # \u274c WRONG: should be .odibi/system/meta_state\n    meta_runs_path=\".odibi/system/runs\",     # \u274c WRONG: should be .odibi/system/meta_runs\n)\n</code></pre>"},{"location":"catalog_audit_report/#14-azure-blob-structure-observed","title":"1.4 Azure Blob Structure (Observed)","text":"<pre><code>datalake/_odibi_system/\n\u251c\u2500\u2500 meta_lineage/     \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_metrics/     \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_nodes/       \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_patterns/    \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_pipelines/   \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_runs/        \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_schemas/     \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_state/       \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_tables/      \u2705 Catalog creates this\n\u251c\u2500\u2500 state/            \u26a0\ufe0f May be created by state backend (orphan)\n\u2514\u2500\u2500 runs/             \u26a0\ufe0f May be created by state backend (orphan)\n</code></pre>"},{"location":"catalog_audit_report/#2-catalog-manager-implementation","title":"2. Catalog Manager Implementation","text":""},{"location":"catalog_audit_report/#21-initialization-flow","title":"2.1 Initialization Flow","text":"<pre><code>PipelineManager.from_yaml()\n    \u2514\u2500\u2500 __init__()\n        \u2514\u2500\u2500 CatalogManager(spark, config, base_path, engine)\n            \u2514\u2500\u2500 bootstrap()\n                \u2514\u2500\u2500 _ensure_table() for each meta_* table\n</code></pre>"},{"location":"catalog_audit_report/#22-table-creation-bootstrap-method","title":"2.2 Table Creation (bootstrap method)","text":"<p>The 9 meta tables are created with proper schemas:</p> Table Schema Method Partition Cols Purpose meta_tables <code>_get_schema_meta_tables()</code> None Asset inventory meta_runs <code>_get_schema_meta_runs()</code> pipeline_name, date Execution history meta_patterns <code>_get_schema_meta_patterns()</code> None Pattern compliance meta_metrics <code>_get_schema_meta_metrics()</code> None Business metrics meta_state <code>_get_schema_meta_state()</code> None HWM key-value store meta_pipelines <code>_get_schema_meta_pipelines()</code> None Pipeline configs meta_nodes <code>_get_schema_meta_nodes()</code> None Node configs meta_schemas <code>_get_schema_meta_schemas()</code> None Schema versions meta_lineage <code>_get_schema_meta_lineage()</code> None Table lineage"},{"location":"catalog_audit_report/#23-unity-catalog-vs-path-based","title":"2.3 Unity Catalog vs Path-Based","text":"<p>Tables are path-based only, not registered in Unity Catalog. The code uses: - <code>spark.read.format(\"delta\").load(path)</code> for reads - <code>df.write.format(\"delta\").save(path)</code> for writes - <code>MERGE INTO delta.\\</code>{path}`` for upserts</p>"},{"location":"catalog_audit_report/#3-state-backend-integration","title":"3. State Backend Integration","text":""},{"location":"catalog_audit_report/#31-catalogstatebackend-class","title":"3.1 CatalogStateBackend Class","text":"<p>File: <code>odibi/state/__init__.py</code> (lines 105-336)</p> <pre><code>class CatalogStateBackend(StateBackend):\n    def __init__(\n        self,\n        meta_runs_path: str,\n        meta_state_path: str,\n        spark_session: Any = None,\n        storage_options: Optional[Dict[str, str]] = None,\n    ):\n        self.meta_runs_path = meta_runs_path\n        self.meta_state_path = meta_state_path\n</code></pre>"},{"location":"catalog_audit_report/#32-hwm-operations","title":"3.2 HWM Operations","text":"Method Reads From Writes To Issue <code>get_hwm(key)</code> meta_state_path - \u274c Wrong path if created by create_state_backend <code>set_hwm(key, value)</code> - meta_state_path \u274c Creates orphan table <code>get_last_run_info()</code> meta_runs_path - \u274c Wrong path if created by create_state_backend"},{"location":"catalog_audit_report/#33-path-construction-bug-location","title":"3.3 Path Construction (Bug Location)","text":"<p><code>create_state_backend()</code> function (lines 392-483):</p> <pre><code># Lines 475-476 - THE BUG\nmeta_state_path = f\"{base_uri}/state\"    # Should be: f\"{base_uri}/meta_state\"\nmeta_runs_path = f\"{base_uri}/runs\"      # Should be: f\"{base_uri}/meta_runs\"\n</code></pre>"},{"location":"catalog_audit_report/#4-readwrite-operations-matrix","title":"4. Read/Write Operations Matrix","text":""},{"location":"catalog_audit_report/#41-meta_state-table","title":"4.1 meta_state Table","text":"Operation Module Function Path Used CREATE catalog.py bootstrap() \u2705 <code>meta_state</code> WRITE state/init.py set_hwm() \u274c Depends on backend creation READ state/init.py get_hwm() \u274c Depends on backend creation"},{"location":"catalog_audit_report/#42-meta_runs-table","title":"4.2 meta_runs Table","text":"Operation Module Function Path Used CREATE catalog.py bootstrap() \u2705 <code>meta_runs</code> WRITE catalog.py log_run() \u2705 <code>tables[\"meta_runs\"]</code> READ state/init.py get_last_run_info() \u274c Depends on backend creation READ catalog.py get_average_duration() \u2705 <code>tables[\"meta_runs\"]</code>"},{"location":"catalog_audit_report/#5-connection-handling","title":"5. Connection Handling","text":""},{"location":"catalog_audit_report/#51-base-uri-construction","title":"5.1 Base URI Construction","text":"<p>Azure Blob:</p> <pre><code>base_uri = f\"abfss://{container}@{account_name}.dfs.core.windows.net/{system.path}\"\n</code></pre> <p>Local:</p> <pre><code>base_uri = os.path.join(base_path, system.path)\n</code></pre>"},{"location":"catalog_audit_report/#52-spark-session-injection","title":"5.2 Spark Session Injection","text":"<ul> <li>PipelineManager correctly injects spark session to CatalogManager</li> <li>Node class gets spark from <code>self.engine.spark</code></li> <li>CatalogStateBackend receives spark via constructor</li> </ul>"},{"location":"catalog_audit_report/#6-error-handling-review","title":"6. Error Handling Review","text":""},{"location":"catalog_audit_report/#61-silent-exception-patterns-found","title":"6.1 Silent Exception Patterns Found","text":"<p>Total: 52 instances of <code>except Exception:</code> without proper handling</p> <p>Critical locations in state/init.py: - Line 66: LocalJSONStateBackend load - Line 164-167: _get_last_run_spark (catches all, returns None) - Line 203-209: _get_last_run_local (catches all, returns None) - Line 232-234: _get_hwm_spark (catches all, returns None) - Line 256-258: _get_hwm_local (catches all, returns None) - Line 335: _spark_table_exists</p>"},{"location":"catalog_audit_report/#62-impact","title":"6.2 Impact","text":"<p>These silent exceptions mask the path mismatch bug: 1. State backend tries to read from <code>/state</code> 2. Table doesn't exist (it's at <code>/meta_state</code>) 3. Exception caught silently 4. Returns None 5. HWM appears to be empty 6. Full data reload instead of incremental</p>"},{"location":"catalog_audit_report/#7-nodepipeline-integration","title":"7. Node/Pipeline Integration","text":""},{"location":"catalog_audit_report/#71-node-class-state-manager-initialization","title":"7.1 Node Class State Manager Initialization","text":"<p>File: <code>odibi/node.py</code> (lines 1442-1461)</p> <pre><code># Initialize State Manager\nif self.catalog_manager and self.catalog_manager.tables:\n    backend = CatalogStateBackend(\n        spark_session=spark_session,\n        meta_state_path=self.catalog_manager.tables.get(\"meta_state\"),  # \u2705 Correct\n        meta_runs_path=self.catalog_manager.tables.get(\"meta_runs\"),    # \u2705 Correct\n    )\nelse:\n    # Fallback to default local paths\n    backend = CatalogStateBackend(\n        spark_session=spark_session,\n        meta_state_path=\".odibi/system/state\",   # \u274c Wrong name\n        meta_runs_path=\".odibi/system/runs\",     # \u274c Wrong name\n    )\n</code></pre>"},{"location":"catalog_audit_report/#72-pipeline-class-state-manager-creation","title":"7.2 Pipeline Class State Manager Creation","text":"<p>File: <code>odibi/pipeline.py</code> (lines 242-256)</p> <pre><code>if resume_from_failure:\n    if self.project_config:\n        backend = create_state_backend(   # \u274c Uses buggy function\n            config=self.project_config,\n            project_root=\".\",\n            spark_session=getattr(self.engine, \"spark\", None),\n        )\n</code></pre>"},{"location":"catalog_audit_report/#8-recommendations","title":"8. Recommendations","text":""},{"location":"catalog_audit_report/#81-critical-fix-path-names","title":"8.1 Critical Fix: Path Names","text":"<p>Fix 1: Update <code>create_state_backend()</code> in <code>state/__init__.py</code>:</p> <pre><code># Change lines 475-476 from:\nmeta_state_path = f\"{base_uri}/state\"\nmeta_runs_path = f\"{base_uri}/runs\"\n\n# To:\nmeta_state_path = f\"{base_uri}/meta_state\"\nmeta_runs_path = f\"{base_uri}/meta_runs\"\n</code></pre> <p>Fix 2: Update Node fallback in <code>node.py</code>:</p> <pre><code># Change lines 1457-1458 from:\nmeta_state_path=\".odibi/system/state\",\nmeta_runs_path=\".odibi/system/runs\",\n\n# To:\nmeta_state_path=\".odibi/system/meta_state\",\nmeta_runs_path=\".odibi/system/meta_runs\",\n</code></pre>"},{"location":"catalog_audit_report/#82-add-logging-to-silent-exceptions","title":"8.2 Add Logging to Silent Exceptions","text":"<p>Replace silent catches with logged warnings:</p> <pre><code>except Exception as e:\n    logger.warning(f\"Failed to get HWM for key '{key}': {e}\")\n    return None\n</code></pre>"},{"location":"catalog_audit_report/#83-add-test-coverage","title":"8.3 Add Test Coverage","text":"<p>Create tests for: 1. Path consistency between catalog and state backend 2. HWM round-trip (set then get) 3. State backend with catalog manager integration</p>"},{"location":"catalog_audit_report/#9-architecture-diagram","title":"9. Architecture Diagram","text":"<p>See generated Mermaid diagrams above showing: 1. Path flow from config to storage 2. Read/write operations and their path sources</p>"},{"location":"catalog_audit_report/#10-files-changed-summary","title":"10. Files Changed Summary","text":"File Changes Needed <code>odibi/state/__init__.py</code> Fix path names (lines 475-476), add logging <code>odibi/node.py</code> Fix fallback path names (lines 1457-1458) <code>tests/</code> Add catalog integration tests"},{"location":"phase_9a_autonomous_learning/","title":"Phase 9.A: Guarded Autonomous Learning (Observation-Only)","text":""},{"location":"phase_9a_autonomous_learning/#implementation-summary","title":"Implementation Summary","text":"<p>Phase 9.A enables unattended, large-scale learning cycles that safely execute real pipelines against frozen datasets while maintaining strict safety guarantees.</p>"},{"location":"phase_9a_autonomous_learning/#key-components","title":"Key Components","text":"Component Location Purpose <code>autonomous_learning.py</code> <code>agents/core/</code> Core module with scheduler, guards, and config <code>LearningModeGuard</code> <code>agents/core/autonomous_learning.py</code> Runtime safety enforcement <code>GuardedCycleRunner</code> <code>agents/core/autonomous_learning.py</code> Extended CycleRunner with learning guards <code>AutonomousLearningScheduler</code> <code>agents/core/autonomous_learning.py</code> Multi-cycle session management Learning Mode Disclaimer <code>agents/core/reports.py</code> Report section for learning cycles Observation Indexing <code>agents/core/indexing.py</code> Extended to index learning observations"},{"location":"phase_9a_autonomous_learning/#files-modified","title":"Files Modified","text":"<ol> <li><code>agents/core/autonomous_learning.py</code> (NEW)</li> <li><code>LearningCycleConfig</code>: Configuration with validation for learning mode</li> <li><code>LearningModeGuard</code>: Runtime guard that detects violations</li> <li><code>GuardedCycleRunner</code>: CycleRunner that skips improvement/review steps</li> <li><code>AutonomousLearningScheduler</code>: Session manager for multi-cycle runs</li> <li> <p><code>validate_learning_profile()</code>: YAML profile validation</p> </li> <li> <p><code>agents/core/reports.py</code></p> </li> <li>Added <code>_generate_learning_mode_disclaimer()</code> method</li> <li> <p>Reports now include \"Learning Mode \u2014 No Changes Applied\" disclaimer</p> </li> <li> <p><code>agents/core/indexing.py</code></p> </li> <li>Added <code>is_learning_mode</code>, <code>observations</code>, <code>patterns_discovered</code> fields</li> <li>Extended <code>_build_document()</code> to extract observations and patterns</li> <li> <p>Added <code>_extract_observations()</code> and <code>_extract_patterns()</code> methods</p> </li> <li> <p><code>agents/core/__init__.py</code></p> </li> <li> <p>Exported all new Phase 9.A components</p> </li> <li> <p><code>agents/core/tests/test_autonomous_learning.py</code> (NEW)</p> </li> <li>38 comprehensive tests for all invariants</li> </ol>"},{"location":"phase_9a_autonomous_learning/#verification-checklist","title":"Verification Checklist","text":""},{"location":"phase_9a_autonomous_learning/#phase-9a-invariants-all-verified","title":"\u2705 Phase 9.A Invariants (ALL VERIFIED)","text":"Invariant Status Enforcement Mechanism \u274c No ImprovementAgent execution \u2705 <code>GUARDED_SKIP_STEPS</code>, <code>LearningModeGuard</code> \u274c No code edits \u2705 <code>LearningModeGuard.assert_no_code_edits()</code> \u274c No file writes outside evidence/reports/artifacts/indexes \u2705 <code>ALLOWED_WRITE_PATHS</code> whitelist \u274c No source downloads \u2705 Profile <code>require_frozen=True</code> enforced \u274c No schema inference \u2705 Learning mode uses frozen pools only \u274c No agent memory writes outside LEARNING scope \u2705 <code>memory_scope: \"learning\"</code> in context \u274c No retries or recovery logic \u2705 Fail-fast on any error"},{"location":"phase_9a_autonomous_learning/#functional-requirements","title":"\u2705 Functional Requirements","text":"Requirement Status Implementation Execute real pipelines \u2705 <code>GuardedCycleRunner.run_learning_cycle()</code> Collect execution evidence \u2705 <code>ExecutionEvidence</code> artifacts persisted Produce human-readable reports \u2705 <code>CycleReportGenerator</code> with learning disclaimer Accumulate indexed learnings \u2705 <code>CycleIndexManager</code> with observations/patterns Wall-clock limit enforcement \u2705 <code>CycleTerminationEnforcer</code> + scheduler limits Step limit enforcement \u2705 <code>max_steps</code> in <code>LearningCycleConfig</code> Clean termination on error \u2705 Try/except with <code>LearningCycleResult.status=FAILED</code>"},{"location":"phase_9a_autonomous_learning/#profile-validation","title":"\u2705 Profile Validation","text":"<p>The <code>large_scale_learning.yaml</code> profile is validated for:</p> <ul> <li><code>mode: learning</code> \u2705</li> <li><code>max_improvements: 0</code> \u2705</li> <li><code>require_frozen: true</code> \u2705</li> <li><code>deterministic: true</code> \u2705</li> <li><code>allow_messy: true</code> \u2705</li> <li><code>allow_tier_mixing: false</code> \u2705</li> <li><code>tiers: tier100gb, tier600gb</code> \u2705</li> </ul>"},{"location":"phase_9a_autonomous_learning/#usage","title":"Usage","text":""},{"location":"phase_9a_autonomous_learning/#run-a-single-learning-cycle","title":"Run a Single Learning Cycle","text":"<pre><code>from agents.core import (\n    AutonomousLearningScheduler,\n    LearningCycleConfig,\n)\n\nscheduler = AutonomousLearningScheduler(odibi_root=\"d:/odibi\")\n\nconfig = LearningCycleConfig(\n    project_root=\"d:/odibi/examples\",\n    max_runtime_hours=8.0,\n)\n\nresult = scheduler.run_single_cycle(config)\nprint(f\"Cycle {result.cycle_id}: {result.status.value}\")\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#run-a-multi-cycle-session","title":"Run a Multi-Cycle Session","text":"<pre><code>session = scheduler.run_session(\n    config=config,\n    max_cycles=100,\n    max_wall_clock_hours=168.0,  # 1 week\n)\n\nprint(f\"Session completed: {session.cycles_completed} cycles\")\nprint(f\"Failed: {session.cycles_failed}\")\nprint(f\"Total duration: {session.total_duration_seconds:.2f}s\")\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#validate-a-profile","title":"Validate a Profile","text":"<pre><code>from agents.core import validate_learning_profile\n\nerrors = validate_learning_profile(\n    \".odibi/cycle_profiles/large_scale_learning.yaml\"\n)\nif errors:\n    for e in errors:\n        print(f\"ERROR: {e}\")\nelse:\n    print(\"Profile is valid for autonomous learning\")\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#test-coverage","title":"Test Coverage","text":"<pre><code>agents/core/tests/test_autonomous_learning.py - 38 tests\n\nTestLearningCycleConfig: 6 tests\nTestLearningModeGuard: 8 tests\nTestGuardedSkipSteps: 5 tests\nTestLearningModeViolation: 2 tests\nTestGuardedCycleRunnerSkipping: 3 tests\nTestLearningCycleResult: 1 test\nTestAutonomousLearningSession: 2 tests\nTestValidateLearningProfile: 6 tests\nTestPhase9AInvariants: 5 tests\n\nAll tests passing \u2705\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#safety-confirmation","title":"Safety Confirmation","text":"<p>This phase is safe to run unattended for days or weeks.</p> <p>The implementation guarantees:</p> <ol> <li>No code mutations: ImprovementAgent and ReviewerAgent are mechanically skipped</li> <li>No proposals: <code>max_improvements=0</code> is enforced at config validation</li> <li>Deterministic execution: Same inputs \u2192 same outputs (verified via hashes)</li> <li>Clean termination: All cycles terminate via max_steps, timeout, or error</li> <li>Audit trail: All cycles produce reports and indexed learnings</li> <li>Fail-fast behavior: Any violation raises <code>LearningModeViolation</code></li> </ol>"},{"location":"phase_9a_autonomous_learning/#non-goals-explicitly-not-implemented","title":"Non-Goals (Explicitly NOT Implemented)","text":"<ul> <li>\u274c New agents</li> <li>\u274c New UI</li> <li>\u274c New policies</li> <li>\u274c Behavior changes to improvement or scheduled modes</li> <li>\u274c Auto-advancement from learning to improvement mode</li> <li>\u274c Memory promotion to validated curriculum</li> </ul>"},{"location":"phase_9a_autonomous_learning/#next-phase-considerations","title":"Next Phase Considerations","text":"<p>When Phase 9.B (Guarded Autonomous Improvement) is implemented:</p> <ol> <li>Create separate <code>ImprovementCycleConfig</code> with <code>max_improvements &gt; 0</code></li> <li>Require explicit human approval to switch from learning to improvement</li> <li>Add golden project requirements for improvement cycles</li> <li>Maintain learning mode as the default for autonomous execution</li> </ol>"},{"location":"phase_9c_cycle_profiles/","title":"Phase 9.C: Cycle Profiles","text":""},{"location":"phase_9c_cycle_profiles/#overview","title":"Overview","text":"<p>Phase 9.C makes cycle profiles executable, auditable, and selectable from the UI.</p> <p>Cycle profiles are YAML configuration files stored in <code>.odibi/cycle_profiles/</code> that define deterministic learning behavior. They replace hardcoded parameters in the scheduler with versioned, hashable, frozen configurations.</p>"},{"location":"phase_9c_cycle_profiles/#goals","title":"Goals","text":"<ul> <li>\u2705 Profiles loaded from <code>.odibi/cycle_profiles/</code></li> <li>\u2705 Profile config frozen for entire session</li> <li>\u2705 Profile name + hash recorded in heartbeat, reports, session metadata</li> <li>\u2705 UI dropdown to select profiles</li> <li>\u2705 Same profile + same inputs \u2192 same execution</li> </ul>"},{"location":"phase_9c_cycle_profiles/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        cycle_profile.py                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  CycleProfile (Pydantic)    \u2500\u2500\u2500\u2500\u2500\u25ba  FrozenCycleProfile          \u2502\n\u2502      \u2191                                   \u2193                      \u2502\n\u2502  YAML File                    LearningCycleConfig               \u2502\n\u2502      \u2191                                   \u2193                      \u2502\n\u2502  CycleProfileLoader  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500  AutonomousLearningScheduler       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#components","title":"Components","text":""},{"location":"phase_9c_cycle_profiles/#1-cycleprofile-pydantic-schema","title":"1. CycleProfile (Pydantic Schema)","text":"<p>Located in <code>agents/core/cycle_profile.py</code>.</p> <p>Validates profile YAML against Phase 9.A learning mode invariants:</p> <pre><code>from agents.core import CycleProfile\n\nprofile = CycleProfile(\n    profile_id=\"large_scale_learning\",\n    profile_name=\"Learning at Scale\",\n    mode=\"learning\",           # Must be \"learning\"\n    max_improvements=0,        # Must be 0\n    cycle_source_config=CycleSourceConfigSchema(\n        require_frozen=True,   # Must be True\n        deterministic=True,    # Must be True\n    ),\n    guardrails=GuardrailsSchema(\n        allow_source_mutation=False,  # Must be False\n    ),\n)\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#2-frozencycleprofile-immutable-runtime-config","title":"2. FrozenCycleProfile (Immutable Runtime Config)","text":"<p>A frozen dataclass that cannot be modified after loading:</p> <pre><code>from agents.core import CycleProfileLoader\n\nloader = CycleProfileLoader(odibi_root=\"d:/odibi\")\nfrozen = loader.load_profile(\"large_scale_learning\")\n\n# Immutable - this raises an error:\nfrozen.profile_id = \"changed\"  # \u274c FrozenInstanceError\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#3-profile-hash","title":"3. Profile Hash","text":"<p>Every profile has a SHA-256 content hash (first 16 chars) for auditability:</p> <pre><code>from agents.core import compute_profile_hash\n\ncontent = open(\"profile.yaml\").read()\nhash = compute_profile_hash(content)  # e.g., \"a1b2c3d4e5f6g7h8\"\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#4-cycleprofileloader","title":"4. CycleProfileLoader","text":"<p>Discovers and loads profiles from <code>.odibi/cycle_profiles/</code>:</p> <pre><code>from agents.core import CycleProfileLoader\n\nloader = CycleProfileLoader(\"d:/odibi\")\n\n# List available profiles\nprofiles = loader.list_profiles()  # [\"large_scale_learning\", \"quick_test\"]\n\n# Load and freeze a profile\nfrozen = loader.load_profile(\"large_scale_learning\")\n\n# Get summary for UI display\nsummary = loader.get_profile_summary(\"large_scale_learning\")\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#usage","title":"Usage","text":""},{"location":"phase_9c_cycle_profiles/#running-a-session-with-a-profile","title":"Running a Session with a Profile","text":"<pre><code>from agents.core import AutonomousLearningScheduler\n\nscheduler = AutonomousLearningScheduler(odibi_root=\"d:/odibi\")\n\n# Preferred: Use run_session_with_profile()\nsession = scheduler.run_session_with_profile(\n    profile_name=\"large_scale_learning\",\n    project_root=\"d:/odibi/examples\",\n    max_cycles=10,\n    max_wall_clock_hours=8.0,\n)\n\n# Alternative: Create config from frozen profile\nfrozen = scheduler.load_profile(\"large_scale_learning\")\nconfig = LearningCycleConfig.from_frozen_profile(\n    profile=frozen,\n    project_root=\"d:/odibi/examples\",\n)\nsession = scheduler.run_session(config=config)\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#creating-a-profile","title":"Creating a Profile","text":"<p>Create a YAML file in <code>.odibi/cycle_profiles/</code>:</p> <pre><code># .odibi/cycle_profiles/my_learning.yaml\n\nprofile_id: my_learning\nprofile_name: \"My Learning Profile\"\nprofile_version: \"1.0.0\"\ndescription: |\n  Custom learning profile for my use case.\n\nmode: learning\nmax_improvements: 0\n\ncycle_source_config:\n  selection_policy: learning_default\n  allowed_tiers:\n    - tier100gb\n  max_sources: 2\n  require_frozen: true\n  deterministic: true\n\nguardrails:\n  allow_execution: false\n  allow_downloads: false\n  allow_source_mutation: false\n  max_duration_hours: 12\n\nmetadata:\n  author: you\n  intended_use: \"Custom learning runs\"\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#auditability","title":"Auditability","text":""},{"location":"phase_9c_cycle_profiles/#heartbeat","title":"Heartbeat","text":"<p>The heartbeat file includes profile info:</p> <pre><code>{\n  \"last_cycle_id\": \"abc123\",\n  \"timestamp\": \"2024-01-01T12:00:00\",\n  \"profile_id\": \"large_scale_learning\",\n  \"profile_name\": \"Learning at Scale\",\n  \"profile_hash\": \"a1b2c3d4e5f6g7h8\"\n}\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#cycle-reports","title":"Cycle Reports","text":"<p>Reports include a profile section:</p> <pre><code>## Cycle Metadata\n\n- **Cycle ID:** `abc123`\n- **Mode:** learning\n\n### Cycle Profile\n\n- **Profile ID:** `large_scale_learning`\n- **Profile Name:** Learning at Scale\n- **Profile Hash:** `a1b2c3d4e5f6g7h8`\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#session-metadata","title":"Session Metadata","text":"<pre><code>session.profile_id     # \"large_scale_learning\"\nsession.profile_name   # \"Learning at Scale\"  \nsession.profile_hash   # \"a1b2c3d4e5f6g7h8\"\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#ui-integration","title":"UI Integration","text":"<p>The Learning Session panel includes:</p> <ol> <li>Profile Dropdown - Select from available profiles</li> <li>Profile Info Display - Shows name, description, and hash</li> <li>Refresh Button - Reload available profiles</li> <li>Status Display - Shows active profile in session monitor</li> </ol>"},{"location":"phase_9c_cycle_profiles/#safety-invariants","title":"Safety Invariants","text":"<p>Phase 9.C maintains all Phase 9.A learning mode safety guards:</p> <ul> <li>\u274c No ImprovementAgent execution</li> <li>\u274c No code edits</li> <li>\u274c No file writes outside allowed directories</li> <li>\u274c No profile modification at runtime</li> <li>\u274c No agent writes to profiles</li> <li>\u2705 Profile frozen for entire session</li> <li>\u2705 Deterministic execution</li> <li>\u2705 Full audit trail</li> </ul>"},{"location":"phase_9c_cycle_profiles/#error-handling","title":"Error Handling","text":"<p>Profile loading fails fast on:</p> <ul> <li>Invalid YAML syntax \u2192 <code>CycleProfileError</code></li> <li>Missing required fields \u2192 <code>CycleProfileError</code> with validation errors</li> <li>Learning mode invariant violations \u2192 <code>ValueError</code></li> <li>Profile not found \u2192 <code>CycleProfileError</code></li> </ul> <pre><code>try:\n    frozen = loader.load_profile(\"invalid_profile\")\nexcept CycleProfileError as e:\n    print(f\"Profile error: {e}\")\n    print(f\"Validation errors: {e.errors}\")\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#testing","title":"Testing","text":"<pre><code># Run profile tests\npytest agents/core/tests/test_cycle_profile.py -v\n\n# Run all Phase 9 tests\npytest agents/core/tests/test_autonomous_learning.py -v\npytest agents/core/tests/test_disk_guard.py -v\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#files-changed","title":"Files Changed","text":"<ul> <li><code>agents/core/cycle_profile.py</code> - New module</li> <li><code>agents/core/autonomous_learning.py</code> - Profile integration</li> <li><code>agents/core/disk_guard.py</code> - HeartbeatData profile fields</li> <li><code>agents/core/reports.py</code> - Profile section in reports</li> <li><code>agents/core/__init__.py</code> - Exports</li> <li><code>agents/ui/components/cycle_panel.py</code> - UI dropdown</li> <li><code>agents/core/tests/test_cycle_profile.py</code> - New tests</li> </ul>"},{"location":"source_pools_design/","title":"Source Pools Design Document","text":"<p>Phase: 7.B.1 (Preparation Only) Status: Design Complete Last Updated: 2024-12-14</p>"},{"location":"source_pools_design/#1-overview","title":"1. Overview","text":""},{"location":"source_pools_design/#11-purpose","title":"1.1 Purpose","text":"<p>Source Pools provide deterministic, replayable test data that exercises all supported Odibi data types and ingestion paths. They enable:</p> <ul> <li>Reproducible test runs across environments</li> <li>Coverage of all file formats and source types</li> <li>Testing of both clean and messy data scenarios</li> <li>Cryptographic verification of data integrity</li> </ul>"},{"location":"source_pools_design/#12-design-principles","title":"1.2 Design Principles","text":"Principle Description Determinism All data is pre-generated and frozen. No random generation at runtime. Explicit Schemas No schema inference. All types declared upfront. Disk-Backed All sources are local files with SHA256 hashes. Immutability Frozen pools cannot be modified during test cycles. Discoverability Central index enables programmatic pool discovery."},{"location":"source_pools_design/#13-non-goals","title":"1.3 Non-Goals","text":"<ul> <li>\u274c Runtime data generation</li> <li>\u274c Live API connections</li> <li>\u274c Authentication-requiring sources</li> <li>\u274c Agent autonomy to modify pools during cycles</li> <li>\u274c Schema inference at runtime</li> </ul>"},{"location":"source_pools_design/#2-sourcepool-schema","title":"2. SourcePool Schema","text":""},{"location":"source_pools_design/#21-pydantic-model-location","title":"2.1 Pydantic Model Location","text":"<pre><code>odibi/testing/source_pool.py\n</code></pre>"},{"location":"source_pools_design/#22-core-classes","title":"2.2 Core Classes","text":"Class Purpose <code>FileFormat</code> Enum: csv, json, parquet, avro, delta <code>SourceType</code> Enum: local, adls_emulated, azure_blob_emulated, sql_jdbc_local, cloudfiles <code>DataQuality</code> Enum: clean, messy, mixed <code>PoolStatus</code> Enum: draft, frozen, deprecated <code>ColumnSchema</code> Column definition (name, dtype, nullable, etc.) <code>TableSchema</code> Table schema with columns and keys <code>DataCharacteristics</code> Metadata about data properties <code>IntegrityManifest</code> SHA256 hashes for frozen pools <code>SourcePoolConfig</code> Main pool definition <code>SourcePoolIndex</code> Registry of all pools"},{"location":"source_pools_design/#23-schema-example","title":"2.3 Schema Example","text":"<pre><code>pool_id: nyc_taxi_csv_clean\nversion: \"1.0.0\"\nname: \"NYC Taxi Trips - CSV Clean\"\nfile_format: csv\nsource_type: local\ndata_quality: clean\nschema:\n  columns:\n    - name: trip_id\n      dtype: string\n      nullable: false\n      primary_key: true\n    - name: fare_amount\n      dtype: float64\n      nullable: false\n  primary_keys: [trip_id]\ncache_path: \"nyc_taxi/csv/clean/\"\ncharacteristics:\n  row_count: 10000\n  has_nulls: false\nstatus: frozen\nintegrity:\n  algorithm: sha256\n  file_hashes:\n    \"data.csv\": \"abc123...\"\n  manifest_hash: \"def456...\"\n  frozen_at: \"2024-12-14T00:00:00Z\"\n</code></pre>"},{"location":"source_pools_design/#3-proposed-source-pools","title":"3. Proposed Source Pools","text":""},{"location":"source_pools_design/#31-coverage-matrix","title":"3.1 Coverage Matrix","text":"Pool ID Format Source Type Quality Row Count Primary Use Case <code>nyc_taxi_csv_clean</code> CSV LOCAL Clean 10,000 Bronze ingestion baseline <code>nyc_taxi_csv_messy</code> CSV LOCAL Messy 5,000 Validation &amp; quarantine <code>github_events_json_clean</code> JSON LOCAL Clean 10,000 JSON parsing, nested structures <code>tpch_lineitem_parquet</code> Parquet LOCAL Clean 60,175 Columnar reads, partitioning <code>synthetic_customers_avro</code> Avro LOCAL Clean 5,000 Schema evolution, complex types <code>cdc_orders_delta</code> Delta LOCAL Clean 1,200 CDC/MERGE, time travel <code>northwind_sqlite</code> CSV* SQL_JDBC_LOCAL Clean 830 SQL ingestion, HWM <code>edge_cases_mixed</code> CSV LOCAL Messy 500 All edge cases, regression <p>*Extracted from SQLite database</p>"},{"location":"source_pools_design/#32-format-coverage","title":"3.2 Format Coverage","text":"<ul> <li>\u2705 CSV (3 pools)</li> <li>\u2705 JSON (1 pool)</li> <li>\u2705 Parquet (1 pool)</li> <li>\u2705 Avro (1 pool)</li> <li>\u2705 Delta (1 pool)</li> </ul>"},{"location":"source_pools_design/#33-source-type-coverage","title":"3.3 Source Type Coverage","text":"<ul> <li>\u2705 LOCAL (6 pools)</li> <li>\u2705 SQL_JDBC_LOCAL (1 pool)</li> <li>\ud83d\udd32 ADLS_EMULATED (planned Phase 7.C)</li> <li>\ud83d\udd32 AZURE_BLOB_EMULATED (planned Phase 7.C)</li> <li>\ud83d\udd32 CLOUDFILES (planned Phase 7.C)</li> </ul>"},{"location":"source_pools_design/#34-data-quality-coverage","title":"3.4 Data Quality Coverage","text":"<ul> <li>\u2705 Clean (6 pools)</li> <li>\u2705 Messy (2 pools)</li> </ul>"},{"location":"source_pools_design/#4-recommended-public-datasets","title":"4. Recommended Public Datasets","text":""},{"location":"source_pools_design/#41-real-datasets-for-local-caching","title":"4.1 Real Datasets for Local Caching","text":"Dataset Source License Use For NYC TLC Taxi Data nyc.gov/tlc Public Domain CSV ingestion, large files GitHub Archive gharchive.org CC BY 4.0 JSON/NDJSON, nested structures TPC-H tpc.org TPC Fair Use Parquet, benchmarking Northwind Database GitHub Public Domain SQLite/JDBC testing Faker-based Synthetic Generated locally CC0 Avro, controlled schema"},{"location":"source_pools_design/#42-data-preparation-guidelines","title":"4.2 Data Preparation Guidelines","text":"<ol> <li>Download once \u2192 Store in <code>.odibi/source_cache/</code></li> <li>Subset if needed \u2192 Keep pools &lt; 100MB each</li> <li>Convert formats \u2192 Generate Parquet/Avro from CSV</li> <li>Freeze with hashes \u2192 Generate <code>IntegrityManifest</code></li> <li>Never re-download during cycles \u2192 All data pre-staged</li> </ol>"},{"location":"source_pools_design/#5-disk-layout","title":"5. Disk Layout","text":""},{"location":"source_pools_design/#51-directory-structure","title":"5.1 Directory Structure","text":"<pre><code>.odibi/\n\u251c\u2500\u2500 source_cache/                    # Actual data files\n\u2502   \u251c\u2500\u2500 nyc_taxi/\n\u2502   \u2502   \u251c\u2500\u2500 csv/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 clean/\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 data.csv         # Frozen data file\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 messy/\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 data.csv\n\u2502   \u2502   \u2514\u2500\u2500 parquet/                 # Future: same data in Parquet\n\u2502   \u251c\u2500\u2500 github_events/\n\u2502   \u2502   \u2514\u2500\u2500 json/\n\u2502   \u2502       \u2514\u2500\u2500 clean/\n\u2502   \u2502           \u2514\u2500\u2500 events.ndjson\n\u2502   \u251c\u2500\u2500 tpch/\n\u2502   \u2502   \u2514\u2500\u2500 parquet/\n\u2502   \u2502       \u2514\u2500\u2500 lineitem/\n\u2502   \u2502           \u251c\u2500\u2500 l_shipdate_year=1992/\n\u2502   \u2502           \u2502   \u2514\u2500\u2500 part-0000.parquet\n\u2502   \u2502           \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 synthetic/\n\u2502   \u2502   \u2514\u2500\u2500 avro/\n\u2502   \u2502       \u2514\u2500\u2500 customers/\n\u2502   \u2502           \u2514\u2500\u2500 customers.avro\n\u2502   \u251c\u2500\u2500 cdc/\n\u2502   \u2502   \u2514\u2500\u2500 delta/\n\u2502   \u2502       \u2514\u2500\u2500 orders/\n\u2502   \u2502           \u251c\u2500\u2500 _delta_log/      # Delta transaction log\n\u2502   \u2502           \u2514\u2500\u2500 part-0000.parquet\n\u2502   \u251c\u2500\u2500 northwind/\n\u2502   \u2502   \u2514\u2500\u2500 sqlite/\n\u2502   \u2502       \u2514\u2500\u2500 northwind.db         # SQLite database file\n\u2502   \u2514\u2500\u2500 edge_cases/\n\u2502       \u2514\u2500\u2500 mixed/\n\u2502           \u251c\u2500\u2500 data.csv\n\u2502           \u251c\u2500\u2500 data.json\n\u2502           \u2514\u2500\u2500 data.parquet\n\u2502\n\u2514\u2500\u2500 source_metadata/                 # Pool definitions &amp; index\n    \u251c\u2500\u2500 pool_index.yaml              # Central registry\n    \u2514\u2500\u2500 pools/                       # Individual pool metadata\n        \u251c\u2500\u2500 nyc_taxi_csv_clean.yaml\n        \u251c\u2500\u2500 nyc_taxi_csv_messy.yaml\n        \u251c\u2500\u2500 github_events_json_clean.yaml\n        \u251c\u2500\u2500 tpch_lineitem_parquet.yaml\n        \u251c\u2500\u2500 synthetic_customers_avro.yaml\n        \u251c\u2500\u2500 cdc_orders_delta.yaml\n        \u251c\u2500\u2500 northwind_sqlite.yaml\n        \u2514\u2500\u2500 edge_cases_mixed.yaml\n</code></pre>"},{"location":"source_pools_design/#52-file-naming-conventions","title":"5.2 File Naming Conventions","text":"Component Convention Example Pool ID <code>snake_case</code> <code>nyc_taxi_csv_clean</code> Cache Path <code>{dataset}/{format}/{quality}/</code> <code>nyc_taxi/csv/clean/</code> Metadata File <code>{pool_id}.yaml</code> <code>nyc_taxi_csv_clean.yaml</code> Data Files Format-specific <code>data.csv</code>, <code>events.ndjson</code>, <code>part-*.parquet</code>"},{"location":"source_pools_design/#6-registration-freezing-and-indexing","title":"6. Registration, Freezing, and Indexing","text":""},{"location":"source_pools_design/#61-pool-lifecycle","title":"6.1 Pool Lifecycle","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      POOL LIFECYCLE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   DRAFT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; FROZEN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; DEPRECATED   \u2502\n\u2502     \u2502                      \u2502                       \u2502        \u2502\n\u2502     \u2502 - Schema defined     \u2502 - Data prepared       \u2502 - EOL  \u2502\n\u2502     \u2502 - No data yet        \u2502 - Hashes computed     \u2502 - Keep \u2502\n\u2502     \u2502 - Can modify         \u2502 - IMMUTABLE           \u2502   for  \u2502\n\u2502     \u2502                      \u2502 - Used in tests       \u2502   ref  \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"source_pools_design/#62-freezing-process","title":"6.2 Freezing Process","text":"<ol> <li>Validate Schema: Ensure all columns match actual data</li> <li>Compute File Hashes: SHA256 for each file in <code>cache_path</code></li> <li>Compute Manifest Hash: SHA256 of sorted <code>{path: hash}</code> pairs</li> <li>Update Metadata: Set <code>status: frozen</code>, add <code>integrity</code> block</li> <li>Update Index: Ensure pool is registered in <code>pool_index.yaml</code></li> </ol>"},{"location":"source_pools_design/#63-verification-process","title":"6.3 Verification Process","text":"<pre><code>def verify_pool(pool: SourcePoolConfig) -&gt; bool:\n    \"\"\"Verify frozen pool integrity.\"\"\"\n    if pool.status != PoolStatus.FROZEN:\n        return True  # Only verify frozen pools\n\n    for file_path, expected_hash in pool.integrity.file_hashes.items():\n        actual_hash = compute_sha256(pool.cache_path / file_path)\n        if actual_hash != expected_hash:\n            return False\n    return True\n</code></pre>"},{"location":"source_pools_design/#64-index-operations","title":"6.4 Index Operations","text":"Operation Description When <code>register_pool()</code> Add pool to index After creating metadata <code>freeze_pool()</code> Compute hashes, update status After data is ready <code>deprecate_pool()</code> Mark as deprecated Before removal <code>list_pools()</code> Enumerate all pools Test discovery <code>get_pool()</code> Load pool by ID Test setup <code>verify_pool()</code> Check integrity Before test run"},{"location":"source_pools_design/#7-agent-rules","title":"7. Agent Rules","text":""},{"location":"source_pools_design/#71-what-agents-may-do-with-sources","title":"7.1 What Agents MAY Do with Sources","text":"Action Allowed Notes Read pool metadata \u2705 YES Discovery and planning Read cached data files \u2705 YES For test execution List available pools \u2705 YES Test coverage analysis Verify pool integrity \u2705 YES Pre-test validation Use pools in pipelines \u2705 YES Primary purpose Report on pool coverage \u2705 YES Observability"},{"location":"source_pools_design/#72-what-agents-may-not-do-with-sources","title":"7.2 What Agents MAY NOT Do with Sources","text":"Action Forbidden Reason Modify frozen data files \u274c NO Breaks determinism Delete pool metadata \u274c NO Breaks registry Create new pools during cycles \u274c NO Phase 7.B.1 constraint Download live data during cycles \u274c NO No network during tests Change pool status \u274c NO Lifecycle is controlled Add entries to <code>pool_index.yaml</code> \u274c NO Human-controlled Infer schemas at runtime \u274c NO Explicit schemas only"},{"location":"source_pools_design/#73-enforcement-invariants","title":"7.3 Enforcement Invariants","text":"<pre><code># These invariants MUST hold during any test cycle:\n\nINVARIANT_1 = \"pool_index.yaml is read-only during execution\"\nINVARIANT_2 = \"source_cache/ contents match integrity manifests\"\nINVARIANT_3 = \"No network calls during pool access\"\nINVARIANT_4 = \"Schema comes from metadata, not from data inspection\"\nINVARIANT_5 = \"Frozen pools are immutable\"\n</code></pre>"},{"location":"source_pools_design/#8-implementation-recommendations","title":"8. Implementation Recommendations","text":""},{"location":"source_pools_design/#81-phase-7b2-next-steps","title":"8.1 Phase 7.B.2 (Next Steps)","text":"<ol> <li>Data Preparation Script</li> <li>Download public datasets</li> <li>Convert to target formats</li> <li>Subset to manageable sizes</li> <li> <p>Generate hash manifests</p> </li> <li> <p>Freeze Tool <code>bash    python -m odibi.testing.freeze_pool --pool-id nyc_taxi_csv_clean</code></p> </li> <li> <p>Verification Tool <code>bash    python -m odibi.testing.verify_pools --all</code></p> </li> </ol>"},{"location":"source_pools_design/#82-integration-points","title":"8.2 Integration Points","text":"Component Integration <code>TestRunner</code> Load pools via <code>SourcePoolIndex</code> <code>ReadConfig</code> Accept <code>source_pool</code> as input type <code>ExecutionGateway</code> Verify pool integrity before cycle <code>Story</code> Record which pools were used"},{"location":"source_pools_design/#83-future-extensions","title":"8.3 Future Extensions","text":"<ul> <li>ADLS Emulator: Azurite for blob storage testing</li> <li>CloudFiles Simulation: Directory-based auto-ingest</li> <li>Schema Evolution: Multiple versions per pool</li> <li>Streaming Pools: Simulated real-time data feeds</li> </ul>"},{"location":"source_pools_design/#9-appendix","title":"9. Appendix","text":""},{"location":"source_pools_design/#91-example-integrity-manifest","title":"9.1 Example Integrity Manifest","text":"<pre><code>integrity:\n  algorithm: sha256\n  file_hashes:\n    \"data.csv\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"\n    \"schema.json\": \"d7a8fbb307d7809469ca9abcb0082e4f8d5651e46d3cdb762d02d0bf37c9e592\"\n  manifest_hash: \"a948904f2f0f479b8f8564cbf12dac6b89824ca8f989c8c44c3a\"\n  frozen_at: \"2024-12-14T10:30:00Z\"\n  frozen_by: \"prep-script-v1\"\n</code></pre>"},{"location":"source_pools_design/#92-valid-data-types","title":"9.2 Valid Data Types","text":"Type Python Spark Description <code>string</code> <code>str</code> <code>StringType</code> Text data <code>int64</code> <code>int</code> <code>LongType</code> 64-bit integer <code>float64</code> <code>float</code> <code>DoubleType</code> 64-bit float <code>bool</code> <code>bool</code> <code>BooleanType</code> Boolean <code>datetime</code> <code>datetime</code> <code>TimestampType</code> Timestamp <code>date</code> <code>date</code> <code>DateType</code> Date only <code>binary</code> <code>bytes</code> <code>BinaryType</code> Raw bytes"},{"location":"source_pools_design/#93-edge-case-categories","title":"9.3 Edge Case Categories","text":"Category Examples Unicode Emoji, RTL text, combining characters Null Variants NULL, empty string, whitespace-only Numeric Edge NaN, Inf, -Inf, MAX_INT, MIN_INT, -0 Date Edge 1970-01-01, 2000-01-01, 2038-01-19, far future Injection SQL injection, JSON breaking, CSV escaping Length Empty, single char, 10KB string Duplicates Exact duplicates, case-insensitive duplicates"},{"location":"source_tiers/","title":"Source Tiers - Offline Data Preparation","text":"<p>Phase: 7.E Status: Active Last Updated: 2025-12-14</p>"},{"location":"source_tiers/#overview","title":"Overview","text":"<p>Source Tiers provide disk-backed, deterministic datasets at various scales for Odibi testing and development. All data is:</p> <ul> <li>Deterministic: Fixed seed-based transformations, fully reproducible</li> <li>Hash-locked: SHA256 integrity manifests for every file</li> <li>Replayable: No network access needed after initial download</li> <li>Scalable: From 1GB (tier0) to 2TB (tier2tb)</li> </ul>"},{"location":"source_tiers/#tier-summary","title":"Tier Summary","text":"Tier Size Datasets Use Case <code>tier0</code> ~1GB 1 Quick testing, CI <code>tier20gb</code> ~20GB 2 Local development <code>tier100gb</code> ~100GB 2 Integration testing <code>tier600gb</code> ~600GB 3 Large-scale validation <code>tier2tb</code> ~2TB 6 Full production simulation"},{"location":"source_tiers/#tier-details","title":"Tier Details","text":""},{"location":"source_tiers/#tier0-minimal-1gb","title":"tier0 (Minimal - ~1GB)","text":"<p>Quick testing tier for CI and rapid iteration.</p> Dataset Format Size Source <code>sample_enwiki_pages</code> xml.bz2 ~300MB Wikimedia Foundation (Simple English Wikipedia) <p>Use Cases: - CI pipeline testing - Quick smoke tests - Schema validation</p>"},{"location":"source_tiers/#tier20gb-small-20gb","title":"tier20gb (Small - ~20GB)","text":"<p>Local development tier with real-world datasets.</p> Dataset Format Size Source Messy Variant <code>enwiki_all_titles</code> gz ~400MB Wikimedia Foundation No <code>osm_liechtenstein</code> osm.pbf ~3MB OpenStreetMap / Geofabrik Yes (seed=42) <p>Use Cases: - Local development testing - Feature development - Performance baselines</p>"},{"location":"source_tiers/#tier100gb-medium-100gb","title":"tier100gb (Medium - ~100GB)","text":"<p>Integration testing tier with full Wikipedia and OSM extracts.</p> Dataset Format Size Source Messy Variant <code>enwiki_pages_articles</code> xml.bz2 ~22GB Wikimedia Foundation Yes (seed=12345) <code>osm_iceland</code> osm.pbf ~60MB OpenStreetMap / Geofabrik No <p>Messy Variant Recipe (enwiki_pages_articles):</p> <pre><code>operations:\n  - inject_nulls\n  - type_drift\n  - invalid_timestamps\nnull_rate: 0.01\ntype_drift_rate: 0.005\ninvalid_timestamp_rate: 0.002\nseed: 12345\n</code></pre>"},{"location":"source_tiers/#tier600gb-large-600gb","title":"tier600gb (Large - ~600GB)","text":"<p>Large-scale validation tier with massive real-world datasets.</p> Dataset Format Size Source Messy Variant <code>fineweb_cc_2024_10_sample</code> parquet ~500GB Hugging Face / FineWeb Yes (seed=77777) <code>osm_planet</code> osm.pbf ~75GB OpenStreetMap Foundation No <code>enwiki_pages_articles_full</code> xml.bz2 ~22GB Wikimedia Foundation No <p>FineWeb Dataset: - Common Crawl web text dataset - License: ODC-BY 1.0 - Columns: <code>text</code>, <code>id</code>, <code>dump</code>, <code>url</code>, <code>date</code>, <code>file_path</code>, <code>language</code>, <code>language_score</code></p> <p>Messy Variant Recipe (fineweb_cc_2024_10_sample):</p> <pre><code>operations:\n  - inject_nulls\n  - duplicate_rows\n  - type_drift\n  - truncate_lines\nnull_rate: 0.02\nduplicate_rate: 0.01\ntype_drift_rate: 0.005\ntruncate_rate: 0.003\nseed: 77777\n</code></pre>"},{"location":"source_tiers/#tier2tb-massive-2tb","title":"tier2tb (Massive - ~2TB)","text":"<p>Full production simulation tier with multiple FineWeb snapshots.</p> <p>Includes all <code>tier600gb</code> datasets plus:</p> Dataset Format Size Source <code>fineweb_cc_2024_06_sample</code> parquet ~500GB Hugging Face / FineWeb <code>fineweb_cc_2024_02_sample</code> parquet ~500GB Hugging Face / FineWeb <code>fineweb_cc_2023_10_sample</code> parquet ~500GB Hugging Face / FineWeb"},{"location":"source_tiers/#storage-budget","title":"Storage Budget","text":"Tier Raw Data Messy Variants Metadata Total tier0 1 GB 0 GB &lt;1 MB ~1 GB tier20gb 20 GB 0.5 GB &lt;1 MB ~21 GB tier100gb 100 GB 25 GB &lt;1 MB ~125 GB tier600gb 600 GB 100 GB &lt;1 MB ~700 GB tier2tb 2,000 GB 200 GB &lt;1 MB ~2.2 TB <p>Note: Messy variants are optional and require <code>--build-messy-variants</code> flag.</p>"},{"location":"source_tiers/#directory-layout","title":"Directory Layout","text":"<pre><code>.odibi/\n\u251c\u2500\u2500 source_cache/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 integrity_manifests.json\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 tiers/\n\u2502       \u251c\u2500\u2500 tier0/\n\u2502       \u2502   \u2514\u2500\u2500 sample_enwiki_pages/\n\u2502       \u2502       \u2514\u2500\u2500 raw/\n\u2502       \u2502           \u2514\u2500\u2500 simplewiki-latest-pages-articles.xml.bz2\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 tier20gb/\n\u2502       \u2502   \u251c\u2500\u2500 enwiki_abstract/\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 raw/\n\u2502       \u2502   \u2514\u2500\u2500 osm_liechtenstein/\n\u2502       \u2502       \u251c\u2500\u2500 raw/\n\u2502       \u2502       \u2514\u2500\u2500 messy/\n\u2502       \u2502           \u251c\u2500\u2500 liechtenstein-latest_messy.osm.pbf\n\u2502       \u2502           \u2514\u2500\u2500 transform_log.json\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 tier100gb/\n\u2502       \u2502   \u251c\u2500\u2500 enwiki_pages_articles/\n\u2502       \u2502   \u2514\u2500\u2500 osm_iceland/\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 tier600gb/\n\u2502       \u2502   \u251c\u2500\u2500 fineweb_cc_2024_10_sample/\n\u2502       \u2502   \u251c\u2500\u2500 osm_planet/\n\u2502       \u2502   \u2514\u2500\u2500 enwiki_pages_articles_full/\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 tier2tb/\n\u2502           \u2514\u2500\u2500 [all tier600gb + additional fineweb snapshots]\n\u2502\n\u2514\u2500\u2500 source_metadata/\n    \u251c\u2500\u2500 pool_index.yaml\n    \u2514\u2500\u2500 pools/\n        \u251c\u2500\u2500 tier0_sample_enwiki_pages.yaml\n        \u251c\u2500\u2500 tier20gb_enwiki_abstract.yaml\n        \u251c\u2500\u2500 tier20gb_osm_liechtenstein.yaml\n        \u251c\u2500\u2500 tier20gb_osm_liechtenstein_messy.yaml\n        \u2514\u2500\u2500 [...]\n</code></pre>"},{"location":"source_tiers/#usage","title":"Usage","text":""},{"location":"source_tiers/#list-available-tiers","title":"List Available Tiers","text":"<pre><code>python scripts/prepare_source_tiers.py --list-tiers\n</code></pre>"},{"location":"source_tiers/#dry-run-show-what-would-happen","title":"Dry Run (Show What Would Happen)","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier600gb --download --verify --dry-run\n</code></pre>"},{"location":"source_tiers/#download-and-verify","title":"Download and Verify","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier0 --download --verify\n</code></pre>"},{"location":"source_tiers/#build-messy-variants","title":"Build Messy Variants","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier100gb --build-messy-variants\n</code></pre>"},{"location":"source_tiers/#full-preparation","title":"Full Preparation","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier20gb --download --verify --build-messy-variants --emit-metadata\n</code></pre>"},{"location":"source_tiers/#resume-interrupted-download","title":"Resume Interrupted Download","text":"<p>Simply rerun the same command - downloads are resume-capable:</p> <pre><code>python scripts/prepare_source_tiers.py --tier tier600gb --download\n</code></pre>"},{"location":"source_tiers/#invariants","title":"Invariants","text":"<ol> <li>No Network During Cycles: Once prepared, tiers require no network access</li> <li>Hash Verification: All files verified against SHA256 manifests</li> <li>Atomic Writes: Temp files + atomic rename prevents corruption</li> <li>Deterministic Messy Variants: Fixed seed ensures reproducible transforms</li> <li>Frozen Pools: <code>lifecycle: FROZEN</code> in pool metadata</li> <li>No Agent Modifications: Agents MAY NOT modify tier data</li> </ol>"},{"location":"source_tiers/#integrity-verification","title":"Integrity Verification","text":"<p>All tier datasets are registered in <code>integrity_manifests.json</code>:</p> <pre><code>{\n  \"tier600gb_fineweb_cc_2024_10_sample\": {\n    \"algorithm\": \"sha256\",\n    \"file_hashes\": {\n      \"000_00000.parquet\": \"abc123...\"\n    },\n    \"manifest_hash\": \"def456...\",\n    \"frozen_at\": \"2025-12-14T10:00:00Z\",\n    \"frozen_by\": \"prepare_source_tiers.py\"\n  }\n}\n</code></pre> <p>To verify manually:</p> <pre><code>import hashlib\nfrom pathlib import Path\n\ndef verify_file(path: Path, expected_hash: str) -&gt; bool:\n    actual = hashlib.sha256(path.read_bytes()).hexdigest()\n    return actual == expected_hash\n</code></pre>"},{"location":"source_tiers/#messy-variant-transforms","title":"Messy Variant Transforms","text":"<p>Available transform operations:</p> Operation Description <code>inject_nulls</code> Replace random values with empty strings/null <code>duplicate_rows</code> Insert duplicate rows at random positions <code>type_drift</code> Append units to numeric values (e.g., \"100 units\") <code>truncate_lines</code> Randomly truncate CSV rows <code>invalid_timestamps</code> Replace dates with \"9999-99-99 00:00:00\" <p>All transforms are logged in <code>messy/transform_log.json</code>:</p> <pre><code>[\n  {\n    \"input\": \".../raw/data.csv\",\n    \"output\": \".../messy/data_messy.csv\",\n    \"seed\": 12345,\n    \"recipe\": {...},\n    \"stats\": {\n      \"nulls_injected\": 1234,\n      \"duplicates_added\": 567,\n      \"total_rows_input\": 100000,\n      \"total_rows_output\": 100567\n    },\n    \"timestamp\": \"2025-12-14T10:00:00Z\"\n  }\n]\n</code></pre>"},{"location":"source_tiers/#dataset-sources","title":"Dataset Sources","text":"Source URL License Wikimedia Foundation https://dumps.wikimedia.org/ CC-BY-SA 4.0 OpenStreetMap https://planet.openstreetmap.org/ ODbL 1.0 OpenStreetMap (Geofabrik) https://download.geofabrik.de/ ODbL 1.0 Hugging Face FineWeb https://huggingface.co/datasets/HuggingFaceFW/fineweb ODC-BY 1.0"},{"location":"source_tiers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"source_tiers/#download-interrupted","title":"Download Interrupted","text":"<p>Downloads are resume-capable. Simply rerun the command.</p>"},{"location":"source_tiers/#hash-mismatch","title":"Hash Mismatch","text":"<p>If a file fails verification:</p> <ol> <li>Delete the corrupted file</li> <li>Rerun with <code>--download --verify</code></li> </ol>"},{"location":"source_tiers/#disk-space","title":"Disk Space","text":"<p>Check available space before large tier downloads:</p> <pre><code># Windows\nwmic logicaldisk get size,freespace,caption\n\n# Linux/Mac\ndf -h\n</code></pre>"},{"location":"source_tiers/#permission-errors","title":"Permission Errors","text":"<p>Ensure write permissions to <code>.odibi/source_cache/tiers/</code>.</p>"},{"location":"source_tiers/#related-documentation","title":"Related Documentation","text":"<ul> <li>Source Pools Design</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Quick reference for diagnosing and fixing common Odibi issues.</p> <p>For beginners: Each error section includes: - \ud83d\udccb Exact error message - Copy-paste to search - \ud83d\udca1 What it means - Plain English explanation - \ud83d\udd0d Why it happened - Root cause - \u2705 Step-by-step fix - How to resolve it - \ud83d\udee1\ufe0f How to prevent it - Stop it from happening again - \ud83d\udcdd YAML before/after - Broken vs fixed config</p>"},{"location":"troubleshooting/#quick-diagnostic-steps","title":"Quick Diagnostic Steps","text":"<p>My pipeline failed, now what?</p> <ol> <li>Check the error message - Look for the specific error type (validation, engine, pattern)</li> <li>Check logs for context - Use <code>get_logging_context()</code> for structured logs</li> <li>Run with verbose logging: <code>python    import logging    logging.basicConfig(level=logging.DEBUG)</code></li> <li>Check data quality issues - Look for null keys, schema mismatches, FK violations</li> </ol>"},{"location":"troubleshooting/#common-errors-and-fixes","title":"Common Errors and Fixes","text":""},{"location":"troubleshooting/#import-and-installation-issues","title":"Import and Installation Issues","text":""},{"location":"troubleshooting/#module-not-found-errors","title":"Module Not Found Errors","text":"<pre><code>ModuleNotFoundError: No module named 'odibi'\n</code></pre> <p>Fix: Install odibi in your environment:</p> <pre><code>pip install -e .  # Development install\n# or\npip install odibi\n</code></pre>"},{"location":"troubleshooting/#python-39-type-hint-compatibility","title":"Python 3.9 Type Hint Compatibility","text":"<pre><code>TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\n</code></pre> <p>Cause: Code uses Python 3.10+ union syntax (<code>str | None</code>) on Python 3.9.</p> <p>Fix: Use typing module syntax:</p> <pre><code># Python 3.10+ (will fail on 3.9)\ndef func(param: str | None = None) -&gt; list[str]:\n    ...\n\n# Python 3.9 compatible\nfrom typing import Optional, List\ndef func(param: Optional[str] = None) -&gt; List[str]:\n    ...\n</code></pre>"},{"location":"troubleshooting/#engine-errors","title":"Engine Errors","text":""},{"location":"troubleshooting/#spark-python-version-mismatch","title":"Spark Python Version Mismatch","text":"<pre><code>PYTHON_VERSION_MISMATCH: Python in worker has different version (3, 8) than that in driver 3.9\n</code></pre> <p>Cause: Spark workers and driver use different Python versions.</p> <p>Fix: Set environment variables before starting Spark:</p> <pre><code>export PYSPARK_PYTHON=python3.9\nexport PYSPARK_DRIVER_PYTHON=python3.9\n</code></pre> <p>Or in Python:</p> <pre><code>import os\nos.environ['PYSPARK_PYTHON'] = 'python3.9'\nos.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n</code></pre>"},{"location":"troubleshooting/#pandaspolars-compatibility","title":"Pandas/Polars Compatibility","text":"<p>Pandas FutureWarning (fillna downcasting):</p> <pre><code>FutureWarning: Downcasting object dtype arrays on .fillna is deprecated\n</code></pre> <p>Fix: Chain <code>.infer_objects(copy=False)</code> after fillna:</p> <pre><code>df['column'].fillna(value).infer_objects(copy=False)\n</code></pre> <p>Polars API Changes:</p> <pre><code>DeprecationWarning: `columns` argument renamed to `on`\n</code></pre> <p>Fix: Use the new parameter name:</p> <pre><code>df.pivot(on=\"column\", ...)  # Not columns=\"column\"\n</code></pre>"},{"location":"troubleshooting/#delta-lake-issues","title":"Delta Lake Issues","text":""},{"location":"troubleshooting/#schema-mismatch-errors","title":"Schema Mismatch Errors","text":"<pre><code>Schema of data does not match table schema\n</code></pre> <p>Cause: DataFrame columns don't match the Delta table schema.</p> <p>Fix: Ensure column types match exactly:</p> <pre><code># Check schemas before writing\nprint(df.dtypes)\n# Cast columns if needed\ndf['column'] = df['column'].astype('string')\n</code></pre>"},{"location":"troubleshooting/#pyarrow-engine-limitations","title":"PyArrow Engine Limitations","text":"<pre><code>schema_mode 'merge' is not supported in pyarrow engine. Use engine=rust\n</code></pre> <p>Cause: The PyArrow engine doesn't support schema evolution with <code>schema_mode='merge'</code>.</p> <p>Fix: Either: 1. Use the Rust engine: <code>engine='rust'</code> 2. Remove <code>schema_mode='merge'</code> for append-only operations (schema is fixed at bootstrap)</p>"},{"location":"troubleshooting/#catalog-log_run-failures","title":"Catalog log_run Failures","text":"<p>If <code>log_run</code> fails with schema errors: 1. Schema is fixed at bootstrap time 2. Use exact column types that match the run log schema 3. Serialize complex types (like lists) to JSON strings</p>"},{"location":"troubleshooting/#validation-errors","title":"Validation Errors","text":""},{"location":"troubleshooting/#fk-validation-failures","title":"FK Validation Failures","text":"<pre><code>Foreign key validation failed: 3 orphan records found\n</code></pre> <p>Diagnosis:</p> <pre><code>result = validator.validate_foreign_key(df, 'fk_column', ref_df, 'pk_column')\nprint(result.orphan_records)  # See which records failed\n</code></pre> <p>Common causes: - Null FK values (decide: allow nulls or require matches) - Stale reference data - Case sensitivity mismatches</p>"},{"location":"troubleshooting/#quality-gate-blocks","title":"Quality Gate Blocks","text":"<pre><code>Quality gate blocked execution: data_quality_score &lt; 0.95\n</code></pre> <p>Diagnosis: Check which rules failed:</p> <pre><code>result = validator.run_all()\nfor check in result.failed_checks:\n    print(f\"{check.rule}: {check.message}\")\n</code></pre>"},{"location":"troubleshooting/#quarantine-issues","title":"Quarantine Issues","text":"<p>If records are unexpectedly quarantined: 1. Check quarantine rules configuration 2. Review quarantine output for specific failures 3. Verify data types match expected patterns</p>"},{"location":"troubleshooting/#pattern-specific-issues","title":"Pattern-Specific Issues","text":""},{"location":"troubleshooting/#dimension-pattern-unknown-member-concat-failures","title":"Dimension Pattern: Unknown Member Concat Failures","text":"<pre><code>ValueError: all the input array dimensions except for the concatenation axis must match exactly\n</code></pre> <p>Cause: Datetime columns have mismatched types when concatenating unknown member row with data.</p> <p>Fix (already applied in framework): Unknown member row columns are cast to match DataFrame dtypes. If you see this on an older version, upgrade.</p>"},{"location":"troubleshooting/#scd2-merge-key-issues","title":"SCD2: Merge Key Issues","text":"<pre><code>KeyError: 'merge_key' not found\n</code></pre> <p>Checklist: 1. Verify merge key column exists in source data 2. Check column name spelling/case sensitivity 3. Ensure key isn't being dropped by upstream transforms</p>"},{"location":"troubleshooting/#aggregation-null-handling","title":"Aggregation: Null Handling","text":"<pre><code>Cannot aggregate on null values\n</code></pre> <p>Fix: Handle nulls before aggregation:</p> <pre><code># Option 1: Filter nulls\ndf = df.dropna(subset=['group_column'])\n\n# Option 2: Replace nulls with placeholder\ndf['group_column'] = df['group_column'].fillna('UNKNOWN')\n</code></pre>"},{"location":"troubleshooting/#pipeline-configuration-errors","title":"Pipeline &amp; Configuration Errors","text":""},{"location":"troubleshooting/#cannot-resolve-column-name","title":"Cannot Resolve Column Name","text":"<p>\ud83d\udccb Error Message:</p> <pre><code>AnalysisException: Cannot resolve column name 'customer_id' among (CustomerID, Name, Email, Address)\n</code></pre> <p>\ud83d\udca1 What It Means: You're trying to use a column that doesn't exist in your DataFrame. The column names you specified don't match the actual column names in your data.</p> <p>\ud83d\udd0d Why It Happens: - Column names are case-sensitive (e.g., <code>customer_id</code> \u2260 <code>CustomerID</code>) - The source system changed column names - An upstream transformation renamed columns - You have a typo in your YAML</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Print actual column names: <code>python    df = spark.read.format(\"delta\").load(\"your/path\")    print(df.columns)    # Output: ['CustomerID', 'Name', 'Email', 'Address']</code></p> </li> <li> <p>Update YAML to match exactly:    ```yaml    # BEFORE (broken)    params:      keys: [\"customer_id\"]  # \u274c Wrong case</p> </li> </ol> <p># AFTER (fixed)    params:      keys: [\"CustomerID\"]   # \u2705 Matches actual column    ```</p> <p>\ud83d\udee1\ufe0f Prevention: Normalize column names to lowercase in Bronze/Silver:</p> <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        lowercase: true\n</code></pre>"},{"location":"troubleshooting/#column-not-found","title":"Column Not Found","text":"<p>\ud83d\udccb Error Message:</p> <pre><code>KeyError: 'effective_date'\n# or\nColumn 'effective_date' does not exist\n</code></pre> <p>\ud83d\udca1 What It Means: A column you referenced in your config doesn't exist in the DataFrame at that point in the pipeline.</p> <p>\ud83d\udd0d Why It Happens: - Column was dropped by an earlier transformation - Column was renamed upstream - Column is in the wrong format (e.g., expecting <code>effective_date</code> but source has <code>EffectiveDate</code>) - You're referencing a column before it's created</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Add a debug step to see columns at each stage:    ```yaml    nodes:</p> <ul> <li>name: \"debug_columns\"    depends_on: [\"previous_node\"]    transform:      steps:<ul> <li>sql: \"SELECT *, 'columns:' as debug FROM df LIMIT 1\"    # Check logs for actual columns    ```</li> </ul> </li> </ul> </li> <li> <p>Track column renames through your pipeline:    ```yaml    # Node 1: Rename columns</p> </li> <li>name: \"clean_data\"      transform:        steps:          - function: \"rename_columns\"            params:              columns:                EffectiveDate: \"effective_date\"  # Now it's lowercase</li> </ol> <p># Node 2: Use the NEW name    - name: \"process_data\"      depends_on: [\"clean_data\"]      params:        effective_time_col: \"effective_date\"  # \u2705 Use renamed column    ```</p> <p>\ud83d\udcdd YAML Before/After:</p> <pre><code># BEFORE (broken) - Column renamed but old name used\nnodes:\n  - name: \"prep\"\n    transform:\n      steps:\n        - function: \"rename_columns\"\n          params: { columns: { OldName: \"new_name\" } }\n\n  - name: \"process\"\n    depends_on: [\"prep\"]\n    params:\n      key_col: \"OldName\"  # \u274c Still using old name!\n\n# AFTER (fixed) - Use the new column name\nnodes:\n  - name: \"prep\"\n    transform:\n      steps:\n        - function: \"rename_columns\"\n          params: { columns: { OldName: \"new_name\" } }\n\n  - name: \"process\"\n    depends_on: [\"prep\"]\n    params:\n      key_col: \"new_name\"  # \u2705 Use the renamed column\n</code></pre>"},{"location":"troubleshooting/#unionbyname-failures","title":"unionByName Failures","text":"<p>\ud83d\udccb Error Message:</p> <pre><code>AnalysisException: Union can only be performed on tables with compatible column types.\nColumn customer_id is of type StringType in first table and IntegerType in second.\n# or\nCannot resolve column 'new_column' in the right table\n</code></pre> <p>\ud83d\udca1 What It Means: You're trying to combine (union) two DataFrames that have different schemas\u2014either column types don't match, or columns are missing.</p> <p>\ud83d\udd0d Why It Happens: - Source schema changed but target table has old schema - SCD2 target has different columns than current source - Appending data with different types (e.g., source sends <code>\"123\"</code> as string, target expects integer)</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li>Compare schemas:    ```python    source_df = spark.read.format(\"delta\").load(\"source/path\")    target_df = spark.read.format(\"delta\").load(\"target/path\")</li> </ol> <p>print(\"Source columns:\", source_df.dtypes)    print(\"Target columns:\", target_df.dtypes)    ```</p> <ol> <li> <p>Cast columns to match:    ```yaml    transform:      steps:</p> <ul> <li>function: \"cast_columns\"      params:        columns:          customer_id: \"integer\"  # Match target type          amount: \"double\"    ```</li> </ul> </li> <li> <p>Or enable schema merging: <code>yaml    write:      format: delta      delta_options:        mergeSchema: true</code></p> </li> </ol> <p>\ud83d\udcdd YAML Before/After:</p> <pre><code># BEFORE (broken) - Mismatched types\nnodes:\n  - name: \"load_new_data\"\n    read:\n      connection: landing\n      path: new_customers.csv  # customer_id is STRING in CSV\n\n    write:\n      connection: silver\n      table: dim_customers  # customer_id is INTEGER in target\n      mode: append  # \u274c Fails due to type mismatch\n\n# AFTER (fixed) - Cast types before writing\nnodes:\n  - name: \"load_new_data\"\n    read:\n      connection: landing\n      path: new_customers.csv\n\n    transform:\n      steps:\n        - function: \"cast_columns\"\n          params:\n            columns:\n              customer_id: \"integer\"  # \u2705 Match target type\n\n    write:\n      connection: silver\n      table: dim_customers\n      mode: append\n</code></pre>"},{"location":"troubleshooting/#spaces-in-column-names","title":"Spaces in Column Names","text":"<p>\ud83d\udccb Error Message:</p> <pre><code>AnalysisException: Syntax error in SQL: unexpected token 'Date'\n# or\nParseException: mismatched input 'Name' expecting &lt;EOF&gt;\n</code></pre> <p>\ud83d\udca1 What It Means: Your column names have spaces (e.g., <code>Customer Name</code>) which breaks SQL parsing.</p> <p>\ud83d\udd0d Why It Happens: - Source data came from Excel with friendly column headers - API returned columns with spaces - Someone created columns with spaces in the source system</p> <p>\u2705 Step-by-Step Fix:</p> <p>Option 1: Use backticks in SQL:</p> <pre><code>transform:\n  steps:\n    - sql: \"SELECT `Customer Name`, `Order Date`, `Total Amount` FROM df\"\n</code></pre> <p>Option 2: Rename columns (recommended):</p> <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        columns:\n          \"Customer Name\": \"customer_name\"\n          \"Order Date\": \"order_date\"\n          \"Total Amount\": \"total_amount\"\n</code></pre> <p>Option 3: Auto-normalize all columns:</p> <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        snake_case: true  # Converts \"Customer Name\" \u2192 \"customer_name\"\n</code></pre> <p>\ud83d\udcdd YAML Before/After:</p> <pre><code># BEFORE (broken) - Spaces in column names\nnodes:\n  - name: \"process_data\"\n    transform:\n      steps:\n        - sql: \"SELECT Customer Name, Order Date FROM df\"  # \u274c Syntax error!\n\n# AFTER (fixed) - Rename first\nnodes:\n  - name: \"process_data\"\n    transform:\n      steps:\n        - function: \"rename_columns\"\n          params:\n            snake_case: true\n        - sql: \"SELECT customer_name, order_date FROM df\"  # \u2705 Works now\n</code></pre>"},{"location":"troubleshooting/#connection-not-found","title":"Connection Not Found","text":"<p>\ud83d\udccb Error Message:</p> <pre><code>KeyError: Connection 'prod_warehouse' not found\n# or\nConnectionError: No connection named 'gold' is defined\n</code></pre> <p>\ud83d\udca1 What It Means: Your pipeline references a connection name that isn't defined in your project config.</p> <p>\ud83d\udd0d Why It Happens: - Connection name is misspelled - Connection is defined in a different config file - Environment-specific connection isn't loaded - Connection was renamed but references weren't updated</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Check available connections: <code>yaml    # In your odibi.yaml, list all connections:    connections:      bronze_storage:  # \u2190 These are your available names        type: azure_blob        ...      silver_storage:        type: azure_blob        ...</code></p> </li> <li> <p>Fix the reference:    ```yaml    # BEFORE (broken)    nodes:</p> <ul> <li>name: \"load_data\"    write:      connection: gold  # \u274c Not defined in connections!</li> </ul> </li> </ol> <p># AFTER (fixed)    nodes:      - name: \"load_data\"        write:          connection: silver_storage  # \u2705 Matches defined connection    ```</p> <p>\ud83d\udee1\ufe0f Prevention: Use a consistent naming convention:</p> <pre><code>connections:\n  landing:   # Source data\n    ...\n  bronze:    # Raw layer\n    ...\n  silver:    # Cleaned layer\n    ...\n  gold:      # Business layer\n    ...\n</code></pre>"},{"location":"troubleshooting/#delta-table-version-conflicts","title":"Delta Table Version Conflicts","text":"<p>\ud83d\udccb Error Message:</p> <pre><code>ConcurrentAppendException: Files were added to the root of the table by a concurrent update.\n# or\nConcurrentDeleteReadException: This transaction attempted to read files that were deleted by a concurrent commit.\n</code></pre> <p>\ud83d\udca1 What It Means: Multiple processes tried to write to the same Delta table at the same time, causing a conflict.</p> <p>\ud83d\udd0d Why It Happens: - Two pipeline runs overlap (same table, same time) - Parallel nodes trying to write to the same table - Streaming job and batch job writing to same table - Previous job didn't complete before retry started</p> <p>\u2705 Step-by-Step Fix:</p> <p>Option 1: Add retry with backoff:</p> <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n  initial_delay: 5  # seconds\n</code></pre> <p>Option 2: Use merge instead of append (for some cases):</p> <pre><code># Merge is idempotent - safe to retry\ntransformer: \"merge\"\nparams:\n  target: \"silver.dim_customers\"\n  keys: [\"customer_id\"]\n</code></pre> <p>Option 3: Ensure serial execution:</p> <pre><code># In your orchestrator (Airflow, Databricks Workflows):\n# - Don't allow concurrent runs of same pipeline\n# - Or partition writes by date\n</code></pre> <p>\ud83d\udee1\ufe0f Prevention: - Use unique write paths for parallel jobs - Configure orchestrator to prevent overlapping runs - Use Delta Lake isolation levels appropriately</p>"},{"location":"troubleshooting/#common-odibi-configuration-errors","title":"Common Odibi Configuration Errors","text":"<p>These are the most frequent errors beginners encounter when configuring Odibi pipelines. Don't panic\u2014each one has a straightforward fix.</p>"},{"location":"troubleshooting/#schema-mismatch-column-not-found-in-dataframe","title":"Schema Mismatch: Column Not Found in DataFrame","text":"<p>Error message:</p> <pre><code>AnalysisException: Cannot resolve column name 'customer_id' among [CustomerID, order_date, amount]\n</code></pre> <p>What it means: You referenced a column name in your YAML config that doesn't exist in the actual DataFrame.</p> <p>Why it happened: - Typo in the column name - Wrong case (column names are case-sensitive) - Column was renamed or dropped in an upstream step</p> <p>Step-by-step fix:</p> <ol> <li>Check the exact column names in your DataFrame:    <code>python    print(df.columns)  # Pandas/Polars    df.printSchema()   # Spark</code></li> <li>Compare with what you have in your YAML</li> <li>Update the YAML to match the exact column name (including case)</li> </ol> <p>YAML before (broken):</p> <pre><code>pattern: dimension\nconfig:\n  natural_key: customer_id  # Wrong case!\n  columns:\n    - customer_id\n    - name\n</code></pre> <p>YAML after (fixed):</p> <pre><code>pattern: dimension\nconfig:\n  natural_key: CustomerID  # Matches DataFrame exactly\n  columns:\n    - CustomerID\n    - name\n</code></pre> <p>How to prevent it next time: - Always print <code>df.columns</code> before writing your YAML - Use consistent naming conventions (snake_case recommended) - Add a schema validation step at pipeline start</p>"},{"location":"troubleshooting/#column-not-found-in-pattern-config","title":"Column Not Found in Pattern Config","text":"<p>Error message:</p> <pre><code>KeyError: 'customer_id'\n</code></pre> <p>or</p> <pre><code>Column 'customer_id' not found in DataFrame\n</code></pre> <p>What it means: A column specified in your pattern configuration doesn't exist in the data.</p> <p>Why it happened: - Column name mismatch (typo or case difference) - Column was renamed in a previous transform - Column exists in source but not in transformed data</p> <p>Step-by-step fix:</p> <ol> <li>Identify which config field is causing the error (the traceback usually shows this)</li> <li>Print your DataFrame columns at the point of failure</li> <li>Match your config to the actual column names</li> </ol> <p>YAML before (broken):</p> <pre><code>pattern: fact\nconfig:\n  grain:\n    - order_id\n    - line_item\n  measures:\n    - qty      # Wrong! Column is actually 'quantity'\n    - amount\n</code></pre> <p>YAML after (fixed):</p> <pre><code>pattern: fact\nconfig:\n  grain:\n    - order_id\n    - line_item\n  measures:\n    - quantity  # Matches DataFrame column\n    - amount\n</code></pre> <p>How to prevent it next time: - Document expected column names in comments - Use a pre-flight check that validates all columns exist before processing</p>"},{"location":"troubleshooting/#unionbyname-failures-scd2-targetsource-mismatch","title":"UnionByName Failures (SCD2 Target/Source Mismatch)","text":"<p>Error message:</p> <pre><code>AnalysisException: Cannot resolve column name 'is_current' among [customer_id, name, effective_date]\n</code></pre> <p>or</p> <pre><code>ValueError: Cannot union DataFrames with different columns\n</code></pre> <p>What it means: When merging source data with an existing target table (common in SCD2), the schemas don't match. The target has columns the source doesn't have.</p> <p>Why it happened: - Target table has SCD2-specific columns (<code>is_current</code>, <code>effective_from</code>, <code>effective_to</code>, <code>row_hash</code>) - Source data doesn't include these columns (and shouldn't\u2014the pattern adds them) - Previous schema changes weren't migrated properly</p> <p>Step-by-step fix:</p> <ol> <li>Let the SCD2 pattern add the tracking columns\u2014don't add them to source</li> <li>If manually fixing, ensure both schemas match:    ```python    # Check target schema    target_df.printSchema()</li> </ol> <p># Check what SCD2 expects to add    # is_current, effective_from, effective_to, row_hash    ``` 3. If target has extra columns, either:    - Add them to source with null values    - Rebuild target with correct schema</p> <p>YAML before (broken):</p> <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: load_date\n  # Source has: customer_id, name, load_date\n  # Target has: customer_id, name, load_date, is_current, effective_from, effective_to, row_hash\n  # MISMATCH! But this is expected - SCD2 adds those columns\n</code></pre> <p>YAML after (fixed):</p> <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: load_date\n  # Let the pattern handle the SCD2 columns automatically\n  # Don't pre-add them to your source data\n</code></pre> <p>How to prevent it next time: - Never manually add SCD2 tracking columns to source data - When bootstrapping, let the pattern create the initial schema - Document which columns are managed by the pattern</p>"},{"location":"troubleshooting/#spaces-in-column-names_1","title":"Spaces in Column Names","text":"<p>Error message:</p> <pre><code>AnalysisException: Column name 'Customer Name' cannot be resolved\n</code></pre> <p>or</p> <pre><code>KeyError: 'Customer Name'\n</code></pre> <p>What it means: Your column names contain spaces, which cause parsing issues.</p> <p>Why it happened: - Data imported from Excel with human-readable headers - Source system uses spaces in column names - CSV headers weren't cleaned before loading</p> <p>Step-by-step fix:</p> <ol> <li>Option A: Rename columns in source (recommended):    ```python    # Pandas    df.columns = df.columns.str.replace(' ', '_')</li> </ol> <p># Spark    for col in df.columns:        df = df.withColumnRenamed(col, col.replace(' ', '_'))</p> <p># Polars    df = df.rename({col: col.replace(' ', '_') for col in df.columns})    ```</p> <ol> <li>Option B: Use backticks in YAML (Spark only):    ```yaml    columns:<ul> <li>\"<code>Customer Name</code>\"    ```</li> </ul> </li> </ol> <p>YAML before (broken):</p> <pre><code>pattern: dimension\nconfig:\n  natural_key: Customer ID  # Space causes issues!\n  columns:\n    - Customer ID\n    - Customer Name\n    - Email Address\n</code></pre> <p>YAML after (fixed):</p> <pre><code>pattern: dimension\nconfig:\n  natural_key: customer_id  # Clean snake_case\n  columns:\n    - customer_id\n    - customer_name\n    - email_address\n</code></pre> <p>Pre-processing step to add:</p> <pre><code># Add this before any pattern processing\ndf.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n</code></pre> <p>How to prevent it next time: - Always clean column names at the start of your pipeline - Establish a naming convention (snake_case is standard) - Add a column name validator to your ingestion layer</p>"},{"location":"troubleshooting/#scd2-effective_time_col-errors","title":"SCD2 effective_time_col Errors","text":"<p>Error message:</p> <pre><code>KeyError: 'effective_time_col'\n</code></pre> <p>or</p> <pre><code>Column 'txn_date' not found in DataFrame\n</code></pre> <p>What it means: The column you specified as <code>effective_time_col</code> doesn't exist in your source data at the point where SCD2 needs it.</p> <p>Why it happened: - Column was renamed in an earlier transform step - Column name has a typo - Column was dropped before reaching SCD2 pattern - You're referencing a derived column that doesn't exist yet</p> <p>Step-by-step fix:</p> <ol> <li>Verify the column exists in your source data:    <code>python    print('txn_date' in df.columns)  # Should be True</code></li> <li>Check if any transform renamed or dropped it</li> <li>Update the config to use the correct column name</li> </ol> <p>YAML before (broken):</p> <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: txn_date  # Oops! Column was renamed to 'transaction_date'\n</code></pre> <p>YAML after (fixed):</p> <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: transaction_date  # Matches actual column name\n</code></pre> <p>Common gotcha: The <code>effective_time_col</code> must exist in your source DataFrame. It gets used to set <code>effective_from</code> dates and may be dropped after processing.</p> <p>How to prevent it next time: - Print <code>df.columns</code> right before the SCD2 pattern runs - Keep your transformation pipeline documented - Use meaningful, consistent column names throughout</p>"},{"location":"troubleshooting/#connection-not-found_1","title":"Connection Not Found","text":"<p>Error message:</p> <pre><code>ConnectionError: Connection 'warehouse' not found in project config\n</code></pre> <p>or</p> <pre><code>KeyError: 'warehouse'\n</code></pre> <p>What it means: You referenced a connection name that isn't defined in your project configuration.</p> <p>Why it happened: - Connection not defined in project config - Typo in connection name - Connection section missing entirely - Environment-specific config not loaded</p> <p>Step-by-step fix:</p> <ol> <li>Check your project config file structure</li> <li>Add or fix the connections section</li> <li>Ensure connection name matches exactly (case-sensitive)</li> </ol> <p>YAML before (broken):</p> <pre><code># pipeline.yaml\nsources:\n  - name: customers\n    connection: Warehouse  # Wrong case!\n    table: dim_customer\n</code></pre> <p>YAML after (fixed):</p> <pre><code># project_config.yaml - Must have connections defined\nconnections:\n  warehouse:  # lowercase to match\n    type: databricks\n    catalog: main\n    schema: gold\n\n# pipeline.yaml\nsources:\n  - name: customers\n    connection: warehouse  # Matches connection name exactly\n    table: dim_customer\n</code></pre> <p>How to prevent it next time: - Use lowercase connection names consistently - Keep a template project config with all required sections - Validate config on pipeline startup</p>"},{"location":"troubleshooting/#delta-table-version-conflicts_1","title":"Delta Table Version Conflicts","text":"<p>Error message:</p> <pre><code>ConcurrentModificationException: Conflicting commits\n</code></pre> <p>or</p> <pre><code>DeltaTableVersionMismatch: Expected version X but found version Y\n</code></pre> <p>or</p> <pre><code>ConcurrentAppendException: Files were added by a concurrent update\n</code></pre> <p>What it means: Multiple processes tried to write to the same Delta table simultaneously, or your reference to the table is stale.</p> <p>Why it happened: - Two pipelines writing to the same table at the same time - Long-running transaction conflicted with another write - Cached table reference is outdated - Overwrite operation conflicted with append</p> <p>Step-by-step fix:</p> <ol> <li> <p>Identify the conflict source: <code>python    # Check Delta table history    from delta.tables import DeltaTable    dt = DeltaTable.forPath(spark, \"path/to/table\")    dt.history().show()</code></p> </li> <li> <p>Use merge instead of overwrite:    ```python    # Instead of overwrite (can conflict)    df.write.format(\"delta\").mode(\"overwrite\").save(path)</p> </li> </ol> <p># Use merge (handles concurrency better)    delta_table.alias(\"target\").merge(        df.alias(\"source\"),        \"target.id = source.id\"    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()    ```</p> <ol> <li>Add retry logic:    ```python    from tenacity import retry, stop_after_attempt, wait_exponential</li> </ol> <p>@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))    def safe_write(df, path):        df.write.format(\"delta\").mode(\"append\").save(path)    ```</p> <p>YAML before (problematic):</p> <pre><code>pattern: merge\nconfig:\n  write_mode: overwrite  # Can cause conflicts with concurrent writes\n</code></pre> <p>YAML after (safer):</p> <pre><code>pattern: merge\nconfig:\n  write_mode: merge  # Handles concurrent operations better\n  merge_keys:\n    - customer_id\n</code></pre> <p>How to prevent it next time: - Avoid concurrent writes to the same table - Use merge patterns instead of overwrite when possible - Implement job orchestration to serialize conflicting writes - Enable Delta Lake optimistic concurrency settings</p>"},{"location":"troubleshooting/#azure-specific-troubleshooting","title":"Azure-Specific Troubleshooting","text":""},{"location":"troubleshooting/#adls-authentication","title":"ADLS Authentication","text":""},{"location":"troubleshooting/#credential-issues","title":"Credential Issues","text":"<pre><code>AuthenticationError: Invalid credentials\n</code></pre> <p>Checklist: 1. Verify service principal credentials are correct 2. Check tenant ID, client ID, client secret 3. Ensure service principal has Storage Blob Data Contributor role</p> <pre><code>from azure.identity import DefaultAzureCredential\ncredential = DefaultAzureCredential()\n# If using service principal:\nfrom azure.identity import ClientSecretCredential\ncredential = ClientSecretCredential(tenant_id, client_id, client_secret)\n</code></pre>"},{"location":"troubleshooting/#access-token-expiry","title":"Access Token Expiry","text":"<pre><code>TokenExpiredError: Token has expired\n</code></pre> <p>Fix: Use <code>DefaultAzureCredential</code> which handles token refresh automatically.</p>"},{"location":"troubleshooting/#delta-table-errors","title":"Delta Table Errors","text":""},{"location":"troubleshooting/#storage-throttling-429-errors","title":"Storage Throttling (429 Errors)","text":"<pre><code>TooManyRequests: Rate limit exceeded\n</code></pre> <p>Fixes: 1. Implement retry logic with exponential backoff 2. Reduce concurrent operations 3. Batch smaller writes</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, max=60))\ndef write_with_retry(df, path):\n    df.write.format(\"delta\").save(path)\n</code></pre>"},{"location":"troubleshooting/#concurrent-write-conflicts","title":"Concurrent Write Conflicts","text":"<pre><code>ConcurrentAppendException: Files were added by a concurrent update\n</code></pre> <p>Fixes: 1. Enable optimistic concurrency: Set <code>delta.enableChangeDataFeed = true</code> 2. Use merge instead of overwrite when possible 3. Coordinate write operations to avoid conflicts</p>"},{"location":"troubleshooting/#file-locking-issues","title":"File Locking Issues","text":"<p>If writes hang or fail with lock errors: 1. Check for stale lock files in <code>_delta_log/</code> 2. Wait for other operations to complete 3. Consider using a single writer pattern</p>"},{"location":"troubleshooting/#azure-sql-issues","title":"Azure SQL Issues","text":""},{"location":"troubleshooting/#odbc-driver-setup","title":"ODBC Driver Setup","text":"<pre><code>Error: [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 17 for SQL Server'\n</code></pre> <p>Fix (Ubuntu/WSL):</p> <pre><code>curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/20.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql17\n</code></pre>"},{"location":"troubleshooting/#connection-timeout","title":"Connection Timeout","text":"<pre><code>OperationalError: Connection timed out\n</code></pre> <p>Fix: Increase connection timeout:</p> <pre><code>from sqlalchemy import create_engine\nengine = create_engine(\n    connection_string,\n    connect_args={\"timeout\": 60}\n)\n</code></pre>"},{"location":"troubleshooting/#sqlalchemy-configuration","title":"SQLAlchemy Configuration","text":"<pre><code># Full connection string example\nconnection_string = (\n    \"mssql+pyodbc://user:password@server.database.windows.net:1433/\"\n    \"database?driver=ODBC+Driver+17+for+SQL+Server&amp;Encrypt=yes&amp;TrustServerCertificate=no\"\n)\n</code></pre>"},{"location":"troubleshooting/#wsllinux-setup-issues","title":"WSL/Linux Setup Issues","text":""},{"location":"troubleshooting/#python-command-not-found","title":"Python Command Not Found","text":"<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'python'\n</code></pre> <p>Fix: Either create a symlink or install the package:</p> <pre><code># Option 1: Install symlink package\nsudo apt install python-is-python3\n\n# Option 2: Use python3.9 explicitly\npython3.9 -m pytest tests/\n</code></pre>"},{"location":"troubleshooting/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code>ModuleNotFoundError: No module named 'sqlalchemy'\n</code></pre> <p>Fix:</p> <pre><code>pip3.9 install sqlalchemy pyodbc\n</code></pre>"},{"location":"troubleshooting/#spark-workers-python-version-mismatch","title":"Spark Workers Python Version Mismatch","text":"<p>Ensure all workers use the same Python:</p> <pre><code># Add to ~/.bashrc or set before running Spark\nexport PYSPARK_PYTHON=/usr/bin/python3.9\nexport PYSPARK_DRIVER_PYTHON=/usr/bin/python3.9\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#how-to-report-bugs","title":"How to Report Bugs","text":"<ol> <li>Check if it's already documented in BUGS.md</li> <li>Check GAPS.md for known limitations</li> <li>Create a GitHub issue with:</li> <li>Odibi version</li> <li>Python version</li> <li>Engine (Pandas/Spark/Polars)</li> <li>Full error message and traceback</li> <li>Minimal reproducible example</li> </ol>"},{"location":"troubleshooting/#required-info-for-bug-reports","title":"Required Info for Bug Reports","text":"<pre><code>**Environment:**\n- Odibi version: X.X.X\n- Python version: 3.X\n- OS: Windows/Linux/WSL\n- Engine: Pandas/Spark/Polars\n\n**Error:**\n[Paste full traceback]\n\n**Reproduction:**\n[Minimal code to reproduce]\n\n**Expected behavior:**\n[What you expected to happen]\n</code></pre>"},{"location":"troubleshooting/#links","title":"Links","text":"<ul> <li>BUGS.md - Fixed bugs and workarounds</li> <li>GAPS.md - Known gaps and roadmap</li> <li>GitHub Issues - Report new issues</li> </ul>"},{"location":"troubleshooting/#learning-resources","title":"Learning Resources","text":"<p>New to Odibi or data engineering? Start here:</p> <ul> <li>Data Engineering 101 - Complete beginner's guide</li> <li>Glossary - Every term explained simply</li> <li>4-Week Curriculum - Structured learning path</li> <li>Anti-Patterns Guide - What NOT to do</li> <li>SCD2 Pattern - History tracking with troubleshooting</li> </ul>"},{"location":"api/context/","title":"Context API","text":"<p>The Context API provides access to DataFrames, engine-specific features, and execution state within transforms.</p>"},{"location":"api/context/#overview","title":"Overview","text":"<p>Every transform function receives a <code>context</code> parameter:</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef my_transform(context, current, param1: str):\n    # context: ExecutionContext with DataFrame access\n    # current: The input DataFrame for this node\n    return current\n</code></pre>"},{"location":"api/context/#context-classes","title":"Context Classes","text":"<p>Odibi has engine-specific context implementations:</p> Engine Context Class Key Features Pandas <code>PandasContext</code> In-memory DataFrames Polars <code>PolarsContext</code> Lazy/eager DataFrames Spark <code>SparkContext</code> Distributed DataFrames, SQL <p>All contexts implement the base <code>Context</code> interface.</p>"},{"location":"api/context/#core-methods","title":"Core Methods","text":""},{"location":"api/context/#getname","title":"get(name)","text":"<p>Retrieve a DataFrame by node name:</p> <pre><code>@transform\ndef join_data(context, current):\n    # Get another node's output\n    customers = context.get(\"load_customers\")\n    return current.join(customers, on=\"customer_id\")\n</code></pre>"},{"location":"api/context/#setname-df","title":"set(name, df)","text":"<p>Register a DataFrame for downstream nodes:</p> <pre><code>@transform\ndef split_data(context, current):\n    valid = current.filter(\"is_valid = true\")\n    invalid = current.filter(\"is_valid = false\")\n\n    # Register additional output\n    context.set(\"invalid_records\", invalid)\n\n    return valid  # Primary output\n</code></pre>"},{"location":"api/context/#sqlquery","title":"sql(query)","text":"<p>Execute SQL against registered DataFrames (Spark only):</p> <pre><code>@transform\ndef sql_transform(context, current):\n    # Register current DataFrame as a view\n    context.set(\"input_data\", current)\n\n    # Execute SQL\n    result = context.sql(\"\"\"\n        SELECT customer_id, SUM(amount) as total\n        FROM input_data\n        GROUP BY customer_id\n    \"\"\")\n    return result\n</code></pre>"},{"location":"api/context/#engine-context","title":"Engine Context","text":"<p>Access engine-specific features via <code>engine_context</code>:</p>"},{"location":"api/context/#spark","title":"Spark","text":"<pre><code>@transform\ndef spark_specific(context, current):\n    spark = context.engine_context.spark\n\n    # Use Spark session directly\n    df = spark.read.parquet(\"/path/to/data\")\n\n    # Access catalog\n    spark.catalog.listTables()\n\n    return current\n</code></pre>"},{"location":"api/context/#pandas","title":"Pandas","text":"<pre><code>@transform\ndef pandas_specific(context, current):\n    # current is already a pd.DataFrame\n    # No special engine context needed\n    return current.groupby(\"category\").sum()\n</code></pre>"},{"location":"api/context/#polars","title":"Polars","text":"<pre><code>@transform\ndef polars_specific(context, current):\n    # current is a polars DataFrame\n    import polars as pl\n\n    return current.with_columns(\n        pl.col(\"amount\").sum().over(\"category\").alias(\"category_total\")\n    )\n</code></pre>"},{"location":"api/context/#enginecontext-class","title":"EngineContext Class","text":"<p>The <code>EngineContext</code> provides engine metadata:</p> <pre><code>class EngineContext:\n    engine_type: str          # \"pandas\", \"polars\", \"spark\"\n    spark: SparkSession       # Only for Spark engine\n\n    @property\n    def is_spark(self) -&gt; bool: ...\n\n    @property\n    def is_pandas(self) -&gt; bool: ...\n\n    @property\n    def is_polars(self) -&gt; bool: ...\n</code></pre>"},{"location":"api/context/#example-engine-agnostic-transform","title":"Example: Engine-Agnostic Transform","text":"<p>Write transforms that work on all engines:</p> <pre><code>@transform\ndef engine_agnostic(context, current, threshold: float = 100):\n    engine = context.engine_context\n\n    if engine.is_spark:\n        from pyspark.sql import functions as F\n        return current.filter(F.col(\"amount\") &gt; threshold)\n\n    elif engine.is_polars:\n        import polars as pl\n        return current.filter(pl.col(\"amount\") &gt; threshold)\n\n    else:  # pandas\n        return current[current[\"amount\"] &gt; threshold]\n</code></pre>"},{"location":"api/context/#available-in-context","title":"Available in Context","text":"Property Description <code>engine_context</code> Engine-specific context with <code>spark</code>, <code>engine_type</code> <code>config</code> Current node configuration <code>connections</code> Connection registry <code>state_manager</code> Access to HWM and run state"},{"location":"api/context/#related","title":"Related","text":"<ul> <li>Writing Transformations \u2014 Transform authoring guide</li> </ul>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/","title":"\ud83c\udfd7\ufe0f Odibi 2.1 Architecture: Unified Catalog &amp; Pattern Engine","text":"<p>Status: Blueprint Target Version: Odibi 2.1 Philosophy: \"Intent-Based Engineering\" backed by a Stateful Catalog.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#1-executive-summary","title":"1. Executive Summary","text":""},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#the-vision","title":"The Vision","text":"<p>Odibi 2.1 unifies the System Catalog (the \"Brain\") with a Pattern-Driven Execution Engine (the \"Intent\"). Instead of just executing tasks, Odibi now understands the semantics of the data (e.g., \"This is a Fact Table\", \"This is an SCD2 Dimension\") and enforces the appropriate behavior, lineage, and quality checks automatically.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#the-core-shift","title":"The Core Shift","text":"Feature Odibi 1.0 (Task Runner) Odibi 2.1 (Data Platform) Configuration <code>transformer: scd2</code> (Implementation Detail) <code>pattern: scd2</code> (Business Intent) State Local Files / Stateless System Catalog (Delta Tables) Validation Row-level checks (<code>email != null</code>) History-Aware &amp; Pattern-Aware checks Lineage Implicit / Text Logs Queryable Meta-Tables Semantics Buried in SQL strings Metric Registry (First-class citizen)"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#2-the-system-catalog-the-brain","title":"2. The System Catalog (The Brain)","text":"<p>A set of Delta Tables auto-bootstrapped in <code>_odibi_system/</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#1-meta_tables-inventory","title":"1. <code>meta_tables</code> (Inventory)","text":"<p>Tracks physical assets. *   <code>project_name</code>: STRING (Partition) *   <code>table_name</code>: STRING (Logical Name, e.g., \"gold.orders\") *   <code>path</code>: STRING (Physical Location) *   <code>format</code>: STRING *   <code>pattern_type</code>: STRING (e.g., \"scd2\", \"merge\") *   <code>schema_hash</code>: STRING (Drift Detection) *   <code>updated_at</code>: TIMESTAMP</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#2-meta_runs-observability","title":"2. <code>meta_runs</code> (Observability)","text":"<p>Tracks execution history. *   <code>run_id</code>: STRING *   <code>pipeline_name</code>: STRING *   <code>node_name</code>: STRING *   <code>status</code>: STRING *   <code>rows_processed</code>: LONG *   <code>duration_ms</code>: LONG *   <code>metrics_json</code>: STRING (JSON stats) *   <code>timestamp</code>: TIMESTAMP</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#3-meta_patterns-governance","title":"3. <code>meta_patterns</code> (Governance)","text":"<p>Tracks pattern compliance. *   <code>table_name</code>: STRING *   <code>pattern_type</code>: STRING *   <code>configuration</code>: STRING (JSON: params used) *   <code>compliance_score</code>: DOUBLE (0.0 - 1.0)     *   Example: An SCD2 table missing a <code>valid_to</code> column gets a score of 0.5.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#4-meta_metrics-semantics","title":"4. <code>meta_metrics</code> (Semantics)","text":"<p>Tracks business logic. *   <code>metric_name</code>: STRING *   <code>definition_sql</code>: STRING *   <code>dimensions</code>: ARRAY *   <code>source_table</code>: STRING"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#5-meta_state-checkpoints","title":"5. <code>meta_state</code> (Checkpoints)","text":"<p>Tracks incremental progress. *   <code>pipeline_name</code>: STRING *   <code>node_name</code>: STRING *   <code>hwm_value</code>: STRING</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#3-pattern-driven-execution-the-intent","title":"3. Pattern-Driven Execution (The Intent)","text":"<p>Users declare the Pattern, and Odibi configures the implementation.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#supported-patterns","title":"Supported Patterns","text":"<ol> <li><code>scd2</code>: Slowly Changing Dimension (History tracking).</li> <li><code>merge</code>: Smart Upsert (Conditional updates).</li> </ol> <p>Note: For simple append or overwrite operations, use <code>write.mode: append</code> or <code>write.mode: overwrite</code> directly\u2014no pattern needed.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#yaml-example","title":"YAML Example","text":"<pre><code>- name: \"dim_customers\"\n  pattern: \"scd2\"  # &lt;--- The Intent\n  params:\n    keys: [\"customer_id\"]\n    time_col: \"updated_at\"\n\n  # Odibi Automatically:\n  # 1. Selects the SCD2 Transformer.\n  # 2. Configures the 'Merge' writer mode.\n  # 3. Validates output has 'is_current' and 'valid_to' columns.\n  # 4. Registers 'scd2' pattern in meta_tables.\n</code></pre>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#4-phased-implementation-roadmap","title":"4. Phased Implementation Roadmap","text":""},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-1-the-foundation-catalog-schema","title":"Phase 1: The Foundation (Catalog &amp; Schema)","text":"<p>Goal: Build the \"Brain\" that holds state. *   1.1 Catalog Manager: Implement <code>odibi/catalog.py</code>. Logic to bootstrap the 5 meta-tables. *   1.2 Config Update: Add <code>SystemConfig</code> (<code>connection</code>, <code>path</code>) to <code>odibi.yaml</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-2-the-wiring-engine-integration","title":"Phase 2: The Wiring (Engine Integration)","text":"<p>Goal: Connect the \"Muscle\" to the \"Brain\". *   2.1 Auto-Registration: <code>Engine.write()</code> upserts to <code>meta_tables</code> and <code>meta_patterns</code>. *   2.2 Smart Read: <code>Engine.read()</code> resolves logical names (<code>gold.orders</code>) via <code>meta_tables</code>. *   2.3 Telemetry: <code>Node.execute()</code> flushes stats to <code>meta_runs</code>. *   2.4 State Migration: <code>StateManager</code> reads/writes to <code>meta_state</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-3-the-pattern-engine","title":"Phase 3: The Pattern Engine","text":"<p>Goal: Implement \"Intent-Based\" Logic. *   3.1 Pattern Module: Create <code>odibi/patterns/</code>. Implement classes for <code>SCD2</code>, <code>Fact</code>, <code>Snapshot</code>. *   3.2 Execution Router: Update <code>Node</code> to detect <code>pattern:</code> config. If present, delegate to Pattern Class instead of generic Transformer. *   3.3 Smart Merge: Implement conditional logic (<code>whenMatched...</code>) within the Merge pattern.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-4-intelligence-guardrails","title":"Phase 4: Intelligence (Guardrails)","text":"<p>Goal: Use Catalog data for smart checks. *   4.1 History-Aware Validation: <code>Validator</code> queries <code>meta_runs</code> for anomalies (Volume Drops). *   4.2 Pattern Validation: Pattern classes enforce schema rules (e.g., \"Fact table cannot have duplicates\"). *   4.3 Semantic Module: Parse <code>semantic_model</code>, register to <code>meta_metrics</code>, and implement <code>odibi export-views</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-5-observability","title":"Phase 5: Observability","text":"<p>Goal: Visualization. *   5.1 Dashboard: Standard Databricks Notebook to query <code>meta_runs</code> and show platform health.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#5-migration-strategy","title":"5. Migration Strategy","text":"<ol> <li>Config: Users add <code>system</code> block to <code>odibi.yaml</code>.</li> <li>Bootstrap: First run creates <code>_odibi_system/</code> tables.</li> <li>Adoption: Users gradually switch from <code>transformer: scd2</code> to <code>pattern: scd2</code> (backward compatible).</li> </ol>"},{"location":"architecture/delete-detection-design/","title":"Delete Detection Implementation Design","text":"<p>Status: Design Complete Target: Silver layer CDC-like correctness for non-CDC sources</p>"},{"location":"architecture/delete-detection-design/#1-overview","title":"1. Overview","text":"<p>Delete detection identifies records that existed in a previous extraction but no longer exist, enabling CDC-like behavior for sources without native Change Data Capture.</p> <pre><code>Bronze (append-only) \u2192 Silver (dedupe + delete detection) \u2192 Gold (clean KPIs)\n</code></pre>"},{"location":"architecture/delete-detection-design/#2-delete-detection-modes","title":"2. Delete Detection Modes","text":"Mode How it works Use when <code>none</code> Pass-through (no detection) Immutable facts (logs, events, sensors) <code>snapshot_diff</code> Compare Delta version N vs N-1 keys Dimensions, staging tables, RPA sources <code>sql_compare</code> LEFT ANTI JOIN Silver keys against live source SQL is authoritative &amp; always reachable"},{"location":"architecture/delete-detection-design/#mode-none-default","title":"Mode: <code>none</code> (Default)","text":"<p>No delete detection. Use for append-only facts.</p> <pre><code>transform:\n  steps:\n    - operation: deduplicate\n      params:\n        keys: [event_id]\n        order_by: _extracted_at DESC\n    # detect_deletes omitted = none\n</code></pre>"},{"location":"architecture/delete-detection-design/#mode-snapshot_diff","title":"Mode: <code>snapshot_diff</code>","text":"<p>Compares current Delta version to previous version. Keys missing = deleted.</p> <p>Important: This mode only works correctly with full snapshot extracts, not HWM incremental. Use <code>sql_compare</code> if you're doing HWM ingestion.</p> <pre><code>transform:\n  steps:\n    - operation: deduplicate\n      params:\n        keys: [customer_id]\n        order_by: _extracted_at DESC\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [customer_id]\n</code></pre> <p>Logic:</p> <pre><code>prev_keys = Delta version N-1, SELECT DISTINCT keys\ncurr_keys = Delta version N, SELECT DISTINCT keys\ndeleted   = prev_keys EXCEPT curr_keys\n\u2192 Flag with _is_deleted = true (soft) or remove rows (hard)\n</code></pre> <p>Implementation: Uses Delta time travel (works in both Spark and Pandas via <code>deltalake</code> library).</p>"},{"location":"architecture/delete-detection-design/#mode-sql_compare","title":"Mode: <code>sql_compare</code>","text":"<p>Queries live source to find keys that no longer exist. Recommended for HWM ingestion when source is authoritative and reachable.</p> <pre><code>transform:\n  steps:\n    - operation: deduplicate\n      params:\n        keys: [customer_id]\n        order_by: _extracted_at DESC\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n</code></pre> <p>Logic:</p> <pre><code>silver_keys = SELECT DISTINCT keys FROM silver\nsource_keys = SELECT DISTINCT keys FROM live_source\ndeleted     = silver_keys EXCEPT source_keys\n\u2192 Flag with _is_deleted = true (soft) or remove rows (hard)\n</code></pre> <p>Warning: Never use <code>sql_compare</code> on staging tables - they may be empty between loads, causing false deletes.</p>"},{"location":"architecture/delete-detection-design/#3-config-model","title":"3. Config Model","text":"<pre><code>from enum import Enum\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass DeleteDetectionMode(str, Enum):\n    NONE = \"none\"\n    SNAPSHOT_DIFF = \"snapshot_diff\"\n    SQL_COMPARE = \"sql_compare\"\n\n\nclass DeleteDetectionConfig(BaseModel):\n    \"\"\"\n    Configuration for delete detection in Silver layer.\n\n    Example (snapshot_diff):\n    ```yaml\n    detect_deletes:\n      mode: snapshot_diff\n      keys: [customer_id]\n      soft_delete_col: _is_deleted\n    ```\n\n    Example (sql_compare):\n    ```yaml\n    detect_deletes:\n      mode: sql_compare\n      keys: [customer_id]\n      source_connection: azure_sql\n      source_table: dbo.Customers\n    ```\n    \"\"\"\n\n    mode: DeleteDetectionMode = Field(\n        default=DeleteDetectionMode.NONE,\n        description=\"Delete detection strategy: none, snapshot_diff, sql_compare\"\n    )\n\n    keys: List[str] = Field(\n        default_factory=list,\n        description=\"Business key columns for comparison\"\n    )\n\n    # Soft vs Hard delete\n    soft_delete_col: Optional[str] = Field(\n        default=\"_is_deleted\",\n        description=\"Column to flag deletes. Set to null for hard-delete.\"\n    )\n\n    # sql_compare mode options\n    source_connection: Optional[str] = Field(\n        default=None,\n        description=\"For sql_compare: connection name to query source\"\n    )\n    source_table: Optional[str] = Field(\n        default=None,\n        description=\"For sql_compare: table to query\"\n    )\n    source_query: Optional[str] = Field(\n        default=None,\n        description=\"For sql_compare: custom SQL query (overrides source_table)\"\n    )\n\n    # Fallback for non-Delta sources (rare)\n    snapshot_column: Optional[str] = Field(\n        default=None,\n        description=\"For snapshot_diff on non-Delta: column to identify snapshots. \"\n                    \"If None, uses Delta version (default).\"\n    )\n\n    # Edge case handling\n    on_first_run: Literal[\"skip\", \"error\"] = Field(\n        default=\"skip\",\n        description=\"Behavior when no previous version exists for snapshot_diff\"\n    )\n\n    max_delete_percent: Optional[float] = Field(\n        default=50.0,\n        description=\"Safety threshold: warn/error if more than X% of rows would be deleted\"\n    )\n\n    on_threshold_breach: Literal[\"warn\", \"error\", \"skip\"] = Field(\n        default=\"warn\",\n        description=\"Behavior when delete percentage exceeds max_delete_percent\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_mode_requirements(self):\n        if self.mode == DeleteDetectionMode.NONE:\n            return self\n\n        if not self.keys:\n            raise ValueError(\n                f\"delete_detection: 'keys' required for mode='{self.mode}'\"\n            )\n\n        if self.mode == DeleteDetectionMode.SQL_COMPARE:\n            if not self.source_connection:\n                raise ValueError(\n                    \"delete_detection: 'source_connection' required for sql_compare\"\n                )\n            if not self.source_table and not self.source_query:\n                raise ValueError(\n                    \"delete_detection: 'source_table' or 'source_query' required\"\n                )\n\n        return self\n</code></pre>"},{"location":"architecture/delete-detection-design/#4-transformer-interface","title":"4. Transformer Interface","text":"<pre><code>@transform(\"detect_deletes\", category=\"transformer\", param_model=DeleteDetectionConfig)\ndef detect_deletes(context: EngineContext, params: DeleteDetectionConfig) -&gt; EngineContext:\n    \"\"\"\n    Detects deleted records based on configured mode.\n\n    Returns:\n    - soft_delete_col set: Adds boolean column (True = deleted)\n    - soft_delete_col = None: Removes deleted rows (hard delete)\n    \"\"\"\n    if params.mode == DeleteDetectionMode.NONE:\n        return context  # Pass-through\n\n    if params.mode == DeleteDetectionMode.SNAPSHOT_DIFF:\n        return _detect_deletes_snapshot_diff(context, params)\n\n    if params.mode == DeleteDetectionMode.SQL_COMPARE:\n        return _detect_deletes_sql_compare(context, params)\n\n    raise ValueError(f\"Unknown delete detection mode: {params.mode}\")\n</code></pre>"},{"location":"architecture/delete-detection-design/#snapshot_diff-implementation","title":"snapshot_diff Implementation","text":"<pre><code>def _detect_deletes_snapshot_diff(\n    context: EngineContext,\n    params: DeleteDetectionConfig\n) -&gt; EngineContext:\n    \"\"\"\n    Compare current Delta version to previous version.\n    Keys in previous but not in current = deleted.\n    \"\"\"\n    keys = params.keys\n\n    if context.engine_type == EngineType.SPARK:\n        # Spark: Delta time travel\n        current_version = context.delta_version\n        prev_version = current_version - 1\n\n        curr_keys = context.df.select(keys).distinct()\n        prev_keys = (\n            context.spark.read\n            .format(\"delta\")\n            .option(\"versionAsOf\", prev_version)\n            .load(context.table_path)\n            .select(keys)\n            .distinct()\n        )\n\n        deleted_keys = prev_keys.exceptAll(curr_keys)\n\n    else:\n        # Pandas: deltalake time travel\n        from deltalake import DeltaTable\n\n        dt = DeltaTable(context.table_path)\n        current_version = dt.version()\n        prev_version = current_version - 1\n\n        curr_keys = context.df[keys].drop_duplicates()\n        prev_df = DeltaTable(context.table_path, version=prev_version).to_pandas()\n        prev_keys = prev_df[keys].drop_duplicates()\n\n        # Use merge with indicator to find deleted\n        merged = prev_keys.merge(curr_keys, on=keys, how=\"left\", indicator=True)\n        deleted_keys = merged[merged[\"_merge\"] == \"left_only\"][keys]\n\n    return _apply_deletes(context, deleted_keys, params)\n\n\ndef _apply_deletes(\n    context: EngineContext,\n    deleted_keys,\n    params: DeleteDetectionConfig\n) -&gt; EngineContext:\n    \"\"\"Apply soft or hard delete based on config.\"\"\"\n    keys = params.keys\n\n    if params.soft_delete_col:\n        # Soft delete: add flag column\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql.functions import lit, col, when\n\n            deleted_keys_flagged = deleted_keys.withColumn(\n                params.soft_delete_col, lit(True)\n            )\n            result = context.df.join(\n                deleted_keys_flagged,\n                on=keys,\n                how=\"left\"\n            ).withColumn(\n                params.soft_delete_col,\n                when(col(params.soft_delete_col).isNull(), False)\n                .otherwise(True)\n            )\n        else:\n            # Pandas\n            df = context.df.copy()\n            deleted_keys[params.soft_delete_col] = True\n            df = df.merge(deleted_keys, on=keys, how=\"left\")\n            df[params.soft_delete_col] = df[params.soft_delete_col].fillna(False)\n            result = df\n\n        return context.with_df(result)\n\n    else:\n        # Hard delete: remove rows\n        if context.engine_type == EngineType.SPARK:\n            result = context.df.join(deleted_keys, on=keys, how=\"left_anti\")\n        else:\n            df = context.df.copy()\n            merged = df.merge(deleted_keys, on=keys, how=\"left\", indicator=True)\n            result = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n        return context.with_df(result)\n</code></pre>"},{"location":"architecture/delete-detection-design/#sql_compare-implementation","title":"sql_compare Implementation","text":"<pre><code>def _detect_deletes_sql_compare(\n    context: EngineContext,\n    params: DeleteDetectionConfig\n) -&gt; EngineContext:\n    \"\"\"\n    Compare Silver keys against live source.\n    Keys in Silver but not in source = deleted.\n    \"\"\"\n    from odibi.connections import get_connection\n\n    keys = params.keys\n    conn = get_connection(params.source_connection)\n\n    # Build source keys query\n    if params.source_query:\n        source_keys_query = params.source_query\n    else:\n        key_cols = \", \".join(keys)\n        source_keys_query = f\"SELECT DISTINCT {key_cols} FROM {params.source_table}\"\n\n    if context.engine_type == EngineType.SPARK:\n        source_keys = (\n            context.spark.read\n            .format(\"jdbc\")\n            .option(\"url\", conn.jdbc_url)\n            .option(\"query\", source_keys_query)\n            .load()\n        )\n        silver_keys = context.df.select(keys).distinct()\n        deleted_keys = silver_keys.exceptAll(source_keys)\n\n    else:\n        # Pandas\n        import pandas as pd\n\n        source_keys = pd.read_sql(source_keys_query, conn.engine)\n        silver_keys = context.df[keys].drop_duplicates()\n\n        merged = silver_keys.merge(source_keys, on=keys, how=\"left\", indicator=True)\n        deleted_keys = merged[merged[\"_merge\"] == \"left_only\"][keys]\n\n    return _apply_deletes(context, deleted_keys, params)\n</code></pre>"},{"location":"architecture/delete-detection-design/#5-engine-parity","title":"5. Engine Parity","text":"<p>Both Pandas and Spark support all operations:</p> Operation Spark Pandas Delta time travel <code>versionAsOf</code> option <code>DeltaTable(..., version=N)</code> EXCEPT ALL Native DataFrame <code>merge</code> + indicator LEFT ANTI JOIN Native DataFrame <code>merge</code> + filter JDBC read Native SQLAlchemy <code>pd.read_sql</code>"},{"location":"architecture/delete-detection-design/#6-bronze-metadata-columns","title":"6. Bronze Metadata Columns","text":"<p>Bronze writes should include metadata for lineage and debugging. Use <code>add_metadata: true</code> for all applicable columns, or specify individual ones.</p>"},{"location":"architecture/delete-detection-design/#available-metadata-columns","title":"Available Metadata Columns","text":"Column Description Applies to <code>_extracted_at</code> Pipeline execution timestamp All sources <code>_source_file</code> Source filename/path File sources (CSV, Parquet, JSON) <code>_source_connection</code> Connection name used All sources <code>_source_table</code> Table or query name SQL sources"},{"location":"architecture/delete-detection-design/#metadata-config-model","title":"Metadata Config Model","text":"<pre><code>class WriteMetadataConfig(BaseModel):\n    \"\"\"Metadata columns to add during Bronze writes.\"\"\"\n\n    extracted_at: bool = Field(default=True)\n    source_file: bool = Field(default=True)\n    source_connection: bool = Field(default=False)\n    source_table: bool = Field(default=False)\n</code></pre>"},{"location":"architecture/delete-detection-design/#yaml-examples","title":"YAML Examples","text":"<p>Add all applicable metadata (recommended):</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n</code></pre> <p>Specify individual columns:</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: true\n</code></pre>"},{"location":"architecture/delete-detection-design/#implementation-notes","title":"Implementation Notes","text":"<p>Spark:</p> <pre><code>from pyspark.sql.functions import input_file_name, current_timestamp, lit\n\ndf = df.withColumn(\"_extracted_at\", current_timestamp())\ndf = df.withColumn(\"_source_file\", input_file_name())  # file sources only\ndf = df.withColumn(\"_source_connection\", lit(connection_name))\n</code></pre> <p>Pandas:</p> <pre><code>df[\"_extracted_at\"] = pd.Timestamp.now()\ndf[\"_source_file\"] = source_path  # tracked during file read\ndf[\"_source_connection\"] = connection_name\n</code></pre>"},{"location":"architecture/delete-detection-design/#7-hash-optimization-for-merge","title":"7. Hash Optimization for Merge","text":"<p>For large tables, use hashes to optimize change detection. Use the existing <code>generate_surrogate_key</code> transformer twice:</p> <pre><code>transform:\n  steps:\n    # REQUIRED: Deduplication\n    - operation: deduplicate\n      params:\n        keys: [customer_id]\n        order_by: _extracted_at DESC\n\n    # Hash for join key (optional - useful for composite keys)\n    - operation: generate_surrogate_key\n      params:\n        columns: [customer_id]\n        output_col: _hash_key\n\n    # Hash for change detection\n    - operation: generate_surrogate_key\n      params:\n        columns: [name, email, address, phone, status]  # non-key columns\n        output_col: _hash_diff\n\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n\nwrite:\n  connection: silver\n  table: customers\n  mode: upsert\n  keys: [customer_id]\n  update_condition: \"source._hash_diff != target._hash_diff\"  # skip unchanged rows\n</code></pre> <p>Benefits: - Skip unchanged rows during merge (performance) - Single-column comparison vs N columns - Deterministic across runs</p>"},{"location":"architecture/delete-detection-design/#8-full-yaml-examples","title":"8. Full YAML Examples","text":""},{"location":"architecture/delete-detection-design/#example-1-dimension-with-snapshot_diff-full-snapshot-only","title":"Example 1: Dimension with snapshot_diff (Full Snapshot Only)","text":"<p>Note: Only use <code>snapshot_diff</code> with full snapshot ingestion, not HWM.</p> <pre><code>nodes:\n  - name: bronze_customers\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.Customers\n      # No incremental = full snapshot\n    write:\n      connection: bronze\n      table: customers\n      mode: append\n      add_metadata: true\n\n  - name: silver_customers\n    read:\n      connection: bronze\n      table: customers\n    transform:\n      steps:\n        - operation: deduplicate\n          params:\n            keys: [customer_id]\n            order_by: _extracted_at DESC\n        - operation: detect_deletes\n          params:\n            mode: snapshot_diff\n            keys: [customer_id]\n            soft_delete_col: _is_deleted\n    write:\n      connection: silver\n      table: customers\n      mode: upsert\n      keys: [customer_id]\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-2-dimension-with-sql_compare-recommended-for-hwm","title":"Example 2: Dimension with sql_compare (Recommended for HWM)","text":"<p>Recommended: Use <code>sql_compare</code> when source is authoritative and reachable.</p> <pre><code>nodes:\n  - name: bronze_products\n    read:\n      connection: erp_sql\n      format: sql\n      table: dbo.Products\n      incremental:\n        mode: stateful\n        column: updated_at\n        watermark_lag: 2h\n    write:\n      connection: bronze\n      table: products\n      mode: append\n      add_metadata: true\n\n  - name: silver_products\n    read:\n      connection: bronze\n      table: products\n    transform:\n      steps:\n        - operation: deduplicate\n          params:\n            keys: [product_id]\n            order_by: _extracted_at DESC\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [product_id]\n            source_connection: erp_sql\n            source_table: dbo.Products\n            soft_delete_col: _is_deleted\n    write:\n      connection: silver\n      table: products\n      mode: upsert\n      keys: [product_id]\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-3-fact-table-no-delete-detection","title":"Example 3: Fact table (no delete detection)","text":"<pre><code>nodes:\n  - name: silver_orders\n    read:\n      connection: bronze\n      table: orders\n    transform:\n      steps:\n        - operation: deduplicate\n          params:\n            keys: [order_id]\n            order_by: _extracted_at DESC\n        # No detect_deletes - facts are immutable\n    write:\n      connection: silver\n      table: orders\n      mode: upsert\n      keys: [order_id]\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-4-hard-delete-instead-of-soft-delete","title":"Example 4: Hard delete instead of soft delete","text":"<pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n        soft_delete_col: null  # removes rows instead of flagging\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-5-conservative-threshold-for-critical-dimension","title":"Example 5: Conservative threshold for critical dimension","text":"<pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: erp\n        source_table: dbo.Customers\n        max_delete_percent: 20.0    # fail if &gt;20% would be deleted\n        on_threshold_breach: error  # stop pipeline\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-6-first-run-handling","title":"Example 6: First run handling","text":"<pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [product_id]\n        on_first_run: skip  # default: no deletes on initial load\n</code></pre>"},{"location":"architecture/delete-detection-design/#9-edge-case-handling","title":"9. Edge Case Handling","text":"Scenario Default Behavior Config Override First run (no previous version) Skip detection <code>on_first_run: error</code> &gt;50% rows deleted Warn and continue <code>max_delete_percent</code>, <code>on_threshold_breach</code> Source returns 0 rows Triggers threshold warning Set <code>max_delete_percent: 0</code> to disable Connection failure Pipeline fails Handle via standard error handling"},{"location":"architecture/delete-detection-design/#threshold-calculation","title":"Threshold Calculation","text":"<pre><code>delete_percent = (deleted_count / total_silver_rows) * 100\n\nif delete_percent &gt; params.max_delete_percent:\n    if params.on_threshold_breach == \"error\":\n        raise DeleteThresholdExceeded(f\"{delete_percent:.1f}% exceeds {params.max_delete_percent}%\")\n    elif params.on_threshold_breach == \"warn\":\n        logger.warning(f\"Delete detection: {delete_percent:.1f}% of rows flagged for deletion\")\n    elif params.on_threshold_breach == \"skip\":\n        logger.info(\"Delete threshold exceeded, skipping delete detection\")\n        return context  # no changes\n</code></pre>"},{"location":"architecture/delete-detection-design/#10-design-decisions-summary","title":"10. Design Decisions Summary","text":"Decision Choice Rationale Default delete behavior Soft delete (<code>_is_deleted</code>) Audit trail, undo capability State for snapshot_diff Delta time travel No extra columns needed, works in Pandas + Spark batch_id column Not needed Delta versioning handles this _snapshot_date column Not needed Over-engineering for edge cases Version picker for snapshot_diff Not supported Always compare N to N-1, correct semantics Bundled SilverTransformConfig Not implemented Keep transformers separate for flexibility source_flagged mode Not implemented Use existing filter/rename transforms Hash for merge Use <code>generate_surrogate_key</code> twice No new transformer needed First run behavior Skip by default No deletes to detect on initial load Delete threshold 50% default, warn Safety net for staging table accidents Bronze metadata Flexible config <code>true</code> for all, or specify individual columns"},{"location":"architecture/delete-detection-design/#11-implementation-checklist","title":"11. Implementation Checklist","text":""},{"location":"architecture/delete-detection-design/#phase-1-config-models","title":"Phase 1: Config Models","text":"<ul> <li>[ ] Add <code>DeleteDetectionMode</code> enum to <code>odibi/config.py</code></li> <li>[ ] Add <code>DeleteDetectionConfig</code> model to <code>odibi/config.py</code></li> <li>[ ] Add <code>WriteMetadataConfig</code> model to <code>odibi/config.py</code></li> <li>[ ] Update <code>WriteConfig</code> to accept <code>add_metadata: bool | WriteMetadataConfig</code></li> </ul>"},{"location":"architecture/delete-detection-design/#phase-2-delete-detection-transformer","title":"Phase 2: Delete Detection Transformer","text":"<ul> <li>[ ] Create <code>odibi/transformers/delete_detection.py</code></li> <li>[ ] Implement <code>detect_deletes</code> transformer function</li> <li>[ ] Implement <code>_detect_deletes_snapshot_diff</code> for Spark</li> <li>[ ] Implement <code>_detect_deletes_snapshot_diff</code> for Pandas</li> <li>[ ] Implement <code>_detect_deletes_sql_compare</code> for Spark</li> <li>[ ] Implement <code>_detect_deletes_sql_compare</code> for Pandas</li> <li>[ ] Implement <code>_apply_deletes</code> helper (soft/hard delete logic)</li> <li>[ ] Implement threshold check logic</li> <li>[ ] Implement first-run detection (no previous version)</li> <li>[ ] Register transformer in <code>odibi/transformers/__init__.py</code></li> </ul>"},{"location":"architecture/delete-detection-design/#phase-3-bronze-metadata","title":"Phase 3: Bronze Metadata","text":"<ul> <li>[ ] Add <code>add_metadata</code> support to write path (Spark engine)</li> <li>[ ] Add <code>add_metadata</code> support to write path (Pandas engine)</li> <li>[ ] Implement <code>_extracted_at</code> column addition</li> <li>[ ] Implement <code>_source_file</code> column addition (file sources)</li> <li>[ ] Implement <code>_source_connection</code> column addition</li> <li>[ ] Implement <code>_source_table</code> column addition (SQL sources)</li> </ul>"},{"location":"architecture/delete-detection-design/#phase-4-testing","title":"Phase 4: Testing","text":"<ul> <li>[ ] Write unit tests for <code>DeleteDetectionConfig</code> validation</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: none</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: snapshot_diff (Spark)</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: snapshot_diff (Pandas)</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: sql_compare (Spark)</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: sql_compare (Pandas)</li> <li>[ ] Write unit tests for edge cases (first run, threshold breach)</li> <li>[ ] Write unit tests for soft delete vs hard delete</li> <li>[ ] Write unit tests for metadata columns</li> </ul>"},{"location":"architecture/delete-detection-design/#phase-5-introspect-documentation","title":"Phase 5: Introspect &amp; Documentation","text":"<ul> <li>[ ] Update <code>odibi/introspect.py</code> GROUP_MAPPING with new configs</li> <li>[ ] Update <code>odibi/introspect.py</code> TRANSFORM_CATEGORY_MAP for delete_detection</li> <li>[ ] Update <code>odibi/introspect.py</code> CUSTOM_ORDER for new configs</li> <li>[ ] Ensure <code>DeleteDetectionConfig</code> has top-tier docstrings with examples (like MergeParams)</li> <li>[ ] Ensure <code>WriteMetadataConfig</code> has comprehensive docstrings</li> <li>[ ] Add YAML examples in docstrings following existing patterns</li> <li>[ ] Update <code>docs/reference/yaml_schema.md</code></li> <li>[ ] Update <code>docs/architecture/ingestion-correctness.md</code> (done)</li> <li>[ ] Add cookbook examples</li> </ul>"},{"location":"architecture/delete-detection-design/#12-docstring-standards-for-introspect","title":"12. Docstring Standards (for Introspect)","text":"<p>All new config models must follow the existing docstring pattern for auto-generated docs:</p> <pre><code>class DeleteDetectionConfig(BaseModel):\n    \"\"\"\n    Configuration for delete detection in Silver layer.\n\n    ### \ud83d\udd0d \"CDC Without CDC\" Guide\n\n    **Business Problem:**\n    \"Records are deleted in our Azure SQL source, but our Silver tables still show them.\"\n\n    **The Solution:**\n    Use delete detection to identify and flag records that no longer exist in the source.\n\n    **Recipe 1: SQL Compare (Recommended for HWM)**\n    ```yaml\n    transform:\n      steps:\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [customer_id]\n            source_connection: azure_sql\n            source_table: dbo.Customers\n    ```\n\n    **Recipe 2: Snapshot Diff (For Staging Tables)**\n    ```yaml\n    transform:\n      steps:\n        - operation: detect_deletes\n          params:\n            mode: snapshot_diff\n            keys: [customer_id]\n    ```\n\n    **Recipe 3: Conservative Threshold**\n    ```yaml\n    transform:\n      steps:\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [customer_id]\n            source_connection: erp\n            source_table: dbo.Customers\n            max_delete_percent: 20.0\n            on_threshold_breach: error\n    ```\n    \"\"\"\n</code></pre> <p>This ensures the introspect tool generates high-quality, example-rich documentation.</p>"},{"location":"architecture/ingestion-correctness/","title":"Ingestion Correctness for Non-CDC Sources","text":"<p>Target Audience: Data engineers implementing pipelines against Azure SQL Database or other sources without native Change Data Capture (CDC).</p> <p>This document defines the architecture for achieving CDC-like correctness without real CDC, covering Bronze/Silver/Gold layer invariants, deduplication, and delete detection patterns.</p>"},{"location":"architecture/ingestion-correctness/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Invariants</li> <li>Bronze Layer: Append-Only Raw Truth</li> <li>Silver Layer: Dedupe + Delete Detection</li> <li>Delete Detection Patterns</li> <li>HWM vs Snapshot: Decision Guide</li> <li>YAML Configuration Reference</li> <li>Quick Reference Table</li> <li>Common Pitfalls</li> </ol>"},{"location":"architecture/ingestion-correctness/#core-invariants","title":"Core Invariants","text":"<pre><code>Bronze = append-only raw truth\nSilver = dedupe + clean + optional delete-detection\nGold   = KPIs (computed from clean Silver)\n</code></pre> <pre><code>flowchart TB\n    subgraph Source[\"Source (Azure SQL)\"]\n        SQL[(Azure SQL DB)]\n    end\n\n    subgraph Bronze[\"Bronze Layer\"]\n        B1[Append-Only Raw Truth]\n        B2[Every extraction is a snapshot]\n        B3[Never mutate, never delete]\n    end\n\n    subgraph Silver[\"Silver Layer\"]\n        S1[Dedupe: Latest per Business Key]\n        S2[Delete Detection: Optional]\n        S3[Clean + Typed + Validated]\n    end\n\n    subgraph Gold[\"Gold Layer\"]\n        G1[KPIs / Aggregates]\n    end\n\n    SQL --&gt;|HWM or Snapshot| B1\n    B1 --&gt; S1\n    S1 --&gt; G1\n</code></pre>"},{"location":"architecture/ingestion-correctness/#bronze-layer-append-only-raw-truth","title":"Bronze Layer: Append-Only Raw Truth","text":""},{"location":"architecture/ingestion-correctness/#the-rule","title":"The Rule","text":"<pre><code>Bronze = append(raw_row + _extracted_at + metadata)\n</code></pre> <p>Never dedupe in Bronze. Never delete in Bronze.</p>"},{"location":"architecture/ingestion-correctness/#what-goes-wrong-without-append-only-bronze","title":"What Goes Wrong Without Append-Only Bronze?","text":"Problem Consequence Overwrites destroy lineage You can't answer \"what did the source say on Tuesday?\" Reprocessing is impossible If Silver logic is wrong, you can't replay from Bronze Late-arriving data is lost If you overwrite with \"current\", late rows vanish Audit trail breaks Compliance (GDPR, SOX) requires provenance"},{"location":"architecture/ingestion-correctness/#bronze-yaml-example","title":"Bronze YAML Example","text":"<pre><code>nodes:\n  - name: bronze_customers\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.Customers\n      incremental:\n        mode: stateful\n        column: updated_at\n        watermark_lag: 2h\n    write:\n      connection: bronze\n      table: customers\n      mode: append              # ALWAYS append for Bronze\n      add_metadata: true        # adds _extracted_at, _source_file, etc.\n</code></pre>"},{"location":"architecture/ingestion-correctness/#silver-layer-dedupe-delete-detection","title":"Silver Layer: Dedupe + Delete Detection","text":""},{"location":"architecture/ingestion-correctness/#dedupe-is-required","title":"Dedupe is REQUIRED","text":"<p>Every Silver table must declare: - <code>keys</code>: business key columns - <code>order_by</code>: how to pick the \"latest\" (typically <code>_extracted_at DESC</code>)</p>"},{"location":"architecture/ingestion-correctness/#what-goes-wrong-without-dedupe","title":"What Goes Wrong Without Dedupe?","text":"Scenario Problem HWM overlap Watermark lag pulls same row twice \u2192 duplicates in Silver Snapshot re-extraction Same row appears in batch N and batch N+1 \u2192 duplicates Source system backfills Row updated retroactively \u2192 old and new version both exist Retry after partial failure Same batch re-ingested \u2192 duplicates"},{"location":"architecture/ingestion-correctness/#dedupe-logic","title":"Dedupe Logic","text":"<pre><code>SELECT * EXCEPT(_rn) FROM (\n    SELECT *,\n           ROW_NUMBER() OVER (\n               PARTITION BY {business_key}\n               ORDER BY _extracted_at DESC, updated_at DESC\n           ) as _rn\n    FROM bronze\n) WHERE _rn = 1\n</code></pre>"},{"location":"architecture/ingestion-correctness/#delete-detection-patterns","title":"Delete Detection Patterns","text":"<p>Delete detection is optional and has three modes:</p> <pre><code>flowchart LR\n    subgraph Modes[\"Delete Detection Modes\"]\n        direction TB\n        NONE[\"none&lt;br/&gt;(ignore deletes)\"]\n        SNAP[\"snapshot_diff&lt;br/&gt;(compare Bronze snapshots)\"]\n        SQL[\"sql_compare&lt;br/&gt;(query source directly)\"]\n    end\n\n    subgraph When[\"When to Use\"]\n        direction TB\n        W1[\"Append-only facts&lt;br/&gt;(events, logs)\"]\n        W2[\"Dimensions with soft-delete&lt;br/&gt;or staging/RPA systems\"]\n        W3[\"SQL is authoritative&lt;br/&gt;and always reachable\"]\n    end\n\n    NONE --&gt; W1\n    SNAP --&gt; W2\n    SQL --&gt; W3\n</code></pre>"},{"location":"architecture/ingestion-correctness/#mode-none-default-for-facts","title":"Mode: <code>none</code> (Default for Facts)","text":"<p>Use when: Immutable events (logs, transactions, sensor readings)</p> <pre><code>transform:\n  - dedupe:\n      keys: [event_id]\n      order_by: _extracted_at DESC\n  # delete_detection omitted = none\n</code></pre> <p>Why no delete detection for facts? - Facts don't delete by nature - Wastes compute comparing millions of events - Risk of accidentally soft-deleting valid events</p>"},{"location":"architecture/ingestion-correctness/#mode-snapshot_diff-compare-bronze-snapshots","title":"Mode: <code>snapshot_diff</code> (Compare Bronze Snapshots)","text":"<p>Use when: - Dimensions (customers, products) - Staging/RPA tables where SQL is NOT authoritative - Source system doesn't support point-in-time queries</p> <p>Logic:</p> <pre><code>deleted_keys = (keys in previous_snapshot) - (keys in current_snapshot)\n</code></pre> <p>Implementation Concept:</p> <pre><code># Uses Delta time travel (no batch_id column needed)\nprev = delta_table.version(N-1).select(keys).distinct()\ncurr = delta_table.version(N).select(keys).distinct()\ndeleted = prev.exceptAll(curr)\n</code></pre> <p>What goes wrong if we compare to SQL instead? - If SQL is a staging table, it may be empty between loads \u2192 false deletes - If SQL has different retention than Bronze \u2192 false deletes - If SQL is behind a firewall/VPN during processing \u2192 failures</p> <p>What goes wrong if we push this to Bronze? - Bronze becomes stateful (needs to know previous batch) - Bronze loses append-only guarantee - Can't replay Bronze independently</p>"},{"location":"architecture/ingestion-correctness/#mode-sql_compare-query-source-directly","title":"Mode: <code>sql_compare</code> (Query Source Directly)","text":"<p>Use when: - SQL is the authoritative source of truth - You need real-time delete detection - SQL is always reachable during Silver processing</p> <p>Logic:</p> <pre><code>-- Keys in Silver but NOT in SQL = deleted\nSELECT s.business_key\nFROM silver s\nLEFT ANTI JOIN sql_source ON s.business_key = sql_source.business_key\n</code></pre> <p>What goes wrong if we compare too early (in Bronze)? - Bronze becomes coupled to SQL availability - Can't process Bronze offline - Latency spikes if SQL is slow</p> <p>What goes wrong if we compare too late (after Gold)? - KPIs already computed with ghost records - Downstream dashboards show incorrect data - Audit logs contaminated</p> <p>What goes wrong if SQL is NOT authoritative? - Staging tables empty between loads \u2192 everything looks deleted - ERP snapshots \u2192 previous day's data appears deleted - RPA extracts \u2192 intermittent failures look like deletes</p>"},{"location":"architecture/ingestion-correctness/#hwm-vs-snapshot-decision-guide","title":"HWM vs Snapshot: Decision Guide","text":"<pre><code>flowchart TD\n    START[\"Does the source table&lt;br/&gt;support UPDATE timestamps?\"]\n\n    START --&gt;|Yes| HAS_UPDATE[\"Is updated_at reliable&lt;br/&gt;(always updated on change)?\"]\n    START --&gt;|No| SNAPSHOT[\"Use SNAPSHOT&lt;br/&gt;(full extract + snapshot_diff)\"]\n\n    HAS_UPDATE --&gt;|Yes| DELETES[\"Do you care about&lt;br/&gt;hard deletes in source?\"]\n    HAS_UPDATE --&gt;|No| SNAPSHOT\n\n    DELETES --&gt;|No| HWM[\"Use HWM&lt;br/&gt;(incremental is enough)\"]\n    DELETES --&gt;|Yes| AUTH[\"Is SQL authoritative&lt;br/&gt;(not a staging table)?\"]\n\n    AUTH --&gt;|Yes| SQL_COMPARE[\"Use HWM + sql_compare\"]\n    AUTH --&gt;|No| SNAP_DIFF[\"Use SNAPSHOT + snapshot_diff\"]\n</code></pre>"},{"location":"architecture/ingestion-correctness/#decision-matrix","title":"Decision Matrix","text":"Source Characteristics Ingestion Mode Delete Mode Has <code>updated_at</code>, no hard deletes HWM <code>none</code> Has <code>updated_at</code>, hard deletes, SQL = authoritative HWM <code>sql_compare</code> Has <code>updated_at</code>, hard deletes, SQL = staging HWM <code>snapshot_diff</code> (periodic full extract) No <code>updated_at</code>, full snapshot daily SNAPSHOT <code>snapshot_diff</code> Append-only facts (immutable) HWM on <code>event_id</code> or <code>created_at</code> <code>none</code>"},{"location":"architecture/ingestion-correctness/#hybrid-pattern-for-updatable-facts","title":"Hybrid Pattern for Updatable Facts","text":"<p>For tables like downtime, production, energy where: - Records can be updated (corrections) - Records can be hard-deleted (reversals) - But volume is high</p> <p>Recommended approaches:</p> <ol> <li> <p>Use <code>sql_compare</code> (simpler) - If source is authoritative and reachable, just use HWM with <code>sql_compare</code>. No full snapshots needed.</p> </li> <li> <p>Hybrid HWM + Snapshot (complex) - If source is unreliable/staging:</p> </li> <li>Daily: HWM ingestion</li> <li>Weekly: Full snapshot (enables <code>snapshot_diff</code>)</li> <li>Requires filtering Bronze to latest snapshot in Silver</li> </ol> <pre><code># Option 1: sql_compare (recommended)\nincremental:\n  mode: stateful\n  column: updated_at\n  watermark_lag: 2h\n# Then use detect_deletes with mode: sql_compare\n\n# Option 2: Hybrid (only if sql_compare won't work)\n# Use two separate pipeline runs or orchestrator-controlled mode\n</code></pre>"},{"location":"architecture/ingestion-correctness/#yaml-configuration-reference","title":"YAML Configuration Reference","text":""},{"location":"architecture/ingestion-correctness/#full-silver-node-example","title":"Full Silver Node Example","text":"<pre><code>nodes:\n  - name: silver_customers\n    read:\n      connection: bronze\n      table: customers\n    transform:\n      steps:\n        # REQUIRED: Deduplication\n        - operation: deduplicate\n          params:\n            keys: [customer_id]\n            order_by: _extracted_at DESC\n\n        # OPTIONAL: Delete Detection (sql_compare recommended for HWM)\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [customer_id]\n            source_connection: azure_sql\n            source_table: dbo.Customers\n            soft_delete_col: _is_deleted\n    write:\n      connection: silver\n      table: customers\n      mode: upsert\n      keys: [customer_id]\n</code></pre>"},{"location":"architecture/ingestion-correctness/#delete-detection-config-options","title":"Delete Detection Config Options","text":"Option Type Description <code>mode</code> enum <code>none</code>, <code>snapshot_diff</code>, <code>sql_compare</code> <code>keys</code> list Business key columns for comparison <code>soft_delete_col</code> string Column to flag deletes (default: <code>_is_deleted</code>). If null, hard-delete. <code>source_connection</code> string For <code>sql_compare</code>: connection to query <code>source_table</code> string For <code>sql_compare</code>: table to query <code>source_query</code> string For <code>sql_compare</code>: custom query (overrides source_table) <code>max_delete_percent</code> float Safety threshold: warn/error if more than X% would be deleted (default: 50) <code>on_threshold_breach</code> enum Behavior when threshold exceeded: <code>warn</code>, <code>error</code>, <code>skip</code> <code>on_first_run</code> enum Behavior when no previous version exists: <code>skip</code>, <code>error</code>"},{"location":"architecture/ingestion-correctness/#quick-reference-table","title":"Quick Reference Table","text":"Table Type Example Ingestion Delete Mode Why Immutable facts Events, logs, sensor data HWM on <code>created_at</code> <code>none</code> Facts don't delete Mutable facts Downtime, production records HWM on <code>updated_at</code> <code>sql_compare</code> Source is authoritative Dimensions (SQL auth) Customers from ERP HWM on <code>updated_at</code> <code>sql_compare</code> ERP is truth Dimensions (staging) Staging from RPA Full snapshot <code>snapshot_diff</code> Staging is ephemeral Reference data Country codes, units Full snapshot <code>snapshot_diff</code> Rarely changes, small tables"},{"location":"architecture/ingestion-correctness/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"architecture/ingestion-correctness/#1-never-use-sql_compare-on-staging-tables","title":"1. Never use <code>sql_compare</code> on staging tables","text":"<p>Staging tables may be truncated between loads. If Silver compares to an empty staging table, everything appears deleted.</p>"},{"location":"architecture/ingestion-correctness/#2-always-add-watermark_lag","title":"2. Always add <code>watermark_lag</code>","text":"<p>Source systems have clock skew and replication lag. A 1-2 hour lag buffer prevents missed records.</p> <pre><code>incremental:\n  mode: stateful\n  column: updated_at\n  watermark_lag: 2h  # Safety buffer\n</code></pre>"},{"location":"architecture/ingestion-correctness/#3-prefer-soft-deletes-over-hard-deletes","title":"3. Prefer soft-deletes over hard-deletes","text":"<p>Use <code>soft_delete_col: _is_deleted</code> instead of actually removing rows. This: - Preserves audit trail - Allows undo if delete detection was wrong - Supports downstream \"as-of\" queries</p>"},{"location":"architecture/ingestion-correctness/#4-run-periodic-full-snapshots-for-hwm-tables","title":"4. Run periodic full snapshots for HWM tables","text":"<p>Even with reliable HWM, run a weekly full snapshot to: - Catch missed updates (source bugs, clock issues) - Detect hard deletes - Validate HWM accuracy</p>"},{"location":"architecture/ingestion-correctness/#summary","title":"Summary","text":"<p>The architecture for non-CDC sources follows these principles:</p> <ol> <li>Bronze is stateless and append-only \u2014 capture everything, decide nothing</li> <li>Silver owns deduplication \u2014 mandatory, not optional</li> <li>Silver owns delete detection \u2014 optional, with three explicit modes</li> <li>Gold assumes clean data \u2014 no deduplication logic needed</li> </ol> <p>This design achieves CDC-like correctness while keeping the logic simple, deterministic, and replayable.</p>"},{"location":"architecture/performance-analysis-findings/","title":"Odibi Delta Write Performance Analysis","text":""},{"location":"architecture/performance-analysis-findings/#executive-summary","title":"Executive Summary","text":"<p>The write phase accounts for ~96% of pipeline runtime (~50s for slowest nodes). Analysis identified 8 major bottlenecks with cumulative potential savings of 30-45 seconds per node.</p>"},{"location":"architecture/performance-analysis-findings/#write-path-overview","title":"Write Path Overview","text":"<pre><code>_execute_write_phase() [node.py:1585]\n    \u2502\n    \u251c\u2500 skip_if_unchanged check (if enabled)\n    \u2502   \u251c\u2500 compute_spark_dataframe_hash() [content_hash.py:57]\n    \u2502   \u2502   \u2514\u2500 _compute_spark_hash_distributed() - xxhash64 + agg\n    \u2502   \u2514\u2500 get_content_hash_from_state() - Delta read for lookup\n    \u2502\n    \u251c\u2500 schema_policy check (if enabled)\n    \u2502   \u2514\u2500 get_table_schema() - Delta table read\n    \u2502\n    \u251c\u2500 add_metadata columns (if enabled)\n    \u2502   \u2514\u2500 withColumn(\"_extracted_at\", current_timestamp())\n    \u2502\n    \u2514\u2500 engine.write() [spark_engine.py:717]\n        \u251c\u2500 df.rdd.getNumPartitions() - Spark action!\n        \u251c\u2500 [Delta path-based write]\n        \u2502   \u251c\u2500 writer.save(full_path)\n        \u2502   \u251c\u2500 register_table SQL (CREATE TABLE IF NOT EXISTS)\n        \u2502   \u251c\u2500 _apply_table_properties() - ALTER TABLE per property\n        \u2502   \u251c\u2500 _optimize_delta_write() - OPTIMIZE/ZORDER\n        \u2502   \u2514\u2500 _get_last_delta_commit_info() - Delta history read\n        \u2502\n        \u2514\u2500 _register_catalog_entries() [node.py:1806]\n            \u251c\u2500 register_asset() - Delta append\n            \u251c\u2500 track_schema() - Delta merge\n            \u251c\u2500 log_pattern() - Delta merge\n            \u2514\u2500 record_lineage() - Delta append\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#identified-bottlenecks","title":"Identified Bottlenecks","text":""},{"location":"architecture/performance-analysis-findings/#1-repeated-table-existence-checks-high-impact","title":"1. Repeated Table Existence Checks \u26a0\ufe0f HIGH IMPACT","text":"<p>Location: Multiple locations throughout write path - <code>_execute_write_phase</code> \u2192 <code>get_table_schema()</code> [line 1635] - <code>spark_engine.write()</code> \u2192 <code>table_exists()</code> [line 857] - <code>_generate_incremental_sql_filter()</code> \u2192 <code>table_exists()</code> [line 662]</p> <p>Cost: ~3-5s per redundant check (Delta table open + limit(0).collect())</p> <p>Evidence:</p> <pre><code># spark_engine.py:643-648\ndef table_exists(self, connection, table=None, path=None):\n    if table:\n        if not self.spark.catalog.tableExists(table):\n            return False\n        self.spark.table(table).limit(0).collect()  # Expensive verify\n</code></pre> <p>Fix: Cache existence check result at node level</p> <pre><code># Add to NodeExecutor.__init__\nself._table_exists_cache: Dict[str, bool] = {}\n\n# In table_exists calls\ncache_key = f\"{connection}:{table or path}\"\nif cache_key in self._table_exists_cache:\n    return self._table_exists_cache[cache_key]\n</code></pre> <p>Expected Savings: 5-10s per node</p>"},{"location":"architecture/performance-analysis-findings/#2-schema-capture-on-every-write-medium-high-impact","title":"2. Schema Capture on Every Write \u26a0\ufe0f MEDIUM-HIGH IMPACT","text":"<p>Location: <code>_execute_write_phase</code> lines 1634-1642</p> <pre><code>if config.schema_policy and df is not None:\n    target_schema = self.engine.get_table_schema(  # Delta table read!\n        connection=connection,\n        table=write_config.table,\n        path=write_config.path,\n        format=write_config.format,\n    )\n</code></pre> <p>Cost: ~2-4s (full Delta table schema read, even for small tables)</p> <p>Fix: Skip schema check if table doesn't exist (first run) or use cached schema</p> <pre><code># Only check if target exists and schema_policy requires it\nif config.schema_policy and config.schema_policy.mode != SchemaMode.EVOLVE:\n    if self._table_exists_cache.get(cache_key):\n        target_schema = self.engine.get_table_schema(...)\n</code></pre> <p>Expected Savings: 2-4s per node</p>"},{"location":"architecture/performance-analysis-findings/#3-delta-table-registration-on-every-append-high-impact","title":"3. Delta Table Registration on EVERY Append \u26a0\ufe0f HIGH IMPACT","text":"<p>Location: <code>spark_engine.py</code> lines 1050-1061</p> <pre><code>if register_table:\n    create_sql = (\n        f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n        f\"USING DELTA LOCATION '{full_path}'\"\n    )\n    self.spark.sql(create_sql)  # Runs EVERY write, even for appends\n</code></pre> <p>Cost: ~5-15s (catalog operation + metadata update)</p> <p>This is the primary bottleneck for incremental appends. Running <code>CREATE TABLE IF NOT EXISTS</code> on every write is extremely expensive.</p> <p>Fix: Check if table already registered before running SQL</p> <pre><code>if register_table:\n    if not self.spark.catalog.tableExists(register_table):\n        create_sql = ...\n        self.spark.sql(create_sql)\n</code></pre> <p>Expected Savings: 10-20s per incremental write</p>"},{"location":"architecture/performance-analysis-findings/#4-table-properties-applied-per-property-medium-impact","title":"4. Table Properties Applied Per-Property \u26a0\ufe0f MEDIUM IMPACT","text":"<p>Location: <code>spark_engine.py:225-252</code></p> <pre><code>def _apply_table_properties(self, target, properties, is_table=False):\n    for prop_name, prop_value in properties.items():\n        sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ('{prop_name}' = '{prop_value}')\"\n        self.spark.sql(sql)  # One SQL per property!\n</code></pre> <p>Cost: ~1-2s per property (3-4 properties = 5-8s total)</p> <p>Fix: Batch properties into single ALTER TABLE statement</p> <pre><code>if properties:\n    props_str = \", \".join([f\"'{k}' = '{v}'\" for k, v in properties.items()])\n    sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n    self.spark.sql(sql)\n</code></pre> <p>Expected Savings: 3-6s per node with table properties</p>"},{"location":"architecture/performance-analysis-findings/#5-catalog-entries-written-synchronously-medium-impact","title":"5. Catalog Entries Written Synchronously \u26a0\ufe0f MEDIUM IMPACT","text":"<p>Location: <code>_register_catalog_entries()</code> [node.py:1806-1910]</p> <p>Each write triggers 4 separate Delta operations: 1. <code>register_asset()</code> - Delta append 2. <code>track_schema()</code> - Delta merge (upsert) 3. <code>log_pattern()</code> - Delta merge (upsert) 4. <code>record_lineage()</code> - Delta append</p> <p>Cost: ~3-8s total (each Delta operation has transaction overhead)</p> <p>Fix: Batch catalog writes or make them async</p> <pre><code># Option 1: Batch all catalog writes into single transaction\n# Option 2: Fire-and-forget async writes for non-critical metadata\n# Option 3: Add config to disable catalog writes for performance-critical pipelines\n</code></pre> <p>Expected Savings: 3-6s per node</p>"},{"location":"architecture/performance-analysis-findings/#6-skip_if_unchanged-state-lookup-overhead-low-medium-impact","title":"6. skip_if_unchanged State Lookup Overhead \u26a0\ufe0f LOW-MEDIUM IMPACT","text":"<p>Location: <code>_check_skip_if_unchanged()</code> [node.py:1978-2041]</p> <p>State Lookup Path:</p> <pre><code>state_backend = getattr(self.state_manager, \"backend\", None)\nprevious_hash = get_content_hash_from_state(state_backend, config.name, table_name)\n</code></pre> <p>This triggers <code>_get_hwm_spark()</code> which:</p> <pre><code>df = self.spark.read.format(\"delta\").load(self.meta_state_path)  # Full table read\nrow = df.filter(F.col(\"key\") == key).select(\"value\").first()\n</code></pre> <p>Cost: ~2-3s (Delta table open + filter + collect)</p> <p>The hash computation itself is efficient (distributed xxhash64), but the state lookup reads the entire meta_state table.</p> <p>Fix: Add predicate pushdown or use data skipping</p> <pre><code># Use partition pruning or data skipping\ndf = self.spark.read.format(\"delta\").load(self.meta_state_path)\nrow = df.filter(F.col(\"key\") == key).select(\"value\").limit(1).first()\n</code></pre> <p>Or cache recent HWM values in memory during pipeline execution.</p> <p>Expected Savings: 1-2s per node with skip_if_unchanged</p>"},{"location":"architecture/performance-analysis-findings/#7-dfrddgetnumpartitions-triggers-job-low-impact","title":"7. df.rdd.getNumPartitions() Triggers Job \u26a0\ufe0f LOW IMPACT","text":"<p>Location: <code>spark_engine.write()</code> line 764</p> <pre><code>partition_count = df.rdd.getNumPartitions()  # Spark action!\n</code></pre> <p>For lazy DataFrames, this can trigger computation of the lineage to determine partitions.</p> <p>Cost: ~0.5-2s (variable based on DAG complexity)</p> <p>Fix: Make partition count optional or use lazy evaluation</p> <pre><code>try:\n    # Quick check if already materialized\n    if df.isStreaming:\n        partition_count = None\n    elif hasattr(df, \"_jdf\") and df._jdf.queryExecution().isInstanceOf(...):\n        partition_count = df.rdd.getNumPartitions()\n    else:\n        partition_count = None\nexcept:\n    partition_count = None\n</code></pre> <p>Expected Savings: 0.5-2s per node</p>"},{"location":"architecture/performance-analysis-findings/#8-delta-commit-metadata-fetched-after-write-low-impact","title":"8. Delta Commit Metadata Fetched After Write \u26a0\ufe0f LOW IMPACT","text":"<p>Location: <code>_get_last_delta_commit_info()</code> [spark_engine.py:299-348]</p> <pre><code>dt.history(1).collect()  # Opens Delta table again!\n</code></pre> <p>Cost: ~1-2s (Delta table open + history scan)</p> <p>Fix: Extract commit info from write operation directly if possible, or cache DeltaTable reference.</p> <p>Expected Savings: 1-2s per node</p>"},{"location":"architecture/performance-analysis-findings/#why-small-tables-31-84-rows-take-30-50s","title":"Why Small Tables (31-84 rows) Take 30-50s","text":"<p>The overhead is constant per write, not proportional to data volume:</p> Operation Estimated Time Table existence check 3-5s Schema capture 2-4s Delta write itself 2-5s CREATE TABLE IF NOT EXISTS 5-15s Table properties (3x) 3-6s Catalog entries (4x) 3-8s Delta commit metadata 1-2s Total 19-45s <p>The actual DataFrame write is only ~2-5s. Everything else is metadata overhead.</p>"},{"location":"architecture/performance-analysis-findings/#specific-node-analysis","title":"Specific Node Analysis","text":""},{"location":"architecture/performance-analysis-findings/#tblgrinddailyproduction-59-rows-30-45s","title":"tblGrindDailyProduction (59 rows, 30-45s)","text":"<ul> <li>Likely has: table registration, schema policy, table properties</li> <li>All constant overhead applies regardless of 59 rows</li> </ul>"},{"location":"architecture/performance-analysis-findings/#vwdryershiftlineproductrunwithdryeronhours-684-rows-40s","title":"vwDryerShiftLineProductRunWithDryerOnHours (684 rows, 40s)","text":"<ul> <li>Possibly running OPTIMIZE/ZORDER after write</li> <li>Check if <code>auto_optimize</code> or <code>zorder_by</code> is configured</li> </ul>"},{"location":"architecture/performance-analysis-findings/#recommended-priority-fixes","title":"Recommended Priority Fixes","text":""},{"location":"architecture/performance-analysis-findings/#phase-1-quick-wins-expected-15-25s-savings","title":"Phase 1: Quick Wins (Expected: 15-25s savings)","text":"<ol> <li>Skip <code>CREATE TABLE IF NOT EXISTS</code> for existing tables (10-20s)</li> <li>Batch table properties into single ALTER TABLE (3-6s)</li> <li>Cache table existence checks (5-10s)</li> </ol>"},{"location":"architecture/performance-analysis-findings/#phase-2-medium-effort-expected-5-10s-savings","title":"Phase 2: Medium Effort (Expected: 5-10s savings)","text":"<ol> <li>Skip schema capture when not needed (2-4s)</li> <li>Batch catalog entries (3-6s)</li> </ol>"},{"location":"architecture/performance-analysis-findings/#phase-3-architecture-changes-expected-5-10s-savings","title":"Phase 3: Architecture Changes (Expected: 5-10s savings)","text":"<ol> <li>Async catalog writes</li> <li>State lookup optimization</li> <li>Lazy partition count</li> </ol>"},{"location":"architecture/performance-analysis-findings/#implementation-sketch","title":"Implementation Sketch","text":""},{"location":"architecture/performance-analysis-findings/#fix-1-skip-redundant-table-registration","title":"Fix #1: Skip Redundant Table Registration","text":"<pre><code># spark_engine.py, around line 1050\nif register_table:\n    # Only register if table doesn't exist in catalog\n    if not self.spark.catalog.tableExists(register_table):\n        create_sql = (\n            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n            f\"USING DELTA LOCATION '{full_path}'\"\n        )\n        self.spark.sql(create_sql)\n        ctx.info(f\"Registered table: {register_table}\", path=full_path)\n    else:\n        ctx.debug(f\"Table {register_table} already registered, skipping\")\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#fix-2-batch-table-properties","title":"Fix #2: Batch Table Properties","text":"<pre><code># spark_engine.py, replace _apply_table_properties\ndef _apply_table_properties(self, target: str, properties: Dict[str, str], is_table: bool = False):\n    if not properties:\n        return\n\n    table_ref = target if is_table else f\"delta.`{target}`\"\n    props_list = [f\"'{k}' = '{v}'\" for k, v in properties.items()]\n    props_str = \", \".join(props_list)\n\n    sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n    self.spark.sql(sql)\n    ctx.debug(f\"Set {len(properties)} table properties in single statement\")\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#fix-3-add-table-existence-cache","title":"Fix #3: Add Table Existence Cache","text":"<pre><code># node.py, NodeExecutor class\ndef __init__(self, ...):\n    ...\n    self._table_exists_cache: Dict[str, bool] = {}\n\ndef _cached_table_exists(self, connection, table=None, path=None) -&gt; bool:\n    cache_key = f\"{id(connection)}:{table}:{path}\"\n    if cache_key not in self._table_exists_cache:\n        self._table_exists_cache[cache_key] = self.engine.table_exists(connection, table, path)\n    return self._table_exists_cache[cache_key]\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#next-steps","title":"Next Steps","text":"<ol> <li>Implement Fix #1 (register_table skip) - highest ROI</li> <li>Add performance timing logs to validate bottleneck locations</li> <li>A/B test with timing before/after each fix</li> <li>Consider adding a <code>write.performance_mode: fast</code> option that skips non-essential metadata operations</li> </ol>"},{"location":"architecture/performance-optimization-plan/","title":"Pipeline Performance Optimization Plan","text":""},{"location":"architecture/performance-optimization-plan/#current-state-analysis","title":"Current State Analysis","text":"<p>Bronze Pipeline: 55.10s for 35 nodes (as of Dec 2024) - Write phase: 53.01s (96.2%) - Read phase: 1.91s (3.5%) - Transform phase: 1.33s (2.4%)</p> <p>Previous State (before optimization): 75.95s for 35 nodes</p> <p>Identified Bottlenecks:</p>"},{"location":"architecture/performance-optimization-plan/#1-slow-sql-source-reads","title":"1. Slow SQL Source Reads","text":"<p>Problem: Several nodes show extremely slow row rates: | Node | Duration | Rows | Rate | |------|----------|------|------| | <code>nkcmfgproduction_vwDryerShiftLineProductRunWithDryerOnHours</code> | 54.98s | 684 | 12 rows/s | | <code>nkcmfgproduction_tblGrindDailyProduction</code> | 40.71s | 59 | 1.4 rows/s | | <code>indyProduction_tblDryerDowntime</code> | 43.68s | 1.6K | 37 rows/s | | <code>opsvisdata_vw_ref_annualgoal</code> | 23.34s | 47 | 2 rows/s |</p> <p>Root Cause: These are SQL Server views with complex underlying joins. The pipeline reads the entire view without incremental filtering, causing full table scans at the source.</p> <p>Solution:</p> <pre><code># Add incremental config to slow nodes\nread:\n  connection: sql_source\n  format: sql_server\n  table: vwDryerShiftLineProductRunWithDryerOnHours\n  incremental:\n    mode: rolling_window\n    column: ModifiedDate  # or appropriate timestamp column\n    lookback: 7\n    unit: day\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#2-write-phase-dominates-pipeline-time","title":"2. Write Phase Dominates Pipeline Time","text":"<p>Problem: 80.8% of pipeline time spent in write phase.</p> <p>Root Causes: 1. Many small files creating metadata overhead 2. Delta transaction log operations for each table 3. No coalescing of small DataFrames</p> <p>Solutions:</p>"},{"location":"architecture/performance-optimization-plan/#a-coalesce-dataframes-opt-in","title":"a) Coalesce DataFrames (Opt-in)","text":"<p>Status: \u2705 AVAILABLE (Opt-in via config)</p> <p>Coalesce DataFrames to fewer partitions before writing to reduce file overhead.</p> <p>Why opt-in instead of automatic: Automatic coalescing required <code>df.count()</code> which triggers double-evaluation of lazy DataFrames, causing severe performance regression.</p> <p>How to use:</p> <pre><code>write:\n  format: delta\n  path: bronze/table\n  options:\n    coalesce_partitions: 1  # Coalesce to 1 partition before write\n</code></pre> <p>When to use: - Small incremental loads (&lt; 1000 rows) - Tables that accumulate many small files</p>"},{"location":"architecture/performance-optimization-plan/#b-enable-optimized-write","title":"b) Enable Optimized Write","text":"<pre><code>write:\n  format: delta\n  path: bronze/table\n  options:\n    optimize_write: true  # Let Delta optimize file sizes\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#c-parallel-execution","title":"c) Parallel Execution","text":"<p>Status: \u2705 IN USE</p> <pre><code>manager.run(pipeline=\"bronze\", parallel=True, max_workers=16)\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#3-skip_if_unchanged-hash-computation","title":"3. skip_if_unchanged Hash Computation","text":"<p>Status: \u2705 IMPLEMENTED</p> <p>Problem: Legacy implementation collected entire DataFrame to driver for hashing.</p> <p>Solution: Distributed hash computation using <code>xxhash64</code> (now the default).</p> <p>Code Location: <code>odibi/utils/content_hash.py:100-122</code></p> <pre><code>def _compute_spark_hash_distributed(df) -&gt; str:\n    \"\"\"Compute hash distributedly using Spark's xxhash64.\"\"\"\n    from pyspark.sql import functions as F\n\n    hash_cols = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"__NULL__\")) for c in df.columns]\n    work_df = df.withColumn(\"_row_hash\", F.xxhash64(*hash_cols))\n\n    result = work_df.agg(\n        F.count(\"*\").alias(\"row_count\"),\n        F.sum(\"_row_hash\").alias(\"hash_sum\"),\n    ).collect()[0]\n\n    row_count = result[\"row_count\"] or 0\n    hash_sum = result[\"hash_sum\"] or 0\n    combined = f\"v2:{row_count}:{hash_sum}:{','.join(sorted(df.columns))}\"\n    return hashlib.sha256(combined.encode()).hexdigest()\n</code></pre> <p>Benefits: - No data collection to driver (except 2 scalar values) - No full sort required (uses commutative sum) - O(1) memory on driver - Safe for arbitrarily large DataFrames</p>"},{"location":"architecture/performance-optimization-plan/#4-detect_deletes-schema-warnings","title":"4. detect_deletes Schema Warnings","text":"<p>Problem: Warnings about missing keys in previous version.</p> <pre><code>detect_deletes: Keys ['OEE_EVENT_START', ...] not found in previous version (v3).\nSchema may have changed. Skipping delete detection.\n</code></pre> <p>Root Cause: Schema evolution between runs. The previous Delta version has different column names.</p> <p>Solution: Add schema migration handling:</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      mode: snapshot_diff\n      keys: [OEE_EVENT_START, OEE_EVENT_END, Plant, Channel, Asset]\n      on_first_run: skip  # Don't error on first run/schema change\n      on_schema_change: skip  # Skip if schema incompatible\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#5-high-delete-percentage-warnings","title":"5. High Delete Percentage Warnings","text":"<p>Problem: 1279% and 3961% deletion thresholds exceeded.</p> <p>Root Cause: Either: - Wrong keys configured (causing all rows to appear deleted) - Source data genuinely changed significantly - First-time comparison against wrong version</p> <p>Solution: 1. Verify key columns are correct primary keys 2. Set reasonable threshold with warning:</p> <pre><code>- operation: detect_deletes\n  mode: snapshot_diff\n  keys: [correct_key_column]\n  max_delete_percent: 50\n  on_threshold_breach: warn  # Log warning instead of failing\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#implementation-priority","title":"Implementation Priority","text":"Priority Fix Impact Effort Status \ud83d\udd34 High Distributed hash for skip_if_unchanged Avoid driver OOM, faster Medium \u2705 Done \ud83d\udd34 High Coalesce option for writes Reduce write overhead Low \u2705 Available (opt-in) \ud83d\udd34 High Add incremental to slow SQL sources 40-50s savings Low \u2705 Done \ud83d\udfe1 Medium Fix detect_deletes key configuration Clean logs, correct behavior Low \ud83d\udd04 Ongoing \ud83d\udfe2 Low Enable parallel execution Overlap I/O Low \u2705 Done"},{"location":"architecture/performance-optimization-plan/#remaining-bottleneck-delta-append-overhead","title":"Remaining Bottleneck: Delta Append Overhead","text":"<p>The remaining ~30-50s overhead per Delta append to Azure Blob Storage is inherent to the Delta Lake protocol on cloud storage. Each append requires: 1. Read transaction log from Azure 2. Write new Parquet file(s) to Azure 3. Write new transaction log entry 4. Table registration SQL (if applicable)</p> <p>This cannot be significantly reduced further without architectural changes like: - Batching multiple tables into fewer writes - Using a different storage format - Reducing write frequency</p>"},{"location":"architecture/performance-optimization-plan/#quick-wins-applied","title":"Quick Wins Applied","text":"<ol> <li>\u2705 Parallel execution: <code>manager.run(pipeline=\"bronze\", parallel=True, max_workers=16)</code></li> <li>\u2705 Rolling window incremental added to slow SQL views</li> <li>\u2705 Distributed hash now default for skip_if_unchanged</li> <li>\u2705 Coalesce option available via <code>coalesce_partitions</code> write option</li> <li>\ud83d\udd04 Set on_threshold_breach: warn for detect_deletes</li> </ol>"},{"location":"architecture/performance-optimization-plan/#lessons-learned","title":"Lessons Learned","text":"<p>\u26a0\ufe0f Never call <code>df.count()</code> before write - This triggers double-evaluation of lazy DataFrames, causing SQL sources to be read twice. An attempted auto-coalesce feature caused 2.5x performance regression (55s \u2192 138s) due to this issue.</p>"},{"location":"explanation/architecture/","title":"Architecture Guide - Odibi System Design","text":"<p>Visual guide to how Odibi works. See the big picture!</p>"},{"location":"explanation/architecture/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        USER                                  \u2502\n\u2502                          \u2502                                   \u2502\n\u2502                          \u25bc                                   \u2502\n\u2502                   config.yaml                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CONFIG LAYER                               \u2502\n\u2502                                                               \u2502\n\u2502  config.yaml \u2192 Pydantic Models \u2192 ProjectConfig               \u2502\n\u2502                     \u2193                                         \u2502\n\u2502              Validation happens here                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  PIPELINE LAYER                              \u2502\n\u2502                                                               \u2502\n\u2502  Pipeline \u2192 DependencyGraph \u2192 Execution Order                \u2502\n\u2502                     \u2193                                         \u2502\n\u2502              [Node A, Node B, Node C]                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   NODE LAYER                                 \u2502\n\u2502                                                               \u2502\n\u2502  Node \u2192 Read/Transform/Write \u2192 Engine                        \u2502\n\u2502           \u2193                      \u2193                            \u2502\n\u2502    Transformation           PandasEngine                      \u2502\n\u2502      Registry               PolarsEngine                      \u2502\n\u2502                             SparkEngine                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STATE &amp; METADATA LAYER                     \u2502\n\u2502                                                               \u2502\n\u2502  System Catalog (Delta Tables) \u2190\u2192 OpenLineage Emitter        \u2502\n\u2502           \u2193                           \u2193                       \u2502\n\u2502    _odibi_system/state          DataHub / Marquez             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STORAGE LAYER                              \u2502\n\u2502                                                               \u2502\n\u2502  Connections \u2192 Local / Azure / SQL                           \u2502\n\u2502                     \u2193                                         \u2502\n\u2502               Actual Data                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STORY LAYER                                \u2502\n\u2502                                                               \u2502\n\u2502  Metadata \u2192 Renderers \u2192 HTML/MD/JSON                         \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Automatic audit trail                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"explanation/architecture/#pipeline-execution-flow","title":"Pipeline Execution Flow","text":""},{"location":"explanation/architecture/#step-by-step-what-happens-when-you-run-odibi-run-configyaml","title":"Step-by-Step: What Happens When You Run <code>odibi run config.yaml</code>","text":"<pre><code>1. CLI Entry Point (cli/main.py)\n   \u2502\n   \u251c\u2500\u2192 Parse arguments\n   \u2514\u2500\u2192 Call run_command(args)\n\n2. Load Configuration (cli/run.py)\n   \u2502\n   \u251c\u2500\u2192 Read YAML file\n   \u251c\u2500\u2192 Parse to ProjectConfig (Pydantic validation)\n   \u2514\u2500\u2192 Create PipelineManager\n\n3. Build Dependency Graph (graph.py)\n   \u2502\n   \u251c\u2500\u2192 Extract all nodes\n   \u251c\u2500\u2192 Build dependency edges\n   \u251c\u2500\u2192 Check for cycles\n   \u2514\u2500\u2192 Topological sort \u2192 execution order\n\n4. Execute Nodes (pipeline.py)\n   \u2502\n   \u251c\u2500\u2192 For each node in order:\n   \u2502   \u2502\n   \u2502   \u251c\u2500\u2192 Create Node instance (node.py)\n   \u2502   \u251c\u2500\u2192 Execute read/transform/write\n   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u251c\u2500\u2192 Read: Engine.read() \u2192 DataFrame\n   \u2502   \u2502   \u251c\u2500\u2192 Transform: Registry.get(operation) \u2192 transformed DataFrame  \n   \u2502   \u2502   \u2514\u2500\u2192 Write: Engine.write(DataFrame)\n   \u2502   \u2502\n   \u2502   \u251c\u2500\u2192 Store result in Context\n   \u2502   \u2514\u2500\u2192 Track metadata (timing, rows, schema)\n   \u2502\n   \u2514\u2500\u2192 All nodes complete\n\n5. Generate Story (story/generator.py)\n   \u2502\n   \u251c\u2500\u2192 Collect all node metadata\n   \u251c\u2500\u2192 Calculate aggregates (success rate, total rows)\n   \u251c\u2500\u2192 Render to HTML/MD/JSON\n   \u2514\u2500\u2192 Save to stories/runs/\n\n6. Return to User\n   \u2502\n   \u2514\u2500\u2192 \"Pipeline completed successfully\" \u2705\n</code></pre>"},{"location":"explanation/architecture/#module-dependencies","title":"Module Dependencies","text":""},{"location":"explanation/architecture/#core-dependencies","title":"Core Dependencies","text":"<pre><code>config.py (no dependencies - pure Pydantic models)\n    \u2193\ncontext.py (stores DataFrames)\n    \u2193\ntransformations/ (registry + decorators)\n    \u2193\noperations/ (uses transformations)\n    \u2193\nengine/ (executes operations)\n    \u2193\nnode.py (uses engine + context)\n    \u2193\ngraph.py (orders nodes)\n    \u2193\npipeline.py (orchestrates everything)\n    \u2193\nstory/ (documents execution)\n    \u2193\ncli/ (user interface)\n</code></pre>"},{"location":"explanation/architecture/#module-relationships","title":"Module Relationships","text":"<pre><code>transformations/\n    \u251c\u2500\u2192 Used by: node.py, story/doc_story.py\n    \u2514\u2500\u2192 Uses: registry.py (core)\n\nregistry.py\n    \u251c\u2500\u2192 Used by: transformations/, engine/\n    \u2514\u2500\u2192 Uses: Nothing (singleton)\n\nstate/\n    \u251c\u2500\u2192 Used by: node.py, pipeline.py\n    \u2514\u2500\u2192 Uses: deltalake (local), spark (distributed)\n\nlineage/\n    \u251c\u2500\u2192 Used by: node.py, pipeline.py\n    \u2514\u2500\u2192 Uses: openlineage-python (optional)\n\nconnections/\n    \u251c\u2500\u2192 Used by: engine/\n    \u2514\u2500\u2192 Uses: Nothing (independent connectors)\n\nengine/\n    \u251c\u2500\u2192 Used by: node.py\n    \u2514\u2500\u2192 Uses: connections/, transformations/\n\ncli/\n    \u251c\u2500\u2192 Used by: Users!\n    \u2514\u2500\u2192 Uses: Everything\n</code></pre> <p>Key insight: <code>transformations/</code> provides the logic, <code>engine/</code> provides the horsepower, and <code>state/</code> provides the memory.</p>"},{"location":"explanation/architecture/#data-flow","title":"Data Flow","text":""},{"location":"explanation/architecture/#how-data-moves-through-a-pipeline","title":"How Data Moves Through a Pipeline","text":"<pre><code>1. User YAML Config\n   \u2193\n2. Parsed to ProjectConfig (in-memory objects)\n   \u2193\n3. Pipeline.run() starts execution\n   \u2193\n4. For each node:\n\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Node Execution                           \u2502\n   \u2502                                          \u2502\n   \u2502  1. Read Phase (if configured)           \u2502\n   \u2502     \u2514\u2500\u2192 Engine.read() \u2192 DataFrame        \u2502\n   \u2502           \u2514\u2500\u2192 Connection.get_path()      \u2502\n   \u2502                 \u2514\u2500\u2192 Actual file/DB       \u2502\n   \u2502                                          \u2502\n   \u2502  2. Transform Phase (if configured)      \u2502\n   \u2502     \u251c\u2500\u2192 Get DataFrame from context       \u2502\n   \u2502     \u251c\u2500\u2192 Registry.get(operation)          \u2502\n   \u2502     \u2514\u2500\u2192 func(df, **params) \u2192 DataFrame   \u2502\n   \u2502                                          \u2502\n   \u2502  3. Write Phase (if configured)          \u2502\n   \u2502     \u2514\u2500\u2192 Engine.write(DataFrame)          \u2502\n   \u2502           \u2514\u2500\u2192 Connection + format        \u2502\n   \u2502                                          \u2502\n   \u2502  4. Store Result                         \u2502\n   \u2502     \u2514\u2500\u2192 Context.set(node_name, df)       \u2502\n   \u2502                                          \u2502\n   \u2502  5. Track Metadata                       \u2502\n   \u2502     \u2514\u2500\u2192 NodeExecutionMetadata            \u2502\n   \u2502           \u251c\u2500\u2192 Row counts                 \u2502\n   \u2502           \u251c\u2500\u2192 Schema                     \u2502\n   \u2502           \u2514\u2500\u2192 Timing                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   \u2193\n5. All nodes complete\n   \u2193\n6. Generate Story\n   \u2514\u2500\u2192 PipelineStoryMetadata\n       \u251c\u2500\u2192 All node metadata\n       \u2514\u2500\u2192 Rendered to HTML/MD/JSON\n</code></pre>"},{"location":"explanation/architecture/#transformation-lifecycle","title":"Transformation Lifecycle","text":""},{"location":"explanation/architecture/#registration-import-time","title":"Registration (Import Time)","text":"<pre><code># When Python imports odibi/operations/unpivot.py:\n\n@transformation(\"unpivot\", category=\"reshaping\")  # \u2190 This runs immediately!\ndef unpivot(df, ...):\n    ...\n\n# What happens:\n# 1. transformation(\"unpivot\", ...) returns a decorator\n# 2. Decorator wraps unpivot function\n# 3. Decorator calls registry.register(\"unpivot\", wrapped_unpivot)\n# 4. Registry stores it globally\n# 5. Function is now available to all pipelines\n</code></pre>"},{"location":"explanation/architecture/#lookup-runtime","title":"Lookup (Runtime)","text":"<pre><code># During pipeline execution:\n\n# 1. Node config says: operation=\"unpivot\"\n# 2. Node calls: registry.get(\"unpivot\")\n# 3. Registry returns the function\n# 4. Node calls: func(df, id_vars=\"ID\", ...)\n# 5. Result returned\n</code></pre>"},{"location":"explanation/architecture/#explanation-story-generation","title":"Explanation (Story Generation)","text":"<pre><code># During story generation:\n\n# 1. Story generator calls: func.get_explanation(**params, **context)\n# 2. ExplainableFunction looks for attached explain_func\n# 3. If found: calls explain_func(**params, **context)\n# 4. Returns formatted markdown\n# 5. Included in HTML story\n</code></pre>"},{"location":"explanation/architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"explanation/architecture/#connection-abstraction","title":"Connection Abstraction","text":"<pre><code>BaseConnection (interface)\n    \u2193\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        \u2502          \u2502             \u2502\nLocal   ADLS    AzureSQL      (more...)\n\u2502        \u2502          \u2502\n\u2193        \u2193          \u2193\n./data  Azure Blob  SQL Database\n</code></pre> <p>All connections implement: - <code>get_path(relative_path)</code> - Resolve full path - <code>validate()</code> - Check configuration</p> <p>Storage-specific methods: - ADLS: <code>pandas_storage_options()</code>, <code>configure_spark()</code> - AzureSQL: <code>read_sql()</code>, <code>write_table()</code>, <code>get_engine()</code> - Local: (just path manipulation)</p>"},{"location":"explanation/architecture/#engine-abstraction","title":"Engine Abstraction","text":"<pre><code>Engine (interface)\n    \u2193\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                \u2502\nPandasEngine  SparkEngine\n\u2502                \u2502\n\u2193                \u2193\nDataFrame    pyspark.DataFrame\n</code></pre> <p>All engines implement: - <code>read(connection, path, format, options)</code> - <code>write(df, connection, path, format, mode, options)</code> - <code>execute_sql(df, query)</code></p> <p>Why? Swap Pandas \u2194 Spark without changing config!</p>"},{"location":"explanation/architecture/#story-generation-architecture","title":"Story Generation Architecture","text":""},{"location":"explanation/architecture/#three-types-of-stories","title":"Three Types of Stories","text":"<pre><code>1. RUN STORIES (automatic)\n   \u2502\n   \u2514\u2500\u2192 Generated during pipeline.run()\n       \u251c\u2500\u2192 Captures actual execution\n       \u251c\u2500\u2192 Saved to stories/runs/\n       \u2514\u2500\u2192 For audit/debugging\n\n2. DOC STORIES (on-demand)\n   \u2502\n   \u2514\u2500\u2192 Generated via CLI: odibi story generate\n       \u251c\u2500\u2192 Pulls operation explanations\n       \u251c\u2500\u2192 For stakeholder communication\n       \u2514\u2500\u2192 Saved to docs/\n\n3. DIFF STORIES (comparison)\n   \u2502\n   \u2514\u2500\u2192 Generated via CLI: odibi story diff\n       \u251c\u2500\u2192 Compares two run stories\n       \u251c\u2500\u2192 Shows what changed\n       \u2514\u2500\u2192 For troubleshooting\n</code></pre>"},{"location":"explanation/architecture/#story-generation-pipeline","title":"Story Generation Pipeline","text":"<pre><code>Execution \u2192 Metadata Collection \u2192 Rendering \u2192 Output\n   \u2193              \u2193                   \u2193          \u2193\nNodes run    NodeExecution      Renderer    HTML file\n             Metadata          (HTML/MD/JSON)\n             tracked\n</code></pre>"},{"location":"explanation/architecture/#the-registry-pattern-deep-dive","title":"The Registry Pattern (Deep Dive)","text":""},{"location":"explanation/architecture/#why-this-pattern","title":"Why This Pattern?","text":"<p>Problem: How do we make operations available globally?</p> <p>Bad Solution 1: Import everything</p> <pre><code>from odibi.operations import pivot, unpivot, join, sql, ...\n# Breaks as we add more operations\n</code></pre> <p>Bad Solution 2: String-based imports</p> <pre><code>op_module = __import__(f\"odibi.operations.{operation_name}\")\n# Fragile, hard to debug\n</code></pre> <p>Good Solution: Registry</p> <pre><code># Operations register themselves:\n@transformation(\"pivot\")\ndef pivot(...): ...\n\n# Look up by name:\nfunc = registry.get(\"pivot\")\n\n# Easy! Scalable! Type-safe!\n</code></pre>"},{"location":"explanation/architecture/#registry-singleton-pattern","title":"Registry Singleton Pattern","text":"<p>One registry for entire process:</p> <pre><code># odibi/transformations/registry.py\n\n# Create once at module level\n_global_registry = TransformationRegistry()\n\ndef get_registry():\n    return _global_registry  # Always same instance\n</code></pre> <p>Benefits: - \u2705 Single source of truth - \u2705 Operations registered once - \u2705 Available everywhere - \u2705 Easy to test (registry.clear() in tests)</p>"},{"location":"explanation/architecture/#error-handling-strategy","title":"Error Handling Strategy","text":""},{"location":"explanation/architecture/#validation-layers","title":"Validation Layers","text":"<pre><code>Layer 1: Pydantic (config validation)\n   \u2193\nLayer 2: Connection.validate() (connection validation)\n   \u2193\nLayer 3: Graph.validate() (dependency validation)\n   \u2193\nLayer 4: Runtime (execution errors)\n</code></pre> <p>Fail fast: Catch errors before execution starts!</p>"},{"location":"explanation/architecture/#error-propagation","title":"Error Propagation","text":"<pre><code>try:\n    # Node execution\n    result = node.execute()\nexcept Exception as e:\n    # Caught by node.py\n    node_result = NodeResult(\n        success=False,\n        error=e\n    )\n    # Stored in metadata\n    # Shown in story\n    # Pipeline continues (or stops, depending on config)\n</code></pre> <p>Stories capture all errors - makes debugging easy!</p>"},{"location":"explanation/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"explanation/architecture/#time-complexity","title":"Time Complexity","text":"<ul> <li>Config loading: O(1) - just YAML parse</li> <li>Dependency graph: O(n + e) - n nodes, e edges</li> <li>Node execution: O(n) - linear in number of nodes</li> <li>Story generation: O(n) - linear in number of nodes</li> </ul>"},{"location":"explanation/architecture/#space-complexity","title":"Space Complexity","text":"<ul> <li>Context storage: O(n \u00d7 m) - n nodes, m average DataFrame size</li> <li>Metadata: O(n) - one metadata object per node</li> <li>Stories: O(n) - proportional to nodes</li> </ul>"},{"location":"explanation/architecture/#optimization-points","title":"Optimization Points","text":"<p>1. Parallel Execution (future)</p> <pre><code>Current: A \u2192 B \u2192 C \u2192 D (sequential)\nFuture:  A \u2192 B \u2510\n         A \u2192 C \u2534\u2192 D (parallel)\n</code></pre> <p>2. Lazy Evaluation (future)</p> <pre><code>Current: Execute all nodes\nFuture:  Only execute nodes needed for requested output\n</code></pre> <p>3. Incremental Processing (future)</p> <pre><code>Current: Reprocess all data every time\nFuture:  Only process new/changed data\n</code></pre>"},{"location":"explanation/architecture/#testing-architecture","title":"Testing Architecture","text":""},{"location":"explanation/architecture/#test-pyramid","title":"Test Pyramid","text":"<pre><code>        /\\\n       /E2E\\          \u2190 10 tests (slow, comprehensive)\n      /\u2500\u2500\u2500\u2500\u2500\u2500\\\n     /Integration\\    \u2190 30 tests (medium speed)\n    /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n   /   Unit Tests  \\  \u2190 380+ tests (fast, focused)\n  /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n</code></pre> <p>Unit Tests (380+) - Test individual functions/classes - Use mocks for external dependencies - Run in &lt;5 seconds - Cover edge cases</p> <p>Integration Tests (30) - Test components working together - Use real files (temp directories) - Run in &lt;10 seconds - Cover common scenarios</p> <p>E2E Tests (10) - Test complete pipelines - Real configs, real data - Run in &lt;30 seconds - Cover critical paths</p>"},{"location":"explanation/architecture/#mocking-strategy","title":"Mocking Strategy","text":"<p>Mock external dependencies, not internal ones:</p> <pre><code># \u2705 Good: Mock external SQLAlchemy\n@patch('sqlalchemy.create_engine')\ndef test_azure_sql(mock_engine):\n    conn = AzureSQL(...)\n    # Test connection logic without real DB\n\n# \u274c Bad: Mock internal functions\n@patch('odibi.operations.pivot')\ndef test_something(mock_pivot):\n    # Doesn't test real code!\n</code></pre>"},{"location":"explanation/architecture/#design-patterns-used","title":"Design Patterns Used","text":""},{"location":"explanation/architecture/#1-registry-pattern","title":"1. Registry Pattern","text":"<p>Where: <code>odibi/registry.py</code></p> <p>Purpose: Centralized operation lookup</p> <p>Example: All operations register themselves globally</p> <pre><code># In odibi/transformers/math.py\n@transform(\"calculate_sum\")\ndef calculate_sum(df, ...): ...\n</code></pre>"},{"location":"explanation/architecture/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>Where: <code>odibi/connections/factory.py</code></p> <p>Purpose: Create connections by type name</p> <pre><code>conn = create_connection(config)  # Returns AzureBlobConnection, LocalConnection, etc.\n</code></pre>"},{"location":"explanation/architecture/#3-adapter-pattern-state","title":"3. Adapter Pattern (State)","text":"<p>Where: <code>odibi/state/__init__.py</code></p> <p>Purpose: Uniform interface for state management</p> <pre><code># Unified CatalogStateBackend handles both local and distributed modes:\nbackend = CatalogStateBackend(...)\n# Local: uses delta-rs (writes to local delta tables)\n# Spark: uses Spark SQL (writes to delta tables on ADLS/S3)\n</code></pre>"},{"location":"explanation/architecture/#4-observer-pattern-lineage","title":"4. Observer Pattern (Lineage)","text":"<p>Where: <code>odibi/lineage/</code></p> <p>Purpose: Emit events without coupling execution logic</p> <pre><code># Node execution emits events:\nlineage.emit_start(node)\n# ... execution ...\nlineage.emit_complete(node)\n</code></pre>"},{"location":"explanation/architecture/#5-strategy-pattern","title":"5. Strategy Pattern","text":"<p>Where: <code>engine/</code> (PandasEngine vs SparkEngine vs PolarsEngine)</p> <p>Purpose: Swap execution strategies</p> <pre><code># Same interface, different implementation:\nengine = PandasEngine()  # or PolarsEngine()\ndf = engine.read(...)    # Works with either!\n</code></pre>"},{"location":"explanation/architecture/#5-builder-pattern","title":"5. Builder Pattern","text":"<p>Where: <code>story/doc_story.py</code></p> <p>Purpose: Construct complex documentation</p> <pre><code>generator = DocStoryGenerator(config)\ngenerator.generate(\n    output_path=\"doc.html\",\n    format=\"html\",\n    theme=CORPORATE_THEME\n)\n</code></pre>"},{"location":"explanation/architecture/#6-template-method-pattern","title":"6. Template Method Pattern","text":"<p>Where: <code>story/renderers.py</code></p> <p>Purpose: Define rendering algorithm skeleton</p> <pre><code>class BaseRenderer:\n    def render_to_file(self, metadata, path):\n        content = self.render(metadata)  # Subclass implements\n        self._save(content, path)        # Common logic\n</code></pre>"},{"location":"explanation/architecture/#key-abstractions","title":"Key Abstractions","text":""},{"location":"explanation/architecture/#1-engine-abstraction","title":"1. Engine Abstraction","text":"<p>Why? Support multiple execution backends</p> <pre><code># User doesn't care if Pandas or Spark:\ndf = engine.read(connection, \"data.parquet\", \"parquet\")\n\n# PandasEngine: uses pd.read_parquet()\n# SparkEngine: uses spark.read.parquet()\n# Same interface!\n</code></pre>"},{"location":"explanation/architecture/#2-connection-abstraction","title":"2. Connection Abstraction","text":"<p>Why? Support multiple storage systems</p> <pre><code># User writes: path: \"data.csv\"\n# Connection resolves to:\n# - Local: ./data/data.csv\n# - ADLS: abfss://container@account.dfs.core.windows.net/data.csv\n# - SQL: Table reference\n\n# Same code, different storage!\n</code></pre>"},{"location":"explanation/architecture/#3-transformation-abstraction","title":"3. Transformation Abstraction","text":"<p>Why? User-defined operations work same as built-in</p> <pre><code># Built-in:\n@transformation(\"pivot\")\ndef pivot(...): ...\n\n# User-defined:\n@transformation(\"my_custom_op\")\ndef my_custom_op(...): ...\n\n# Both registered the same way!\n# Both available in YAML!\n</code></pre>"},{"location":"explanation/architecture/#extensibility-points","title":"Extensibility Points","text":""},{"location":"explanation/architecture/#where-you-can-extend-odibi","title":"Where You Can Extend Odibi","text":"<p>1. Add New Operations</p> <pre><code>Location: odibi/operations/\nPattern: Use @transformation decorator\nImpact: Available in all pipelines\n</code></pre> <p>2. Add New Connections</p> <pre><code>Location: odibi/connections/\nPattern: Extend BaseConnection\nImpact: New storage backends\n</code></pre> <p>3. Add New Engines</p> <pre><code>Location: odibi/engine/\nPattern: Implement Engine interface\nImpact: New execution backends\n</code></pre> <p>4. Add New Renderers</p> <pre><code>Location: odibi/story/renderers.py\nPattern: Implement .render() method\nImpact: New story output formats\n</code></pre> <p>5. Add New Themes</p> <pre><code>Location: odibi/story/themes.py\nPattern: Create StoryTheme instance\nImpact: Custom branding\n</code></pre> <p>6. Add New Validators</p> <pre><code>Location: odibi/validation/\nPattern: Create validator class\nImpact: Quality enforcement\n</code></pre>"},{"location":"explanation/architecture/#configuration-model","title":"Configuration Model","text":""},{"location":"explanation/architecture/#pydantic-model-hierarchy","title":"Pydantic Model Hierarchy","text":"<pre><code>ProjectConfig (root)\n    \u251c\u2500\u2500 connections: Dict[str, ConnectionConfig]\n    \u251c\u2500\u2500 story: StoryConfig\n    \u2514\u2500\u2500 pipelines: List[PipelineConfig]\n            \u2514\u2500\u2500 nodes: List[NodeConfig]\n                    \u251c\u2500\u2500 read: ReadConfig (optional)\n                    \u251c\u2500\u2500 transform: TransformConfig (optional)\n                    \u2514\u2500\u2500 write: WriteConfig (optional)\n</code></pre>"},{"location":"explanation/architecture/#validation-flow","title":"Validation Flow","text":"<pre><code>YAML file\n    \u2193\nyaml.safe_load() \u2192 dict\n    \u2193\nProjectConfig(**dict)  \u2190 Pydantic validation happens here!\n    \u2193\nIf valid: ProjectConfig instance\nIf invalid: ValidationError with helpful message\n</code></pre> <p>Example error:</p> <pre><code>ValidationError: 1 validation error for NodeConfig\nname\n  Field required [type=missing]\n</code></pre>"},{"location":"explanation/architecture/#thread-safety","title":"Thread Safety","text":""},{"location":"explanation/architecture/#current-state-single-threaded","title":"Current State: Single-Threaded","text":"<p>Registry: Thread-safe (read-only after startup) Context: NOT thread-safe (single pipeline execution) Pipeline: NOT thread-safe (sequential execution)</p>"},{"location":"explanation/architecture/#future-parallel-execution","title":"Future: Parallel Execution","text":"<p>Possible:</p> <pre><code># Execute independent nodes in parallel\nLayer 0: [A]\nLayer 1: [B, C]  \u2190 Can run in parallel!\nLayer 2: [D]\n</code></pre> <p>Required changes: - Thread-safe Context - Parallel node execution - Coordinated metadata collection</p>"},{"location":"explanation/architecture/#memory-management","title":"Memory Management","text":""},{"location":"explanation/architecture/#dataframe-lifecycle","title":"DataFrame Lifecycle","text":"<pre><code>1. Read \u2192 DataFrame created (stored in memory)\n   \u2193\n2. Transform \u2192 New DataFrame (old one can be GC'd if not reused)\n   \u2193\n3. Write \u2192 DataFrame written to disk\n   \u2193\n4. Context stores DataFrame for downstream nodes\n   \u2193\n5. Pipeline completes \u2192 Context cleared \u2192 memory freed\n</code></pre>"},{"location":"explanation/architecture/#large-dataset-strategies","title":"Large Dataset Strategies","text":"<p>Option 1: Don't store in context</p> <pre><code># Future: Streaming mode\n# Don't keep DataFrames in memory\n# Process and write immediately\n</code></pre> <p>Option 2: Use Spark</p> <pre><code>engine: spark\n# Spark handles large data with partitioning\n</code></pre> <p>Option 3: Chunk processing</p> <pre><code># Write in chunks\nchunksize: 10000  # Process 10K rows at a time\n</code></pre>"},{"location":"explanation/architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"explanation/architecture/#credential-handling","title":"Credential Handling","text":"<p>\u2705 Good:</p> <pre><code>connections:\n  azure:\n    auth_mode: key_vault  # Credentials in Key Vault\n    key_vault_name: myvault\n    secret_name: storage-key\n</code></pre> <p>\u274c Bad:</p> <pre><code>connections:\n  azure:\n    account_key: \"hardcoded_key_here\"  # DON'T!\n</code></pre>"},{"location":"explanation/architecture/#sql-injection-protection","title":"SQL Injection Protection","text":"<p>Odibi uses DuckDB which executes on DataFrames (not databases): - No SQL injection risk - DataFrames are local - Safe execution</p> <p>For Azure SQL, use parameterized queries:</p> <pre><code># \u2705 Safe\nconn.read_sql(\n    \"SELECT * FROM users WHERE id = :user_id\",\n    params={\"user_id\": 123}\n)\n\n# \u274c Unsafe\nconn.read_sql(f\"SELECT * FROM users WHERE id = {user_id}\")\n</code></pre>"},{"location":"explanation/architecture/#next-steps","title":"Next Steps","text":"<p>You now understand the architecture!</p> <p>Learn how to build on it: - Transformation Guide - Create custom operations - Troubleshooting - Debug issues - Read the code! Start with <code>operations/</code> directory</p> <p>Remember: The tests are comprehensive examples. Use them! \ud83e\uddea</p>"},{"location":"explanation/case_studies/","title":"Case Studies (Reference Projects)","text":"<p>Odibi is battle-tested. To prove it, we built \"The Gauntlet\"\u2014a series of reference implementations covering diverse industries and data challenges.</p> <p>These projects live in the <code>examples/</code> directory and serve as blueprints for your own architectures.</p>"},{"location":"explanation/case_studies/#1-odibiflix-high-volume-clickstream","title":"1. OdibiFlix (High-Volume Clickstream)","text":"<p>Scenario: A streaming service processing millions of user events (play, pause, buffer). Challenge: *   Sessionization: Grouping raw events into distinct user sessions (30-minute timeout). *   State Management: Calculating \"buffer ratio\" per session.</p> <p>Architecture: *   Engine: Spark (required for window functions). *   Pattern: Medallion (Bronze JSON -&gt; Silver Delta -&gt; Gold Aggregates). *   Key Feature: Uses <code>odibi.yaml</code> to orchestrate complex SQL window functions without writing Python code.</p>"},{"location":"explanation/case_studies/#2-odibieats-real-time-delivery","title":"2. OdibiEats (Real-Time Delivery)","text":"<p>Scenario: A food delivery app with geospatial data. Challenge: *   Geospatial Joins: Mapping driver lat/long pings to neighborhood polygons. *   SCD Type 2: Tracking menu price changes over time (historical accuracy).</p> <p>Architecture: *   Engine: Pandas (Geopandas extension). *   Pattern: Star Schema (Fact Orders, Dim Restaurants, Dim Drivers). *   Key Feature: Demonstrates how to handle \"slowly changing dimensions\" using Odibi's snapshotting capabilities.</p>"},{"location":"explanation/case_studies/#3-odibihealth-sensitive-pii","title":"3. OdibiHealth (Sensitive PII)","text":"<p>Scenario: A healthcare provider managing patient records. Challenge: *   Compliance: HIPAA requirements to mask PII (Patient Identity). *   Audit: Strict lineage tracking of who accessed what data.</p> <p>Architecture: *   Security: Uses Odibi's <code>sensitive: true</code> flag to redact columns in logs/stories. *   Observability: High-fidelity logging enabled. *   Key Feature: Shows how to build secure pipelines that generate audit trails automatically via <code>odibi story</code>.</p>"},{"location":"explanation/case_studies/#4-odibiquant-financial-time-series","title":"4. OdibiQuant (Financial Time-Series)","text":"<p>Scenario: High-frequency trading data analysis. Challenge: *   Data Quality: Ensuring no gaps in timestamps. *   Precision: Handling 64-bit floats without rounding errors.</p> <p>Architecture: *   Validation: Heavy use of <code>expectations</code> (e.g., <code>row_count &gt; 0</code>, <code>price &gt; 0</code>). *   Engine: DuckDB (via Pandas engine) for fast analytical queries.</p>"},{"location":"explanation/case_studies/#why-use-these","title":"Why use these?","text":"<p>Don't start from scratch. If you are building a fintech app, copy <code>OdibiQuant</code>. If you are building an e-commerce site, look at <code>OdibiEats</code>.</p> <p>Run them locally:</p> <pre><code>cd examples/odibi_flix\nodibi run odibi.yaml\n</code></pre>"},{"location":"features/alerting/","title":"Enhanced Alerting","text":"<p>Real-time notifications for pipeline events with throttling, event-specific payloads, and support for Slack, Teams, and generic webhooks.</p>"},{"location":"features/alerting/#overview","title":"Overview","text":"<p>Odibi's alerting system provides: - Multiple channels: Slack, Teams, generic webhooks - Event-specific payloads: Contextual information for each event type - Throttling: Prevent alert spam with rate limiting - Rich formatting: Adaptive cards for Teams, Block Kit for Slack</p>"},{"location":"features/alerting/#configuration","title":"Configuration","text":""},{"location":"features/alerting/#basic-alert-setup","title":"Basic Alert Setup","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n</code></pre>"},{"location":"features/alerting/#alert-config-options","title":"Alert Config Options","text":"Field Type Required Description <code>type</code> string Yes Alert type: <code>slack</code>, <code>teams</code>, <code>webhook</code> <code>url</code> string Yes Webhook URL <code>on_events</code> list No Events to trigger on (default: <code>on_failure</code>) <code>metadata</code> object No Extra settings (throttling, channel, etc.)"},{"location":"features/alerting/#event-types","title":"Event Types","text":"Event Description <code>on_start</code> Pipeline started <code>on_success</code> Pipeline completed successfully <code>on_failure</code> Pipeline failed <code>on_quarantine</code> Rows were quarantined <code>on_gate_block</code> Quality gate blocked the pipeline <code>on_threshold_breach</code> A threshold was exceeded"},{"location":"features/alerting/#throttling","title":"Throttling","text":"<p>Prevent alert spam with time-based and rate-based throttling:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15   # Min 15 minutes between same alerts\n      max_per_hour: 10       # Max 10 alerts per hour\n</code></pre>"},{"location":"features/alerting/#throttle-key","title":"Throttle Key","text":"<p>Throttling is applied per unique combination of: - Pipeline name - Event type</p> <p>So a <code>process_orders</code> pipeline failing twice in 5 minutes sends one alert, but a different pipeline can still alert.</p>"},{"location":"features/alerting/#event-specific-payloads","title":"Event-Specific Payloads","text":""},{"location":"features/alerting/#quarantine-events","title":"Quarantine Events","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n</code></pre> <p>Payload includes: - Rows quarantined count - Quarantine table path - Failed test names - Node name</p>"},{"location":"features/alerting/#gate-block-events","title":"Gate Block Events","text":"<pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_gate_block\n</code></pre> <p>Payload includes: - Pass rate achieved - Required pass rate - Number of failed rows - Failure reasons</p>"},{"location":"features/alerting/#threshold-breach-events","title":"Threshold Breach Events","text":"<pre><code>alerts:\n  - type: webhook\n    url: \"https://api.example.com/alerts\"\n    on_events:\n      - on_threshold_breach\n</code></pre> <p>Payload includes: - Metric name - Threshold value - Actual value - Node name</p>"},{"location":"features/alerting/#complete-example","title":"Complete Example","text":"<pre><code>project: DataPipeline\nengine: spark\n\nalerts:\n  # Critical failures - immediate notification\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 5\n      channel: \"#data-critical\"\n\n  # Data quality issues - less urgent\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n      - on_threshold_breach\n    metadata:\n      throttle_minutes: 30\n      max_per_hour: 5\n      channel: \"#data-quality\"\n\n  # Teams for management visibility\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n    metadata:\n      throttle_minutes: 60\n\nconnections:\n  # ...\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: validate_orders\n        validation:\n          tests:\n            - type: not_null\n              columns: [order_id]\n              on_fail: quarantine\n          quarantine:\n            connection: silver\n            path: quarantine/orders\n          gate:\n            require_pass_rate: 0.95\n        # ...\n</code></pre>"},{"location":"features/alerting/#slack-configuration","title":"Slack Configuration","text":""},{"location":"features/alerting/#block-kit-format","title":"Block Kit Format","text":"<p>Slack alerts use Block Kit for rich formatting:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udeab ODIBI: process_orders - GATE_BLOCKED \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Project:    DataPipeline                 \u2502\n\u2502 Status:     GATE_BLOCKED                 \u2502\n\u2502 Duration:   45.23s                       \u2502\n\u2502 Pass Rate:  92.3%                        \u2502\n\u2502 Required:   95.0%                        \u2502\n\u2502 Rows Failed: 1,542                       \u2502\n\u2502                                          \u2502\n\u2502 [\ud83d\udcca View Story]                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/alerting/#custom-channel","title":"Custom Channel","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n    metadata:\n      channel: \"#data-alerts\"  # Override default channel\n</code></pre>"},{"location":"features/alerting/#teams-configuration","title":"Teams Configuration","text":""},{"location":"features/alerting/#adaptive-card-format","title":"Adaptive Card Format","text":"<p>Teams alerts use Adaptive Cards:</p> <pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 30\n</code></pre> <p>Cards include: - Color-coded header (red for failures, orange for warnings) - Fact set with key metrics - Action button to view story</p>"},{"location":"features/alerting/#generic-webhooks","title":"Generic Webhooks","text":"<p>For custom integrations:</p> <pre><code>alerts:\n  - type: webhook\n    url: \"https://api.example.com/webhooks/odibi\"\n    on_events:\n      - on_failure\n      - on_quarantine\n</code></pre>"},{"location":"features/alerting/#webhook-payload","title":"Webhook Payload","text":"<pre><code>{\n  \"pipeline\": \"process_orders\",\n  \"status\": \"QUARANTINE\",\n  \"duration\": 45.23,\n  \"message\": \"150 rows quarantined in validate_orders\",\n  \"timestamp\": \"2024-01-30T10:15:00Z\",\n  \"event_type\": \"on_quarantine\",\n  \"quarantine_details\": {\n    \"rows_quarantined\": 150,\n    \"quarantine_path\": \"silver/quarantine/orders\",\n    \"failed_tests\": [\"not_null\", \"email_format\"],\n    \"node_name\": \"validate_orders\"\n  }\n}\n</code></pre>"},{"location":"features/alerting/#programmatic-alerts","title":"Programmatic Alerts","text":"<p>Send alerts programmatically:</p> <pre><code>from odibi.utils.alerting import (\n    send_quarantine_alert,\n    send_gate_block_alert,\n    send_threshold_breach_alert,\n)\nfrom odibi.config import AlertConfig, AlertType\n\nconfig = AlertConfig(\n    type=AlertType.SLACK,\n    url=\"https://hooks.slack.com/...\",\n)\n\n# Quarantine alert\nsend_quarantine_alert(\n    config=config,\n    pipeline=\"process_orders\",\n    node_name=\"validate_orders\",\n    rows_quarantined=150,\n    quarantine_path=\"silver/quarantine/orders\",\n    failed_tests=[\"not_null\", \"email_format\"],\n)\n\n# Gate block alert\nsend_gate_block_alert(\n    config=config,\n    pipeline=\"process_orders\",\n    node_name=\"validate_orders\",\n    pass_rate=0.92,\n    required_rate=0.95,\n    failed_rows=1542,\n    total_rows=20000,\n    failure_reasons=[\"Pass rate 92.0% &lt; required 95.0%\"],\n)\n</code></pre>"},{"location":"features/alerting/#best-practices","title":"Best Practices","text":"<ol> <li>Use throttling - Prevent alert fatigue during incidents</li> <li>Separate channels - Critical vs informational alerts</li> <li>Include context - Story URLs help with debugging</li> <li>Test webhooks - Verify connectivity before production</li> <li>Monitor alert volume - High volumes indicate systemic issues</li> </ol>"},{"location":"features/alerting/#related","title":"Related","text":"<ul> <li>Quarantine Tables - Quarantine event source</li> <li>Quality Gates - Gate block event source</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/catalog/","title":"System Catalog","text":"<p>Centralized governance and metadata management for pipelines, execution history, schema evolution, and lineage tracking.</p>"},{"location":"features/catalog/#overview","title":"Overview","text":"<p>Odibi's System Catalog (\"The Brain\") provides: - Pipeline Registry: Track pipeline and node definitions with version hashing - Execution History: Complete run history with metrics and duration - State Management: High-water marks (HWM) for incremental processing - Schema Evolution: Automatic tracking of schema changes over time - Lineage Tracking: Table-level upstream/downstream relationships - Pattern Compliance: Track medallion architecture adherence</p>"},{"location":"features/catalog/#configuration","title":"Configuration","text":""},{"location":"features/catalog/#basic-catalog-setup","title":"Basic Catalog Setup","text":"<pre><code>system:\n  connection: system_storage\n  path: _odibi_system\n\nconnections:\n  system_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n</code></pre>"},{"location":"features/catalog/#system-config-options","title":"System Config Options","text":"Field Type Required Description <code>connection</code> string Yes Connection name for catalog storage <code>path</code> string No Subdirectory for catalog tables (default: <code>_odibi_system</code>)"},{"location":"features/catalog/#catalog-tables","title":"Catalog Tables","text":"<p>The System Catalog consists of Delta tables that automatically bootstrap on first run:</p>"},{"location":"features/catalog/#meta_pipelines","title":"meta_pipelines","text":"<p>Tracks pipeline definitions and deployment versions.</p> Column Type Description <code>pipeline_name</code> string Unique pipeline identifier <code>version_hash</code> string MD5 hash of pipeline configuration <code>description</code> string Pipeline description <code>layer</code> string Medallion layer (bronze/silver/gold) <code>schedule</code> string Cron schedule (if defined) <code>tags_json</code> string JSON array of aggregated tags <code>updated_at</code> timestamp Last deployment timestamp"},{"location":"features/catalog/#meta_nodes","title":"meta_nodes","text":"<p>Tracks node configurations within pipelines.</p> Column Type Description <code>pipeline_name</code> string Parent pipeline name <code>node_name</code> string Unique node identifier <code>version_hash</code> string MD5 hash of node configuration <code>type</code> string Node type: read/transform/write <code>config_json</code> string Full node configuration as JSON <code>updated_at</code> timestamp Last deployment timestamp"},{"location":"features/catalog/#meta_runs","title":"meta_runs","text":"<p>Execution history with metrics. Partitioned by <code>pipeline_name</code> and <code>date</code>.</p> Column Type Description <code>run_id</code> string Unique execution identifier <code>pipeline_name</code> string Pipeline name <code>node_name</code> string Node name <code>status</code> string SUCCESS, FAILED, RUNNING <code>rows_processed</code> long Number of rows processed <code>duration_ms</code> long Execution time in milliseconds <code>metrics_json</code> string Additional metrics as JSON <code>timestamp</code> timestamp Execution timestamp <code>date</code> date Partition date"},{"location":"features/catalog/#meta_state","title":"meta_state","text":"<p>High-water mark (HWM) storage for incremental processing. Partitioned by <code>pipeline_name</code>.</p> Column Type Description <code>pipeline_name</code> string Pipeline name <code>node_name</code> string Node name <code>hwm_value</code> string Serialized high-water mark value"},{"location":"features/catalog/#meta_patterns","title":"meta_patterns","text":"<p>Tracks pattern compliance for governance.</p> Column Type Description <code>table_name</code> string Table identifier <code>pattern_type</code> string Pattern type (SCD2, append, etc.) <code>configuration</code> string Pattern configuration as JSON <code>compliance_score</code> double Compliance score (0.0 - 1.0)"},{"location":"features/catalog/#meta_schemas","title":"meta_schemas","text":"<p>Schema version history for drift detection.</p> Column Type Description <code>table_path</code> string Full table path <code>schema_version</code> long Incrementing version number <code>schema_hash</code> string MD5 hash of column definitions <code>columns</code> string JSON: {\"column\": \"type\", ...} <code>captured_at</code> timestamp When schema was captured <code>pipeline</code> string Pipeline that wrote the schema <code>node</code> string Node that wrote the schema <code>run_id</code> string Execution run ID <code>columns_added</code> array New columns in this version <code>columns_removed</code> array Removed columns <code>columns_type_changed</code> array Columns with type changes"},{"location":"features/catalog/#meta_lineage","title":"meta_lineage","text":"<p>Cross-pipeline table lineage relationships.</p> Column Type Description <code>source_table</code> string Source table path <code>target_table</code> string Target table path <code>source_pipeline</code> string Source pipeline (if known) <code>source_node</code> string Source node (if known) <code>target_pipeline</code> string Target pipeline <code>target_node</code> string Target node <code>relationship</code> string \"feeds\" or \"derived_from\" <code>last_observed</code> timestamp Last time relationship was seen <code>run_id</code> string Execution run ID"},{"location":"features/catalog/#meta_tables","title":"meta_tables","text":"<p>Registry of all written tables/assets for discovery.</p> Column Type Description <code>table_path</code> string Full path to the table <code>table_name</code> string Table name <code>pipeline</code> string Pipeline that owns the table <code>node</code> string Node that writes the table <code>format</code> string Storage format (delta, parquet, etc.) <code>connection</code> string Connection name <code>last_updated</code> timestamp Last write timestamp"},{"location":"features/catalog/#meta_metrics","title":"meta_metrics","text":"<p>Business metric definitions for governance and documentation.</p> Column Type Description <code>metric_name</code> string Unique metric identifier <code>definition_sql</code> string SQL definition of the metric <code>dimensions</code> array List of dimension columns <code>source_table</code> string Source table for the metric"},{"location":"features/catalog/#features","title":"Features","text":""},{"location":"features/catalog/#auto-registration","title":"Auto-Registration","text":"<p>Pipelines and nodes are automatically registered when you run them\u2014no explicit <code>deploy()</code> calls required:</p> <pre><code>from odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# Auto-registers pipeline and nodes before execution\nmanager.run(\"my_pipeline\")\n</code></pre> <p>This ensures <code>meta_pipelines</code> and <code>meta_nodes</code> are always populated. Version hashes detect configuration drift automatically.</p>"},{"location":"features/catalog/#pipeline-registration","title":"Pipeline Registration","text":"<p>For explicit registration (e.g., CI/CD pipelines), use:</p> <pre><code>from odibi.catalog import CatalogManager\n\n# Explicit registration\ncatalog.register_pipeline(pipeline_config)\n</code></pre> <p>When a pipeline's configuration changes, the <code>version_hash</code> updates, providing: - Configuration drift detection - Deployment history tracking - Audit trail for changes</p>"},{"location":"features/catalog/#schema-tracking","title":"Schema Tracking","text":"<p>Schema evolution is tracked automatically after every successful write. No manual calls required:</p> <ul> <li><code>meta_schemas</code> is updated with column changes (added, removed, type changes)</li> <li>Version numbers increment on each schema change</li> <li>Change detection compares against the previous version</li> </ul> <p>Querying schema history:</p> <pre><code># Get schema history for a table\nhistory = manager.get_schema_history(\"silver/customers\", limit=10)\n\n# Returns DataFrame with columns_added, columns_removed, columns_type_changed\n</code></pre>"},{"location":"features/catalog/#lineage-tracking","title":"Lineage Tracking","text":"<p>Lineage is tracked automatically based on node dependencies and read/write operations:</p> <ul> <li>Source tables (from <code>read</code> config) are recorded as upstream</li> <li>Target tables (from <code>write</code> config) are recorded as downstream  </li> <li>Cross-pipeline relationships are captured via <code>meta_lineage</code></li> </ul> <p>Querying lineage:</p> <pre><code># Get upstream and downstream lineage\nlineage_df = manager.get_lineage(\"silver/orders\", direction=\"both\")\n\n# Or use CatalogManager directly\nupstream = catalog.get_upstream(\"gold/order_summary\", depth=3)\ndownstream = catalog.get_downstream(\"bronze/raw_orders\", depth=3)\n</code></pre>"},{"location":"features/catalog/#run-history-and-metrics","title":"Run History and Metrics","text":"<p>Execution runs are logged automatically after each node completes:</p> <ul> <li>Status (SUCCESS/FAILURE), duration, rows processed</li> <li>Metrics stored in <code>meta_runs</code>, partitioned by pipeline and date</li> </ul> <p>Querying run history:</p> <pre><code># Get recent runs\nruns_df = manager.list_runs(pipeline=\"orders_pipeline\", limit=20)\n\n# Get average duration for a node\navg_seconds = catalog.get_average_duration(\"transform_orders\", days=7)\n</code></pre>"},{"location":"features/catalog/#asset-registration","title":"Asset Registration","text":"<p>Tables are registered automatically in <code>meta_tables</code> after writes, enabling discovery across the catalog.</p>"},{"location":"features/catalog/#catalog-optimization","title":"Catalog Optimization","text":"<p>Maintenance operations for Spark deployments:</p> <pre><code># Run VACUUM and OPTIMIZE on meta_runs\ncatalog.optimize()\n</code></pre>"},{"location":"features/catalog/#cleanup-and-removal","title":"Cleanup and Removal","text":"<p>Remove stale pipelines, nodes, or orphaned entries:</p> <pre><code># Remove a pipeline and cascade to associated nodes\ndeleted = catalog.remove_pipeline(\"old_pipeline\")\n\n# Remove a specific node\ndeleted = catalog.remove_node(\"my_pipeline\", \"deprecated_node\")\n\n# Cleanup orphans: remove entries not in current config\nresults = catalog.cleanup_orphans(project_config)\n# Returns: {\"meta_pipelines\": 2, \"meta_nodes\": 5}\n\n# Clear state entries\ncatalog.clear_state_key(\"my_pipeline::my_node::hwm\")\ncatalog.clear_state_pattern(\"my_pipeline::*\")  # Wildcards supported\n</code></pre>"},{"location":"features/catalog/#catalogmanager-api","title":"CatalogManager API","text":""},{"location":"features/catalog/#initialization","title":"Initialization","text":"<pre><code>from odibi.catalog import CatalogManager\nfrom odibi.config import SystemConfig\n\ncatalog = CatalogManager(\n    spark=spark_session,           # SparkSession (or None for Pandas)\n    config=system_config,          # SystemConfig object\n    base_path=\"abfss://...\",       # Resolved catalog path\n    engine=pandas_engine           # Optional: for Pandas mode\n)\n</code></pre>"},{"location":"features/catalog/#key-methods","title":"Key Methods","text":"Method Description <code>bootstrap()</code> Create all system tables if missing <code>register_pipeline(config)</code> Register/update pipeline definition <code>register_nodes(config)</code> Register/update node definitions <code>log_run(...)</code> Record execution run <code>track_schema(...)</code> Track schema version <code>get_schema_history(table, limit)</code> Get schema version history <code>record_lineage(...)</code> Record table lineage relationship <code>get_upstream(table, depth)</code> Get upstream dependencies <code>get_downstream(table, depth)</code> Get downstream consumers <code>get_average_duration(node, days)</code> Get average node duration <code>log_metrics(...)</code> Log business metric definitions <code>remove_pipeline(name)</code> Remove pipeline and cascade to nodes <code>remove_node(pipeline, node)</code> Remove a specific node <code>cleanup_orphans(config)</code> Remove entries not in current config <code>clear_state_key(key)</code> Remove a state entry by key <code>clear_state_pattern(pattern)</code> Remove state entries matching pattern <code>optimize()</code> Run VACUUM and OPTIMIZE (Spark only)"},{"location":"features/catalog/#pipelinemanager-query-api","title":"PipelineManager Query API","text":"<p>The <code>PipelineManager</code> provides convenient query methods that wrap catalog operations with smart path resolution:</p>"},{"location":"features/catalog/#smart-path-resolution","title":"Smart Path Resolution","text":"<p>Query methods accept user-friendly identifiers that are automatically resolved:</p> <pre><code># All these work:\nmanager.get_schema_history(\"silver/orders\")           # Relative path\nmanager.get_lineage(\"test.vw_customers\")              # Registered table\nmanager.get_lineage(\"transform_orders\")               # Node name\nmanager.get_schema_history(\"abfss://container/...\")   # Full path (as-is)\n</code></pre>"},{"location":"features/catalog/#query-methods","title":"Query Methods","text":"Method Description <code>list_registered_pipelines()</code> DataFrame of all pipelines from <code>meta_pipelines</code> <code>list_registered_nodes(pipeline=None)</code> DataFrame of nodes, optionally filtered by pipeline <code>list_runs(pipeline, node, status, limit)</code> DataFrame of recent runs with filters <code>list_tables()</code> DataFrame of registered assets from <code>meta_tables</code> <code>get_state(key)</code> Get specific state entry (HWM, etc.) as dict <code>get_all_state(prefix=None)</code> DataFrame of state entries, optionally filtered <code>clear_state(key)</code> Remove a state entry <code>get_schema_history(table, limit)</code> DataFrame of schema versions <code>get_lineage(table, direction)</code> DataFrame of upstream/downstream lineage <code>get_pipeline_status(pipeline)</code> Dict with last run status, duration, timestamp <code>get_node_stats(node, days)</code> Dict with success rate, avg duration, avg rows"},{"location":"features/catalog/#usage-examples","title":"Usage Examples","text":"<pre><code>from odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# List all registered pipelines\npipelines_df = manager.list_registered_pipelines()\n\n# List nodes in a specific pipeline\nnodes_df = manager.list_registered_nodes(pipeline=\"orders_pipeline\")\n\n# Get recent failed runs\nfailed_runs = manager.list_runs(status=\"FAILURE\", limit=20)\n\n# Get HWM state for a node\nhwm = manager.get_state(\"orders_pipeline::load_orders::hwm\")\n\n# Get lineage for a table (both directions)\nlineage_df = manager.get_lineage(\"silver/orders\", direction=\"both\")\n\n# Get node statistics\nstats = manager.get_node_stats(\"transform_orders\", days=7)\n# Returns: {\"node\": \"...\", \"runs\": 42, \"success_rate\": 0.95, \"avg_duration_s\": 12.5, ...}\n\n# Get pipeline status\nstatus = manager.get_pipeline_status(\"orders_pipeline\")\n# Returns: {\"pipeline\": \"...\", \"last_status\": \"SUCCESS\", \"last_run_at\": \"...\", ...}\n</code></pre>"},{"location":"features/catalog/#cli-integration","title":"CLI Integration","text":"<p>Query the catalog from the command line:</p>"},{"location":"features/catalog/#list-execution-runs","title":"List Execution Runs","text":"<pre><code># Recent runs\nodibi catalog runs config.yaml\n\n# Filter by pipeline, status, and time range\nodibi catalog runs config.yaml --pipeline orders_pipeline --status FAILED --days 3\n\n# JSON output\nodibi catalog runs config.yaml --format json --limit 50\n</code></pre>"},{"location":"features/catalog/#list-registered-pipelines","title":"List Registered Pipelines","text":"<pre><code>odibi catalog pipelines config.yaml\nodibi catalog pipelines config.yaml --format json\n</code></pre>"},{"location":"features/catalog/#list-registered-nodes","title":"List Registered Nodes","text":"<pre><code>odibi catalog nodes config.yaml\nodibi catalog nodes config.yaml --pipeline orders_pipeline\n</code></pre>"},{"location":"features/catalog/#view-hwm-state","title":"View HWM State","text":"<pre><code>odibi catalog state config.yaml\nodibi catalog state config.yaml --pipeline orders_pipeline\n</code></pre>"},{"location":"features/catalog/#list-registered-assets","title":"List Registered Assets","text":"<pre><code>odibi catalog tables config.yaml\nodibi catalog tables config.yaml --project MyProject\n</code></pre>"},{"location":"features/catalog/#view-execution-statistics","title":"View Execution Statistics","text":"<pre><code># Statistics for last 7 days\nodibi catalog stats config.yaml\n\n# Filter by pipeline and time range\nodibi catalog stats config.yaml --pipeline orders_pipeline --days 30\n</code></pre> <p>Output includes: - Total runs, success/failure counts - Success rate percentage - Total and average rows processed - Average and total runtime - Runs by pipeline - Most failed nodes</p>"},{"location":"features/catalog/#cli-options","title":"CLI Options","text":"Command Options <code>runs</code> <code>--pipeline</code>, <code>--node</code>, <code>--status</code>, <code>--days</code>, <code>--limit</code>, <code>--format</code> <code>pipelines</code> <code>--format</code> <code>nodes</code> <code>--pipeline</code>, <code>--format</code> <code>state</code> <code>--pipeline</code>, <code>--format</code> <code>tables</code> <code>--project</code>, <code>--format</code> <code>metrics</code> <code>--format</code> <code>patterns</code> <code>--format</code> <code>stats</code> <code>--pipeline</code>, <code>--days</code>"},{"location":"features/catalog/#complete-example","title":"Complete Example","text":""},{"location":"features/catalog/#project-configuration","title":"Project Configuration","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nsystem:\n  connection: catalog_storage\n  path: _odibi_catalog\n\nconnections:\n  catalog_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n\n  bronze:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\n  silver:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: silver\n\npipelines:\n  - pipeline: orders_bronze_to_silver\n    description: \"Transform raw orders to silver layer\"\n    layer: silver\n    nodes:\n      - name: read_raw_orders\n        type: read\n        connection: bronze\n        path: raw/orders\n        format: delta\n\n      - name: transform_orders\n        type: transform\n        input: read_raw_orders\n        transform: |\n          SELECT\n            order_id,\n            customer_id,\n            order_date,\n            total_amount\n          FROM {input}\n          WHERE order_date &gt;= '2024-01-01'\n\n      - name: write_orders\n        type: write\n        input: transform_orders\n        connection: silver\n        path: orders\n        format: delta\n        mode: merge\n        merge_keys: [order_id]\n</code></pre>"},{"location":"features/catalog/#querying-the-catalog","title":"Querying the Catalog","text":"<pre><code># Check registered pipelines\nodibi catalog pipelines config.yaml\n\n# Output:\n# pipeline_name            | layer  | description                          | version_hash | updated_at\n# -------------------------+--------+--------------------------------------+--------------+--------------------\n# orders_bronze_to_silver  | silver | Transform raw orders to silver layer | a1b2c3d4...  | 2024-01-30 10:15:00\n\n# View execution history\nodibi catalog runs config.yaml --pipeline orders_bronze_to_silver --days 7\n\n# Get statistics\nodibi catalog stats config.yaml --pipeline orders_bronze_to_silver\n\n# Output:\n# === Execution Statistics (Last 7 Days) ===\n#\n# Total Runs:     42\n# Successful:     40\n# Failed:         2\n# Success Rate:   95.2%\n#\n# Total Rows:     1,250,000\n# Avg Rows/Run:   29,762\n#\n# Avg Duration:   12.45s\n# Total Runtime:  522.90s\n</code></pre>"},{"location":"features/catalog/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from odibi.pipeline import PipelineManager\n\n# Load configuration\nmanager = PipelineManager.from_yaml(\"config.yaml\")\ncatalog = manager.catalog_manager\n\n# Query schema history\nhistory = catalog.get_schema_history(\"silver/orders\")\nfor version in history:\n    print(f\"v{version['schema_version']}: {version['columns_added']} added\")\n\n# Trace lineage\nupstream = catalog.get_upstream(\"gold/order_summary\")\nfor source in upstream:\n    print(f\"  {'  ' * source['depth']}{source['source_table']}\")\n</code></pre>"},{"location":"features/catalog/#best-practices","title":"Best Practices","text":"<ol> <li>Enable catalog early - Configure the system catalog from project start</li> <li>Use descriptive names - Pipeline and node names become permanent identifiers</li> <li>Monitor statistics - Regular <code>odibi catalog stats</code> reveals performance trends</li> <li>Review schema changes - Track breaking changes before they impact downstream</li> <li>Query lineage - Understand impact before modifying source tables</li> <li>Run optimization - Periodically run <code>catalog.optimize()</code> for Spark deployments</li> </ol>"},{"location":"features/catalog/#related","title":"Related","text":"<ul> <li>Pipeline Configuration - YAML schema reference</li> <li>State Management - HWM-based incremental loads</li> <li>Alerting - Notifications for pipeline events</li> </ul>"},{"location":"features/cli/","title":"Command-Line Interface","text":"<p>The Odibi CLI provides a comprehensive set of commands for running pipelines, managing configurations, exploring lineage, and querying the System Catalog.</p>"},{"location":"features/cli/#overview","title":"Overview","text":"<p>The Odibi CLI is your primary tool for: - Pipeline execution: Run, validate, and monitor data pipelines - Configuration management: Validate and scaffold YAML configs - Catalog queries: Explore runs, nodes, and execution metadata - Lineage exploration: Trace upstream/downstream dependencies - Schema tracking: View schema history and compare versions</p>"},{"location":"features/cli/#commands","title":"Commands","text":""},{"location":"features/cli/#odibi-run","title":"odibi run","text":"<p>Execute a pipeline from a YAML configuration file.</p> <pre><code>odibi run config.yaml\n</code></pre>"},{"location":"features/cli/#options","title":"Options","text":"Flag Description <code>--env</code> Environment to use (default: <code>development</code>) <code>--dry-run</code> Simulate execution without writing data <code>--resume</code> Resume from last failure (skip successful nodes) <code>--parallel</code> Run independent nodes in parallel <code>--workers</code> Number of worker threads for parallel execution (default: 4) <code>--on-error</code> Override error handling: <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code>"},{"location":"features/cli/#examples","title":"Examples","text":"<pre><code># Basic execution\nodibi run my_pipeline.yaml\n\n# Production run with parallel execution\nodibi run my_pipeline.yaml --env production --parallel --workers 8\n\n# Test without writing data\nodibi run my_pipeline.yaml --dry-run\n\n# Resume a failed run\nodibi run my_pipeline.yaml --resume\n</code></pre>"},{"location":"features/cli/#odibi-validate","title":"odibi validate","text":"<p>Validate a YAML configuration file for syntax and logical errors.</p> <pre><code>odibi validate config.yaml\n</code></pre> <p>Validation checks include: - YAML syntax - Required fields - Connection references - Transform function existence - Node dependency cycles</p>"},{"location":"features/cli/#example-output","title":"Example Output","text":"<pre><code>[OK] Config is valid\n</code></pre> <p>Or with errors:</p> <pre><code>[!] Pipeline 'process_orders' Errors:\n  - Node 'transform_orders' references unknown connection: missing_db\n  - Circular dependency detected: nodeA -&gt; nodeB -&gt; nodeA\n\n[X] Validation failed\n</code></pre>"},{"location":"features/cli/#odibi-catalog","title":"odibi catalog","text":"<p>Query the System Catalog for execution metadata, registered pipelines, and statistics.</p> <pre><code>odibi catalog &lt;command&gt; config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands","title":"Subcommands","text":"Subcommand Description <code>runs</code> List execution runs from <code>meta_runs</code> <code>pipelines</code> List registered pipelines from <code>meta_pipelines</code> <code>nodes</code> List registered nodes from <code>meta_nodes</code> <code>state</code> List HWM state checkpoints from <code>meta_state</code> <code>tables</code> List registered assets from <code>meta_tables</code> <code>metrics</code> List metrics definitions from <code>meta_metrics</code> <code>patterns</code> List pattern compliance from <code>meta_patterns</code> <code>stats</code> Show execution statistics"},{"location":"features/cli/#common-options","title":"Common Options","text":"Flag Description <code>--format</code>, <code>-f</code> Output format: <code>table</code> (default) or <code>json</code> <code>--pipeline</code>, <code>-p</code> Filter by pipeline name <code>--days</code>, <code>-d</code> Show data from last N days (default: 7) <code>--limit</code>, <code>-l</code> Maximum number of results (default: 20) <code>--status</code>, <code>-s</code> Filter by status: <code>SUCCESS</code>, <code>FAILED</code>, <code>RUNNING</code>"},{"location":"features/cli/#examples_1","title":"Examples","text":"<pre><code># List recent runs\nodibi catalog runs config.yaml\n\n# Filter runs by pipeline and status\nodibi catalog runs config.yaml --pipeline my_etl --status FAILED --days 14\n\n# List all registered pipelines\nodibi catalog pipelines config.yaml\n\n# View nodes for a specific pipeline\nodibi catalog nodes config.yaml --pipeline silver_pipeline\n\n# Check HWM state checkpoints\nodibi catalog state config.yaml\n\n# Get execution statistics\nodibi catalog stats config.yaml --days 30\n\n# Output as JSON\nodibi catalog runs config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output_1","title":"Example Output","text":"<pre><code>run_id     | pipeline_name | node_name      | status  | rows  | duration_ms | timestamp\n-----------+---------------+----------------+---------+-------+-------------+---------------------\nabc123     | bronze_etl    | ingest_orders  | SUCCESS | 15420 | 3250        | 2024-01-30 10:15:00\ndef456     | bronze_etl    | ingest_custo...| SUCCESS | 8932  | 2100        | 2024-01-30 10:14:00\n\nShowing 2 runs from the last 7 days.\n</code></pre>"},{"location":"features/cli/#odibi-lineage","title":"odibi lineage","text":"<p>Explore cross-pipeline data lineage and perform impact analysis.</p> <pre><code>odibi lineage &lt;command&gt; &lt;table&gt; --config config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands_1","title":"Subcommands","text":"Subcommand Description <code>upstream</code> Trace upstream sources of a table <code>downstream</code> Trace downstream consumers of a table <code>impact</code> Impact analysis for schema changes"},{"location":"features/cli/#options_1","title":"Options","text":"Flag Description <code>--config</code> Path to YAML config file (required) <code>--depth</code> Maximum depth to traverse (default: 3) <code>--format</code> Output format: <code>tree</code> (default) or <code>json</code>"},{"location":"features/cli/#examples_2","title":"Examples","text":"<pre><code># Trace upstream sources\nodibi lineage upstream gold/customer_360 --config config.yaml\n\n# Trace downstream consumers\nodibi lineage downstream bronze/customers_raw --config config.yaml\n\n# Impact analysis\nodibi lineage impact bronze/customers_raw --config config.yaml --depth 5\n\n# Output as JSON\nodibi lineage upstream gold/customer_360 --config config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output-upstream","title":"Example Output (upstream)","text":"<pre><code>Upstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre>"},{"location":"features/cli/#example-output-impact","title":"Example Output (impact)","text":"<pre><code>\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 3 downstream table(s) in 2 pipeline(s)\n</code></pre>"},{"location":"features/cli/#odibi-schema","title":"odibi schema","text":"<p>Track schema version history and compare schema changes over time.</p> <pre><code>odibi schema &lt;command&gt; &lt;table&gt; --config config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands_2","title":"Subcommands","text":"Subcommand Description <code>history</code> Show schema version history for a table <code>diff</code> Compare two schema versions"},{"location":"features/cli/#options_2","title":"Options","text":"Flag Description <code>--config</code> Path to YAML config file (required) <code>--limit</code> Maximum versions to show (default: 10) <code>--format</code> Output format: <code>table</code> (default) or <code>json</code> <code>--from-version</code> Source version number (for diff) <code>--to-version</code> Target version number (for diff)"},{"location":"features/cli/#examples_3","title":"Examples","text":"<pre><code># View schema history\nodibi schema history silver/customers --config config.yaml\n\n# Compare specific versions\nodibi schema diff silver/customers --config config.yaml --from-version 3 --to-version 5\n\n# Output history as JSON\nodibi schema history silver/customers --config config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output-history","title":"Example Output (history)","text":"<pre><code>Schema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre>"},{"location":"features/cli/#example-output-diff","title":"Example Output (diff)","text":"<pre><code>Schema Diff: silver/customers\nFrom v3 \u2192 v5\n============================================================\n+ loyalty_tier                   STRING               (added in v5)\n~ email                          VARCHAR \u2192 STRING\n- legacy_id                      INTEGER              (removed in v5)\n  customer_id                    INTEGER              (unchanged)\n  name                           STRING               (unchanged)\n</code></pre>"},{"location":"features/cli/#global-options","title":"Global Options","text":"<p>These options are available for all commands:</p> Flag Description <code>--log-level</code> Set logging verbosity: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> (default: <code>INFO</code>) <pre><code># Enable debug logging\nodibi run config.yaml --log-level DEBUG\n</code></pre>"},{"location":"features/cli/#examples_4","title":"Examples","text":""},{"location":"features/cli/#complete-workflow","title":"Complete Workflow","text":"<pre><code># 1. Validate configuration\nodibi validate my_pipeline.yaml\n\n# 2. Dry run to test logic\nodibi run my_pipeline.yaml --dry-run\n\n# 3. Execute pipeline\nodibi run my_pipeline.yaml --env production --parallel\n\n# 4. Check execution results\nodibi catalog runs my_pipeline.yaml --days 1\n\n# 5. View statistics\nodibi catalog stats my_pipeline.yaml --pipeline bronze_etl\n</code></pre>"},{"location":"features/cli/#debugging-a-failed-pipeline","title":"Debugging a Failed Pipeline","text":"<pre><code># Check recent failures\nodibi catalog runs config.yaml --status FAILED --limit 10\n\n# Resume from failure\nodibi run config.yaml --resume\n\n# Enable verbose logging\nodibi run config.yaml --log-level DEBUG\n</code></pre>"},{"location":"features/cli/#schema-change-impact-assessment","title":"Schema Change Impact Assessment","text":"<pre><code># Check schema history before making changes\nodibi schema history bronze/customers_raw --config config.yaml\n\n# Assess downstream impact\nodibi lineage impact bronze/customers_raw --config config.yaml\n\n# After changes, verify schema was captured\nodibi schema history bronze/customers_raw --config config.yaml --limit 1\n</code></pre>"},{"location":"features/cli/#monitoring-pipeline-health","title":"Monitoring Pipeline Health","text":"<pre><code># Daily stats check\nodibi catalog stats config.yaml --days 7\n\n# Find problematic nodes\nodibi catalog runs config.yaml --status FAILED --days 30\n\n# Check state for incremental loads\nodibi catalog state config.yaml --pipeline my_incremental_etl\n</code></pre>"},{"location":"features/cli/#related","title":"Related","text":"<ul> <li>Getting Started - Getting started with Odibi</li> <li>CLI Master Guide - Comprehensive CLI reference</li> <li>System Catalog - Catalog metadata details</li> <li>YAML Schema Reference - Configuration reference</li> </ul>"},{"location":"features/configuration/","title":"Configuration System","text":"<p>YAML-based configuration for defining projects, pipelines, and nodes with built-in validation, environment variable support, and environment-specific overrides.</p>"},{"location":"features/configuration/#overview","title":"Overview","text":"<p>Odibi's configuration system provides: - YAML-based: Human-readable, version-controllable configuration files - Pydantic validation: Type-safe configuration with helpful error messages - Environment variables: Secure secret injection with <code>${VAR}</code> syntax - Environment overrides: Dev/staging/prod configurations in a single file - Hierarchical structure: Project \u2192 Pipelines \u2192 Nodes</p>"},{"location":"features/configuration/#project-configuration","title":"Project Configuration","text":"<p><code>ProjectConfig</code> is the root configuration defining the entire Odibi project.</p>"},{"location":"features/configuration/#required-fields","title":"Required Fields","text":"Field Type Description <code>project</code> string Project name <code>connections</code> object Named connections (at least one required) <code>pipelines</code> list Pipeline definitions (at least one required) <code>story</code> object Story generation configuration <code>system</code> object System Catalog configuration"},{"location":"features/configuration/#optional-fields","title":"Optional Fields","text":"Field Type Default Description <code>engine</code> string <code>pandas</code> Execution engine: <code>spark</code>, <code>pandas</code> <code>version</code> string <code>1.0.0</code> Project version <code>description</code> string - Project description <code>owner</code> string - Project owner/contact <code>vars</code> object <code>{}</code> Global variables for substitution <code>retry</code> object See below Retry configuration <code>logging</code> object See below Logging configuration <code>alerts</code> list <code>[]</code> Alert configurations <code>performance</code> object See below Performance tuning <code>lineage</code> object - OpenLineage configuration <code>environments</code> object - Environment-specific overrides"},{"location":"features/configuration/#basic-example","title":"Basic Example","text":"<pre><code>project: \"Customer360\"\nengine: \"spark\"\nversion: \"1.0.0\"\n\nconnections:\n  bronze:\n    type: \"local\"\n    base_path: \"./data/bronze\"\n  silver:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"silver_db\"\n\nstory:\n  connection: \"bronze\"\n  path: \"stories/\"\n  retention_days: 30\n\nsystem:\n  connection: \"bronze\"\n  path: \"_odibi_system\"\n\npipelines:\n  - pipeline: \"customer_ingestion\"\n    nodes:\n      - name: \"load_customers\"\n        read: { connection: \"bronze\", format: \"csv\", path: \"customers.csv\" }\n        write: { connection: \"silver\", table: \"customers\" }\n</code></pre>"},{"location":"features/configuration/#retry-configuration","title":"Retry Configuration","text":"<pre><code>retry:\n  enabled: true\n  max_attempts: 3        # 1-10\n  backoff: \"exponential\" # exponential, linear, constant\n</code></pre>"},{"location":"features/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>logging:\n  level: \"INFO\"          # DEBUG, INFO, WARNING, ERROR\n  structured: true       # JSON logs for Splunk/Datadog\n  metadata:\n    team: \"data-platform\"\n</code></pre>"},{"location":"features/configuration/#performance-configuration","title":"Performance Configuration","text":"<pre><code>performance:\n  use_arrow: true  # Use Apache Arrow-backed DataFrames (Pandas only)\n</code></pre>"},{"location":"features/configuration/#story-configuration","title":"Story Configuration","text":"<pre><code>story:\n  connection: \"bronze\"        # Must exist in connections\n  path: \"stories/\"            # Path relative to connection\n  max_sample_rows: 10         # 0-100\n  auto_generate: true\n  retention_days: 30          # Days to keep stories\n  retention_count: 100        # Max stories to keep\n</code></pre>"},{"location":"features/configuration/#system-configuration","title":"System Configuration","text":"<pre><code>system:\n  connection: \"bronze\"        # Must exist in connections\n  path: \"_odibi_system\"       # Path relative to connection root\n</code></pre>"},{"location":"features/configuration/#lineage-configuration","title":"Lineage Configuration","text":"<pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n  api_key: \"${LINEAGE_API_KEY}\"\n</code></pre>"},{"location":"features/configuration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p><code>PipelineConfig</code> groups related nodes into a logical unit.</p> Field Type Required Description <code>pipeline</code> string Yes Pipeline name <code>description</code> string No Pipeline description <code>layer</code> string No Logical layer: <code>bronze</code>, <code>silver</code>, <code>gold</code> <code>nodes</code> list Yes List of nodes (unique names required) <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"load_users\"\n        # ...\n      - name: \"clean_users\"\n        depends_on: [\"load_users\"]\n        # ...\n</code></pre>"},{"location":"features/configuration/#node-configuration","title":"Node Configuration","text":"<p><code>NodeConfig</code> defines individual data processing steps.</p>"},{"location":"features/configuration/#core-fields","title":"Core Fields","text":"Field Type Required Description <code>name</code> string Yes Unique node name <code>description</code> string No Human-readable description <code>enabled</code> bool No If <code>false</code>, node is skipped (default: <code>true</code>) <code>tags</code> list No Tags for selective execution (<code>odibi run --tag daily</code>) <code>depends_on</code> list No Parent nodes that must complete first"},{"location":"features/configuration/#operations-at-least-one-required","title":"Operations (at least one required)","text":"Field Type Description <code>read</code> object Input operation (load data) <code>transformer</code> string Built-in transformation app (e.g., <code>deduplicate</code>, <code>scd2</code>) <code>params</code> object Parameters for transformer <code>transform</code> object Chain of transformation steps <code>write</code> object Output operation (save data)"},{"location":"features/configuration/#execution-order","title":"Execution Order","text":"<ol> <li>Read (or dependency injection if no read block)</li> <li>Transformer (the \"App\" logic)</li> <li>Transform Steps (the \"Script\" logic)</li> <li>Validation</li> <li>Write</li> </ol>"},{"location":"features/configuration/#read-configuration","title":"Read Configuration","text":"<pre><code>read:\n  connection: \"bronze\"\n  format: \"parquet\"           # csv, parquet, delta, json, sql\n  path: \"customers/\"\n  # OR for SQL\n  query: \"SELECT * FROM customers WHERE active = 1\"\n\n  # Incremental loading\n  incremental:\n    mode: \"rolling_window\"    # or \"stateful\"\n    column: \"updated_at\"\n    lookback: 3\n    unit: \"day\"\n\n  # Time travel (Delta)\n  time_travel:\n    as_of_version: 10\n    # OR as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre>"},{"location":"features/configuration/#transform-configuration","title":"Transform Configuration","text":"<pre><code>transform:\n  steps:\n    # SQL step\n    - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n    # Function step\n    - function: \"clean_text\"\n      params:\n        columns: [\"email\"]\n        case: \"lower\"\n\n    # Operation step\n    - operation: \"detect_deletes\"\n      params:\n        mode: \"sql_compare\"\n        keys: [\"customer_id\"]\n</code></pre>"},{"location":"features/configuration/#write-configuration","title":"Write Configuration","text":"<pre><code>write:\n  connection: \"silver\"\n  format: \"delta\"\n  table: \"customers\"\n  mode: \"upsert\"              # overwrite, append, upsert, append_once\n\n  # Metadata columns\n  add_metadata: true          # or selective: {extracted_at: true, source_file: false}\n</code></pre>"},{"location":"features/configuration/#validation-configuration","title":"Validation Configuration","text":"<pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id, email]\n      on_fail: quarantine     # fail, warn, quarantine\n\n    - type: unique\n      columns: [customer_id]\n\n    - type: accepted_values\n      column: status\n      values: [\"active\", \"inactive\", \"pending\"]\n\n    - type: custom_sql\n      sql: \"COUNT(*) FILTER (WHERE age &lt; 0) = 0\"\n      message: \"Negative ages found\"\n\n  quarantine:\n    connection: \"silver\"\n    path: \"quarantine/customers\"\n\n  gate:\n    require_pass_rate: 0.95   # Block if &lt; 95% pass\n</code></pre>"},{"location":"features/configuration/#contracts-pre-conditions","title":"Contracts (Pre-conditions)","text":"<pre><code>contracts:\n  - type: row_count\n    min: 1000\n    on_fail: fail\n\n  - type: freshness\n    column: \"updated_at\"\n    max_age_hours: 24\n\n  - type: schema\n    columns:\n      id: \"int\"\n      name: \"string\"\n</code></pre>"},{"location":"features/configuration/#privacy-configuration","title":"Privacy Configuration","text":"<pre><code>privacy:\n  enabled: true\n  rules:\n    - column: \"email\"\n      method: \"hash\"          # hash, mask, redact, fake\n    - column: \"ssn\"\n      method: \"mask\"\n      params:\n        pattern: \"XXX-XX-####\"\n</code></pre>"},{"location":"features/configuration/#error-handling","title":"Error Handling","text":"<pre><code>on_error: \"fail_later\"        # fail_fast, fail_later, ignore\n</code></pre> Strategy Description <code>fail_fast</code> Stop pipeline immediately on error <code>fail_later</code> Continue pipeline, skip dependents (default) <code>ignore</code> Treat as success with warning, dependents run"},{"location":"features/configuration/#complete-node-example","title":"Complete Node Example","text":"<pre><code>- name: \"process_orders\"\n  description: \"Clean and deduplicate orders\"\n  tags: [\"daily\", \"critical\"]\n  depends_on: [\"load_orders\"]\n\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"order_id\"]\n    order_by: \"updated_at DESC\"\n\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status != 'cancelled'\"\n\n  validation:\n    tests:\n      - type: not_null\n        columns: [order_id, customer_id]\n        on_fail: quarantine\n    quarantine:\n      connection: \"silver\"\n      path: \"quarantine/orders\"\n    gate:\n      require_pass_rate: 0.98\n\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"orders_clean\"\n    mode: \"upsert\"\n\n  on_error: \"fail_fast\"\n  cache: true\n  log_level: \"DEBUG\"\n</code></pre>"},{"location":"features/configuration/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR_NAME}</code> syntax to inject environment variables:</p> <pre><code>connections:\n  azure_blob:\n    type: \"azure_blob\"\n    account_name: \"myaccount\"\n    container: \"data\"\n    auth:\n      mode: \"account_key\"\n      account_key: \"${AZURE_STORAGE_KEY}\"\n\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n</code></pre> <p>Variables are resolved at configuration load time. Missing variables raise an error.</p>"},{"location":"features/configuration/#global-variables","title":"Global Variables","text":"<p>Define reusable variables in <code>vars</code>:</p> <pre><code>vars:\n  env: \"production\"\n  team: \"data-platform\"\n\nlogging:\n  metadata:\n    environment: \"${vars.env}\"\n    team: \"${vars.team}\"\n</code></pre>"},{"location":"features/configuration/#environment-overrides","title":"Environment Overrides","text":"<p>Define environment-specific configurations that override base settings:</p> <pre><code>project: \"Customer360\"\nengine: \"pandas\"\n\nconnections:\n  database:\n    type: \"sql_server\"\n    host: \"dev-server.database.windows.net\"\n    database: \"dev_db\"\n\nenvironments:\n  staging:\n    connections:\n      database:\n        host: \"staging-server.database.windows.net\"\n        database: \"staging_db\"\n\n  production:\n    engine: \"spark\"\n    connections:\n      database:\n        host: \"prod-server.database.windows.net\"\n        database: \"prod_db\"\n    logging:\n      level: \"WARNING\"\n      structured: true\n</code></pre> <p>Select environment at runtime:</p> <pre><code>odibi run --env production\n</code></pre>"},{"location":"features/configuration/#validation","title":"Validation","text":"<p>Odibi uses Pydantic for configuration validation, providing:</p>"},{"location":"features/configuration/#type-checking","title":"Type Checking","text":"<pre><code># This will fail: max_attempts must be integer 1-10\nretry:\n  max_attempts: 100  # Error: ensure this value is less than or equal to 10\n</code></pre>"},{"location":"features/configuration/#required-field-validation","title":"Required Field Validation","text":"<pre><code># This will fail: 'project' is required\nengine: \"spark\"\npipelines: []\n# Error: field required - project\n</code></pre>"},{"location":"features/configuration/#cross-field-validation","title":"Cross-Field Validation","text":"<pre><code># This will fail: story.connection must exist in connections\nconnections:\n  bronze:\n    type: \"local\"\n    base_path: \"./data\"\n\nstory:\n  connection: \"silver\"  # Error: Story connection 'silver' not found\n  path: \"stories/\"\n</code></pre>"},{"location":"features/configuration/#node-validation","title":"Node Validation","text":"<pre><code># This will fail: node must have at least one operation\n- name: \"empty_node\"\n  # Error: Node 'empty_node' must have at least one of: read, transform, write, transformer\n</code></pre>"},{"location":"features/configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from odibi.config import load_config_from_file, ProjectConfig\n\n# From file (with env var substitution)\nconfig = load_config_from_file(\"odibi.yaml\")\n\n# From dict (programmatic)\nconfig = ProjectConfig(\n    project=\"MyProject\",\n    connections={\"local\": {\"type\": \"local\", \"base_path\": \"./data\"}},\n    pipelines=[...],\n    story={\"connection\": \"local\", \"path\": \"stories/\"},\n    system={\"connection\": \"local\"},\n)\n</code></pre>"},{"location":"features/configuration/#complete-example","title":"Complete Example","text":"<pre><code>project: \"E-Commerce Analytics\"\nversion: \"2.0.0\"\nengine: \"spark\"\nowner: \"data-team@company.com\"\n\nvars:\n  env: \"production\"\n\n# Resilience\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n\n# Observability\nlogging:\n  level: \"INFO\"\n  structured: true\n  metadata:\n    environment: \"${vars.env}\"\n\n# Alerting\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n\n# Performance\nperformance:\n  use_arrow: true\n\n# Lineage\nlineage:\n  url: \"http://marquez:5000\"\n  namespace: \"ecommerce\"\n\n# Connections\nconnections:\n  landing:\n    type: \"azure_blob\"\n    account_name: \"datalake\"\n    container: \"landing\"\n    auth:\n      mode: \"aad_msi\"\n\n  bronze:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"bronze\"\n\n  silver:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"silver\"\n\n  gold:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"gold\"\n\n# Story output\nstory:\n  connection: \"bronze\"\n  path: \"_stories/\"\n  max_sample_rows: 10\n  retention_days: 30\n\n# System catalog\nsystem:\n  connection: \"bronze\"\n  path: \"_odibi_system\"\n\n# Pipelines\npipelines:\n  - pipeline: \"orders_bronze\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: \"landing\"\n          format: \"json\"\n          path: \"orders/*.json\"\n          incremental:\n            mode: \"stateful\"\n            column: \"order_date\"\n        write:\n          connection: \"bronze\"\n          table: \"raw_orders\"\n          mode: \"append\"\n          add_metadata: true\n\n  - pipeline: \"orders_silver\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_orders\"\n        depends_on: [\"ingest_orders\"]\n\n        transformer: \"deduplicate\"\n        params:\n          keys: [\"order_id\"]\n          order_by: \"updated_at DESC\"\n\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_total &gt; 0\"\n            - function: \"clean_text\"\n              params:\n                columns: [\"customer_email\"]\n                case: \"lower\"\n\n        validation:\n          tests:\n            - type: not_null\n              columns: [order_id, customer_id]\n              on_fail: quarantine\n            - type: range\n              column: \"order_total\"\n              min: 0\n          quarantine:\n            connection: \"silver\"\n            path: \"quarantine/orders\"\n          gate:\n            require_pass_rate: 0.95\n\n        write:\n          connection: \"silver\"\n          table: \"orders\"\n          mode: \"upsert\"\n\n# Environment overrides\nenvironments:\n  dev:\n    engine: \"pandas\"\n    logging:\n      level: \"DEBUG\"\n    connections:\n      landing:\n        type: \"local\"\n        base_path: \"./test_data/landing\"\n      bronze:\n        type: \"local\"\n        base_path: \"./test_data/bronze\"\n</code></pre>"},{"location":"features/configuration/#related","title":"Related","text":"<ul> <li>YAML Schema Reference - Complete field reference</li> <li>Alerting - Alert configuration details</li> <li>Quality Gates - Validation and gates</li> <li>Quarantine Tables - Quarantine configuration</li> </ul>"},{"location":"features/connections/","title":"Connections","text":"<p>Unified connection system for accessing local filesystems, cloud storage, databases, and HTTP endpoints with pluggable authentication.</p>"},{"location":"features/connections/#overview","title":"Overview","text":"<p>Odibi's connection system provides: - Multiple backends: Local filesystem, Azure ADLS, Azure SQL, HTTP APIs - Flexible authentication: Service principals, managed identity, Key Vault, connection strings - Environment variables: Secure secret injection via <code>${VAR}</code> syntax - Plugin architecture: Register custom connection types via factory pattern</p>"},{"location":"features/connections/#built-in-connection-types","title":"Built-in Connection Types","text":"Type Description <code>local</code> Local filesystem or URI-based paths <code>local_dbfs</code> Databricks File System mock for local development <code>azure_adls</code> Azure Data Lake Storage Gen2 <code>azure_sql</code> Azure SQL Database <code>http</code> HTTP/REST API endpoints <code>delta</code> Delta Lake tables (path-based or catalog)"},{"location":"features/connections/#configuration","title":"Configuration","text":""},{"location":"features/connections/#basic-structure","title":"Basic Structure","text":"<pre><code>connections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n\n  silver:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    path_prefix: silver\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-key\n</code></pre>"},{"location":"features/connections/#connection-config-options","title":"Connection Config Options","text":"Field Type Required Description <code>type</code> string Yes Connection type (see table above) <code>auth</code> object No Authentication configuration <code>auth_mode</code> string No Authentication mode (auto-detected if omitted) <code>validation_mode</code> string No <code>eager</code> or <code>lazy</code> validation (default: <code>lazy</code>)"},{"location":"features/connections/#local-connection","title":"Local Connection","text":"<p>Simple filesystem connection for local development or mounted volumes.</p> <pre><code>connections:\n  raw_data:\n    type: local\n    base_path: ./data/raw\n\n  mounted_volume:\n    type: local\n    base_path: /mnt/storage/data\n</code></pre>"},{"location":"features/connections/#uri-based-paths","title":"URI-Based Paths","text":"<p>Supports URI schemes like <code>file://</code> or <code>dbfs:/</code>:</p> <pre><code>connections:\n  dbfs_data:\n    type: local\n    base_path: dbfs:/FileStore/data\n</code></pre>"},{"location":"features/connections/#config-options","title":"Config Options","text":"Field Type Default Description <code>base_path</code> string <code>./data</code> Base directory for all paths"},{"location":"features/connections/#local-dbfs-connection","title":"Local DBFS Connection","text":"<p>Mock DBFS for testing Databricks pipelines locally.</p> <pre><code>connections:\n  dbfs:\n    type: local_dbfs\n    root: .dbfs\n</code></pre> <p>Maps <code>dbfs:/FileStore/data.csv</code> to <code>.dbfs/FileStore/data.csv</code>.</p>"},{"location":"features/connections/#config-options_1","title":"Config Options","text":"Field Type Default Description <code>root</code> string <code>.dbfs</code> Local directory to use as DBFS root"},{"location":"features/connections/#azure-data-lake-storage-adls-connection","title":"Azure Data Lake Storage (ADLS) Connection","text":"<p>Azure Data Lake Storage Gen2 with multi-mode authentication.</p> <pre><code>connections:\n  datalake:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: datalake\n    path_prefix: bronze\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n</code></pre>"},{"location":"features/connections/#config-options_2","title":"Config Options","text":"Field Type Required Description <code>account_name</code> string Yes Storage account name <code>container</code> string Yes Container/filesystem name <code>path_prefix</code> string No Optional prefix for all paths <code>auth_mode</code> string No Authentication mode (auto-detected)"},{"location":"features/connections/#authentication-modes","title":"Authentication Modes","text":""},{"location":"features/connections/#key-vault-recommended","title":"Key Vault (Recommended)","text":"<p>Retrieves storage account key from Azure Key Vault:</p> <pre><code>connections:\n  secure_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n</code></pre>"},{"location":"features/connections/#service-principal","title":"Service Principal","text":"<p>OAuth authentication with Azure AD service principal:</p> <pre><code>connections:\n  sp_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: service_principal\n    auth:\n      tenant_id: ${AZURE_TENANT_ID}\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n</code></pre>"},{"location":"features/connections/#managed-identity","title":"Managed Identity","text":"<p>Use Azure Managed Identity (recommended for Azure-hosted workloads):</p> <pre><code>connections:\n  msi_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: managed_identity\n</code></pre>"},{"location":"features/connections/#sas-token","title":"SAS Token","text":"<p>Shared Access Signature for time-limited access:</p> <pre><code>connections:\n  sas_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: sas_token\n    auth:\n      sas_token: ${STORAGE_SAS_TOKEN}\n</code></pre>"},{"location":"features/connections/#direct-key-development-only","title":"Direct Key (Development Only)","text":"<p>\u26a0\ufe0f Not recommended for production</p> <pre><code>connections:\n  dev_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: direct_key\n    auth:\n      account_key: ${STORAGE_ACCOUNT_KEY}\n</code></pre>"},{"location":"features/connections/#path-resolution","title":"Path Resolution","text":"<p>ADLS connections generate <code>abfss://</code> URIs:</p> <pre><code>conn.get_path(\"folder/file.parquet\")\n# Returns: abfss://data@mystorageaccount.dfs.core.windows.net/bronze/folder/file.parquet\n</code></pre>"},{"location":"features/connections/#azure-sql-connection","title":"Azure SQL Connection","text":"<p>Azure SQL Database with SQL auth, Managed Identity, or Key Vault.</p> <pre><code>connections:\n  warehouse:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: analytics\n    auth_mode: aad_msi\n</code></pre>"},{"location":"features/connections/#config-options_3","title":"Config Options","text":"Field Type Default Description <code>host</code> / <code>server</code> string Required SQL Server hostname <code>database</code> string Required Database name <code>driver</code> string <code>ODBC Driver 18 for SQL Server</code> ODBC driver <code>port</code> int <code>1433</code> SQL Server port <code>timeout</code> int <code>30</code> Connection timeout (seconds) <code>auth_mode</code> string Auto <code>sql</code>, <code>aad_msi</code>, <code>key_vault</code>"},{"location":"features/connections/#authentication-modes_1","title":"Authentication Modes","text":""},{"location":"features/connections/#sql-authentication","title":"SQL Authentication","text":"<pre><code>connections:\n  sql_auth:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: sql\n    auth:\n      username: ${SQL_USERNAME}\n      password: ${SQL_PASSWORD}\n</code></pre>"},{"location":"features/connections/#managed-identity_1","title":"Managed Identity","text":"<pre><code>connections:\n  msi_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: aad_msi\n</code></pre>"},{"location":"features/connections/#key-vault","title":"Key Vault","text":"<pre><code>connections:\n  keyvault_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: key_vault\n    auth:\n      username: sqladmin\n      key_vault_name: my-keyvault\n      secret_name: sql-password\n</code></pre>"},{"location":"features/connections/#usage","title":"Usage","text":"<pre><code>from odibi.connections.azure_sql import AzureSQL\n\nconn = AzureSQL(\n    server=\"myserver.database.windows.net\",\n    database=\"analytics\",\n    auth_mode=\"aad_msi\",\n)\n\n# Read data\ndf = conn.read_sql(\"SELECT * FROM customers WHERE region = 'US'\")\n\n# Read entire table\ndf = conn.read_table(\"orders\", schema=\"dbo\")\n\n# Write data\nconn.write_table(df, \"processed_orders\", if_exists=\"replace\")\n\n# Execute statements\nconn.execute(\"DELETE FROM staging WHERE processed = 1\")\n</code></pre>"},{"location":"features/connections/#http-connection","title":"HTTP Connection","text":"<p>Connect to REST APIs with various authentication methods.</p> <pre><code>connections:\n  api:\n    type: http\n    base_url: https://api.example.com/v1/\n    auth:\n      token: ${API_TOKEN}\n</code></pre>"},{"location":"features/connections/#config-options_4","title":"Config Options","text":"Field Type Required Description <code>base_url</code> string Yes Base URL for API <code>headers</code> object No Default request headers <code>auth</code> object No Authentication configuration"},{"location":"features/connections/#authentication-methods","title":"Authentication Methods","text":""},{"location":"features/connections/#bearer-token","title":"Bearer Token","text":"<pre><code>connections:\n  bearer_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      token: ${API_BEARER_TOKEN}\n</code></pre>"},{"location":"features/connections/#basic-auth","title":"Basic Auth","text":"<pre><code>connections:\n  basic_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      username: ${API_USER}\n      password: ${API_PASSWORD}\n</code></pre>"},{"location":"features/connections/#api-key","title":"API Key","text":"<pre><code>connections:\n  apikey_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      api_key: ${API_KEY}\n      header_name: X-API-Key  # Optional, defaults to X-API-Key\n</code></pre>"},{"location":"features/connections/#custom-headers","title":"Custom Headers","text":"<pre><code>connections:\n  custom_api:\n    type: http\n    base_url: https://api.example.com/\n    headers:\n      Content-Type: application/json\n      X-Custom-Header: custom-value\n    auth:\n      token: ${API_TOKEN}\n</code></pre>"},{"location":"features/connections/#delta-connection","title":"Delta Connection","text":"<p>Delta Lake tables via path or Unity Catalog.</p>"},{"location":"features/connections/#path-based-delta","title":"Path-Based Delta","text":"<pre><code>connections:\n  delta_lake:\n    type: delta\n    path: /mnt/delta/tables\n</code></pre>"},{"location":"features/connections/#catalog-based-delta-spark","title":"Catalog-Based Delta (Spark)","text":"<pre><code>connections:\n  unity_catalog:\n    type: delta\n    catalog: main\n    schema: analytics\n</code></pre>"},{"location":"features/connections/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR}</code> syntax to inject secrets from environment variables:</p> <pre><code>connections:\n  secure:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: data\n    auth:\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n      tenant_id: ${AZURE_TENANT_ID}\n</code></pre> <p>Environment variables are resolved at runtime, keeping secrets out of configuration files.</p>"},{"location":"features/connections/#connection-factory","title":"Connection Factory","text":"<p>Odibi uses a plugin system for connection types. Built-in types are registered automatically.</p>"},{"location":"features/connections/#registering-custom-connections","title":"Registering Custom Connections","text":"<pre><code>from odibi.plugins import register_connection_factory\nfrom odibi.connections.base import BaseConnection\n\nclass MyCustomConnection(BaseConnection):\n    def __init__(self, endpoint: str, api_key: str):\n        self.endpoint = endpoint\n        self.api_key = api_key\n\n    def get_path(self, relative_path: str) -&gt; str:\n        return f\"{self.endpoint}/{relative_path}\"\n\n    def validate(self) -&gt; None:\n        if not self.endpoint:\n            raise ValueError(\"Endpoint is required\")\n\ndef create_custom_connection(name: str, config: dict):\n    return MyCustomConnection(\n        endpoint=config[\"endpoint\"],\n        api_key=config.get(\"api_key\", \"\"),\n    )\n\n# Register the factory\nregister_connection_factory(\"my_custom\", create_custom_connection)\n</code></pre> <p>Then use in YAML:</p> <pre><code>connections:\n  custom:\n    type: my_custom\n    endpoint: https://custom-service.example.com\n    api_key: ${CUSTOM_API_KEY}\n</code></pre>"},{"location":"features/connections/#built-in-factory-registration","title":"Built-in Factory Registration","text":"<p>Built-in connections are registered via <code>register_builtins()</code>:</p> Factory Name Connection Class <code>local</code> <code>LocalConnection</code> <code>http</code> <code>HttpConnection</code> <code>azure_blob</code> <code>AzureADLS</code> <code>azure_adls</code> <code>AzureADLS</code> <code>delta</code> <code>LocalConnection</code> or <code>DeltaCatalogConnection</code> <code>sql_server</code> <code>AzureSQL</code> <code>azure_sql</code> <code>AzureSQL</code>"},{"location":"features/connections/#complete-examples","title":"Complete Examples","text":""},{"location":"features/connections/#multi-environment-setup","title":"Multi-Environment Setup","text":"<pre><code>project: DataPipeline\nengine: spark\n\nconnections:\n  # Local development\n  local_bronze:\n    type: local\n    base_path: ./data/bronze\n\n  local_silver:\n    type: local\n    base_path: ./data/silver\n\n  # Azure production\n  azure_bronze:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    path_prefix: bronze\n    auth_mode: managed_identity\n\n  azure_silver:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    path_prefix: silver\n    auth_mode: managed_identity\n\n  # SQL database\n  warehouse:\n    type: azure_sql\n    host: ${SQL_SERVER}\n    database: analytics\n    auth_mode: aad_msi\n\n  # External API\n  weather_api:\n    type: http\n    base_url: https://api.weather.com/v1/\n    auth:\n      api_key: ${WEATHER_API_KEY}\n\npipelines:\n  - pipeline: ingest_orders\n    nodes:\n      - name: read_orders\n        source:\n          connection: azure_bronze\n          path: orders/\n        # ...\n</code></pre>"},{"location":"features/connections/#service-principal-authentication","title":"Service Principal Authentication","text":"<pre><code>connections:\n  adls_sp:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    path_prefix: ingestion\n    auth_mode: service_principal\n    auth:\n      tenant_id: ${AZURE_TENANT_ID}\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n\n  sql_sp:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: warehouse\n    auth_mode: sql\n    auth:\n      username: ${SQL_USER}\n      password: ${SQL_PASSWORD}\n</code></pre>"},{"location":"features/connections/#key-vault-integration","title":"Key Vault Integration","text":"<pre><code>connections:\n  secure_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: sensitive-data\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n\n  secure_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: secure_db\n    auth_mode: key_vault\n    auth:\n      username: sqladmin\n      key_vault_name: my-keyvault\n      secret_name: sql-admin-password\n</code></pre>"},{"location":"features/connections/#best-practices","title":"Best Practices","text":"<ol> <li>Use Managed Identity - Preferred for Azure-hosted workloads (no secrets to manage)</li> <li>Use Key Vault - Store secrets in Key Vault, not config files</li> <li>Environment variables - Use <code>${VAR}</code> for any sensitive values</li> <li>Lazy validation - Default <code>validation_mode: lazy</code> defers validation until first use</li> <li>Separate connections - Use different connections for different security zones</li> <li>Register secrets - Secrets are automatically registered for log redaction</li> </ol>"},{"location":"features/connections/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/connections/#connection-not-found-error","title":"\"Connection not found\" error","text":"<p>Symptom: <code>ConnectionError: Connection 'my_conn' not found</code></p> <p>Causes: - Typo in connection name (check spelling, case-sensitive) - Connection defined in wrong environment block - YAML indentation error</p> <p>Fix:</p> <pre><code># Validate your config\nodibi validate config.yaml\n</code></pre>"},{"location":"features/connections/#azure-authentication-failures","title":"Azure authentication failures","text":"<p>Symptom: <code>AuthenticationError: DefaultAzureCredential failed</code></p> <p>Causes: - Service principal credentials incorrect or expired - Managed Identity not enabled on compute - Missing RBAC permissions on storage account</p> <p>Fixes:</p> <pre><code># Check if Azure CLI is authenticated\naz account show\n\n# For Service Principal, verify credentials\naz login --service-principal -u $CLIENT_ID -p $CLIENT_SECRET --tenant $TENANT_ID\n\n# For Managed Identity, ensure it's enabled and has Storage Blob Data Contributor role\n</code></pre>"},{"location":"features/connections/#path-not-found-on-azure-adls","title":"\"Path not found\" on Azure ADLS","text":"<p>Symptom: File reads fail with path errors</p> <p>Causes: - Container name missing or incorrect - Path prefix doesn't match actual structure - SAS token doesn't have read permissions</p> <p>Fix: Verify the full path:</p> <pre><code>connections:\n  adls_data:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data          # Container name\n    path_prefix: bronze      # Prefix within container\n</code></pre> <p>The actual path read will be: <code>abfss://data@mystorageaccount.dfs.core.windows.net/bronze/&lt;your_path&gt;</code></p>"},{"location":"features/connections/#environment-variable-not-substituted","title":"Environment variable not substituted","text":"<p>Symptom: Literal <code>${VAR}</code> appears in logs or errors</p> <p>Causes: - Environment variable not set - Variable name typo - Running in wrong shell/environment</p> <p>Fix:</p> <pre><code># Check if variable is set\necho $MY_SECRET\n\n# Use odibi secrets to validate\nodibi secrets validate config.yaml\n</code></pre>"},{"location":"features/connections/#related","title":"Related","text":"<ul> <li>YAML Schema Reference</li> <li>Pipeline Configuration</li> <li>Secrets Management</li> </ul>"},{"location":"features/cross-pipeline-dependencies/","title":"Cross-Pipeline Dependencies","text":"<p>Last Updated: 2025-12-03 Status: \u2705 Implemented</p>"},{"location":"features/cross-pipeline-dependencies/#overview","title":"Overview","text":"<p>Cross-pipeline dependencies enable pipelines to reference outputs from other pipelines using the <code>$pipeline.node</code> syntax. This is essential for implementing the medallion architecture pattern where silver nodes depend on bronze outputs, and gold nodes depend on silver outputs.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Bronze    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Silver    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    Gold     \u2502\n\u2502  Pipeline   \u2502     \u2502  Pipeline   \u2502     \u2502  Pipeline   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n   writes to          reads from          reads from\n   meta_outputs       $read_bronze.*      $transform_silver.*\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#use-cases","title":"Use Cases","text":""},{"location":"features/cross-pipeline-dependencies/#1-medallion-architecture-bronze-silver-gold","title":"1. Medallion Architecture (Bronze \u2192 Silver \u2192 Gold)","text":"<p>The most common pattern: ingest raw data in bronze, clean/enrich in silver, aggregate for business in gold.</p> <pre><code># Bronze: Raw ingestion\npipeline: read_bronze\nnodes:\n  - name: raw_orders\n    read:\n      connection: source_db\n      table: sales.orders\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"bronze/orders\"\n\n# Silver: Enriched data\npipeline: transform_silver\nnodes:\n  - name: enriched_orders\n    inputs:\n      orders: $read_bronze.raw_orders        # \u2190 Cross-pipeline reference\n      customers: $read_bronze.raw_customers\n    transform:\n      steps:\n        - operation: join\n          left: orders\n          right: customers\n          on: [customer_id]\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"silver/enriched_orders\"\n\n# Gold: Business aggregates\npipeline: build_gold\nnodes:\n  - name: daily_sales\n    inputs:\n      orders: $transform_silver.enriched_orders\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              DATE(order_date) as date,\n              SUM(amount) as total_sales\n            FROM orders\n            GROUP BY 1\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"gold/daily_sales\"\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#2-multi-source-joins","title":"2. Multi-Source Joins","text":"<p>When a node needs to join data from multiple sources:</p> <pre><code>- name: enriched_downtime\n  inputs:\n    events: $read_bronze.shift_events\n    calendar: $read_bronze.calendar_dim\n    plant: $read_bronze.plant_dim\n  transform:\n    steps:\n      - operation: join\n        left: events\n        right: calendar\n        on: [date_id]\n      - operation: join\n        right: plant\n        on: [plant_id]\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#3-mixing-references-with-explicit-reads","title":"3. Mixing References with Explicit Reads","text":"<p>You can combine cross-pipeline references with explicit read configs:</p> <pre><code>- name: combined_data\n  inputs:\n    # Cross-pipeline reference\n    events: $read_bronze.events\n\n    # Explicit read (for data not from another pipeline)\n    reference_data:\n      connection: static_files\n      path: \"reference/lookup_table.csv\"\n      format: csv\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#yaml-syntax","title":"YAML Syntax","text":""},{"location":"features/cross-pipeline-dependencies/#the-inputs-block","title":"The <code>inputs</code> Block","text":"<pre><code>nodes:\n  - name: node_name\n    inputs:\n      &lt;input_name&gt;: $&lt;pipeline_name&gt;.&lt;node_name&gt;    # Cross-pipeline reference\n      &lt;input_name&gt;:                                  # Explicit read config\n        connection: &lt;connection_name&gt;\n        path: &lt;path&gt;\n        format: &lt;format&gt;\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#reference-syntax-pipelinenode","title":"Reference Syntax: <code>$pipeline.node</code>","text":"Component Description <code>$</code> Prefix indicating a cross-pipeline reference <code>pipeline</code> Name of the source pipeline (from <code>pipeline:</code> field) <code>.</code> Separator <code>node</code> Name of the source node (from <code>name:</code> field) <p>Examples: - <code>$read_bronze.orders</code> \u2192 Output from node <code>orders</code> in pipeline <code>read_bronze</code> - <code>$ingest_daily.customers</code> \u2192 Output from node <code>customers</code> in pipeline <code>ingest_daily</code></p>"},{"location":"features/cross-pipeline-dependencies/#how-the-meta_outputs-catalog-table-works","title":"How the <code>meta_outputs</code> Catalog Table Works","text":"<p>When a node with a <code>write</code> block completes, its output metadata is recorded in the system catalog.</p>"},{"location":"features/cross-pipeline-dependencies/#schema","title":"Schema","text":"Column Type Description <code>pipeline_name</code> STRING Pipeline identifier <code>node_name</code> STRING Node identifier <code>output_type</code> STRING <code>\"external_table\"</code> or <code>\"managed_table\"</code> <code>connection_name</code> STRING Connection used (for external tables) <code>path</code> STRING Storage path <code>format</code> STRING Data format (delta, parquet, etc.) <code>table_name</code> STRING Registered table name (if any) <code>last_run</code> TIMESTAMP Last execution time <code>row_count</code> LONG Row count at last write <code>updated_at</code> TIMESTAMP Record update time"},{"location":"features/cross-pipeline-dependencies/#resolution-flow","title":"Resolution Flow","text":"<pre><code>1. Silver node has: inputs: {events: $read_bronze.shift_events}\n\n2. At load time, Odibi queries meta_outputs:\n   SELECT * FROM meta_outputs\n   WHERE pipeline_name = 'read_bronze' AND node_name = 'shift_events'\n\n3. Returns: {connection: 'goat_prod', path: 'bronze/OEE/shift_events', format: 'delta'}\n\n4. At runtime: engine.read(connection='goat_prod', path='bronze/OEE/shift_events', format='delta')\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#performance-notes","title":"Performance Notes","text":""},{"location":"features/cross-pipeline-dependencies/#batch-writes-only","title":"Batch Writes Only","text":"<p>Output metadata is collected in-memory during pipeline execution and written to the catalog once at pipeline completion. This avoids per-node I/O overhead.</p> <p>Before optimization: 17 nodes \u00d7 ~2-3s = ~40s overhead After optimization: Single batch MERGE = ~2s total</p>"},{"location":"features/cross-pipeline-dependencies/#caching","title":"Caching","text":"<p>The <code>get_node_output()</code> method uses caching to avoid repeated catalog queries within the same session.</p>"},{"location":"features/cross-pipeline-dependencies/#validate-early","title":"Validate Early","text":"<p>All <code>$references</code> are validated at pipeline load time (fail fast), not at execution time. This provides immediate feedback if a referenced pipeline hasn't run.</p>"},{"location":"features/cross-pipeline-dependencies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/cross-pipeline-dependencies/#error-no-output-found-for-pipelinenode","title":"Error: \"No output found for $pipeline.node\"","text":"<pre><code>ReferenceResolutionError: No output found for $read_bronze.shift_events.\nEnsure pipeline 'read_bronze' has run and node 'shift_events' has a write block.\n</code></pre> <p>Causes: 1. The referenced pipeline hasn't been run yet 2. The referenced node doesn't have a <code>write</code> block 3. Typo in pipeline or node name</p> <p>Solutions: 1. Run the source pipeline first: <code>odibi run bronze.yaml</code> 2. Add a <code>write</code> block to the source node 3. Check spelling matches exactly (case-sensitive)</p>"},{"location":"features/cross-pipeline-dependencies/#error-cannot-have-both-read-and-inputs","title":"Error: \"Cannot have both 'read' and 'inputs'\"","text":"<pre><code>ValidationError: Node 'my_node': Cannot have both 'read' and 'inputs'.\nUse 'read' for single-source nodes or 'inputs' for multi-source cross-pipeline dependencies.\n</code></pre> <p>Solution: Choose one approach: - Use <code>read</code> for simple single-source reads - Use <code>inputs</code> for multi-source or cross-pipeline reads</p>"},{"location":"features/cross-pipeline-dependencies/#error-invalid-reference-format","title":"Error: \"Invalid reference format\"","text":"<pre><code>ValueError: Invalid reference format: $read_bronze. Expected $pipeline.node\n</code></pre> <p>Solution: Ensure the reference includes both pipeline and node names separated by a dot.</p>"},{"location":"features/cross-pipeline-dependencies/#engine-compatibility","title":"Engine Compatibility","text":"Feature Spark Pandas Polars <code>meta_outputs</code> writes \u2705 \u2705 \u2705 <code>$pipeline.node</code> (path-based) \u2705 \u2705 \u2705 <code>$pipeline.node</code> (managed table) \u2705 \u274c \u274c <code>inputs:</code> block \u2705 \u2705 \u2705 <p>Best Practice: Always use <code>path:</code> in write config for cross-engine compatibility.</p>"},{"location":"features/cross-pipeline-dependencies/#files-changed-in-implementation","title":"Files Changed in Implementation","text":"File Changes <code>odibi/catalog.py</code> Added <code>meta_outputs</code> table, <code>register_outputs_batch()</code>, <code>get_node_output()</code> <code>odibi/config.py</code> Added <code>inputs</code> field to <code>NodeConfig</code> <code>odibi/node.py</code> Added <code>_execute_inputs_phase()</code>, <code>_create_output_record()</code> <code>odibi/pipeline.py</code> Added batch output registration at pipeline end <code>odibi/references.py</code> New module for reference resolution <code>tests/unit/test_cross_pipeline_dependencies.py</code> 26 new tests"},{"location":"features/cross-pipeline-dependencies/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Reference - NodeConfig with <code>inputs</code> field</li> <li>YAML Schema Reference - Full schema documentation</li> <li>Catalog Feature - System catalog details</li> <li>Pipelines - Pipeline execution flow</li> </ul>"},{"location":"features/diagnostics/","title":"Diagnostics","text":"<p>Tools for debugging, monitoring, and comparing pipeline runs with Delta Lake version analysis and data diff capabilities.</p>"},{"location":"features/diagnostics/#overview","title":"Overview","text":"<p>Odibi's diagnostics module provides: - Delta Diagnostics: Table history, version comparison, metrics extraction - Data Diff: Row-level comparison, schema comparison, change detection - Run Comparison: Compare pipeline executions to identify drift - History Management: Access and analyze historical pipeline runs</p>"},{"location":"features/diagnostics/#delta-diagnostics","title":"Delta Diagnostics","text":""},{"location":"features/diagnostics/#table-version-comparison","title":"Table Version Comparison","text":"<p>Compare two versions of a Delta table to understand what changed:</p> <pre><code>from odibi.diagnostics import get_delta_diff\n\n# Basic comparison (metadata only)\ndiff = get_delta_diff(\n    table_path=\"/path/to/delta/table\",\n    version_a=5,\n    version_b=10,\n    spark=spark,  # Optional: use Spark or deltalake (Pandas)\n)\n\nprint(f\"Rows changed: {diff.rows_change}\")\nprint(f\"Files changed: {diff.files_change}\")\nprint(f\"Size change: {diff.size_change_bytes} bytes\")\nprint(f\"Operations: {diff.operations}\")\n</code></pre>"},{"location":"features/diagnostics/#deltadiffresult-fields","title":"DeltaDiffResult Fields","text":"Field Type Description <code>table_path</code> str Path to the Delta table <code>version_a</code> int Start version <code>version_b</code> int End version <code>rows_change</code> int Net row count change <code>files_change</code> int Net file count change <code>size_change_bytes</code> int Net size change in bytes <code>schema_added</code> List[str] Columns added between versions <code>schema_removed</code> List[str] Columns removed between versions <code>schema_current</code> List[str] Current schema columns <code>schema_previous</code> List[str] Previous schema columns <code>operations</code> List[str] Operations that occurred between versions"},{"location":"features/diagnostics/#deep-diff-mode","title":"Deep Diff Mode","text":"<p>Enable row-level comparison for detailed analysis:</p> <pre><code># Deep comparison with key-based diff\ndiff = get_delta_diff(\n    table_path=\"/path/to/delta/table\",\n    version_a=5,\n    version_b=10,\n    spark=spark,\n    deep=True,\n    keys=[\"order_id\"],  # Primary key columns for update detection\n)\n\nprint(f\"Rows added: {diff.rows_added}\")\nprint(f\"Rows removed: {diff.rows_removed}\")\nprint(f\"Rows updated: {diff.rows_updated}\")\n\n# Sample data\nprint(\"Added rows:\", diff.sample_added[:5])\nprint(\"Removed rows:\", diff.sample_removed[:5])\nprint(\"Updated rows:\", diff.sample_updated[:5])\n</code></pre>"},{"location":"features/diagnostics/#drift-detection","title":"Drift Detection","text":"<p>Automatically detect significant changes between versions:</p> <pre><code>from odibi.diagnostics import detect_drift\n\nwarning = detect_drift(\n    table_path=\"/path/to/delta/table\",\n    current_version=10,\n    baseline_version=5,\n    spark=spark,\n    threshold_pct=10.0,  # Alert if &gt;10% row count change\n)\n\nif warning:\n    print(f\"Drift detected: {warning}\")\n</code></pre> <p>Drift detection checks for: - Schema drift: Columns added or removed - Data volume drift: Row count changes exceeding threshold</p>"},{"location":"features/diagnostics/#data-diff","title":"Data Diff","text":""},{"location":"features/diagnostics/#node-comparison","title":"Node Comparison","text":"<p>Compare two executions of the same node:</p> <pre><code>from odibi.diagnostics import diff_nodes\n\ndiff = diff_nodes(node_a, node_b)\n\nprint(f\"Status change: {diff.status_change}\")\nprint(f\"Rows diff: {diff.rows_diff}\")\nprint(f\"Schema changed: {diff.schema_change}\")\nprint(f\"SQL changed: {diff.sql_changed}\")\nprint(f\"Has drift: {diff.has_drift}\")\n</code></pre>"},{"location":"features/diagnostics/#nodediffresult-fields","title":"NodeDiffResult Fields","text":"Field Type Description <code>node_name</code> str Name of the node <code>status_change</code> str Status change (e.g., \"success -&gt; failed\") <code>rows_out_a</code> int Output rows from run A <code>rows_out_b</code> int Output rows from run B <code>rows_diff</code> int Row count difference (B - A) <code>schema_change</code> bool Whether schema changed <code>columns_added</code> List[str] Columns added in run B <code>columns_removed</code> List[str] Columns removed in run B <code>sql_changed</code> bool Whether SQL logic changed <code>config_changed</code> bool Whether configuration changed <code>transformation_changed</code> bool Whether transformation stack changed <code>delta_version_change</code> str Delta version change (e.g., \"v1 -&gt; v2\") <code>has_drift</code> bool True if any significant drift occurred"},{"location":"features/diagnostics/#run-comparison","title":"Run Comparison","text":"<p>Compare two complete pipeline runs:</p> <pre><code>from odibi.diagnostics import diff_runs\n\nrun_diff = diff_runs(run_a, run_b)\n\nprint(f\"Nodes added: {run_diff.nodes_added}\")\nprint(f\"Nodes removed: {run_diff.nodes_removed}\")\nprint(f\"Drift sources: {run_diff.drift_source_nodes}\")\nprint(f\"Impacted downstream: {run_diff.impacted_downstream_nodes}\")\n\n# Examine individual node diffs\nfor name, node_diff in run_diff.node_diffs.items():\n    if node_diff.has_drift:\n        print(f\"  {name}: {node_diff.status_change or 'data drift'}\")\n</code></pre>"},{"location":"features/diagnostics/#rundiffresult-fields","title":"RunDiffResult Fields","text":"Field Type Description <code>run_id_a</code> str Run ID of baseline <code>run_id_b</code> str Run ID of current run <code>node_diffs</code> Dict[str, NodeDiffResult] Per-node comparison results <code>nodes_added</code> List[str] Nodes present in B but not A <code>nodes_removed</code> List[str] Nodes present in A but not B <code>drift_source_nodes</code> List[str] Nodes where logic changed <code>impacted_downstream_nodes</code> List[str] Nodes affected by upstream drift"},{"location":"features/diagnostics/#historymanager","title":"HistoryManager","text":"<p>Manage and access pipeline run history:</p> <pre><code>from odibi.diagnostics import HistoryManager\n\nmanager = HistoryManager(history_path=\"stories/\")\n\n# List available runs\nruns = manager.list_runs(\"process_orders\")\nfor run in runs:\n    print(f\"Run: {run['run_id']} at {run['timestamp']}\")\n\n# Get specific runs\nlatest = manager.get_latest_run(\"process_orders\")\nspecific = manager.get_run_by_id(\"process_orders\", \"20240130_101500\")\nprevious = manager.get_previous_run(\"process_orders\", \"20240130_101500\")\n</code></pre>"},{"location":"features/diagnostics/#historymanager-methods","title":"HistoryManager Methods","text":"Method Description <code>list_runs(pipeline_name)</code> List all runs for a pipeline (newest first) <code>get_latest_run(pipeline_name)</code> Get the most recent run metadata <code>get_run_by_id(pipeline_name, run_id)</code> Get specific run by ID <code>get_previous_run(pipeline_name, run_id)</code> Get the run immediately before specified run <code>load_run(path)</code> Load run metadata from JSON file"},{"location":"features/diagnostics/#examples","title":"Examples","text":""},{"location":"features/diagnostics/#debugging-a-failed-pipeline","title":"Debugging a Failed Pipeline","text":"<pre><code>from odibi.diagnostics import HistoryManager, diff_runs\n\nmanager = HistoryManager(\"stories/\")\n\n# Get the failed run and the last successful run\nfailed_run = manager.get_latest_run(\"process_orders\")\nprevious_run = manager.get_previous_run(\"process_orders\", failed_run.run_id)\n\nif previous_run:\n    diff = diff_runs(previous_run, failed_run)\n\n    # Find what changed\n    print(\"Changes that may have caused failure:\")\n    for node in diff.drift_source_nodes:\n        node_diff = diff.node_diffs[node]\n        if node_diff.sql_changed:\n            print(f\"  - {node}: SQL logic changed\")\n        if node_diff.config_changed:\n            print(f\"  - {node}: Configuration changed\")\n</code></pre>"},{"location":"features/diagnostics/#monitoring-data-quality-over-time","title":"Monitoring Data Quality Over Time","text":"<pre><code>from odibi.diagnostics import get_delta_diff, detect_drift\n\n# Check for unexpected changes after a pipeline run\ntable_path = \"/delta/silver/orders\"\n\n# Compare with yesterday's version\ndrift_warning = detect_drift(\n    table_path=table_path,\n    current_version=100,\n    baseline_version=95,\n    spark=spark,\n    threshold_pct=5.0,  # Alert on &gt;5% change\n)\n\nif drift_warning:\n    # Get detailed diff\n    diff = get_delta_diff(\n        table_path=table_path,\n        version_a=95,\n        version_b=100,\n        spark=spark,\n        deep=True,\n    )\n\n    print(f\"Warning: {drift_warning}\")\n    print(f\"Details: +{diff.rows_added} / -{diff.rows_removed} rows\")\n\n    if diff.schema_added:\n        print(f\"New columns: {diff.schema_added}\")\n</code></pre>"},{"location":"features/diagnostics/#comparing-spark-vs-pandas-execution","title":"Comparing Spark vs Pandas Execution","text":"<p>The diagnostics module supports both Spark and Pandas (via <code>deltalake</code> library):</p> <pre><code>from odibi.diagnostics import get_delta_diff\n\n# With Spark (for large tables)\ndiff_spark = get_delta_diff(\n    table_path=\"/delta/table\",\n    version_a=1,\n    version_b=5,\n    spark=spark,\n    deep=True,\n)\n\n# With Pandas/deltalake (for local development)\ndiff_pandas = get_delta_diff(\n    table_path=\"/delta/table\",\n    version_a=1,\n    version_b=5,\n    spark=None,  # Uses deltalake library\n    deep=True,\n)\n</code></pre>"},{"location":"features/diagnostics/#tracking-schema-evolution","title":"Tracking Schema Evolution","text":"<pre><code>from odibi.diagnostics import get_delta_diff\n\ndiff = get_delta_diff(\n    table_path=\"/delta/silver/customers\",\n    version_a=0,  # Initial version\n    version_b=50,  # Current version\n    spark=spark,\n)\n\nprint(\"Schema Evolution:\")\nprint(f\"  Initial columns: {diff.schema_previous}\")\nprint(f\"  Current columns: {diff.schema_current}\")\nprint(f\"  Added over time: {diff.schema_added}\")\nprint(f\"  Removed over time: {diff.schema_removed}\")\n</code></pre>"},{"location":"features/diagnostics/#best-practices","title":"Best Practices","text":"<ol> <li>Use deep mode sparingly - Deep diff is expensive; use metadata-only diffs for routine monitoring</li> <li>Define primary keys - Key-based diff enables update detection, not just add/remove</li> <li>Set appropriate thresholds - Tune drift detection thresholds based on expected data patterns</li> <li>Store history - Enable story persistence to enable run comparisons over time</li> <li>Automate drift checks - Integrate drift detection into pipeline post-run hooks</li> </ol>"},{"location":"features/diagnostics/#related","title":"Related","text":"<ul> <li>Stories - Pipeline execution history</li> <li>Schema Tracking - Schema change monitoring</li> <li>Quality Gates - Data quality validation</li> <li>Lineage - Data lineage tracking</li> </ul>"},{"location":"features/engines/","title":"Execution Engines","text":"<p>Multi-engine architecture for flexible data processing across local development, high-performance workloads, and big data environments.</p>"},{"location":"features/engines/#overview","title":"Overview","text":"<p>Odibi's engine system provides: - Multiple backends: Pandas, Spark, Polars - Unified API: Consistent interface across engines - Automatic selection: Choose based on workload and environment - Performance tuning: Engine-specific optimizations</p>"},{"location":"features/engines/#supported-engines","title":"Supported Engines","text":"Engine Best For Dependencies <code>pandas</code> Local development, small datasets (&lt;1GB) <code>pip install odibi</code> <code>spark</code> Big data, Databricks, distributed processing <code>pip install odibi[spark]</code> <code>polars</code> High-performance local processing, medium datasets <code>pip install polars</code>"},{"location":"features/engines/#pandasengine","title":"PandasEngine","text":"<p>Default engine for local development with broad format support.</p> <p>Strengths: - Extensive format support (CSV, Parquet, JSON, Excel, Avro, Delta) - Rich ecosystem integration - Familiar API for data scientists - SQL support via DuckDB (optional)</p> <p>Best for: - Local development and testing - Small to medium datasets (&lt;1GB) - Complex transformations with pandas operations</p>"},{"location":"features/engines/#sparkengine","title":"SparkEngine","text":"<p>Distributed processing engine for big data workloads.</p> <p>Strengths: - Horizontal scalability - Native Databricks integration - Delta Lake support with ACID transactions - Streaming pipelines - Multi-account ADLS support</p> <p>Best for: - Large datasets (&gt;1GB) - Production Databricks workflows - Distributed processing - Real-time streaming</p>"},{"location":"features/engines/#polarsengine","title":"PolarsEngine","text":"<p>High-performance engine with lazy evaluation.</p> <p>Strengths: - Extremely fast (Rust-based) - Memory efficient with lazy execution - Multi-threaded by default - Native scan operations (scan_csv, scan_parquet)</p> <p>Best for: - High-performance local processing - Medium to large datasets (1GB-10GB) - CPU-bound transformations</p>"},{"location":"features/engines/#configuration","title":"Configuration","text":""},{"location":"features/engines/#basic-engine-setup","title":"Basic Engine Setup","text":"<pre><code>project: DataPipeline\nengine: pandas  # or spark, polars\n\nconnections:\n  # ...\n\npipelines:\n  # ...\n</code></pre>"},{"location":"features/engines/#engine-options","title":"Engine Options","text":"Field Type Description <code>engine</code> string Engine type: <code>pandas</code>, <code>spark</code>, <code>polars</code> <code>performance</code> object Performance tuning options"},{"location":"features/engines/#engine-selection-guide","title":"Engine Selection Guide","text":"Scenario Recommended Engine Local development <code>pandas</code> Unit testing <code>pandas</code> Databricks production <code>spark</code> Large datasets (&gt;1GB) <code>spark</code> or <code>polars</code> CPU-bound local processing <code>polars</code> Streaming pipelines <code>spark</code> Quick prototyping <code>pandas</code>"},{"location":"features/engines/#engine-api","title":"Engine API","text":"<p>All engines implement the same core interface defined in <code>Engine</code> base class.</p>"},{"location":"features/engines/#core-methods","title":"Core Methods","text":"Method Description <code>read()</code> Read data from source <code>write()</code> Write data to destination <code>execute_sql()</code> Execute SQL query <code>execute_operation()</code> Execute built-in operation (pivot, sort, etc.) <code>get_schema()</code> Get DataFrame schema <code>get_shape()</code> Get DataFrame dimensions <code>count_rows()</code> Count rows in DataFrame <code>count_nulls()</code> Count nulls in specified columns"},{"location":"features/engines/#data-operations","title":"Data Operations","text":"Method Description <code>validate_schema()</code> Validate DataFrame schema against rules <code>validate_data()</code> Validate data against validation config <code>get_sample()</code> Get sample rows as dictionaries <code>profile_nulls()</code> Calculate null percentage per column <code>harmonize_schema()</code> Match DataFrame to target schema <code>anonymize()</code> Anonymize columns (hash, mask, redact)"},{"location":"features/engines/#table-operations","title":"Table Operations","text":"Method Description <code>table_exists()</code> Check if table/path exists <code>get_table_schema()</code> Get schema of existing table <code>maintain_table()</code> Run maintenance (optimize, vacuum) <code>materialize()</code> Materialize lazy dataset into memory"},{"location":"features/engines/#custom-format-support","title":"Custom Format Support","text":"<pre><code>from odibi.engine import PandasEngine\n\ndef read_netcdf(path, **options):\n    import xarray as xr\n    return xr.open_dataset(path).to_dataframe()\n\ndef write_netcdf(df, path, **options):\n    import xarray as xr\n    xr.Dataset.from_dataframe(df).to_netcdf(path)\n\nPandasEngine.register_format(\"netcdf\", reader=read_netcdf, writer=write_netcdf)\n</code></pre>"},{"location":"features/engines/#performance-configuration","title":"Performance Configuration","text":""},{"location":"features/engines/#pandas-performance","title":"Pandas Performance","text":"<pre><code>engine: pandas\nperformance:\n  use_arrow: true    # Use PyArrow backend (faster, less memory)\n  use_duckdb: false  # Use DuckDB for SQL (experimental)\n</code></pre> <p>Arrow backend benefits: - Faster I/O for Parquet files - Reduced memory usage - Better type preservation</p>"},{"location":"features/engines/#spark-performance","title":"Spark Performance","text":"<pre><code>engine: spark\n</code></pre> <p>Spark is automatically configured with: - Arrow-based PySpark conversions - Adaptive Query Execution (AQE) - Dynamic partition overwrite mode</p> <p>Additional optimizations via write options:</p> <pre><code>pipelines:\n  - pipeline: optimize_example\n    nodes:\n      - name: write_optimized\n        write:\n          connection: silver\n          format: delta\n          path: optimized_table\n          options:\n            optimize_write: true\n            zorder_by: [customer_id, date]\n</code></pre>"},{"location":"features/engines/#polars-performance","title":"Polars Performance","text":"<pre><code>engine: polars\n</code></pre> <p>Polars features: - Lazy evaluation by default (scan operations) - Automatic query optimization - Multi-threaded execution - Streaming writes (sink operations)</p>"},{"location":"features/engines/#examples","title":"Examples","text":""},{"location":"features/engines/#switching-engines","title":"Switching Engines","text":"<p>Same pipeline, different engines:</p> <pre><code># Local development\nproject: DataPipeline\nengine: pandas\n\nconnections:\n  bronze:\n    type: local\n    path: ./data/bronze\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          format: csv\n          path: orders.csv\n</code></pre> <pre><code># Production (Databricks)\nproject: DataPipeline\nengine: spark\n\nconnections:\n  bronze:\n    type: azure_adls\n    storage_account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          format: delta\n          path: orders\n</code></pre>"},{"location":"features/engines/#using-different-connections","title":"Using Different Connections","text":"<pre><code>project: MultiSource\nengine: spark\n\nconnections:\n  raw_data:\n    type: azure_adls\n    storage_account: rawstorage\n    container: raw\n\n  processed:\n    type: azure_adls\n    storage_account: procstorage\n    container: silver\n\n  sql_source:\n    type: azure_sql\n    server: myserver.database.windows.net\n    database: mydb\n\npipelines:\n  - pipeline: ingest_sql\n    nodes:\n      - name: read_sql\n        read:\n          connection: sql_source\n          format: sql\n          table: dbo.customers\n\n      - name: write_delta\n        write:\n          connection: processed\n          format: delta\n          path: customers\n</code></pre>"},{"location":"features/engines/#lazy-vs-eager-execution","title":"Lazy vs Eager Execution","text":"<pre><code># Polars with lazy execution (default)\nengine: polars\n\npipelines:\n  - pipeline: lazy_example\n    nodes:\n      - name: scan_data\n        read:\n          connection: bronze\n          format: parquet\n          path: large_dataset/*.parquet\n        # Returns LazyFrame - no data loaded yet\n\n      - name: filter_transform\n        sql: |\n          SELECT * FROM scan_data WHERE status = 'active'\n        # Still lazy - builds query plan\n\n      - name: write_result\n        write:\n          connection: silver\n          format: parquet\n          path: filtered_data\n        # Execution happens here (sink_parquet)\n</code></pre>"},{"location":"features/engines/#engine-specific-features","title":"Engine-Specific Features","text":"<p>Pandas with Delta Time Travel:</p> <pre><code>engine: pandas\n\npipelines:\n  - pipeline: time_travel\n    nodes:\n      - name: read_historical\n        read:\n          connection: bronze\n          format: delta\n          path: orders\n          options:\n            versionAsOf: 5  # Read version 5\n</code></pre> <p>Spark with Streaming:</p> <pre><code>engine: spark\n\npipelines:\n  - pipeline: streaming_ingest\n    nodes:\n      - name: stream_read\n        read:\n          connection: bronze\n          format: delta\n          path: events\n          streaming: true\n</code></pre>"},{"location":"features/engines/#programmatic-engine-usage","title":"Programmatic Engine Usage","text":"<pre><code>from odibi.engine import get_engine_class\n\n# Get engine by name\nEngineClass = get_engine_class(\"pandas\")\nengine = EngineClass(connections=my_connections)\n\n# Read data\ndf = engine.read(\n    connection=my_connection,\n    format=\"parquet\",\n    path=\"data/*.parquet\"\n)\n\n# Execute SQL\nfrom odibi.context import PandasContext\nctx = PandasContext()\nctx.register(\"orders\", df)\nresult = engine.execute_sql(\"SELECT * FROM orders WHERE total &gt; 100\", ctx)\n\n# Write data\nengine.write(\n    df=result,\n    connection=output_connection,\n    format=\"delta\",\n    path=\"filtered_orders\",\n    mode=\"overwrite\"\n)\n</code></pre>"},{"location":"features/engines/#register-custom-engine","title":"Register Custom Engine","text":"<pre><code>from odibi.engine import Engine, register_engine\n\nclass DuckDBEngine(Engine):\n    name = \"duckdb\"\n\n    def read(self, connection, format, **kwargs):\n        # Custom implementation\n        pass\n\n    # Implement other required methods...\n\nregister_engine(\"duckdb\", DuckDBEngine)\n</code></pre>"},{"location":"features/engines/#best-practices","title":"Best Practices","text":"<ol> <li>Match engine to workload - Use pandas for development, spark for production</li> <li>Use lazy execution - Polars and Spark defer computation until needed</li> <li>Enable Arrow - Faster I/O and reduced memory for Pandas</li> <li>Partition large tables - Use <code>partition_by</code> for write performance</li> <li>Run maintenance - Enable auto-optimize for Delta tables</li> <li>Test locally first - Develop with pandas, deploy with spark</li> </ol>"},{"location":"features/engines/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/engines/#switching-from-pandas-to-spark-breaks-my-pipeline","title":"Switching from Pandas to Spark breaks my pipeline","text":"<p>Symptom: Code works locally with pandas but fails on Spark.</p> <p>Common Causes: - Using pandas-specific operations (<code>.apply()</code>, <code>.iterrows()</code>) - Column name case sensitivity (Spark is case-insensitive by default) - Type coercion differences</p> <p>Fixes: - Use SQL transforms instead of Python where possible - Use <code>transform.steps</code> with SQL strings (engine-agnostic) - Test with <code>--dry-run</code> before switching engines</p>"},{"location":"features/engines/#engine-polars-not-found","title":"\"Engine 'polars' not found\"","text":"<p>Cause: Polars not installed.</p> <p>Fix:</p> <pre><code>pip install polars\n</code></pre>"},{"location":"features/engines/#memory-errors-with-pandas-engine","title":"Memory errors with Pandas engine","text":"<p>Symptom: <code>MemoryError</code> or system becomes unresponsive.</p> <p>Causes: - Dataset too large for available RAM - Multiple DataFrames held in memory</p> <p>Fixes: - Switch to Spark for datasets &gt;1GB - Use chunked reading if staying on Pandas:</p> <pre><code>read:\n  options:\n    chunksize: 100000\n</code></pre>"},{"location":"features/engines/#polars-lazy-vs-eager-mode-issues","title":"Polars lazy vs eager mode issues","text":"<p>Symptom: Operations don't execute or return LazyFrame instead of DataFrame.</p> <p>Fix: Odibi handles collection automatically, but if using custom transforms:</p> <pre><code># Force collection in custom transforms\nif hasattr(df, 'collect'):\n    df = df.collect()\n</code></pre>"},{"location":"features/engines/#related","title":"Related","text":"<ul> <li>Connections - Data source configuration</li> <li>Pipelines - Pipeline definition</li> <li>YAML Schema Reference - Full configuration options</li> </ul>"},{"location":"features/lineage/","title":"Cross-Pipeline Lineage","text":"<p>Track table-level lineage relationships across pipelines for impact analysis and data governance.</p>"},{"location":"features/lineage/#overview","title":"Overview","text":"<p>Odibi tracks lineage at two levels: - OpenLineage integration: Standards-based lineage emission - Cross-pipeline lineage: Table-to-table relationships in the System Catalog</p> <p>This document covers the cross-pipeline lineage tracking stored in <code>meta_lineage</code>.</p>"},{"location":"features/lineage/#how-it-works","title":"How It Works","text":"<ol> <li>During pipeline execution, read/write operations are recorded</li> <li>Source \u2192 Target relationships are stored in <code>meta_lineage</code></li> <li>CLI commands query the lineage graph</li> <li>Impact analysis identifies affected downstream tables</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   bronze/   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   silver/   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   gold/     \u2502\n\u2502  customers  \u2502     \u2502 dim_customer\u2502     \u2502customer_360 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2502                   \u2502                   \u2502\n   Pipeline A          Pipeline B          Pipeline C\n</code></pre>"},{"location":"features/lineage/#cli-commands","title":"CLI Commands","text":""},{"location":"features/lineage/#trace-upstream-lineage","title":"Trace Upstream Lineage","text":"<p>Find all sources for a table:</p> <pre><code>odibi lineage upstream &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi lineage upstream gold/customer_360 --config pipeline.yaml\n\nUpstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre> <p>Options:</p> <pre><code>odibi lineage upstream gold/customer_360 --config config.yaml \\\n    --depth 5 \\         # Traverse up to 5 levels (default: 3)\n    --format json       # Output as JSON\n</code></pre>"},{"location":"features/lineage/#trace-downstream-lineage","title":"Trace Downstream Lineage","text":"<p>Find all consumers of a table:</p> <pre><code>odibi lineage downstream &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi lineage downstream bronze/customers_raw --config pipeline.yaml\n\nDownstream Lineage: bronze/customers_raw\n============================================================\nbronze/customers_raw\n\u251c\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n\u2502   \u251c\u2500\u2500 gold/customer_360 (gold_pipeline.build_360)\n\u2502   \u2514\u2500\u2500 gold/churn_features (ml_pipeline.build_features)\n\u2514\u2500\u2500 silver/customer_events (silver_pipeline.process_events)\n</code></pre>"},{"location":"features/lineage/#impact-analysis","title":"Impact Analysis","text":"<p>Assess the impact of changes to a table:</p> <pre><code>odibi lineage impact &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi lineage impact bronze/customers_raw --config pipeline.yaml\n\n\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n    - silver/customer_events (pipeline: silver_pipeline)\n\n  Summary:\n    Total: 4 downstream table(s) in 3 pipeline(s)\n</code></pre>"},{"location":"features/lineage/#programmatic-access","title":"Programmatic Access","text":""},{"location":"features/lineage/#using-lineagetracker","title":"Using LineageTracker","text":"<pre><code>from odibi.lineage import LineageTracker\nfrom odibi.catalog import CatalogManager\n\n# Initialize\ncatalog = CatalogManager(spark, config, base_path, engine)\ntracker = LineageTracker(catalog)\n\n# Record lineage manually\ntracker.record_lineage(\n    read_config=node.read,\n    write_config=node.write,\n    pipeline=\"my_pipeline\",\n    node=\"process_data\",\n    run_id=\"run-12345\",\n    connections=connections,\n)\n\n# Query upstream\nupstream = tracker.get_upstream(\"gold/customer_360\", depth=3)\nfor record in upstream:\n    print(f\"{record['source_table']} \u2192 {record['target_table']}\")\n\n# Query downstream\ndownstream = tracker.get_downstream(\"bronze/customers_raw\", depth=3)\n\n# Impact analysis\nimpact = tracker.get_impact_analysis(\"bronze/customers_raw\")\nprint(f\"Affected tables: {impact['affected_tables']}\")\nprint(f\"Affected pipelines: {impact['affected_pipelines']}\")\n</code></pre>"},{"location":"features/lineage/#direct-catalog-access","title":"Direct Catalog Access","text":"<pre><code># Record lineage directly\ncatalog.record_lineage(\n    source_table=\"bronze/customers_raw\",\n    target_table=\"silver/dim_customers\",\n    target_pipeline=\"silver_pipeline\",\n    target_node=\"process_customers\",\n    run_id=\"run-12345\",\n    relationship=\"feeds\",\n)\n\n# Query upstream\nupstream = catalog.get_upstream(\"gold/customer_360\", depth=3)\n\n# Query downstream\ndownstream = catalog.get_downstream(\"bronze/customers_raw\", depth=3)\n</code></pre>"},{"location":"features/lineage/#lineage-record-structure","title":"Lineage Record Structure","text":"<p>Each lineage record includes:</p> Field Description <code>source_table</code> Source table path <code>target_table</code> Target table path <code>source_pipeline</code> Pipeline reading from source <code>source_node</code> Node reading from source <code>target_pipeline</code> Pipeline writing to target <code>target_node</code> Node writing to target <code>relationship</code> Type: \"feeds\" or \"derived_from\" <code>last_observed</code> Last time this relationship was seen <code>run_id</code> Run ID when recorded"},{"location":"features/lineage/#automatic-tracking","title":"Automatic Tracking","text":"<p>Lineage is automatically tracked when: 1. A node has both <code>read</code> and <code>write</code> configurations 2. The System Catalog is configured 3. The pipeline runs successfully</p> <pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n      format: delta\n\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE active = true\"\n\n    write:\n      connection: silver\n      path: dim_customers\n      format: delta\n</code></pre> <p>This automatically records: <code>bronze/customers_raw \u2192 silver/dim_customers</code></p>"},{"location":"features/lineage/#dependency-based-lineage","title":"Dependency-Based Lineage","text":"<p>Lineage is also tracked for <code>depends_on</code> relationships:</p> <pre><code>nodes:\n  - name: source_node\n    read: { connection: bronze, path: raw_data }\n    write: { connection: silver, path: processed_data }\n\n  - name: consumer_node\n    depends_on: [source_node]  # Lineage tracked!\n    transform:\n      steps:\n        - sql: \"SELECT * FROM source_node\"\n    write: { connection: gold, path: final_data }\n</code></pre>"},{"location":"features/lineage/#storage-location","title":"Storage Location","text":"<p>Lineage is stored in the System Catalog:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n</code></pre> <p>Location: <code>{connection_base_path}/_odibi_system/meta_lineage/</code></p>"},{"location":"features/lineage/#example-pre-deployment-impact-check","title":"Example: Pre-Deployment Impact Check","text":"<p>Before deploying schema changes, check impact:</p> <pre><code>def pre_deployment_check(catalog, table_to_change):\n    \"\"\"Check impact before deploying changes.\"\"\"\n    downstream = catalog.get_downstream(table_to_change, depth=5)\n\n    if not downstream:\n        print(f\"\u2705 No downstream dependencies for {table_to_change}\")\n        return True\n\n    affected_tables = set()\n    affected_pipelines = set()\n\n    for record in downstream:\n        affected_tables.add(record['target_table'])\n        if record.get('target_pipeline'):\n            affected_pipelines.add(record['target_pipeline'])\n\n    print(f\"\u26a0\ufe0f  Changes to {table_to_change} will affect:\")\n    print(f\"   - {len(affected_tables)} tables\")\n    print(f\"   - {len(affected_pipelines)} pipelines\")\n\n    for table in sorted(affected_tables):\n        print(f\"     \u2022 {table}\")\n\n    return len(downstream) == 0\n</code></pre>"},{"location":"features/lineage/#integration-with-schema-tracking","title":"Integration with Schema Tracking","text":"<p>Combine lineage with schema tracking for comprehensive governance:</p> <pre><code>def assess_schema_change_impact(catalog, table_path):\n    \"\"\"Assess impact of recent schema changes.\"\"\"\n    # Get schema changes\n    history = catalog.get_schema_history(table_path, limit=2)\n    if len(history) &lt; 2:\n        return\n\n    latest = history[0]\n    removed = latest.get('columns_removed', [])\n\n    if removed:\n        # Check downstream impact\n        downstream = catalog.get_downstream(table_path)\n        print(f\"\u26a0\ufe0f  Columns {removed} were removed from {table_path}\")\n        print(f\"   This may break {len(downstream)} downstream tables\")\n</code></pre>"},{"location":"features/lineage/#best-practices","title":"Best Practices","text":"<ol> <li>Run impact analysis before changes - Know what you'll affect</li> <li>Use consistent table naming - Makes lineage easier to follow</li> <li>Document cross-pipeline boundaries - Clarify ownership</li> <li>Monitor lineage depth - Deep chains may indicate complexity</li> <li>Integrate with CI/CD - Block deployments with unknown impact</li> </ol>"},{"location":"features/lineage/#related","title":"Related","text":"<ul> <li>Schema Version Tracking - Track schema changes</li> <li>OpenLineage Integration - Standards-based lineage</li> </ul>"},{"location":"features/observability/","title":"Observability","text":"<p>Odibi integrates with OpenTelemetry for distributed tracing and metrics export.</p>"},{"location":"features/observability/#overview","title":"Overview","text":"<p>When configured, Odibi automatically emits:</p> <ul> <li>Traces: Spans for pipeline runs, node executions, and operations</li> <li>Metrics: Execution counts, durations, and error rates</li> </ul>"},{"location":"features/observability/#configuration","title":"Configuration","text":""},{"location":"features/observability/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables to enable OpenTelemetry export:</p> Variable Description Example <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> OTLP endpoint URL <code>http://localhost:4317</code> <code>OTEL_SERVICE_NAME</code> Service name (default: <code>odibi</code>) <code>my-data-pipeline</code>"},{"location":"features/observability/#example-export-to-jaeger","title":"Example: Export to Jaeger","text":"<pre><code># Start Jaeger\ndocker run -d --name jaeger \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  jaegertracing/all-in-one:latest\n\n# Run Odibi with tracing\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\nodibi run config.yaml\n\n# View traces at http://localhost:16686\n</code></pre>"},{"location":"features/observability/#example-export-to-grafana-tempo","title":"Example: Export to Grafana Tempo","text":"<pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo.monitoring:4317\nexport OTEL_SERVICE_NAME=odibi-prod\nodibi run config.yaml\n</code></pre>"},{"location":"features/observability/#what-gets-traced","title":"What Gets Traced","text":""},{"location":"features/observability/#pipeline-execution","title":"Pipeline Execution","text":"<p>Each pipeline run creates a root span with:</p> <ul> <li>Pipeline name</li> <li>Start/end timestamps</li> <li>Success/failure status</li> <li>Node count</li> </ul>"},{"location":"features/observability/#node-execution","title":"Node Execution","text":"<p>Each node creates a child span with:</p> <ul> <li>Node name and type</li> <li>Duration</li> <li>Row counts (input/output)</li> <li>Error details (if failed)</li> </ul>"},{"location":"features/observability/#retry-operations","title":"Retry Operations","text":"<p>Retry attempts are recorded as span events:</p> <ul> <li>Retry count</li> <li>Delay duration</li> <li>Error that triggered retry</li> </ul>"},{"location":"features/observability/#telemetry-api","title":"Telemetry API","text":"<p>For custom instrumentation in transforms:</p> <pre><code>from odibi.utils.telemetry import get_tracer, get_meter\n\ntracer = get_tracer()\nmeter = get_meter()\n\n# Create a span\nwith tracer.start_as_current_span(\"my_operation\") as span:\n    span.set_attribute(\"custom.key\", \"value\")\n    # ... your code\n\n# Record a metric\ncounter = meter.create_counter(\"my_counter\")\ncounter.add(1, {\"dimension\": \"value\"})\n</code></pre>"},{"location":"features/observability/#structured-logging","title":"Structured Logging","text":"<p>Odibi uses structured logging with context:</p> <pre><code>from odibi.utils.logging_context import get_logging_context\n\nctx = get_logging_context()\nctx.info(\"Processing started\", node=\"my_node\", rows=1000)\n</code></pre> <p>Log output includes:</p> <ul> <li>Timestamp</li> <li>Log level</li> <li>Message</li> <li>Structured attributes (JSON)</li> </ul>"},{"location":"features/observability/#disabling-telemetry","title":"Disabling Telemetry","text":"<p>Telemetry is disabled by default. It only activates when <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> is set.</p> <p>To explicitly disable in an environment where it might be configured:</p> <pre><code>unset OTEL_EXPORTER_OTLP_ENDPOINT\n</code></pre>"},{"location":"features/observability/#related","title":"Related","text":"<ul> <li>Lineage \u2014 OpenLineage for data lineage</li> <li>Stories \u2014 Built-in execution reports</li> <li>Alerting \u2014 Failure notifications</li> </ul>"},{"location":"features/orchestration/","title":"Orchestration","text":"<p>Generate production-ready workflow definitions for Apache Airflow and Dagster from your Odibi pipelines.</p>"},{"location":"features/orchestration/#overview","title":"Overview","text":"<p>Odibi's orchestration module provides: - Airflow Integration: Generate DAG files with proper task dependencies - Dagster Integration: Create asset definitions with dependency graphs - Automatic Dependency Mapping: Node dependencies become task/asset dependencies - CLI Execution: Each node runs via <code>odibi run</code> for isolation</p>"},{"location":"features/orchestration/#airflow-integration","title":"Airflow Integration","text":""},{"location":"features/orchestration/#airflowexporter-class","title":"AirflowExporter Class","text":"<p>The <code>AirflowExporter</code> generates Airflow DAG Python files from Odibi pipeline configurations.</p> <pre><code>from odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\nconfig = load_config(\"odibi.yaml\")\nexporter = AirflowExporter(config)\n\n# Generate DAG code for a specific pipeline\ndag_code = exporter.generate_code(\"process_orders\")\n\n# Write to Airflow DAGs folder\nwith open(\"/airflow/dags/odibi_process_orders.py\", \"w\") as f:\n    f.write(dag_code)\n</code></pre>"},{"location":"features/orchestration/#generated-dag-structure","title":"Generated DAG Structure","text":"<p>The exporter creates a DAG with: - <code>BashOperator</code> tasks for each node - Proper upstream/downstream dependencies - Configurable retries from your Odibi config - Tags for filtering (<code>odibi</code>, layer name)</p> <pre><code># Generated DAG example\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\nwith DAG(\n    'odibi_process_orders',\n    default_args=default_args,\n    description='Process incoming orders',\n    schedule_interval=None,\n    catchup=False,\n    tags=['odibi', 'silver'],\n) as dag:\n\n    ingest_orders = BashOperator(\n        task_id='ingest_orders',\n        bash_command='odibi run --pipeline process_orders --node ingest_orders',\n    )\n\n    validate_orders = BashOperator(\n        task_id='validate_orders',\n        bash_command='odibi run --pipeline process_orders --node validate_orders',\n    )\n\n    # Dependencies\n    [ingest_orders] &gt;&gt; validate_orders\n</code></pre>"},{"location":"features/orchestration/#airflow-configuration-options","title":"Airflow Configuration Options","text":"Option Source Description <code>dag_id</code> Auto-generated <code>odibi_{pipeline_name}</code> <code>owner</code> <code>config.owner</code> DAG owner for Airflow UI <code>retries</code> <code>config.retry.max_attempts</code> Retry count (0 if disabled) <code>tags</code> <code>pipeline.layer</code> Includes <code>odibi</code> and layer name <code>description</code> <code>pipeline.description</code> Pipeline description"},{"location":"features/orchestration/#dagster-integration","title":"Dagster Integration","text":""},{"location":"features/orchestration/#dagsterfactory-class","title":"DagsterFactory Class","text":"<p>The <code>DagsterFactory</code> creates Dagster asset definitions directly from your Odibi configuration.</p> <pre><code># definitions.py\nfrom odibi.config import load_config\nfrom odibi.orchestration.dagster import DagsterFactory\n\nconfig = load_config(\"odibi.yaml\")\ndefs = DagsterFactory(config).create_definitions()\n</code></pre>"},{"location":"features/orchestration/#asset-features","title":"Asset Features","text":"<p>Each Odibi node becomes a Dagster asset with: - Dependency tracking: <code>depends_on</code> becomes asset dependencies - Grouping: Assets grouped by pipeline name - Compute kind: Tagged as <code>odibi</code> for UI identification - Op tags: Pipeline and node names for filtering</p>"},{"location":"features/orchestration/#generated-assets","title":"Generated Assets","text":"<pre><code># Dagster creates assets like:\n@asset(\n    name=\"validate_orders\",\n    deps=[\"ingest_orders\"],\n    group_name=\"process_orders\",\n    description=\"Validate order data quality\",\n    compute_kind=\"odibi\",\n    op_tags={\"odibi/pipeline\": \"process_orders\", \"odibi/node\": \"validate_orders\"},\n)\ndef validate_orders(context: AssetExecutionContext):\n    # Runs: odibi run --pipeline process_orders --node validate_orders\n    ...\n</code></pre>"},{"location":"features/orchestration/#dagster-installation","title":"Dagster Installation","text":"<p>Dagster is an optional dependency:</p> <pre><code>pip install dagster dagster-webserver\n</code></pre>"},{"location":"features/orchestration/#configuration","title":"Configuration","text":""},{"location":"features/orchestration/#project-configuration-for-orchestration","title":"Project Configuration for Orchestration","text":"<pre><code>project: DataPipeline\nowner: data-team      # Used as Airflow DAG owner\n\nretry:\n  enabled: true\n  max_attempts: 3      # Airflow retry count\n\npipelines:\n  - pipeline: process_orders\n    layer: silver      # Used as Airflow tag\n    description: \"Process incoming orders\"\n    nodes:\n      - name: ingest_orders\n        # ...\n\n      - name: validate_orders\n        depends_on:\n          - ingest_orders\n        # ...\n\n      - name: transform_orders\n        depends_on:\n          - validate_orders\n        # ...\n</code></pre>"},{"location":"features/orchestration/#dependency-mapping","title":"Dependency Mapping","text":"<p>Node dependencies in Odibi map directly to orchestrator dependencies:</p> Odibi Config Airflow Dagster <code>depends_on: [node_a]</code> <code>[node_a] &gt;&gt; node_b</code> <code>deps=[\"node_a\"]</code> <code>depends_on: [a, b]</code> <code>[a, b] &gt;&gt; node_c</code> <code>deps=[\"a\", \"b\"]</code> No dependencies First task No deps"},{"location":"features/orchestration/#examples","title":"Examples","text":""},{"location":"features/orchestration/#complete-airflow-integration","title":"Complete Airflow Integration","text":"<pre><code># scripts/generate_dags.py\nfrom pathlib import Path\nfrom odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\ndef generate_all_dags(config_path: str, output_dir: str):\n    config = load_config(config_path)\n    exporter = AirflowExporter(config)\n    output = Path(output_dir)\n\n    for pipeline in config.pipelines:\n        dag_code = exporter.generate_code(pipeline.pipeline)\n        dag_file = output / f\"odibi_{pipeline.pipeline}.py\"\n        dag_file.write_text(dag_code)\n        print(f\"Generated: {dag_file}\")\n\nif __name__ == \"__main__\":\n    generate_all_dags(\"odibi.yaml\", \"/opt/airflow/dags\")\n</code></pre>"},{"location":"features/orchestration/#complete-dagster-integration","title":"Complete Dagster Integration","text":"<pre><code># definitions.py\nfrom odibi.config import load_config\nfrom odibi.orchestration.dagster import DagsterFactory\n\n# Load Odibi configuration\nconfig = load_config(\"odibi.yaml\")\n\n# Create Dagster definitions\ndefs = DagsterFactory(config).create_definitions()\n\n# Run with: dagster dev -f definitions.py\n</code></pre>"},{"location":"features/orchestration/#multi-pipeline-setup","title":"Multi-Pipeline Setup","text":"<pre><code># odibi.yaml\nproject: DataWarehouse\nowner: platform-team\n\npipelines:\n  - pipeline: bronze_ingestion\n    layer: bronze\n    nodes:\n      - name: ingest_customers\n        source:\n          connection: raw_db\n          path: customers\n\n      - name: ingest_orders\n        source:\n          connection: raw_db\n          path: orders\n\n  - pipeline: silver_transformation\n    layer: silver\n    nodes:\n      - name: clean_customers\n        depends_on: []\n        source:\n          connection: bronze\n          path: customers\n\n      - name: clean_orders\n        depends_on: []\n        source:\n          connection: bronze\n          path: orders\n\n      - name: join_customer_orders\n        depends_on:\n          - clean_customers\n          - clean_orders\n</code></pre> <pre><code># Generate DAGs for all pipelines\nfrom odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\nconfig = load_config(\"odibi.yaml\")\nexporter = AirflowExporter(config)\n\n# Generates separate DAGs:\n# - odibi_bronze_ingestion\n# - odibi_silver_transformation\nfor pipeline in config.pipelines:\n    code = exporter.generate_code(pipeline.pipeline)\n    print(f\"--- {pipeline.pipeline} ---\")\n    print(code)\n</code></pre>"},{"location":"features/orchestration/#best-practices","title":"Best Practices","text":"<ol> <li>Use CLI execution - Both adapters use <code>odibi run</code> for process isolation</li> <li>Set owner - Configure <code>owner</code> in YAML for Airflow ownership</li> <li>Enable retries - Configure retry settings in Odibi config</li> <li>Layer tags - Use <code>layer</code> field for organizing DAGs in Airflow</li> <li>Generate on deploy - Regenerate DAG files during CI/CD deployment</li> </ol>"},{"location":"features/orchestration/#related","title":"Related","text":"<ul> <li>Pipeline Configuration - YAML schema reference</li> <li>CLI Reference - <code>odibi run</code> command details</li> <li>Retry configuration is defined in your YAML config under the <code>retry</code> section</li> </ul>"},{"location":"features/patterns/","title":"Loading Patterns","text":"<p>Pre-built execution patterns for common data warehouse loading scenarios including SCD2 and Merge operations.</p>"},{"location":"features/patterns/#overview","title":"Overview","text":"<p>Odibi's pattern system provides: - Declarative loading: Configure complex loading logic via YAML - Engine agnostic: Works with Spark and Pandas engines - Built-in validation: Patterns validate required parameters before execution - Extensible: Create custom patterns by extending the <code>Pattern</code> base class</p>"},{"location":"features/patterns/#available-patterns","title":"Available Patterns","text":"Pattern Description Use Case <code>scd2</code> Slowly Changing Dimension Type 2 Historical tracking of dimension changes <code>merge</code> Upsert/merge operations Incremental updates, CDC <p>Note: For simple append or overwrite operations, use <code>write.mode: append</code> or <code>write.mode: overwrite</code> directly\u2014no pattern needed.</p>"},{"location":"features/patterns/#configuration","title":"Configuration","text":"<p>Patterns are configured via the <code>transformer</code> field in node configuration:</p> <pre><code>nodes:\n  - name: load_customers\n    transformer: scd2\n    params:\n      target: \"gold/customers\"\n      keys: [\"customer_id\"]\n      track_cols: [\"address\", \"tier\"]\n      effective_time_col: \"updated_at\"\n</code></pre>"},{"location":"features/patterns/#config-options","title":"Config Options","text":"Field Type Required Description <code>transformer</code> string Yes Pattern name: <code>scd2</code>, <code>merge</code> <code>params</code> object Yes Pattern-specific parameters"},{"location":"features/patterns/#pattern-parameters","title":"Pattern Parameters","text":""},{"location":"features/patterns/#scd2-pattern","title":"SCD2 Pattern","text":"<p>Tracks history by creating new rows for updates. When a tracked column changes, the old record is closed and a new record is inserted.</p> Parameter Type Required Default Description <code>target</code> string Yes - Target table name or path containing history <code>keys</code> list Yes - Natural keys to identify unique entities <code>track_cols</code> list Yes - Columns to monitor for changes <code>effective_time_col</code> string Yes - Source column indicating when the change occurred <code>end_time_col</code> string No <code>valid_to</code> Name of the end timestamp column <code>current_flag_col</code> string No <code>is_current</code> Name of the current record flag column <code>delete_col</code> string No <code>null</code> Column indicating soft deletion (boolean)"},{"location":"features/patterns/#merge-pattern","title":"Merge Pattern","text":"<p>Upsert/merge logic with support for multiple strategies.</p> Parameter Type Required Default Description <code>target</code> string Yes - Target table name or path <code>keys</code> list Yes - Join keys for matching records <code>strategy</code> string No <code>upsert</code> <code>upsert</code>, <code>append_only</code>, <code>delete_match</code> <code>audit_cols</code> object No <code>null</code> <code>{created_col: \"...\", updated_col: \"...\"}</code> <code>update_condition</code> string No <code>null</code> SQL condition for update clause <code>insert_condition</code> string No <code>null</code> SQL condition for insert clause <code>delete_condition</code> string No <code>null</code> SQL condition for delete clause <code>optimize_write</code> bool No <code>false</code> Run OPTIMIZE after write (Spark only) <code>zorder_by</code> list No <code>null</code> Columns to Z-Order by <code>cluster_by</code> list No <code>null</code> Columns to Liquid Cluster by (Delta)"},{"location":"features/patterns/#merge-strategies","title":"Merge Strategies","text":"Strategy Description <code>upsert</code> Update existing records, insert new ones <code>append_only</code> Ignore duplicates, only insert new keys <code>delete_match</code> Delete records in target that match keys in source"},{"location":"features/patterns/#pattern-api","title":"Pattern API","text":"<p>All patterns extend the <code>Pattern</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\nfrom odibi.config import NodeConfig\nfrom odibi.context import EngineContext\nfrom odibi.engine.base import Engine\n\nclass Pattern(ABC):\n    \"\"\"Base class for Execution Patterns.\"\"\"\n\n    def __init__(self, engine: Engine, config: NodeConfig):\n        self.engine = engine\n        self.config = config\n        self.params = config.params\n\n    @abstractmethod\n    def execute(self, context: EngineContext) -&gt; Any:\n        \"\"\"\n        Execute the pattern logic.\n\n        Args:\n            context: EngineContext containing current DataFrame and helpers.\n\n        Returns:\n            The transformed DataFrame.\n        \"\"\"\n        pass\n\n    def validate(self) -&gt; None:\n        \"\"\"\n        Validate pattern configuration.\n        Raises ValueError if invalid.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"features/patterns/#creating-custom-patterns","title":"Creating Custom Patterns","text":"<ol> <li>Extend the <code>Pattern</code> base class</li> <li>Implement <code>execute()</code> method</li> <li>Optionally override <code>validate()</code> for parameter validation</li> </ol> <pre><code>from odibi.patterns.base import Pattern\nfrom odibi.context import EngineContext\n\nclass MyCustomPattern(Pattern):\n\n    def validate(self) -&gt; None:\n        if not self.params.get(\"required_param\"):\n            raise ValueError(\"MyCustomPattern: 'required_param' is required.\")\n\n    def execute(self, context: EngineContext):\n        df = context.df\n        # Custom transformation logic\n        return df\n</code></pre>"},{"location":"features/patterns/#examples","title":"Examples","text":""},{"location":"features/patterns/#scd2-customer-dimension-with-history","title":"SCD2: Customer Dimension with History","text":"<p>Track customer address and tier changes over time:</p> <pre><code>project: CustomerDW\nengine: spark\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/gold\n\npipelines:\n  - pipeline: load_customer_dim\n    nodes:\n      - name: customer_scd2\n        read:\n          connection: bronze\n          path: customers\n        transformer: scd2\n        params:\n          target: \"gold/customers\"\n          keys: [\"customer_id\"]\n          track_cols: [\"address\", \"city\", \"state\", \"tier\"]\n          effective_time_col: \"updated_at\"\n          end_time_col: \"valid_to\"\n          current_flag_col: \"is_active\"\n</code></pre>"},{"location":"features/patterns/#merge-incremental-customer-updates","title":"Merge: Incremental Customer Updates","text":"<p>Upsert with audit columns:</p> <pre><code>pipelines:\n  - pipeline: sync_customers\n    nodes:\n      - name: customers_merge\n        read:\n          connection: bronze\n          path: customer_updates\n        transformer: merge\n        params:\n          target: \"silver.customers\"\n          keys: [\"customer_id\"]\n          strategy: upsert\n          audit_cols:\n            created_col: \"dw_created_at\"\n            updated_col: \"dw_updated_at\"\n</code></pre>"},{"location":"features/patterns/#merge-gdpr-delete-request","title":"Merge: GDPR Delete Request","text":"<p>Delete records matching source keys:</p> <pre><code>pipelines:\n  - pipeline: gdpr_delete\n    nodes:\n      - name: delete_customers\n        read:\n          connection: compliance\n          path: deletion_requests\n        transformer: merge\n        params:\n          target: \"silver.customers\"\n          keys: [\"customer_id\"]\n          strategy: delete_match\n</code></pre>"},{"location":"features/patterns/#merge-conditional-update","title":"Merge: Conditional Update","text":"<p>Only update if source record is newer:</p> <pre><code>pipelines:\n  - pipeline: sync_products\n    nodes:\n      - name: products_merge\n        read:\n          connection: bronze\n          path: product_updates\n        transformer: merge\n        params:\n          target: \"silver.products\"\n          keys: [\"product_id\"]\n          strategy: upsert\n          update_condition: \"source.updated_at &gt; target.updated_at\"\n          insert_condition: \"source.is_deleted = false\"\n</code></pre>"},{"location":"features/patterns/#simple-append-no-pattern-needed","title":"Simple Append (No Pattern Needed)","text":"<p>For event/fact data, just use write mode:</p> <pre><code>pipelines:\n  - pipeline: load_events\n    nodes:\n      - name: events\n        read:\n          connection: bronze\n          path: events\n        write:\n          connection: gold\n          path: events\n          mode: append\n</code></pre>"},{"location":"features/patterns/#simple-overwrite-no-pattern-needed","title":"Simple Overwrite (No Pattern Needed)","text":"<p>For full refresh of small tables:</p> <pre><code>pipelines:\n  - pipeline: refresh_products\n    nodes:\n      - name: products\n        read:\n          connection: bronze\n          path: products\n        write:\n          connection: gold\n          path: products\n          mode: overwrite\n</code></pre>"},{"location":"features/patterns/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right pattern - Use SCD2 for dimensions needing history, Merge for incremental CDC</li> <li>Use write.mode for simple cases - <code>append</code> for events, <code>overwrite</code> for full refresh</li> <li>Define business keys - Ensure <code>keys</code> uniquely identify records in your domain</li> <li>Monitor tracked columns - For SCD2, only track columns that represent meaningful business changes</li> <li>Use audit columns - Track <code>created_at</code> and <code>updated_at</code> for debugging and lineage</li> <li>Optimize large tables - Use <code>zorder_by</code> or <code>cluster_by</code> for frequently queried columns</li> </ol>"},{"location":"features/patterns/#related","title":"Related","text":"<ul> <li>Transformers - Built-in transformation functions</li> <li>Pipelines - Pipeline configuration</li> <li>YAML Schema Reference - Full schema documentation</li> </ul>"},{"location":"features/pipelines/","title":"Pipelines","text":"<p>Orchestrate complex data workflows with dependency-aware execution, parallel processing, and intelligent error handling.</p>"},{"location":"features/pipelines/#overview","title":"Overview","text":"<p>Odibi's pipeline system provides: - DAG-based execution: Automatic dependency resolution with cycle detection - Parallel processing: Execute independent nodes concurrently - Error strategies: Fine-grained control over failure behavior - Resume capability: Skip successfully completed nodes on retry - Drift detection: Compare local config against deployed definitions</p>"},{"location":"features/pipelines/#pipeline-vs-pipelinemanager","title":"Pipeline vs PipelineManager","text":"Component Purpose <code>Pipeline</code> Executes a single pipeline (nodes, graph, engine) <code>PipelineManager</code> Manages multiple pipelines from a YAML config file"},{"location":"features/pipelines/#core-concepts","title":"Core Concepts","text":""},{"location":"features/pipelines/#pipeline","title":"Pipeline","text":"<p>The <code>Pipeline</code> class is the executor and orchestrator for a single pipeline. It: - Builds a dependency graph from node configurations - Resolves execution order via topological sort - Manages the execution engine (Pandas or Spark) - Generates execution stories for observability</p>"},{"location":"features/pipelines/#node","title":"Node","text":"<p>A <code>Node</code> is the atomic unit of work. Each node follows a four-phase execution pattern:</p> <pre><code>Read \u2192 Transform \u2192 Validate \u2192 Write\n</code></pre> Phase Description Read Load data from a connection (file, table, API) Transform Apply transformations (SQL, functions, patterns) Validate Run data quality tests, quarantine bad rows Write Persist output to a connection"},{"location":"features/pipelines/#dependencygraph","title":"DependencyGraph","text":"<p>The <code>DependencyGraph</code> class builds and validates the DAG:</p> Feature Description Missing dependency check Fails if <code>depends_on</code> references undefined nodes Cycle detection Detects circular dependencies before execution Topological sort Returns nodes in valid execution order Execution layers Groups independent nodes for parallel execution"},{"location":"features/pipelines/#pipelineresults","title":"PipelineResults","text":"<p>Execution results are captured in <code>PipelineResults</code>:</p> Field Type Description <code>pipeline_name</code> string Name of the executed pipeline <code>completed</code> list Successfully completed node names <code>failed</code> list Failed node names <code>skipped</code> list Skipped node names (dependency failures) <code>node_results</code> dict Detailed <code>NodeResult</code> per node <code>duration</code> float Total execution time in seconds <code>story_path</code> string Path to generated execution story"},{"location":"features/pipelines/#configuration","title":"Configuration","text":""},{"location":"features/pipelines/#yaml-structure","title":"YAML Structure","text":"<pre><code>project: MyDataPipeline\nengine: spark  # or 'pandas'\n\nconnections:\n  bronze:\n    type: local\n    path: ./data/bronze\n  silver:\n    type: local\n    path: ./data/silver\n\npipelines:\n  - pipeline: bronze_to_silver\n    nodes:\n      - name: load_orders\n        read:\n          connection: bronze\n          path: orders.parquet\n          format: parquet\n\n      - name: clean_orders\n        depends_on: [load_orders]\n        transform:\n          steps:\n            - function: drop_nulls\n              params:\n                columns: [order_id, customer_id]\n\n      - name: write_orders\n        depends_on: [clean_orders]\n        write:\n          connection: silver\n          path: orders_clean.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"features/pipelines/#pipeline-config-options","title":"Pipeline Config Options","text":"Field Type Required Description <code>pipeline</code> string Yes Unique pipeline name <code>nodes</code> list Yes List of node configurations"},{"location":"features/pipelines/#node-config-options","title":"Node Config Options","text":"Field Type Required Description <code>name</code> string Yes Unique node name within pipeline <code>depends_on</code> list No List of upstream node names <code>read</code> object No Read configuration <code>transform</code> object No Transform steps configuration <code>validation</code> object No Data quality tests <code>write</code> object No Write configuration <code>on_error</code> string No Error strategy: <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code> <code>cache</code> bool No Cache output in memory"},{"location":"features/pipelines/#execution-modes","title":"Execution Modes","text":""},{"location":"features/pipelines/#serial-vs-parallel","title":"Serial vs Parallel","text":"<pre><code># Serial execution (default)\nresults = manager.run()\n\n# Parallel execution with 4 workers\nresults = manager.run(parallel=True, max_workers=4)\n</code></pre> <p>In parallel mode, nodes are grouped into execution layers. Nodes within the same layer have no dependencies on each other and execute concurrently.</p>"},{"location":"features/pipelines/#dry-run-mode","title":"Dry Run Mode","text":"<p>Simulate execution without performing actual read/write operations:</p> <pre><code>results = manager.run(dry_run=True)\n</code></pre> <p>Dry run validates: - Configuration syntax - Dependency graph structure - Connection availability</p>"},{"location":"features/pipelines/#resume-from-failure","title":"Resume from Failure","text":"<p>Skip nodes that completed successfully in the previous run:</p> <pre><code>results = manager.run(resume_from_failure=True)\n</code></pre> <p>Resume capability: - Tracks node version hashes to detect config changes - Restores output from persisted writes - Invalidates downstream nodes when upstream re-executes</p>"},{"location":"features/pipelines/#error-strategies","title":"Error Strategies","text":"<p>Control how the pipeline handles node failures:</p> Strategy Behavior <code>fail_fast</code> Stop immediately on first failure <code>fail_later</code> Complete current layer, then stop <code>ignore</code> Log error and continue execution <pre><code>nodes:\n  - name: optional_enrichment\n    on_error: ignore  # Continue even if this fails\n    # ...\n</code></pre> <p>Override at runtime:</p> <pre><code>results = manager.run(on_error=\"fail_fast\")\n</code></pre>"},{"location":"features/pipelines/#features","title":"Features","text":""},{"location":"features/pipelines/#dependency-resolution","title":"Dependency Resolution","text":"<p>The pipeline automatically determines execution order:</p> <pre><code># Get execution order\norder = pipeline.graph.topological_sort()\n# ['load_orders', 'clean_orders', 'write_orders']\n\n# Visualize the graph\nprint(pipeline.visualize())\n</code></pre> <p>Output:</p> <pre><code>Dependency Graph:\n\nLayer 1:\n  - load_orders\n\nLayer 2:\n  - clean_orders (depends on: load_orders)\n\nLayer 3:\n  - write_orders (depends on: clean_orders)\n</code></pre>"},{"location":"features/pipelines/#execution-layers","title":"Execution Layers","text":"<p>For parallel execution, nodes are grouped into layers:</p> <pre><code>layers = pipeline.get_execution_layers()\n# [['load_orders'], ['clean_orders'], ['write_orders']]\n</code></pre> <p>Nodes in the same layer can run simultaneously.</p>"},{"location":"features/pipelines/#drift-detection","title":"Drift Detection","text":"<p>When a System Catalog is configured, the pipeline detects drift between local and deployed configurations:</p> <pre><code>\u26a0\ufe0f DRIFT DETECTED: Local pipeline definition differs from Catalog.\n   Local Hash: a1b2c3d4\n   Catalog Hash: e5f6g7h8\n   Advice: Deploy changes using 'odibi deploy' before running in production.\n</code></pre> <p>Deploy to sync:</p> <pre><code>manager.deploy()  # Deploy all pipelines\nmanager.deploy(\"bronze_to_silver\")  # Deploy specific pipeline\n</code></pre>"},{"location":"features/pipelines/#lineage-integration","title":"Lineage Integration","text":"<p>Pipelines automatically emit OpenLineage events when configured:</p> <pre><code>lineage:\n  enabled: true\n  backend: file\n  path: ./lineage\n</code></pre> <p>Events include: - Pipeline start/complete - Node start/complete - Input/output datasets - Schema information</p>"},{"location":"features/pipelines/#api-examples","title":"API Examples","text":""},{"location":"features/pipelines/#create-from-yaml","title":"Create from YAML","text":"<pre><code>from odibi.pipeline import Pipeline, PipelineManager\n\n# Recommended: Use Pipeline.from_yaml() for convenience\nmanager = Pipeline.from_yaml(\"config.yaml\")\n\n# Or directly use PipelineManager\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# With environment overrides\nmanager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n</code></pre>"},{"location":"features/pipelines/#run-pipelines","title":"Run Pipelines","text":"<pre><code># Run all pipelines\nresults = manager.run()\n\n# Run specific pipeline\nresults = manager.run(\"bronze_to_silver\")\n\n# Run multiple pipelines\nresults = manager.run([\"bronze_to_silver\", \"silver_to_gold\"])\n\n# Run with options\nresults = manager.run(\n    parallel=True,\n    max_workers=8,\n    dry_run=False,\n    resume_from_failure=True,\n    on_error=\"fail_fast\"\n)\n</code></pre>"},{"location":"features/pipelines/#check-results","title":"Check Results","text":"<pre><code># Single pipeline returns PipelineResults\nif not results.failed:\n    print(f\"Success! Processed {len(results.completed)} nodes in {results.duration:.2f}s\")\nelse:\n    print(f\"Failed nodes: {results.failed}\")\n\n# Access individual node results\nfor node_name, node_result in results.node_results.items():\n    print(f\"{node_name}: {node_result.rows_processed} rows in {node_result.duration:.2f}s\")\n\n# Get story path\nif results.story_path:\n    print(f\"Execution story: {results.story_path}\")\n</code></pre>"},{"location":"features/pipelines/#pipeline-validation","title":"Pipeline Validation","text":"<pre><code># Validate without executing\nvalidation = pipeline.validate()\n\nif validation[\"valid\"]:\n    print(f\"Pipeline valid with {validation['node_count']} nodes\")\n    print(f\"Execution order: {validation['execution_order']}\")\nelse:\n    print(f\"Errors: {validation['errors']}\")\n\nif validation[\"warnings\"]:\n    print(f\"Warnings: {validation['warnings']}\")\n</code></pre>"},{"location":"features/pipelines/#list-and-access-pipelines","title":"List and Access Pipelines","text":"<pre><code># List available pipelines\nprint(manager.list_pipelines())\n# ['bronze_to_silver', 'silver_to_gold']\n\n# Get specific pipeline instance\npipeline = manager.get_pipeline(\"bronze_to_silver\")\n\n# Execute single node (for debugging)\nresult = pipeline.execute_node(\"clean_orders\")\n</code></pre>"},{"location":"features/pipelines/#complete-example","title":"Complete Example","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nconnections:\n  raw:\n    type: azure_adls\n    account: ${AZURE_STORAGE_ACCOUNT}\n    container: raw\n  silver:\n    type: delta\n    path: abfss://silver@${AZURE_STORAGE_ACCOUNT}.dfs.core.windows.net/\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    on_events: [on_failure]\n\npipelines:\n  - pipeline: sales_daily\n    nodes:\n      - name: ingest_transactions\n        read:\n          connection: raw\n          path: transactions/\n          format: parquet\n          incremental:\n            mode: rolling_window\n            column: transaction_date\n            lookback: 7\n            unit: day\n\n      - name: validate_transactions\n        depends_on: [ingest_transactions]\n        validation:\n          tests:\n            - type: not_null\n              columns: [transaction_id, amount]\n            - type: positive\n              columns: [amount]\n          on_fail: quarantine\n          quarantine:\n            connection: silver\n            path: quarantine/transactions\n\n      - name: aggregate_daily\n        depends_on: [validate_transactions]\n        transform:\n          steps:\n            - function: group_by_sum\n              params:\n                group_cols: [store_id, transaction_date]\n                sum_cols: [amount]\n        on_error: fail_fast\n\n      - name: write_daily_sales\n        depends_on: [aggregate_daily]\n        write:\n          connection: silver\n          path: sales/daily\n          format: delta\n          mode: merge\n          merge_keys: [store_id, transaction_date]\n</code></pre> <pre><code>from odibi.pipeline import Pipeline\n\nmanager = Pipeline.from_yaml(\"sales_config.yaml\")\nresults = manager.run(parallel=True, max_workers=4)\n\nif results.failed:\n    print(f\"Pipeline failed: {results.failed}\")\nelse:\n    print(f\"Daily sales updated: {results.story_path}\")\n</code></pre>"},{"location":"features/pipelines/#related","title":"Related","text":"<ul> <li>Alerting - Configure notifications for pipeline events</li> <li>Quality Gates - Block pipelines on data quality failures</li> <li>Quarantine Tables - Handle invalid rows</li> <li>Lineage - Track data flow across pipelines</li> </ul>"},{"location":"features/plugins/","title":"Plugins &amp; Extensibility","text":"<p>Extend Odibi with custom connections, transforms, and engines through a flexible plugin system.</p>"},{"location":"features/plugins/#overview","title":"Overview","text":"<p>Odibi's plugin system provides: - Connection plugins: Add custom data source connectors - Transform plugins: Register custom data transformation functions - Engine plugins: Support for different processing engines - Auto-discovery: Automatic loading of <code>transforms.py</code> and <code>plugins.py</code> - Entry points: Standard Python packaging for distributable plugins</p>"},{"location":"features/plugins/#plugin-types","title":"Plugin Types","text":""},{"location":"features/plugins/#connection-plugins","title":"Connection Plugins","text":"<p>Add support for new data sources by registering connection factories:</p> <pre><code>from odibi.plugins import register_connection_factory\n\ndef create_my_connection(name: str, config: dict):\n    \"\"\"Factory function for custom connection.\"\"\"\n    return MyCustomConnection(\n        host=config.get(\"host\"),\n        port=config.get(\"port\", 5432),\n    )\n\nregister_connection_factory(\"my_db\", create_my_connection)\n</code></pre> <p>Once registered, use in YAML config:</p> <pre><code>connections:\n  my_source:\n    type: my_db\n    host: localhost\n    port: 5432\n</code></pre>"},{"location":"features/plugins/#transform-plugins","title":"Transform Plugins","text":"<p>Register custom data transformation functions using the <code>@transform</code> decorator:</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef clean_phone_numbers(context, current, country_code=\"US\"):\n    \"\"\"Standardize phone number format.\"\"\"\n    df = current\n    # Transform logic here\n    return df\n\n@transform(\"custom_name\")\ndef my_transform(context, current):\n    \"\"\"Transform with custom registration name.\"\"\"\n    return current\n</code></pre> <p>Use in pipeline YAML:</p> <pre><code>pipelines:\n  - pipeline: process_customers\n    nodes:\n      - name: standardize\n        transform: clean_phone_numbers\n        params:\n          country_code: \"UK\"\n</code></pre>"},{"location":"features/plugins/#engine-plugins","title":"Engine Plugins","text":"<p>Odibi supports multiple processing engines. The engine is specified at the project level:</p> <pre><code>project: MyProject\nengine: spark  # or 'pandas', 'polars'\n</code></pre>"},{"location":"features/plugins/#functionregistry","title":"FunctionRegistry","text":"<p>The <code>FunctionRegistry</code> is the central registry for transform functions.</p>"},{"location":"features/plugins/#registration-methods","title":"Registration Methods","text":"<pre><code>from odibi.registry import FunctionRegistry, transform\n\n# Method 1: Using decorator\n@transform\ndef my_transform(context, current, param1: str):\n    return current\n\n# Method 2: Direct registration\ndef another_transform(context, current):\n    return current\n\nFunctionRegistry.register(another_transform, name=\"alt_transform\")\n</code></pre>"},{"location":"features/plugins/#registry-api","title":"Registry API","text":"Method Description <code>register(func, name, param_model)</code> Register a function with optional name and Pydantic model <code>get(name)</code> Retrieve a registered function <code>validate_params(name, params)</code> Validate parameters against signature <code>list_functions()</code> List all registered function names <code>get_function_info(name)</code> Get function metadata and signature <code>get_param_model(name)</code> Get Pydantic model for parameter validation"},{"location":"features/plugins/#parameter-validation","title":"Parameter Validation","text":"<p>Use Pydantic models for strict parameter validation:</p> <pre><code>from pydantic import BaseModel\nfrom odibi.registry import transform, FunctionRegistry\n\nclass FilterParams(BaseModel):\n    column: str\n    min_value: float\n    max_value: float = 100.0\n\n@transform(param_model=FilterParams)\ndef filter_range(context, current, column: str, min_value: float, max_value: float = 100.0):\n    return current.filter((current[column] &gt;= min_value) &amp; (current[column] &lt;= max_value))\n\n# Validation happens automatically\nFunctionRegistry.validate_params(\"filter_range\", {\"column\": \"price\", \"min_value\": 10})\n</code></pre>"},{"location":"features/plugins/#connection-factory","title":"Connection Factory","text":"<p>The connection factory system allows registering custom connection types.</p>"},{"location":"features/plugins/#built-in-connections","title":"Built-in Connections","text":"Type Description <code>local</code> Local filesystem <code>http</code> HTTP/REST endpoints <code>azure_blob</code> / <code>azure_adls</code> Azure Blob Storage / ADLS Gen2 <code>delta</code> Delta Lake tables <code>sql_server</code> / <code>azure_sql</code> SQL Server / Azure SQL"},{"location":"features/plugins/#custom-factory-pattern","title":"Custom Factory Pattern","text":"<pre><code>from odibi.plugins import register_connection_factory, get_connection_factory\n\ndef create_postgres_connection(name: str, config: dict):\n    \"\"\"Create a PostgreSQL connection.\"\"\"\n    from my_connections import PostgresConnection\n\n    return PostgresConnection(\n        host=config[\"host\"],\n        port=config.get(\"port\", 5432),\n        database=config[\"database\"],\n        username=config.get(\"username\"),\n        password=config.get(\"password\"),\n    )\n\n# Register the factory\nregister_connection_factory(\"postgres\", create_postgres_connection)\n\n# Retrieve a factory (if needed)\nfactory = get_connection_factory(\"postgres\")\n</code></pre>"},{"location":"features/plugins/#auto-discovery","title":"Auto-Discovery","text":"<p>Odibi automatically loads extension files from your project directory.</p>"},{"location":"features/plugins/#supported-files","title":"Supported Files","text":"File Purpose <code>transforms.py</code> Custom transform functions <code>plugins.py</code> Connection factories and other plugins"},{"location":"features/plugins/#search-locations","title":"Search Locations","text":"<ol> <li>Config directory: Same directory as your YAML config</li> <li>Current working directory: Where you run the CLI</li> </ol>"},{"location":"features/plugins/#example-structure","title":"Example Structure","text":"<pre><code>my_project/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 transforms.py      # Auto-loaded\n\u251c\u2500\u2500 plugins.py         # Auto-loaded\n\u2514\u2500\u2500 data/\n</code></pre>"},{"location":"features/plugins/#transformspy-example","title":"transforms.py Example","text":"<pre><code>\"\"\"Custom transforms for my project.\"\"\"\n\nfrom odibi.registry import transform\n\n@transform\ndef calculate_metrics(context, current, metrics: list):\n    \"\"\"Calculate custom business metrics.\"\"\"\n    df = current\n    for metric in metrics:\n        df = df.withColumn(f\"{metric}_calculated\", ...)\n    return df\n\n@transform\ndef apply_business_rules(context, current, rule_set: str):\n    \"\"\"Apply business rules based on rule set name.\"\"\"\n    # Implementation\n    return current\n</code></pre>"},{"location":"features/plugins/#pluginspy-example","title":"plugins.py Example","text":"<pre><code>\"\"\"Custom plugins for my project.\"\"\"\n\nfrom odibi.plugins import register_connection_factory\n\ndef create_snowflake_connection(name, config):\n    from snowflake.connector import connect\n    # Create connection\n    return SnowflakeWrapper(connect(**config))\n\nregister_connection_factory(\"snowflake\", create_snowflake_connection)\n</code></pre>"},{"location":"features/plugins/#plugin-configuration","title":"Plugin Configuration","text":""},{"location":"features/plugins/#entry-points-setup","title":"Entry Points Setup","text":"<p>For distributable plugins, use Python entry points in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"odibi.connections\"]\npostgres = \"my_plugin.connections:create_postgres_connection\"\nsnowflake = \"my_plugin.connections:create_snowflake_connection\"\n</code></pre> <p>Or in <code>setup.py</code>:</p> <pre><code>setup(\n    name=\"odibi-postgres-plugin\",\n    entry_points={\n        \"odibi.connections\": [\n            \"postgres = my_plugin.connections:create_postgres_connection\",\n        ],\n    },\n)\n</code></pre>"},{"location":"features/plugins/#entry-point-groups","title":"Entry Point Groups","text":"Group Purpose <code>odibi.connections</code> Connection factory functions"},{"location":"features/plugins/#loading-plugins","title":"Loading Plugins","text":"<p>Plugins are loaded automatically at startup:</p> <pre><code>from odibi.plugins import load_plugins\n\n# Called automatically, but can be invoked manually\nload_plugins()\n</code></pre>"},{"location":"features/plugins/#complete-example","title":"Complete Example","text":""},{"location":"features/plugins/#project-structure","title":"Project Structure","text":"<pre><code>my_etl_project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 transforms.py\n\u251c\u2500\u2500 plugins.py\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_etl/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 connections/\n            \u2514\u2500\u2500 custom.py\n</code></pre>"},{"location":"features/plugins/#transformspy","title":"transforms.py","text":"<pre><code>\"\"\"Project-specific transforms.\"\"\"\n\nfrom odibi.registry import transform\nfrom pydantic import BaseModel\n\nclass EnrichmentParams(BaseModel):\n    lookup_table: str\n    key_column: str\n    value_columns: list[str]\n\n@transform(param_model=EnrichmentParams)\ndef enrich_with_lookup(\n    context,\n    current,\n    lookup_table: str,\n    key_column: str,\n    value_columns: list[str],\n):\n    \"\"\"Enrich data with lookup table values.\"\"\"\n    lookup_df = context.read(lookup_table)\n    return current.join(\n        lookup_df.select(key_column, *value_columns),\n        on=key_column,\n        how=\"left\",\n    )\n\n@transform\ndef deduplicate(context, current, key_columns: list, order_by: str = None):\n    \"\"\"Remove duplicate rows based on key columns.\"\"\"\n    if order_by:\n        from pyspark.sql import Window\n        from pyspark.sql.functions import row_number\n\n        window = Window.partitionBy(*key_columns).orderBy(order_by)\n        return current.withColumn(\"_rn\", row_number().over(window)) \\\n                      .filter(\"_rn = 1\") \\\n                      .drop(\"_rn\")\n    return current.dropDuplicates(key_columns)\n</code></pre>"},{"location":"features/plugins/#pluginspy","title":"plugins.py","text":"<pre><code>\"\"\"Connection plugins.\"\"\"\n\nfrom odibi.plugins import register_connection_factory\n\ndef create_redis_connection(name: str, config: dict):\n    \"\"\"Redis cache connection.\"\"\"\n    import redis\n\n    return redis.Redis(\n        host=config.get(\"host\", \"localhost\"),\n        port=config.get(\"port\", 6379),\n        db=config.get(\"db\", 0),\n        password=config.get(\"password\"),\n    )\n\nregister_connection_factory(\"redis\", create_redis_connection)\n</code></pre>"},{"location":"features/plugins/#configyaml","title":"config.yaml","text":"<pre><code>project: CustomerETL\nengine: spark\n\nconnections:\n  source:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: raw\n\n  cache:\n    type: redis\n    host: localhost\n    port: 6379\n\npipelines:\n  - pipeline: process_customers\n    nodes:\n      - name: load_customers\n        source: source\n        path: customers/\n\n      - name: enrich\n        input: load_customers\n        transform: enrich_with_lookup\n        params:\n          lookup_table: reference/regions\n          key_column: region_code\n          value_columns:\n            - region_name\n            - country\n\n      - name: dedupe\n        input: enrich\n        transform: deduplicate\n        params:\n          key_columns:\n            - customer_id\n          order_by: updated_at desc\n</code></pre>"},{"location":"features/plugins/#best-practices","title":"Best Practices","text":"<ol> <li>Use Pydantic models - Validate transform parameters with type safety</li> <li>Keep plugins focused - One connection type per factory function</li> <li>Handle imports lazily - Import heavy dependencies inside factory functions</li> <li>Log appropriately - Use <code>logger.info()</code> for successful loads</li> <li>Provide good defaults - Make configuration optional where sensible</li> <li>Document parameters - Use docstrings for transform functions</li> </ol>"},{"location":"features/plugins/#related","title":"Related","text":"<ul> <li>Connections - Built-in connection types</li> <li>Transformers - Built-in transform functions</li> <li>Engines - Supported processing engines</li> <li>Configuration - YAML configuration reference</li> </ul>"},{"location":"features/quality_gates/","title":"Quality Gates","text":"<p>Batch-level quality validation that evaluates the entire dataset before writing.</p>"},{"location":"features/quality_gates/#overview","title":"Overview","text":"<p>While validation tests run per-row, quality gates evaluate aggregate metrics: - Overall pass rate - What percentage of rows passed all tests? - Per-test thresholds - Different requirements for different tests - Row count anomalies - Detect unexpected batch sizes</p>"},{"location":"features/quality_gates/#configuration","title":"Configuration","text":""},{"location":"features/quality_gates/#basic-gate-setup","title":"Basic Gate Setup","text":"<pre><code>nodes:\n  - name: load_silver_customers\n    read:\n      connection: bronze\n      path: customers\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n        - type: unique\n          columns: [customer_id]\n\n      gate:\n        require_pass_rate: 0.95  # 95% must pass\n        on_fail: abort           # Stop if gate fails\n</code></pre>"},{"location":"features/quality_gates/#gate-config-options","title":"Gate Config Options","text":"Field Type Required Default Description <code>require_pass_rate</code> float No 0.95 Minimum % of rows passing ALL tests <code>on_fail</code> string No \"abort\" Action on failure <code>thresholds</code> list No [] Per-test thresholds <code>row_count</code> object No null Row count validation"},{"location":"features/quality_gates/#on-fail-actions","title":"On-Fail Actions","text":"Action Description <code>abort</code> Stop pipeline, write nothing (default) <code>warn_and_write</code> Log warning, write all rows anyway <code>write_valid_only</code> Write only rows that passed validation"},{"location":"features/quality_gates/#per-test-thresholds","title":"Per-Test Thresholds","text":"<p>Set different requirements for specific tests:</p> <pre><code>gate:\n  require_pass_rate: 0.95  # Global: 95% must pass all tests\n\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99  # 99% for not_null (stricter)\n    - test: unique\n      min_pass_rate: 1.0   # 100% unique (no duplicates allowed)\n    - test: email_format   # Named test\n      min_pass_rate: 0.90  # 90% for email format (more lenient)\n</code></pre>"},{"location":"features/quality_gates/#row-count-validation","title":"Row Count Validation","text":"<p>Detect anomalies in batch size:</p> <pre><code>gate:\n  row_count:\n    min: 100              # Fail if fewer than 100 rows\n    max: 1000000          # Fail if more than 1M rows\n    change_threshold: 0.5 # Fail if count changes &gt;50% vs last run\n</code></pre>"},{"location":"features/quality_gates/#row-count-options","title":"Row Count Options","text":"Field Type Description <code>min</code> int Minimum expected row count <code>max</code> int Maximum expected row count <code>change_threshold</code> float Max allowed change vs previous run (0.5 = 50%)"},{"location":"features/quality_gates/#complete-example","title":"Complete Example","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: bronze\n      path: orders_raw\n\n    validation:\n      tests:\n        # Critical fields\n        - type: not_null\n          name: required_fields\n          columns: [order_id, customer_id, order_date]\n\n        # Uniqueness\n        - type: unique\n          name: unique_orders\n          columns: [order_id]\n\n        # Business rules\n        - type: range\n          name: valid_amount\n          column: amount\n          min: 0\n\n        - type: accepted_values\n          name: valid_status\n          column: status\n          values: [pending, completed, cancelled]\n\n      gate:\n        # Global threshold\n        require_pass_rate: 0.95\n\n        # Per-test overrides\n        thresholds:\n          - test: required_fields\n            min_pass_rate: 0.99\n          - test: unique_orders\n            min_pass_rate: 1.0\n\n        # Row count checks\n        row_count:\n          min: 1000\n          change_threshold: 0.3\n\n        # What to do on failure\n        on_fail: abort\n\n    write:\n      connection: silver\n      path: orders\n      format: delta\n</code></pre>"},{"location":"features/quality_gates/#combining-gates-with-quarantine","title":"Combining Gates with Quarantine","text":"<p>Use both for comprehensive data quality:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine  # Route failures to quarantine\n\n    - type: unique\n      columns: [customer_id]\n      on_fail: fail        # Critical - must pass\n\n  quarantine:\n    connection: silver\n    path: quarantine/customers\n\n  gate:\n    require_pass_rate: 0.95  # Still need 95% overall\n    on_fail: abort\n</code></pre> <p>Flow: 1. Rows failing <code>not_null</code> are quarantined 2. Gate evaluates remaining rows 3. If &lt;95% pass, pipeline aborts 4. Otherwise, valid rows are written</p>"},{"location":"features/quality_gates/#gate-failure-alerts","title":"Gate Failure Alerts","text":"<p>Get notified when gates fail:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n</code></pre> <p>Alert payload includes: - Pass rate achieved vs required - Number of failed rows - Failure reasons</p>"},{"location":"features/quality_gates/#gatefailederror","title":"GateFailedError","text":"<p>When a gate fails with <code>on_fail: abort</code>, a <code>GateFailedError</code> is raised:</p> <pre><code>from odibi.exceptions import GateFailedError\n\ntry:\n    pipeline.run()\nexcept GateFailedError as e:\n    print(f\"Gate failed: {e.pass_rate:.1%} &lt; {e.required_rate:.1%}\")\n    print(f\"Reasons: {e.failure_reasons}\")\n</code></pre>"},{"location":"features/quality_gates/#best-practices","title":"Best Practices","text":"<ol> <li>Start with high thresholds - Be strict initially, relax as needed</li> <li>Use per-test thresholds - Critical tests (uniqueness) should be 100%</li> <li>Monitor row count changes - Sudden changes often indicate problems</li> <li>Combine with quarantine - Don't lose failed data, route it for analysis</li> <li>Set up alerts - Know immediately when gates fail</li> </ol>"},{"location":"features/quality_gates/#related","title":"Related","text":"<ul> <li>Quarantine Tables - Route failed rows</li> <li>Alerting - Alert on gate failures</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/quarantine/","title":"Quarantine Tables","text":"<p>Route failed validation rows to a dedicated quarantine table with rejection metadata for later analysis and reprocessing.</p>"},{"location":"features/quarantine/#overview","title":"Overview","text":"<p>When validation tests fail, Odibi provides three options via <code>on_fail</code>: - <code>fail</code> - Stop the entire pipeline (default) - <code>warn</code> - Log and continue with all rows - <code>quarantine</code> - Route failed rows to a quarantine table, continue with valid rows</p> <p>The quarantine option preserves bad data for debugging without blocking production pipelines.</p>"},{"location":"features/quarantine/#configuration","title":"Configuration","text":""},{"location":"features/quarantine/#basic-quarantine-setup","title":"Basic Quarantine Setup","text":"<pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id, email]\n          on_fail: quarantine  # Route failures to quarantine\n        - type: regex_match\n          column: email\n          pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n          on_fail: quarantine\n\n      quarantine:\n        connection: silver\n        path: quarantine/customers\n        add_columns:\n          _rejection_reason: true\n          _rejected_at: true\n          _source_batch_id: true\n          _failed_tests: true\n</code></pre>"},{"location":"features/quarantine/#quarantine-config-options","title":"Quarantine Config Options","text":"Field Type Required Description <code>connection</code> string Yes Connection for quarantine writes <code>path</code> string No* Path for quarantine data <code>table</code> string No* Table name for quarantine <code>add_columns</code> object No Metadata columns to add <code>retention_days</code> int No Days to retain (default: 90) <p>*Either <code>path</code> or <code>table</code> is required.</p>"},{"location":"features/quarantine/#metadata-columns","title":"Metadata Columns","text":"<p>Control which metadata columns are added to quarantined rows:</p> <pre><code>quarantine:\n  connection: silver\n  path: quarantine/customers\n  add_columns:\n    _rejection_reason: true    # Description of why row failed\n    _rejected_at: true         # UTC timestamp of rejection\n    _source_batch_id: true     # Run ID for traceability\n    _failed_tests: true        # Comma-separated list of failed tests\n    _original_node: false      # Node name (disabled by default)\n</code></pre>"},{"location":"features/quarantine/#how-it-works","title":"How It Works","text":"<ol> <li>Test Evaluation: Each test with <code>on_fail: quarantine</code> is evaluated per-row</li> <li>Row Splitting: DataFrame is split into valid and invalid portions</li> <li>Metadata Addition: Failed rows receive metadata columns</li> <li>Quarantine Write: Invalid rows are appended to the quarantine table</li> <li>Pipeline Continues: Valid rows proceed through the pipeline</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Input Data    \u2502\n\u2502   (100 rows)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Validation    \u2502\n\u2502   Tests Run     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n    \u25bc         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Valid \u2502  \u2502  Invalid  \u2502\n\u2502(95)   \u2502  \u2502   (5)     \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502            \u2502\n    \u25bc            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Target \u2502  \u2502Quarantine \u2502\n\u2502 Table \u2502  \u2502  Table    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/quarantine/#example-complete-quarantine-pipeline","title":"Example: Complete Quarantine Pipeline","text":"<pre><code>project: CustomerData\nengine: spark\n\nconnections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n  silver:\n    type: local\n    base_path: ./data/silver\n\npipelines:\n  - pipeline: ingest_customers\n    layer: silver\n    nodes:\n      - name: validate_customers\n        read:\n          connection: bronze\n          path: customers_raw\n          format: parquet\n\n        validation:\n          tests:\n            # Required fields\n            - type: not_null\n              columns: [customer_id, email, created_at]\n              on_fail: quarantine\n\n            # Email format\n            - type: regex_match\n              column: email\n              pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n              on_fail: quarantine\n\n            # Age validation\n            - type: range\n              column: age\n              min: 0\n              max: 150\n              on_fail: quarantine\n\n          quarantine:\n            connection: silver\n            path: quarantine/customers\n            add_columns:\n              _rejection_reason: true\n              _rejected_at: true\n              _source_batch_id: true\n              _failed_tests: true\n\n        write:\n          connection: silver\n          path: customers\n          format: delta\n          mode: append\n</code></pre>"},{"location":"features/quarantine/#querying-quarantine-data","title":"Querying Quarantine Data","text":"<p>After running the pipeline, query the quarantine table to analyze failures:</p> <pre><code>-- View recent quarantined rows\nSELECT\n    customer_id,\n    email,\n    _rejection_reason,\n    _failed_tests,\n    _rejected_at,\n    _source_batch_id\nFROM quarantine.customers\nWHERE _rejected_at &gt;= current_date() - INTERVAL 7 DAYS\nORDER BY _rejected_at DESC;\n\n-- Count failures by test type\nSELECT\n    _failed_tests,\n    COUNT(*) as count\nFROM quarantine.customers\nGROUP BY _failed_tests\nORDER BY count DESC;\n</code></pre>"},{"location":"features/quarantine/#alerts-for-quarantine-events","title":"Alerts for Quarantine Events","text":"<p>Configure alerts to notify when rows are quarantined:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-quality\"\n</code></pre>"},{"location":"features/quarantine/#best-practices","title":"Best Practices","text":"<ol> <li>Use meaningful test names - Helps identify failures in quarantine data</li> <li>Set appropriate retention - Balance storage costs vs debugging needs</li> <li>Monitor quarantine rates - High rates may indicate upstream data issues</li> <li>Combine with gates - Use quality gates to abort if too many rows are quarantined</li> <li>Automate reprocessing - Build workflows to reprocess fixed quarantine data</li> </ol>"},{"location":"features/quarantine/#related","title":"Related","text":"<ul> <li>Quality Gates - Batch-level validation</li> <li>Alerting - Alert on quarantine events</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/schema_tracking/","title":"Schema Version Tracking","text":"<p>Track schema changes over time with automatic versioning, change detection, and CLI tools.</p>"},{"location":"features/schema_tracking/#overview","title":"Overview","text":"<p>Odibi automatically tracks schema changes in the System Catalog: - Version history: Every schema change creates a new version - Change detection: Identifies added, removed, and modified columns - CLI tools: Query history and compare versions</p>"},{"location":"features/schema_tracking/#how-it-works","title":"How It Works","text":"<ol> <li>After each pipeline run, the output schema is captured</li> <li>Schema is hashed and compared to the previous version</li> <li>If changed, a new version is recorded with change details</li> <li>History is stored in <code>meta_schemas</code> table</li> </ol>"},{"location":"features/schema_tracking/#automatic-tracking","title":"Automatic Tracking","text":"<p>Schema tracking happens automatically during pipeline execution when: - A node writes to a table/path - The System Catalog is configured</p> <p>No additional configuration is required.</p>"},{"location":"features/schema_tracking/#cli-commands","title":"CLI Commands","text":""},{"location":"features/schema_tracking/#view-schema-history","title":"View Schema History","text":"<pre><code>odibi schema history &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi schema history silver/customers --config pipeline.yaml\n\nSchema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre> <p>Options:</p> <pre><code>odibi schema history &lt;table&gt; --config config.yaml \\\n    --limit 20 \\        # Show last 20 versions (default: 10)\n    --format json       # Output as JSON\n</code></pre>"},{"location":"features/schema_tracking/#compare-schema-versions","title":"Compare Schema Versions","text":"<pre><code>odibi schema diff &lt;table&gt; --config config.yaml --from-version 3 --to-version 5\n</code></pre> <p>Example:</p> <pre><code>$ odibi schema diff silver/customers --config pipeline.yaml --from-version 3 --to-version 5\n\nSchema Diff: silver/customers\nFrom v3 \u2192 v5\n============================================================\n  customer_id                  STRING               (unchanged)\n  email                        STRING               (unchanged)\n  name                         STRING               (unchanged)\n- legacy_id                    STRING               (removed in v5)\n+ loyalty_tier                 STRING               (added in v5)\n+ created_at                   TIMESTAMP            (added in v5)\n+ updated_at                   TIMESTAMP            (added in v5)\n</code></pre> <p>Without versions (compares latest two):</p> <pre><code>odibi schema diff silver/customers --config pipeline.yaml\n</code></pre>"},{"location":"features/schema_tracking/#programmatic-access","title":"Programmatic Access","text":""},{"location":"features/schema_tracking/#track-schema-manually","title":"Track Schema Manually","text":"<pre><code>from odibi.catalog import CatalogManager\n\ncatalog = CatalogManager(spark, config, base_path, engine)\n\n# Track a schema change\nresult = catalog.track_schema(\n    table_path=\"silver/customers\",\n    schema={\"customer_id\": \"STRING\", \"email\": \"STRING\", \"age\": \"INT\"},\n    pipeline=\"customer_pipeline\",\n    node=\"process_customers\",\n    run_id=\"run-12345\",\n)\n\nprint(f\"Changed: {result['changed']}\")\nprint(f\"Version: {result['version']}\")\nprint(f\"Columns added: {result.get('columns_added', [])}\")\nprint(f\"Columns removed: {result.get('columns_removed', [])}\")\nprint(f\"Types changed: {result.get('columns_type_changed', [])}\")\n</code></pre>"},{"location":"features/schema_tracking/#query-schema-history","title":"Query Schema History","text":"<pre><code># Get history for a table\nhistory = catalog.get_schema_history(\"silver/customers\", limit=10)\n\nfor record in history:\n    print(f\"v{record['schema_version']}: {record['captured_at']}\")\n    if record.get('columns_added'):\n        print(f\"  Added: {record['columns_added']}\")\n</code></pre>"},{"location":"features/schema_tracking/#schema-record-structure","title":"Schema Record Structure","text":"<p>Each schema version record includes:</p> Field Description <code>table_path</code> Full path to the table <code>schema_version</code> Auto-incrementing version number <code>schema_hash</code> MD5 hash of column definitions <code>columns</code> JSON of column names and types <code>captured_at</code> Timestamp of capture <code>pipeline</code> Pipeline that made the change <code>node</code> Node that made the change <code>run_id</code> Execution run ID <code>columns_added</code> List of new columns <code>columns_removed</code> List of removed columns <code>columns_type_changed</code> List of columns with type changes"},{"location":"features/schema_tracking/#storage-location","title":"Storage Location","text":"<p>Schema history is stored in the System Catalog:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n</code></pre> <p>Location: <code>{connection_base_path}/_odibi_system/meta_schemas/</code></p>"},{"location":"features/schema_tracking/#example-detecting-breaking-changes","title":"Example: Detecting Breaking Changes","text":"<p>Use schema tracking to detect breaking changes before they impact downstream:</p> <pre><code>def check_for_breaking_changes(catalog, table_path):\n    \"\"\"Check if recent schema changes might break downstream.\"\"\"\n    history = catalog.get_schema_history(table_path, limit=2)\n\n    if len(history) &lt; 2:\n        return False  # No previous version\n\n    latest = history[0]\n    removed = latest.get('columns_removed', [])\n    type_changes = latest.get('columns_type_changed', [])\n\n    if removed or type_changes:\n        print(f\"\u26a0\ufe0f Potential breaking changes in {table_path}\")\n        if removed:\n            print(f\"  Removed columns: {removed}\")\n        if type_changes:\n            print(f\"  Type changes: {type_changes}\")\n        return True\n\n    return False\n</code></pre>"},{"location":"features/schema_tracking/#integration-with-lineage","title":"Integration with Lineage","text":"<p>Combine schema tracking with lineage to assess impact:</p> <pre><code># Check what would be affected by a schema change\nodibi lineage impact silver/customers --config config.yaml\n</code></pre> <pre><code>\u26a0\ufe0f  Impact Analysis: silver/customers\n============================================================\n\nChanges to silver/customers would affect:\n\n  Affected Tables:\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 2 downstream table(s) in 2 pipeline(s)\n</code></pre>"},{"location":"features/schema_tracking/#best-practices","title":"Best Practices","text":"<ol> <li>Review schema changes - Check history after deployments</li> <li>Monitor for removals - Removed columns often break downstream</li> <li>Document type changes - Type changes may affect queries</li> <li>Use lineage for impact - Know what's affected before changing</li> <li>Automate checks - Add schema validation to CI/CD</li> </ol>"},{"location":"features/schema_tracking/#related","title":"Related","text":"<ul> <li>Cross-Pipeline Lineage - Impact analysis</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/state/","title":"State Management","text":"<p>Odibi tracks pipeline execution state, high-water marks (HWM), and run history for resumable, incremental processing.</p>"},{"location":"features/state/#overview","title":"Overview","text":"<p>State management enables:</p> <ul> <li>Resume from failure: Skip successfully completed nodes</li> <li>High-water marks: Track last processed timestamp/ID for incremental loads</li> <li>Run history: Query past executions and their outcomes</li> <li>Concurrent writes: Safe multi-pipeline execution with retry logic</li> </ul>"},{"location":"features/state/#state-backends","title":"State Backends","text":""},{"location":"features/state/#localjsonstatebackend","title":"LocalJSONStateBackend","text":"<p>Simple JSON file storage for local development:</p> <pre><code>from odibi.state import LocalJSONStateBackend\n\nbackend = LocalJSONStateBackend(\".odibi/state.json\")\n\n# Used automatically when no system catalog is configured\n</code></pre> <p>Storage location: <code>.odibi/state.json</code></p>"},{"location":"features/state/#catalogstatebackend","title":"CatalogStateBackend","text":"<p>Delta Lake-based storage for production (supports Spark and local deltalake):</p> <pre><code>from odibi.state import CatalogStateBackend\n\nbackend = CatalogStateBackend(\n    meta_runs_path=\"/path/to/meta_runs\",\n    meta_state_path=\"/path/to/meta_state\",\n    spark_session=spark,           # Optional\n    storage_options={\"key\": \"val\"} # For Azure/S3\n)\n</code></pre> <p>Configured via <code>system</code> in your YAML config:</p> <pre><code>system:\n  connection: my_storage\n  path: _system\n\nconnections:\n  my_storage:\n    type: local  # or azure_blob\n    base_path: ./data\n</code></pre>"},{"location":"features/state/#statemanager-api","title":"StateManager API","text":"<p>The <code>StateManager</code> wraps a backend and provides high-level operations:</p> <pre><code>from odibi.state import StateManager, create_state_backend\nfrom odibi.config import load_config_from_file\n\n# Create from config\nconfig = load_config_from_file(\"odibi.yaml\")\nbackend = create_state_backend(config, project_root=\".\")\nstate_mgr = StateManager(backend=backend)\n</code></pre>"},{"location":"features/state/#high-water-marks","title":"High-Water Marks","text":"<pre><code># Get HWM value\nlast_id = state_mgr.get_hwm(\"orders.last_processed_id\")\n\n# Set HWM value\nstate_mgr.set_hwm(\"orders.last_processed_id\", 12345)\n\n# Batch set (efficient for parallel pipelines)\nstate_mgr.set_hwm_batch([\n    {\"key\": \"orders.hwm\", \"value\": 100},\n    {\"key\": \"customers.hwm\", \"value\": 200},\n])\n</code></pre>"},{"location":"features/state/#run-status","title":"Run Status","text":"<pre><code># Check if node succeeded in last run\nsuccess = state_mgr.get_last_run_status(\"pipeline_name\", \"node_name\")\n\n# Get full run info (metadata, timestamp)\ninfo = state_mgr.get_last_run_info(\"pipeline_name\", \"node_name\")\n# Returns: {\"success\": True, \"metadata\": {...}}\n</code></pre>"},{"location":"features/state/#save-pipeline-run","title":"Save Pipeline Run","text":"<pre><code># Called automatically by PipelineManager\nstate_mgr.save_pipeline_run(\"my_pipeline\", results)\n</code></pre>"},{"location":"features/state/#concurrent-write-handling","title":"Concurrent Write Handling","text":"<p>The <code>CatalogStateBackend</code> handles Delta Lake concurrent write conflicts with automatic retry:</p> <ul> <li>Exponential backoff: 1s, 2s, 4s, 8s, 16s delays</li> <li>Jitter: Random 0-1s added to prevent thundering herd</li> <li>Max retries: 5 attempts before failing</li> <li>Conflict detection: Catches <code>ConcurrentAppendException</code> and similar</li> </ul> <p>This enables safe parallel pipeline execution on shared state tables.</p>"},{"location":"features/state/#configuration","title":"Configuration","text":""},{"location":"features/state/#using-system-catalog-recommended","title":"Using System Catalog (Recommended)","text":"<pre><code>project: MyProject\nengine: spark\n\nsystem:\n  connection: catalog_storage\n  path: _system\n\nconnections:\n  catalog_storage:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: odibi\n    auth:\n      mode: account_key\n      account_key: ${AZURE_STORAGE_KEY}\n</code></pre>"},{"location":"features/state/#local-development-auto-fallback","title":"Local Development (Auto-fallback)","text":"<p>If no <code>system</code> config is provided, Odibi uses <code>LocalJSONStateBackend</code> automatically:</p> <pre><code>\u26a0\ufe0f No system catalog configured. Using local JSON state backend (local-only mode).\n</code></pre>"},{"location":"features/state/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/state/#resume-not-working-nodes-re-run-every-time","title":"Resume not working - nodes re-run every time","text":"<p>Symptom: <code>--resume</code> flag doesn't skip completed nodes.</p> <p>Causes: - No system catalog configured (state not persisted) - State file deleted or corrupted - Node name changed between runs</p> <p>Fixes:</p> <pre><code># Ensure system catalog is configured\nsystem:\n  connection: catalog_conn\n  meta_runs_path: meta/runs\n  meta_state_path: meta/state\n</code></pre>"},{"location":"features/state/#high-water-mark-hwm-not-updating","title":"High Water Mark (HWM) not updating","text":"<p>Symptom: Incremental reads fetch all data instead of new records.</p> <p>Causes: - First run always does full load (expected) - HWM column has NULL values - State backend not persisting</p> <p>Fixes:</p> <pre><code># Check current HWM state\nodibi catalog state --config config.yaml\n\n# Verify HWM column has no NULLs\n# HWM is set to MAX(column) after successful run\n</code></pre>"},{"location":"features/state/#state-corruption-after-failed-run","title":"State corruption after failed run","text":"<p>Symptom: Pipeline behaves unexpectedly after a failure.</p> <p>Fix: Reset state for specific node:</p> <pre><code># View current state\nodibi catalog state --config config.yaml\n\n# If needed, delete and re-run (full load)\n# State will be rebuilt on next successful run\n</code></pre>"},{"location":"features/state/#local-json-state-lost-between-environments","title":"Local JSON state lost between environments","text":"<p>Cause: <code>LocalJSONStateBackend</code> stores state in <code>.odibi/state.json</code> locally.</p> <p>Fix: Use <code>CatalogStateBackend</code> with Delta Lake for shared/persistent state:</p> <pre><code>system:\n  connection: shared_storage\n  meta_runs_path: _odibi/runs\n  meta_state_path: _odibi/state\n</code></pre>"},{"location":"features/state/#concurrent-pipeline-runs-corrupt-state","title":"Concurrent pipeline runs corrupt state","text":"<p>Symptom: State inconsistent when multiple pipelines run simultaneously.</p> <p>Fix: Use Delta Lake catalog backend (supports concurrent writes with retry):</p> <pre><code>system:\n  connection: delta_catalog\n  meta_state_path: _odibi/state  # Delta table with ACID support\n</code></pre>"},{"location":"features/state/#related","title":"Related","text":"<ul> <li>Incremental Loading \u2014 HWM-based incremental</li> <li>Catalog \u2014 System catalog for metadata</li> <li>CLI Guide \u2014 <code>odibi catalog state</code> command</li> </ul>"},{"location":"features/stories/","title":"Execution Stories","text":"<p>Auto-generated pipeline execution documentation with rich metadata, sample data, and multiple output formats.</p>"},{"location":"features/stories/#overview","title":"Overview","text":"<p>Odibi's Story system provides: - Execution timeline: Complete record of pipeline runs with timestamps - Node-level metrics: Duration, row counts, schema changes per node - Sample data capture: Input/output samples with automatic redaction - Multiple renderers: HTML, Markdown, JSON output formats - Themes: Customizable styling for HTML reports - Retention policies: Automatic cleanup of old stories</p>"},{"location":"features/stories/#configuration","title":"Configuration","text":""},{"location":"features/stories/#basic-story-setup","title":"Basic Story Setup","text":"<pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre>"},{"location":"features/stories/#story-config-options","title":"Story Config Options","text":"Field Type Required Default Description <code>connection</code> string Yes - Connection name for story output <code>path</code> string Yes - Path for stories (relative to connection base_path) <code>max_sample_rows</code> int No <code>10</code> Maximum rows to include in samples <code>retention_days</code> int No <code>30</code> Days to keep stories before cleanup <code>retention_count</code> int No <code>100</code> Maximum number of stories to retain <code>failure_sample_size</code> int No <code>100</code> Rows to capture per validation failure <code>max_failure_samples</code> int No <code>500</code> Total failed rows across all validations <code>max_sampled_validations</code> int No <code>5</code> After this many validations, show only counts <code>theme</code> string No <code>default</code> Theme name or path to YAML theme file <code>include_samples</code> bool No <code>true</code> Whether to include data samples"},{"location":"features/stories/#remote-storage","title":"Remote Storage","text":"<p>Stories can be written to remote storage (ADLS, S3) using fsspec:</p> <pre><code>story:\n  output_path: abfss://container@account.dfs.core.windows.net/stories/\n  storage_options:\n    account_key: \"${STORAGE_ACCOUNT_KEY}\"\n</code></pre>"},{"location":"features/stories/#story-contents","title":"Story Contents","text":"<p>Each story captures comprehensive execution metadata:</p>"},{"location":"features/stories/#execution-timeline","title":"Execution Timeline","text":"Metric Description <code>started_at</code> ISO timestamp when pipeline started <code>completed_at</code> ISO timestamp when pipeline finished <code>duration</code> Total execution time in seconds <code>run_id</code> Unique identifier for the run"},{"location":"features/stories/#node-results","title":"Node Results","text":"<p>For each node in the pipeline:</p> Metric Description <code>node_name</code> Name of the node <code>operation</code> Operation type (read, transform, write) <code>status</code> Execution status: <code>success</code>, <code>failed</code>, <code>skipped</code> <code>duration</code> Node execution time in seconds <code>rows_in</code> Input row count <code>rows_out</code> Output row count <code>rows_change</code> Row count difference <code>rows_change_pct</code> Percentage change in row count"},{"location":"features/stories/#sample-data","title":"Sample Data","text":"<p>Sample data is captured with automatic redaction of sensitive values:</p> <pre><code>sample_data:\n  - order_id: 12345\n    customer_email: \"[REDACTED]\"\n    amount: 99.99\n  - order_id: 12346\n    customer_email: \"[REDACTED]\"\n    amount: 149.99\n</code></pre> <p>Configure sample capture:</p> <pre><code>story:\n  max_sample_rows: 5      # Limit sample size\n  include_samples: true   # Enable/disable samples\n</code></pre>"},{"location":"features/stories/#schema-changes","title":"Schema Changes","text":"<p>Stories track schema evolution:</p> Field Description <code>schema_in</code> Input column names <code>schema_out</code> Output column names <code>columns_added</code> New columns added <code>columns_removed</code> Columns removed <code>columns_renamed</code> Renamed columns"},{"location":"features/stories/#validation-results","title":"Validation Results","text":"<p>Validation warnings and errors are captured:</p> <pre><code>validation_warnings:\n  - \"Column 'email' has 5% null values\"\n  - \"Date range extends beyond expected bounds\"\n</code></pre> <p>Error details for failed nodes:</p> <pre><code>error_type: ValueError\nerror_message: \"Column 'order_id' contains duplicate values\"\nerror_traceback: \"Full Python traceback...\"\nerror_traceback_cleaned: \"Cleaned traceback (Spark/Java noise removed)\"\n</code></pre>"},{"location":"features/stories/#execution-steps","title":"Execution Steps","text":"<p>Stories capture the execution steps taken during node processing for debugging:</p> <pre><code>execution_steps:\n  - \"Read from bronze_db\"\n  - \"Applied pattern 'deduplicate'\"\n  - \"Executed 2 pre-SQL statement(s)\"\n  - \"Passed 3 contract checks\"\n</code></pre>"},{"location":"features/stories/#failed-rows-samples","title":"Failed Rows Samples","text":"<p>When validations fail, stories capture sample rows that failed each validation:</p> <pre><code>failed_rows_samples:\n  not_null_customer_id:\n    - { order_id: 123, customer_id: null, amount: 50.00 }\n    - { order_id: 456, customer_id: null, amount: 75.00 }\n  positive_amount:\n    - { order_id: 789, customer_id: \"C001\", amount: -10.00 }\n\nfailed_rows_counts:\n  not_null_customer_id: 150\n  positive_amount: 25\n</code></pre> <p>Configure failure sample limits:</p> <pre><code>story:\n  failure_sample_size: 100        # Max rows per validation\n  max_failure_samples: 500        # Total rows across all validations\n  max_sampled_validations: 5      # After 5 validations, show only counts\n</code></pre>"},{"location":"features/stories/#retry-history","title":"Retry History","text":"<p>When retries occur, the full history is captured:</p> <pre><code>retry_history:\n  - attempt: 1\n    success: false\n    error: \"Connection timeout\"\n    error_type: \"TimeoutError\"\n    duration: 1.2\n  - attempt: 2\n    success: false\n    error: \"Connection timeout\"\n    error_type: \"TimeoutError\"\n    duration: 2.4\n  - attempt: 3\n    success: true\n    duration: 0.8\n</code></pre>"},{"location":"features/stories/#delta-lake-info","title":"Delta Lake Info","text":"<p>For Delta Lake writes, version and operation metrics are captured:</p> <pre><code>delta_info:\n  version: 42\n  operation: MERGE\n  operation_metrics:\n    numTargetRowsInserted: 150\n    numTargetRowsUpdated: 25\n</code></pre>"},{"location":"features/stories/#themes","title":"Themes","text":"<p>Customize HTML story appearance with built-in or custom themes.</p>"},{"location":"features/stories/#built-in-themes","title":"Built-in Themes","text":"Theme Description <code>default</code> Clean, professional blue theme <code>corporate</code> Traditional business styling with serif headings <code>dark</code> Dark mode with high-contrast colors <code>minimal</code> Simple black and white, compact layout"},{"location":"features/stories/#using-themes","title":"Using Themes","text":"<pre><code>story:\n  theme: dark\n</code></pre>"},{"location":"features/stories/#custom-theme-file","title":"Custom Theme File","text":"<p>Create a custom theme YAML file:</p> <pre><code># my_theme.yaml\nname: company_brand\nprimary_color: \"#003366\"\nsuccess_color: \"#2e7d32\"\nerror_color: \"#c62828\"\nwarning_color: \"#ff9900\"\nbg_color: \"#ffffff\"\ntext_color: \"#333333\"\nfont_family: \"Arial, sans-serif\"\nheading_font: \"Georgia, serif\"\nlogo_url: \"https://example.com/logo.png\"\ncompany_name: \"Acme Corp\"\nfooter_text: \"Confidential - Internal Use Only\"\n</code></pre> <p>Reference in config:</p> <pre><code>story:\n  theme: path/to/my_theme.yaml\n</code></pre>"},{"location":"features/stories/#theme-options","title":"Theme Options","text":"Option Type Description <code>name</code> string Theme identifier <code>primary_color</code> hex Main accent color <code>success_color</code> hex Success status color <code>error_color</code> hex Error status color <code>warning_color</code> hex Warning status color <code>bg_color</code> hex Background color <code>text_color</code> hex Primary text color <code>border_color</code> hex Border color <code>code_bg</code> hex Code block background <code>font_family</code> string Body font stack <code>heading_font</code> string Heading font stack <code>code_font</code> string Monospace font stack <code>font_size</code> string Base font size <code>max_width</code> string Container max width <code>logo_url</code> string URL to company logo <code>company_name</code> string Company name for branding <code>footer_text</code> string Custom footer text <code>custom_css</code> string Additional CSS rules"},{"location":"features/stories/#renderers","title":"Renderers","text":"<p>Stories can be rendered in multiple formats.</p>"},{"location":"features/stories/#html-renderer","title":"HTML Renderer","text":"<p>Default format with interactive, responsive design:</p> <pre><code>from odibi.story.renderers import HTMLStoryRenderer, get_renderer\nfrom odibi.story.themes import get_theme\n\n# Using the factory\nrenderer = get_renderer(\"html\")\nhtml = renderer.render(metadata)\n\n# With custom theme\ntheme = get_theme(\"dark\")\nrenderer = HTMLStoryRenderer(theme=theme)\nhtml = renderer.render(metadata)\n</code></pre> <p>Features: - Collapsible node sections - Status indicators with color coding - Summary statistics dashboard - Responsive layout</p>"},{"location":"features/stories/#json-renderer","title":"JSON Renderer","text":"<p>Machine-readable format for API integration:</p> <pre><code>from odibi.story.renderers import JSONStoryRenderer\n\nrenderer = JSONStoryRenderer()\njson_str = renderer.render(metadata)\n</code></pre> <p>Output structure:</p> <pre><code>{\n  \"pipeline_name\": \"process_orders\",\n  \"run_id\": \"20240130_101500\",\n  \"started_at\": \"2024-01-30T10:15:00\",\n  \"completed_at\": \"2024-01-30T10:15:45\",\n  \"duration\": 45.23,\n  \"total_nodes\": 5,\n  \"completed_nodes\": 4,\n  \"failed_nodes\": 1,\n  \"skipped_nodes\": 0,\n  \"success_rate\": 80.0,\n  \"total_rows_processed\": 15000,\n  \"nodes\": [...]\n}\n</code></pre>"},{"location":"features/stories/#markdown-renderer","title":"Markdown Renderer","text":"<p>GitHub-flavored markdown for documentation:</p> <pre><code>from odibi.story.renderers import MarkdownStoryRenderer\n\nrenderer = MarkdownStoryRenderer()\nmd = renderer.render(metadata)\n</code></pre>"},{"location":"features/stories/#renderer-factory","title":"Renderer Factory","text":"<p>Use the factory function to get a renderer by format:</p> <pre><code>from odibi.story.renderers import get_renderer\n\n# Supported formats: \"html\", \"markdown\", \"md\", \"json\"\nrenderer = get_renderer(\"json\")\noutput = renderer.render(metadata)\n</code></pre>"},{"location":"features/stories/#retention","title":"Retention","text":"<p>Stories are automatically cleaned up based on retention policies.</p>"},{"location":"features/stories/#retention-configuration","title":"Retention Configuration","text":"<pre><code>story:\n  retention_days: 30    # Delete stories older than 30 days\n  retention_count: 100  # Keep maximum 100 stories per pipeline\n</code></pre>"},{"location":"features/stories/#how-retention-works","title":"How Retention Works","text":"<ol> <li>Count-based: When story count exceeds <code>retention_count</code>, oldest stories are deleted first</li> <li>Time-based: Stories older than <code>retention_days</code> are deleted</li> <li>Both apply: A story is deleted if it exceeds either limit</li> </ol>"},{"location":"features/stories/#storage-structure","title":"Storage Structure","text":"<p>Stories are organized by pipeline and date:</p> <pre><code>stories/\n\u251c\u2500\u2500 process_orders/\n\u2502   \u251c\u2500\u2500 2024-01-30/\n\u2502   \u2502   \u251c\u2500\u2500 run_10-15-00.html\n\u2502   \u2502   \u251c\u2500\u2500 run_10-15-00.json\n\u2502   \u2502   \u251c\u2500\u2500 run_14-30-00.html\n\u2502   \u2502   \u2514\u2500\u2500 run_14-30-00.json\n\u2502   \u2514\u2500\u2500 2024-01-31/\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 process_customers/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"features/stories/#remote-storage-cleanup","title":"Remote Storage Cleanup","text":"<p>Note: Automatic cleanup for remote storage (ADLS, S3) is not yet implemented. Monitor storage usage manually.</p>"},{"location":"features/stories/#examples","title":"Examples","text":""},{"location":"features/stories/#complete-story-configuration","title":"Complete Story Configuration","text":"<pre><code>project: DataPipeline\nengine: spark\n\nstory:\n  output_path: stories/\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n  theme: corporate\n  include_samples: true\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          path: orders/\n\n      - name: transform_orders\n        transform:\n          operation: sql\n          query: |\n            SELECT order_id, customer_id, amount\n            FROM {read_orders}\n            WHERE amount &gt; 0\n\n      - name: write_orders\n        write:\n          connection: silver\n          path: orders/\n          mode: merge\n</code></pre>"},{"location":"features/stories/#generated-story-output-json","title":"Generated Story Output (JSON)","text":"<pre><code>{\n  \"pipeline_name\": \"process_orders\",\n  \"pipeline_layer\": \"silver\",\n  \"run_id\": \"20240130_101500\",\n  \"started_at\": \"2024-01-30T10:15:00\",\n  \"completed_at\": \"2024-01-30T10:15:45\",\n  \"duration\": 45.23,\n  \"total_nodes\": 3,\n  \"completed_nodes\": 3,\n  \"failed_nodes\": 0,\n  \"skipped_nodes\": 0,\n  \"success_rate\": 100.0,\n  \"total_rows_processed\": 15000,\n  \"project\": \"DataPipeline\",\n  \"nodes\": [\n    {\n      \"node_name\": \"read_orders\",\n      \"operation\": \"read\",\n      \"status\": \"success\",\n      \"duration\": 5.12,\n      \"rows_out\": 15500,\n      \"schema_out\": [\"order_id\", \"customer_id\", \"amount\", \"created_at\"]\n    },\n    {\n      \"node_name\": \"transform_orders\",\n      \"operation\": \"transform\",\n      \"status\": \"success\",\n      \"duration\": 2.34,\n      \"rows_in\": 15500,\n      \"rows_out\": 15000,\n      \"rows_change\": -500,\n      \"rows_change_pct\": -3.2,\n      \"columns_removed\": [\"created_at\"]\n    },\n    {\n      \"node_name\": \"write_orders\",\n      \"operation\": \"write\",\n      \"status\": \"success\",\n      \"duration\": 37.77,\n      \"rows_out\": 15000,\n      \"delta_info\": {\n        \"version\": 42,\n        \"operation\": \"MERGE\",\n        \"operation_metrics\": {\n          \"numTargetRowsInserted\": 500,\n          \"numTargetRowsUpdated\": 14500\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"features/stories/#programmatic-story-generation","title":"Programmatic Story Generation","text":"<pre><code>from odibi.story.generator import StoryGenerator\nfrom odibi.story.metadata import PipelineStoryMetadata\nfrom odibi.story.themes import get_theme\n\n# Create generator\ngenerator = StoryGenerator(\n    pipeline_name=\"process_orders\",\n    max_sample_rows=10,\n    output_path=\"stories/\",\n    retention_days=30,\n    retention_count=100,\n)\n\n# Generate story after pipeline execution\nstory_path = generator.generate(\n    node_results=node_results,\n    completed=[\"read_orders\", \"transform_orders\", \"write_orders\"],\n    failed=[],\n    skipped=[],\n    duration=45.23,\n    start_time=\"2024-01-30T10:15:00\",\n    end_time=\"2024-01-30T10:15:45\",\n)\n\n# Get summary for alerts\nalert_summary = generator.get_alert_summary()\n</code></pre>"},{"location":"features/stories/#documentation-stories","title":"Documentation Stories","text":"<p>Generate stakeholder-ready documentation from pipeline config:</p> <pre><code>from odibi.story.doc_story import DocStoryGenerator\nfrom odibi.config import PipelineConfig\n\n# Load pipeline config\npipeline_config = PipelineConfig.from_yaml(\"pipeline.yaml\")\n\n# Generate documentation\ndoc_generator = DocStoryGenerator(pipeline_config)\ndoc_path = doc_generator.generate(\n    output_path=\"docs/pipeline_doc.html\",\n    format=\"html\",\n    include_flow_diagram=True,\n)\n</code></pre>"},{"location":"features/stories/#related","title":"Related","text":"<ul> <li>Alerting - Stories linked in alert payloads</li> <li>Quality Gates - Gate results captured in stories</li> <li>Schema Tracking - Schema changes in stories</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/transformers/","title":"Transformers","text":"<p>Declarative data transformations with SQL-first semantics, dual-engine support (Spark/Pandas), and extensible custom transforms.</p>"},{"location":"features/transformers/#overview","title":"Overview","text":"<p>Odibi's transformer system provides: - SQL-First Design: All core operations leverage SQL for optimal engine performance - Dual-Engine Support: Seamless execution on Spark or Pandas/DuckDB - Built-in Library: 30+ production-ready transformers - Extensibility: Register custom transforms with the <code>@transform</code> decorator - Chained Operations: Compose multiple transforms in <code>transform.steps</code></p>"},{"location":"features/transformers/#configuration","title":"Configuration","text":""},{"location":"features/transformers/#basic-transformer-usage","title":"Basic Transformer Usage","text":"<pre><code>nodes:\n  - name: clean_orders\n    source: raw_orders\n    transformer: \"filter_rows\"\n    params:\n      condition: \"status = 'active'\"\n</code></pre>"},{"location":"features/transformers/#transformer-config-options","title":"Transformer Config Options","text":"Field Type Required Description <code>transformer</code> string Yes Transformer name (e.g., <code>filter_rows</code>, <code>scd2</code>) <code>params</code> object Yes Transformer-specific parameters"},{"location":"features/transformers/#transform-steps","title":"Transform Steps","text":"<p>Chain multiple transformations in sequence using <code>transform.steps</code>:</p> <pre><code>nodes:\n  - name: process_customers\n    source: raw_customers\n    transform:\n      steps:\n        - transformer: \"clean_text\"\n          params:\n            columns: [\"email\", \"name\"]\n            trim: true\n            case: \"lower\"\n\n        - transformer: \"filter_rows\"\n          params:\n            condition: \"email IS NOT NULL\"\n\n        - transformer: \"derive_columns\"\n          params:\n            derivations:\n              full_name: \"concat(first_name, ' ', last_name)\"\n\n        - transformer: \"deduplicate\"\n          params:\n            keys: [\"customer_id\"]\n            order_by: \"updated_at DESC\"\n</code></pre>"},{"location":"features/transformers/#built-in-transformers","title":"Built-in Transformers","text":""},{"location":"features/transformers/#sql-core-transformers","title":"SQL Core Transformers","text":"<p>Basic SQL operations that work across all engines.</p>"},{"location":"features/transformers/#filter_rows","title":"filter_rows","text":"<p>Filter rows using SQL WHERE conditions.</p> <pre><code>transformer: \"filter_rows\"\nparams:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre>"},{"location":"features/transformers/#derive_columns","title":"derive_columns","text":"<p>Add new columns using SQL expressions.</p> <pre><code>transformer: \"derive_columns\"\nparams:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre>"},{"location":"features/transformers/#cast_columns","title":"cast_columns","text":"<p>Cast columns to different types.</p> <pre><code>transformer: \"cast_columns\"\nparams:\n  casts:\n    age: \"int\"\n    salary: \"double\"\n    created_at: \"timestamp\"\n</code></pre>"},{"location":"features/transformers/#clean_text","title":"clean_text","text":"<p>Apply text cleaning operations (trim, case conversion).</p> <pre><code>transformer: \"clean_text\"\nparams:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"  # Options: lower, upper, preserve\n</code></pre>"},{"location":"features/transformers/#extract_date_parts","title":"extract_date_parts","text":"<p>Extract year, month, day, hour from timestamps.</p> <pre><code>transformer: \"extract_date_parts\"\nparams:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\", \"day\"]\n</code></pre>"},{"location":"features/transformers/#normalize_schema","title":"normalize_schema","text":"<p>Rename, drop, and reorder columns.</p> <pre><code>transformer: \"normalize_schema\"\nparams:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre>"},{"location":"features/transformers/#sort","title":"sort","text":"<p>Sort data by columns.</p> <pre><code>transformer: \"sort\"\nparams:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre>"},{"location":"features/transformers/#limit-sample","title":"limit / sample","text":"<p>Limit or randomly sample rows.</p> <pre><code># Limit\ntransformer: \"limit\"\nparams:\n  n: 100\n  offset: 0\n\n# Sample\ntransformer: \"sample\"\nparams:\n  fraction: 0.1\n  seed: 42\n</code></pre>"},{"location":"features/transformers/#distinct","title":"distinct","text":"<p>Remove duplicate rows.</p> <pre><code>transformer: \"distinct\"\nparams:\n  columns: [\"category\", \"status\"]  # Optional: subset of columns\n</code></pre>"},{"location":"features/transformers/#fill_nulls","title":"fill_nulls","text":"<p>Replace null values with defaults.</p> <pre><code>transformer: \"fill_nulls\"\nparams:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre>"},{"location":"features/transformers/#split_part","title":"split_part","text":"<p>Extract parts of strings by delimiter.</p> <pre><code>transformer: \"split_part\"\nparams:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre>"},{"location":"features/transformers/#date_add-date_trunc-date_diff","title":"date_add / date_trunc / date_diff","text":"<p>Date arithmetic operations.</p> <pre><code># Add interval\ntransformer: \"date_add\"\nparams:\n  col: \"created_at\"\n  value: 7\n  unit: \"day\"\n\n# Truncate to precision\ntransformer: \"date_trunc\"\nparams:\n  col: \"created_at\"\n  unit: \"month\"\n\n# Calculate difference\ntransformer: \"date_diff\"\nparams:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre>"},{"location":"features/transformers/#case_when","title":"case_when","text":"<p>Conditional logic.</p> <pre><code>transformer: \"case_when\"\nparams:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre>"},{"location":"features/transformers/#convert_timezone","title":"convert_timezone","text":"<p>Convert timestamps between timezones.</p> <pre><code>transformer: \"convert_timezone\"\nparams:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre>"},{"location":"features/transformers/#concat_columns","title":"concat_columns","text":"<p>Concatenate multiple columns.</p> <pre><code>transformer: \"concat_columns\"\nparams:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre>"},{"location":"features/transformers/#relational-transformers","title":"Relational Transformers","text":"<p>Operations involving multiple datasets.</p>"},{"location":"features/transformers/#join","title":"join","text":"<p>Join with another dataset.</p> <pre><code>transformer: \"join\"\nparams:\n  right_dataset: \"customers\"  # Must be in depends_on\n  on: [\"customer_id\"]\n  how: \"left\"  # inner, left, right, full, cross\n  prefix: \"cust\"  # Prefix for right columns (avoid collisions)\n</code></pre>"},{"location":"features/transformers/#union","title":"union","text":"<p>Union multiple datasets.</p> <pre><code>transformer: \"union\"\nparams:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true  # Match columns by name\n</code></pre>"},{"location":"features/transformers/#pivot","title":"pivot","text":"<p>Pivot rows into columns.</p> <pre><code>transformer: \"pivot\"\nparams:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n  values: [\"Jan\", \"Feb\", \"Mar\"]  # Optional: explicit pivot values\n</code></pre>"},{"location":"features/transformers/#unpivot","title":"unpivot","text":"<p>Unpivot (melt) columns into rows.</p> <pre><code>transformer: \"unpivot\"\nparams:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre>"},{"location":"features/transformers/#aggregate","title":"aggregate","text":"<p>Group and aggregate data.</p> <pre><code>transformer: \"aggregate\"\nparams:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre>"},{"location":"features/transformers/#advanced-transformers","title":"Advanced Transformers","text":"<p>Complex data processing operations.</p>"},{"location":"features/transformers/#deduplicate","title":"deduplicate","text":"<p>Remove duplicates using window functions.</p> <pre><code>transformer: \"deduplicate\"\nparams:\n  keys: [\"customer_id\"]\n  order_by: \"updated_at DESC\"  # Keep most recent\n</code></pre>"},{"location":"features/transformers/#explode_list_column","title":"explode_list_column","text":"<p>Flatten array/list columns into rows.</p> <pre><code>transformer: \"explode_list_column\"\nparams:\n  column: \"items\"\n  outer: true  # Keep rows with empty lists\n</code></pre>"},{"location":"features/transformers/#dict_based_mapping","title":"dict_based_mapping","text":"<p>Map values using a dictionary.</p> <pre><code>transformer: \"dict_based_mapping\"\nparams:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre>"},{"location":"features/transformers/#regex_replace","title":"regex_replace","text":"<p>Replace patterns using regex.</p> <pre><code>transformer: \"regex_replace\"\nparams:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre>"},{"location":"features/transformers/#unpack_struct","title":"unpack_struct","text":"<p>Flatten struct/dict columns.</p> <pre><code>transformer: \"unpack_struct\"\nparams:\n  column: \"user_info\"\n</code></pre>"},{"location":"features/transformers/#hash_columns","title":"hash_columns","text":"<p>Hash columns for PII anonymization.</p> <pre><code>transformer: \"hash_columns\"\nparams:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"  # or \"md5\"\n</code></pre>"},{"location":"features/transformers/#generate_surrogate_key","title":"generate_surrogate_key","text":"<p>Create deterministic surrogate keys.</p> <pre><code>transformer: \"generate_surrogate_key\"\nparams:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre>"},{"location":"features/transformers/#parse_json","title":"parse_json","text":"<p>Parse JSON strings into structured data.</p> <pre><code>transformer: \"parse_json\"\nparams:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre>"},{"location":"features/transformers/#validate_and_flag","title":"validate_and_flag","text":"<p>Flag rows that fail validation rules.</p> <pre><code>transformer: \"validate_and_flag\"\nparams:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre>"},{"location":"features/transformers/#window_calculation","title":"window_calculation","text":"<p>Apply window functions.</p> <pre><code>transformer: \"window_calculation\"\nparams:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre>"},{"location":"features/transformers/#normalize_json","title":"normalize_json","text":"<p>Flatten nested JSON/struct into columns.</p> <pre><code>transformer: \"normalize_json\"\nparams:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre>"},{"location":"features/transformers/#sessionize","title":"sessionize","text":"<p>Assign session IDs based on inactivity threshold.</p> <pre><code>transformer: \"sessionize\"\nparams:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800  # 30 minutes\n  session_col: \"session_id\"\n</code></pre>"},{"location":"features/transformers/#scd-slowly-changing-dimensions","title":"SCD (Slowly Changing Dimensions)","text":"<p>Track historical changes with SCD Type 2.</p> <pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"  # Registered table name\n  keys: [\"customer_id\"]           # Entity keys\n  track_cols: [\"address\", \"tier\"] # Columns to monitor for changes\n  effective_time_col: \"txn_date\"  # When change occurred\n  end_time_col: \"valid_to\"        # End timestamp column\n  current_flag_col: \"is_current\"  # Current record flag\n</code></pre> <p>Connection-Based Path (ADLS):</p> <pre><code>transformer: \"scd2\"\nparams:\n  connection: adls_prod           # Connection name\n  path: OEE/silver/dim_customers  # Relative path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre> <p>How SCD2 Works: 1. Match: Finds existing records using <code>keys</code> 2. Compare: Checks <code>track_cols</code> to detect changes 3. Close: Updates old record's <code>end_time_col</code> if changed 4. Insert: Adds new record with open-ended validity</p> <p>Note: SCD2 returns a DataFrame. You must add a <code>write:</code> block with <code>mode: overwrite</code>.</p>"},{"location":"features/transformers/#merge-transformer","title":"Merge Transformer","text":"<p>Upsert, append, or delete records in target tables.</p> <pre><code># Upsert (Update + Insert)\ntransformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"upsert\"\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre> <p>Merge Strategies:</p> Strategy Description <code>upsert</code> Update existing, insert new (default) <code>append_only</code> Only insert new keys, ignore duplicates <code>delete_match</code> Delete records matching source keys <p>Advanced Merge Options:</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n  insert_condition: \"source.is_deleted = false\"\n  delete_condition: \"source.is_deleted = true\"\n  optimize_write: true\n  zorder_by: [\"customer_id\"]\n  cluster_by: [\"region\"]\n</code></pre> <p>Connection-Based Path (ADLS):</p> <p>Use <code>connection</code> + <code>path</code> instead of <code>target</code> to leverage connection-based path resolution:</p> <pre><code>transform:\n  steps:\n    - function: merge\n      params:\n        connection: adls_prod           # Connection name\n        path: OEE/silver/customers      # Relative path\n        register_table: silver.customers  # Register in metastore\n        keys: [\"customer_id\"]\n        strategy: \"upsert\"\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre>"},{"location":"features/transformers/#validation-transformers","title":"Validation Transformers","text":"<p>Cross-dataset validation checks.</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"  # or \"schema_match\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre>"},{"location":"features/transformers/#delete-detection","title":"Delete Detection","text":"<p>Detect deleted records for CDC-like behavior.</p> <pre><code>transformer: \"detect_deletes\"\nparams:\n  mode: \"snapshot_diff\"  # Compare Delta versions\n  keys: [\"customer_id\"]\n  soft_delete_col: \"is_deleted\"  # Add flag column\n  max_delete_percent: 10.0  # Safety threshold\n  on_threshold_breach: \"error\"  # error, warn, skip\n</code></pre> <p>Delete Detection Modes:</p> Mode Description <code>none</code> Disabled <code>snapshot_diff</code> Compare current vs previous Delta version <code>sql_compare</code> Compare against live source via JDBC"},{"location":"features/transformers/#creating-custom-transformers","title":"Creating Custom Transformers","text":"<p>Use the <code>@transform</code> decorator with <code>FunctionRegistry</code> to create custom transformers.</p>"},{"location":"features/transformers/#basic-custom-transformer","title":"Basic Custom Transformer","text":"<pre><code>from pydantic import BaseModel, Field\nfrom odibi.context import EngineContext\nfrom odibi.registry import transform\n\n\nclass MyTransformParams(BaseModel):\n    \"\"\"Parameters for my custom transform.\"\"\"\n    column: str = Field(..., description=\"Column to process\")\n    multiplier: float = Field(default=1.0, description=\"Multiplier value\")\n\n\n@transform(\"my_custom_transform\", param_model=MyTransformParams)\ndef my_custom_transform(context: EngineContext, **params) -&gt; EngineContext:\n    \"\"\"My custom transformation.\"\"\"\n    config = MyTransformParams(**params)\n\n    # Use SQL for cross-engine compatibility\n    sql_query = f\"\"\"\n        SELECT *, {config.column} * {config.multiplier} AS {config.column}_scaled\n        FROM df\n    \"\"\"\n    return context.sql(sql_query)\n</code></pre>"},{"location":"features/transformers/#using-custom-transformers-in-yaml","title":"Using Custom Transformers in YAML","text":"<pre><code>nodes:\n  - name: process_data\n    source: raw_data\n    transformer: \"my_custom_transform\"\n    params:\n      column: \"price\"\n      multiplier: 1.1\n</code></pre>"},{"location":"features/transformers/#engine-specific-logic","title":"Engine-Specific Logic","text":"<pre><code>from odibi.enums import EngineType\n\n@transform(\"dual_engine_transform\", param_model=MyParams)\ndef dual_engine_transform(context: EngineContext, **params) -&gt; EngineContext:\n    config = MyParams(**params)\n\n    if context.engine_type == EngineType.SPARK:\n        # Spark-specific implementation\n        import pyspark.sql.functions as F\n        df = context.df.withColumn(\"new_col\", F.lit(\"spark\"))\n        return context.with_df(df)\n\n    elif context.engine_type == EngineType.PANDAS:\n        # Pandas-specific implementation\n        df = context.df.copy()\n        df[\"new_col\"] = \"pandas\"\n        return context.with_df(df)\n</code></pre>"},{"location":"features/transformers/#complete-example","title":"Complete Example","text":"<pre><code>project: ECommerceETL\nengine: spark\n\nconnections:\n  bronze:\n    type: delta\n    path: \"dbfs:/bronze\"\n  silver:\n    type: delta\n    path: \"dbfs:/silver\"\n  gold:\n    type: delta\n    path: \"dbfs:/gold\"\n\npipelines:\n  - pipeline: orders_to_gold\n    nodes:\n      # Clean raw data\n      - name: clean_orders\n        source:\n          connection: bronze\n          path: orders\n        transform:\n          steps:\n            - transformer: \"clean_text\"\n              params:\n                columns: [\"customer_email\"]\n                trim: true\n                case: \"lower\"\n\n            - transformer: \"cast_columns\"\n              params:\n                casts:\n                  order_date: \"timestamp\"\n                  total_amount: \"double\"\n\n            - transformer: \"filter_rows\"\n              params:\n                condition: \"total_amount &gt; 0\"\n\n      # Deduplicate and enrich\n      - name: enriched_orders\n        source: clean_orders\n        depends_on: [clean_orders, customers]\n        transform:\n          steps:\n            - transformer: \"deduplicate\"\n              params:\n                keys: [\"order_id\"]\n                order_by: \"updated_at DESC\"\n\n            - transformer: \"join\"\n              params:\n                right_dataset: \"customers\"\n                on: [\"customer_id\"]\n                how: \"left\"\n\n            - transformer: \"derive_columns\"\n              params:\n                derivations:\n                  order_year: \"YEAR(order_date)\"\n                  order_month: \"MONTH(order_date)\"\n\n      # Final merge to gold\n      - name: gold_orders\n        source: enriched_orders\n        transformer: \"merge\"\n        params:\n          target: \"gold.orders\"\n          keys: [\"order_id\"]\n          strategy: \"upsert\"\n          audit_cols:\n            created_col: \"dw_created_at\"\n            updated_col: \"dw_updated_at\"\n        destination:\n          connection: gold\n          path: orders\n</code></pre>"},{"location":"features/transformers/#best-practices","title":"Best Practices","text":"<ol> <li>Use SQL-first transforms - They push computation to the engine for optimal performance</li> <li>Chain with transform.steps - Compose multiple operations declaratively</li> <li>Prefer built-in transforms - They're tested for dual-engine compatibility</li> <li>Use Pydantic models - Define parameter schemas for custom transforms</li> <li>Handle nulls explicitly - Use <code>fill_nulls</code> or <code>COALESCE</code> in derivations</li> <li>Document custom transforms - Include docstrings and param descriptions</li> </ol>"},{"location":"features/transformers/#related","title":"Related","text":"<ul> <li>Quality Gates - Validate transform outputs</li> <li>Quarantine Tables - Handle failed validations</li> <li>YAML Schema Reference - Complete configuration options</li> </ul>"},{"location":"features/ui/","title":"Web UI","text":"<p>Odibi includes a web dashboard for viewing pipeline status, browsing story reports, and inspecting configuration.</p>"},{"location":"features/ui/#overview","title":"Overview","text":"<p>The UI is a FastAPI application with three main views:</p> Endpoint Description <code>/</code> Pipeline dashboard \u2014 status, last run, node counts <code>/stories</code> Story browser \u2014 HTML reports from past runs <code>/config</code> Config viewer \u2014 current YAML configuration"},{"location":"features/ui/#starting-the-ui","title":"Starting the UI","text":"<pre><code># Start with a specific config file\nodibi ui config.yaml\n\n# Or set via environment variable\nexport ODIBI_CONFIG=config.yaml\nodibi ui\n</code></pre> <p>The server runs on <code>http://localhost:8000</code> by default.</p>"},{"location":"features/ui/#dashboard","title":"Dashboard (<code>/</code>)","text":"<p>The main dashboard shows:</p> <ul> <li>Pipeline name: Each pipeline defined in your config</li> <li>Last run: Timestamp of most recent execution</li> <li>Status: SUCCESS, FAILED, or UNKNOWN</li> <li>Node counts: Total nodes and successful nodes</li> </ul> <p>Status is determined by checking node results: - All nodes succeeded \u2192 SUCCESS - Any node failed \u2192 FAILED - No nodes \u2192 UNKNOWN</p>"},{"location":"features/ui/#stories-browser-stories","title":"Stories Browser (<code>/stories</code>)","text":"<p>Browse HTML story reports generated by pipeline runs.</p> <p>Features: - Lists all runs organized by pipeline \u2192 date \u2192 report - Links to view full HTML reports - Sorted by date (newest first)</p> <p>Story files are served from the configured story path:</p> <pre><code>story:\n  connection: system\n  path: stories/\n</code></pre>"},{"location":"features/ui/#config-viewer-config","title":"Config Viewer (<code>/config</code>)","text":"<p>View the current YAML configuration:</p> <ul> <li>Read-only view of your pipeline config</li> <li>Useful for debugging and verification</li> <li>Shows which config file is loaded</li> </ul>"},{"location":"features/ui/#configuration","title":"Configuration","text":"<p>The UI reads configuration from:</p> <ol> <li><code>ODIBI_CONFIG</code> environment variable</li> <li>Default locations: <code>odibi.yaml</code>, <code>odibi.yml</code>, <code>project.yaml</code></li> </ol> <p>Story paths are resolved from the <code>story</code> and <code>connections</code> config.</p>"},{"location":"features/ui/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the FastAPI app directly:</p> <pre><code>from odibi.ui.app import app\nimport uvicorn\n\n# Run with custom settings\nuvicorn.run(app, host=\"0.0.0.0\", port=8080)\n</code></pre>"},{"location":"features/ui/#related","title":"Related","text":"<ul> <li>Stories \u2014 Pipeline execution reports</li> <li>CLI Guide \u2014 All CLI commands</li> <li>Configuration \u2014 YAML config reference</li> </ul>"},{"location":"features/validation_performance/","title":"Validation Performance Optimization Guide","text":"<p>This document details the performance optimizations made to odibi's contracts and validation system.</p>"},{"location":"features/validation_performance/#summary-of-issues-found-and-fixed","title":"Summary of Issues Found and Fixed","text":""},{"location":"features/validation_performance/#1-critical-bottlenecks-identified","title":"1. Critical Bottlenecks Identified","text":"Issue Engine Impact Fix Applied Double scan in UNIQUE check Pandas 2x slower Single <code>duplicated()</code> call with cached result Full DataFrame copy for invalid rows Pandas O(N) memory waste Mask-based operations only Eager LazyFrame collection Polars Defeats optimizer Lazy aggregations, scalar-only collects Missing contract types Polars Inconsistent behavior Added UNIQUE, ACCEPTED_VALUES, RANGE, REGEX, ROW_COUNT Per-row test_results lists Quarantine O(N\u00d7tests) memory Aggregate counts only No fail-fast mode All Full scan even on early failure Added <code>fail_fast</code> config option No FRESHNESS minutes support Spark/Pandas Missing 'm' unit Added minutes parsing"},{"location":"features/validation_performance/#2-performance-anti-patterns-fixed","title":"2. Performance Anti-Patterns Fixed","text":""},{"location":"features/validation_performance/#pandas-engine","title":"Pandas Engine","text":"<p>Before (UNIQUE check):</p> <pre><code># TWO full scans + two boolean Series allocations\nif df.duplicated(subset=cols).any():\n    dup_count = df.duplicated(subset=cols).sum()\n</code></pre> <p>After:</p> <pre><code># ONE scan, cached result\ndups = df.duplicated(subset=cols)\ndup_count = int(dups.sum())\nif dup_count &gt; 0:\n    ...\n</code></pre> <p>Before (ACCEPTED_VALUES):</p> <pre><code># Creates full invalid DataFrame in memory\ninvalid = df[~df[col].isin(test.values)]\nif not invalid.empty:\n    examples = invalid[col].unique()[:3]\n</code></pre> <p>After:</p> <pre><code># Mask-only, minimal memory\nmask = ~df[col].isin(test.values)\ninvalid_count = int(mask.sum())\nif invalid_count &gt; 0:\n    examples = df.loc[mask, col].dropna().unique()[:3]\n</code></pre>"},{"location":"features/validation_performance/#polars-engine","title":"Polars Engine","text":"<p>Before:</p> <pre><code># Forces full collection, defeats lazy optimization\nif isinstance(df, pl.LazyFrame):\n    df = df.collect()  # Materializes everything!\n</code></pre> <p>After:</p> <pre><code># Keeps lazy, collects only scalars\nif is_lazy:\n    row_count = df.select(pl.len()).collect().item()\n    null_count = df.select(pl.col(col).is_null().sum()).collect().item()\n</code></pre>"},{"location":"features/validation_performance/#quarantine-system","title":"Quarantine System","text":"<p>Before:</p> <pre><code># O(N \u00d7 num_tests) memory usage\nfor name, mask in test_masks.items():\n    test_results[name] = mask.tolist()  # Huge Python list per test!\n</code></pre> <p>After:</p> <pre><code># O(num_tests) memory - aggregate counts only\nfor name, mask in test_masks.items():\n    pass_count = int(mask.sum())\n    fail_count = len(df) - pass_count\n    test_results[name] = {\"pass_count\": pass_count, \"fail_count\": fail_count}\n</code></pre>"},{"location":"features/validation_performance/#new-configuration-options","title":"New Configuration Options","text":""},{"location":"features/validation_performance/#fail-fast-mode","title":"Fail-Fast Mode","text":"<p>Stop validation on first failure for faster feedback:</p> <pre><code>validation:\n  fail_fast: true  # Stop on first failure\n  tests:\n    - type: not_null\n      columns: [id, customer_id]\n    - type: unique\n      columns: [id]\n</code></pre>"},{"location":"features/validation_performance/#dataframe-caching-spark","title":"DataFrame Caching (Spark)","text":"<p>Cache DataFrame before validation when running many tests:</p> <pre><code>validation:\n  cache_df: true  # Cache for multi-test validation\n  tests:\n    - type: not_null\n      columns: [id]\n    # ... 10+ more tests\n</code></pre>"},{"location":"features/validation_performance/#quarantine-sampling","title":"Quarantine Sampling","text":"<p>Limit quarantined rows to prevent storage blowup:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    max_rows: 10000         # Cap at 10K rows\n    sample_fraction: 0.1    # Or sample 10% of invalid rows\n</code></pre>"},{"location":"features/validation_performance/#engine-parity","title":"Engine Parity","text":"<p>All three engines now support the same contract types:</p> Contract Type Pandas Polars Spark not_null \u2705 \u2705 \u2705 unique \u2705 \u2705 \u2705 accepted_values \u2705 \u2705 \u2705 row_count \u2705 \u2705 \u2705 range \u2705 \u2705 \u2705 regex_match \u2705 \u2705 \u2705 freshness \u2705 \u2705 \u2705 schema \u2705 \u2705 \u2705 custom_sql \u2705 \u26a0\ufe0f Skipped \u2705"},{"location":"features/validation_performance/#freshness-duration-units","title":"FRESHNESS Duration Units","text":"<p>All engines now support: - <code>\"24h\"</code> - hours - <code>\"7d\"</code> - days - <code>\"30m\"</code> - minutes (NEW)</p>"},{"location":"features/validation_performance/#benchmark-tests","title":"Benchmark Tests","text":"<p>Run benchmarks to verify performance:</p> <pre><code>pytest tests/benchmarks/test_validation_perf.py -v -s\n</code></pre> <p>Benchmark scenarios: - 10 contracts on 100K rows (Pandas) - 15 contracts on 100K rows (Pandas) - Fail-fast vs full validation comparison - Polars eager vs lazy comparison - Quarantine split performance - Memory efficiency verification</p>"},{"location":"features/validation_performance/#expected-performance-gains","title":"Expected Performance Gains","text":"Scenario Before After Improvement 10 contracts / 100K rows (Pandas) ~5s ~3s 40% faster Fail-fast on early failure Full scan Early exit Up to 10x faster Polars LazyFrame (10 contracts) Eager collect Lazy 30-50% faster Quarantine memory (100K rows, 5 tests) ~4MB lists ~1KB dicts 4000x less memory"},{"location":"features/validation_performance/#best-practices","title":"Best Practices","text":"<ol> <li>Use fail-fast for CI/CD: Quick feedback when data is clearly bad</li> <li>Enable cache_df for Spark: When running 5+ contracts per node</li> <li>Set quarantine limits: Prevent storage blowup on high-failure batches</li> <li>Prefer Polars LazyFrame: Let the query optimizer work for you</li> <li>Batch similar tests: Multiple NOT_NULL columns in one test</li> </ol>"},{"location":"guides/MIGRATION_GUIDE/","title":"Odibi V3 Migration Guide","text":""},{"location":"guides/MIGRATION_GUIDE/#1-privacy-inheritance-safety-upgrade","title":"1. Privacy Inheritance (Safety Upgrade)","text":"<p>Change: PII status now inherits from upstream nodes. If a column is marked as <code>pii: true</code> in a source node, it will remain PII in all downstream nodes unless explicitly declassified.</p> <p>Impact: *   Existing Pipelines: Pipelines that relied on implicit declassification (i.e., assuming PII status is lost after one node) may now trigger anonymization in downstream nodes if privacy is configured. *   New Behavior: Safer by default. You cannot accidentally expose PII by forgetting to re-tag it.</p> <p>Action Required: If you have columns that are no longer PII (e.g., you hashed them or dropped the sensitive part), you must now explicitly declassify them in the node configuration if you want to stop tracking them as PII.</p> <pre><code>- name: downstream_node\n  privacy:\n    method: \"hash\"\n    declassify:\n      - \"hashed_email\"  # Stop tracking this as PII\n</code></pre>"},{"location":"guides/MIGRATION_GUIDE/#2-spark-write-modes","title":"2. Spark Write Modes","text":"<p>Change: The Spark engine now supports <code>upsert</code> and <code>append_once</code> modes, bringing it to parity with the Pandas engine.</p> <p>Usage: These modes require <code>keys</code> to be defined in the <code>write.options</code> (or <code>params</code> if using a transformer that passes them). They are supported only for Delta Lake format.</p> <pre><code>- name: merge_users\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"users\"\n    mode: \"upsert\"\n    options:\n      keys: [\"user_id\"]\n</code></pre>"},{"location":"guides/MIGRATION_GUIDE/#3-context-api","title":"3. Context API","text":"<p>Change: The <code>NodeExecutionContext</code> (available in custom transformers as <code>ctx</code>) has been updated. *   Added <code>ctx.schema</code>: Returns a dictionary of column types. *   Added <code>ctx.pii_metadata</code>: Dictionary of active PII columns.</p>"},{"location":"guides/avoiding_the_builder_trap/","title":"Avoiding the Builder Trap","text":"<p>A Guide for Odibi Maintainers and Contributors</p>"},{"location":"guides/avoiding_the_builder_trap/#what-is-the-builder-trap","title":"What is the Builder Trap?","text":"<p>The Builder Trap is the tendency to continuously add features, refactor code, and \"improve\" a framework without ever using it on real problems. It feels productive, but it creates:</p> <ul> <li>Blind spots: Features that seem useful but aren't</li> <li>Complexity: Code that solves imaginary problems</li> <li>Burnout: Endless work with no validation</li> <li>Drift: The framework diverges from real user needs</li> </ul> <p>The antidote: Use the framework more than you build it.</p>"},{"location":"guides/avoiding_the_builder_trap/#the-golden-ratio","title":"The Golden Ratio","text":"<p>Aim for this balance:</p> Activity Time Allocation Using Odibi (real pipelines, real data) 60% Fixing/Improving (based on usage pain) 30% New Features (planned, prioritized) 10% <p>If you're spending more than 30% of your time building new features, you're probably in the trap.</p>"},{"location":"guides/avoiding_the_builder_trap/#issue-driven-development","title":"Issue-Driven Development","text":""},{"location":"guides/avoiding_the_builder_trap/#the-rule","title":"The Rule","text":"<p>If it's not critical, create an issue instead of fixing it immediately.</p>"},{"location":"guides/avoiding_the_builder_trap/#the-decision-framework","title":"The Decision Framework","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Is this blocking you?           \u2502\n\u2502         (Tests fail, can't run)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502               \u2502\n        YES              NO\n         \u2502               \u2502\n         \u25bc               \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Fix Now \u2502    \u2502 Is it &lt; 5 min   \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 AND obvious?    \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502               \u2502\n                  YES              NO\n                   \u2502               \u2502\n                   \u25bc               \u25bc\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502 Fix Now \u2502    \u2502 CREATE      \u2502\n             \u2502 No Issue\u2502    \u2502 AN ISSUE    \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/avoiding_the_builder_trap/#why-this-works","title":"Why This Works","text":"<ol> <li>Prevents scope creep - \"Just one quick fix\" becomes 3 hours of yak shaving</li> <li>Forces prioritization - Is this actually important, or just visible right now?</li> <li>Creates documentation - Future you can see what was decided and why</li> <li>Enables batching - Related issues can be fixed together efficiently</li> <li>Protects focus - You stay on your current task</li> </ol>"},{"location":"guides/avoiding_the_builder_trap/#issue-hygiene","title":"Issue Hygiene","text":"<p>When creating issues, include:</p> <pre><code>## Problem\nWhat's broken or missing?\n\n## Impact\nWho is affected? How badly?\n\n## Proposed Solution (optional)\nQuick sketch of the fix\n\n## Effort Estimate\n- Trivial (&lt; 30 min)\n- Small (1-2 hours)\n- Medium (half day)\n- Large (1+ days)\n</code></pre> <p>Use labels consistently: - <code>bug</code> - Something is broken - <code>enhancement</code> - New feature or improvement - <code>documentation</code> - Docs updates - <code>good-first-issue</code> - Easy wins for new contributors - <code>priority:high</code> - Needs attention soon - <code>priority:low</code> - Nice to have</p>"},{"location":"guides/avoiding_the_builder_trap/#dogfooding-practices","title":"Dogfooding Practices","text":""},{"location":"guides/avoiding_the_builder_trap/#1-build-real-pipelines","title":"1. Build Real Pipelines","text":"<p>Don't just test with toy data. Use Odibi for actual work:</p> <ul> <li>Personal projects: Analyze your finances, fitness data, reading list</li> <li>Work tasks: If appropriate, use Odibi for real ETL jobs</li> <li>Side projects: Build something you actually need</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#2-experience-the-onboarding","title":"2. Experience the Onboarding","text":"<p>Periodically, pretend you're a new user:</p> <pre><code># Start fresh\nrm -rf .venv\npython -m venv .venv\npip install odibi\n\n# Follow your own Getting Started guide\n# Note every point of friction\n</code></pre>"},{"location":"guides/avoiding_the_builder_trap/#3-run-the-full-workflow","title":"3. Run the Full Workflow","text":"<p>Regularly exercise the complete user journey:</p> <pre><code>odibi init-pipeline mytest\nodibi validate odibi.yaml\nodibi run odibi.yaml --dry-run\nodibi run odibi.yaml\nodibi doctor odibi.yaml\nodibi story list\n</code></pre> <p>Ask yourself: - Was anything confusing? - Did error messages help or frustrate? - What would I Google if I got stuck?</p>"},{"location":"guides/avoiding_the_builder_trap/#4-break-it-on-purpose","title":"4. Break It On Purpose","text":"<p>Try to make Odibi fail:</p> <ul> <li>Missing connections</li> <li>Circular dependencies</li> <li>Invalid YAML</li> <li>Wrong column names in SQL</li> <li>Network failures (disconnect wifi mid-run)</li> </ul> <p>Good frameworks fail gracefully. Bad ones fail mysteriously.</p>"},{"location":"guides/avoiding_the_builder_trap/#signs-youre-in-the-builder-trap","title":"Signs You're in the Builder Trap","text":"<p>Watch for these warning signs:</p> Sign Reality Check \"I need to refactor X before I can use it\" No, you don't. Use it messy. \"Just one more feature, then it's ready\" It's ready now. Ship it. \"Nobody can use this until I fix Y\" Let them try. Their feedback &gt; your assumptions. \"I'll write docs after I finish building\" Docs ARE building. Write them now. \"This code isn't clean enough\" Clean code that isn't used is worthless."},{"location":"guides/avoiding_the_builder_trap/#the-cure","title":"The Cure","text":"<p>When you catch yourself building instead of using:</p> <ol> <li>Stop immediately</li> <li>Create an issue for what you were about to do</li> <li>Open <code>odibi.yaml</code> and run a real pipeline</li> <li>Note what actually bothers you during usage</li> <li>Those are your real priorities</li> </ol>"},{"location":"guides/avoiding_the_builder_trap/#weekly-review-ritual","title":"Weekly Review Ritual","text":"<p>Every week, spend 30 minutes on this:</p>"},{"location":"guides/avoiding_the_builder_trap/#1-triage-issues-10-min","title":"1. Triage Issues (10 min)","text":"<ul> <li>Review new issues</li> <li>Close duplicates or \"won't fix\"</li> <li>Prioritize the backlog</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#2-usage-reflection-10-min","title":"2. Usage Reflection (10 min)","text":"<ul> <li>What pipelines did I run this week?</li> <li>What frustrated me?</li> <li>What worked well?</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#3-plan-next-week-10-min","title":"3. Plan Next Week (10 min)","text":"<ul> <li>Pick 1-2 issues to address</li> <li>Schedule time for USING, not just building</li> <li>Resist the urge to add \"just one more thing\"</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#the-north-star-question","title":"The North Star Question","text":"<p>Before any work session, ask:</p> <p>\"Am I building this because a real user (including myself) hit this problem, or because I think someone might need it someday?\"</p> <p>If the answer is \"someday\" \u2192 Create an issue and move on.</p> <p>If the answer is \"I hit this yesterday\" \u2192 Fix it.</p>"},{"location":"guides/avoiding_the_builder_trap/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>The Mom Test - How to validate ideas through usage</li> <li>Shape Up - Basecamp's approach to shipping</li> <li>Just Fucking Ship - Amy Hoy on finishing things</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#summary","title":"Summary","text":"Do This Not This Use Odibi on real data Build features in isolation Create issues for future work Fix everything immediately Experience your own onboarding Assume the UX is fine Ship small, validate, iterate Wait until it's \"perfect\" Ask \"did I hit this problem?\" Ask \"might someone need this?\" <p>The framework is ready. Go use it.</p>"},{"location":"guides/best_practices/","title":"Odibi Best Practices Guide","text":"<p>Version: 2.4.0 Last Updated: 2025-12-03 Audience: Data Engineers, Analytics Engineers, Team Leads</p> <p>This guide covers recommended patterns for building maintainable, scalable, and production-ready Odibi pipelines.</p>"},{"location":"guides/best_practices/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Project Organization</li> <li>Pipeline Design</li> <li>Node Design</li> <li>Naming Conventions</li> <li>Configuration Management</li> <li>Performance</li> <li>Data Quality</li> <li>Cross-Pipeline Dependencies</li> <li>Security</li> <li>Version Control</li> </ol>"},{"location":"guides/best_practices/#1-project-organization","title":"1. Project Organization","text":""},{"location":"guides/best_practices/#recommended-folder-structure","title":"Recommended Folder Structure","text":"<pre><code>my-odibi-project/\n\u251c\u2500\u2500 project.yaml                    # Core config (connections, settings)\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 bronze/\n\u2502   \u2502   \u2514\u2500\u2500 read_bronze.yaml        # Bronze layer pipeline\n\u2502   \u251c\u2500\u2500 silver/\n\u2502   \u2502   \u2514\u2500\u2500 transform_silver.yaml   # Silver layer pipeline\n\u2502   \u2514\u2500\u2500 gold/\n\u2502       \u2514\u2500\u2500 build_gold.yaml         # Gold layer pipeline\n\u251c\u2500\u2500 transformations/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 custom_transforms.py        # Custom Python transformations\n\u251c\u2500\u2500 sql/\n\u2502   \u2514\u2500\u2500 complex_queries.sql         # Complex SQL (optional)\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_pipelines.py           # Pipeline tests\n\u251c\u2500\u2500 .env                            # Local secrets (git-ignored)\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"guides/best_practices/#separation-of-concerns","title":"Separation of Concerns","text":"File Contains Does NOT Contain <code>project.yaml</code> Connections, system config, story config, imports Pipeline definitions <code>pipelines/*.yaml</code> Pipeline and node definitions Connection details <code>transformations/</code> Custom Python logic YAML configuration"},{"location":"guides/best_practices/#example-projectyaml","title":"Example <code>project.yaml</code>","text":"<pre><code>project: OEE\ndescription: \"OEE Analytics Platform\"\nengine: spark\nversion: \"1.0.0\"\nowner: \"Data Team\"\n\n# === Connections (defined once, used everywhere) ===\nconnections:\n  source_db:\n    type: sql_server\n    host: ${DB_HOST}\n    database: ${DB_NAME}\n    auth:\n      mode: sql_login\n      username: ${DB_USER}\n      password: ${DB_PASS}\n\n  lakehouse:\n    type: azure_blob\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    auth:\n      mode: account_key\n      account_key: ${STORAGE_KEY}\n\n# === System Catalog ===\nsystem:\n  connection: lakehouse\n  path: _odibi_system\n\n# === Story Configuration ===\nstory:\n  connection: lakehouse\n  path: stories/\n  retention_days: 30\n  auto_generate: true\n\n# === Global Settings ===\nperformance:\n  use_arrow: true\n  skip_null_profiling: true\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\nlogging:\n  level: INFO\n  structured: true\n\n# === Import Pipelines ===\nimports:\n  - pipelines/bronze/read_bronze.yaml\n  - pipelines/silver/transform_silver.yaml\n  - pipelines/gold/build_gold.yaml\n</code></pre>"},{"location":"guides/best_practices/#example-pipeline-file","title":"Example Pipeline File","text":"<p>pipelines/bronze/read_bronze.yaml:</p> <pre><code>pipelines:\n  - pipeline: read_bronze\n    description: \"Ingest raw data from source systems\"\n    layer: bronze\n    nodes:\n      - name: orders\n        description: \"Raw orders from ERP\"\n        read:\n          connection: source_db\n          format: sql\n          table: sales.orders\n          incremental:\n            mode: stateful\n            column: updated_at\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/orders\"\n          mode: append\n          add_metadata: true\n\n      - name: customers\n        description: \"Customer master data\"\n        read:\n          connection: source_db\n          format: sql\n          table: sales.customers\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/customers\"\n          mode: append\n          add_metadata: true\n          skip_if_unchanged: true\n          skip_hash_columns: [customer_id]\n</code></pre>"},{"location":"guides/best_practices/#2-pipeline-design","title":"2. Pipeline Design","text":""},{"location":"guides/best_practices/#one-pipeline-per-layer-per-domain","title":"One Pipeline Per Layer Per Domain","text":"<p>\u2705 Good:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 bronze/\n\u2502   \u2514\u2500\u2500 read_bronze.yaml           # All bronze ingestion\n\u251c\u2500\u2500 silver/\n\u2502   \u2514\u2500\u2500 transform_silver.yaml      # All silver transformations\n\u2514\u2500\u2500 gold/\n    \u251c\u2500\u2500 gold_sales.yaml            # Sales domain aggregates\n    \u2514\u2500\u2500 gold_inventory.yaml        # Inventory domain aggregates\n</code></pre> <p>\u274c Avoid:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 orders_bronze_silver_gold.yaml  # Too many concerns in one file\n\u2514\u2500\u2500 everything.yaml                 # Unmaintainable\n</code></pre>"},{"location":"guides/best_practices/#pipeline-sizing-guidelines","title":"Pipeline Sizing Guidelines","text":"Node Count Recommendation 1-20 nodes Single pipeline file 20-50 nodes Consider splitting by sub-domain 50+ nodes Split into multiple pipelines"},{"location":"guides/best_practices/#keep-nodes-with-their-pipeline","title":"Keep Nodes with Their Pipeline","text":"<p>\u274c Don't split nodes into separate files:</p> <pre><code># nodes/orders.yaml - BAD: nodes scattered across files\npipelines:\n  - pipeline: read_bronze\n    nodes:\n      - name: orders\n</code></pre> <p>\u2705 Keep nodes together:</p> <pre><code># read_bronze.yaml - GOOD: all nodes in one place\npipelines:\n  - pipeline: read_bronze\n    nodes:\n      - name: orders\n        # ...\n      - name: customers\n        # ...\n      - name: products\n        # ...\n</code></pre> <p>Why? - <code>depends_on</code> relationships are visible in one file - Easier to understand the full pipeline flow - One file = one pipeline = one commit for changes</p>"},{"location":"guides/best_practices/#3-node-design","title":"3. Node Design","text":""},{"location":"guides/best_practices/#single-responsibility","title":"Single Responsibility","text":"<p>Each node should do one thing well:</p> <p>\u2705 Good:</p> <pre><code>- name: load_orders\n  read: ...\n  write: ...\n\n- name: clean_orders\n  depends_on: [load_orders]\n  transform:\n    steps:\n      - sql: \"SELECT * FROM load_orders WHERE status IS NOT NULL\"\n  write: ...\n\n- name: enrich_orders\n  depends_on: [clean_orders, customers]\n  transform:\n    steps:\n      - operation: join\n        left: clean_orders\n        right: customers\n        on: [customer_id]\n  write: ...\n</code></pre> <p>\u274c Avoid:</p> <pre><code>- name: do_everything\n  read: ...\n  transform:\n    steps:\n      - sql: \"...\"  # 500 lines of SQL doing everything\n  write: ...\n</code></pre>"},{"location":"guides/best_practices/#use-descriptions","title":"Use Descriptions","text":"<p>Always add descriptions for documentation and debugging:</p> <pre><code>- name: calculate_daily_revenue\n  description: \"Aggregates order amounts by day for finance reporting\"\n  tags: [daily, finance, critical]\n</code></pre>"},{"location":"guides/best_practices/#cache-strategically","title":"Cache Strategically","text":"<p>Use <code>cache: true</code> for nodes that are: - Read by multiple downstream nodes - Expensive to compute - Small enough to fit in memory</p> <pre><code>- name: dimension_products\n  description: \"Product dimension - cached for multiple joins\"\n  read: ...\n  cache: true  # Multiple nodes will join to this\n</code></pre>"},{"location":"guides/best_practices/#4-naming-conventions","title":"4. Naming Conventions","text":""},{"location":"guides/best_practices/#pipeline-names","title":"Pipeline Names","text":"<p>Use <code>snake_case</code> with layer prefix:</p> Pattern Example <code>{action}_{layer}</code> <code>read_bronze</code>, <code>transform_silver</code>, <code>build_gold</code> <code>{layer}_{domain}</code> <code>bronze_sales</code>, <code>silver_inventory</code>"},{"location":"guides/best_practices/#node-names","title":"Node Names","text":"<p>Use descriptive <code>snake_case</code>:</p> Pattern Example Source nodes <code>orders</code>, <code>customers</code>, <code>products</code> Transformed nodes <code>clean_orders</code>, <code>enriched_customers</code> Aggregated nodes <code>daily_sales</code>, <code>monthly_revenue</code> Dimension nodes <code>dim_product</code>, <code>dim_customer</code> Fact nodes <code>fact_orders</code>, <code>fact_inventory</code>"},{"location":"guides/best_practices/#connection-names","title":"Connection Names","text":"<p>Use environment + purpose:</p> <pre><code>connections:\n  prod_source_db:    # Production source database\n  prod_lakehouse:    # Production data lake\n  dev_lakehouse:     # Development data lake\n</code></pre>"},{"location":"guides/best_practices/#5-configuration-management","title":"5. Configuration Management","text":""},{"location":"guides/best_practices/#environment-variables-for-secrets","title":"Environment Variables for Secrets","text":"<p>\u2705 Always use environment variables for sensitive data:</p> <pre><code>connections:\n  database:\n    host: ${DB_HOST}\n    username: ${DB_USER}\n    password: ${DB_PASSWORD}\n</code></pre> <p>\u274c Never hardcode secrets:</p> <pre><code>connections:\n  database:\n    password: \"my_secret_password\"  # NEVER DO THIS\n</code></pre>"},{"location":"guides/best_practices/#use-env-for-local-development","title":"Use <code>.env</code> for Local Development","text":"<pre><code># .env (git-ignored)\nDB_HOST=localhost\nDB_USER=dev_user\nDB_PASSWORD=dev_password\nSTORAGE_ACCOUNT=devaccount\nSTORAGE_KEY=abc123...\n</code></pre>"},{"location":"guides/best_practices/#environment-specific-overrides","title":"Environment-Specific Overrides","text":"<pre><code># In project.yaml\nenvironments:\n  dev:\n    connections:\n      lakehouse:\n        container: dev-datalake\n  prod:\n    logging:\n      level: WARNING\n    connections:\n      lakehouse:\n        container: prod-datalake\n</code></pre> <p>Run with: <code>odibi run project.yaml --env prod</code></p>"},{"location":"guides/best_practices/#6-performance","title":"6. Performance","text":""},{"location":"guides/best_practices/#enable-arrow-for-pandas","title":"Enable Arrow for Pandas","text":"<pre><code>performance:\n  use_arrow: true  # Major speedup for Parquet I/O\n</code></pre>"},{"location":"guides/best_practices/#use-incremental-loading","title":"Use Incremental Loading","text":"<p>Don't reload full tables every time:</p> <pre><code>read:\n  connection: source_db\n  table: orders\n  incremental:\n    mode: stateful\n    column: updated_at\n    watermark_lag: \"1d\"\n</code></pre>"},{"location":"guides/best_practices/#skip-unchanged-data","title":"Skip Unchanged Data","text":"<p>For dimension tables that rarely change:</p> <pre><code>write:\n  mode: append\n  skip_if_unchanged: true\n  skip_hash_columns: [id]\n</code></pre>"},{"location":"guides/best_practices/#optimize-delta-writes-spark","title":"Optimize Delta Writes (Spark)","text":"<pre><code>write:\n  format: delta\n  options:\n    optimize_write: true\n    cluster_by: [date, region]\n</code></pre>"},{"location":"guides/best_practices/#skip-null-profiling-for-large-tables","title":"Skip Null Profiling for Large Tables","text":"<pre><code>performance:\n  skip_null_profiling: true  # Faster for very large DataFrames\n</code></pre>"},{"location":"guides/best_practices/#7-data-quality-validation","title":"7. Data Quality &amp; Validation","text":""},{"location":"guides/best_practices/#validation-strategy-overview","title":"Validation Strategy Overview","text":"<p>Odibi provides three validation mechanisms for different use cases:</p> Mechanism When Executed Purpose On Failure Contracts Before processing Input validation Always stops pipeline Validation After transformation Output checks Configurable (warn/error) Gates Before write Critical path checks Blocks downstream nodes"},{"location":"guides/best_practices/#use-contracts-for-input-validation","title":"Use Contracts for Input Validation","text":"<p>Fail fast if source data is bad:</p> <pre><code>- name: process_orders\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read: ...\n  transform: ...\n</code></pre>"},{"location":"guides/best_practices/#use-validation-for-output-checks","title":"Use Validation for Output Checks","text":"<p>Warn (or fail) if output doesn't meet expectations:</p> <pre><code>- name: daily_revenue\n  transform: ...\n  validation:\n    tests:\n      - type: not_null\n        columns: [date, revenue]\n      - type: unique\n        columns: [date]\n      - type: range\n        column: revenue\n        min: 0\n    on_failure: warn  # or \"error\" to fail the pipeline\n</code></pre>"},{"location":"guides/best_practices/#available-validation-types","title":"Available Validation Types","text":"Type Description Example <code>not_null</code> Check for null values <code>columns: [id, name]</code> <code>unique</code> Check for duplicates <code>columns: [id]</code> <code>row_count</code> Validate row counts <code>min: 100, max: 1000000</code> <code>freshness</code> Check data recency <code>column: updated_at, max_age: \"24h\"</code> <code>range</code> Numeric bounds <code>column: amount, min: 0, max: 10000</code> <code>regex</code> Pattern matching <code>column: email, pattern: \"^.+@.+$\"</code> <code>referential</code> FK validation <code>column: customer_id, reference: dim_customer.id</code> <code>custom</code> Custom Python function <code>function: my_validation_func</code>"},{"location":"guides/best_practices/#use-quality-gates-for-critical-paths","title":"Use Quality Gates for Critical Paths","text":"<pre><code>- name: load_orders\n  gate:\n    - type: row_count\n      min: 1000\n      on_failure: block  # Stops pipeline if &lt; 1000 rows\n</code></pre>"},{"location":"guides/best_practices/#fk-validation-for-fact-tables","title":"FK Validation for Fact Tables","text":"<p>Ensure referential integrity before loading fact tables:</p> <pre><code>- name: fact_orders\n  depends_on: [dim_customer, dim_product]\n  read:\n    connection: staging\n    path: orders\n  validation:\n    tests:\n      - type: referential\n        column: customer_id\n        reference: dim_customer.customer_id\n        on_orphan: warn\n      - type: referential\n        column: product_id\n        reference: dim_product.product_id\n        on_orphan: filter  # Remove orphan rows\n  write:\n    connection: warehouse\n    path: fact_orders\n</code></pre>"},{"location":"guides/best_practices/#custom-validation-functions","title":"Custom Validation Functions","text":"<p>Register custom validation logic:</p> <pre><code>from odibi import transform\n\n@transform(\"validate_business_rules\")\ndef validate_business_rules(context, current):\n    \"\"\"Custom business rule validation.\"\"\"\n    errors = []\n\n    # Rule 1: Order amount must match line items\n    mismatched = current[current['total'] != current['line_items_sum']]\n    if len(mismatched) &gt; 0:\n        errors.append(f\"{len(mismatched)} orders with mismatched totals\")\n\n    # Rule 2: Future dates not allowed\n    future_orders = current[current['order_date'] &gt; pd.Timestamp.now()]\n    if len(future_orders) &gt; 0:\n        errors.append(f\"{len(future_orders)} orders with future dates\")\n\n    if errors:\n        context.log_warning(f\"Validation issues: {'; '.join(errors)}\")\n\n    return current\n</code></pre> <p>Use in YAML:</p> <pre><code>transform:\n  steps:\n    - function: validate_business_rules\n</code></pre>"},{"location":"guides/best_practices/#quarantine-bad-records","title":"Quarantine Bad Records","text":"<p>Separate bad data for review instead of failing:</p> <pre><code>- name: process_orders\n  validation:\n    tests:\n      - type: not_null\n        columns: [order_id, amount]\n    on_failure: quarantine\n    quarantine:\n      connection: warehouse\n      path: quarantine/orders\n      include_reason: true  # Adds _quarantine_reason column\n</code></pre>"},{"location":"guides/best_practices/#8-cross-pipeline-dependencies","title":"8. Cross-Pipeline Dependencies","text":""},{"location":"guides/best_practices/#use-pipelinenode-references","title":"Use <code>$pipeline.node</code> References","text":"<p>When silver needs bronze outputs:</p> <pre><code># pipelines/silver/transform_silver.yaml\npipelines:\n  - pipeline: transform_silver\n    nodes:\n      - name: enriched_orders\n        inputs:\n          orders: $read_bronze.orders           # Cross-pipeline reference\n          customers: $read_bronze.customers\n        transform:\n          steps:\n            - operation: join\n              left: orders\n              right: customers\n              on: [customer_id]\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"silver/enriched_orders\"\n</code></pre>"},{"location":"guides/best_practices/#run-pipelines-in-order","title":"Run Pipelines in Order","text":"<pre><code># Bronze first\nodibi run project.yaml --pipeline read_bronze\n\n# Then silver (references bronze outputs)\nodibi run project.yaml --pipeline transform_silver\n</code></pre>"},{"location":"guides/best_practices/#best-practices-for-references","title":"Best Practices for References","text":"<ol> <li>Always use <code>path:</code> in write config \u2014 ensures cross-engine compatibility</li> <li>Run source pipeline first \u2014 references require catalog entries</li> <li>Use meaningful node names \u2014 <code>$read_bronze.orders</code> is clearer than <code>$p1.n1</code></li> </ol>"},{"location":"guides/best_practices/#9-security","title":"9. Security","text":""},{"location":"guides/best_practices/#mask-sensitive-columns-in-stories","title":"Mask Sensitive Columns in Stories","text":"<pre><code>- name: process_users\n  sensitive: [email, ssn, phone]  # Masked in Data Stories\n</code></pre>"},{"location":"guides/best_practices/#full-node-masking-for-pii-heavy-nodes","title":"Full Node Masking for PII-Heavy Nodes","text":"<pre><code>- name: medical_records\n  sensitive: true  # Entire sample redacted\n</code></pre>"},{"location":"guides/best_practices/#use-key-vault-in-production","title":"Use Key Vault in Production","text":"<pre><code>connections:\n  lakehouse:\n    auth:\n      mode: key_vault\n      key_vault: my-key-vault\n      secret: storage-account-key\n</code></pre>"},{"location":"guides/best_practices/#never-log-secrets","title":"Never Log Secrets","text":"<p>Odibi automatically redacts values that look like secrets, but be careful in custom transformations:</p> <pre><code>@transform\ndef my_transform(context, params):\n    # \u274c NEVER do this\n    print(f\"Using password: {params['password']}\")\n\n    # \u2705 Do this instead\n    logger.info(\"Connecting to database...\")\n</code></pre>"},{"location":"guides/best_practices/#10-version-control","title":"10. Version Control","text":""},{"location":"guides/best_practices/#git-ignore-list","title":"Git Ignore List","text":"<pre><code># .gitignore\n.env\n*.pyc\n__pycache__/\n.odibi/\nstories/\n*.log\n.venv/\n</code></pre>"},{"location":"guides/best_practices/#commit-guidelines","title":"Commit Guidelines","text":"Change Type Commit Message New pipeline <code>feat(bronze): add customer ingestion pipeline</code> New node <code>feat(silver): add order enrichment node</code> Bug fix <code>fix(gold): correct revenue calculation</code> Config change <code>chore: update retry settings</code>"},{"location":"guides/best_practices/#branch-strategy","title":"Branch Strategy","text":"<pre><code>main           # Production-ready pipelines\n\u251c\u2500\u2500 develop    # Integration branch\n\u251c\u2500\u2500 feature/*  # New pipelines/nodes\n\u2514\u2500\u2500 fix/*      # Bug fixes\n</code></pre>"},{"location":"guides/best_practices/#pr-checklist","title":"PR Checklist","text":"<ul> <li>[ ] Pipeline runs locally without errors</li> <li>[ ] Node descriptions added</li> <li>[ ] Sensitive columns marked</li> <li>[ ] Incremental config for large tables</li> <li>[ ] Tests pass</li> </ul>"},{"location":"guides/best_practices/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/best_practices/#project-organization-cheat-sheet","title":"Project Organization Cheat Sheet","text":"<pre><code>project.yaml          \u2192 Connections, settings, imports (NO pipelines)\npipelines/{layer}/    \u2192 One YAML per pipeline\ntransformations/      \u2192 Custom Python code\n.env                  \u2192 Local secrets (git-ignored)\n</code></pre>"},{"location":"guides/best_practices/#node-checklist","title":"Node Checklist","text":"<ul> <li>[ ] Descriptive name (<code>clean_orders</code> not <code>node_1</code>)</li> <li>[ ] Description explaining purpose</li> <li>[ ] Tags for filtering (<code>daily</code>, <code>critical</code>)</li> <li>[ ] <code>cache: true</code> if used by multiple nodes</li> <li>[ ] <code>sensitive</code> for PII columns</li> <li>[ ] Incremental config for large tables</li> </ul>"},{"location":"guides/best_practices/#performance-checklist","title":"Performance Checklist","text":"<ul> <li>[ ] <code>use_arrow: true</code> for Pandas</li> <li>[ ] Incremental loading for large sources</li> <li>[ ] <code>skip_if_unchanged</code> for dimensions</li> <li>[ ] <code>skip_null_profiling</code> for very large tables</li> <li>[ ] <code>cluster_by</code> for Spark/Delta</li> </ul>"},{"location":"guides/best_practices/#related-documentation","title":"Related Documentation","text":"<ul> <li>The Definitive Guide \u2014 Deep dive into architecture</li> <li>Performance Tuning \u2014 Optimization details</li> <li>Production Deployment \u2014 Going to production</li> <li>Cross-Pipeline Dependencies \u2014 <code>$pipeline.node</code> references</li> <li>YAML Schema Reference \u2014 Full configuration options</li> <li>Validation Overview \u2014 Data quality framework</li> <li>Glossary \u2014 Terminology reference</li> </ul>"},{"location":"guides/cli_master_guide/","title":"Odibi CLI: Zero to Hero","text":"<p>Ultimate Cheatsheet &amp; Reference (v2.4.0)</p> <p>The Command Line Interface (CLI) is your primary tool for managing Odibi projects.</p>"},{"location":"guides/cli_master_guide/#level-1-the-basics","title":"\ud83d\udfe2 Level 1: The Basics","text":""},{"location":"guides/cli_master_guide/#1-create-a-new-pipeline-file","title":"1. Create a New Pipeline File","text":"<p>Generate a \"Master Kitchen Sink\" reference file with all features enabled.</p> <pre><code>odibi create my_pipeline.yaml\n</code></pre>"},{"location":"guides/cli_master_guide/#2-run-a-pipeline","title":"2. Run a Pipeline","text":"<p>Execute the pipeline defined in your YAML file.</p> <pre><code>odibi run my_pipeline.yaml\n</code></pre> <p>Common Flags: *   <code>--dry-run</code>: Simulate execution (don't write data). *   <code>--resume</code>: Resume from the last failure (skips successful nodes). *   <code>--env prod</code>: Load production environment variables.</p>"},{"location":"guides/cli_master_guide/#level-2-intermediate-management","title":"\ud83d\udfe1 Level 2: Intermediate (Management)","text":""},{"location":"guides/cli_master_guide/#1-initialize-a-full-project","title":"1. Initialize a Full Project","text":"<p>Don't just create a file; create a full folder structure with best practices (Bronze/Silver/Gold layers).</p> <pre><code># Creates folder 'my_project' with organized subfolders\nodibi init-pipeline my_project --template kitchen-sink\n</code></pre>"},{"location":"guides/cli_master_guide/#2-validate-configuration","title":"2. Validate Configuration","text":"<p>Check if your YAML is valid before running it.</p> <pre><code>odibi validate my_pipeline.yaml\n</code></pre>"},{"location":"guides/cli_master_guide/#3-visualize-dependencies","title":"3. Visualize Dependencies","text":"<p>Generate a dependency graph to understand flow.</p> <pre><code># ASCII Art (Default)\nodibi graph my_pipeline.yaml\n\n# Mermaid Diagram (for Markdown)\nodibi graph my_pipeline.yaml --format mermaid\n</code></pre>"},{"location":"guides/cli_master_guide/#level-3-hero-advanced-tools","title":"\ud83d\udd34 Level 3: Hero (Advanced Tools)","text":""},{"location":"guides/cli_master_guide/#1-deep-diff-compare-runs","title":"1. Deep Diff (Compare Runs)","text":"<p>Did a pipeline run suddenly output fewer rows? Use <code>story diff</code> to compare two runs.</p> <pre><code># List available runs\nodibi story list\n\n# Compare two story JSON files\nodibi story diff stories/runs/20231027_120000.json stories/runs/20231027_120500.json\n</code></pre> <p>Output: Shows execution time differences, row count changes, and success rates.</p>"},{"location":"guides/cli_master_guide/#2-manage-secrets","title":"2. Manage Secrets","text":"<p>Securely manage local secrets for your pipelines.</p> <pre><code># Initialize secrets store (creates .env.template)\nodibi secrets init\n\n# Validate all secrets are configured\nodibi secrets validate\n</code></pre>"},{"location":"guides/cli_master_guide/#level-4-system-catalog-the-brain","title":"\ud83e\udde0 Level 4: System Catalog (The Brain)","text":""},{"location":"guides/cli_master_guide/#query-the-system-catalog","title":"Query the System Catalog","text":"<p>The System Catalog stores metadata about all your runs, pipelines, nodes, and state. Query it without manually reading Delta tables.</p> <pre><code># List recent runs\nodibi catalog runs config.yaml\n\n# Filter by pipeline and status\nodibi catalog runs config.yaml --pipeline my_etl --status SUCCESS --days 14\n\n# List registered pipelines\nodibi catalog pipelines config.yaml\n\n# List nodes (optionally filter by pipeline)\nodibi catalog nodes config.yaml --pipeline my_etl\n\n# View HWM state checkpoints\nodibi catalog state config.yaml\n\n# Get execution statistics\nodibi catalog stats config.yaml --days 30\n</code></pre> <p>Catalog Subcommands: | Subcommand | Description | | :--- | :--- | | <code>runs</code> | List execution runs from <code>meta_runs</code> | | <code>pipelines</code> | List registered pipelines from <code>meta_pipelines</code> | | <code>nodes</code> | List registered nodes from <code>meta_nodes</code> | | <code>state</code> | List HWM state checkpoints from <code>meta_state</code> | | <code>tables</code> | List registered assets from <code>meta_tables</code> | | <code>metrics</code> | List metrics definitions from <code>meta_metrics</code> | | <code>patterns</code> | List pattern compliance from <code>meta_patterns</code> | | <code>stats</code> | Show execution statistics (success rate, avg duration, etc.) |</p> <p>Common Flags: * <code>--format json</code>: Output as JSON instead of ASCII table * <code>--pipeline &lt;name&gt;</code>: Filter by pipeline name * <code>--days &lt;n&gt;</code>: Show data from last N days (default: 7) * <code>--limit &lt;n&gt;</code>: Limit number of results (default: 20)</p>"},{"location":"guides/cli_master_guide/#level-5-schema-lineage-tracking","title":"\ud83d\udd0d Level 5: Schema &amp; Lineage Tracking","text":""},{"location":"guides/cli_master_guide/#schema-version-history","title":"Schema Version History","text":"<p>Track how table schemas evolve over time.</p> <pre><code># View schema history for a table\nodibi schema history silver/customers --config config.yaml\n\n# Compare two schema versions\nodibi schema diff silver/customers --config config.yaml --from-version 3 --to-version 5\n\n# Output as JSON\nodibi schema history silver/customers --config config.yaml --format json\n</code></pre> <p>Example Output:</p> <pre><code>Schema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre>"},{"location":"guides/cli_master_guide/#cross-pipeline-lineage","title":"Cross-Pipeline Lineage","text":"<p>Trace data dependencies across pipelines.</p> <pre><code># Trace upstream sources\nodibi lineage upstream gold/customer_360 --config config.yaml\n\n# Trace downstream consumers\nodibi lineage downstream bronze/customers_raw --config config.yaml\n\n# Impact analysis - what would be affected by changes?\nodibi lineage impact bronze/customers_raw --config config.yaml\n</code></pre> <p>Example Output (upstream):</p> <pre><code>Upstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre> <p>Example Output (impact):</p> <pre><code>\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 3 downstream table(s) in 2 pipeline(s)\n</code></pre> <p>Schema Subcommands: | Subcommand | Description | | :--- | :--- | | <code>history</code> | View schema version history for a table | | <code>diff</code> | Compare two schema versions |</p> <p>Lineage Subcommands: | Subcommand | Description | | :--- | :--- | | <code>upstream</code> | Trace upstream sources of a table | | <code>downstream</code> | Trace downstream consumers of a table | | <code>impact</code> | Impact analysis for schema changes |</p> <p>Common Flags: * <code>--config &lt;path&gt;</code>: Path to YAML config file (required) * <code>--depth &lt;n&gt;</code>: Maximum depth to traverse (default: 3) * <code>--format json</code>: Output as JSON * <code>--limit &lt;n&gt;</code>: Limit results (schema history only)</p>"},{"location":"guides/cli_master_guide/#command-reference","title":"\ud83d\udcc4 Command Reference","text":"Command Description <code>run</code> Execute a pipeline. <code>create</code> Create a single YAML config file. <code>init-pipeline</code> Scaffold a full project directory. <code>validate</code> Check YAML syntax and logic. <code>graph</code> Visualize pipeline dependencies. <code>story</code> Manage and compare execution reports (<code>generate</code>, <code>diff</code>, <code>list</code>). <code>secrets</code> Manage local secure secrets (<code>init</code>, <code>validate</code>). <code>catalog</code> Query System Catalog (<code>runs</code>, <code>pipelines</code>, <code>nodes</code>, <code>state</code>, <code>stats</code>). <code>schema</code> Schema version tracking (<code>history</code>, <code>diff</code>). <code>lineage</code> Cross-pipeline lineage (<code>upstream</code>, <code>downstream</code>, <code>impact</code>). <code>init-vscode</code> Setup VS Code environment."},{"location":"guides/dimensional_modeling_guide/","title":"Dimensional Modeling Guide for ODIBI","text":"<p>A practical reference for building data warehouses with ODIBI.</p>"},{"location":"guides/dimensional_modeling_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Problem We're Solving</li> <li>Facts and Dimensions</li> <li>The Star Schema</li> <li>Natural Keys vs Surrogate Keys</li> <li>Bronze \u2192 Silver \u2192 Gold Flow</li> <li>Where Do IDs Come From?</li> <li>Lookup Tables vs Dimension Tables</li> <li>Slowly Changing Dimensions (SCD)</li> <li>The Date Dimension</li> <li>Aggregations</li> <li>Common Mistakes to Avoid</li> <li>ODIBI Current State vs Target</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#the-problem-were-solving","title":"The Problem We're Solving","text":""},{"location":"guides/dimensional_modeling_guide/#raw-data-is-hard-to-query","title":"Raw Data is Hard to Query","text":"<p>Your source systems store data for operations, not analysis:</p> <pre><code>orders.csv\n| customer_email    | product_name | quantity | price | timestamp           |\n|-------------------|--------------|----------|-------|---------------------|\n| john@mail.com     | Latte        | 2        | 11.00 | 2024-01-15 09:15:00 |\n</code></pre> <p>To answer \"What was revenue by product category for Q4 weekends?\", you need: - Product category (not in orders) - Whether it's a weekend (calculated from timestamp) - Q4 filter (calculated from timestamp)</p> <p>Dimensional modeling pre-calculates and organizes this context.</p>"},{"location":"guides/dimensional_modeling_guide/#facts-and-dimensions","title":"Facts and Dimensions","text":""},{"location":"guides/dimensional_modeling_guide/#fact-table-what-happened","title":"Fact Table = What Happened","text":"<p>Events, transactions, measurements. Numbers you can add/count/average.</p> Contains Example Measures (numbers) <code>quantity</code>, <code>price</code>, <code>total_amount</code> Foreign keys (pointers to dimensions) <code>customer_sk</code>, <code>product_sk</code>, <code>date_sk</code> Degenerate dimensions (IDs with no table) <code>order_id</code>, <code>invoice_number</code> <p>Key insight: Facts are usually append-only. Once it happened, it happened.</p>"},{"location":"guides/dimensional_modeling_guide/#dimension-table-context-about-what-happened","title":"Dimension Table = Context About What Happened","text":"<p>Who, what, when, where, why. Descriptive attributes.</p> Contains Example Primary key <code>customer_sk</code> Natural key <code>customer_id</code> (from source system) Descriptive attributes <code>name</code>, <code>email</code>, <code>city</code>, <code>segment</code> Hierarchies <code>city</code> \u2192 <code>state</code> \u2192 <code>country</code> <p>Key insight: Dimensions change over time. A customer might move cities.</p>"},{"location":"guides/dimensional_modeling_guide/#the-star-schema","title":"The Star Schema","text":"<pre><code>                    dim_customer\n                         \u2502\n                         \u2502 customer_sk\n                         \u2502\ndim_product \u2500\u2500\u2500\u2500\u2500\u2500\u2500 fact_orders \u2500\u2500\u2500\u2500\u2500\u2500\u2500 dim_date\n         product_sk      \u2502        date_sk\n                         \u2502\n                    dim_region\n                         \u2502\n                    region_sk\n</code></pre> <p>Why this shape?</p> <ol> <li>Storage efficiency \u2014 Store \"John Smith, Premium, NYC\" once, reference by ID</li> <li>Query speed \u2014 Filter small dimension tables first, then join to facts</li> <li>Flexibility \u2014 Add new attributes to dimensions, all reports get them</li> <li>Single source of truth \u2014 Customer's segment defined in ONE place</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#natural-keys-vs-surrogate-keys","title":"Natural Keys vs Surrogate Keys","text":"Type What It Is Example Who Creates It Natural Key Business identifier <code>customer_id</code>, <code>email</code>, <code>product_sku</code> Source system or you (via hash) Surrogate Key Warehouse-generated integer <code>customer_sk = 1001</code> Data warehouse (auto-increment)"},{"location":"guides/dimensional_modeling_guide/#why-use-surrogate-keys","title":"Why Use Surrogate Keys?","text":"<ol> <li>Faster JOINs \u2014 Integers are faster than strings</li> <li>SCD2 support \u2014 One <code>customer_id</code> can have multiple <code>customer_sk</code> values (history)</li> <li>Survives source changes \u2014 If source system changes IDs, yours don't</li> <li>Unknown member \u2014 <code>customer_sk = 0</code> for orphan records</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#the-unknown-member","title":"The \"Unknown\" Member","text":"<p>What if an order references <code>customer_id = 999</code> but that customer doesn't exist?</p> <p>Solution: Create a special row in every dimension:</p> customer_sk customer_id name city 0 -1 Unknown Unknown 1001 47 John NYC <p>Orphan facts JOIN to <code>customer_sk = 0</code> \u2014 you never lose data.</p>"},{"location":"guides/dimensional_modeling_guide/#bronze-silver-gold-flow","title":"Bronze \u2192 Silver \u2192 Gold Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           BRONZE                                     \u2502\n\u2502   Raw data, as-is from source. Messy, duplicates, nulls, no IDs.    \u2502\n\u2502   Example: raw_orders.csv, raw_customers.json                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           SILVER                                     \u2502\n\u2502   Cleaned, validated, deduplicated. NATURAL KEYS assigned.          \u2502\n\u2502   Example: clean_orders, clean_customers                             \u2502\n\u2502   Keys: customer_id (hash or from source), product_id, order_id     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            GOLD                                      \u2502\n\u2502   Dimensional model. SURROGATE KEYS, SCD history, aggregates.       \u2502\n\u2502   Example: dim_customer, dim_product, fact_orders, agg_daily_sales  \u2502\n\u2502   Keys: customer_sk, product_sk, date_sk (integers)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#key-point-silver-uses-natural-keys-gold-uses-surrogate-keys","title":"Key Point: Silver Uses Natural Keys, Gold Uses Surrogate Keys","text":"Layer Keys Used Bronze Whatever source provides (emails, names, raw IDs) Silver Natural keys \u2014 business identifiers (customer_id, product_id) Gold Surrogate keys \u2014 warehouse-generated integers (customer_sk)"},{"location":"guides/dimensional_modeling_guide/#where-do-ids-come-from","title":"Where Do IDs Come From?","text":""},{"location":"guides/dimensional_modeling_guide/#scenario-a-source-system-has-ids-common","title":"Scenario A: Source System Has IDs (Common)","text":"<p>Your CRM already has <code>customer_id = 47</code>. Just pass it through.</p> <pre><code>Source \u2192 Bronze \u2192 Silver \u2192 Gold\n  47       47       47      + customer_sk = 1001\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#scenario-b-source-system-has-no-ids-your-data","title":"Scenario B: Source System Has NO IDs (Your Data)","text":"<p>You only have <code>email</code> and <code>product_name</code>. You must generate IDs.</p> <p>Option 1: Hash (Recommended)</p> <pre><code>SELECT MD5(LOWER(TRIM(email))) as customer_id, email, name\nFROM clean_customers\n</code></pre> <ul> <li>Same email = same ID, forever</li> <li>Deterministic, stateless, simple</li> <li>Ugly IDs but who cares</li> </ul> <p>Option 2: Persistent Lookup Table</p> <ul> <li>Store <code>email \u2192 customer_id</code> mapping</li> <li>On each run, only assign new IDs to new emails</li> <li>Nice sequential numbers (1, 2, 3...)</li> <li>More complex, requires state</li> </ul> <p>\u26a0\ufe0f NEVER use RANK() or ROW_NUMBER() alone \u2014 IDs will shift when data changes!</p>"},{"location":"guides/dimensional_modeling_guide/#lookup-tables-vs-dimension-tables","title":"Lookup Tables vs Dimension Tables","text":"<p>These are NOT the same thing.</p> Lookup Table Dimension Table Silver layer Gold layer Maps <code>email \u2192 customer_id</code> Has <code>customer_sk, customer_id, name, city, ...</code> Just a helper for ID generation Official, versioned, surrogate-keyed entity No history SCD2 history tracking Simple key-value Rich with attributes"},{"location":"guides/dimensional_modeling_guide/#the-flow","title":"The Flow","text":"<pre><code>Bronze: raw_customers (email, name, city)\n    \u2502\n    \u25bc\nSilver: customer_lookup (email \u2192 customer_id via hash)\n    \u2502\n    \u25bc\nSilver: clean_customers (customer_id, email, name, city)\n    \u2502\n    \u25bc\nGold: dim_customer (customer_SK, customer_id, name, city, valid_from, valid_to, is_current)\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>What happens when a customer moves from NYC to LA?</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-0-keep-original","title":"SCD Type 0: Keep Original","text":"<p>Never update. Always shows original value.</p> <p>Use case: Birth date, original signup date</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-1-overwrite","title":"SCD Type 1: Overwrite","text":"<p>Update in place. Lose history.</p> customer_sk customer_id city 1001 47 LA <p>Use case: Correcting typos, current-state-only reporting</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-2-track-history-most-common","title":"SCD Type 2: Track History (Most Common)","text":"<p>Create new row, close old row.</p> customer_sk customer_id city valid_from valid_to is_current 1001 47 NYC 2020-01-01 2023-03-15 false 1002 47 LA 2023-03-15 NULL true <p>Use case: Historical analysis, \"What was their city when they ordered?\"</p>"},{"location":"guides/dimensional_modeling_guide/#looking-up-surrogate-keys-for-facts","title":"Looking Up Surrogate Keys for Facts","text":"<p>When building fact tables, filter by <code>is_current = true</code>:</p> <pre><code>SELECT \n  o.order_id,\n  dc.customer_sk\nFROM clean_orders o\nLEFT JOIN dim_customer dc \n  ON o.customer_id = dc.customer_id \n  AND dc.is_current = true  -- Only match current version!\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#the-date-dimension","title":"The Date Dimension","text":"<p>Every business question involves time. Pre-calculate all date attributes.</p> Column Example Why Useful date_sk 20240115 Primary key full_date 2024-01-15 Actual date day_of_week Monday Filter by weekday is_weekend false Weekend analysis month 1 Monthly aggregation month_name January Display-friendly quarter 1 Quarterly reporting year 2024 Annual comparison is_holiday false Holiday impact"},{"location":"guides/dimensional_modeling_guide/#why-pre-calculate","title":"Why Pre-Calculate?","text":"<p>Without date dimension:</p> <pre><code>-- Calculates for every row\nWHERE DAYOFWEEK(order_date) IN (1, 7)\n</code></pre> <p>With date dimension:</p> <pre><code>-- Pre-calculated, indexed, fast\nWHERE d.is_weekend = true\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#aggregations","title":"Aggregations","text":""},{"location":"guides/dimensional_modeling_guide/#why-pre-aggregate","title":"Why Pre-Aggregate?","text":"<p>Without aggregates:</p> <pre><code>-- Scans 1 BILLION rows every time\nSELECT SUM(revenue) FROM fact_sales WHERE month = 'January'\n</code></pre> <p>With aggregates:</p> <pre><code>-- Scans 31 rows\nSELECT SUM(daily_revenue) FROM agg_daily_sales WHERE month = 'January'\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#the-grain-concept","title":"The Grain Concept","text":"<p>Grain = what one row represents</p> Table Grain fact_sales One order line item agg_daily_sales All sales for a day + product + region agg_monthly_sales All sales for a month + product + region <p>Rule: You can roll UP (daily \u2192 monthly) but not drill DOWN (monthly \u2192 daily) without the source.</p>"},{"location":"guides/dimensional_modeling_guide/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":""},{"location":"guides/dimensional_modeling_guide/#using-rank-to-generate-ids","title":"\u274c Using RANK() to Generate IDs","text":"<pre><code>-- WRONG: IDs will shift when new data arrives!\nSELECT DENSE_RANK() OVER (ORDER BY email) as customer_id\n</code></pre> <p>Fix: Use hash or persistent lookup table.</p>"},{"location":"guides/dimensional_modeling_guide/#building-lookups-from-bronze-dirty-data","title":"\u274c Building Lookups from Bronze (Dirty Data)","text":"<pre><code>-- WRONG: Bronze has duplicates, nulls, deleted records\nSELECT DISTINCT email FROM raw_customers\n</code></pre> <p>Fix: Build lookups from Silver (cleaned data).</p>"},{"location":"guides/dimensional_modeling_guide/#joining-fact-to-dimension-without-is_current-filter","title":"\u274c Joining Fact to Dimension Without is_current Filter","text":"<pre><code>-- WRONG: May get multiple matches (historical + current)\nSELECT * FROM fact_orders f\nJOIN dim_customer dc ON f.customer_id = dc.customer_id\n</code></pre> <p>Fix: Add <code>AND dc.is_current = true</code>.</p>"},{"location":"guides/dimensional_modeling_guide/#confusing-lookup-tables-with-dimension-tables","title":"\u274c Confusing Lookup Tables with Dimension Tables","text":"<ul> <li>Lookup = Silver, just maps keys</li> <li>Dimension = Gold, has surrogate keys + history + attributes</li> </ul>"},{"location":"guides/dimensional_modeling_guide/#not-having-an-unknown-member","title":"\u274c Not Having an Unknown Member","text":"<p>If a fact references a customer that doesn't exist, the JOIN fails and you lose data.</p> <p>Fix: Every dimension should have <code>SK = 0, name = 'Unknown'</code>.</p>"},{"location":"guides/dimensional_modeling_guide/#odibi-current-state-vs-target","title":"ODIBI Current State vs Target","text":""},{"location":"guides/dimensional_modeling_guide/#what-odibi-has-now","title":"What ODIBI Has Now","text":"Pattern What It Does Limitation <code>FactPattern</code> Dedup + pass through No SK lookup, no orphan handling <code>SCD2Pattern</code> Track history No auto surrogate key <code>MergePattern</code> Upsert logic \u2014 <code>SnapshotPattern</code> Point-in-time capture \u2014 <code>generate_surrogate_key</code> Hash-based key Not integrated into patterns"},{"location":"guides/dimensional_modeling_guide/#what-were-adding","title":"What We're Adding","text":"Pattern What It Will Do <code>DimensionPattern</code> Auto SK + SCD + unknown member + audit columns <code>DateDimensionPattern</code> Generate complete date dimension <code>Enhanced FactPattern</code> Auto SK lookups + orphan handling + grain validation <code>AggregationPattern</code> Declarative GROUP BY + time rollups"},{"location":"guides/dimensional_modeling_guide/#target-declarative-dimensional-modeling","title":"Target: Declarative Dimensional Modeling","text":"<pre><code># This should just work\n- name: dim_customer\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      unknown_member: true\n\n- name: fact_orders\n  pattern:\n    type: fact\n    params:\n      grain: [order_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          surrogate_key: customer_sk\n      orphan_handling: unknown\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#quick-reference-the-mental-model","title":"Quick Reference: The Mental Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        BUSINESS QUESTION                             \u2502\n\u2502   \"What was revenue by product category for Q4 weekends?\"           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         GOLD LAYER                                   \u2502\n\u2502   fact_orders \u2190\u2192 dim_product \u2190\u2192 dim_date                            \u2502\n\u2502   Surrogate keys, SCD2 history, pre-aggregated                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        SILVER LAYER                                  \u2502\n\u2502   clean_orders, clean_customers, customer_lookup                    \u2502\n\u2502   Natural keys (hash or source), validated, deduplicated            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        BRONZE LAYER                                  \u2502\n\u2502   raw_orders.csv, raw_customers.json                                \u2502\n\u2502   As-is from source, messy, no IDs if source doesn't have them     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#glossary","title":"Glossary","text":"Term Definition Fact Table of events/transactions with measures Dimension Table of context (who, what, when, where) Grain What one row represents Natural Key Business identifier (customer_id) Surrogate Key Warehouse-generated integer (customer_sk) SCD Slowly Changing Dimension Star Schema Fact in center, dimensions around it Lookup Table Helper table to map values to IDs Unknown Member Row with SK=0 for orphan handling Aggregate Pre-calculated summary table"},{"location":"guides/dogfooding/","title":"\ud83d\udc36 Dogfooding Guide","text":"<p>\"Eat your own dog food.\"</p> <p>This guide explains how we use Odibi to build Odibi, and how you can use the <code>odibi-metrics</code> project to validate your own environment.</p>"},{"location":"guides/dogfooding/#what-is-dogfooding","title":"What is Dogfooding?","text":"<p>Dogfooding means using your own product to do your actual job.</p> <p>Instead of testing Odibi with synthetic data (like \"foo\", \"bar\", \"test_1\"), we use it to track the development velocity of the Odibi framework itself. This forces us to encounter real-world problems\u2014messy API data, rate limits, Unicode errors, schema drift\u2014before our users do.</p>"},{"location":"guides/dogfooding/#the-odibi-metrics-pipeline","title":"The <code>odibi-metrics</code> Pipeline","text":"<p>We have included a reference implementation in <code>examples/odibi-metrics</code>. This is a real pipeline that:</p> <ol> <li>Extracts: Connects to the GitHub API to fetch Issues and PRs from <code>henryodibi11/Odibi</code>.</li> <li>Transforms: Cleans the data, handles timezones, and calculates weekly velocity (opened vs. closed tasks).</li> <li>Loads: Saves the results to a Gold layer (<code>velocity.csv</code>) and updates the System Catalog.</li> </ol>"},{"location":"guides/dogfooding/#how-to-run-it","title":"How to Run It","text":"<pre><code># 1. Go to the example directory\ncd examples/odibi-metrics\n\n# 2. Run the pipeline\nodibi run odibi.yaml\n</code></pre>"},{"location":"guides/dogfooding/#what-it-teaches-you","title":"What It Teaches You","text":"<p>If this pipeline fails, it means Odibi is not stable enough for production. We found (and fixed) the following issues using this exact pipeline:</p> <ul> <li>\u274c Unicode Errors: Windows consoles crashing on emoji output.</li> <li>\u274c Schema Drift: The System Catalog failing when new metadata fields were added.</li> <li>\u274c Timezone Bugs: Pandas crashing when grouping TZ-aware dates.</li> </ul>"},{"location":"guides/dogfooding/#how-to-file-issues","title":"How to File Issues","text":"<p>When you find a bug while running Odibi (or the dogfood pipeline), you should track it using GitHub Issues.</p> <ol> <li>Go to the Issues Tab on GitHub.</li> <li>Click New Issue.</li> <li>Title: Short summary (e.g., \"Crash on Windows 11\").</li> <li>Body: Paste the error log and your <code>odibi.yaml</code>.</li> </ol> <p>The Meta-Loop: Once you file the issue, the <code>odibi-metrics</code> pipeline will actually download that issue the next time it runs, adding it to your project statistics. You are using Odibi to measure how fast you are fixing Odibi.</p>"},{"location":"guides/environments/","title":"Managing Environments","text":"<p>Odibi allows you to define a single pipeline configuration that adapts to different contexts (e.g., Local Development, Testing, Production) using the <code>environments</code> block. This prevents configuration drift and ensures your pipeline logic remains consistent while infrastructure details change.</p>"},{"location":"guides/environments/#how-it-works","title":"How it Works","text":"<p>Odibi uses a Base Configuration + Override model: 1.  Base Configuration: Defines your default settings (typically for local development). 2.  Environment Overrides: Specific blocks that patch or replace values in the base configuration when that environment is active.</p>"},{"location":"guides/environments/#configuration-structure","title":"Configuration Structure","text":"<p>Odibi supports two ways to define environments: 1.  Inline Block: Using an <code>environments</code> block in your main config file. 2.  External Files: Using separate <code>env.{env}.yaml</code> files (e.g., <code>env.prod.yaml</code>).</p>"},{"location":"guides/environments/#method-1-inline-block","title":"Method 1: Inline Block","text":"<p>Add an <code>environments</code> section to your <code>project.yaml</code>:</p> <pre><code># ... base config ...\nenvironments:\n  prod:\n    engine: spark\n</code></pre>"},{"location":"guides/environments/#method-2-external-files-recommended-for-large-configs","title":"Method 2: External Files (Recommended for large configs)","text":"<p>Keep your main <code>odibi.yaml</code> clean by putting overrides in separate files.</p> <p>File: <code>odibi.yaml</code></p> <pre><code>project: Sales Data Pipeline\nengine: pandas\nconnections:\n  data_lake:\n    type: local\n    base_path: ./data\n</code></pre> <p>File: <code>env.prod.yaml</code></p> <pre><code># Automatically merged when running with --env prod\nengine: spark\nconnections:\n  data_lake:\n    type: azure_adls\n    account: prod_acc\n</code></pre> <p>When you run <code>odibi run odibi.yaml --env prod</code>, Odibi will: 1. Load <code>odibi.yaml</code>. 2. Look for <code>env.prod.yaml</code> in the same directory. 3. Merge the prod config on top of the base config.</p>"},{"location":"guides/environments/#inline-example-method-1","title":"Inline Example (Method 1)","text":"<pre><code># --- 1. Base Configuration (Default / Local) ---\nproject: Sales Data Pipeline\nengine: pandas\nretry:\n  enabled: false\n\nconnections:\n  data_lake:\n    type: local\n    base_path: ./data/raw\n\npipelines:\n  - pipeline: ingest_sales\n    nodes:\n      - name: read_csv\n        read:\n          connection: data_lake\n          path: sales.csv\n\n# --- 2. Environment Overrides ---\nenvironments:\n  # Production Environment\n  prod:\n    engine: spark  # Switch to Spark for scale\n    retry:\n      enabled: true\n      max_attempts: 3\n    connections:\n      data_lake:\n        type: azure_adls\n        account: mycompanyprod\n        container: sales-data\n        auth_mode: managed_identity\n    story:\n      max_sample_rows: 0 # Disable data sampling for security\n\n  # Testing Environment\n  test:\n    connections:\n      data_lake:\n        type: local\n        base_path: ./data/test_fixtures\n</code></pre>"},{"location":"guides/environments/#usage","title":"Usage","text":""},{"location":"guides/environments/#cli","title":"CLI","text":"<p>Use the <code>--env</code> flag to activate an environment.</p> <p>Run in Default (Base) Environment:</p> <pre><code>odibi run project.yaml\n</code></pre> <p>Run in Production:</p> <pre><code>odibi run project.yaml --env prod\n</code></pre>"},{"location":"guides/environments/#python-api","title":"Python API","text":"<p>Pass the <code>env</code> parameter when initializing the <code>PipelineManager</code>.</p> <pre><code>from odibi.pipeline import PipelineManager\n\n# Load Prod Configuration\nmanager = PipelineManager.from_yaml(\"project.yaml\", env=\"prod\")\n\n# Run Pipeline\nmanager.run(\"ingest_sales\")\n</code></pre>"},{"location":"guides/environments/#databricks-example","title":"Databricks Example","text":"<p>In a Databricks notebook, you can use widgets to switch environments dynamically without changing code.</p> <pre><code># 1. Create Widget\ndbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"test\", \"prod\"])\n\n# 2. Get Selection\ncurrent_env = dbutils.widgets.get(\"environment\")\n\n# 3. Run Pipeline\nmanager = PipelineManager.from_yaml(\"/dbfs/project.yaml\", env=current_env)\nmanager.run()\n</code></pre>"},{"location":"guides/environments/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/environments/#1-swapping-storage-local-vs-cloud","title":"1. Swapping Storage (Local vs. Cloud)","text":"<p>Develop locally with CSVs, deploy to ADLS/S3 without changing pipeline code.</p> <pre><code>connections:\n  storage: { type: local, base_path: ./data }\n\nenvironments:\n  prod:\n    connections:\n      storage: { type: azure_adls, account: prod_acc, container: data }\n</code></pre>"},{"location":"guides/environments/#2-scaling-engines-pandas-vs-spark","title":"2. Scaling Engines (Pandas vs. Spark)","text":"<p>Use Pandas for fast local iteration and unit tests, but switch to Spark for distributed processing in production.</p> <pre><code>engine: pandas\n\nenvironments:\n  prod:\n    engine: spark\n</code></pre>"},{"location":"guides/environments/#3-security-privacy","title":"3. Security &amp; Privacy","text":"<p>Disable data sampling in stories for production to prevent PII leakage, while keeping it enabled in dev for debugging.</p> <pre><code>story:\n  max_sample_rows: 20\n\nenvironments:\n  prod:\n    story:\n      max_sample_rows: 0\n</code></pre>"},{"location":"guides/environments/#4-alerting","title":"4. Alerting","text":"<p>Only send Slack/Teams notifications when running in production.</p> <pre><code>alerts: []  # No alerts in dev\n\nenvironments:\n  prod:\n    alerts:\n      - type: slack\n        url: ${SLACK_WEBHOOK}\n</code></pre>"},{"location":"guides/medallion_architecture/","title":"Medallion Architecture Guide","text":"<p>A beginner-friendly guide to understanding Bronze, Silver, and Gold data layers.</p>"},{"location":"guides/medallion_architecture/#what-is-medallion-architecture","title":"What is Medallion Architecture?","text":"<p>Medallion Architecture organizes your data into three layers, like refining raw ore into polished gold:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                     \u2502\n\u2502   Source Systems          Bronze           Silver           Gold   \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500   \u2502\n\u2502                                                                     \u2502\n\u2502   [SQL Server] \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  [Raw Copy]  \u2500\u2500\u2500\u25ba  [Cleaned]  \u2500\u2500\u2500\u25ba  [Facts] \u2502\n\u2502   [API]        \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  [Raw Copy]  \u2500\u2500\u2500\u25ba  [Cleaned]  \u2500\u2500\u2500\u25ba  [Dims]  \u2502\n\u2502   [Files]      \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  [Raw Copy]  \u2500\u2500\u2500\u25ba  [Cleaned]  \u2500\u2500\u2500\u25ba  [KPIs]  \u2502\n\u2502                                                                     \u2502\n\u2502   \"Just land it\"      \"Fix it\"         \"Make it                    \u2502\n\u2502                                          business-ready\"           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Think of it like cooking: - Bronze = Raw ingredients from the store (as-is, untouched) - Silver = Ingredients washed, chopped, and prepped (cleaned, standardized) - Gold = The finished dish ready to serve (combined, calculated, business-ready)</p>"},{"location":"guides/medallion_architecture/#layer-1-bronze-raw-data","title":"Layer 1: Bronze (Raw Data)","text":""},{"location":"guides/medallion_architecture/#purpose","title":"Purpose","text":"<p>Land data exactly as it comes from the source. No transformations. Just copy it.</p>"},{"location":"guides/medallion_architecture/#what-happens-here","title":"What Happens Here","text":"Operation Example Why Raw ingestion Copy SQL table to Delta Preserve original data Add metadata <code>_extracted_at</code> timestamp Track when data arrived Schema preservation Keep all columns, even unused Don't lose anything"},{"location":"guides/medallion_architecture/#what-does-not-happen-here","title":"What Does NOT Happen Here","text":"<p>\u274c No cleaning \u274c No transformations \u274c No joins \u274c No filtering (except maybe date ranges for incremental loads)</p>"},{"location":"guides/medallion_architecture/#example-bronze-node","title":"Example Bronze Node","text":"<pre><code>- name: bronze_sales_orders\n  read:\n    connection: erp_database\n    table: dbo.SalesOrders\n  write:\n    connection: datalake\n    path: bronze/sales_orders\n    format: delta\n</code></pre>"},{"location":"guides/medallion_architecture/#the-golden-rule-of-bronze","title":"The Golden Rule of Bronze","text":"<p>\"If the source system has garbage data, Bronze has garbage data. That's okay.\"</p>"},{"location":"guides/medallion_architecture/#layer-2-silver-cleaned-conformed","title":"Layer 2: Silver (Cleaned &amp; Conformed)","text":""},{"location":"guides/medallion_architecture/#purpose_1","title":"Purpose","text":"<p>Clean and standardize ONE source at a time. Make it trustworthy.</p>"},{"location":"guides/medallion_architecture/#the-key-question","title":"The Key Question","text":"<p>\"Could this node run if only ONE source system existed?\"</p> <p>If YES \u2192 Silver \u2713 If NO \u2192 Probably Gold</p> <p>!!! note \"Reference Tables Are Allowed in Silver\"     The One-Source Test refers to business source systems, not reference/lookup data.</p> <pre><code>**Silver CAN join with:**\n\n- Reference/lookup tables (code mappings, static lists)\n- Dimension lookups for enrichment (product_code \u2192 product_name)\n- Self-joins within the same source\n\n**Silver should NOT join:**\n\n- Multiple business source systems (SAP + Salesforce \u2192 use Gold)\n</code></pre>"},{"location":"guides/medallion_architecture/#what-happens-here_1","title":"What Happens Here","text":""},{"location":"guides/medallion_architecture/#1-data-cleaning","title":"1. Data Cleaning","text":"<p>Fixing problems in the source data.</p> Operation Example Category Deduplication <code>ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC)</code> Cleaning Remove bad characters <code>REPLACE(name, '\"', '')</code> Cleaning Fix typos <code>REPLACE(status, 'Actve', 'Active')</code> Cleaning Handle nulls <code>COALESCE(middle_name, '')</code> Cleaning Trim whitespace <code>TRIM(customer_name)</code> Cleaning <pre><code>-- Example: Cleaning product codes\nCASE\n    WHEN LEFT(REPLACE(product_code, '\"', ''), 1) = 'X'\n    THEN SUBSTRING(REPLACE(product_code, '\"', ''), 2)\n    ELSE REPLACE(product_code, '\"', '')\nEND AS product_code\n</code></pre>"},{"location":"guides/medallion_architecture/#2-type-casting-standardization","title":"2. Type Casting &amp; Standardization","text":"<p>Making data types consistent.</p> Operation Example Category Cast types <code>CAST(date_string AS DATE)</code> Standardization Parse timestamps <code>to_timestamp(date_col)</code> Standardization Unit conversion <code>hours * 60 AS duration_minutes</code> Standardization Standardize casing <code>UPPER(country_code)</code> Standardization <pre><code>-- Example: Standardizing dates\nto_timestamp(order_date) AS order_date,\nDATEDIFF(to_date(order_date), to_date('2020-01-01')) + 1 AS date_id\n</code></pre>"},{"location":"guides/medallion_architecture/#3-conforming-to-standard-schema","title":"3. Conforming to Standard Schema","text":"<p>Mapping source-specific values to enterprise-standard values.</p> Operation Example Category Code mapping <code>'M1' \u2192 'Machine 1'</code> Conforming Category standardization <code>'Sched%' \u2192 'Scheduled'</code> Conforming Rename columns <code>cust_id AS customer_id</code> Conforming Add source context <code>'West Region' AS region_name</code> Conforming <pre><code>-- Example: Mapping source codes to standard names\nCASE\n    WHEN machine_code = 'M1' THEN 'Machine 1'\n    WHEN machine_code = 'M2' THEN 'Machine 2'\n    WHEN machine_code = 'M3' THEN 'Machine 3'\nEND AS machine_name,\n\nCASE\n    WHEN category LIKE '%Sched%' THEN 'Scheduled'\n    WHEN category LIKE '%Maint%' THEN 'Maintenance'\n    WHEN category LIKE '%Breakdown%' THEN 'Unplanned'\n    ELSE 'Other'\nEND AS downtime_category\n</code></pre>"},{"location":"guides/medallion_architecture/#4-enrichment-via-lookups","title":"4. Enrichment via Lookups","text":"<p>Adding dimension attributes from reference tables.</p> Operation Example Category Join to calendar Get <code>date_id</code> from date dimension Enrichment Join to location Get <code>location_id</code> from location dimension Enrichment Join to reason codes Get <code>reason_id</code> from reason lookup Enrichment Join to product master Get <code>product_name</code> from product dim Enrichment <pre><code>-- Example: Enriching with dimension lookups\nSELECT\n    e.event_id,\n    e.event_date,\n    e.duration_minutes,\n    r.reason_id,           -- From reason code lookup\n    l.location_id          -- From location dimension\nFROM events e\nLEFT JOIN reason_codes r\n    ON r.category = e.category \nLEFT JOIN dim_location l\n    ON e.site_code = l.site_code\n</code></pre>"},{"location":"guides/medallion_architecture/#5-soft-delete-detection","title":"5. Soft Delete Detection","text":"<p>Tracking records that exist in Bronze but no longer exist in the source.</p> Operation Example Category Compare snapshots Find missing keys Delete Detection Flag deleted records <code>_is_deleted = true</code> Delete Detection Filter active records <code>WHERE _is_deleted = false</code> Delete Detection"},{"location":"guides/medallion_architecture/#what-does-not-happen-here_1","title":"What Does NOT Happen Here","text":"<p>\u274c Combining data from multiple source systems \u274c Business calculations (like KPIs, ratios) \u274c Aggregations for reporting \u274c Creating facts that span multiple sources</p>"},{"location":"guides/medallion_architecture/#example-silver-node","title":"Example Silver Node","text":"<pre><code>- name: cleaned_warehouse_events\n  inputs:\n    input_name: $bronze.warehouse_event_log\n  depends_on:\n    - cleaned_reason_codes     # Lookup table\n    - cleaned_dim_location     # Dimension table\n  transformer: deduplicate\n  params:\n    keys: [event_id]\n    order_by: \"_extracted_at DESC\"\n  transform:\n    steps:\n      - sql: |\n          SELECT\n              to_timestamp(event_date) AS event_date,\n              DATEDIFF(to_date(event_date), '2020-01-01') + 1 AS date_id,\n              'Warehouse A' AS location_name,\n              CASE WHEN machine = 'M1' THEN 'Machine 1' ... END AS machine_name,\n              duration_hours * 60 AS duration_minutes\n          FROM df\n      - function: detect_deletes\n        params:\n          mode: sql_compare\n          keys: [event_id]\n</code></pre>"},{"location":"guides/medallion_architecture/#the-golden-rule-of-silver","title":"The Golden Rule of Silver","text":"<p>\"One source in, one cleaned version out. The output should be the best possible version of that single source.\"</p>"},{"location":"guides/medallion_architecture/#layer-3-gold-business-ready","title":"Layer 3: Gold (Business-Ready)","text":""},{"location":"guides/medallion_architecture/#purpose_2","title":"Purpose","text":"<p>Combine cleaned Silver data into business-meaningful outputs.</p>"},{"location":"guides/medallion_architecture/#the-key-question_1","title":"The Key Question","text":"<p>\"Does this require data from MULTIPLE Silver sources?\"</p> <p>If YES \u2192 Gold \u2713 If NO \u2192 Probably Silver</p>"},{"location":"guides/medallion_architecture/#what-happens-here_2","title":"What Happens Here","text":""},{"location":"guides/medallion_architecture/#1-combining-multiple-sources-union","title":"1. Combining Multiple Sources (UNION)","text":"<p>Merging the same type of data from different systems.</p> Operation Example Category Union facts Combine events from System A + System B + System C Combining Reconciliation UNION (not UNION ALL) to dedupe across sources Combining Cross-system dedup Same event recorded in multiple systems Combining <pre><code>-- Example: Combining events from all sources\nSELECT date_id, location_id, duration_minutes, notes\nFROM cleaned_system_a_events\n\nUNION ALL\n\nSELECT date_id, location_id, duration_minutes, notes\nFROM cleaned_system_b_events\n\nUNION ALL\n\nSELECT date_id, location_id, duration_minutes, notes\nFROM cleaned_system_c_events\n</code></pre>"},{"location":"guides/medallion_architecture/#2-business-calculations","title":"2. Business Calculations","text":"<p>Applying business definitions and formulas.</p> Operation Example Category Define metrics <code>total_output = COALESCE(revised_qty, original_qty)</code> Business Rule Calculate KPIs <code>efficiency = actual_output / expected_output * 100</code> Business Rule Apply business logic \"If negative, treat as zero\" Business Rule Default values \"Use default reason if null and duration &lt; 10 min\" Business Rule <pre><code>-- Example: Business definition of Total Output\nCOALESCE(\n    CASE\n        WHEN COALESCE(revised_quantity, original_quantity) &lt;= 0 THEN 0\n        ELSE COALESCE(revised_quantity, original_quantity)\n    END, \n0) AS total_output\n</code></pre> <p>This is Gold because it answers: \"What does 'output' MEAN to the business?\"</p>"},{"location":"guides/medallion_architecture/#3-cross-fact-joins","title":"3. Cross-Fact Joins","text":"<p>Joining multiple fact tables together.</p> Operation Example Category Join facts Production + Downtime + Quality \u2192 Efficiency Cross-Fact Build wide tables Denormalized reporting tables Cross-Fact Calculate ratios Downtime / Available Hours Cross-Fact <pre><code>-- Example: Joining facts for efficiency calculation\nSELECT \n    c.date_id,\n    c.location_id,\n    p.total_output,\n    d.downtime_minutes,\n    q.defect_count,\n    -- Efficiency uses multiple facts\n    (p.total_output / p.target_output) * 100 AS efficiency_pct\nFROM calendar_scaffold c\nLEFT JOIN combined_production p \n    ON c.date_id = p.date_id AND c.location_id = p.location_id\nLEFT JOIN combined_downtime d \n    ON c.date_id = d.date_id AND c.location_id = d.location_id\nLEFT JOIN combined_quality q \n    ON c.date_id = q.date_id AND c.location_id = q.location_id\n</code></pre>"},{"location":"guides/medallion_architecture/#4-aggregations-for-reporting","title":"4. Aggregations for Reporting","text":"<p>Pre-computing summaries for dashboards and reports.</p> Operation Example Category Daily rollups SUM(production) GROUP BY date, location Aggregation Weekly summaries AVG(efficiency) by week Aggregation YTD calculations Running totals Aggregation"},{"location":"guides/medallion_architecture/#5-derived-dimensions","title":"5. Derived Dimensions","text":"<p>Creating dimensions that don't exist in source systems.</p> Operation Example Category Date spine Calendar \u00d7 Locations for all combinations Derived Dim Distinct lists All locations with any activity Derived Dim <pre><code>-- Example: Create all Date \u00d7 Location combinations\nSELECT *\nFROM dim_calendar\nCROSS JOIN distinct_locations\n</code></pre>"},{"location":"guides/medallion_architecture/#example-gold-node","title":"Example Gold Node","text":"<pre><code>- name: fact_daily_efficiency\n  description: \"Daily efficiency metrics by location\"\n  depends_on:\n    - combined_production   # Multiple sources unioned\n    - combined_downtime     # Multiple sources unioned\n    - combined_quality      # Multiple sources unioned\n    - calendar_scaffold     # Date \u00d7 Location scaffold\n  transform:\n    steps:\n      - sql: |\n          SELECT\n              c.date_id,\n              c.location_id,\n              COALESCE(p.total_output, 0) AS output_units,\n              COALESCE(d.downtime_minutes, 0) AS downtime_min,\n              COALESCE(q.defect_count, 0) AS defects,\n              -- Efficiency Calculation (Business Formula)\n              CASE \n                  WHEN p.target_output &gt; 0 \n                  THEN (p.total_output / p.target_output) * 100\n                  ELSE 0 \n              END AS efficiency_pct\n          FROM calendar_scaffold c\n          LEFT JOIN combined_production p \n              ON c.date_id = p.date_id AND c.location_id = p.location_id\n          LEFT JOIN combined_downtime d \n              ON c.date_id = d.date_id AND c.location_id = d.location_id\n          LEFT JOIN combined_quality q \n              ON c.date_id = q.date_id AND c.location_id = q.location_id\n  write:\n    connection: gold\n    table: fact_daily_efficiency\n    format: delta\n</code></pre>"},{"location":"guides/medallion_architecture/#the-golden-rule-of-gold","title":"The Golden Rule of Gold","text":"<p>\"This is what the business sees. Every row and column should have business meaning.\"</p>"},{"location":"guides/medallion_architecture/#quick-reference-where-does-this-belong","title":"Quick Reference: Where Does This Belong?","text":""},{"location":"guides/medallion_architecture/#by-operation-type","title":"By Operation Type","text":"Operation Bronze Silver Gold Raw ingestion \u2713 Add <code>_extracted_at</code> \u2713 Deduplication \u2713 Remove bad characters \u2713 Fix typos \u2713 Type casting \u2713 Unit conversion \u2713 Map codes to standard names \u2713 Join to dimension/lookup tables \u2713 Soft delete detection \u2713 UNION multiple sources \u2713 Business calculations \u2713 Cross-fact joins \u2713 Aggregations for reporting \u2713 KPI definitions \u2713"},{"location":"guides/medallion_architecture/#by-question","title":"By Question","text":"Question Layer \"How do I get data from the source?\" Bronze \"How do I fix this source's data quality issues?\" Silver \"How do I standardize this source to our schema?\" Silver \"How do I look up IDs from a dimension table?\" Silver \"How do I combine data from System A + B + C?\" Gold \"What does 'Total Output' mean to the business?\" Gold \"How do I calculate efficiency?\" Gold \"What should the dashboard show?\" Gold"},{"location":"guides/medallion_architecture/#the-one-source-test","title":"The One-Source Test","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                             \u2502\n\u2502   \"Could this node work with only ONE source system?\"       \u2502\n\u2502                                                             \u2502\n\u2502   YES \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba SILVER                \u2502\n\u2502    \u2502                                                        \u2502\n\u2502    \u2502   Examples:                                            \u2502\n\u2502    \u2502   \u2022 Cleaning System A data                             \u2502\n\u2502    \u2502   \u2022 Joining System B data to calendar dimension        \u2502\n\u2502    \u2502   \u2022 Mapping System C codes to standard categories      \u2502\n\u2502    \u2502                                                        \u2502\n\u2502   NO \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba GOLD                  \u2502\n\u2502    \u2502                                                        \u2502\n\u2502    \u2502   Examples:                                            \u2502\n\u2502    \u2502   \u2022 Combining all event sources                        \u2502\n\u2502    \u2502   \u2022 Calculating efficiency from production + downtime  \u2502\n\u2502    \u2502   \u2022 Creating unified fact tables                       \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/medallion_architecture/#common-mistakes","title":"Common Mistakes","text":""},{"location":"guides/medallion_architecture/#mistake-1-business-logic-in-silver","title":"\u274c Mistake 1: Business Logic in Silver","text":"<pre><code># WRONG - Business calculation in Silver\n- name: cleaned_production\n  transform:\n    steps:\n      - sql: |\n          SELECT \n              *,\n              (actual / target) * 100 AS efficiency  -- Business formula!\n          FROM df\n</code></pre> <p>Why it's wrong: Efficiency is a business definition. Silver should just clean the data.</p> <p>Fix: Move the efficiency calculation to Gold.</p>"},{"location":"guides/medallion_architecture/#mistake-2-raw-data-in-silver","title":"\u274c Mistake 2: Raw Data in Silver","text":"<pre><code># WRONG - No Bronze layer, reading directly from source\n- name: cleaned_orders\n  read:\n    connection: erp\n    table: dbo.Orders  # Reading directly from source!\n  transform:\n    steps:\n      - sql: SELECT * FROM df WHERE status != 'DELETED'\n</code></pre> <p>Why it's wrong: If the source changes, you lose the original data.</p> <p>Fix: Add a Bronze layer that preserves the raw data first.</p>"},{"location":"guides/medallion_architecture/#mistake-3-combining-sources-in-silver","title":"\u274c Mistake 3: Combining Sources in Silver","text":"<pre><code># WRONG - UNION in Silver\n- name: cleaned_all_events\n  depends_on:\n    - cleaned_system_a_events\n    - cleaned_system_b_events\n  transform:\n    steps:\n      - sql: |\n          SELECT * FROM cleaned_system_a_events\n          UNION ALL\n          SELECT * FROM cleaned_system_b_events  -- Combining sources!\n</code></pre> <p>Why it's wrong: Silver should process one source at a time.</p> <p>Fix: Move the UNION to a Gold layer node.</p>"},{"location":"guides/medallion_architecture/#project-structure-example","title":"Project Structure Example","text":"<pre><code>pipelines/\n\u251c\u2500\u2500 bronze/\n\u2502   \u2514\u2500\u2500 bronze.yaml\n\u2502       # Nodes: bronze_system_a, bronze_system_b, bronze_system_c\n\u2502\n\u251c\u2500\u2500 silver/\n\u2502   \u2514\u2500\u2500 silver.yaml\n\u2502       # Nodes: cleaned_system_a, cleaned_system_b, cleaned_system_c\n\u2502       # Each cleans ONE source\n\u2502\n\u2514\u2500\u2500 gold/\n    \u2514\u2500\u2500 gold.yaml\n        # Nodes: combined_events, combined_production, fact_daily_efficiency\n        # Combines Silver outputs, applies business logic\n</code></pre>"},{"location":"guides/medallion_architecture/#summary","title":"Summary","text":"Layer Input Output Key Activities Bronze Source systems Raw copy Ingest, add metadata Silver Bronze (one source) Cleaned version Clean, standardize, enrich with lookups Gold Silver (multiple sources) Business facts Combine, calculate, aggregate <p>Remember: - Bronze = \"Land it as-is\" - Silver = \"Clean this ONE source\" - Gold = \"Combine and calculate for business\"</p>"},{"location":"guides/performance_tuning/","title":"Performance Tuning Guide","text":"<p>Odibi v2.2 introduces a \"High-Performance Core\" designed to handle everything from local laptop development to petabyte-scale Spark jobs. This guide explains the optimizations available and how to use them.</p>"},{"location":"guides/performance_tuning/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>For 90% of users, just add this to your <code>odibi.yaml</code>:</p> <pre><code>performance:\n  use_arrow: true  # Massive speedup for Pandas I/O\n</code></pre>"},{"location":"guides/performance_tuning/#1-pandas-engine-optimizations","title":"1. Pandas Engine Optimizations","text":"<p>The Pandas engine is designed for speed on a single machine.</p>"},{"location":"guides/performance_tuning/#apache-arrow-backend-use_arrow-true","title":"\ud83c\udff9 Apache Arrow Backend (<code>use_arrow: true</code>)","text":"<p>What it does: Replaces standard NumPy memory layout with Apache Arrow. Arrow is a columnar memory format that allows \"Zero-Copy\" data transfer.</p> <p>Why use it? -   Speed: Reading Parquet files becomes nearly instant because the data maps directly from disk to memory without conversion overhead. -   Memory: Reduces RAM usage by ~50% for string-heavy datasets (no more Python objects for strings).</p> <p>Configuration:</p> <pre><code># odibi.yaml\nperformance:\n  use_arrow: true\n</code></pre>"},{"location":"guides/performance_tuning/#parallel-file-io-multi-threading","title":"\u26a1 Parallel File I/O (Multi-Threading)","text":"<p>What it does: When reading multiple files (e.g., <code>path: data/sales_*.csv</code>), Odibi now uses a thread pool to read them in parallel instead of one by one.</p> <p>Why use it? Pandas is normally single-threaded. If you have 8 CPU cores, reading 50 CSV files sequentially wastes 7 of them. Parallel I/O saturates your CPU/Disk bandwidth for linear speedups.</p> <p>How to use: Automatic! Just use a glob pattern in your path:</p> <pre><code>read:\n  path: raw/data_*.csv  # &lt;--- Parallel reading activates automatically\n</code></pre>"},{"location":"guides/performance_tuning/#2-spark-engine-optimizations","title":"2. Spark Engine Optimizations","text":"<p>The Spark engine focuses on \"Data Layout\" optimizations\u2014making sure downstream queries are fast.</p>"},{"location":"guides/performance_tuning/#liquid-clustering-cluster_by","title":"\ud83d\udca7 Liquid Clustering (<code>cluster_by</code>)","text":"<p>What it does: Replaces traditional Hive-style partitioning (<code>year=2023/month=01</code>) with a flexible, dynamic clustering system. It physically groups related data together in the files.</p> <p>Why use it? -   No \"Small File\" Problem: Traditional partitioning creates too many tiny files if you pick the wrong column (e.g., <code>user_id</code>). Liquid handles this automatically. -   Skew Resistance: Handles uneven data (e.g., 90% of users in US, 1% in JP) without performance cliffs. -   Query Speed: Massive data skipping. Queries filtering by clustered columns skip 99% of the file scans.</p> <p>How to use: Add <code>cluster_by</code> to your write node. If the table doesn't exist, Odibi creates it with clustering enabled.</p> <pre><code>- name: write_sales\n  write:\n    table: silver.sales\n    mode: append\n    options:\n      cluster_by: [region, date]  # &lt;--- Enables Liquid Clustering\n      optimize_write: true        # &lt;--- Keeps clustering healthy\n</code></pre>"},{"location":"guides/performance_tuning/#auto-optimization-optimize_write","title":"\ud83e\uddf9 Auto-Optimization (<code>optimize_write</code>)","text":"<p>What it does: Runs the Delta Lake <code>OPTIMIZE</code> command immediately after a write/merge operation.</p> <p>Why use it? Streaming and frequent batch jobs create \"small files\" (fragmentation) which kill read performance. This option compacts them into larger, efficient files (Bin-packing) and enforces clustering (Z-Order/Liquid).</p> <p>Configuration:</p> <pre><code># In a standard Write node\noptions:\n  optimize_write: true\n\n# In a Merge Transformer\nparams:\n  optimize_write: true\n</code></pre>"},{"location":"guides/performance_tuning/#streaming-support","title":"\ud83c\udf0a Streaming Support","text":"<p>What it does: Allows you to switch from Batch (<code>read</code>/<code>write</code>) to Streaming (<code>readStream</code>/<code>writeStream</code>) with a single flag.</p> <p>Why use it? For real-time latency or processing infinite datasets (Kafka, Auto-Loader) without managing state manually.</p> <p>How to use:</p> <pre><code>- name: read_stream\n  read:\n    streaming: true  # &lt;--- Activates Spark Structured Streaming\n    format: cloudFiles\n    path: raw_landing/\n</code></pre>"},{"location":"guides/performance_tuning/#3-polars-engine-optimizations","title":"3. Polars Engine Optimizations","text":"<p>The Polars engine is a lightweight, fast alternative to Pandas with native Rust performance.</p>"},{"location":"guides/performance_tuning/#lazy-execution","title":"\ud83e\uddba Lazy Execution","text":"<p>What it does: Polars uses a lazy execution model where queries are not executed until you call <code>.collect()</code>. This allows the query optimizer to reorder, combine, and skip operations.</p> <p>Why use it? -   Query Optimization: Predicate pushdown, projection pruning, and filter hoisting happen automatically. -   Memory Efficiency: Only columns you need are loaded into memory. -   Performance: Often 5-10x faster than Pandas for analytical workloads.</p> <p>How to use: Set your engine to Polars and Odibi handles lazy evaluation automatically:</p> <pre><code>engine: polars\n</code></pre>"},{"location":"guides/performance_tuning/#scan-methods-streaming-large-files","title":"\ud83d\udcc2 Scan Methods (Streaming Large Files)","text":"<p>What it does: Uses <code>scan_csv</code>, <code>scan_parquet</code>, and <code>scan_ndjson</code> to read files lazily without loading them entirely into memory.</p> <p>Why use it? Process files larger than RAM by streaming them in chunks.</p> <p>Automatic: Odibi's Polars engine uses scan methods by default for supported formats.</p>"},{"location":"guides/performance_tuning/#native-parallelism","title":"\u26a1 Native Parallelism","text":"<p>What it does: Polars uses all available CPU cores automatically\u2014no configuration needed.</p> <p>Why use it? Unlike Pandas (single-threaded), Polars parallelizes operations like groupby, join, and filter across all cores.</p>"},{"location":"guides/performance_tuning/#summary-cheat-sheet","title":"Summary Cheat Sheet","text":"Optimization Engine Use Case Impact <code>use_arrow: true</code> Pandas Local processing, large Parquet files High (Speed + Memory) Parallel I/O Pandas Reading split CSV/JSON files High (Linear I/O speedup) <code>cluster_by</code> Spark High-cardinality filters, skewed data High (Read performance) <code>optimize_write</code> Spark Frequent writes, streaming, \"small files\" High (Prevents degradation) <code>streaming: true</code> Spark Real-time ingestion Architectural Lazy Execution Polars Analytical workloads, large datasets High (Speed + Memory) Scan Methods Polars Files larger than RAM High (Streaming) Native Parallelism Polars Multi-core utilization High (Automatic)"},{"location":"guides/planning_walkthroughs/","title":"Data Engineering Planning Walkthroughs","text":"<p>\"30 minutes of planning saves 3 hours of debugging.\"</p> <p>Professional-grade Excel workbooks that guide you through planning data pipelines before writing code. By the time you finish filling them out, writing YAML is just typing.</p>"},{"location":"guides/planning_walkthroughs/#download-the-walkthroughs","title":"Download the Walkthroughs","text":"Workbook Sheets Download Bronze 7 sheets (B0-B5 + Reference) <code>Bronze_Walkthrough.xlsx</code> Silver 6 sheets (S0-S4 + Reference) <code>Silver_Walkthrough.xlsx</code> Gold 10 sheets (G0-G8 + Reference) <code>Gold_Walkthrough.xlsx</code> <p>Or regenerate them locally:</p> <pre><code>python scripts/create_de_walkthroughs.py\n</code></pre>"},{"location":"guides/planning_walkthroughs/#philosophy","title":"Philosophy","text":"<p>Plan first, code second.</p> Traditional Approach Walkthrough Approach Jump into code Answer hard questions upfront Hit edge cases Document decisions Backtrack Write code once Discover missing requirements Ship"},{"location":"guides/planning_walkthroughs/#the-three-layers","title":"The Three Layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GOLD    Business sees this. Dimensions, Facts, KPIs.       \u2502\n\u2502          Combines sources. Applies business logic.          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  SILVER  Clean &amp; standardize ONE source at a time.          \u2502\n\u2502          Deduplicate, cast types, map codes.                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  BRONZE  Land raw data as-is. No transforms.                 \u2502\n\u2502          Pure append. Add metadata. Duplicates expected.     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/planning_walkthroughs/#layer-rules-definitive","title":"Layer Rules (Definitive)","text":""},{"location":"guides/planning_walkthroughs/#bronze-layer-rules","title":"Bronze Layer Rules","text":"\u2705 ALLOWED \u274c NOT ALLOWED Land data exactly as-is from source Any data transformation Append mode only (accumulate history) Merge, upsert, or overwrite Add metadata columns (<code>_extracted_at</code>, <code>_batch_id</code>, <code>_source_file</code>) Filter or remove rows Schema evolution (allow new columns) Clean or standardize data Smart Read (rolling_window, stateful) for incremental Join any tables Route bad records to quarantine path Apply business logic Duplicates (expected - Silver handles them) Deduplicate <p>Bronze is your undo button. If something goes wrong downstream, you can always reprocess from Bronze.</p>"},{"location":"guides/planning_walkthroughs/#silver-layer-rules","title":"Silver Layer Rules","text":"\u2705 ALLOWED \u274c NOT ALLOWED Deduplicate (remove exact duplicates) Join multiple business source systems Clean text (trim, case, remove bad chars) UNION multiple source systems Cast data types Build dimensions with surrogate keys Standardize codes (M1 \u2192 Machine 1) SCD2 history tracking Join with reference/lookup tables Cross-source conformed dimensions Enrich via dimension lookups (code \u2192 name) Business KPIs or aggregations Validate and flag bad data Self-joins within the same source <p>The One-Source Test: \"Could this node run if only ONE business source system existed?\"</p> <ul> <li>Reference tables don't count as a \"source system\" - they're supporting data</li> <li>Joining <code>orders</code> with <code>product_codes</code> lookup table = \u2705 Silver</li> <li>Joining <code>sap_orders</code> with <code>salesforce_customers</code> = \u274c Gold</li> </ul>"},{"location":"guides/planning_walkthroughs/#gold-layer-rules","title":"Gold Layer Rules","text":"\u2705 ALLOWED \u274c NOT ALLOWED Dimensions with surrogate keys Silver-level cleaning (data should arrive clean) SCD2 history tracking SCD2 on fact tables DateDimension generation Undefined grain on facts Fact tables with dimension lookups SCD2 without prior deduplication UNION multiple source systems JOIN across source systems Business KPIs and calculated metrics Aggregations (daily, monthly, etc.) Semantic layer metrics <p>The Multi-Source Test: \"Does this require MULTIPLE source systems OR business modeling?\"</p> <ul> <li>If combining SAP + Salesforce + Excel \u2192 Gold</li> <li>If building a dimension with surrogate keys \u2192 Gold</li> <li>If creating business KPIs \u2192 Gold</li> </ul>"},{"location":"guides/planning_walkthroughs/#who-fills-what","title":"Who Fills What","text":""},{"location":"guides/planning_walkthroughs/#business-stakeholders","title":"Business Stakeholders","text":"Layer Sheet What to Fill Bronze B1_Source_Inventory What data exists, who owns it, what it's for Gold G1_Business_Questions What questions need answers (plain English) Gold G8_Semantic_Metrics How to calculate KPIs (plain English) <p>!!! tip \"No SQL Required\"     Describe things like you're explaining to a new employee.</p>"},{"location":"guides/planning_walkthroughs/#data-engineers","title":"Data Engineers","text":"Layer Sheets What to Fill Bronze B2-B5 How to land data technically Silver S2-S4 How to clean and standardize Gold G2-G7 How to model dimensions, facts, relationships"},{"location":"guides/planning_walkthroughs/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"guides/planning_walkthroughs/#step-1-bronze-layer","title":"Step 1: Bronze Layer","text":"<p>Start here. You can't clean data you haven't landed.</p> <pre><code>Bronze_Walkthrough.xlsx\n\u251c\u2500\u2500 B0_Overview        \u2192 Read first. Understand Bronze principles.\n\u251c\u2500\u2500 B1_Source_Inventory \u2192 List ALL source systems\n\u251c\u2500\u2500 B2_Node_Design     \u2192 One row per Bronze node\n\u251c\u2500\u2500 B3_Column_Mapping  \u2192 Document what columns land\n\u251c\u2500\u2500 B4_Validation      \u2192 Plan quality checks\n\u251c\u2500\u2500 B5_Migration       \u2192 For legacy ETL migrations only\n\u2514\u2500\u2500 Reference          \u2192 Best practices &amp; anti-patterns\n</code></pre> <p>The Bronze Rule: Pure append, no transforms, no cleaning. Add metadata (<code>_extracted_at</code>). Duplicates are expected.</p> <p>Odibi Patterns: <code>append</code>, <code>rolling_window</code>, <code>stateful</code></p>"},{"location":"guides/planning_walkthroughs/#step-2-silver-layer","title":"Step 2: Silver Layer","text":"<p>Only start when Bronze is landing correctly.</p> <pre><code>Silver_Walkthrough.xlsx\n\u251c\u2500\u2500 S0_Overview        \u2192 Read the \"One-Source Test\"\n\u251c\u2500\u2500 S1_Domain_Overview \u2192 Group nodes by business domain\n\u251c\u2500\u2500 S2_Node_Design     \u2192 One row per Silver node\n\u251c\u2500\u2500 S3_Column_Mapping  \u2192 Source \u2192 Target with transformations\n\u251c\u2500\u2500 S4_Validation      \u2192 Ensure data is trustworthy\n\u2514\u2500\u2500 Reference          \u2192 What belongs (and doesn't)\n</code></pre> <p>The One-Source Test: \"Could this node run if only ONE source system existed?\"</p> <ul> <li>YES \u2192 Silver \u2713</li> <li>NO \u2192 Probably Gold</li> </ul> <p>!!! note \"Reference Tables Are Allowed\"     The One-Source Test refers to business source systems, not reference data.</p> <pre><code>**Silver CAN join with:**\n\n- Reference/lookup tables (code mappings, static lists)\n- Dimension lookups for enrichment (product_code \u2192 product_name)\n- Self-joins within the same source\n\n**Silver should NOT join:**\n\n- Multiple business source systems (SAP + Salesforce \u2192 Gold)\n- Cross-source conformed dimensions \u2192 Gold\n</code></pre> <p>Odibi Patterns: <code>deduplicate</code>, <code>merge</code>, <code>clean_text</code>, <code>validate_and_flag</code></p>"},{"location":"guides/planning_walkthroughs/#step-3-gold-layer","title":"Step 3: Gold Layer","text":"<p>Only start when Silver data is clean and deduplicated.</p> <pre><code>Gold_Walkthrough.xlsx\n\u251c\u2500\u2500 G0_Overview        \u2192 Read the \"Multi-Source Test\"\n\u251c\u2500\u2500 G1_Business_Questions \u2192 START HERE - what does business need?\n\u251c\u2500\u2500 G2_Dimension_Design \u2192 Plan dimensions with SCD strategy\n\u251c\u2500\u2500 G3_Fact_Planning   \u2192 Design fact tables (grain is critical)\n\u251c\u2500\u2500 G4_Node_Design     \u2192 One row per Gold node\n\u251c\u2500\u2500 G5_Column_Mapping  \u2192 SQL transformation spec\n\u251c\u2500\u2500 G6_Validation      \u2192 Reconciliation checks\n\u251c\u2500\u2500 G7_FK_Relationships \u2192 Foreign key definitions\n\u251c\u2500\u2500 G8_Semantic_Metrics \u2192 Business metrics for self-service\n\u2514\u2500\u2500 Reference          \u2192 Best practices &amp; anti-patterns\n</code></pre> <p>!!! warning \"Grain is Critical\"     Every fact table needs a grain statement: \"One row = one ___ per ___.\"     If you can't say it, you haven't defined the grain.</p> <p>Odibi Patterns: <code>Dimension</code>, <code>SCD2</code>, <code>DateDimension</code>, <code>Fact</code>, <code>Aggregation</code>, <code>union</code>, <code>join</code></p>"},{"location":"guides/planning_walkthroughs/#walkthrough-yaml-translation","title":"Walkthrough \u2192 YAML Translation","text":"<p>Once your walkthrough is complete, translation is mechanical:</p> Walkthrough Field YAML Location <code>Node_ID</code> / <code>Node_Name</code> <code>nodes.name</code> <code>Read_Connection</code> <code>read.connection</code> <code>Read_Format</code> <code>read.format</code> <code>Incremental_Mode</code> <code>incremental.mode</code> <code>Incremental_Column</code> <code>incremental.column</code> <code>Dedup_Keys</code> <code>params.keys</code> <code>Write_Mode</code> <code>write.mode</code> <code>Contract_Type</code> <code>validation.contracts[].type</code> <code>On_Failure</code> <code>validation.contracts[].on_failure</code> <code>Is_Quality_Gate</code> <code>validation.contracts[].is_quality_gate</code>"},{"location":"guides/planning_walkthroughs/#validation-fields","title":"Validation Fields","text":"<p>Every validation sheet uses these fields:</p> Field Purpose Values <code>Contract_Type</code> What check to run <code>not_null</code>, <code>unique</code>, <code>row_count</code>, <code>freshness</code>, <code>range</code>, <code>regex_match</code>, <code>accepted_values</code>, <code>custom_sql</code> <code>On_Failure</code> What happens when it fails <code>error</code> (stop), <code>warn</code> (log), <code>filter</code> (remove rows), <code>quarantine</code> (save bad rows) <code>Is_Quality_Gate</code> Does this block the pipeline? <code>Yes</code> / <code>No</code>"},{"location":"guides/planning_walkthroughs/#common-mistakes","title":"Common Mistakes","text":""},{"location":"guides/planning_walkthroughs/#bronze","title":"Bronze","text":"Mistake Why It's Wrong Transforming or cleaning data That's Silver's job Filtering out \"bad\" rows Land everything, filter later Forgetting <code>_extracted_at</code> You lose audit trail Using merge/upsert mode Always append in Bronze"},{"location":"guides/planning_walkthroughs/#silver","title":"Silver","text":"Mistake Why It's Wrong Joining multiple sources That's Gold Building dimensions with SKs That's Gold Adding business logic Keep it to standardization only"},{"location":"guides/planning_walkthroughs/#gold","title":"Gold","text":"Mistake Why It's Wrong Doing Silver-level cleaning Data should arrive clean Undefined grain on facts You'll get duplicates or gaps SCD2 without prior dedup History will be wrong Building without business questions You'll build the wrong thing"},{"location":"guides/planning_walkthroughs/#quick-start-checklist","title":"Quick Start Checklist","text":"<ul> <li>[ ] Download or regenerate the workbooks</li> <li>[ ] Open <code>Bronze_Walkthrough.xlsx</code>, read B0_Overview</li> <li>[ ] Complete the \"BEFORE YOU START\" checklist</li> <li>[ ] Fill B1 with business stakeholder help</li> <li>[ ] Fill B2 (one row per source to land)</li> <li>[ ] Fill B3 (column-level documentation)</li> <li>[ ] Fill B4 (validation contracts)</li> <li>[ ] Generate YAML from B2</li> <li>[ ] Build, test, deploy Bronze</li> <li>[ ] Repeat for Silver, then Gold</li> </ul>"},{"location":"guides/planning_walkthroughs/#example-bronze-node","title":"Example: Bronze Node","text":"<p>Scenario: Land Production Orders from SAP ERP</p>"},{"location":"guides/planning_walkthroughs/#filled-walkthrough-b2_node_design","title":"Filled Walkthrough (B2_Node_Design)","text":"Field Value Node_ID BRZ_001 Node_Name bronze_production_orders Read_Connection sap_erp_prod Read_Format sql Read_Table_or_Path SAPPR1.dbo.AFKO Incremental_Mode rolling_window Incremental_Column AEDAT Lookback 3 Lookback_Unit day Write_Path bronze/production/orders"},{"location":"guides/planning_walkthroughs/#generated-yaml","title":"Generated YAML","text":"<pre><code>pipelines:\n  - pipeline: bronze_production\n    nodes:\n      - name: bronze_production_orders\n        read:\n          connection: sap_erp_prod\n          format: sql\n          table: SAPPR1.dbo.AFKO\n        incremental:\n          mode: rolling_window\n          column: AEDAT\n          lookback: 3\n          lookback_unit: day\n        write:\n          connection: delta_lake\n          path: bronze/production/orders\n          mode: append\n</code></pre> <p>The walkthrough made this trivial.</p>"},{"location":"guides/planning_walkthroughs/#see-also","title":"See Also","text":"<ul> <li>Medallion Architecture - Layer philosophy</li> <li>YAML Schema Reference - Configuration details</li> <li>Validation Contracts - Quality check options</li> <li>Best Practices - General guidance</li> </ul>"},{"location":"guides/production_deployment/","title":"\ud83c\udfed Production Deployment","text":"<p>Moving from your laptop to production (e.g., Databricks, Azure Data Factory, Airflow) requires handling secrets, environments, and logging differently.</p>"},{"location":"guides/production_deployment/#1-secrets-management","title":"1. Secrets Management","text":"<p>NEVER commit passwords to Git.</p> <p>Odibi supports environment variable substitution in <code>odibi.yaml</code>. Use the <code>${VAR_NAME}</code> syntax.</p> <p>Bad:</p> <pre><code>connections:\n  db:\n    password: \"super_secret_password\"  # \u274c Security Risk\n</code></pre> <p>Good:</p> <pre><code>connections:\n  db:\n    password: \"${DB_PASSWORD}\"         # \u2705 Safe\n</code></pre> <p>Then, set the environment variable <code>DB_PASSWORD</code> in your production environment (or <code>.env</code> file locally).</p>"},{"location":"guides/production_deployment/#automatic-redaction","title":"Automatic Redaction","text":"<p>Odibi automatically detects values that look like secrets (keys, tokens, passwords) and replaces them with <code>[REDACTED]</code> in logs and Data Stories.</p>"},{"location":"guides/production_deployment/#2-data-privacy-pii","title":"2. Data Privacy &amp; PII","text":"<p>When processing personal data (GDPR/HIPAA), you must ensure that sensitive data does not leak into your logs or execution reports.</p>"},{"location":"guides/production_deployment/#column-level-redaction","title":"Column-Level Redaction","text":"<p>If you want to see non-sensitive data in your reports but hide PII (Personally Identifiable Information), specify the columns list.</p> <pre><code>nodes:\n  - name: ingest_users\n    read: ...\n    # Only masks these columns in the HTML report\n    sensitive: [\"email\", \"ssn\", \"phone\", \"credit_card\"]\n</code></pre>"},{"location":"guides/production_deployment/#full-node-redaction","title":"Full Node Redaction","text":"<p>For highly sensitive nodes (e.g., medical records, financial transactions), you can mask the entire sample.</p> <pre><code>nodes:\n  - name: process_health_records\n    transform: ...\n    # Replaces entire sample with \"[REDACTED: Sensitive Data]\"\n    sensitive: true\n</code></pre> <p>Note: This only affects the Data Story (logs/html). The actual data moving through the pipeline is not modified.</p>"},{"location":"guides/production_deployment/#3-azure-integration","title":"3. Azure Integration","text":"<p>Odibi has native support for Azure resources.</p>"},{"location":"guides/production_deployment/#authentication","title":"Authentication","text":"<p>We support DefaultAzureCredential. This means you don't need to manage keys manually. 1.  Local: It uses your Azure CLI login (<code>az login</code>). 2.  Production: It uses the Managed Identity of the VM/Pod.</p> <pre><code>connections:\n  data_lake:\n    type: azure_adls\n    account: mydatalake\n    auth_mode: key_vault  # Fetches keys from Key Vault automatically\n    key_vault: my-key-vault-name\n</code></pre>"},{"location":"guides/production_deployment/#3-running-on-databricks","title":"3. Running on Databricks","text":"<p>Odibi runs natively on Databricks clusters.</p> <ol> <li>Install: Add <code>odibi[spark,azure]</code> to your cluster libraries.</li> <li>Deploy: Copy your project folder (YAML + SQL) to DBFS or git checkout.</li> <li>Job: Create a job that runs:     <code>bash     odibi run odibi.yaml</code></li> </ol> <p>Tip: Use the \"Spark\" engine for clusters or \"Polars\" engine for high-performance single-node tasks.</p> <pre><code>project: My Big Data Project\nengine: spark  # Options: pandas, polars, spark\n</code></pre>"},{"location":"guides/production_deployment/#4-system-catalog-unified-state","title":"4. System Catalog (Unified State)","text":"<p>Odibi uses a System Catalog (Delta Tables) to track execution history, high-water marks, and metadata. This unifies state management for both local and distributed environments.</p>"},{"location":"guides/production_deployment/#1-local-development-default","title":"1. Local Development (Default)","text":"<p>When running locally, the catalog is automatically created in a hidden directory (<code>.odibi/system/</code>). This uses the <code>deltalake</code> library (Rust core) for high-performance ACID transactions without needing Spark.</p>"},{"location":"guides/production_deployment/#2-production-distributed","title":"2. Production (Distributed)","text":"<p>In production (e.g., Databricks, Kubernetes), you should configure the System Catalog to store state in your Data Lake (ADLS/S3). This allows multiple concurrent pipelines to share state safely.</p> <pre><code>system:\n  connection: \"adls_bronze\"  # Points to your data lake connection\n  path: \"_odibi_system\"      # Directory for system tables\n</code></pre> <p>If utilizing Spark, Odibi leverages Delta Lake's optimistic concurrency control automatically.</p>"},{"location":"guides/production_deployment/#5-monitoring-observability","title":"5. Monitoring &amp; Observability","text":""},{"location":"guides/production_deployment/#openlineage-integration","title":"OpenLineage Integration","text":"<p>Odibi emits standard OpenLineage events. To integrate with DataHub, Marquez, or Atlan:</p> <pre><code>lineage:\n  url: \"http://marquez-api:5000\"\n  namespace: \"odibi-production\"\n</code></pre>"},{"location":"guides/production_deployment/#logging","title":"Logging","text":"<p>Odibi logs structured JSON to stdout by default in production. This is easily ingested by Datadog, Splunk, or Azure Monitor.</p> <pre><code># Force JSON logging\nexport ODIBI_LOG_FORMAT=json\nodibi run odibi.yaml\n</code></pre>"},{"location":"guides/production_deployment/#data-stories-as-artifacts","title":"Data Stories as Artifacts","text":"<p>Configure Odibi to save Data Stories to a permanent location (like an S3 bucket or ADLS container) so you have a permanent audit trail.</p> <pre><code>story:\n  connection: data_lake  # Save reports to the cloud\n  path: audit_reports/\n</code></pre>"},{"location":"guides/python_api_guide/","title":"\ud83d\udc0d Odibi Python API: Zero to Hero","text":"<p>Ultimate Cheatsheet &amp; Reference (v2.4.0)</p> <p>Welcome to the Python API guide. While the CLI is great for running pipelines, the Python API allows you to automate, test, and extend Odibi deeply into your infrastructure.</p>"},{"location":"guides/python_api_guide/#level-1-the-basics-running-pipelines","title":"\ud83d\udfe2 Level 1: The Basics (Running Pipelines)","text":"<p>The core entry point is the <code>PipelineManager</code>. It reads your YAML configuration and manages execution.</p>"},{"location":"guides/python_api_guide/#1-load-and-run","title":"1. Load and Run","text":"<pre><code>from odibi.pipeline import PipelineManager\n\n# 1. Load your project configuration\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\n\n# 2. Run EVERYTHING (All pipelines defined in yaml)\nresults = manager.run()\n\n# 3. Check if it worked\nif results['main_pipeline'].failed:\n    print(\"\u274c Pipeline Failed!\")\nelse:\n    print(\"\u2705 Success!\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-run-a-specific-pipeline","title":"2. Run a Specific Pipeline","text":"<p>If your YAML has multiple pipelines (e.g., <code>ingest</code>, <code>transform</code>, <code>export</code>), run just one:</p> <pre><code># Returns a single PipelineResults object instead of a dict\nresult = manager.run(\"ingest\")\n\nprint(f\"Duration: {result.duration:.2f}s\")\n</code></pre>"},{"location":"guides/python_api_guide/#3-dry-run-simulation","title":"3. Dry Run (Simulation)","text":"<p>Check logic without moving data:</p> <pre><code>manager.run(\"ingest\", dry_run=True)\n</code></pre>"},{"location":"guides/python_api_guide/#level-2-intermediate-inspection-automation","title":"\ud83d\udfe1 Level 2: Intermediate (Inspection &amp; Automation)","text":"<p>Once you have <code>PipelineResults</code>, you can inspect exactly what happened.</p>"},{"location":"guides/python_api_guide/#1-inspect-node-results","title":"1. Inspect Node Results","text":"<pre><code>result = manager.run(\"ingest\")\n\nfor node_name, node_result in result.node_results.items():\n    status = \"\u2705\" if node_result.success else \"\u274c\"\n    print(f\"{status} {node_name}: {node_result.duration:.2f}s\")\n\n    # See metadata (row counts, schema output, etc.)\n    if node_result.metadata:\n        print(f\"   Rows: {node_result.metadata.get('rows_out', 0)}\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-resume-from-failure","title":"2. Resume from Failure","text":"<p>If a pipeline fails at step 5 of 10, you don't want to re-run steps 1-4.</p> <pre><code># Automatically skips successfully completed nodes from the last run\nmanager.run(\"ingest\", resume_from_failure=True)\n</code></pre>"},{"location":"guides/python_api_guide/#level-3-hero-advanced-usage","title":"\ud83d\udd34 Level 3: Hero (Advanced Usage)","text":"<p>This is where Odibi shines. You can unit test individual logic units without running the full pipeline.</p>"},{"location":"guides/python_api_guide/#1-unit-testing-nodes","title":"1. Unit Testing Nodes","text":"<p>You don't need to run the whole pipeline to test one complex SQL transformation.</p> <pre><code>from odibi.pipeline import PipelineManager\nimport pandas as pd\n\n# 1. Setup Manager\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\npipeline = manager.get_pipeline(\"main_etl\")\n\n# 2. Mock Input Data (Inject test data into context)\nmock_data = {\n    \"read_customers\": pd.DataFrame([\n        {\"id\": 1, \"email\": \"BAD_EMAIL\"},\n        {\"id\": 2, \"email\": \"good@test.com\"}\n    ])\n}\n\n# 3. Run ONE specific node with mocked input\nresult = pipeline.run_node(\"clean_customers\", mock_data=mock_data)\n\n# 4. Assertions\noutput_df = pipeline.context.get(\"clean_customers\")\nassert len(output_df) == 1  # Should have filtered bad email\nassert output_df.iloc[0]['email'] == \"good@test.com\"\nprint(\"\u2705 Unit Test Passed\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-accessing-story-data","title":"2. Accessing Story Data","text":"<p>Want to send the pipeline report to Slack or Email programmatically?</p> <pre><code>result = manager.run(\"ingest\")\n\nif result.story_path:\n    print(f\"HTML Report generated at: {result.story_path}\")\n\n    # Read the HTML content\n    with open(result.story_path, \"r\") as f:\n        html_content = f.read()\n\n    # send_email(to=\"team@company.com\", subject=\"Pipeline Report\", body=html_content)\n</code></pre>"},{"location":"guides/python_api_guide/#3-deep-diff-pipeline-runs","title":"3. Deep Diff (Pipeline Runs)","text":"<p>Programmatically detect changes between two pipeline runs (schema drift, row count changes, logic changes).</p> <pre><code>from odibi.diagnostics.diff import diff_runs\nfrom odibi.story.metadata import PipelineStoryMetadata\n\n# Load run metadata (generated by odibi run in odibi_stories/metadata/)\n# Note: You need to know the paths to the JSON files\nrun_a = PipelineStoryMetadata.from_json(\"odibi_stories/metadata/run_20231027_120000.json\")\nrun_b = PipelineStoryMetadata.from_json(\"odibi_stories/metadata/run_20231027_120500.json\")\n\n# Calculate differences\ndiff = diff_runs(run_a, run_b)\n\n# Inspect Results\nif diff.nodes_added:\n    print(f\"New Nodes: {diff.nodes_added}\")\n\nfor node_name, node_diff in diff.node_diffs.items():\n    if node_diff.has_drift:\n        print(f\"\u26a0\ufe0f DRIFT in {node_name}:\")\n        if node_diff.schema_change:\n            print(f\"   - Schema changed! Added: {node_diff.columns_added}\")\n        if node_diff.sql_changed:\n            print(f\"   - SQL Logic changed\")\n        if node_diff.rows_diff != 0:\n            print(f\"   - Row count changed by {node_diff.rows_diff}\")\n</code></pre>"},{"location":"guides/python_api_guide/#4-deep-diff-delta-lake","title":"4. Deep Diff (Delta Lake)","text":"<p>Directly compare two versions of a Delta table to see what changed (rows added, removed, updated).</p> <pre><code>from odibi.diagnostics.delta import get_delta_diff\n\ntable_path = \"data/delta_tables/silver/customers\"\n\n# Compare version 1 vs version 2\ndiff = get_delta_diff(\n    table_path=table_path,\n    version_a=1,\n    version_b=2,\n    deep=True,          # Perform row-by-row comparison\n    keys=[\"id\"]         # Primary key for detecting updates\n)\n\nprint(f\"Rows Added: {diff.rows_added}\")\nprint(f\"Rows Removed: {diff.rows_removed}\")\nprint(f\"Rows Updated: {diff.rows_updated}\")\n\nif diff.sample_updated:\n    print(\"Sample Updates:\", diff.sample_updated[0])\n</code></pre>"},{"location":"guides/python_api_guide/#reference-custom-transformations","title":"\ud83d\udcda Reference: Custom Transformations","text":"<p>To extend the Python API with your own functions, see the Writing Custom Transformations guide.</p> <p>Quick Snippet:</p> <pre><code>from odibi import transform\n\n@transform\ndef my_custom_logic(context, current, threshold=100):\n    return current[current['value'] &gt; threshold]\n</code></pre>"},{"location":"guides/recipes/","title":"Odibi Cookbook: Recipes for Common Patterns","text":"<p>This guide provides copy-pasteable solutions for real-world Data Engineering problems.</p>"},{"location":"guides/recipes/#recipe-1-the-unstable-api-ingestion","title":"Recipe 1: The \"Unstable API\" Ingestion \ud83c\udf2a\ufe0f","text":"<p>Problem: \"My source JSON adds new fields constantly and is deeply nested. My pipeline breaks whenever the schema changes.\"</p> <p>Solution: Use <code>schema_policy: { mode: \"evolve\" }</code> to automatically adapt to new columns, and <code>normalize_json</code> to flatten the structure.</p> <pre><code>- name: \"ingest_unstable_api\"\n  read:\n    connection: \"api_source\"\n    format: \"json\"\n    path: \"events/v1/*.json\"\n\n  # 1. Handle Drift: Automatically add new columns as NULLable\n  schema_policy:\n    mode: \"evolve\"\n    on_new_columns: \"add_nullable\"\n\n  # 2. Flatten: Convert nested JSON into columns (e.g. payload.id -&gt; payload_id)\n  transformer: \"normalize_json\"\n  params:\n    column: \"payload\"\n    sep: \"_\"\n\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"events_flat\"\n</code></pre>"},{"location":"guides/recipes/#recipe-2-the-privacy-first-customer-table","title":"Recipe 2: The \"Privacy-First\" Customer Table \ud83d\udd12","text":"<p>Problem: \"I need to ingest customer data but Hash emails and Mask credit card numbers for compliance (GDPR/CCPA).\"</p> <p>Solution: Use the <code>privacy</code> block for global anonymization and <code>sensitive</code> columns for masking in stories. You can also mix methods using <code>hash_columns</code> transformer.</p> <pre><code>- name: \"load_secure_customers\"\n  read:\n    connection: \"s3_raw\"\n    format: \"parquet\"\n    path: \"customers/\"\n\n  # 1. Global Privacy Policy (Applies to PII columns)\n  privacy:\n    method: \"hash\"\n    salt: \"${PRIVACY_SALT}\"  # Load from env var\n\n  # 2. Mark Columns as PII (Triggers Privacy Policy)\n  columns:\n    email:\n      pii: true\n    phone:\n      pii: true\n\n  # 3. Explicit Masking for Credit Cards (Transformers run before Write)\n  transform:\n    steps:\n      # Mask CCNs (keep last 4)\n      - function: \"regex_replace\"\n        params:\n          column: \"credit_card\"\n          pattern: \".(?=.{4})\"  # Regex to match all except last 4\n          replacement: \"*\"\n\n  # 4. Hide from Stories (Documentation)\n  sensitive: [\"email\", \"credit_card\", \"phone\"]\n\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"dim_customers_anonymized\"\n</code></pre>"},{"location":"guides/recipes/#recipe-3-sessionizing-clickstream-data","title":"Recipe 3: Sessionizing Clickstream Data \u23f1\ufe0f","text":"<p>Problem: \"I have raw events. I need to group them into User Sessions (30-minute timeout) and load them incrementally.\"</p> <p>Solution: Combine the <code>sessionize</code> transformer with <code>incremental: { mode: \"stateful\" }</code> to process only new data while maintaining session logic.</p> <pre><code>- name: \"clickstream_sessions\"\n  read:\n    connection: \"kafka_landing\"\n    format: \"json\"\n    path: \"clicks/\"\n\n    # 1. Incremental Loading (Stateful)\n    # Tracks the last processed timestamp to only read new events\n    incremental:\n      mode: \"stateful\"\n      state_key: \"clickstream_hwm\"\n      watermark_lag: \"1h\"  # Handle late arriving data\n\n  # 2. Session Logic (30 min timeout)\n  transformer: \"sessionize\"\n  params:\n    timestamp_col: \"event_time\"\n    user_col: \"user_id\"\n    threshold_seconds: 1800  # 30 minutes\n    session_col: \"session_id\"\n\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sessions\"\n    mode: \"append\"\n</code></pre>"},{"location":"guides/secrets/","title":"Managing Secrets","text":"<p>Odibi provides a unified way to handle secrets (API keys, database passwords, storage tokens) across local development and production environments. It supports <code>.env</code> files for local use and native Azure Key Vault integration for production.</p>"},{"location":"guides/secrets/#1-variable-substitution","title":"1. Variable Substitution","text":"<p>You can reference environment variables in your <code>project.yaml</code> using the <code>${VAR_NAME}</code> syntax.</p> <pre><code>connections:\n  my_database:\n    type: azure_sql\n    host: ${DB_HOST}\n    auth:\n      username: ${DB_USER}\n      password: ${DB_PASS}\n</code></pre>"},{"location":"guides/secrets/#2-local-development-env","title":"2. Local Development (<code>.env</code>)","text":"<p>For local development, store your secrets in a <code>.env</code> file in your project root. Odibi automatically loads this file.</p> <p>Note: Always add <code>.env</code> to your <code>.gitignore</code> to prevent committing secrets.</p>"},{"location":"guides/secrets/#cli-commands","title":"CLI Commands","text":"<p>Initialize a template: Generate a <code>.env.template</code> file based on the variables used in your config.</p> <pre><code>odibi secrets init project.yaml\n</code></pre> <p>Validate your environment: Check if all required variables are set in your current environment.</p> <pre><code>odibi secrets validate project.yaml\n</code></pre>"},{"location":"guides/secrets/#3-production-azure-key-vault","title":"3. Production (Azure Key Vault)","text":"<p>In production (e.g., Databricks, Azure Functions), relying on environment variables for everything can be insecure. Odibi supports fetching secrets directly from Azure Key Vault.</p>"},{"location":"guides/secrets/#configuration","title":"Configuration","text":"<p>To use Key Vault, specify <code>key_vault_name</code> and <code>secret_name</code> in your connection config. Odibi will automatically fetch the secret securely using <code>DefaultAzureCredential</code> (Managed Identity / Service Principal).</p> <pre><code>connections:\n  adls_prod:\n    type: azure_adls\n    account: myprodstorage\n    container: data\n    # Instead of a hardcoded key or env var:\n    key_vault_name: \"my-key-vault\"\n    secret_name: \"adls-prod-key\"\n</code></pre>"},{"location":"guides/secrets/#how-it-works","title":"How it Works","text":"<ol> <li>Auth Detection: If <code>key_vault_name</code> is present, Odibi attempts to authenticate with Azure using the environment's identity (e.g., the Databricks cluster's Managed Identity).</li> <li>Parallel Fetching: If multiple connections use Key Vault, Odibi fetches them in parallel during startup to minimize latency.</li> <li>Caching: Secrets are cached in memory for the duration of the run.</li> </ol>"},{"location":"guides/secrets/#best-practices","title":"Best Practices","text":"<ol> <li>Never Commit Secrets: Do not put actual passwords in <code>project.yaml</code>. Use <code>${VAR}</code> placeholders.</li> <li>Use <code>.env.template</code>: Commit a template file with empty values so other developers know which variables they need to set.</li> <li>Use Key Vault in Prod: Avoid setting sensitive environment variables in cloud compute configurations if possible. Use Key Vault integration for rotation and auditing.</li> <li>Redaction: Odibi automatically attempts to redact known secret values from logs and generated stories.</li> </ol>"},{"location":"guides/setup_azure/","title":"Azure Integration Setup Guide (v2.1.0)","text":"<p>This guide covers authenticating and connecting Odibi to Azure services (ADLS Gen2, Azure SQL, Key Vault) using the latest v2.1.0 standards.</p> <p>Key Features in v2.1.0: - Auto-Auth: Zero-config authentication using Managed Identity or Environment Variables (<code>DefaultAzureCredential</code>). - Universal Key Vault: Retrieve ANY secret (Account Key, SAS Token, SQL Password) from Key Vault by referencing it in the config.</p>"},{"location":"guides/setup_azure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription</li> <li>Azure CLI installed: Install Azure CLI</li> <li>Odibi installed with Azure extras: <code>pip install \"odibi[azure]\"</code></li> </ul>"},{"location":"guides/setup_azure/#1-azure-data-lake-storage-gen2-adls","title":"1. Azure Data Lake Storage Gen2 (ADLS)","text":"<p>Use the <code>azure_blob</code> connection type.</p>"},{"location":"guides/setup_azure/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/setup_azure/#option-a-auto-auth-recommended","title":"Option A: Auto-Auth (Recommended)","text":"<p>Best for: Production (Managed Identity) or Local Dev (Azure CLI login). How it works: Odibi uses <code>DefaultAzureCredential</code> to find your identity automatically. No secrets in the config!</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    # No 'auth' section needed!\n    # Odibi will automatically try Managed Identity, CLI, or Env Vars.\n</code></pre> <p>Setup: 1. Azure: Grant your identity (User or Managed Identity) the Storage Blob Data Contributor role on the storage account. 2. Local: Run <code>az login</code>. 3. Production: Assign Managed Identity to the VM/Function/Databricks cluster.</p>"},{"location":"guides/setup_azure/#option-b-universal-key-vault-account-key","title":"Option B: Universal Key Vault (Account Key)","text":"<p>Best for: Scenarios where Managed Identity is not possible. How it works: Store the Account Key in Key Vault, and tell Odibi where to find it.</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    auth:\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"adls-account-key\"  # The secret containing the Account Key\n</code></pre>"},{"location":"guides/setup_azure/#option-c-universal-key-vault-sas-token","title":"Option C: Universal Key Vault (SAS Token)","text":"<p>Best for: Restricted access with SAS tokens.</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    auth:\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"adls-sas-token\"  # The secret containing the SAS Token\n</code></pre>"},{"location":"guides/setup_azure/#2-azure-sql-database","title":"2. Azure SQL Database","text":"<p>Use the <code>sql_server</code> connection type.</p>"},{"location":"guides/setup_azure/#configuration-patterns_1","title":"Configuration Patterns","text":""},{"location":"guides/setup_azure/#option-a-auto-auth-managed-identity","title":"Option A: Auto-Auth (Managed Identity)","text":"<p>Best for: Production pipelines running in Azure.</p> <pre><code>connections:\n  my_sql_db:\n    type: sql_server\n    host: \"odibi-sql.database.windows.net\"\n    database: \"analytics_db\"\n    port: 1433\n    auth: {}  # Empty auth dict signals \"Use Default Driver Auth / Managed Identity\"\n</code></pre> <p>Setup: 1. Enable Managed Identity on your compute resource. 2. In Azure SQL, create a user for the identity: <code>CREATE USER [my-identity] FROM EXTERNAL PROVIDER;</code>. 3. Grant permissions: <code>ALTER ROLE db_datareader ADD MEMBER [my-identity];</code>.</p>"},{"location":"guides/setup_azure/#option-b-universal-key-vault-sql-password","title":"Option B: Universal Key Vault (SQL Password)","text":"<p>Best for: Legacy SQL Auth (Username/Password).</p> <pre><code>connections:\n  my_sql_db:\n    type: sql_server\n    host: \"odibi-sql.database.windows.net\"\n    database: \"analytics_db\"\n    port: 1433\n    auth:\n      username: \"sqladmin\"\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"sql-password\"  # The secret containing the password\n</code></pre>"},{"location":"guides/setup_azure/#3-key-vault-setup","title":"3. Key Vault Setup","text":"<p>If you use Key Vault references, Odibi needs to authenticate to the Key Vault first. It uses Auto-Auth for this too!</p> <ol> <li> <p>Create Key Vault: <code>bash    az keyvault create --name my-keyvault --resource-group my-rg</code></p> </li> <li> <p>Grant Access:    Grant your identity (User or Managed Identity) the Key Vault Secrets User role.    <code>bash    az role assignment create \\      --role \"Key Vault Secrets User\" \\      --assignee &lt;your-email-or-identity-id&gt; \\      --scope /subscriptions/.../resourceGroups/my-rg/providers/Microsoft.KeyVault/vaults/my-keyvault</code></p> </li> <li> <p>Store Secrets: <code>bash    az keyvault secret set --vault-name my-keyvault --name adls-account-key --value \"your-key-here\"</code></p> </li> </ol>"},{"location":"guides/setup_azure/#summary-of-changes-v20-v21","title":"Summary of Changes (v2.0 -&gt; v2.1)","text":"Feature v2.0 (Old) v2.1 (New) Connection Type <code>azure_adls</code>, <code>azure_sql</code> <code>azure_blob</code>, <code>sql_server</code> Auth Mode <code>auth_mode: key_vault</code> (top-level) Removed. Use <code>auth: { key_vault_name: ... }</code> Managed Identity Explicit <code>auth_mode: managed_identity</code> Implicit (Auto-Auth) via <code>DefaultAzureCredential</code> Key Vault Limited to specific auth modes Universal (works for any secret in <code>auth</code> dict) <p>For a complete configuration reference, see docs/reference/configuration.md.</p>"},{"location":"guides/testing/","title":"Testing Guide","text":"<p>Test your Odibi pipelines with built-in utilities for assertions, fixtures, and deterministic data.</p>"},{"location":"guides/testing/#overview","title":"Overview","text":"<p>Odibi provides testing utilities in <code>odibi.testing</code>:</p> <ul> <li>Assertions: Compare DataFrames and schemas</li> <li>Fixtures: Generate sample data and temporary directories</li> <li>Source Pools: Deterministic, frozen test data for replay</li> </ul>"},{"location":"guides/testing/#assertions","title":"Assertions","text":""},{"location":"guides/testing/#assert_frame_equal","title":"assert_frame_equal","text":"<p>Compare two DataFrames for equality (supports Pandas and Spark):</p> <pre><code>from odibi.testing.assertions import assert_frame_equal\n\n# Compare two DataFrames\nassert_frame_equal(actual_df, expected_df)\n\n# With options\nassert_frame_equal(\n    actual_df, \n    expected_df,\n    check_dtype=True,      # Check column types\n    check_exact=False,     # Allow float tolerance\n    atol=1e-8,             # Absolute tolerance\n    rtol=1e-5              # Relative tolerance\n)\n</code></pre>"},{"location":"guides/testing/#assert_schema_equal","title":"assert_schema_equal","text":"<p>Compare schemas (column names and types):</p> <pre><code>from odibi.testing.assertions import assert_schema_equal\n\nassert_schema_equal(df_a, df_b)\n</code></pre>"},{"location":"guides/testing/#fixtures","title":"Fixtures","text":""},{"location":"guides/testing/#temp_directory","title":"temp_directory","text":"<p>Create a temporary directory that auto-cleans:</p> <pre><code>from odibi.testing.fixtures import temp_directory\n\nwith temp_directory() as temp_dir:\n    path = os.path.join(temp_dir, \"test.csv\")\n    df.to_csv(path)\n    # Directory is deleted after context exits\n</code></pre>"},{"location":"guides/testing/#generate_sample_data","title":"generate_sample_data","text":"<p>Generate sample DataFrames for testing:</p> <pre><code>from odibi.testing.fixtures import generate_sample_data\n\n# Default schema: id (int), value (float), category (str), timestamp (date)\ndf = generate_sample_data(rows=100)\n\n# Custom schema\ndf = generate_sample_data(\n    rows=50,\n    engine_type=\"spark\",  # or \"pandas\"\n    schema={\n        \"user_id\": \"int\",\n        \"score\": \"float\",\n        \"name\": \"str\",\n        \"created_at\": \"date\"\n    }\n)\n</code></pre>"},{"location":"guides/testing/#unit-testing-nodes","title":"Unit Testing Nodes","text":"<p>Test individual pipeline nodes with mock data:</p> <pre><code>from odibi.pipeline import PipelineManager\nimport pandas as pd\n\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\npipeline = manager.get_pipeline(\"main_etl\")\n\n# Mock input data\nmock_data = {\n    \"read_customers\": pd.DataFrame([\n        {\"id\": 1, \"email\": \"BAD_EMAIL\"},\n        {\"id\": 2, \"email\": \"good@test.com\"}\n    ])\n}\n\n# Run single node with mock\nresult = pipeline.run_node(\"clean_customers\", mock_data=mock_data)\n\n# Assert output\noutput_df = pipeline.context.get(\"clean_customers\")\nassert len(output_df) == 1\n</code></pre>"},{"location":"guides/testing/#source-pools","title":"Source Pools","text":"<p>For deterministic, replayable tests, see Source Pools Design.</p> <p>Source pools provide:</p> <ul> <li>Frozen data: Hash-verified, immutable test datasets</li> <li>Quality variants: Clean, messy, and mixed data</li> <li>Schema definitions: Explicit, no runtime inference</li> <li>Test coverage hints: Know what scenarios each pool covers</li> </ul>"},{"location":"guides/testing/#related","title":"Related","text":"<ul> <li>Python API Guide \u2014 Programmatic pipeline execution</li> <li>Source Pools Design \u2014 Deterministic test data</li> <li>Best Practices \u2014 Testing recommendations</li> </ul>"},{"location":"guides/the_definitive_guide/","title":"The Definitive Guide to Odibi","text":"<p>Version: 2.4.0 Audience: Data Engineers, Analytics Engineers, Architects Prerequisites: Basic Python and SQL knowledge Goal: From \"Hello World\" to Enterprise Production</p>"},{"location":"guides/the_definitive_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction &amp; Philosophy</li> <li>Architecture Deep Dive</li> <li>The Object Model (Python API)</li> <li>Context, State, and Dependencies</li> <li>Transformation Engineering</li> <li>The Dual Engine: Pandas &amp; Spark</li> <li>Declarative Configuration (YAML)</li> <li>Observability: Data Stories</li> <li>Reliability &amp; Governance</li> <li>Production Deployment Patterns</li> <li>Comprehensive End-to-End Example</li> </ol>"},{"location":"guides/the_definitive_guide/#1-introduction-philosophy","title":"1. Introduction &amp; Philosophy","text":"<p>Odibi is a Declarative Data Framework designed to solve the \"Two Language Problem\" in data engineering: 1.  Local Development is often done in Python/Pandas on a laptop (fast iteration, small data). 2.  Production runs on distributed clusters like Spark/Databricks (high latency, massive data).</p> <p>Traditionally, this requires rewriting code or wrapping Spark in complex local docker containers. Odibi provides a unified abstraction layer that allows the exact same pipeline definition to run on your MacBook (using Pandas) and on a 100-node Databricks cluster (using Spark).</p>"},{"location":"guides/the_definitive_guide/#core-principles","title":"Core Principles","text":"<ol> <li>Declarative over Imperative: Define what you want (Input -&gt; Transform -&gt; Output), not how to loop through files.</li> <li>Code First, Configuration Second: Learn the Python API to understand the system; use YAML for deployment.</li> <li>Engine Agnostic: Logic written for Odibi runs on Pandas (Local) and Spark (Scale) without code changes.</li> <li>Observability by Default: Every run generates a rich HTML \"Data Story\" with lineage, profile, and logs.</li> </ol>"},{"location":"guides/the_definitive_guide/#2-architecture-deep-dive","title":"2. Architecture Deep Dive","text":"<p>Odibi is built on a \"Three Layer\" Architecture. Understanding these layers helps you debug and extend the framework.</p> <pre><code>graph TD\n    subgraph \"Layer 1: Declarative (User Interface)\"\n        A[odibi.yaml] --&gt;|Parsed by| B(ProjectConfig)\n        CLI[odibi run] --&gt;|Invokes| B\n    end\n\n    subgraph \"Layer 2: Object Model (The Brain)\"\n        B --&gt; C[PipelineConfig]\n        C --&gt; D[NodeConfig]\n        D --&gt;|Validates| E[DependencyGraph]\n        E --&gt;|Orchestrates| F[Pipeline Executor]\n    end\n\n    subgraph \"Layer 3: Runtime Engine (The Muscle)\"\n        F --&gt;|Delegates to| G{Engine Interface}\n        G --&gt;|Local Mode| H[Pandas Engine]\n        G --&gt;|Scale Mode| I[Spark Engine]\n        H --&gt; J[(Local Files / DuckDB)]\n        I --&gt; K[(Delta Lake / ADLS / Databricks)]\n    end\n</code></pre> <ul> <li>Layer 1 (YAML): Simple configuration files.</li> <li>Layer 2 (Pydantic Objects): The <code>odibi.config</code> module. This is where validation happens. If your graph has a cycle, or you miss a parameter, Layer 2 catches it before execution starts.</li> <li>Layer 3 (Engines): The <code>odibi.engine</code> module. This adapts your abstract \"Read CSV\" command into <code>pd.read_csv(...)</code> or <code>spark.read.csv(...)</code>.</li> </ul>"},{"location":"guides/the_definitive_guide/#3-the-object-model-python-api","title":"3. The Object Model (Python API)","text":"<p>The most effective way to learn Odibi is to build a pipeline using the Python classes directly. This demystifies the YAML tags.</p>"},{"location":"guides/the_definitive_guide/#31-the-hierarchy","title":"3.1 The Hierarchy","text":"<ol> <li><code>ProjectConfig</code>: Top-level container. Holds global settings (Engine type, Retry policy, Connections).</li> <li><code>PipelineConfig</code>: A logical grouping of tasks (e.g., \"Daily Sales Batch\").</li> <li><code>NodeConfig</code>: A single step in the pipeline.</li> <li>Operations: <code>ReadConfig</code>, <code>TransformConfig</code>, <code>WriteConfig</code>.</li> </ol>"},{"location":"guides/the_definitive_guide/#32-building-a-complex-node-programmatically","title":"3.2 Building a Complex Node Programmatically","text":"<p>Let's look at a feature-rich node configuration.</p> <pre><code>from odibi.config import NodeConfig, ReadConfig, TransformConfig, WriteConfig, ErrorStrategy\n\nnode = NodeConfig(\n    name=\"process_orders\",\n    description=\"Cleans orders and calculates tax\",\n    tags=[\"daily\", \"critical\"],\n\n    # 1. Dependency Management\n    depends_on=[\"raw_orders\", \"ref_tax_rates\"],\n\n    # 2. Transformation Logic\n    transform=TransformConfig(\n        steps=[\n            # Step A: Filter (SQL)\n            \"SELECT * FROM raw_orders WHERE status != 'CANCELLED'\",\n            # Step B: Custom Python Function\n            {\"function\": \"calculate_tax\", \"params\": {\"rate_source\": \"ref_tax_rates\"}}\n        ]\n    ),\n\n    # 3. Output Configuration\n    write=WriteConfig(\n        connection=\"silver_db\",\n        format=\"delta\",\n        table=\"orders_cleaned\",\n        mode=\"append\"\n    ),\n\n    # 4. Reliability &amp; Governance\n    on_error=ErrorStrategy.FAIL_FAST,  # Stop whole pipeline if this fails\n    sensitive=[\"customer_email\"],      # Mask this column in logs/stories\n    cache=True                         # Cache result in memory for downstream nodes\n)\n</code></pre>"},{"location":"guides/the_definitive_guide/#33-node-operations-guide","title":"3.3 Node Operations Guide","text":"Operation Description Key Fields Read Ingests data from a connection. <code>connection</code>, <code>format</code> (<code>csv</code>, <code>parquet</code>, <code>delta</code>, <code>sql</code>), <code>path</code>, <code>options</code> Transform Runs a sequence of steps. <code>steps</code> (List of SQL strings or <code>{function: name, params: {}}</code> dicts) Transformer Runs a single \"App-like\" transformation. <code>transformer</code> (Name string), <code>params</code> (Dict). Used for complex logic like SCD2 or Deduplication. Write Saves the result. <code>connection</code>, <code>format</code>, <code>path</code>/<code>table</code>, <code>mode</code> (<code>overwrite</code>, <code>append</code>, <code>upsert</code>)"},{"location":"guides/the_definitive_guide/#4-context-state-and-dependencies","title":"4. Context, State, and Dependencies","text":"<p>In Odibi, you don't pass variables between functions. You rely on the Dependency Graph.</p>"},{"location":"guides/the_definitive_guide/#41-the-global-context-odibicontextcontext","title":"4.1 The Global Context (<code>odibi.context.Context</code>)","text":"<p>The Pipeline Executor maintains a Global Context. *   Registry: When a node named <code>raw_orders</code> finishes, its result (DataFrame) is registered in the context under the key <code>\"raw_orders\"</code>. *   Access: Downstream nodes request data from the context by name. *   Memory Management: In the Pandas engine, this is a dictionary of DataFrames in RAM. In Spark, these are Temp Views registered in the Spark Session.</p>"},{"location":"guides/the_definitive_guide/#42-dependency-resolution","title":"4.2 Dependency Resolution","text":"<p>When you define <code>depends_on=[\"node_A\"]</code> for <code>node_B</code>: 1.  Graph Construction: Odibi builds a DAG (Directed Acyclic Graph). 2.  Topological Sort: It determines the execution order (A -&gt; B). 3.  Execution:     *   <code>node_A</code> runs. Output registered as <code>\"node_A\"</code>.     *   <code>node_B</code> starts. It can now execute <code>SELECT * FROM node_A</code>.</p>"},{"location":"guides/the_definitive_guide/#43-the-enginecontext-odibicontextenginecontext","title":"4.3 The EngineContext (<code>odibi.context.EngineContext</code>)","text":"<p>When writing custom transformations, you interact with <code>EngineContext</code>, not the Global Context directly. This wraps the global state and provides uniform APIs.</p> Method Description <code>context.df</code> The DataFrame resulting from the previous step in the current node. <code>context.get(name)</code> Retrieve a DataFrame from an upstream node (Global Context). <code>context.sql(query)</code> Run SQL on <code>context.df</code>. Returns a new Context with updated <code>df</code>. <code>context.register_temp_view(name, df)</code> Register a DF manually for complex SQL joins."},{"location":"guides/the_definitive_guide/#5-transformation-engineering","title":"5. Transformation Engineering","text":"<p>While SQL is great for filtering and projection, complex logic belongs in Python. Odibi uses a registry pattern.</p>"},{"location":"guides/the_definitive_guide/#51-the-transform-decorator","title":"5.1 The <code>@transform</code> Decorator","text":"<p>You must register functions so Odibi can find them by name from the YAML configuration.</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef categorize_users(context, params: dict):\n    \"\"\"\n    Categorizes users based on spend.\n\n    YAML:\n      function: categorize_users\n      params:\n        threshold: 1000\n    \"\"\"\n    threshold = params.get(\"threshold\", 1000)\n\n    # 1. Get Input\n    df = context.df\n\n    # 2. Use Engine-Native commands (Pandas/Spark agnostic via SQL)\n    # Or check context.engine_type if you need specific optimization\n\n    return context.sql(f\"\"\"\n        SELECT\n            *,\n            CASE WHEN total_spend &gt; {threshold} THEN 'VIP' ELSE 'Regular' END as category\n        FROM df\n    \"\"\").df\n</code></pre>"},{"location":"guides/the_definitive_guide/#52-transformer-vs-transform-steps","title":"5.2 Transformer vs. Transform Steps","text":"<p>There are two ways to define logic in a node:</p> <ol> <li> <p>Top-Level Transformer (<code>transformer: \"name\"</code>):</p> <ul> <li>The node acts as a specific \"App\" (e.g., <code>deduplicate</code>, <code>scd2</code>).</li> <li>It usually takes the dependencies as input and produces one output.</li> <li>Use this for heavy, reusable logic (Slowly Changing Dimensions, Merges).</li> </ul> </li> <li> <p>Transform Steps (<code>transform: { steps: [...] }</code>):</p> <ul> <li>A \"Script\" of sequential operations.</li> <li>Mixes SQL and lightweight Python functions.</li> <li>Use this for business logic specific to that pipeline (Filter -&gt; Join -&gt; Map -&gt; Reduce).</li> </ul> </li> </ol>"},{"location":"guides/the_definitive_guide/#6-the-dual-engine-pandas-spark","title":"6. The Dual Engine: Pandas &amp; Spark","text":"<p>Odibi abstracts the engine, but knowing how they differ is crucial for performance.</p>"},{"location":"guides/the_definitive_guide/#61-comparison","title":"6.1 Comparison","text":"Feature Pandas Engine Spark Engine Compute Single Node (CPU/RAM bound) Distributed (Cluster) SQL DuckDB / PandasQL Spark SQL (Catalyst) IO Local FS, S3/Blob (via fsspec) HDFS, S3, ADLS (Native) Setup <code>pip install odibi</code> <code>pip install odibi[spark]</code> Best For Dev, Testing, Small Data (&lt;10GB) Prod, Big Data (TB/PB)"},{"location":"guides/the_definitive_guide/#62-spark-specific-features","title":"6.2 Spark-Specific Features","text":"<p>The Spark engine enables advanced Data Lakehouse features via <code>odibi.engine.spark_engine.SparkEngine</code>.</p>"},{"location":"guides/the_definitive_guide/#delta-lake-integration","title":"Delta Lake Integration","text":"<p>Odibi treats Delta Lake as a first-class citizen.</p> <ul> <li> <p>Time Travel: <code>python     ReadConfig(..., options={\"as_of_version\": 5})     # OR     ReadConfig(..., options={\"as_of_timestamp\": \"2023-10-01\"})</code></p> </li> <li> <p>Upserts (MERGE): <code>python     WriteConfig(         ...,         format=\"delta\",         mode=\"upsert\",         options={\"keys\": [\"user_id\"]} # Matches on key, updates all other cols     )</code></p> </li> <li> <p>Optimization (Write Options): <code>python     WriteConfig(..., options={         \"optimize_write\": \"true\",  # Auto-compaction         \"zorder_by\": [\"region\"]    # Spatial indexing     })</code></p> </li> </ul>"},{"location":"guides/the_definitive_guide/#7-declarative-configuration-yaml","title":"7. Declarative Configuration (YAML)","text":"<p>The YAML configuration is the deployment artifact. It maps 1:1 to the Pydantic models.</p>"},{"location":"guides/the_definitive_guide/#71-project-structure","title":"7.1 Project Structure","text":"<p>Recommended folder structure for an Odibi project:</p> <pre><code>my_project/\n\u251c\u2500\u2500 odibi.yaml            # Main entry point\n\u251c\u2500\u2500 transforms.py         # Custom python logic (@transform)\n\u251c\u2500\u2500 data/                 # Local data (for dev)\n\u251c\u2500\u2500 stories/              # Generated reports\n\u2514\u2500\u2500 .env                  # Secrets (API keys)\n</code></pre>"},{"location":"guides/the_definitive_guide/#72-advanced-yaml-features","title":"7.2 Advanced YAML Features","text":""},{"location":"guides/the_definitive_guide/#environment-variables","title":"Environment Variables","text":"<p>You can inject secrets or environment-specific paths using <code>${VAR_NAME}</code>.</p> <pre><code>connections:\n  snowflake:\n    type: \"sql_server\"\n    host: \"${DB_HOST}\"\n    password: \"${DB_PASSWORD}\" # Redacted in logs automatically\n</code></pre>"},{"location":"guides/the_definitive_guide/#yaml-anchors-aliases","title":"YAML Anchors &amp; Aliases","text":"<p>Reuse configuration blocks to keep YAML DRY (Don't Repeat Yourself).</p> <pre><code># Define a template\n.default_write: &amp;default_write\n  connection: \"datalake\"\n  format: \"delta\"\n  mode: \"overwrite\"\n\nnodes:\n  - name: \"customers\"\n    write:\n      &lt;&lt;: *default_write\n      table: \"dim_customers\"\n\n  - name: \"orders\"\n    write:\n      &lt;&lt;: *default_write\n      table: \"fact_orders\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#8-observability-data-stories","title":"8. Observability: Data Stories","text":"<p>Running a pipeline blindly is dangerous. Odibi generates Data Stories.</p>"},{"location":"guides/the_definitive_guide/#81-what-is-a-story","title":"8.1 What is a Story?","text":"<p>A Story is an HTML file generated at the end of a run. It answers: 1.  Lineage: \"Where did this data come from?\" (Visual Graph) 2.  Profile: \"How many rows? How many nulls?\" (Schema &amp; Stats) 3.  Sample: \"What does the data look like?\" (Preview rows) 4.  Logic: \"What code actually ran?\" (SQL/Python snippet)</p>"},{"location":"guides/the_definitive_guide/#82-configuration","title":"8.2 Configuration","text":"<p>Enable story generation in <code>ProjectConfig</code>.</p> <pre><code>story:\n  connection: \"local_data\" # Where to save the HTML\n  path: \"stories/\"\n  max_sample_rows: 20      # Number of rows to preview\n  retention_days: 30       # Auto-cleanup old reports\n</code></pre>"},{"location":"guides/the_definitive_guide/#83-openlineage","title":"8.3 OpenLineage","text":"<p>Odibi supports the OpenLineage standard for integration with tools like Marquez or Atlan.</p> <pre><code>lineage:\n  url: \"http://localhost:5000\" # Marquez URL\n  namespace: \"odibi_prod\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#9-reliability-governance","title":"9. Reliability &amp; Governance","text":""},{"location":"guides/the_definitive_guide/#91-retries-backoff","title":"9.1 Retries &amp; Backoff","text":"<p>Network blips happen. Configure retries globally or per node.</p> <pre><code># Global setting in project config\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\" # linear, constant\n</code></pre>"},{"location":"guides/the_definitive_guide/#92-alerting","title":"9.2 Alerting","text":"<p>Send notifications when pipelines fail or succeed.</p> <pre><code>alerts:\n  - type: \"slack\"\n    url: \"${SLACK_WEBHOOK}\"\n    on_events: [\"on_failure\"] # on_start, on_success\n    metadata:\n      env: \"production\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#93-pii-protection","title":"9.3 PII Protection","text":"<p>Prevent sensitive data from leaking into logs or Data Stories.</p> <pre><code>nodes:\n  - name: \"load_users\"\n    # Masks columns in the 'Sample Data' section of the Story\n    sensitive: [\"email\", \"ssn\", \"phone_number\"]\n</code></pre>"},{"location":"guides/the_definitive_guide/#10-production-deployment-patterns","title":"10. Production Deployment Patterns","text":""},{"location":"guides/the_definitive_guide/#101-the-hybrid-pattern","title":"10.1 The \"Hybrid\" Pattern","text":"<ol> <li>Develop Locally: Use <code>engine: pandas</code> and local CSVs. Iterate fast.</li> <li>Deploy to Cloud:<ul> <li>Switch <code>engine: spark</code>.</li> <li>Change connections to ADLS/S3.</li> <li>Use <code>odibi run</code> via a job scheduler (Airflow, Databricks Workflows).</li> </ul> </li> </ol>"},{"location":"guides/the_definitive_guide/#102-cicd","title":"10.2 CI/CD","text":"<p>Since Odibi logic is Python code and YAML config: 1.  Linting: Run <code>black</code> and <code>ruff</code> on your <code>transforms.py</code>. 2.  Validation: Run a script that loads <code>ProjectConfig(path=\"odibi.yaml\")</code> to validate the graph structure before deployment. 3.  Testing: Use the Python API to run single nodes with mock data (Unit Tests).</p>"},{"location":"guides/the_definitive_guide/#11-comprehensive-end-to-end-example","title":"11. Comprehensive End-to-End Example","text":"<p>This script demonstrates a complete workflow: 1.  Mocking data generation. 2.  Defining custom logic. 3.  Configuring a multi-stage pipeline (Bronze -&gt; Silver -&gt; Gold). 4.  Executing with error handling.</p> <pre><code>import pandas as pd\nimport os\nimport logging\nfrom odibi.config import (\n    PipelineConfig, NodeConfig, ReadConfig, TransformConfig, WriteConfig,\n    RetryConfig, ErrorStrategy\n)\nfrom odibi.pipeline import Pipeline\nfrom odibi.registry import transform\nfrom odibi.connections import LocalConnection\n\n# ==========================================\n# 0. Setup Environment\n# ==========================================\n# Create dummy data for the example\nos.makedirs(\"data/landing\", exist_ok=True)\nos.makedirs(\"data/silver\", exist_ok=True)\nos.makedirs(\"data/gold\", exist_ok=True)\n\n# Generate Raw JSON Data (Simulating an API dump)\npd.DataFrame([\n    {\"id\": 1, \"user\": \"Alice\", \"tx_amount\": 150.0, \"ts\": \"2023-10-01T10:00:00\"},\n    {\"id\": 2, \"user\": \"Bob\",   \"tx_amount\": 20.0,  \"ts\": \"2023-10-01T10:05:00\"},\n    {\"id\": 3, \"user\": \"Alice\", \"tx_amount\": -50.0, \"ts\": \"2023-10-01T10:10:00\"}, # Invalid\n    {\"id\": 4, \"user\": \"Eve\",   \"tx_amount\": 1000.0, \"ts\": \"2023-10-01T10:15:00\"},\n]).to_json(\"data/landing/transactions.json\", orient=\"records\")\n\n# ==========================================\n# 1. Custom Logic (Business Rules)\n# ==========================================\n@transform\ndef anomaly_detection(context, params):\n    \"\"\"\n    Flags transactions that are 3 std devs above the mean.\n    \"\"\"\n    df = context.df\n\n    # Using SQL for statistical window function\n    # This works in DuckDB (Pandas) and Spark SQL\n    query = \"\"\"\n        SELECT\n            *,\n            AVG(tx_amount) OVER () as mean_amount,\n            STDDEV(tx_amount) OVER () as std_amount,\n            CASE\n                WHEN tx_amount &gt; (AVG(tx_amount) OVER () + 3 * STDDEV(tx_amount) OVER ())\n                THEN true\n                ELSE false\n            END as is_anomaly\n        FROM df\n    \"\"\"\n    return context.sql(query).df\n\n# ==========================================\n# 2. Pipeline Definition\n# ==========================================\npipeline_conf = PipelineConfig(\n    pipeline=\"fraud_detection_batch\",\n    description=\"Ingests transactions and flags anomalies\",\n    nodes=[\n        # --- Bronze Layer: Ingest Raw ---\n        NodeConfig(\n            name=\"bronze_tx\",\n            read=ReadConfig(\n                connection=\"landing\",\n                format=\"json\",\n                path=\"transactions.json\"\n            ),\n            # Keep raw data safe, minimal transform\n            write=WriteConfig(\n                connection=\"silver\",\n                format=\"parquet\",\n                path=\"bronze_tx.parquet\",\n                mode=\"overwrite\"\n            )\n        ),\n\n        # --- Silver Layer: Clean &amp; Enrich ---\n        NodeConfig(\n            name=\"silver_tx\",\n            depends_on=[\"bronze_tx\"],\n            transform=TransformConfig(\n                steps=[\n                    # 1. Filter invalid amounts\n                    \"SELECT * FROM bronze_tx WHERE tx_amount &gt; 0\",\n                    # 2. Run Anomaly Detection\n                    {\"function\": \"anomaly_detection\", \"params\": {}}\n                ]\n            ),\n            write=WriteConfig(\n                connection=\"silver\",\n                format=\"parquet\",\n                path=\"silver_tx.parquet\",\n                mode=\"overwrite\"\n            )\n        ),\n\n        # --- Gold Layer: Business Aggregates ---\n        NodeConfig(\n            name=\"gold_user_summary\",\n            depends_on=[\"silver_tx\"],\n            transform=TransformConfig(\n                steps=[\n                    \"\"\"\n                    SELECT\n                        user,\n                        COUNT(*) as tx_count,\n                        SUM(tx_amount) as total_volume,\n                        SUM(CASE WHEN is_anomaly THEN 1 ELSE 0 END) as suspicious_tx_count\n                    FROM silver_tx\n                    GROUP BY user\n                    \"\"\"\n                ]\n            ),\n            write=WriteConfig(\n                connection=\"gold\",\n                format=\"csv\",\n                path=\"user_risk_report.csv\",\n                mode=\"overwrite\"\n            ),\n            # Fail fast if report generation breaks\n            on_error=ErrorStrategy.FAIL_FAST\n        )\n    ]\n)\n\n# ==========================================\n# 3. Execution Wrapper\n# ==========================================\ndef run_pipeline():\n    print(\"\ud83d\ude80 Starting Fraud Detection Pipeline...\")\n\n    # Connections Definition\n    connections = {\n        \"landing\": LocalConnection(base_path=\"./data/landing\"),\n        \"silver\":  LocalConnection(base_path=\"./data/silver\"),\n        \"gold\":    LocalConnection(base_path=\"./data/gold\")\n    }\n\n    # Initialize Pipeline\n    pipeline = Pipeline(\n        pipeline_conf,\n        connections=connections,\n        generate_story=True,\n        story_config={\n            \"output_path\": \"data/stories\",\n            \"max_sample_rows\": 50\n        },\n        retry_config=RetryConfig(enabled=True, max_attempts=2)\n    )\n\n    # Run\n    results = pipeline.run()\n\n    # Logging Results\n    print(f\"\\n\u23f1\ufe0f Duration: {results.duration:.2f}s\")\n\n    if results.failed:\n        print(f\"\u274c Failed Nodes: {results.failed}\")\n        # Inspect specific error\n        for node in results.failed:\n            res = results.get_node_result(node)\n            print(f\"   Reason ({node}): {res.error}\")\n    else:\n        print(f\"\u2705 Success! Report generated at: {results.story_path}\")\n\n        # Verify output\n        print(\"\\n--- Risk Report ---\")\n        df = pd.read_csv(\"data/gold/user_risk_report.csv\")\n        print(df.to_string(index=False))\n\nif __name__ == \"__main__\":\n    run_pipeline()\n</code></pre> <p>This guide provides a solid foundation for using Odibi. Start with the Python API to understand the mechanics, and transition to YAML for production operations.</p>"},{"location":"guides/writing_transformations/","title":"Writing Transformation Functions in Odibi","text":"<p>This guide explains how to write custom Python transformation functions for Odibi pipelines, focusing on how to access data and manage state.</p>"},{"location":"guides/writing_transformations/#the-basics","title":"The Basics","text":"<p>Every transformation function in Odibi must be decorated with <code>@transform</code>. The Odibi engine automatically injects dependencies based on your function signature.</p>"},{"location":"guides/writing_transformations/#the-context-object","title":"The <code>context</code> Object","text":"<p>The first argument to any transformation function is always <code>context</code>. This object is your gateway to the entire state of the pipeline execution.</p> <p>Through <code>context</code>, you can: - Access the output of any previous node. - Retrieve datasets declared in <code>depends_on</code>. - Inspect available data using <code>context.list_names()</code>.</p>"},{"location":"guides/writing_transformations/#the-current-argument","title":"The <code>current</code> Argument","text":"<p>If your function includes an argument named <code>current</code>, Odibi will automatically pass the output of the immediately preceding step to it.</p> <ul> <li>With <code>current</code>: Continues the \"chain\" of data transformation.</li> <li>Without <code>current</code>: Breaks the chain (useful for generators or starting fresh logic).</li> </ul>"},{"location":"guides/writing_transformations/#accessing-other-datasets","title":"Accessing Other Datasets","text":"<p>While <code>current</code> is great for linear transformations (A \u2192 B \u2192 C), complex logic often requires accessing multiple datasets (e.g., for joins, lookups, or comparisons). You do this using <code>context.get()</code>.</p>"},{"location":"guides/writing_transformations/#pattern-explicit-data-fetching","title":"Pattern: Explicit Data Fetching","text":"<ol> <li>Define the Function: Add a parameter for the dataset name you want to fetch.</li> <li>Fetch from Context: Use <code>context.get(name)</code>.</li> <li>Configure in YAML: Pass the node name as a parameter.</li> </ol>"},{"location":"guides/writing_transformations/#python-implementation-transformspy","title":"Python Implementation (<code>transforms.py</code>)","text":"<pre><code>from odibi import transform\nimport pandas as pd\n\n@transform\ndef enrich_with_lookup(context, current: pd.DataFrame, lookup_node: str):\n    \"\"\"\n    Enriches the current stream with data from a lookup node.\n\n    Args:\n        context: The Odibi execution context.\n        current: The dataframe from the previous step.\n        lookup_node: The name of the node containing lookup data (passed from YAML).\n    \"\"\"\n    # 1. Fetch the other dataset using context\n    if not context.has(lookup_node):\n        raise ValueError(f\"Lookup node '{lookup_node}' not found in context!\")\n\n    lookup_df = context.get(lookup_node)\n\n    # 2. Perform the logic (e.g., merge)\n    # Note: For simple merges, SQL is often preferred, but Python is useful\n    # for fuzzy matching, complex logic, or API-based enrichment.\n    result = current.merge(\n        lookup_df,\n        on=\"common_id\",\n        how=\"left\",\n        suffixes=(\"\", \"_lookup\")\n    )\n\n    return result\n</code></pre>"},{"location":"guides/writing_transformations/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>nodes:\n  - name: main_process\n    depends_on:\n      - raw_orders      # The 'current' stream\n      - customer_info   # The lookup table\n    transform:\n      steps:\n        - function: enrich_with_lookup\n          params:\n            lookup_node: \"customer_info\"\n</code></pre>"},{"location":"guides/writing_transformations/#sql-vs-python-when-to-use-what","title":"SQL vs. Python: When to use what?","text":"<p>Odibi supports mixing SQL and Python steps in the same node.</p> Use SQL when... Use Python when... Joining tables (Standard Joins) Making API calls (e.g., Geocoding, REST APIs) Aggregations (GROUP BY, SUM) Complex loops or procedural logic Filtering (WHERE clauses) Using libraries (NumPy, SciPy, AI models) Renaming/Reordering columns File operations or custom parsing <p>Example of SQL for Multi-Dataset Access: If you just need a standard join, you don't need a Python function. You can reference nodes directly in SQL:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        SELECT o.*, c.email\n        FROM current_df AS o\n        LEFT JOIN customer_info AS c ON o.id = c.id\n</code></pre>"},{"location":"guides/writing_transformations/#sql-transformations","title":"SQL Transformations","text":"<p>For standard data transformations, SQL is often cleaner than Python. Odibi supports inline SQL and SQL file references.</p>"},{"location":"guides/writing_transformations/#inline-sql","title":"Inline SQL","text":"<pre><code>nodes:\n  - name: clean_orders\n    depends_on: [raw_orders]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              order_id,\n              customer_id,\n              UPPER(TRIM(status)) AS status,\n              CAST(amount AS DECIMAL(10,2)) AS amount,\n              COALESCE(discount, 0) AS discount\n            FROM raw_orders\n            WHERE order_id IS NOT NULL\n</code></pre>"},{"location":"guides/writing_transformations/#multi-table-sql-joins","title":"Multi-Table SQL Joins","text":"<p>Reference any node from <code>depends_on</code>:</p> <pre><code>nodes:\n  - name: enriched_orders\n    depends_on: [clean_orders, customers, products]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              o.*,\n              c.customer_name,\n              c.segment,\n              p.product_name,\n              p.category\n            FROM clean_orders o\n            LEFT JOIN customers c ON o.customer_id = c.id\n            LEFT JOIN products p ON o.product_id = p.id\n</code></pre>"},{"location":"guides/writing_transformations/#sql-file-reference","title":"SQL File Reference","text":"<p>For complex queries, use external SQL files. Paths are resolved relative to the YAML file where the node is defined:</p> <pre><code># In silver.yaml\ntransform:\n  steps:\n    - sql_file: sql/complex_aggregation.sql  # relative to silver.yaml\n</code></pre> <p>Example project structure:</p> <pre><code>project/\n\u251c\u2500\u2500 project.yaml              # imports pipelines/silver/silver.yaml\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 silver/\n        \u251c\u2500\u2500 silver.yaml       # defines the node\n        \u2514\u2500\u2500 sql/\n            \u2514\u2500\u2500 transform.sql\n</code></pre> <p>In <code>silver.yaml</code>, use a path relative to <code>silver.yaml</code>:</p> <pre><code># silver.yaml\ntransform:\n  steps:\n    - sql_file: sql/transform.sql   # \u2713 Correct: relative to silver.yaml\n</code></pre> <p>Important: Do NOT use absolute paths or paths relative to project.yaml:</p> <pre><code># \u2717 Wrong - absolute path\n- sql_file: /pipelines/silver/sql/transform.sql\n\n# \u2717 Wrong - relative to project.yaml instead of silver.yaml  \n- sql_file: pipelines/silver/sql/transform.sql\n</code></pre> <p>sql/complex_aggregation.sql:</p> <pre><code>WITH daily_totals AS (\n    SELECT \n        DATE(order_date) AS order_day,\n        customer_id,\n        SUM(amount) AS daily_amount\n    FROM orders\n    GROUP BY DATE(order_date), customer_id\n)\nSELECT \n    order_day,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    SUM(daily_amount) AS revenue\nFROM daily_totals\nGROUP BY order_day\n</code></pre>"},{"location":"guides/writing_transformations/#window-functions-in-sql","title":"Window Functions in SQL","text":"<pre><code>transform:\n  steps:\n    - sql: |\n        SELECT \n          *,\n          ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS order_rank,\n          SUM(amount) OVER (PARTITION BY customer_id) AS customer_lifetime_value,\n          LAG(amount) OVER (PARTITION BY customer_id ORDER BY order_date) AS prev_order_amount\n        FROM orders\n</code></pre>"},{"location":"guides/writing_transformations/#combining-sql-and-python-steps","title":"Combining SQL and Python Steps","text":"<pre><code>transform:\n  steps:\n    # Step 1: SQL for standard transformations\n    - sql: |\n        SELECT * FROM raw_orders \n        WHERE status != 'CANCELLED'\n\n    # Step 2: Python for complex logic\n    - function: enrich_with_api_data\n      params:\n        api_endpoint: \"https://api.example.com/enrichment\"\n\n    # Step 3: SQL for final shaping\n    - sql: |\n        SELECT order_id, customer_id, amount, enriched_data\n        FROM current_df\n        ORDER BY order_date\n</code></pre>"},{"location":"guides/writing_transformations/#registering-custom-transforms-with-transform","title":"Registering Custom Transforms with @transform","text":"<p>The <code>@transform</code> decorator registers your function so Odibi can find it by name in YAML configurations.</p>"},{"location":"guides/writing_transformations/#basic-registration","title":"Basic Registration","text":"<pre><code>from odibi import transform\n\n@transform\ndef clean_names(context, current):\n    \"\"\"Function is registered as 'clean_names' (uses function name).\"\"\"\n    current['name'] = current['name'].str.strip().str.title()\n    return current\n</code></pre>"},{"location":"guides/writing_transformations/#custom-name-registration","title":"Custom Name Registration","text":"<pre><code>@transform(\"normalize_addresses\")\ndef my_address_normalizer(context, current):\n    \"\"\"Function is registered as 'normalize_addresses'.\"\"\"\n    # ... address normalization logic\n    return current\n</code></pre>"},{"location":"guides/writing_transformations/#registration-with-category-and-parameter-model","title":"Registration with Category and Parameter Model","text":"<pre><code>from pydantic import BaseModel\n\nclass EnrichmentParams(BaseModel):\n    lookup_table: str\n    join_key: str\n    columns: list[str]\n\n@transform(name=\"enrich_data\", category=\"enrichment\", param_model=EnrichmentParams)\ndef enrich_data(context, current, lookup_table: str, join_key: str, columns: list):\n    \"\"\"\n    Registered as 'enrich_data' with parameter validation.\n\n    Parameters are validated against EnrichmentParams before execution.\n    \"\"\"\n    lookup_df = context.get(lookup_table)\n    return current.merge(lookup_df[columns + [join_key]], on=join_key, how='left')\n</code></pre>"},{"location":"guides/writing_transformations/#where-to-put-your-transforms","title":"Where to Put Your Transforms","text":"<ol> <li>Project-level: Create <code>transformations/custom_transforms.py</code></li> <li>Import in project.yaml:    ```yaml    python_imports:<ul> <li>transformations.custom_transforms    ```</li> </ul> </li> <li>Use in nodes:    ```yaml    transform:      steps:<ul> <li>function: normalize_addresses    ```</li> </ul> </li> </ol>"},{"location":"guides/writing_transformations/#summary-of-function-signature-rules","title":"Summary of Function Signature Rules","text":"Signature Behavior <code>def func(context):</code> Receives context only. Does not receive previous step output. <code>def func(context, current):</code> Receives context AND the result of the previous step. <code>def func(context, my_param):</code> Receives context and a parameter from YAML. <code>def func(context, current, my_param):</code> Receives all three."},{"location":"guides/writing_transformations/#see-also","title":"See Also","text":"<ul> <li>Patterns Overview - Built-in transformation patterns</li> <li>Best Practices - Code organization guidelines</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"guides/wsl_setup/","title":"\ud83d\udc27 WSL Setup Guide","text":"<p>If you are on Windows, Windows Subsystem for Linux (WSL 2) is the only supported way to develop with Odibi (especially for Spark compatibility).</p>"},{"location":"guides/wsl_setup/#the-golden-rule","title":"The Golden Rule","text":"<p>Edit code in Windows. Run code in WSL.</p> <ul> <li>VS Code: Runs in Windows, but connects to WSL.</li> <li>Terminal: You type commands in the WSL (Ubuntu) terminal.</li> <li>Files: Live in the Linux file system (or <code>/mnt/d/</code>).</li> </ul>"},{"location":"guides/wsl_setup/#1-install-requirements-inside-wsl","title":"1. Install Requirements (Inside WSL)","text":"<p>Open your Ubuntu terminal and run:</p> <pre><code># 1. Update system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# 2. Install Python 3.10\nsudo apt install -y python3.10 python3.10-venv python3.10-dev\n\n# 3. Install Java (Required for Spark)\nsudo apt install -y openjdk-17-jdk\n</code></pre>"},{"location":"guides/wsl_setup/#2-setup-environment","title":"2. Setup Environment","text":"<ol> <li> <p>Clone/Go to your project: <code>bash     cd /mnt/d/odibi  # Accessing D: drive from Linux</code></p> </li> <li> <p>Create Virtual Environment: <code>bash     python3.10 -m venv .venv     source .venv/bin/activate</code></p> </li> <li> <p>Install Odibi: <code>bash     pip install \"odibi[spark]\"</code></p> </li> </ol>"},{"location":"guides/wsl_setup/#3-configure-vs-code","title":"3. Configure VS Code","text":"<ol> <li>Install the \"WSL\" extension in VS Code.</li> <li>Open your folder in Windows.</li> <li>Click the green button in the bottom-left corner (\"Open a Remote Window\").</li> <li>Select \"Reopen in WSL\".</li> </ol> <p>Now your terminal inside VS Code is a Linux terminal!</p>"},{"location":"guides/wsl_setup/#troubleshooting","title":"Troubleshooting","text":"<p>\"Java not found\" Make sure you installed <code>openjdk-17-jdk</code>. Add this to your <code>~/.bashrc</code>:</p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n</code></pre> <p>\"Command not found: odibi\" Did you activate your virtual environment?</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"learning/curriculum/","title":"\ud83c\udf93 Odibi Learning Curriculum","text":"<p>A 4-Week Journey from Zero to Data Engineer</p> <p>This curriculum is designed for complete beginners with no prior data engineering experience. By the end, you'll be able to build production-ready data pipelines.</p>"},{"location":"learning/curriculum/#how-this-course-works","title":"How This Course Works","text":"<ul> <li>Pace: ~1-2 hours per day, 5 days per week</li> <li>Style: Learn by doing \u2014 every concept has hands-on exercises</li> <li>Format: Read \u2192 Try \u2192 Check \u2192 Repeat</li> </ul> <p>Each week builds on the previous one, like stacking building blocks.</p>"},{"location":"learning/curriculum/#week-1-bronze-layer-basic-concepts","title":"\ud83d\udcc5 Week 1: Bronze Layer + Basic Concepts","text":""},{"location":"learning/curriculum/#learning-objectives","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Understand what data is and common file formats - Know what a data pipeline does and why it matters - Install Odibi and run your first pipeline - Load raw data into a Bronze layer</p>"},{"location":"learning/curriculum/#prerequisites","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - A computer (Windows, Mac, or Linux) - Python 3.9+ installed (Download Python) - A text editor (VS Code recommended) - Basic comfort using a terminal/command prompt</p>"},{"location":"learning/curriculum/#day-1-what-is-data","title":"Day 1: What is Data?","text":""},{"location":"learning/curriculum/#kitchen-analogy","title":"\ud83c\udf73 Kitchen Analogy","text":"<p>Think of data like ingredients in your kitchen. You have: - Raw ingredients (flour, eggs, sugar) = raw data files - Recipes = data transformations - Finished dishes = clean, usable reports</p> <p>Data comes in many \"containers\":</p> Format What it looks like When to use CSV Spreadsheet-like rows and columns Simple tabular data JSON Nested key-value pairs API responses, configs Parquet Binary columnar format Large datasets, analytics Delta Parquet + versioning + ACID Production data lakes"},{"location":"learning/curriculum/#hands-on-create-your-first-data-file","title":"\ud83d\udcbb Hands-On: Create Your First Data File","text":"<p>Create a folder called <code>my_first_pipeline</code> and inside it create <code>customers.csv</code>:</p> <pre><code>id,name,email,signup_date\n1,Alice,alice@example.com,2024-01-15\n2,Bob,bob@example.com,2024-02-20\n3,Charlie,charlie@example.com,2024-03-10\n</code></pre> <p>This is tabular data: rows (records) and columns (fields).</p>"},{"location":"learning/curriculum/#self-check","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Can you explain what a CSV file is?</li> <li>[ ] What's the difference between a row and a column?</li> </ul>"},{"location":"learning/curriculum/#day-2-what-is-a-data-pipeline","title":"Day 2: What is a Data Pipeline?","text":""},{"location":"learning/curriculum/#assembly-line-analogy","title":"\ud83c\udfed Assembly Line Analogy","text":"<p>Imagine a car factory. Raw materials enter one end, go through multiple stations (welding, painting, assembly), and a finished car comes out the other end.</p> <p>A data pipeline works the same way: 1. Extract \u2014 Get raw data from somewhere (files, databases, APIs) 2. Transform \u2014 Clean, reshape, and enrich the data 3. Load \u2014 Save the result somewhere useful</p> <p>This is called ETL (Extract, Transform, Load).</p>"},{"location":"learning/curriculum/#the-medallion-architecture","title":"The Medallion Architecture","text":"<p>Odibi uses a \"layered\" approach to organize data:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      YOUR DATA LAKE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   BRONZE    \u2502     SILVER      \u2502           GOLD              \u2502\n\u2502   (Raw)     \u2502    (Cleaned)    \u2502       (Business-Ready)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 As-is     \u2502 \u2022 Deduplicated  \u2502 \u2022 Aggregated                \u2502\n\u2502 \u2022 Untouched \u2502 \u2022 Typed         \u2502 \u2022 Joined                    \u2502\n\u2502 \u2022 Archived  \u2502 \u2022 Validated     \u2502 \u2022 Ready for reporting       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why layers? - If something breaks, you can always go back to Bronze - Each layer has a clear purpose - Teams can work on different layers independently</p> <p>\ud83d\udcd6 Deep Dive: Medallion Architecture Guide</p>"},{"location":"learning/curriculum/#self-check_1","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does ETL stand for?</li> <li>[ ] Why do we have separate Bronze, Silver, and Gold layers?</li> </ul>"},{"location":"learning/curriculum/#day-3-introduction-to-odibi","title":"Day 3: Introduction to Odibi","text":""},{"location":"learning/curriculum/#what-is-odibi","title":"What is Odibi?","text":"<p>Odibi is a YAML-first data pipeline framework. Instead of writing hundreds of lines of code, you describe what you want in simple configuration files.</p>"},{"location":"learning/curriculum/#installation","title":"\ud83d\udd27 Installation","text":"<p>Open your terminal and run:</p> <pre><code># Create a virtual environment (recommended)\npython -m venv .venv\n\n# Activate it\n# Windows:\n.venv\\Scripts\\activate\n# Mac/Linux:\nsource .venv/bin/activate\n\n# Install Odibi\npip install odibi\n</code></pre> <p>Verify it works:</p> <pre><code>odibi --version\n</code></pre>"},{"location":"learning/curriculum/#your-first-odibi-project","title":"Your First Odibi Project","text":"<p>Let Odibi create a project structure for you:</p> <pre><code>odibi init-pipeline my_first_project --template local-medallion\ncd my_first_project\n</code></pre> <p>This creates:</p> <pre><code>my_first_project/\n\u251c\u2500\u2500 odibi.yaml          # Your pipeline configuration\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 landing/        # Where raw files arrive\n\u2502   \u251c\u2500\u2500 bronze/         # Raw data preserved\n\u2502   \u251c\u2500\u2500 silver/         # Cleaned data\n\u2502   \u2514\u2500\u2500 gold/           # Business-ready data\n\u2514\u2500\u2500 README.md\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Getting Started Tutorial</p>"},{"location":"learning/curriculum/#self-check_2","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What command installs Odibi?</li> <li>[ ] What folder does raw data go into?</li> </ul>"},{"location":"learning/curriculum/#day-4-the-bronze-layer","title":"Day 4: The Bronze Layer","text":""},{"location":"learning/curriculum/#filing-cabinet-analogy","title":"\ud83d\udce6 Filing Cabinet Analogy","text":"<p>Think of Bronze as your filing cabinet where you store original documents. You never write on the originals \u2014 you make copies first.</p> <p>The Bronze layer: - Stores data exactly as received - Never modifies or cleans anything - Acts as your \"source of truth\" backup</p>"},{"location":"learning/curriculum/#hands-on-build-a-bronze-pipeline","title":"\ud83d\udcbb Hands-On: Build a Bronze Pipeline","text":"<ol> <li> <p>Copy your <code>customers.csv</code> to <code>data/landing/</code></p> </li> <li> <p>Edit <code>odibi.yaml</code>:</p> </li> </ol> <pre><code>project: \"my_first_project\"\nengine: \"pandas\"\n\nconnections:\n  local:\n    type: local\n    base_path: \"./data\"\n\nstory:\n  connection: local\n  path: stories\n\nsystem:\n  connection: local\n  path: system\n\npipelines:\n  - pipeline: bronze_customers\n    layer: bronze\n    description: \"Load raw customer data\"\n    nodes:\n      - name: raw_customers\n        description: \"Ingest customers from landing zone\"\n\n        read:\n          connection: local\n          path: landing/customers.csv\n          format: csv\n\n        write:\n          connection: local\n          path: bronze/customers\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run your pipeline:</li> </ol> <pre><code>odibi run odibi.yaml\n</code></pre> <ol> <li>Check your output:</li> </ol> <pre><code># You should see a parquet file in data/bronze/customers/\nls data/bronze/customers/\n</code></pre>"},{"location":"learning/curriculum/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>Odibi read your CSV file</li> <li>Converted it to Parquet format (more efficient for analytics)</li> <li>Saved it to the Bronze layer</li> </ol> <p>No data was modified \u2014 just preserved in a better format.</p> <p>\ud83d\udcd6 Deep Dive: Bronze Layer Tutorial</p>"},{"location":"learning/curriculum/#self-check_3","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why don't we clean data in Bronze?</li> <li>[ ] What format did we convert the CSV to?</li> </ul>"},{"location":"learning/curriculum/#day-5-multi-node-pipelines","title":"Day 5: Multi-Node Pipelines","text":""},{"location":"learning/curriculum/#train-cars-analogy","title":"\ud83d\ude82 Train Cars Analogy","text":"<p>A pipeline is like a train. Each node is a train car \u2014 they're connected and run in sequence.</p>"},{"location":"learning/curriculum/#hands-on-add-more-data","title":"\ud83d\udcbb Hands-On: Add More Data","text":"<ol> <li>Create <code>data/landing/orders.csv</code>:</li> </ol> <pre><code>order_id,customer_id,product,amount,order_date\n1001,1,Widget A,29.99,2024-01-20\n1002,2,Widget B,49.99,2024-02-25\n1003,1,Widget C,19.99,2024-03-15\n1004,3,Widget A,29.99,2024-03-20\n</code></pre> <ol> <li>Add a second node to your pipeline:</li> </ol> <pre><code>pipelines:\n  - pipeline: bronze_ingest\n    layer: bronze\n    description: \"Load all raw data\"\n    nodes:\n      - name: raw_customers\n        description: \"Ingest customers\"\n        read:\n          connection: local\n          path: landing/customers.csv\n          format: csv\n        write:\n          connection: local\n          path: bronze/customers\n          format: parquet\n          mode: overwrite\n\n      - name: raw_orders\n        description: \"Ingest orders\"\n        read:\n          connection: local\n          path: landing/orders.csv\n          format: csv\n        write:\n          connection: local\n          path: bronze/orders\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run it:</li> </ol> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Both datasets are now in your Bronze layer.</p>"},{"location":"learning/curriculum/#self-check_4","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What is a \"node\" in Odibi?</li> <li>[ ] Can a pipeline have multiple nodes?</li> </ul>"},{"location":"learning/curriculum/#week-1-summary","title":"\ud83d\udcdd Week 1 Summary","text":"<p>You learned: - Data comes in different formats (CSV, JSON, Parquet) - Pipelines move data through stages (ETL) - Medallion architecture organizes data into layers - Bronze layer stores raw, unmodified data - Odibi uses YAML configuration to define pipelines</p> <p>Congratulations! You've built your first data pipeline. \ud83c\udf89</p>"},{"location":"learning/curriculum/#week-2-silver-layer-scd2-data-quality","title":"\ud83d\udcc5 Week 2: Silver Layer + SCD2 + Data Quality","text":""},{"location":"learning/curriculum/#learning-objectives_1","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Clean and transform data in the Silver layer - Understand and implement SCD2 (history tracking) - Add data quality checks to catch bad data - Handle missing values and invalid data</p>"},{"location":"learning/curriculum/#prerequisites_1","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - Completed Week 1 - A working Bronze layer with customer and order data</p>"},{"location":"learning/curriculum/#day-1-why-data-cleaning-matters","title":"Day 1: Why Data Cleaning Matters","text":""},{"location":"learning/curriculum/#dirty-kitchen-analogy","title":"\ud83e\uddf9 Dirty Kitchen Analogy","text":"<p>Imagine cooking with ingredients covered in dirt, or using expired milk. The result would be... unpleasant.</p> <p>Bad data causes: - Wrong business decisions - Broken reports - Angry users - Lost revenue</p>"},{"location":"learning/curriculum/#common-data-problems","title":"Common Data Problems","text":"Problem Example Impact Missing values <code>email: NULL</code> Can't contact customer Invalid format <code>date: \"not-a-date\"</code> Calculations fail Duplicates Same order twice Revenue doubled incorrectly Inconsistent \"CA\", \"California\", \"ca\" Grouping breaks"},{"location":"learning/curriculum/#the-silver-layers-job","title":"The Silver Layer's Job","text":"<p>The Silver layer is your cleaning station: - Fix data types (strings to dates, etc.) - Remove duplicates - Handle missing values - Validate data quality</p>"},{"location":"learning/curriculum/#self-check_5","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Name 3 common data quality problems</li> <li>[ ] What layer handles data cleaning?</li> </ul>"},{"location":"learning/curriculum/#day-2-building-a-silver-pipeline","title":"Day 2: Building a Silver Pipeline","text":""},{"location":"learning/curriculum/#hands-on-clean-your-customer-data","title":"\ud83d\udcbb Hands-On: Clean Your Customer Data","text":"<ol> <li>Update <code>odibi.yaml</code> to add a Silver pipeline:</li> </ol> <pre><code>pipelines:\n  # ... your bronze pipeline from Week 1 ...\n\n  - pipeline: silver_customers\n    layer: silver\n    description: \"Clean and standardize customers\"\n    nodes:\n      - name: clean_customers\n        description: \"Apply cleaning transformations\"\n\n        read:\n          connection: local\n          path: bronze/customers\n          format: parquet\n\n        transform:\n          - type: rename\n            columns:\n              id: customer_id\n\n          - type: cast\n            columns:\n              customer_id: integer\n              signup_date: date\n\n          - type: fill_null\n            columns:\n              email: \"unknown@example.com\"\n\n          - type: lower\n            columns:\n              - email\n\n        write:\n          connection: local\n          path: silver/customers\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run it:</li> </ol> <pre><code>odibi run odibi.yaml --pipeline silver_customers\n</code></pre>"},{"location":"learning/curriculum/#what-each-transform-does","title":"What Each Transform Does","text":"Transform Purpose Example <code>rename</code> Change column names <code>id</code> \u2192 <code>customer_id</code> <code>cast</code> Change data types String \u2192 Date <code>fill_null</code> Replace missing values NULL \u2192 default value <code>lower</code> Lowercase text \"BOB@EMAIL.COM\" \u2192 \"bob@email.com\" <p>\ud83d\udcd6 Deep Dive: Silver Layer Tutorial</p>"},{"location":"learning/curriculum/#self-check_6","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does <code>cast</code> do?</li> <li>[ ] Why lowercase email addresses?</li> </ul>"},{"location":"learning/curriculum/#day-3-scd2-tracking-history","title":"Day 3: SCD2 \u2014 Tracking History","text":""},{"location":"learning/curriculum/#time-machine-analogy","title":"\u23f0 Time Machine Analogy","text":"<p>Imagine you could look at a customer's record as it was 6 months ago. Where did they live? What tier were they?</p> <p>SCD2 (Slowly Changing Dimension Type 2) makes this possible by: - Never deleting old records - Adding new versions when data changes - Tracking when each version was valid</p>"},{"location":"learning/curriculum/#visual-example","title":"Visual Example","text":"<p>Customer moves from CA to NY on Feb 1:</p> customer_id address valid_from valid_to is_current 101 CA 2024-01-01 2024-02-01 false 101 NY 2024-02-01 NULL true <p>Now you can answer: \"Where did customer 101 live on January 15th?\" \u2192 CA</p>"},{"location":"learning/curriculum/#hands-on-add-scd2-to-customers","title":"\ud83d\udcbb Hands-On: Add SCD2 to Customers","text":"<ol> <li>First, update your source data. Create <code>data/landing/customers_update.csv</code>:</li> </ol> <pre><code>id,name,email,signup_date,tier\n1,Alice,alice@example.com,2024-01-15,Gold\n2,Bob,bob_new@example.com,2024-02-20,Silver\n3,Charlie,charlie@example.com,2024-03-10,Bronze\n4,Diana,diana@example.com,2024-04-01,Gold\n</code></pre> <p>(Notice: Bob has a new email, and Diana is a new customer)</p> <ol> <li>Add an SCD2 node:</li> </ol> <pre><code>  - pipeline: silver_customers_scd2\n    layer: silver\n    description: \"Track customer history\"\n    nodes:\n      - name: customers_with_history\n        description: \"Apply SCD2 for full history\"\n\n        read:\n          connection: local\n          path: landing/customers_update.csv\n          format: csv\n\n        transformer: scd2\n        params:\n          connection: local\n          path: silver/dim_customers\n          keys:\n            - id\n          track_cols:\n            - email\n            - tier\n          effective_time_col: signup_date\n\n        write:\n          connection: local\n          path: silver/dim_customers\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run it:</li> </ol> <pre><code>odibi run odibi.yaml --pipeline silver_customers_scd2\n</code></pre> <p>\ud83d\udcd6 Deep Dive: SCD2 Pattern</p>"},{"location":"learning/curriculum/#self-check_7","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does SCD2 stand for?</li> <li>[ ] What column tells you if a record is the current version?</li> </ul>"},{"location":"learning/curriculum/#day-4-data-quality-validation","title":"Day 4: Data Quality Validation","text":""},{"location":"learning/curriculum/#security-guard-analogy","title":"\ud83d\udea8 Security Guard Analogy","text":"<p>Before entering a building, security checks your ID. Data quality validation checks your data before it enters the Silver layer.</p>"},{"location":"learning/curriculum/#types-of-checks","title":"Types of Checks","text":"Check Type What it does Example not_null Ensure value exists <code>customer_id</code> can't be empty unique No duplicates Each <code>email</code> is unique range Value in bounds <code>age</code> between 0 and 150 regex Pattern matching Email contains <code>@</code> foreign_key Reference exists <code>customer_id</code> exists in customers table"},{"location":"learning/curriculum/#hands-on-add-validation","title":"\ud83d\udcbb Hands-On: Add Validation","text":"<ol> <li>Add validation to your node:</li> </ol> <pre><code>      - name: clean_customers\n        description: \"Clean with validation\"\n\n        read:\n          connection: local\n          path: bronze/customers\n          format: parquet\n\n        validation:\n          rules:\n            - column: customer_id\n              check: not_null\n              severity: error\n\n            - column: email\n              check: not_null\n              severity: warn\n\n            - column: email\n              check: regex\n              pattern: \".*@.*\\\\..*\"\n              severity: error\n\n          on_failure: quarantine  # Bad rows go to quarantine\n\n        write:\n          connection: local\n          path: silver/customers\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"learning/curriculum/#severity-levels","title":"Severity Levels","text":"Level What happens <code>warn</code> Log warning, continue processing <code>error</code> Quarantine bad rows, continue with good rows <code>fatal</code> Stop the entire pipeline <p>\ud83d\udcd6 Deep Dive: Data Validation</p>"},{"location":"learning/curriculum/#self-check_8","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does <code>quarantine</code> mean?</li> <li>[ ] What's the difference between <code>warn</code> and <code>error</code> severity?</li> </ul>"},{"location":"learning/curriculum/#day-5-putting-it-together","title":"Day 5: Putting It Together","text":""},{"location":"learning/curriculum/#hands-on-complete-silver-pipeline","title":"\ud83d\udcbb Hands-On: Complete Silver Pipeline","text":"<p>Create a complete Silver pipeline that: 1. Reads from Bronze 2. Cleans and transforms 3. Validates quality 4. Tracks history with SCD2</p> <pre><code>  - pipeline: silver_complete\n    layer: silver\n    description: \"Complete silver processing\"\n    nodes:\n      - name: stg_customers\n        description: \"Stage and clean customers\"\n\n        read:\n          connection: local\n          path: bronze/customers\n          format: parquet\n\n        transform:\n          - type: rename\n            columns:\n              id: customer_id\n          - type: cast\n            columns:\n              customer_id: integer\n              signup_date: date\n          - type: trim\n            columns:\n              - name\n              - email\n\n        validation:\n          rules:\n            - column: customer_id\n              check: not_null\n              severity: error\n            - column: email\n              check: regex\n              pattern: \".*@.*\"\n              severity: warn\n          on_failure: quarantine\n\n        write:\n          connection: local\n          path: silver/stg_customers\n          format: parquet\n          mode: overwrite\n\n      - name: dim_customers\n        description: \"Create customer dimension with history\"\n        depends_on:\n          - stg_customers\n\n        read:\n          connection: local\n          path: silver/stg_customers\n          format: parquet\n\n        transformer: scd2\n        params:\n          connection: local\n          path: silver/dim_customers\n          keys:\n            - customer_id\n          track_cols:\n            - name\n            - email\n          effective_time_col: signup_date\n\n        write:\n          connection: local\n          path: silver/dim_customers\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"learning/curriculum/#self-check_9","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does <code>depends_on</code> do?</li> <li>[ ] Why do we stage data before applying SCD2?</li> </ul>"},{"location":"learning/curriculum/#week-2-summary","title":"\ud83d\udcdd Week 2 Summary","text":"<p>You learned: - Why data cleaning is critical - How to transform data (rename, cast, fill_null) - SCD2 tracks historical changes - Validation catches bad data before it causes problems - Quarantine isolates bad rows for review</p> <p>Great progress! Your data is now clean and trackable. \ud83c\udf89</p>"},{"location":"learning/curriculum/#week-3-gold-layer-dimensional-modeling","title":"\ud83d\udcc5 Week 3: Gold Layer + Dimensional Modeling","text":""},{"location":"learning/curriculum/#learning-objectives_2","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Understand Facts vs Dimensions - Build a star schema - Use surrogate keys - Create aggregations for reporting - Build a complete data warehouse</p>"},{"location":"learning/curriculum/#prerequisites_2","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - Completed Weeks 1 and 2 - Working Bronze and Silver layers</p>"},{"location":"learning/curriculum/#day-1-facts-vs-dimensions","title":"Day 1: Facts vs Dimensions","text":""},{"location":"learning/curriculum/#theater-analogy","title":"\ud83c\udfad Theater Analogy","text":"<p>Think of a theater production: - Facts = The events (ticket sales, performances) - Dimensions = The context (who, what, when, where)</p> <p>Every fact answers: \"What happened?\" Every dimension answers: \"Tell me more about...\"</p>"},{"location":"learning/curriculum/#examples","title":"Examples","text":"Facts (Events) Dimensions (Context) Order placed Customer, Product, Date Payment received Customer, Account, Date Page viewed User, Page, Date"},{"location":"learning/curriculum/#visual-a-sales-transaction","title":"Visual: A Sales Transaction","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     FACT: Order                             \u2502\n\u2502  order_id=1001, amount=49.99, quantity=2                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502          \u2502          \u2502\n           \u25bc          \u25bc          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 DIM: Customer\u2502 \u2502 DIM: Product \u2502 \u2502 DIM: Date    \u2502\n\u2502 name=Alice   \u2502 \u2502 name=Widget  \u2502 \u2502 date=2024-01 \u2502\n\u2502 tier=Gold    \u2502 \u2502 category=HW  \u2502 \u2502 quarter=Q1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"learning/curriculum/#self-check_10","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Is \"order amount\" a fact or dimension?</li> <li>[ ] Is \"customer name\" a fact or dimension?</li> </ul>"},{"location":"learning/curriculum/#day-2-star-schema-basics","title":"Day 2: Star Schema Basics","text":""},{"location":"learning/curriculum/#star-analogy","title":"\u2b50 Star Analogy","text":"<p>A star schema looks like a star: the fact table is in the center, with dimension tables around it like points.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 dim_product \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dim_customer\u2502\u2500\u2500\u2500\u2500\u2500\u2502  fact_sales \u2502\u2500\u2500\u2500\u2500\u2500\u2502  dim_date   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 dim_location\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why stars? - Simple to understand - Fast to query (fewer joins) - Works with every BI tool</p>"},{"location":"learning/curriculum/#dimension-table-structure","title":"Dimension Table Structure","text":"<pre><code># dim_customers\ncustomer_key: 1          # Surrogate key (system-generated)\ncustomer_id: \"C001\"      # Natural key (from source)\nname: \"Alice\"\nemail: \"alice@example.com\"\ntier: \"Gold\"\neffective_from: \"2024-01-01\"\neffective_to: null\nis_current: true\n</code></pre>"},{"location":"learning/curriculum/#fact-table-structure","title":"Fact Table Structure","text":"<pre><code># fact_orders\norder_key: 1001\ncustomer_key: 1          # Points to dim_customers\nproduct_key: 42          # Points to dim_products\ndate_key: 20240120       # Points to dim_date\nquantity: 2\namount: 49.99\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Dimensional Modeling Guide</p>"},{"location":"learning/curriculum/#self-check_11","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why is it called a \"star\" schema?</li> <li>[ ] What's in the center of the star?</li> </ul>"},{"location":"learning/curriculum/#day-3-surrogate-keys","title":"Day 3: Surrogate Keys","text":""},{"location":"learning/curriculum/#hotel-room-key-analogy","title":"\ud83d\udd11 Hotel Room Key Analogy","text":"<p>When you check into a hotel, they give you a room key. This key is: - Unique to your stay (not your name) - System-generated (you don't choose it) - Internal (the hotel manages it)</p> <p>A surrogate key works the same way: - Unique identifier for each record - Generated by the system (not from source data) - Never changes, even if source data changes</p>"},{"location":"learning/curriculum/#why-not-use-natural-keys","title":"Why Not Use Natural Keys?","text":"Problem Natural Key Example Issue Changes SSN gets corrected Breaks all references Duplicates \"John Smith\" Too common Missing New customer, no ID yet Can't insert Composite firstName + lastName + DOB Slow to join"},{"location":"learning/curriculum/#hands-on-generate-surrogate-keys","title":"\ud83d\udcbb Hands-On: Generate Surrogate Keys","text":"<p>Odibi can auto-generate surrogate keys:</p> <pre><code>  - pipeline: gold_dimensions\n    layer: gold\n    description: \"Build dimension tables\"\n    nodes:\n      - name: dim_customers\n        description: \"Customer dimension with surrogate keys\"\n\n        read:\n          connection: local\n          path: silver/dim_customers\n          format: parquet\n\n        transform:\n          - type: generate_surrogate_key\n            key_column: customer_key\n            source_columns:\n              - customer_id\n\n        write:\n          connection: local\n          path: gold/dim_customers\n          format: parquet\n          mode: overwrite\n</code></pre> <p>The <code>generate_surrogate_key</code> transform creates a unique integer for each unique combination of source columns.</p> <p>\ud83d\udcd6 Deep Dive: Dimension Pattern</p>"},{"location":"learning/curriculum/#self-check_12","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What's wrong with using email as a primary key?</li> <li>[ ] Who generates surrogate keys \u2014 the source system or our data warehouse?</li> </ul>"},{"location":"learning/curriculum/#day-4-building-fact-tables","title":"Day 4: Building Fact Tables","text":""},{"location":"learning/curriculum/#hands-on-create-a-sales-fact-table","title":"\ud83d\udcbb Hands-On: Create a Sales Fact Table","text":"<ol> <li>First, ensure you have a date dimension. Create <code>data/landing/dates.csv</code>:</li> </ol> <pre><code>date_key,full_date,year,quarter,month,day_of_week\n20240115,2024-01-15,2024,Q1,January,Monday\n20240120,2024-01-20,2024,Q1,January,Saturday\n20240220,2024-02-20,2024,Q1,February,Tuesday\n20240225,2024-02-25,2024,Q1,February,Sunday\n20240310,2024-03-10,2024,Q1,March,Sunday\n20240315,2024-03-15,2024,Q1,March,Friday\n20240320,2024-03-20,2024,Q1,March,Wednesday\n</code></pre> <ol> <li>Build the fact table:</li> </ol> <pre><code>      - name: fact_orders\n        description: \"Order fact table\"\n        depends_on:\n          - dim_customers\n\n        read:\n          - connection: local\n            path: silver/orders\n            format: parquet\n            alias: orders\n          - connection: local\n            path: gold/dim_customers\n            format: parquet\n            alias: customers\n\n        transform:\n          - type: join\n            left: orders\n            right: customers\n            on:\n              - left: customer_id\n                right: customer_id\n            how: left\n            filter: \"is_current = true\"  # Only join to current customer version\n\n          - type: select\n            columns:\n              - order_id\n              - customer_key\n              - product\n              - amount\n              - order_date\n\n          - type: cast\n            columns:\n              order_date: date\n\n          - type: add_column\n            name: date_key\n            expression: \"date_format(order_date, 'yyyyMMdd')\"\n\n        write:\n          connection: local\n          path: gold/fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Fact Pattern</p>"},{"location":"learning/curriculum/#self-check_13","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why do we filter for <code>is_current = true</code> when joining?</li> <li>[ ] What's the purpose of <code>date_key</code>?</li> </ul>"},{"location":"learning/curriculum/#day-5-aggregations-for-reporting","title":"Day 5: Aggregations for Reporting","text":""},{"location":"learning/curriculum/#summary-reports-analogy","title":"\ud83d\udcca Summary Reports Analogy","text":"<p>Instead of reading every receipt, store managers want: - \"Total sales this month\" - \"Average order size by customer tier\" - \"Top 10 products\"</p> <p>Aggregations pre-compute these summaries.</p>"},{"location":"learning/curriculum/#hands-on-build-an-aggregation","title":"\ud83d\udcbb Hands-On: Build an Aggregation","text":"<pre><code>  - pipeline: gold_aggregations\n    layer: gold\n    description: \"Pre-computed summaries\"\n    nodes:\n      - name: agg_sales_by_customer\n        description: \"Sales summary per customer\"\n\n        read:\n          connection: local\n          path: gold/fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          group_by:\n            - customer_key\n          metrics:\n            - name: total_orders\n              expression: \"count(*)\"\n            - name: total_revenue\n              expression: \"sum(amount)\"\n            - name: avg_order_value\n              expression: \"avg(amount)\"\n            - name: first_order_date\n              expression: \"min(order_date)\"\n            - name: last_order_date\n              expression: \"max(order_date)\"\n\n        write:\n          connection: local\n          path: gold/agg_sales_by_customer\n          format: parquet\n          mode: overwrite\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Aggregation Pattern</p>"},{"location":"learning/curriculum/#self-check_14","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why pre-compute aggregations instead of calculating on-the-fly?</li> <li>[ ] What does <code>group_by</code> do?</li> </ul>"},{"location":"learning/curriculum/#week-3-summary","title":"\ud83d\udcdd Week 3 Summary","text":"<p>You learned: - Facts record events, dimensions provide context - Star schemas are simple and fast - Surrogate keys are stable, system-generated identifiers - Fact tables link to dimensions via keys - Aggregations pre-compute summaries for fast reporting</p> <p>Amazing work! You've built a complete data warehouse. \ud83c\udf89</p>"},{"location":"learning/curriculum/#week-4-production-deployment-best-practices","title":"\ud83d\udcc5 Week 4: Production Deployment + Best Practices","text":""},{"location":"learning/curriculum/#learning-objectives_3","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Configure connections for different environments - Implement error handling and retry logic - Add monitoring and logging - Tune performance for large datasets - Deploy to production with confidence</p>"},{"location":"learning/curriculum/#prerequisites_3","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - Completed Weeks 1-3 - A complete Bronze \u2192 Silver \u2192 Gold pipeline</p>"},{"location":"learning/curriculum/#day-1-connections-and-environments","title":"Day 1: Connections and Environments","text":""},{"location":"learning/curriculum/#different-homes-analogy","title":"\ud83c\udfe0 Different Homes Analogy","text":"<p>Your pipeline needs to work in different \"homes\": - Development \u2014 Your laptop, small test data - Staging \u2014 Test server, realistic data - Production \u2014 Real deal, live data</p> <p>Each environment has different connection details.</p>"},{"location":"learning/curriculum/#hands-on-configure-environments","title":"\ud83d\udcbb Hands-On: Configure Environments","text":"<pre><code>project: \"my_project\"\nengine: \"pandas\"\n\n# Global variables\nvars:\n  env: ${ODIBI_ENV:dev}  # Default to 'dev' if not set\n\n# Environment-specific overrides\nenvironments:\n  dev:\n    connections:\n      data_lake:\n        type: local\n        base_path: \"./data\"\n\n  staging:\n    connections:\n      data_lake:\n        type: azure_blob\n        account_name: \"mystorageacct\"\n        container: \"staging-data\"\n        credential: ${AZURE_STORAGE_KEY}\n\n  prod:\n    connections:\n      data_lake:\n        type: azure_blob\n        account_name: \"prodstorageacct\"\n        container: \"prod-data\"\n        credential: ${AZURE_STORAGE_KEY}\n\nconnections:\n  data_lake:\n    type: local\n    base_path: \"./data\"\n</code></pre> <p>Run for a specific environment:</p> <pre><code>ODIBI_ENV=staging odibi run odibi.yaml\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Environments Guide</p>"},{"location":"learning/curriculum/#self-check_15","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why use environment variables for credentials?</li> <li>[ ] What's the default environment if <code>ODIBI_ENV</code> isn't set?</li> </ul>"},{"location":"learning/curriculum/#day-2-error-handling-and-retry-logic","title":"Day 2: Error Handling and Retry Logic","text":""},{"location":"learning/curriculum/#retry-analogy","title":"\ud83d\udd04 Retry Analogy","text":"<p>If your phone call fails, you try again. Networks are unreliable; databases timeout. Retries handle temporary failures.</p>"},{"location":"learning/curriculum/#hands-on-configure-retries","title":"\ud83d\udcbb Hands-On: Configure Retries","text":"<pre><code>project: \"production_pipeline\"\nengine: \"spark\"\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential    # Wait longer between each retry\n  initial_delay: 5        # First retry after 5 seconds\n  max_delay: 300          # Never wait more than 5 minutes\n\npipelines:\n  - pipeline: bronze_ingest\n    nodes:\n      - name: fetch_api_data\n        retry:\n          max_attempts: 5  # Override for this node\n        read:\n          connection: external_api\n          path: /customers\n          format: json\n</code></pre>"},{"location":"learning/curriculum/#backoff-strategies","title":"Backoff Strategies","text":"Strategy Wait times (5s initial) Best for <code>constant</code> 5s, 5s, 5s Simple cases <code>linear</code> 5s, 10s, 15s Gradual increase <code>exponential</code> 5s, 10s, 20s, 40s API rate limits"},{"location":"learning/curriculum/#handling-failures","title":"Handling Failures","text":"<pre><code>        on_failure: continue  # Options: fail, continue, skip\n</code></pre> Action Behavior <code>fail</code> Stop entire pipeline (default) <code>continue</code> Log error, continue to next node <code>skip</code> Skip downstream nodes that depend on this one"},{"location":"learning/curriculum/#self-check_16","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does \"exponential backoff\" mean?</li> <li>[ ] When would you use <code>on_failure: continue</code>?</li> </ul>"},{"location":"learning/curriculum/#day-3-monitoring-and-logging","title":"Day 3: Monitoring and Logging","text":""},{"location":"learning/curriculum/#dashboard-analogy","title":"\ud83d\udcfa Dashboard Analogy","text":"<p>A pilot needs instruments to fly safely. You need monitoring to run pipelines safely.</p>"},{"location":"learning/curriculum/#hands-on-configure-logging","title":"\ud83d\udcbb Hands-On: Configure Logging","text":"<pre><code>logging:\n  level: INFO              # DEBUG, INFO, WARNING, ERROR\n  structured: true         # JSON format for log aggregators\n  include_metrics: true    # Row counts, timing\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    on_events:\n      - on_failure\n      - on_success\n\n  - type: email\n    to:\n      - data-team@company.com\n    on_events:\n      - on_failure\n</code></pre>"},{"location":"learning/curriculum/#what-gets-logged","title":"What Gets Logged","text":"<p>Every pipeline run generates a Data Story with: - Start/end timestamps - Row counts (read/written/quarantined) - Validation results - Error messages</p> <p>View your story:</p> <pre><code>odibi story show --latest\n</code></pre>"},{"location":"learning/curriculum/#self-check_17","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What's the difference between INFO and DEBUG logging?</li> <li>[ ] What is a \"Data Story\"?</li> </ul>"},{"location":"learning/curriculum/#day-4-performance-tuning","title":"Day 4: Performance Tuning","text":""},{"location":"learning/curriculum/#race-car-analogy","title":"\ud83c\udfce\ufe0f Race Car Analogy","text":"<p>A race car needs tuning to go fast. Data pipelines need tuning for large datasets.</p>"},{"location":"learning/curriculum/#key-performance-levers","title":"Key Performance Levers","text":"Lever When to use Configuration Partitioning Large tables (&gt;1M rows) Split data by date/category Caching Reused datasets Keep in memory Parallelism Multiple nodes Run independent nodes together Batch size Memory limits Process in chunks"},{"location":"learning/curriculum/#hands-on-add-partitioning","title":"\ud83d\udcbb Hands-On: Add Partitioning","text":"<pre><code>        write:\n          connection: data_lake\n          path: gold/fact_orders\n          format: delta\n          mode: overwrite\n          partition_by:\n            - order_year\n            - order_month\n</code></pre>"},{"location":"learning/curriculum/#hands-on-enable-caching","title":"\ud83d\udcbb Hands-On: Enable Caching","text":"<pre><code>      - name: dim_customers\n        cache: true          # Keep in memory for downstream nodes\n        read:\n          connection: local\n          path: silver/dim_customers\n</code></pre>"},{"location":"learning/curriculum/#hands-on-performance-config","title":"\ud83d\udcbb Hands-On: Performance Config","text":"<pre><code>performance:\n  max_parallel_nodes: 4    # Run up to 4 nodes simultaneously\n  batch_size: 100000       # Process 100k rows at a time\n  shuffle_partitions: 200  # Spark shuffle partitions\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Performance Tuning Guide</p>"},{"location":"learning/curriculum/#self-check_18","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why partition by date?</li> <li>[ ] What does caching do?</li> </ul>"},{"location":"learning/curriculum/#day-5-production-deployment-checklist","title":"Day 5: Production Deployment Checklist","text":""},{"location":"learning/curriculum/#launch-checklist","title":"\ud83d\ude80 Launch Checklist","text":"<p>Before deploying to production, verify:</p>"},{"location":"learning/curriculum/#configuration","title":"Configuration","text":"<ul> <li>[ ] All secrets use environment variables (never hardcoded)</li> <li>[ ] Correct environment settings for prod</li> <li>[ ] Retry logic enabled</li> <li>[ ] Alerts configured</li> </ul>"},{"location":"learning/curriculum/#data-quality","title":"Data Quality","text":"<ul> <li>[ ] Validation rules on all critical columns</li> <li>[ ] Quarantine configured for bad rows</li> <li>[ ] Foreign key checks enabled</li> </ul>"},{"location":"learning/curriculum/#performance","title":"Performance","text":"<ul> <li>[ ] Partitioning on large tables</li> <li>[ ] Appropriate parallelism</li> <li>[ ] Tested with production-scale data</li> </ul>"},{"location":"learning/curriculum/#operations","title":"Operations","text":"<ul> <li>[ ] Logging at INFO level</li> <li>[ ] Monitoring dashboard set up</li> <li>[ ] Runbook for common failures</li> <li>[ ] Backup/restore procedures documented</li> </ul>"},{"location":"learning/curriculum/#complete-production-config","title":"Complete Production Config","text":"<pre><code>project: \"customer360\"\nengine: \"spark\"\nversion: \"1.0.0\"\nowner: \"data-team@company.com\"\ndescription: \"Customer analytics pipeline\"\n\nvars:\n  env: ${ODIBI_ENV:prod}\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\nlogging:\n  level: INFO\n  structured: true\n  include_metrics: true\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK}\n    on_events: [on_failure]\n\nperformance:\n  max_parallel_nodes: 8\n  batch_size: 500000\n\nconnections:\n  data_lake:\n    type: azure_blob\n    account_name: ${AZURE_STORAGE_ACCOUNT}\n    container: \"prod-data\"\n    credential: ${AZURE_STORAGE_KEY}\n\nstory:\n  connection: data_lake\n  path: _odibi/stories\n\nsystem:\n  connection: data_lake\n  path: _odibi/system\n\npipelines:\n  # ... your pipelines ...\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Production Deployment Guide</p>"},{"location":"learning/curriculum/#self-check_19","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What should NEVER be hardcoded in config?</li> <li>[ ] What logging level is recommended for production?</li> </ul>"},{"location":"learning/curriculum/#week-4-summary","title":"\ud83d\udcdd Week 4 Summary","text":"<p>You learned: - Environments separate dev/staging/prod configurations - Retry logic handles temporary failures - Logging and alerts keep you informed - Partitioning and caching improve performance - A production checklist prevents common mistakes</p> <p>Congratulations! You've completed the Odibi curriculum! \ud83c\udf93\ud83c\udf89</p>"},{"location":"learning/curriculum/#whats-next","title":"\ud83c\udfaf What's Next?","text":"<p>Now that you've completed the basics:</p> <ol> <li>Build a real project \u2014 Apply what you learned to actual data</li> <li>Explore advanced patterns \u2014 Browse all patterns</li> <li>Learn the CLI \u2014 CLI Master Guide</li> <li>Join the community \u2014 Share your projects, ask questions</li> </ol>"},{"location":"learning/curriculum/#quick-reference-links","title":"Quick Reference Links","text":"Topic Link All Patterns ../patterns/README.md YAML Reference ../reference/yaml_schema.md Best Practices ../guides/best_practices.md Troubleshooting ../troubleshooting.md <p>Built with \u2764\ufe0f for data engineers who are just getting started.</p>"},{"location":"learning/data_engineering_101/","title":"Data Engineering 101: A Complete Beginner's Guide","text":"<p>Welcome to the world of data engineering. If you've never heard this term before, that's perfectly fine. This guide assumes you're starting from absolute zero. We'll explain every concept, every term, and every idea from the ground up.</p> <p>By the end of this guide, you'll understand what data engineers do, why their work matters, and how tools like Odibi help solve real problems.</p>"},{"location":"learning/data_engineering_101/#what-is-data","title":"What is Data?","text":"<p>Before we talk about \"data engineering,\" let's talk about \"data.\" You interact with data every single day, even if you don't realize it.</p>"},{"location":"learning/data_engineering_101/#data-is-just-information-stored-somewhere","title":"Data is Just Information Stored Somewhere","text":"<p>When you check your bank account online, you see a list of transactions. That list\u2014the dates, amounts, descriptions\u2014is data. It's information that's been recorded and stored so you can look at it later.</p> <p>When you shop online and receive an email receipt, that receipt contains data: what you bought, how much you paid, when the order was placed, where it's being shipped.</p> <p>When you scroll through your phone's photo gallery, each photo has data attached to it: the date it was taken, the location (if GPS was on), the file size, the camera settings.</p> <p>Data is simply recorded information. It can be numbers, text, dates, images, or anything else that can be stored.</p>"},{"location":"learning/data_engineering_101/#where-does-data-live","title":"Where Does Data Live?","text":"<p>Data needs a home. Just like you keep physical documents in folders and filing cabinets, digital data lives in specific places:</p> <p>Files are the simplest form. A file is just a container for data sitting on a computer's hard drive: - A <code>.csv</code> file (Comma-Separated Values) is like a spreadsheet saved as plain text - A <code>.json</code> file stores data in a structured, nested format - A <code>.txt</code> file is just raw text - An <code>.xlsx</code> file is a Microsoft Excel spreadsheet</p> <p>Databases are specialized software designed to store, organize, and retrieve large amounts of data efficiently. Think of a database as a super-powered filing cabinet that can find any document instantly, even if you have millions of them. Examples include PostgreSQL, MySQL, SQL Server, and Oracle.</p> <p>APIs (Application Programming Interfaces) are not storage themselves, but they're how different systems share data. When a weather app on your phone shows today's forecast, it's calling a weather API to get that data. The API is like a waiter in a restaurant\u2014you tell it what you want, and it brings it to you from the kitchen (the database).</p> <p>Data Lakes and Warehouses are massive storage systems designed for analytics. A data lake stores raw data in its original format (like a lake that collects water from many streams). A data warehouse stores cleaned, organized data ready for analysis (like a warehouse with neatly arranged shelves).</p>"},{"location":"learning/data_engineering_101/#types-of-data","title":"Types of Data","text":"<p>Not all data is created equal. Data comes in three main flavors:</p>"},{"location":"learning/data_engineering_101/#structured-data","title":"Structured Data","text":"<p>Structured data fits neatly into rows and columns, like a spreadsheet. Every row follows the same pattern. Every column has a specific type of information.</p> <p>Example: A customer table</p> <pre><code>| customer_id | name          | email                | signup_date |\n|-------------|---------------|----------------------|-------------|\n| 1           | Alice Smith   | alice@email.com      | 2024-01-15  |\n| 2           | Bob Johnson   | bob@email.com        | 2024-02-20  |\n| 3           | Carol White   | carol@email.com      | 2024-03-10  |\n</code></pre> <p>This is structured data because: - Every customer has exactly the same fields - Each field has a consistent type (IDs are numbers, names are text, dates are dates) - You can easily search, sort, and filter this data</p> <p>Most business data is structured: sales records, inventory counts, employee information, financial transactions.</p>"},{"location":"learning/data_engineering_101/#semi-structured-data","title":"Semi-Structured Data","text":"<p>Semi-structured data has some organization, but it's flexible. Not every record needs to have the same fields.</p> <p>Example: JSON data from an API</p> <pre><code>{\n  \"customer_id\": 1,\n  \"name\": \"Alice Smith\",\n  \"email\": \"alice@email.com\",\n  \"preferences\": {\n    \"newsletter\": true,\n    \"theme\": \"dark\"\n  },\n  \"tags\": [\"premium\", \"early-adopter\"]\n}\n</code></pre> <p>This is semi-structured because: - There's a clear organization (fields have names) - But the structure can vary (some customers might have a <code>phone</code> field, others might not) - Nested data is allowed (preferences contains its own sub-fields)</p> <p>JSON (JavaScript Object Notation) and XML are common semi-structured formats. They're often used for web data and configuration files.</p>"},{"location":"learning/data_engineering_101/#unstructured-data","title":"Unstructured Data","text":"<p>Unstructured data has no predefined format. It's raw and messy by nature.</p> <p>Examples: - A paragraph of text from a customer support email - An image file - A video recording - A voice memo</p> <p>Unstructured data is the hardest to work with because computers can't easily understand its meaning without special processing.</p>"},{"location":"learning/data_engineering_101/#data-you-encounter-daily","title":"Data You Encounter Daily","text":"<p>To make this concrete, here are data examples from everyday life:</p> Everyday Activity The Data Behind It Checking your bank balance Account numbers, transaction history, balances Getting a grocery receipt Product names, prices, quantities, totals, timestamps Using GPS navigation Location coordinates, road maps, traffic patterns Streaming music Song titles, artists, play counts, user preferences Ordering food delivery Menu items, prices, addresses, delivery times <p>Every digital interaction creates data. Data engineering is about managing, moving, and transforming all of this information so it can be useful.</p>"},{"location":"learning/data_engineering_101/#what-is-data-engineering","title":"What is Data Engineering?","text":"<p>Now that you understand what data is, let's talk about what data engineers actually do.</p>"},{"location":"learning/data_engineering_101/#the-plumbing-of-the-data-world","title":"The \"Plumbing\" of the Data World","text":"<p>Imagine a city's water system. Water needs to flow from reservoirs, through treatment plants, into pipes, and eventually out of your faucet. Someone has to design those pipes, build them, maintain them, and make sure clean water arrives when you turn the handle.</p> <p>Data engineering is the same idea, but for data instead of water.</p> <p>Data engineers build and maintain the systems that move data from where it's created to where it's needed.</p> <p>Just like you don't think about water pipes when you take a shower, most people don't think about data pipelines when they look at a dashboard. But those pipelines are essential. Without them, the data would never arrive.</p>"},{"location":"learning/data_engineering_101/#what-data-engineers-actually-do","title":"What Data Engineers Actually Do","text":"<p>On a typical day, a data engineer might:</p> <ol> <li> <p>Build data pipelines: Create automated processes that move data from one place to another. For example, extracting sales data from an e-commerce system and loading it into a data warehouse.</p> </li> <li> <p>Transform data: Clean, reshape, and combine data so it's ready for analysis. Raw data is often messy\u2014data engineers make it usable.</p> </li> <li> <p>Ensure data quality: Check that data is accurate, complete, and consistent. If bad data gets into a system, every report and decision based on it will be wrong.</p> </li> <li> <p>Maintain infrastructure: Keep databases running, optimize queries that are too slow, and troubleshoot when something breaks.</p> </li> <li> <p>Collaborate with analysts and scientists: Data engineers don't usually create the final reports or machine learning models\u2014they provide the clean data that makes those things possible.</p> </li> </ol>"},{"location":"learning/data_engineering_101/#data-engineering-vs-data-science-vs-data-analytics","title":"Data Engineering vs Data Science vs Data Analytics","text":"<p>These three roles are related but distinct:</p> Role Focus Key Question Data Engineer Building systems to move and prepare data \"How do we get clean data from A to B?\" Data Analyst Creating reports and dashboards from data \"What happened? What does the data show?\" Data Scientist Building predictive models and finding patterns \"What will happen? Can we predict X?\" <p>Analogy: Think of a restaurant. - The Data Engineer is the chef who prepares the ingredients\u2014washing vegetables, cutting meat, making sure everything is fresh and ready. - The Data Analyst is the cook who follows recipes to create dishes (reports) from those ingredients. - The Data Scientist is the chef who invents new recipes, experimenting with ingredients to create something new (predictions, recommendations).</p> <p>You can't have great dishes without properly prepared ingredients. That's why data engineering is foundational.</p>"},{"location":"learning/data_engineering_101/#a-real-job-scenario","title":"A Real Job Scenario","text":"<p>Let's say you work at an online clothing retailer. The company wants to know which products are selling best in each region.</p> <p>Without data engineering, someone would: 1. Log into the sales system manually 2. Export data to Excel 3. Spend hours cleaning up the data (fixing typos, removing duplicates) 4. Copy-paste data from different sources 5. Manually calculate totals 6. Repeat this process every week</p> <p>With data engineering, you would: 1. Build a pipeline that automatically extracts sales data every night 2. Join it with product and region data 3. Clean and standardize everything automatically 4. Load it into a warehouse where analysts can query it instantly 5. The pipeline runs every night without human intervention</p> <p>The first approach might work for a small company. But when you have millions of transactions, hundreds of products, and dozens of regions, manual work becomes impossible. That's when you need data engineering.</p>"},{"location":"learning/data_engineering_101/#what-is-a-data-pipeline","title":"What is a Data Pipeline?","text":"<p>A data pipeline is an automated process that moves data from one place to another, often transforming it along the way.</p>"},{"location":"learning/data_engineering_101/#the-factory-assembly-line-analogy","title":"The Factory Assembly Line Analogy","text":"<p>Imagine a car factory. Raw materials (steel, glass, rubber) enter at one end. They pass through various stations where workers and machines shape them, assemble them, and quality-check them. At the other end, finished cars roll off the line.</p> <p>A data pipeline works the same way:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Raw Data  \u2502\u2500\u2500\u2500\u25b6\u2502  Station 1  \u2502\u2500\u2500\u2500\u25b6\u2502  Station 2  \u2502\u2500\u2500\u2500\u25b6\u2502 Final Data  \u2502\n\u2502   (Input)   \u2502    \u2502  (Extract)  \u2502    \u2502 (Transform) \u2502    \u2502  (Output)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Input: The raw data enters the pipeline. This might be CSV files, API responses, database tables, or event streams.</p> <p>Processing Stations: Each station does something specific to the data: - Clean up messy values - Convert data types (text to dates, for example) - Join data from multiple sources - Calculate new fields - Filter out irrelevant records</p> <p>Output: Clean, transformed data arrives at its destination\u2014ready to be used for reports, dashboards, or further analysis.</p>"},{"location":"learning/data_engineering_101/#a-visual-explanation","title":"A Visual Explanation","text":"<p>Let's trace a specific piece of data through a pipeline:</p> <pre><code>                        DATA PIPELINE EXAMPLE\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udce6 INPUT: Raw sales data from a point-of-sale system\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 transaction_id,product,price,date,store                           \u2502\n\u2502 T001,SHRT-BLU-M,$29.99,2024-03-15,Store #42                       \u2502\n\u2502 T002,SHRT-BLU-M,29.99,15/03/2024,42                               \u2502\n\u2502 T003,,29.99,2024-03-15,Store 42                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNotice the problems? Inconsistent date formats, missing product, \ndollar signs in some prices, store names formatted differently.\n\n              \u2502\n              \u25bc\n\n\u2699\ufe0f STATION 1: Parse and Validate\n- Convert all dates to YYYY-MM-DD format\n- Remove currency symbols from prices\n- Flag records with missing required fields\n\n              \u2502\n              \u25bc\n\n\u2699\ufe0f STATION 2: Standardize\n- Normalize store names to IDs\n- Convert product codes to full names\n- Ensure prices are decimal numbers\n\n              \u2502\n              \u25bc\n\n\u2699\ufe0f STATION 3: Enrich\n- Look up product category from product catalog\n- Look up store region from store table\n- Calculate sales tax based on region\n\n              \u2502\n              \u25bc\n\n\u2705 OUTPUT: Clean, enriched sales data\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 transaction_id \u2502 product_name   \u2502 price \u2502 date       \u2502 store_id \u2502  \u2502\n\u2502 T001           \u2502 Blue Shirt (M) \u2502 29.99 \u2502 2024-03-15 \u2502 42       \u2502  \u2502\n\u2502 T002           \u2502 Blue Shirt (M) \u2502 29.99 \u2502 2024-03-15 \u2502 42       \u2502  \u2502\n\u2502 T003           \u2502 NULL           \u2502 29.99 \u2502 2024-03-15 \u2502 42       \u2502  \u2502 \u2190 Quarantined\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The pipeline took messy, inconsistent input and produced clean, standardized output. It also flagged the problematic record (T003) so someone can investigate the missing product.</p>"},{"location":"learning/data_engineering_101/#why-manual-excel-work-doesnt-scale","title":"Why Manual Excel Work Doesn't Scale","text":"<p>You might wonder: \"Can't I just do this in Excel?\"</p> <p>For small amounts of data, yes. But consider these scenarios:</p> Scenario Manual Approach Pipeline Approach 100 rows, once \u2705 Totally fine Overkill 100 rows, daily \ud83d\ude13 Tedious, error-prone \u2705 Automated 100,000 rows \u274c Excel slows down \u2705 No problem 10 million rows \u274c Excel crashes \u2705 Still fine 1 billion rows \u274c Impossible \u2705 Use distributed processing <p>Scale is one reason. But there are others:</p> <p>Reproducibility: If you clean data manually in Excel, can you do it exactly the same way next time? What if you're sick and a colleague has to do it? With a pipeline, the logic is written in code\u2014it runs the same way every time.</p> <p>Audit trail: When something goes wrong with a report, you need to trace back to see what happened. With Excel, you'd have to remember every click and formula change. With a pipeline, you can review the code and logs.</p> <p>Speed: A pipeline can run automatically at 3 AM while you sleep. Manual work requires a human to be present.</p>"},{"location":"learning/data_engineering_101/#what-problems-do-pipelines-solve","title":"What Problems Do Pipelines Solve?","text":"<p>Let's get specific about the problems data pipelines address.</p>"},{"location":"learning/data_engineering_101/#problem-1-messy-data","title":"Problem 1: Messy Data","text":"<p>Real-world data is almost never clean. Here are common issues:</p> <p>Inconsistent formatting:</p> <pre><code># These all mean the same thing, but are written differently\n\"United States\", \"USA\", \"U.S.A.\", \"US\", \"united states\", \"UNITED STATES\"\n</code></pre> <p>Duplicate records:</p> <pre><code># Same customer entered twice\ncustomer_id: 1001, name: \"John Smith\", email: \"john@email.com\"\ncustomer_id: 1002, name: \"John Smith\", email: \"john@email.com\"\n</code></pre> <p>Invalid values:</p> <pre><code># Age can't be negative\nage: -5\n\n# Birth date can't be in the future\nbirth_date: 2050-01-01\n\n# Email missing @ symbol\nemail: \"invalidemail.com\"\n</code></pre> <p>Mixed data types:</p> <pre><code># Price column has different formats\nprice: 29.99\nprice: \"$29.99\"\nprice: \"29,99\"  (European format)\nprice: \"twenty-nine ninety-nine\"\n</code></pre> <p>A pipeline includes validation and cleaning steps that catch and fix these issues automatically.</p>"},{"location":"learning/data_engineering_101/#problem-2-manual-work-and-human-error","title":"Problem 2: Manual Work and Human Error","text":"<p>Every time a human touches data manually, there's a chance for error:</p> <ul> <li>Copy-pasting the wrong range</li> <li>Forgetting to apply a filter</li> <li>Typing a formula incorrectly</li> <li>Saving over the wrong file</li> <li>Using last month's template instead of the updated one</li> </ul> <p>These mistakes are expensive. A single typo in a financial report could cost millions. A forgotten filter could lead to wrong business decisions.</p> <p>\ud83d\udca1 Automation removes human error from repetitive tasks. A pipeline runs the same logic every time, exactly as written.</p>"},{"location":"learning/data_engineering_101/#problem-3-scale","title":"Problem 3: Scale","text":"<p>Consider this: A small retail store might have 100 transactions per day. That's 36,500 transactions per year\u2014manageable in Excel.</p> <p>Now consider Amazon. They process roughly 8,500 transactions per second. That's 734 million transactions per day. 268 billion per year.</p> <p>You cannot process 268 billion rows in Excel. You need distributed computing systems that can split the work across hundreds or thousands of machines. Data pipelines built with tools like Apache Spark are designed for this scale.</p> <p>Even at smaller scales, performance matters. If generating a report takes 4 hours manually but 4 minutes with a pipeline, that's meaningful time savings.</p>"},{"location":"learning/data_engineering_101/#problem-4-reproducibility","title":"Problem 4: Reproducibility","text":"<p>Imagine this scenario:</p> <p>\"Hey, the revenue numbers in last month's report look wrong. Can you re-run the analysis?\"</p> <p>If you did it manually in Excel: - Do you still have the original files? - Did you document every step? - Can you replicate exactly what you did?</p> <p>If you have a pipeline: - Run the pipeline with last month's parameters - Get the same result - Compare to find the discrepancy</p> <p>Reproducibility is essential for trust. If you can't reproduce a result, how do you know it was right in the first place?</p>"},{"location":"learning/data_engineering_101/#problem-5-freshness","title":"Problem 5: Freshness","text":"<p>Business moves fast. Executives want to see yesterday's numbers this morning, not next week.</p> <p>Manual processes have inherent delays: - Someone has to be available to run them - They can only run during business hours - If something fails, you might not notice until someone complains</p> <p>Automated pipelines can: - Run on a schedule (every night at 2 AM) - Run on triggers (whenever new data arrives) - Alert you immediately if something fails - Retry automatically after transient failures</p>"},{"location":"learning/data_engineering_101/#etl-vs-elt","title":"ETL vs ELT","text":"<p>You'll often hear these acronyms in data engineering. They describe two different approaches to moving and transforming data.</p>"},{"location":"learning/data_engineering_101/#etl-extract-transform-load","title":"ETL: Extract, Transform, Load","text":"<p>ETL is the traditional approach. It works like this:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    SOURCE    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   TRANSFORM   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  DESTINATION  \u2502\n\u2502   Systems    \u2502 Extract \u2502   (Outside)   \u2502  Load   \u2502  (Warehouse)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Extract: Pull data from source systems (databases, APIs, files)</li> <li>Transform: Clean and reshape the data before loading it</li> <li>Load: Insert the transformed data into the destination</li> </ol> <p>Why ETL? In the past, storage and computing power in data warehouses were expensive. It made sense to clean and shrink the data before loading it\u2014you'd only store what you actually needed.</p> <p>ETL Example:</p> <pre><code>Source: 10 million raw transaction records\nTransform: Filter to only completed orders, aggregate by day\nLoad: 365 daily summary records into the warehouse\n</code></pre> <p>You've reduced 10 million records to 365, saving storage and making queries faster.</p>"},{"location":"learning/data_engineering_101/#elt-extract-load-transform","title":"ELT: Extract, Load, Transform","text":"<p>ELT is the modern approach, enabled by cheap cloud storage and powerful cloud warehouses.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    SOURCE    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  DESTINATION  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   TRANSFORM   \u2502\n\u2502   Systems    \u2502 Extract \u2502  (Warehouse)  \u2502  Load   \u2502   (Inside)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Extract: Pull data from source systems</li> <li>Load: Load the raw data into the destination first</li> <li>Transform: Transform the data inside the warehouse using SQL</li> </ol> <p>Why ELT? Modern cloud warehouses (Snowflake, BigQuery, Databricks) can store huge amounts of data cheaply and process it incredibly fast. Loading raw data first gives you: - A complete historical record (nothing is lost) - Flexibility to transform in different ways later - The ability to re-transform if requirements change</p> <p>ELT Example:</p> <pre><code>Source: 10 million raw transaction records\nLoad: All 10 million records into the warehouse (raw layer)\nTransform: Create views or tables for different purposes\n  - Daily summaries for executives\n  - Detailed records for fraud analysis\n  - Customer-level aggregations for marketing\n</code></pre>"},{"location":"learning/data_engineering_101/#visual-comparison","title":"Visual Comparison","text":"<pre><code>                    ETL (Traditional)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    SOURCE              ETL SERVER              WAREHOUSE\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Sales   \u2502          \u2502         \u2502           \u2502         \u2502\n  \u2502 System  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Clean   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Clean   \u2502\n  \u2502         \u2502          \u2502 Filter  \u2502           \u2502 Data    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502 Aggregate           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502         \u2502\n  \u2502 CRM     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u26a0\ufe0f Transformation happens OUTSIDE the warehouse\n  \u26a0\ufe0f Raw data is discarded after transformation\n\n\n                    ELT (Modern)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    SOURCE              WAREHOUSE                WAREHOUSE\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Sales   \u2502          \u2502 RAW     \u2502           \u2502 CLEAN   \u2502\n  \u2502 System  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Layer   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Layer   \u2502\n  \u2502         \u2502          \u2502 (All    \u2502   SQL     \u2502 (Views  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502  Data)  \u2502  Transforms  &amp; Tables)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502         \u2502           \u2502         \u2502\n  \u2502 CRM     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502         \u2502           \u2502         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u2705 Raw data is preserved\n  \u2705 Transformations use warehouse compute power\n  \u2705 Can re-transform any time\n</code></pre>"},{"location":"learning/data_engineering_101/#when-to-use-which","title":"When to Use Which","text":"Consideration ETL ELT Warehouse storage is expensive \u2705 Preferred Warehouse compute is powerful \u2705 Preferred Need to preserve raw data \u2705 Preferred Complex transformations before loading \u2705 Preferred Privacy/compliance requires filtering before loading \u2705 Preferred Want flexibility to change transformations later \u2705 Preferred"},{"location":"learning/data_engineering_101/#how-odibi-supports-both","title":"How Odibi Supports Both","text":"<p>Odibi is designed to work with either approach:</p> <p>For ETL workflows: You can define transformation patterns that run during the extract phase, cleaning and reshaping data before it reaches your destination.</p> <p>For ELT workflows: You can use Odibi to manage the \"E\" and \"L\" (extracting from sources and loading to raw layers), then define separate transformation jobs that run inside your warehouse.</p> <p>Hybrid approaches: Many real-world pipelines use both. You might: 1. Extract data from a source 2. Do light cleaning (ETL-style) to fix obvious issues 3. Load to a raw layer 4. Do heavy transformations (ELT-style) in the warehouse</p> <p>Odibi's pattern-based approach lets you mix and match as needed.</p>"},{"location":"learning/data_engineering_101/#what-is-a-schema-why-does-it-matter","title":"What is a Schema? Why Does It Matter?","text":"<p>A schema is the structure or blueprint of your data. It defines what fields exist, what types they are, and how they relate to each other.</p>"},{"location":"learning/data_engineering_101/#the-spreadsheet-headers-analogy","title":"The Spreadsheet Headers Analogy","text":"<p>When you create a spreadsheet, the first row usually contains headers:</p> <pre><code>| Name       | Age | Email              | Salary   |\n|------------|-----|--------------------|----------|\n| Alice      | 30  | alice@email.com    | 75000    |\n| Bob        | 25  | bob@email.com      | 65000    |\n</code></pre> <p>Those headers are a simple schema. They tell you: - What each column means - In what order columns appear</p> <p>A database schema goes further: - Name: Text, maximum 100 characters, cannot be empty - Age: Integer, must be between 0 and 150 - Email: Text, must contain \"@\", must be unique - Salary: Decimal number, cannot be negative</p>"},{"location":"learning/data_engineering_101/#type-safety-why-it-matters","title":"Type Safety: Why It Matters","text":"<p>Every piece of data has a type. Common types include:</p> Type What It Stores Examples String/Text Letters, words, sentences \"Hello\", \"Product-123\" Integer Whole numbers 1, 42, -7, 1000000 Float/Decimal Numbers with decimals 3.14, 29.99, -0.5 Boolean True or false true, false Date Calendar dates 2024-03-15 Timestamp Date and time 2024-03-15 10:30:00 <p>Why does this matter?</p> <p>Imagine someone puts text in a number column:</p> <pre><code>| product_id | quantity |\n|------------|----------|\n| P001       | 10       |\n| P002       | 5        |\n| P003       | \"TBD\"    |  \u2190 This is text, not a number!\n</code></pre> <p>Now try to calculate total quantity: <code>10 + 5 + \"TBD\" = ???</code></p> <p>The calculation fails because you can't add a number to text. This is a type mismatch, and it breaks things.</p> <p>A schema with proper types prevents this. If the <code>quantity</code> column is defined as an integer, the system will reject \"TBD\" when someone tries to insert it.</p>"},{"location":"learning/data_engineering_101/#what-happens-when-schemas-dont-match","title":"What Happens When Schemas Don't Match","text":"<p>Schema mismatches are a major source of pipeline failures. Here are common scenarios:</p> <p>Scenario 1: Source adds a new column</p> <pre><code># Your schema expects:\nname, email, signup_date\n\n# Source now sends:\nname, email, signup_date, phone_number  \u2190 New column!\n</code></pre> <p>Your pipeline might fail because it doesn't know what to do with <code>phone_number</code>.</p> <p>Scenario 2: Source changes a column type</p> <pre><code># Previously:\norder_id: 12345 (integer)\n\n# Now:\norder_id: \"ORD-12345\" (string)  \u2190 Type changed!\n</code></pre> <p>Your pipeline expects an integer but receives a string. Crash.</p> <p>Scenario 3: Source renames a column</p> <pre><code># Previously:\ncustomer_name\n\n# Now:\ncust_name  \u2190 Different name, same data\n</code></pre> <p>Your pipeline looks for <code>customer_name</code>, finds nothing, and fails.</p>"},{"location":"learning/data_engineering_101/#schema-evolution","title":"Schema Evolution","text":"<p>Schemas change over time. Business requirements evolve. New data becomes available. This is called schema evolution, and it's one of the trickiest parts of data engineering.</p> <p>Good data engineering practices handle schema evolution gracefully:</p> <ol> <li> <p>Additive changes (new columns): Usually safe. Ignore new columns or add them as optional.</p> </li> <li> <p>Removal changes (dropped columns): Dangerous. If your pipeline depends on that column, it will break.</p> </li> <li> <p>Type changes: Very dangerous. Requires careful migration.</p> </li> <li> <p>Rename changes: Dangerous. Requires mapping old names to new names.</p> </li> </ol> <p>Odibi helps with schema evolution by: - Validating incoming data against expected schemas - Providing clear error messages when schemas don't match - Supporting schema transformation patterns (renaming, type conversion)</p>"},{"location":"learning/data_engineering_101/#data-quality-basics","title":"Data Quality Basics","text":"<p>\"Garbage in, garbage out\" is the oldest saying in data. If the data going into your pipeline is bad, the output will be bad too\u2014no matter how sophisticated your transformations are.</p>"},{"location":"learning/data_engineering_101/#the-five-pillars-of-data-quality","title":"The Five Pillars of Data Quality","text":""},{"location":"learning/data_engineering_101/#1-completeness-are-required-values-present","title":"1. Completeness: Are required values present?","text":"<p>Nulls (missing values) are the most common data quality issue.</p> <pre><code>| order_id | customer_id | total   |\n|----------|-------------|---------|\n| 1001     | C100        | 59.99   |\n| 1002     | NULL        | 29.99   |  \u2190 Who placed this order?\n| 1003     | C102        | NULL    |  \u2190 How much was it?\n</code></pre> <p>Some nulls are acceptable (an optional phone number might be null). Others are critical failures (an order without a total is useless).</p> <p>Questions to ask: - Which fields are required? - What should happen when a required field is null? - Should we reject the record? Fill in a default? Quarantine for review?</p>"},{"location":"learning/data_engineering_101/#2-uniqueness-are-there-duplicate-records","title":"2. Uniqueness: Are there duplicate records?","text":"<p>Duplicates inflate counts and totals, leading to wrong conclusions.</p> <pre><code>| transaction_id | amount |\n|----------------|--------|\n| T001           | 100.00 |\n| T001           | 100.00 |  \u2190 Duplicate!\n| T002           | 50.00  |\n\nTotal without duplicates: $150.00\nTotal with duplicates: $250.00  \u2190 Wrong!\n</code></pre> <p>Questions to ask: - What makes a record unique? (Usually a primary key) - How did duplicates get created? (Fix the source if possible) - How do we handle duplicates? (Keep first? Keep last? Merge?)</p>"},{"location":"learning/data_engineering_101/#3-validity-are-values-within-acceptable-ranges","title":"3. Validity: Are values within acceptable ranges?","text":"<p>Invalid values don't make sense in context.</p> <pre><code># Invalid examples:\nage: -5              # Age cannot be negative\ntemperature: 500\u00b0C   # Unless it's a volcano, this is wrong\ndiscount: 150%       # Can't discount more than 100%\nbirth_date: 2099-01-01  # This date hasn't happened yet\nstatus: \"Pneding\"    # Typo in status value\n</code></pre> <p>Questions to ask: - What are the valid ranges for numeric fields? - What are the valid values for categorical fields? - What date ranges make sense?</p>"},{"location":"learning/data_engineering_101/#4-consistency-does-the-same-thing-have-the-same-representation","title":"4. Consistency: Does the same thing have the same representation?","text":"<p>Inconsistent data is technically valid but difficult to work with.</p> <pre><code># Inconsistent country names:\n\"United States\"\n\"USA\"\n\"U.S.A.\"\n\"United States of America\"\n\"US\"\n\n# Inconsistent date formats:\n\"2024-03-15\"\n\"03/15/2024\"\n\"15-Mar-2024\"\n\"March 15, 2024\"\n</code></pre> <p>All of these mean the same thing, but a computer sees them as different values. Reports will show separate counts for \"USA\" and \"United States\" when they should be combined.</p> <p>Solution: Standardize during transformation. Pick one format and convert everything to it.</p>"},{"location":"learning/data_engineering_101/#5-referential-integrity-do-relationships-make-sense","title":"5. Referential Integrity: Do relationships make sense?","text":"<p>When records refer to other records, those references should be valid.</p> <pre><code># Orders table:\n| order_id | customer_id |\n|----------|-------------|\n| 1001     | C100        |\n| 1002     | C999        |  \u2190 This customer doesn't exist!\n\n# Customers table:\n| customer_id | name    |\n|-------------|---------|\n| C100        | Alice   |\n| C101        | Bob     |\n</code></pre> <p>Order 1002 references customer C999, but that customer isn't in the customers table. This is called an orphan record\u2014it has no parent.</p> <p>Why it matters: - You can't look up the customer name for order 1002 - Reports will show orders without customer information - Joins between tables will lose data</p>"},{"location":"learning/data_engineering_101/#garbage-in-garbage-out-in-practice","title":"\"Garbage In, Garbage Out\" in Practice","text":"<p>Imagine building a dashboard showing \"Average Order Value by Region.\"</p> <p>If your data has: - \u274c Missing order totals \u2192 Averages will be wrong - \u274c Duplicate orders \u2192 Totals will be inflated - \u274c Invalid region codes \u2192 Some orders won't be assigned to any region - \u274c Inconsistent region names \u2192 \"West Coast\" and \"WEST COAST\" counted separately</p> <p>Your dashboard will show wrong numbers, and executives will make wrong decisions based on it.</p> <p>\ud83d\udca1 Data quality is not optional. It's the foundation that everything else depends on.</p> <p>Odibi includes built-in validation patterns to catch these issues before they pollute your data.</p>"},{"location":"learning/data_engineering_101/#the-medallion-architecture-bronzesilvergold","title":"The Medallion Architecture (Bronze/Silver/Gold)","text":"<p>The Medallion Architecture is a design pattern for organizing data in layers. Each layer has a specific purpose and quality level.</p>"},{"location":"learning/data_engineering_101/#the-manufacturing-analogy","title":"The Manufacturing Analogy","text":"<p>Think of a factory that turns raw materials into finished products:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    RAW ORE      \u2502    \u2502  REFINED METAL  \u2502    \u2502 FINISHED PRODUCT\u2502\n\u2502   (Bronze)      \u2502\u2500\u2500\u2500\u25b6\u2502    (Silver)     \u2502\u2500\u2500\u2500\u25b6\u2502     (Gold)      \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 Rocks, dirt,    \u2502    \u2502 Pure metal      \u2502    \u2502 Jewelry, tools  \u2502\n\u2502 impurities      \u2502    \u2502 ingots          \u2502    \u2502 ready to sell   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Data flows through similar stages, becoming more refined at each step.</p>"},{"location":"learning/data_engineering_101/#visual-diagram-of-the-medallion-architecture","title":"Visual Diagram of the Medallion Architecture","text":"<pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                        MEDALLION ARCHITECTURE\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n   SOURCES                BRONZE               SILVER               GOLD\n  (External)            (Raw Data)          (Cleaned)          (Business-Ready)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sales API    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502 Exact copy  \u2502     \u2502 Validated   \u2502     \u2502 Aggregated  \u2502\n\u2502 CRM Database \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 of source   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Cleaned     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Joined      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502 data        \u2502     \u2502 Standardized\u2502     \u2502 Business    \u2502\n\u2502 CSV Files    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502             \u2502     \u2502             \u2502     \u2502 Metrics     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502 No changes  \u2502     \u2502 Quality     \u2502     \u2502             \u2502\n\u2502 Event Stream \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 whatsoever  \u2502     \u2502 checked     \u2502     \u2502 Ready for   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 dashboards  \u2502\n                                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                      \ud83d\udce6 STORAGE:          \ud83d\udce6 STORAGE:        \ud83d\udce6 STORAGE:\n                      Everything           Reliable data      Curated datasets\n                      (even garbage)       for analysis       for specific uses\n</code></pre>"},{"location":"learning/data_engineering_101/#what-belongs-in-each-layer","title":"What Belongs in Each Layer?","text":""},{"location":"learning/data_engineering_101/#bronze-layer-raw","title":"\ud83d\udce6 Bronze Layer (Raw)","text":"<p>Purpose: Preserve the original data exactly as received.</p> <p>Contents: - Exact copies of source data - No transformations - No cleaning - No filtering</p> <p>Why keep raw data? - If something goes wrong, you can reprocess from scratch - If requirements change, you have the original data - For auditing and compliance, you can prove what was received</p> <p>Example Bronze table:</p> <pre><code>| _raw_json                                          | _source   | _loaded_at          |\n|----------------------------------------------------|-----------|---------------------|\n| {\"id\":1,\"name\":\"Alice\",\"date\":\"2024-03-15\"}       | crm_api   | 2024-03-15 10:00:00 |\n| {\"id\":2,\"name\":\"Bob\",\"date\":\"15/03/2024\"}         | crm_api   | 2024-03-15 10:00:00 |\n| {\"id\":\"X\",\"name\":null,\"date\":\"invalid\"}           | crm_api   | 2024-03-15 10:00:00 |\n</code></pre> <p>Even the bad data (ID \"X\" with null name and invalid date) is preserved.</p>"},{"location":"learning/data_engineering_101/#silver-layer-cleaned","title":"\ud83e\udd48 Silver Layer (Cleaned)","text":"<p>Purpose: Validated, cleaned, standardized data ready for analysis.</p> <p>Contents: - Data types enforced (dates are dates, numbers are numbers) - Invalid records filtered out or quarantined - Formats standardized (consistent date formats, normalized text) - Duplicates removed - Basic business logic applied</p> <p>Example Silver table:</p> <pre><code>| id   | name   | registration_date | _is_valid | _quality_score |\n|------|--------|-------------------|-----------|----------------|\n| 1    | Alice  | 2024-03-15        | true      | 100            |\n| 2    | Bob    | 2024-03-15        | true      | 100            |\n</code></pre> <p>Notice the bad record from Bronze didn't make it to Silver. It was quarantined because it had invalid values.</p>"},{"location":"learning/data_engineering_101/#gold-layer-business-ready","title":"\ud83e\udd47 Gold Layer (Business-Ready)","text":"<p>Purpose: Aggregated, joined, business-specific datasets for end users.</p> <p>Contents: - Pre-calculated metrics - Data from multiple Silver tables joined together - Optimized for specific use cases (dashboards, reports, ML models) - Business terminology (not technical column names)</p> <p>Example Gold table (Daily Sales Summary):</p> <pre><code>| report_date | region      | total_orders | total_revenue | avg_order_value |\n|-------------|-------------|--------------|---------------|-----------------|\n| 2024-03-15  | Northeast   | 1,234        | $98,456.78    | $79.78          |\n| 2024-03-15  | Southeast   | 987          | $76,543.21    | $77.55          |\n| 2024-03-15  | Midwest     | 1,567        | $123,456.89   | $78.78          |\n</code></pre> <p>This is what business users see in their dashboards. They don't need to know about raw JSON or data cleaning\u2014they just want the numbers.</p>"},{"location":"learning/data_engineering_101/#why-this-pattern-exists","title":"Why This Pattern Exists","text":"<ol> <li> <p>Separation of concerns: Each layer has one job. Bronze preserves. Silver cleans. Gold serves.</p> </li> <li> <p>Reprocessing: If cleaning logic changes, reprocess Silver from Bronze. If aggregation logic changes, reprocess Gold from Silver.</p> </li> <li> <p>Debugging: When something looks wrong in Gold, you can trace back to Silver, then Bronze, to find where the problem started.</p> </li> <li> <p>Performance: Gold tables are pre-aggregated, so dashboards load instantly instead of calculating on the fly.</p> </li> <li> <p>Access control: Raw data in Bronze might be sensitive. You can restrict access to Bronze while allowing wider access to sanitized Gold data.</p> </li> </ol>"},{"location":"learning/data_engineering_101/#how-odibi-fits-into-the-ecosystem","title":"How Odibi Fits Into the Ecosystem","text":"<p>There are many tools in the data engineering world. Here's how Odibi compares.</p>"},{"location":"learning/data_engineering_101/#odibi-vs-dbt","title":"Odibi vs dbt","text":"<p>dbt (data build tool) is popular for transforming data inside a warehouse using SQL.</p> Aspect dbt Odibi Primary language SQL Python (with YAML config) Transform location Inside the warehouse Flexible (Spark, Pandas, Polars) Focus Transformations only Full pipeline patterns Approach Write SQL, dbt manages Declarative patterns <p>When to use dbt: Your team is SQL-strong, your data is already in a warehouse, and you want a SQL-centric workflow.</p> <p>When to use Odibi: You need to work with data before it reaches the warehouse, want Python flexibility, or prefer pattern-based configuration over writing SQL.</p>"},{"location":"learning/data_engineering_101/#odibi-vs-airflow","title":"Odibi vs Airflow","text":"<p>Airflow is an orchestrator\u2014it schedules and monitors when jobs run, but it doesn't define what those jobs do.</p> Aspect Airflow Odibi Purpose Orchestration (when) Transformation (what) What it does Schedules tasks, manages dependencies Defines data transformations Scope Runs any task (data, ML, DevOps) Data pipelines specifically <p>Relationship: You can use Airflow to schedule Odibi jobs. Airflow says \"run this at 3 AM,\" Odibi says \"here's what to do when you run.\"</p>"},{"location":"learning/data_engineering_101/#odibi-vs-raw-sparkpandas","title":"Odibi vs Raw Spark/Pandas","text":"<p>Spark and Pandas are processing engines. They're extremely powerful but require you to write all the logic yourself.</p> Aspect Raw Spark/Pandas Odibi Approach Write code for everything Use pre-built patterns SCD2 logic Write 50+ lines of code One config option Data quality Build it yourself Built-in validation Learning curve Steep Gentler <p>Odibi adds patterns on top of these engines. Instead of writing the same boilerplate code for SCD2 (Slowly Changing Dimension Type 2) over and over, you declare what you want in YAML and Odibi generates the code.</p> <p>When to use raw Spark/Pandas: You need custom logic that doesn't fit any pattern, or you're learning how these engines work.</p> <p>When to use Odibi: You're building production pipelines and want to move fast with proven patterns.</p>"},{"location":"learning/data_engineering_101/#when-to-use-odibi","title":"When to Use Odibi","text":"<p>Odibi is a good fit when:</p> <p>\u2705 You're building data pipelines in Python \u2705 You want to use Spark, Pandas, or Polars \u2705 You need SCD2, merge patterns, or data quality checks \u2705 You prefer configuration over code \u2705 You're a small team (or solo) and need to move fast  </p> <p>Odibi might not be the best fit when:</p> <p>\u274c Your entire workflow is SQL-based (consider dbt) \u274c You need a scheduler/orchestrator (use Airflow, Dagster, or Prefect) \u274c You're doing real-time streaming (consider Kafka, Flink)  </p>"},{"location":"learning/data_engineering_101/#your-first-mental-model","title":"Your First Mental Model","text":"<p>Let's put everything together with a complete example. We'll trace data from source to dashboard.</p>"},{"location":"learning/data_engineering_101/#the-journey-source-bronze-silver-gold-dashboard","title":"The Journey: Source \u2192 Bronze \u2192 Silver \u2192 Gold \u2192 Dashboard","text":"<pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                    THE COMPLETE DATA JOURNEY\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    \ud83d\udcc4 SOURCE            \ud83d\udce6 BRONZE           \ud83e\udd48 SILVER           \ud83e\udd47 GOLD\n    (CSV file)          (Raw copy)         (Cleaned)          (Aggregated)\n        \u2502                   \u2502                  \u2502                  \u2502\n        \u25bc                   \u25bc                  \u25bc                  \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 orders  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  raw_   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 clean_  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 daily_  \u2502\n   \u2502  .csv   \u2502         \u2502 orders  \u2502        \u2502 orders  \u2502       \u2502 summary \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                  \u2502\n                                                                  \u25bc\n                                                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                            \u2502 Dashboard\u2502\n                                                            \u2502  \ud83d\udcca\ud83d\udcc8    \u2502\n                                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"learning/data_engineering_101/#step-1-sample-source-data-csv","title":"Step 1: Sample Source Data (CSV)","text":"<p>This is what arrives from the source system\u2014an orders CSV file:</p> <pre><code>order_id,customer_name,product,quantity,price,order_date,region\nORD001,Alice Smith,Widget A,2,29.99,2024-03-15,Northeast\nORD002,BOB JOHNSON,Widget B,1,$49.99,03/15/2024,northeast\nORD003,Carol White,Widget A,-1,29.99,2024-03-15,Southeast\nORD004,,Widget C,3,19.99,2024-03-15,Midwest\nORD001,Alice Smith,Widget A,2,29.99,2024-03-15,Northeast\nORD005,Dave Brown,Widget B,1,49.99,2024-03-99,West\n</code></pre> <p>Problems in this data: - \ud83d\udcdb Row 2: Name in ALL CAPS, price has \"$\", date format different, region lowercase - \u274c Row 3: Negative quantity (invalid) - \u274c Row 4: Missing customer name - \u274c Row 5: Duplicate of row 1 - \u274c Row 6: Invalid date (March 99th doesn't exist)</p>"},{"location":"learning/data_engineering_101/#step-2-bronze-layer-raw-copy","title":"Step 2: Bronze Layer (Raw Copy)","text":"<p>The Bronze layer stores this data exactly as received:</p> <pre><code>| order_id | customer_name | product  | quantity | price   | order_date | region    | _loaded_at          |\n|----------|---------------|----------|----------|---------|------------|-----------|---------------------|\n| ORD001   | Alice Smith   | Widget A | 2        | 29.99   | 2024-03-15 | Northeast | 2024-03-16 02:00:00 |\n| ORD002   | BOB JOHNSON   | Widget B | 1        | $49.99  | 03/15/2024 | northeast | 2024-03-16 02:00:00 |\n| ORD003   | Carol White   | Widget A | -1       | 29.99   | 2024-03-15 | Southeast | 2024-03-16 02:00:00 |\n| ORD004   |               | Widget C | 3        | 19.99   | 2024-03-15 | Midwest   | 2024-03-16 02:00:00 |\n| ORD001   | Alice Smith   | Widget A | 2        | 29.99   | 2024-03-15 | Northeast | 2024-03-16 02:00:00 |\n| ORD005   | Dave Brown    | Widget B | 1        | 49.99   | 2024-03-99 | West      | 2024-03-16 02:00:00 |\n</code></pre> <p>Nothing changed except adding a timestamp of when we loaded it. All the messy data is preserved.</p>"},{"location":"learning/data_engineering_101/#step-3-silver-layer-cleaned","title":"Step 3: Silver Layer (Cleaned)","text":"<p>The Silver layer applies transformations and validations:</p> <p>Transformations applied: 1. Standardize customer names to Title Case 2. Remove \"$\" from prices, convert to decimal 3. Parse dates to YYYY-MM-DD format 4. Standardize region names to Title Case 5. Remove duplicates based on order_id</p> <p>Validations applied: 1. Quantity must be positive 2. Customer name cannot be empty 3. Date must be valid</p> <p>Resulting Silver table:</p> <pre><code>| order_id | customer_name | product  | quantity | price | order_date | region    | _is_valid |\n|----------|---------------|----------|----------|-------|------------|-----------|-----------|\n| ORD001   | Alice Smith   | Widget A | 2        | 29.99 | 2024-03-15 | Northeast | \u2705        |\n| ORD002   | Bob Johnson   | Widget B | 1        | 49.99 | 2024-03-15 | Northeast | \u2705        |\n</code></pre> <p>Quarantined records (sent to a separate table for review):</p> <pre><code>| order_id | reason                              |\n|----------|-------------------------------------|\n| ORD003   | Quantity must be positive: -1       |\n| ORD004   | Customer name is required           |\n| ORD005   | Invalid date: 2024-03-99            |\n</code></pre> <p>Only 2 of the original 6 records (after deduplication) made it through. The bad records are quarantined so someone can investigate and fix the source.</p>"},{"location":"learning/data_engineering_101/#step-4-gold-layer-aggregated","title":"Step 4: Gold Layer (Aggregated)","text":"<p>The Gold layer aggregates for business use:</p> <p>Daily Sales Summary:</p> <pre><code>| report_date | region    | total_orders | total_quantity | total_revenue |\n|-------------|-----------|--------------|----------------|---------------|\n| 2024-03-15  | Northeast | 2            | 3              | $79.98        |\n</code></pre> <p>Product Performance:</p> <pre><code>| product  | units_sold | revenue |\n|----------|------------|---------|\n| Widget A | 2          | $59.98  |\n| Widget B | 1          | $49.99  |\n</code></pre>"},{"location":"learning/data_engineering_101/#step-5-dashboard-visualization","title":"Step 5: Dashboard Visualization","text":"<p>Finally, users see clean visualizations:</p> <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                    DAILY SALES DASHBOARD                              \u2551\n\u2551                       March 15, 2024                                  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                                       \u2551\n\u2551   TOTAL REVENUE        TOTAL ORDERS        AVG ORDER VALUE            \u2551\n\u2551   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2551\n\u2551   \u2502  $79.98     \u2502     \u2502     2       \u2502     \u2502   $39.99    \u2502            \u2551\n\u2551   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2551\n\u2551                                                                       \u2551\n\u2551   SALES BY REGION                    TOP PRODUCTS                     \u2551\n\u2551   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2551\n\u2551   \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Northeast  \u2502         \u2502 Widget A  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 $59.98\u2502     \u2551\n\u2551   \u2502            $79.98     \u2502         \u2502 Widget B  \u2588\u2588\u2588\u2588     $49.99\u2502     \u2551\n\u2551   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2551\n\u2551                                                                       \u2551\n\u2551   \u26a0\ufe0f DATA QUALITY ALERT: 3 records quarantined for review            \u2551\n\u2551                                                                       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>What the user sees: Clean numbers, clear charts, actionable insights.</p> <p>What they don't see: All the messy data, parsing errors, and transformations that happened behind the scenes.</p> <p>That's data engineering. You built the invisible plumbing that makes this dashboard possible.</p>"},{"location":"learning/data_engineering_101/#whats-next","title":"What's Next?","text":"<p>Congratulations! You now understand the fundamentals of data engineering:</p> <ul> <li>\u2705 What data is and where it lives</li> <li>\u2705 What data engineers do</li> <li>\u2705 How data pipelines work</li> <li>\u2705 ETL vs ELT approaches</li> <li>\u2705 Schemas and data types</li> <li>\u2705 Data quality fundamentals</li> <li>\u2705 The Medallion Architecture</li> <li>\u2705 How Odibi fits in the ecosystem</li> </ul>"},{"location":"learning/data_engineering_101/#continue-your-learning","title":"Continue Your Learning","text":"<p>\ud83d\udcd8 Curriculum - A structured learning path from beginner to advanced</p> <p>\ud83d\udcd6 Glossary - Quick definitions for data engineering terms</p> <p>\ud83d\udee0\ufe0f First Tutorial - Build your first Odibi pipeline</p>"},{"location":"learning/data_engineering_101/#key-concepts-to-explore-next","title":"Key Concepts to Explore Next","text":"<ol> <li>Slowly Changing Dimensions (SCD2) - How to track historical changes</li> <li>Data Modeling - Designing efficient schemas</li> <li>Orchestration - Scheduling and monitoring pipelines</li> <li>Testing - Validating that your pipelines work correctly</li> </ol>"},{"location":"learning/data_engineering_101/#remember","title":"Remember","text":"<p>Data engineering is a journey. You don't need to understand everything at once. Start with simple pipelines, add complexity gradually, and always focus on data quality.</p> <p>The best data engineers aren't those who know the most tools\u2014they're the ones who understand the fundamentals deeply and can apply them to any situation.</p> <p>Welcome to data engineering. \ud83c\udf89</p>"},{"location":"learning/glossary/","title":"Odibi Glossary","text":"<p>A beginner-friendly guide to every data engineering term you'll encounter in Odibi.</p>"},{"location":"learning/glossary/#a","title":"A","text":""},{"location":"learning/glossary/#aggregation","title":"Aggregation","text":"<p>What it is: Combining many rows of data into summary numbers\u2014like counting, averaging, or totaling.</p> <p>Real-world analogy: Imagine counting votes in an election. You don't care about each individual ballot; you just want the total for each candidate. That's aggregation.</p> <p>Example:</p> <pre><code>pattern: aggregation\naggregations:\n  - column: sales_amount\n    function: sum\n    alias: total_sales\n  - column: order_id\n    function: count\n    alias: order_count\ngroup_by:\n  - store_id\n  - sale_date\n</code></pre> <p>Why it matters: Raw data has millions of rows. Business users need summaries like \"total sales by store\" or \"average order value by month.\" Aggregation turns overwhelming detail into actionable insights.</p> <p>Learn more: Aggregation Pattern</p>"},{"location":"learning/glossary/#append","title":"Append","text":"<p>What it is: Adding new rows to a table without touching the rows that already exist.</p> <p>Real-world analogy: Adding new entries to a guest book. You write on the next blank page\u2014you don't erase or change what previous guests wrote.</p> <p>Example:</p> <pre><code>write_mode: append\n</code></pre> <p>Why it matters: When you receive daily sales data, you want to add today's transactions without accidentally deleting yesterday's. Append mode keeps your historical data safe.</p> <p>Learn more: Write Modes</p>"},{"location":"learning/glossary/#b","title":"B","text":""},{"location":"learning/glossary/#bronze-layer","title":"Bronze Layer","text":"<p>What it is: The first storage layer where raw data lands exactly as it arrived\u2014no cleaning, no changes.</p> <p>Real-world analogy: A mailroom. Letters arrive and get sorted into bins, but nobody opens or edits them. They're stored exactly as received.</p> <p>Example:</p> <pre><code>layer: bronze\nnodes:\n  - name: raw_sales\n    source: landing/sales_*.csv\n    write_mode: append\n    # No transformations - just store it raw\n</code></pre> <p>Why it matters: If something goes wrong later, you can always go back to the original data. Bronze is your \"undo button\" for the entire pipeline.</p> <p>Learn more: Medallion Architecture</p>"},{"location":"learning/glossary/#c","title":"C","text":""},{"location":"learning/glossary/#connection","title":"Connection","text":"<p>What it is: Saved credentials and settings that tell Odibi how to access a data source (database, file storage, API).</p> <p>Real-world analogy: A saved password in your browser. Instead of typing your username and password every time, you save it once and reuse it.</p> <p>Example:</p> <pre><code>connections:\n  warehouse_db:\n    type: postgres\n    host: db.company.com\n    port: 5432\n    database: analytics\n    # Credentials stored securely, not in YAML\n</code></pre> <p>Why it matters: Connections let you reuse access settings across many pipelines. Change the password once, and all pipelines using that connection keep working.</p> <p>Learn more: Connections Reference</p>"},{"location":"learning/glossary/#d","title":"D","text":""},{"location":"learning/glossary/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"<p>What it is: A map showing which pipeline steps must happen before others. \"Directed\" means arrows show order. \"Acyclic\" means no loops\u2014you can't go in circles.</p> <p>Real-world analogy: A recipe. You must chop vegetables before you can saut\u00e9 them. You can't frost a cake before baking it. The steps have a required order.</p> <p>Example:</p> <pre><code>[Load Sales] \u2192 [Clean Sales] \u2192 [Join with Products] \u2192 [Calculate Metrics]\n                                       \u2191\n                              [Load Products]\n</code></pre> <p>Why it matters: Odibi uses the DAG to know what can run in parallel (Load Sales and Load Products) and what must wait (Join can't start until both loads finish).</p> <p>Learn more: Pipeline Concepts</p>"},{"location":"learning/glossary/#data-quality","title":"Data Quality","text":"<p>What it is: Measuring whether your data is correct, complete, and trustworthy.</p> <p>Real-world analogy: Quality control in a factory. Before products ship, inspectors check for defects. Data quality is the same\u2014checking for missing values, wrong formats, or impossible numbers.</p> <p>Example:</p> <pre><code>validation:\n  rules:\n    - column: email\n      rule: not_null\n      severity: error\n    - column: age\n      rule: range\n      min: 0\n      max: 150\n      severity: warning\n    - column: order_total\n      rule: positive\n      severity: error\n</code></pre> <p>Why it matters: Bad data leads to bad decisions. If 20% of your sales records have missing amounts, your revenue reports are wrong. Data quality catches problems before they spread.</p> <p>Learn more: Validation Guide</p>"},{"location":"learning/glossary/#delta-lake","title":"Delta Lake","text":"<p>What it is: A smart file format that stores data in folders but adds superpowers: undo changes, time travel to past versions, and handle updates efficiently.</p> <p>Real-world analogy: Google Docs version history. You can see every change ever made, go back to any previous version, and multiple people can edit without conflicts.</p> <p>Example:</p> <pre><code>format: delta\nwrite_mode: merge\n# Delta enables merge, time travel, and ACID transactions\n</code></pre> <p>Why it matters: Regular files (CSV, Parquet) can't handle updates well\u2014you'd have to rewrite the entire file. Delta Lake lets you update just the rows that changed, and if something goes wrong, you can undo it.</p> <p>Learn more: Delta Lake Integration</p>"},{"location":"learning/glossary/#dimension-table","title":"Dimension Table","text":"<p>What it is: A lookup table containing descriptive information about things\u2014like products, customers, or locations.</p> <p>Real-world analogy: A phone book or contact list. It doesn't record what calls you made (that's a fact table). It just stores information about people: name, address, phone number.</p> <p>Example:</p> <pre><code>pattern: dimension\ntable_type: scd2\nnatural_key:\n  - customer_id\ntracked_columns:\n  - customer_name\n  - email\n  - address\n  - loyalty_tier\n</code></pre> <p>Why it matters: Dimension tables give meaning to your facts. A sales record might say \"customer_id: 12345 bought product_id: 789.\" The dimension tables tell you WHO customer 12345 is and WHAT product 789 is.</p> <p>Learn more: Dimension Pattern</p>"},{"location":"learning/glossary/#e","title":"E","text":""},{"location":"learning/glossary/#engine-spark-vs-pandas-vs-polars","title":"Engine (Spark vs Pandas vs Polars)","text":"<p>What it is: The processing tool that actually does the data work. Different engines handle different data sizes.</p> <p>Real-world analogy:  - Pandas = Kitchen blender. Great for small batches, easy to use. - Polars = Food processor. Faster than a blender, handles bigger jobs. - Spark = Industrial food processing plant. Handles massive volumes across many machines.</p> <p>Example:</p> <pre><code>engine: spark  # For big data (millions+ rows)\n# engine: pandas  # For small data (fits in memory)\n# engine: polars  # For medium data (fast single-machine)\n</code></pre> <p>Why it matters: Using Spark for 100 rows is overkill (slow startup). Using Pandas for 100 million rows crashes your computer. Picking the right engine means your pipeline runs efficiently.</p> <p>Learn more: Engine Guide</p>"},{"location":"learning/glossary/#etl-vs-elt","title":"ETL vs ELT","text":"<p>What it is: Two approaches to moving and transforming data. - ETL (Extract, Transform, Load): Clean data BEFORE storing it. - ELT (Extract, Load, Transform): Store raw data first, clean it AFTER.</p> <p>Real-world analogy:  - ETL = Sorting mail before putting it in your filing cabinet. - ELT = Dumping all mail in a box, then sorting when you need something.</p> <p>Example:</p> <pre><code># ELT approach (Odibi's default - medallion architecture)\n# 1. Load raw to Bronze (Extract, Load)\n# 2. Transform in Silver/Gold (Transform)\n\nbronze_node:\n  source: raw_file.csv\n  write_mode: append  # Just load it\n\nsilver_node:\n  source: bronze_table\n  transformations:    # Transform after loading\n    - type: clean_nulls\n    - type: standardize_dates\n</code></pre> <p>Why it matters: ELT is more flexible because you keep the raw data. If business rules change, you can re-transform from Bronze. ETL might have thrown away data you now need.</p> <p>Learn more: Pipeline Architecture</p>"},{"location":"learning/glossary/#f","title":"F","text":""},{"location":"learning/glossary/#fact-table","title":"Fact Table","text":"<p>What it is: A table storing events or transactions\u2014things that happened at a point in time with measurable values.</p> <p>Real-world analogy: Receipts. Each receipt records: when (timestamp), who (customer), what (products), and how much (amounts). That's a fact.</p> <p>Example:</p> <pre><code>pattern: fact\ntable_type: transaction\nnatural_key:\n  - order_id\n  - line_item_id\nmeasures:\n  - quantity\n  - unit_price\n  - discount_amount\n  - line_total\nforeign_keys:\n  - column: customer_id\n    references: dim_customer\n  - column: product_id\n    references: dim_product\n</code></pre> <p>Why it matters: Fact tables are where the numbers live. When someone asks \"What were our total sales last quarter?\", you're querying a fact table.</p> <p>Learn more: Fact Pattern</p>"},{"location":"learning/glossary/#foreign-key-fk","title":"Foreign Key (FK)","text":"<p>What it is: A column that links one table to another by referencing the other table's unique identifier.</p> <p>Real-world analogy: A reference on a job application. The application says \"Reference: Jane Smith, phone: 555-1234.\" That phone number is a \"foreign key\" linking to a person who exists elsewhere.</p> <p>Example:</p> <pre><code>validation:\n  foreign_key_checks:\n    - column: customer_id\n      reference_table: dim_customer\n      reference_column: customer_id\n      on_violation: quarantine  # Don't load orphan records\n</code></pre> <p>Why it matters: Foreign keys ensure data integrity. If an order references \"customer_id: 99999\" but that customer doesn't exist, something is wrong. FK validation catches these broken links.</p> <p>Learn more: FK Validation</p>"},{"location":"learning/glossary/#g","title":"G","text":""},{"location":"learning/glossary/#gold-layer","title":"Gold Layer","text":"<p>What it is: The final, business-ready layer with curated, aggregated, and report-ready data.</p> <p>Real-world analogy: A finished meal, plated and ready to serve. The raw ingredients (Bronze) were cleaned (Silver) and now it's restaurant-quality (Gold).</p> <p>Example:</p> <pre><code>layer: gold\nnodes:\n  - name: monthly_sales_summary\n    source: silver.fact_sales\n    pattern: aggregation\n    aggregations:\n      - column: total_amount\n        function: sum\n        alias: monthly_revenue\n    group_by:\n      - year\n      - month\n      - region\n</code></pre> <p>Why it matters: Business users and dashboards consume Gold tables directly. These are optimized for fast queries and contain pre-calculated metrics so reports load instantly.</p> <p>Learn more: Medallion Architecture</p>"},{"location":"learning/glossary/#i","title":"I","text":""},{"location":"learning/glossary/#idempotent","title":"Idempotent","text":"<p>What it is: An operation that gives the same result no matter how many times you run it.</p> <p>Real-world analogy: Pressing an elevator button. Pressing it once calls the elevator. Pressing it 10 more times doesn't call 10 elevators\u2014you get the same result.</p> <p>Example:</p> <pre><code># Idempotent merge - safe to rerun\nwrite_mode: merge\nmerge_keys:\n  - order_id\n# Running twice with same data = same result\n\n# NOT idempotent - DON'T do this for reruns\nwrite_mode: append\n# Running twice = duplicate rows!\n</code></pre> <p>Why it matters: Pipelines fail and get retried. If your pipeline isn't idempotent, retrying it corrupts your data (duplicates, wrong totals). Idempotent pipelines are safe to rerun.</p> <p>Learn more: Write Modes</p>"},{"location":"learning/glossary/#incremental-load","title":"Incremental Load","text":"<p>What it is: Only processing data that's new or changed since the last run, instead of reprocessing everything.</p> <p>Real-world analogy: Syncing photos to the cloud. Your phone doesn't upload all 10,000 photos every time\u2014just the new ones since last sync.</p> <p>Example:</p> <pre><code>incremental:\n  enabled: true\n  watermark_column: updated_at\n  lookback_period: 2 days\n# Only process rows where updated_at &gt; last_run_time - 2 days\n</code></pre> <p>Why it matters: Full reloads waste time and compute. If you have 5 years of data but only 1 day is new, why process all 5 years? Incremental loads are faster and cheaper.</p> <p>Learn more: Incremental Processing</p>"},{"location":"learning/glossary/#j","title":"J","text":""},{"location":"learning/glossary/#join","title":"Join","text":"<p>What it is: Combining rows from two or more tables based on matching values in a column.</p> <p>Real-world analogy: Matching students to their grades. The student roster has names and IDs. The grade sheet has IDs and scores. A join combines them so you see \"Name: Alice, Score: 95.\"</p> <p>Example:</p> <pre><code>transformations:\n  - type: join\n    right_source: dim_product\n    join_type: left\n    on:\n      - left: product_id\n        right: product_id\n    select:\n      - orders.*\n      - dim_product.product_name\n      - dim_product.category\n</code></pre> <p>Why it matters: Data lives in separate tables. Joins connect them. Without joins, you'd have order numbers but no customer names, product IDs but no descriptions.</p> <p>Learn more: Join Transformer</p>"},{"location":"learning/glossary/#m","title":"M","text":""},{"location":"learning/glossary/#medallion-architecture","title":"Medallion Architecture","text":"<p>What it is: A three-layer data organization pattern: Bronze (raw) \u2192 Silver (cleaned) \u2192 Gold (business-ready).</p> <p>Real-world analogy: A water treatment plant: - Bronze = Water from the lake (raw, unfiltered) - Silver = Filtered and treated (clean but not packaged) - Gold = Bottled water on store shelves (ready for consumers)</p> <p>Example:</p> <pre><code># Bronze: Land raw data\nbronze_orders:\n  source: kafka/orders_topic\n  layer: bronze\n  write_mode: append\n\n# Silver: Clean and validate\nsilver_orders:\n  source: bronze_orders\n  layer: silver\n  validation:\n    rules:\n      - column: order_id\n        rule: not_null\n\n# Gold: Aggregate for reports\ngold_daily_sales:\n  source: silver_orders\n  layer: gold\n  pattern: aggregation\n</code></pre> <p>Why it matters: This structure makes debugging easy (check Bronze for raw data), ensures data quality (Silver validates), and provides fast analytics (Gold is optimized for queries).</p> <p>Learn more: Architecture Guide</p>"},{"location":"learning/glossary/#merge-upsert","title":"Merge (Upsert)","text":"<p>What it is: A smart write that inserts new rows and updates existing rows in one operation. \"Upsert\" = Update + Insert.</p> <p>Real-world analogy: A contact list sync. New contacts get added. Existing contacts get their info updated (new phone number, new address). Nothing gets duplicated.</p> <p>Example:</p> <pre><code>write_mode: merge\nmerge_keys:\n  - customer_id\n# If customer_id exists \u2192 update the row\n# If customer_id is new \u2192 insert new row\n</code></pre> <p>Why it matters: Without merge, you'd have to delete all matching rows, then insert\u2014risky and slow. Merge handles both cases atomically, keeping your data consistent.</p> <p>Learn more: Merge Pattern</p>"},{"location":"learning/glossary/#n","title":"N","text":""},{"location":"learning/glossary/#natural-key","title":"Natural Key","text":"<p>What it is: A column (or columns) that uniquely identifies a row using real business data, not a generated number.</p> <p>Real-world analogy: Your email address or Social Security Number\u2014something from the real world that identifies you, not a made-up internal ID.</p> <p>Example:</p> <pre><code>natural_key:\n  - employee_id      # HR system's real ID\n  - effective_date   # For SCD2, identifies the version\n# NOT a surrogate key (generated number)\n</code></pre> <p>Why it matters: Natural keys connect your data to the real world. When someone asks about \"employee E12345,\" you can find them. Surrogate keys like \"row 847291\" mean nothing to business users.</p> <p>Learn more: Keys and Identifiers</p>"},{"location":"learning/glossary/#node","title":"Node","text":"<p>What it is: A single step in a pipeline that reads data, optionally transforms it, and writes output.</p> <p>Real-world analogy: A station on an assembly line. Each station does one job: one paints, one installs wheels, one does quality check. Together, they build a car.</p> <p>Example:</p> <pre><code>nodes:\n  - name: load_customers\n    source: raw/customers.csv\n    target: bronze.customers\n\n  - name: clean_customers\n    source: bronze.customers\n    target: silver.customers\n    transformations:\n      - type: trim_strings\n      - type: standardize_phone\n\n  - name: customer_metrics\n    source: silver.customers\n    target: gold.customer_360\n    pattern: aggregation\n</code></pre> <p>Why it matters: Breaking work into nodes makes pipelines easier to understand, debug, and maintain. If something fails, you know exactly which step broke.</p> <p>Learn more: Node Configuration</p>"},{"location":"learning/glossary/#o","title":"O","text":""},{"location":"learning/glossary/#orphan-record","title":"Orphan Record","text":"<p>What it is: A row with a foreign key value that doesn't exist in the parent table.</p> <p>Real-world analogy: A letter addressed to someone who doesn't live at that address. The recipient doesn't exist, so the letter has nowhere to go.</p> <p>Example:</p> <pre><code># Order has customer_id: 999\n# But dim_customer has no customer_id: 999\n# \u2192 This order is an orphan\n\nvalidation:\n  foreign_key_checks:\n    - column: customer_id\n      reference_table: dim_customer\n      on_violation: quarantine\n      # Orphans go to quarantine table for review\n</code></pre> <p>Why it matters: Orphan records break joins and analytics. Queries for \"sales by customer region\" can't work if the customer doesn't exist. Catching orphans prevents broken reports.</p> <p>Learn more: Orphan Detection</p>"},{"location":"learning/glossary/#p","title":"P","text":""},{"location":"learning/glossary/#pattern","title":"Pattern","text":"<p>What it is: A pre-built template for common data processing tasks. Instead of writing complex logic, you declare what pattern to use.</p> <p>Real-world analogy: A recipe. You don't invent how to make bread from scratch\u2014you follow a proven recipe. Patterns are tested recipes for data work.</p> <p>Example:</p> <pre><code># Instead of writing complex SCD2 logic...\npattern: scd2\nnatural_key:\n  - product_id\ntracked_columns:\n  - product_name\n  - price\n  - category\n# Odibi handles all the history tracking automatically\n</code></pre> <p>Available patterns: - <code>scd2</code> - History tracking with versioning - <code>merge</code> - Upsert operations - <code>aggregation</code> - Summarization - <code>dimension</code> - Lookup table management - <code>fact</code> - Transaction table handling</p> <p>Why it matters: Patterns encode best practices. Writing SCD2 logic from scratch takes hours and often has bugs. Using the pattern takes 5 lines and works correctly.</p> <p>Learn more: Patterns Reference</p>"},{"location":"learning/glossary/#pipeline","title":"Pipeline","text":"<p>What it is: A series of connected nodes that move data from sources to targets, with transformations along the way.</p> <p>Real-world analogy: An assembly line in a factory. Raw materials enter, go through stations (cutting, welding, painting), and finished products come out.</p> <p>Example:</p> <pre><code>pipeline:\n  name: daily_sales_pipeline\n  schedule: \"0 6 * * *\"  # 6 AM daily\n\n  nodes:\n    - name: extract_sales\n      source: pos_system.transactions\n      target: bronze.sales\n\n    - name: clean_sales\n      source: bronze.sales\n      target: silver.sales\n      depends_on: [extract_sales]\n\n    - name: aggregate_sales\n      source: silver.sales\n      target: gold.daily_summary\n      depends_on: [clean_sales]\n</code></pre> <p>Why it matters: Pipelines automate data flow. Instead of manually running scripts, pipelines run on schedule, handle failures gracefully, and process data consistently every time.</p> <p>Learn more: Pipeline Guide</p>"},{"location":"learning/glossary/#q","title":"Q","text":""},{"location":"learning/glossary/#quarantine","title":"Quarantine","text":"<p>What it is: A holding area for data that failed validation rules. Bad data is separated so it doesn't contaminate good data.</p> <p>Real-world analogy: Airport customs. If something suspicious is found in your luggage, it's held aside for inspection. It doesn't get through to the destination until it's reviewed.</p> <p>Example:</p> <pre><code>validation:\n  quarantine:\n    enabled: true\n    table: quarantine.failed_records\n    include_reason: true\n  rules:\n    - column: email\n      rule: regex\n      pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n      severity: error  # Failures go to quarantine\n</code></pre> <p>Why it matters: Without quarantine, bad data silently corrupts your analytics. With quarantine, good data flows through while problems are captured for review and correction.</p> <p>Learn more: Quarantine Setup</p>"},{"location":"learning/glossary/#s","title":"S","text":""},{"location":"learning/glossary/#schema","title":"Schema","text":"<p>What it is: The structure of a table\u2014what columns exist, what data type each column holds, and any constraints.</p> <p>Real-world analogy: A form template. It defines: Name (text), Age (number), Email (text with @ symbol). The schema says what information goes where and in what format.</p> <p>Example:</p> <pre><code>schema:\n  columns:\n    - name: customer_id\n      type: string\n      nullable: false\n    - name: email\n      type: string\n      nullable: true\n    - name: signup_date\n      type: date\n      nullable: false\n    - name: lifetime_value\n      type: decimal(10,2)\n      nullable: true\n</code></pre> <p>Why it matters: Schemas catch errors early. If someone tries to put \"hello\" in an integer column, the schema rejects it immediately instead of corrupting downstream reports.</p> <p>Learn more: Schema Definition</p>"},{"location":"learning/glossary/#scd-type-1","title":"SCD Type 1","text":"<p>What it is: Slowly Changing Dimension handling that overwrites old values with new ones. No history is kept.</p> <p>Real-world analogy: Updating your address with the post office. They replace your old address with the new one. They don't keep a record of where you used to live.</p> <p>Example:</p> <pre><code>pattern: scd1\nnatural_key:\n  - employee_id\n# Old values are overwritten:\n# Before: employee_id: 123, department: \"Sales\"\n# After:  employee_id: 123, department: \"Marketing\"\n# No history of \"Sales\" is kept\n</code></pre> <p>Why it matters: Use SCD1 when history doesn't matter (typo corrections, updated contact info). It's simpler and uses less storage than SCD2.</p> <p>Learn more: SCD Patterns</p>"},{"location":"learning/glossary/#scd-type-2","title":"SCD Type 2","text":"<p>What it is: Slowly Changing Dimension handling that keeps full history. Old values are marked as inactive; new values get new rows.</p> <p>Real-world analogy: A medical record. When your weight changes, the doctor doesn't erase the old weight\u2014they add a new entry with today's date. You can see your weight history over time.</p> <p>Example:</p> <pre><code>pattern: scd2\nnatural_key:\n  - customer_id\ntracked_columns:\n  - loyalty_tier\n  - region\nvalid_from_column: effective_start\nvalid_to_column: effective_end\nis_current_column: is_current\n</code></pre> <p>Result:</p> <pre><code>customer_id | loyalty_tier | effective_start | effective_end | is_current\n123         | Bronze       | 2023-01-01      | 2024-06-15    | false\n123         | Gold         | 2024-06-15      | 9999-12-31    | true\n</code></pre> <p>Why it matters: Historical analysis requires history. \"What tier was this customer when they made this purchase?\" Without SCD2, you can't answer that question.</p> <p>Learn more: SCD2 Pattern</p>"},{"location":"learning/glossary/#silver-layer","title":"Silver Layer","text":"<p>What it is: The middle layer where data is cleaned, validated, and standardized\u2014but not yet aggregated.</p> <p>Real-world analogy: A restaurant's prep kitchen. Raw ingredients (Bronze) are washed, chopped, and portioned (Silver). They're ready to cook but not yet finished dishes (Gold).</p> <p>Example:</p> <pre><code>layer: silver\nnodes:\n  - name: clean_orders\n    source: bronze.raw_orders\n    validation:\n      rules:\n        - column: order_id\n          rule: not_null\n        - column: total\n          rule: positive\n    transformations:\n      - type: deduplicate\n        keys: [order_id]\n      - type: standardize_dates\n        columns: [order_date]\n</code></pre> <p>Why it matters: Silver is your \"single source of truth.\" Bronze might have duplicates and errors. Silver has clean, validated data that Gold and other consumers can trust.</p> <p>Learn more: Medallion Architecture</p>"},{"location":"learning/glossary/#star-schema","title":"Star Schema","text":"<p>What it is: A database design where a central fact table connects to multiple dimension tables, forming a star shape.</p> <p>Real-world analogy: A wheel with spokes. The hub (fact table) is at the center. Each spoke leads to a dimension (who, what, where, when). All analysis starts at the center and reaches out.</p> <p>Diagram:</p> <pre><code>                    dim_customer\n                         |\n    dim_product ---- fact_sales ---- dim_date\n                         |\n                    dim_store\n</code></pre> <p>Example:</p> <pre><code># Fact at center\nfact_sales:\n  pattern: fact\n  foreign_keys:\n    - column: customer_id\n      references: dim_customer\n    - column: product_id\n      references: dim_product\n    - column: date_id\n      references: dim_date\n    - column: store_id\n      references: dim_store\n</code></pre> <p>Why it matters: Star schemas are optimized for analytics. Queries like \"sales by region by month by product category\" are fast because the structure matches how business users think.</p> <p>Learn more: Dimensional Modeling</p>"},{"location":"learning/glossary/#surrogate-key","title":"Surrogate Key","text":"<p>What it is: An internally generated unique identifier (usually a number) that has no business meaning. Created by the system, not from source data.</p> <p>Real-world analogy: A library book's barcode number. It's not the ISBN or title\u2014it's a number the library made up to track that specific copy internally.</p> <p>Example:</p> <pre><code>generate_surrogate_key:\n  column_name: customer_sk\n  strategy: hash  # or: sequence, uuid\n  source_columns:\n    - customer_id\n    - effective_start_date\n</code></pre> <p>Why it matters: Surrogate keys are stable (never change), performant (integers join faster than strings), and handle SCD2 (each version gets its own key). They're the internal \"address\" for each row.</p> <p>Learn more: Key Generation</p>"},{"location":"learning/glossary/#t","title":"T","text":""},{"location":"learning/glossary/#transformer","title":"Transformer","text":"<p>What it is: A reusable operation that modifies data\u2014like a function you can apply to any dataset.</p> <p>Real-world analogy: A coffee grinder. You put in beans (input), it grinds them (transformation), you get ground coffee (output). The same grinder works for any type of bean.</p> <p>Example:</p> <pre><code>transformations:\n  - type: rename_columns\n    mapping:\n      cust_nm: customer_name\n      ord_dt: order_date\n\n  - type: add_column\n    name: order_year\n    expression: \"year(order_date)\"\n\n  - type: filter\n    condition: \"order_total &gt; 0\"\n</code></pre> <p>Available transformers: - <code>rename_columns</code> - Change column names - <code>add_column</code> - Create calculated columns - <code>filter</code> - Keep only matching rows - <code>deduplicate</code> - Remove duplicate rows - <code>join</code> - Combine with other tables - And many more...</p> <p>Why it matters: Transformers are composable building blocks. Complex data processing becomes a readable list of simple steps.</p> <p>Learn more: Transformers Reference</p>"},{"location":"learning/glossary/#v","title":"V","text":""},{"location":"learning/glossary/#validation","title":"Validation","text":"<p>What it is: Checking that data meets defined rules before accepting it into your system.</p> <p>Real-world analogy: A bouncer at a club checking IDs. No valid ID? You don't get in. Validation checks if data \"has valid ID\" before letting it into your tables.</p> <p>Example:</p> <pre><code>validation:\n  rules:\n    # Must have a value\n    - column: order_id\n      rule: not_null\n      severity: error\n\n    # Must be a valid email format\n    - column: email\n      rule: regex\n      pattern: \"^[^@]+@[^@]+$\"\n      severity: warning\n\n    # Must be a real date\n    - column: order_date\n      rule: not_in_future\n      severity: error\n\n    # Must be positive\n    - column: quantity\n      rule: positive\n      severity: error\n</code></pre> <p>Severity levels: - <code>error</code> - Stop processing, quarantine the row - <code>warning</code> - Log the issue, continue processing</p> <p>Why it matters: Bad data in = bad decisions out. Validation catches problems at the door instead of letting them corrupt your analytics.</p> <p>Learn more: Validation Guide</p>"},{"location":"learning/glossary/#quick-reference-table","title":"Quick Reference Table","text":"Term One-Line Definition Aggregation Summarizing many rows into totals/averages Append Adding rows without changing existing ones Bronze Layer Raw data storage, untouched Connection Saved credentials for data sources DAG Map of step dependencies Data Quality Measuring data correctness Delta Lake Smart file format with versioning Dimension Table Lookup/reference data Engine Processing tool (Spark/Pandas/Polars) ETL vs ELT When transformation happens Fact Table Transaction/event data Foreign Key Link between tables Gold Layer Business-ready, curated data Idempotent Safe to run multiple times Incremental Load Only process new/changed data Join Combining data from multiple tables Medallion Architecture Bronze \u2192 Silver \u2192 Gold layering Merge Insert new, update existing Natural Key Business identifier Node Single pipeline step Orphan Record FK with no matching parent Pattern Reusable template for common tasks Pipeline Series of connected processing steps Quarantine Holding area for bad data Schema Structure of data SCD Type 1 Overwrite old with new SCD Type 2 Keep full history Silver Layer Cleaned, validated data Star Schema Facts center, dimensions around Surrogate Key Generated internal ID Transformer Reusable data operation Validation Checking data meets rules"},{"location":"learning/glossary/#next-steps","title":"Next Steps","text":"<ul> <li>New to Odibi? Start with Getting Started</li> <li>Building your first pipeline? See Tutorial</li> <li>Looking for specific syntax? Check YAML Schema Reference</li> </ul>"},{"location":"marketing/","title":"Odibi Marketing Campaign","text":"<p>A 6-month LinkedIn + Medium content campaign to build visibility for Odibi and establish expertise.</p>"},{"location":"marketing/#campaign-overview","title":"Campaign Overview","text":"Metric Value Duration 26 weeks (6 months) LinkedIn Posts 78 total (3/week) Medium Articles ~24 total (~1/week) Start Date Your choice End Goal Credibility, job opportunities, Odibi users"},{"location":"marketing/#campaign-files","title":"Campaign Files","text":""},{"location":"marketing/#linkedin-posts-copy-paste-ready","title":"LinkedIn Posts (Copy-Paste Ready)","text":"File Phase Weeks Posts campaign_phase1_credibility.md Credibility Building 1-2 6 campaign_phase2_building_in_public.md Building in Public 3-10 24 campaign_phase3_pattern_deep_dives.md Pattern Deep Dives 11-16 18 campaign_phase4_antipatterns.md Anti-Patterns 17-22 18 campaign_phase5_advanced_community.md Advanced &amp; Community 23-26 12"},{"location":"marketing/#medium-articles-full-content","title":"Medium Articles (Full Content)","text":"File Topic articles/article_01_origin_story.md Your journey from operations to data engineering articles/article_02_bronze_layer_mistake.md Why never to transform in Bronze articles/article_03_bronze_layer_setup.md Complete Bronze layer walkthrough articles/article_10_introducing_odibi.md Odibi launch article medium_articles.md Outlines for remaining articles <p>More full articles can be generated as needed.</p>"},{"location":"marketing/#quick-start","title":"Quick Start","text":"<ol> <li>Week 1: Start with Phase 1, Post 1.1 (Your Origin Story)</li> <li>Post 3x/week: Monday, Wednesday, Friday works well</li> <li>Time: 8-10am your timezone for best reach</li> <li>Engage: Respond to every comment within 24 hours</li> </ol>"},{"location":"marketing/#content-strategy","title":"Content Strategy","text":""},{"location":"marketing/#phase-1-dont-mention-odibi","title":"Phase 1: Don't Mention Odibi","text":"<p>Build credibility first. Share your story, lessons learned. No product pitch.</p>"},{"location":"marketing/#phase-2-introduce-odibi-gradually","title":"Phase 2: Introduce Odibi Gradually","text":"<p>By week 10, reveal that you've been using Odibi for the series.</p>"},{"location":"marketing/#phase-3-4-establish-expertise","title":"Phase 3-4: Establish Expertise","text":"<p>Deep technical content. Show you know your stuff.</p>"},{"location":"marketing/#phase-5-community-building","title":"Phase 5: Community Building","text":"<p>Invite collaboration, thank followers, call to action.</p>"},{"location":"marketing/#tone-guidelines","title":"Tone Guidelines","text":"<p>\u2705 Do: - Share real mistakes and lessons - Use \"I\" not \"we\" (personal brand) - Be humble: \"I'm still learning\" - Ask questions to drive engagement - Use specific numbers and examples</p> <p>\u274c Don't: - Mention your company by name - Complain about specific people - Be preachy or lecture-y - Use corporate speak - Oversell Odibi</p>"},{"location":"marketing/#dataset","title":"Dataset","text":"<p>The series uses the Brazilian E-Commerce dataset from Kaggle.</p> <p>Download and set it up before Week 3.</p>"},{"location":"marketing/#tracking-success","title":"Tracking Success","text":"<p>Track these weekly: - Post impressions - Engagement (likes, comments) - Profile views - Connection requests - Inbound messages</p> <p>Adjust based on what resonates.</p>"},{"location":"marketing/#timeline-example","title":"Timeline Example","text":"<p>If you start January 1st:</p> Month Weeks Phase Focus January 1-4 1-2 Credibility + Bronze/Silver February 5-8 2 Silver/Gold + Dimensions March 9-13 2-3 Odibi reveal + Patterns April 14-17 3-4 Patterns + Anti-patterns May 18-22 4 Anti-patterns + Debugging June 23-26 5 Advanced + Community"},{"location":"marketing/#after-the-campaign","title":"After the Campaign","text":"<ul> <li>Continue posting 1-2x/week</li> <li>Focus on what got engagement</li> <li>Respond to community questions</li> <li>Keep improving Odibi based on feedback</li> <li>Convert visibility into opportunities</li> </ul>"},{"location":"marketing/PUBLISHING_CALENDAR/","title":"Publishing Calendar - Odibi Marketing Campaign","text":"<p>Starting: Monday, January 6, 2025 Schedule: LinkedIn Mon/Wed/Fri, Medium on Tuesdays</p>"},{"location":"marketing/PUBLISHING_CALENDAR/#month-1-january-2025","title":"Month 1: January 2025","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#week-1-jan-6-10-credibility","title":"Week 1 (Jan 6-10) - Credibility","text":"Date Day Platform Content Jan 6 Mon LinkedIn Post 1.1 - Origin Story Jan 7 Tue Medium Article 1 - How I Taught Myself Data Engineering Jan 8 Wed LinkedIn Post 1.2 - Biggest Mistake Jan 10 Fri LinkedIn Post 1.3 - Learning Resources"},{"location":"marketing/PUBLISHING_CALENDAR/#week-2-jan-13-17-credibility","title":"Week 2 (Jan 13-17) - Credibility","text":"Date Day Platform Content Jan 13 Mon LinkedIn Post 2.1 - Working with Constraints Jan 14 Tue Medium Article 2 - The Bronze Layer Mistake Jan 15 Wed LinkedIn Post 2.2 - Solo Data Engineer Life Jan 17 Fri LinkedIn Post 2.3 - Building in Public Announcement"},{"location":"marketing/PUBLISHING_CALENDAR/#week-3-jan-20-24-bronze-layer","title":"Week 3 (Jan 20-24) - Bronze Layer","text":"Date Day Platform Content Jan 20 Mon LinkedIn Post 3.1 - Medallion Architecture Intro Jan 21 Tue Medium Article 3 - Setting Up a Bronze Layer Jan 22 Wed LinkedIn Post 3.2 - Bronze Layer Purpose Jan 24 Fri LinkedIn Post 3.3 - Metadata in Bronze"},{"location":"marketing/PUBLISHING_CALENDAR/#week-4-jan-27-31-data-contracts","title":"Week 4 (Jan 27-31) - Data Contracts","text":"Date Day Platform Content Jan 27 Mon LinkedIn Post 4.1 - Data Quality Intro Jan 28 Tue Medium Article 4 - Data Quality Contracts Jan 29 Wed LinkedIn Post 4.2 - Contract Types Jan 31 Fri LinkedIn Post 4.3 - Quarantine Pattern"},{"location":"marketing/PUBLISHING_CALENDAR/#month-2-february-2025","title":"Month 2: February 2025","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#week-5-feb-3-7-silver-layer","title":"Week 5 (Feb 3-7) - Silver Layer","text":"Date Day Platform Content Feb 3 Mon LinkedIn Post 5.1 - Silver Layer Purpose Feb 4 Tue Medium Article 5 - Complete Silver Layer Guide Feb 5 Wed LinkedIn Post 5.2 - Deduplication Feb 7 Fri LinkedIn Post 5.3 - Type Casting"},{"location":"marketing/PUBLISHING_CALENDAR/#week-6-feb-10-14-facts-vs-dimensions","title":"Week 6 (Feb 10-14) - Facts vs Dimensions","text":"Date Day Platform Content Feb 10 Mon LinkedIn Post 6.1 - Star Schema Intro Feb 11 Tue Medium Article 6 - Facts vs Dimensions Feb 12 Wed LinkedIn Post 6.2 - Surrogate Keys Feb 14 Fri LinkedIn Post 6.3 - Grain Explained"},{"location":"marketing/PUBLISHING_CALENDAR/#week-7-feb-17-21-date-dimension","title":"Week 7 (Feb 17-21) - Date Dimension","text":"Date Day Platform Content Feb 17 Mon LinkedIn Post 7.1 - Why Date Dimension Feb 18 Tue Medium Article 7 - Building a Date Dimension Feb 19 Wed LinkedIn Post 7.2 - Fiscal Calendars Feb 21 Fri LinkedIn Post 7.3 - Unknown Member"},{"location":"marketing/PUBLISHING_CALENDAR/#week-8-feb-24-28-fact-table-patterns","title":"Week 8 (Feb 24-28) - Fact Table Patterns","text":"Date Day Platform Content Feb 24 Mon LinkedIn Post 8.1 - Fact Table Anatomy Feb 25 Tue Medium Article 8 - Fact Table Patterns Feb 26 Wed LinkedIn Post 8.2 - Orphan Handling Feb 28 Fri LinkedIn Post 8.3 - Measures"},{"location":"marketing/PUBLISHING_CALENDAR/#month-3-march-2025","title":"Month 3: March 2025","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#week-9-mar-3-7-complete-walkthrough","title":"Week 9 (Mar 3-7) - Complete Walkthrough","text":"Date Day Platform Content Mar 3 Mon LinkedIn Post 9.1 - CSV to Star Schema Intro Mar 4 Tue Medium Article 9 - From CSV to Star Schema Mar 5 Wed LinkedIn Post 9.2 - Pipeline Architecture Mar 7 Fri LinkedIn Post 9.3 - Results Summary"},{"location":"marketing/PUBLISHING_CALENDAR/#week-10-mar-10-14-introducing-odibi","title":"Week 10 (Mar 10-14) - Introducing Odibi","text":"Date Day Platform Content Mar 10 Mon LinkedIn Post 10.1 - Framework Reveal Mar 11 Tue Medium Article 10 - Introducing Odibi Mar 12 Wed LinkedIn Post 10.2 - Core Concepts Mar 14 Fri LinkedIn Post 10.3 - Getting Started"},{"location":"marketing/PUBLISHING_CALENDAR/#week-11-mar-17-21-scd2-deep-dive","title":"Week 11 (Mar 17-21) - SCD2 Deep Dive","text":"Date Day Platform Content Mar 17 Mon LinkedIn Post 11.1 - SCD2 Intro Mar 18 Tue Medium Article 11 - SCD2 Complete Guide Mar 19 Wed LinkedIn Post 11.2 - When to Use SCD2 Mar 21 Fri LinkedIn Post 11.3 - SCD2 Gotchas"},{"location":"marketing/PUBLISHING_CALENDAR/#week-12-mar-24-28-dimension-patterns","title":"Week 12 (Mar 24-28) - Dimension Patterns","text":"Date Day Platform Content Mar 24 Mon LinkedIn Post 12.1 - Unknown Member Pattern Mar 25 Tue Medium Article 12 - Dimension Table Patterns Mar 26 Wed LinkedIn Post 12.2 - Conformed Dimensions Mar 28 Fri LinkedIn Post 12.3 - Role-Playing Dimensions"},{"location":"marketing/PUBLISHING_CALENDAR/#month-4-april-2025","title":"Month 4: April 2025","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#week-13-mar-31-apr-4-fact-table-design","title":"Week 13 (Mar 31 - Apr 4) - Fact Table Design","text":"Date Day Platform Content Mar 31 Mon LinkedIn Post 13.1 - Grain Deep Dive Apr 1 Tue Medium Article 13 - Fact Table Design Apr 2 Wed LinkedIn Post 13.2 - Measure Types Apr 4 Fri LinkedIn Post 13.3 - Factless Facts"},{"location":"marketing/PUBLISHING_CALENDAR/#week-14-apr-7-11-pre-aggregation","title":"Week 14 (Apr 7-11) - Pre-Aggregation","text":"Date Day Platform Content Apr 7 Mon LinkedIn Post 14.1 - Dashboard Performance Apr 8 Tue Medium Article 14 - Pre-Aggregation Strategies Apr 9 Wed LinkedIn Post 14.2 - Aggregation Grain Apr 11 Fri LinkedIn Post 14.3 - Incremental Aggregation"},{"location":"marketing/PUBLISHING_CALENDAR/#week-15-apr-14-18-data-quality-patterns","title":"Week 15 (Apr 14-18) - Data Quality Patterns","text":"Date Day Platform Content Apr 14 Mon LinkedIn Post 15.1 - Quality Framework Apr 15 Tue Medium Article 15 - Data Quality Patterns Apr 16 Wed LinkedIn Post 15.2 - Layer Strategy Apr 18 Fri LinkedIn Post 15.3 - Quality Metrics"},{"location":"marketing/PUBLISHING_CALENDAR/#week-16-apr-21-25-incremental-loading","title":"Week 16 (Apr 21-25) - Incremental Loading","text":"Date Day Platform Content Apr 21 Mon LinkedIn Post 16.1 - Full vs Incremental Apr 22 Tue Medium Article 16 - Incremental Loading Apr 23 Wed LinkedIn Post 16.2 - Watermarks Apr 25 Fri LinkedIn Post 16.3 - Merge Patterns"},{"location":"marketing/PUBLISHING_CALENDAR/#month-5-may-2025","title":"Month 5: May 2025","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#week-17-apr-28-may-2-bronze-anti-patterns","title":"Week 17 (Apr 28 - May 2) - Bronze Anti-Patterns","text":"Date Day Platform Content Apr 28 Mon LinkedIn Post 17.1 - Bronze Mistakes Intro Apr 29 Tue Medium Article 17 - Bronze Layer Anti-Patterns Apr 30 Wed LinkedIn Post 17.2 - Overwriting Bronze May 2 Fri LinkedIn Post 17.3 - Missing Metadata"},{"location":"marketing/PUBLISHING_CALENDAR/#week-18-may-5-9-silver-best-practices","title":"Week 18 (May 5-9) - Silver Best Practices","text":"Date Day Platform Content May 5 Mon LinkedIn Post 18.1 - Silver as SSOT May 6 Tue Medium Article 18 - Silver Layer Best Practices May 7 Wed LinkedIn Post 18.2 - Validation Strategy May 9 Fri LinkedIn Post 18.3 - Dedup Patterns"},{"location":"marketing/PUBLISHING_CALENDAR/#week-19-may-12-16-scd2-anti-patterns","title":"Week 19 (May 12-16) - SCD2 Anti-Patterns","text":"Date Day Platform Content May 12 Mon LinkedIn Post 19.1 - History Explosion May 13 Tue Medium Article 19 - SCD2 Done Wrong May 14 Wed LinkedIn Post 19.2 - Volatile Columns May 16 Fri LinkedIn Post 19.3 - Fixing Explosions"},{"location":"marketing/PUBLISHING_CALENDAR/#week-20-may-19-23-performance-anti-patterns","title":"Week 20 (May 19-23) - Performance Anti-Patterns","text":"Date Day Platform Content May 19 Mon LinkedIn Post 20.1 - Pipeline Performance May 20 Tue Medium Article 20 - Performance Anti-Patterns May 21 Wed LinkedIn Post 20.2 - Shuffle Problems May 23 Fri LinkedIn Post 20.3 - File Format Choices"},{"location":"marketing/PUBLISHING_CALENDAR/#week-21-may-26-30-configuration-patterns","title":"Week 21 (May 26-30) - Configuration Patterns","text":"Date Day Platform Content May 26 Mon LinkedIn Post 21.1 - Multi-Environment May 27 Tue Medium Article 21 - Configuration Patterns May 28 Wed LinkedIn Post 21.2 - Secret Management May 30 Fri LinkedIn Post 21.3 - Feature Flags"},{"location":"marketing/PUBLISHING_CALENDAR/#month-6-june-2025","title":"Month 6: June 2025","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#week-22-jun-2-6-debugging","title":"Week 22 (Jun 2-6) - Debugging","text":"Date Day Platform Content Jun 2 Mon LinkedIn Post 22.1 - Debug Process Jun 3 Tue Medium Article 22 - Debugging Data Pipelines Jun 4 Wed LinkedIn Post 22.2 - Common Errors Jun 6 Fri LinkedIn Post 22.3 - Recovery Patterns"},{"location":"marketing/PUBLISHING_CALENDAR/#week-23-jun-9-13-semantic-layer","title":"Week 23 (Jun 9-13) - Semantic Layer","text":"Date Day Platform Content Jun 9 Mon LinkedIn Post 23.1 - Metric Chaos Jun 10 Tue Medium Article 23 - Building a Semantic Layer Jun 11 Wed LinkedIn Post 23.2 - Metric Definitions Jun 13 Fri LinkedIn Post 23.3 - Materializations"},{"location":"marketing/PUBLISHING_CALENDAR/#week-24-jun-16-20-production-operations","title":"Week 24 (Jun 16-20) - Production Operations","text":"Date Day Platform Content Jun 16 Mon LinkedIn Post 24.1 - Production Checklist Jun 17 Tue Medium Article 24 - Production Data Pipelines Jun 18 Wed LinkedIn Post 24.2 - Monitoring Jun 20 Fri LinkedIn Post 24.3 - Retry &amp; Alerting"},{"location":"marketing/PUBLISHING_CALENDAR/#week-25-jun-23-27-lessons-learned","title":"Week 25 (Jun 23-27) - Lessons Learned","text":"Date Day Platform Content Jun 23 Mon LinkedIn Post 25.1 - Building in Public Reflections Jun 24 Tue Medium Article 25 - What I Learned Jun 25 Wed LinkedIn Post 25.2 - Key Lessons Jun 27 Fri LinkedIn Post 25.3 - Advice for Builders"},{"location":"marketing/PUBLISHING_CALENDAR/#week-26-jun-30-jul-4-wrap-up","title":"Week 26 (Jun 30 - Jul 4) - Wrap Up","text":"Date Day Platform Content Jun 30 Mon LinkedIn Post 26.1 - Series Recap Jul 1 Tue Medium Article 26 - Complete Index Jul 2 Wed LinkedIn Post 26.2 - Thank You Jul 4 Fri LinkedIn Post 26.3 - What's Next"},{"location":"marketing/PUBLISHING_CALENDAR/#quick-reference","title":"Quick Reference","text":""},{"location":"marketing/PUBLISHING_CALENDAR/#posting-times","title":"Posting Times","text":"<ul> <li>LinkedIn: 8-10 AM your timezone</li> <li>Medium: Any time Tuesday (aim for morning)</li> </ul>"},{"location":"marketing/PUBLISHING_CALENDAR/#weekly-checklist","title":"Weekly Checklist","text":"<ul> <li>[ ] Monday: Post LinkedIn teaser</li> <li>[ ] Tuesday: Publish Medium article</li> <li>[ ] Wednesday: Post LinkedIn (reference article)</li> <li>[ ] Friday: Post LinkedIn (engagement/recap)</li> <li>[ ] Daily: Respond to all comments</li> </ul>"},{"location":"marketing/PUBLISHING_CALENDAR/#tools","title":"Tools","text":"<ul> <li>LinkedIn: Native scheduler or Buffer</li> <li>Medium: Partner Program scheduler or manual</li> <li>Calendar: Set reminders for each publish date</li> </ul>"},{"location":"marketing/PUBLISHING_CALENDAR/#notes","title":"Notes","text":"<ul> <li>All LinkedIn posts are in <code>docs/marketing/campaign_phase*.md</code></li> <li>All Medium articles are in <code>docs/marketing/articles/</code></li> <li>Adjust dates if holidays fall on posting days</li> <li>Track engagement in a spreadsheet for optimization</li> </ul>"},{"location":"marketing/campaign_phase1_credibility/","title":"LinkedIn Campaign - Phase 1: Credibility Building","text":"<p>Duration: Weeks 1-2 (6 posts) Goal: Establish yourself as someone worth following before mentioning Odibi Tone: Humble, teaching from experience, relatable</p>"},{"location":"marketing/campaign_phase1_credibility/#week-1","title":"Week 1","text":""},{"location":"marketing/campaign_phase1_credibility/#post-11-your-origin-story-monday","title":"Post 1.1 - Your Origin Story (Monday)","text":"<p>Hook: I taught myself data engineering while working night shifts as a production supervisor.</p> <p>Body: In 2020, I graduated into a pandemic.</p> <p>I joined a rotational leadership program that started me in operations-12-hour night shifts, 3 on, 3 off, supervising production lines.</p> <p>It was hard, but it taught me how a business actually runs from the ground up.</p> <p>After 18 months, I moved into an engineering role. They asked me to map energy usage across an entire plant.</p> <p>I had no idea what I was doing.</p> <p>So I learned: - YouTube tutorials at 2am - Udemy courses on weekends - LinkedIn Learning during lunch</p> <p>Excel \u2192 Power BI \u2192 Azure \u2192 Python \u2192 Databricks</p> <p>1000+ hours of self-study in one year. Not because I'm special-because I was hungry to learn.</p> <p>5 years later, I'm the go-to data person for global operations at my company.</p> <p>The lesson? You don't need a CS degree. You need a problem you care about solving and the willingness to look stupid while you figure it out.</p> <p>What skill did you teach yourself the hard way?</p> <p>CTA: Engagement question</p>"},{"location":"marketing/campaign_phase1_credibility/#post-12-the-biggest-mistake-wednesday","title":"Post 1.2 - The Biggest Mistake (Wednesday)","text":"<p>Hook: My best work became my biggest headache.</p> <p>Body: Early in my data engineering journey, I built a pipeline that everyone loved.</p> <p>It pulled telemetry from 50+ sources, cleaned it, transformed it, and loaded it into a dashboard. One script. One run. Done.</p> <p>Leadership was impressed. I was proud.</p> <p>Then 3 months later: \"Can we add a new metric?\"</p> <p>I opened the script. 500+ lines. No functions. No comments. Just a wall of nested logic I barely recognized.</p> <p>I spent a week reverse-engineering my own code. Then I rewrote the entire thing.</p> <p>Here's what I learned:</p> <p>Monolithic code is a loan with interest. Fast to write, expensive to maintain.</p> <p>Now I follow three rules: 1. One function, one job. If it does two things, split it. 2. Name things like you'll forget what they do. Because you will. 3. If you can't explain a block of code in one sentence, it's too complex.</p> <p>The goal isn't clever code. It's code your future self won't hate you for.</p> <p>I wrote about my full journey from night shifts to data engineering on Medium. Link in comments.</p> <p>What's a shortcut that came back to haunt you?</p> <p>CTA: Link to Article 1 + Engagement question</p>"},{"location":"marketing/campaign_phase1_credibility/#post-13-what-i-wish-i-knew-friday","title":"Post 1.3 - What I Wish I Knew (Friday)","text":"<p>Hook: 5 things I wish I knew when I started in data engineering:</p> <p>Body: 1. SQL is 80% of the job. I spent months learning fancy tools. Should have mastered SQL first.</p> <ol> <li> <p>The hardest part isn't technical. It's getting access to data, understanding what business actually needs, and communicating with non-technical stakeholders.</p> </li> <li> <p>Documentation is not optional. Future you will forget why you did something. Write it down.</p> </li> <li> <p>Rewriting code is normal. I've rebuilt my pipelines 4+ times as I learned better approaches. That's growth, not failure.</p> </li> <li> <p>You don't need permission to learn. Nobody asked me to learn Python or Azure. I just started. Most valuable career decision I've made.</p> </li> </ol> <p>What would you add to this list?</p> <p>CTA: Engagement question</p>"},{"location":"marketing/campaign_phase1_credibility/#week-2","title":"Week 2","text":""},{"location":"marketing/campaign_phase1_credibility/#post-21-working-with-constraints-monday","title":"Post 2.1 - Working With Constraints (Monday)","text":"<p>Hook: I built a data platform with almost no direct database access.</p> <p>Body: When I started managing data for operations, I had to get creative.</p> <p>In large organizations, data access is governed carefully-and for good reason. Security matters.</p> <p>But I still needed to deliver value.</p> <p>So I found solutions: - Queried semantic models instead of source tables - Used Power Automate to move data in chunks - Built pipelines in Azure Data Factory - Partnered with IT and eventually earned a Databricks workspace</p> <p>Constraints force creativity. And building relationships opened doors.</p> <p>3 things I learned: 1. Build relationships. The Azure admin who helped me provision resources became a key ally. 2. Show value first. Every small win built trust for bigger asks. 3. Document everything. When you're doing things unconventionally, you need a paper trail.</p> <p>Resources are always limited. The question is: what can you build with what you have?</p> <p>CTA: Engagement question</p>"},{"location":"marketing/campaign_phase1_credibility/#post-22-the-solo-data-engineer-wednesday","title":"Post 2.2 - The Solo Data Engineer (Wednesday)","text":"<p>Hook: Being the only data engineer on a team of analysts is a unique challenge.</p> <p>Body: When you're the only one who does what you do: - You make architecture decisions independently - You set your own standards - You're the expert by default</p> <p>It pushes you to: - Document obsessively (future you is your only teammate) - Build systems, not scripts (you can't maintain 50 one-off notebooks) - Learn to explain technical concepts simply (your stakeholders aren't technical) - Automate everything (you don't have time to babysit pipelines)</p> <p>The upside? You own the entire stack. You see problems end-to-end. You learn faster because there's no one else to hand things off to.</p> <p>If you're a solo data person on your team: you're not alone. Many of us are building in isolation.</p> <p>What's the hardest part of being the only data person on your team?</p> <p>CTA: Engagement question</p>"},{"location":"marketing/campaign_phase1_credibility/#post-23-building-in-public-announcement-friday","title":"Post 2.3 - Building in Public Announcement (Friday)","text":"<p>Hook: I'm going to build a data warehouse in public.</p> <p>Body: Over the next few months, I'm going to take a public dataset and walk through the entire process:</p> <p>\ud83d\udce6 Bronze - Landing raw data \ud83e\uddf9 Silver - Cleaning and validating \u2b50 Gold - Building dimensions and facts \ud83d\udcca Output - Ready for dashboards</p> <p>I'll share: - What I do and why - Mistakes I make along the way - Code and configuration you can reuse</p> <p>Why? Because I learned data engineering the hard way-piecing together YouTube videos and documentation at 2am.</p> <p>I want to create the resource I wish I had.</p> <p>Dataset: Brazilian E-Commerce (100k orders, customers, products, reviews)</p> <p>First post drops next week.</p> <p>Follow along if you're interested. Corrections and suggestions welcome-I'm still learning too.</p> <p>I also wrote about the Bronze layer mistake that taught me to never transform raw data. Link in comments.</p> <p>CTA: Link to Article 2 + Teaser for Phase 2</p>"},{"location":"marketing/campaign_phase1_credibility/#medium-articles-for-phase-1","title":"Medium Articles for Phase 1","text":""},{"location":"marketing/campaign_phase1_credibility/#article-11-how-i-taught-myself-data-engineering-while-working-night-shifts","title":"Article 1.1: \"How I Taught Myself Data Engineering While Working Night Shifts\"","text":"<ul> <li>Expanded version of Post 1.1</li> <li>Include specific resources that helped</li> <li>Timeline of skill progression</li> <li>Advice for others in similar situations</li> </ul>"},{"location":"marketing/campaign_phase1_credibility/#article-12-the-bronze-layer-mistake-that-cost-me-6-months-of-data","title":"Article 1.2: \"The Bronze Layer Mistake That Cost Me 6 Months of Data\"","text":"<ul> <li>Expanded version of Post 1.2</li> <li>Technical deep dive on medallion architecture</li> <li>Code examples of correct Bronze layer patterns</li> <li>Link to Odibi docs (soft intro)</li> </ul>"},{"location":"marketing/campaign_phase1_credibility/#posting-schedule","title":"Posting Schedule","text":"Week Day Post Medium Article 1 Mon 1.1 - Origin Story Article 1.1 (later in week) 1 Wed 1.2 - Biggest Mistake - 1 Fri 1.3 - What I Wish I Knew Article 1.2 (later in week) 2 Mon 2.1 - Working With Constraints - 2 Wed 2.2 - Solo Data Engineer - 2 Fri 2.3 - Building in Public Announcement -"},{"location":"marketing/campaign_phase1_credibility/#notes","title":"Notes","text":"<ul> <li>Do not mention Odibi in Phase 1 - focus on credibility</li> <li>Do not mention company name - keep it generic</li> <li>Respond to every comment - algorithm rewards engagement</li> <li>Post between 8-10am your timezone for best reach</li> </ul>"},{"location":"marketing/campaign_phase2_building_in_public/","title":"LinkedIn Campaign - Phase 2: Building a Data Warehouse in Public","text":"<p>Duration: Weeks 3-10 (24 posts) Goal: Walk through Bronze \u2192 Silver \u2192 Gold using Brazilian E-Commerce dataset Tone: Teaching, humble, showing real work Dataset: Brazilian E-Commerce</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-3-introduction-bronze-layer","title":"Week 3: Introduction &amp; Bronze Layer","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-31-meet-the-dataset-monday","title":"Post 3.1 - Meet the Dataset (Monday)","text":"<p>Hook: 100,000 orders. 8 CSV files. Let's build a data warehouse.</p> <p>Body: I'm using the Brazilian E-Commerce dataset from Kaggle.</p> <p>It has everything we need: - \ud83d\udce6 Orders (100k transactions) - \ud83d\udc65 Customers (with locations) - \ud83d\udecd\ufe0f Products (with categories) - \u2b50 Reviews (with scores and comments) - \ud83d\udcb0 Payments (with methods and amounts)</p> <p>Real data. Real messiness. Real problems to solve.</p> <p>Here's what the raw files look like:</p> <pre><code>olist_orders_dataset.csv\nolist_customers_dataset.csv\nolist_products_dataset.csv\nolist_order_items_dataset.csv\nolist_order_payments_dataset.csv\nolist_order_reviews_dataset.csv\nolist_sellers_dataset.csv\nolist_geolocation_dataset.csv\n</code></pre> <p>Over the next few weeks, I'll transform this into: - dim_customer - dim_product - dim_seller - dim_date - fact_orders</p> <p>First step? Land everything in Bronze. No transformations. Just store it.</p> <p>More tomorrow.</p> <p>CTA: Follow along</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-32-the-bronze-layer-philosophy-wednesday","title":"Post 3.2 - The Bronze Layer Philosophy (Wednesday)","text":"<p>Hook: The Bronze layer has one job: don't lose anything.</p> <p>Body: When data arrives, I store it exactly as-is: - Same column names (even if ugly) - Same data types (even if wrong) - Same duplicates (yes, really) - Same nulls (all of them)</p> <p>Why?</p> <p>Because I don't know what I don't know.</p> <p>That \"garbage\" column I want to delete? Business might need it next month. That duplicate I want to remove? It might reveal a source system bug. That null I want to fill? It might mean something specific.</p> <p>Bronze is my insurance policy.</p> <p>Here's my Bronze config:</p> <pre><code>nodes:\n  - name: bronze_orders\n    read:\n      connection: landing\n      path: olist_orders_dataset.csv\n      format: csv\n    write:\n      connection: bronze\n      path: orders\n      format: delta\n      mode: append\n</code></pre> <p>No transforms. Just land it.</p> <p>The only thing I add: <code>_extracted_at</code> timestamp so I know when data arrived.</p> <p>Full article with complete config on Medium \u2192 [link]</p> <p>CTA: Link to Medium</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-33-setting-up-the-project-friday","title":"Post 3.3 - Setting Up the Project (Friday)","text":"<p>Hook: Here's the complete Bronze layer config for 8 CSV files.</p> <p>Body: Every file gets its own node. Every node does the same thing: 1. Read from landing zone 2. Add extraction timestamp 3. Write to Bronze as Delta</p> <pre><code>pipelines:\n  - pipeline: bronze_ecommerce\n    layer: bronze\n    nodes:\n      - name: bronze_orders\n        read:\n          connection: landing\n          path: olist_orders_dataset.csv\n\n      - name: bronze_customers\n        read:\n          connection: landing\n          path: olist_customers_dataset.csv\n\n      - name: bronze_products\n        read:\n          connection: landing\n          path: olist_products_dataset.csv\n\n      # ... 5 more files\n</code></pre> <p>Total setup time: 15 minutes.</p> <p>Now I have: - \u2705 All raw data preserved - \u2705 Delta format (versioning, time travel) - \u2705 Timestamps for debugging - \u2705 Foundation for Silver layer</p> <p>Next week: Start cleaning in Silver.</p> <p>Full YAML on Medium \u2192 [link]</p> <p>CTA: Link to Medium</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-4-silver-layer-cleaning-basics","title":"Week 4: Silver Layer - Cleaning Basics","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-41-welcome-to-silver-monday","title":"Post 4.1 - Welcome to Silver (Monday)","text":"<p>Hook: Bronze stores everything. Silver decides what's worth keeping.</p> <p>Body: Silver layer is where I: - Remove true duplicates - Handle nulls intentionally - Standardize formats - Apply business rules - Validate data quality</p> <p>But NOT where I: - Aggregate - Join across domains - Create business metrics</p> <p>Silver = clean, validated, single-source-of-truth</p> <p>Think of it like a kitchen: - Bronze = raw ingredients from the store (still in packaging) - Silver = washed, chopped, prepped (ready to cook) - Gold = finished dishes (ready to serve)</p> <p>Today's task: Clean the orders table.</p> <p>Problems I found: - 8 orders with null customer IDs - Dates stored as strings - Status values with inconsistent casing</p> <p>Let's fix them.</p> <p>CTA: Continue tomorrow</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-42-handling-nulls-wednesday","title":"Post 4.2 - Handling Nulls (Wednesday)","text":"<p>Hook: Nulls aren't errors. They're information.</p> <p>Body: 8 orders have no customer ID.</p> <p>Options: 1. \u274c Delete them (lose data) 2. \u274c Fill with \"UNKNOWN\" (hide the problem) 3. \u2705 Keep them, flag them, investigate</p> <p>Here's what I actually do:</p> <pre><code>- name: silver_orders\n  read:\n    connection: bronze\n    path: orders\n\n  validation:\n    contracts:\n      - type: not_null\n        columns: [order_id]\n        severity: error  # These MUST exist\n\n      - type: not_null\n        columns: [customer_id]\n        severity: warn   # Flag but don't fail\n</code></pre> <p>Orders without customer_id flow through but get flagged.</p> <p>I can query later:</p> <pre><code>SELECT * FROM silver_orders \nWHERE customer_id IS NULL\n</code></pre> <p>Now I have visibility without losing data.</p> <p>Lesson: Not every null is a problem to fix. Some are problems to understand.</p> <p>I wrote a full guide on data quality contracts on Medium. Link in comments.</p> <p>CTA: Link to Article 4 + Engagement question</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-43-deduplication-friday","title":"Post 4.3 - Deduplication (Friday)","text":"<p>Hook: Same order ID appearing twice. Which one do I keep?</p> <p>Body: Found 23 duplicate order IDs in Bronze.</p> <p>Possible causes: - Source sent same record twice - Extract job ran twice - Legitimate updates to same order</p> <p>How do I know which?</p> <p>I look at the <code>_extracted_at</code> timestamp I added in Bronze.</p> <p>Same timestamp = duplicate extract (keep one) Different timestamp = update (keep latest)</p> <pre><code>transformer: deduplicate\nparams:\n  keys: [order_id]\n  order_by: \"_extracted_at DESC\"\n</code></pre> <p>This keeps the most recent version of each order.</p> <p>Before: 100,023 rows After: 100,000 rows</p> <p>23 duplicates removed. Zero data lost-I can always check Bronze if needed.</p> <p>This is why Bronze exists. It's my safety net.</p> <p>Full guide on setting up a Bronze layer with Delta Lake on Medium. Link in comments.</p> <p>CTA: Link to Article 4 + Follow for more</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-5-silver-layer-data-quality","title":"Week 5: Silver Layer - Data Quality","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-51-data-contracts-monday","title":"Post 5.1 - Data Contracts (Monday)","text":"<p>Hook: A data contract is a promise your pipeline makes.</p> <p>Body: Before I load data, I check: - Are required columns present? - Are values within expected ranges? - Does the data match expected patterns?</p> <p>If not, I want to know BEFORE it breaks a dashboard.</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id]\n\n  - type: accepted_values\n    column: order_status\n    values: [delivered, shipped, canceled, processing]\n\n  - type: row_count\n    min: 1000  # Expect at least 1000 orders\n</code></pre> <p>If any contract fails, the pipeline stops.</p> <p>No silent failures. No \"why is the dashboard empty?\" at 9am.</p> <p>This saved me last month when a source system started sending nulls for a critical column. Contract caught it before it hit production.</p> <p>CTA: Engagement</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-52-date-standardization-wednesday","title":"Post 5.2 - Date Standardization (Wednesday)","text":"<p>Hook: Dates stored as strings are ticking time bombs.</p> <p>Body: The orders table has dates like: - \"2018-06-04 12:00:00\" - \"2018-6-4\" - \"06/04/2018\"</p> <p>All valid. All different formats. All will break something eventually.</p> <p>Silver layer standardizes:</p> <pre><code>transform:\n  steps:\n    - function: cast_columns\n      params:\n        columns:\n          order_purchase_timestamp: timestamp\n          order_delivered_timestamp: timestamp\n          order_estimated_delivery: date\n</code></pre> <p>Now every downstream consumer gets consistent types.</p> <p>Bonus: timestamps enable time-based queries:</p> <pre><code>SELECT * FROM orders \nWHERE order_purchase_timestamp &gt;= '2018-01-01'\n</code></pre> <p>This fails with string dates. Works perfectly with proper timestamps.</p> <p>CTA: Small thing, big impact</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-53-silver-orders-complete-friday","title":"Post 5.3 - Silver Orders Complete (Friday)","text":"<p>Hook: Here's the complete Silver orders config.</p> <p>Body: Everything together:</p> <pre><code>- name: silver_orders\n  read:\n    connection: bronze\n    path: orders\n\n  contracts:\n    - type: not_null\n      columns: [order_id]\n    - type: accepted_values\n      column: order_status\n      values: [delivered, shipped, canceled, processing, created, approved, invoiced, unavailable]\n\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: \"_extracted_at DESC\"\n\n  transform:\n    steps:\n      - function: cast_columns\n        params:\n          columns:\n            order_purchase_timestamp: timestamp\n            order_delivered_timestamp: timestamp\n      - function: clean_text\n        params:\n          columns: [order_status]\n          lowercase: true\n\n  write:\n    connection: silver\n    path: orders\n    format: delta\n</code></pre> <p>Result: - \u2705 Duplicates removed - \u2705 Dates standardized - \u2705 Text normalized - \u2705 Contracts validated</p> <p>One table down. Time to do the same for customers, products, and the rest.</p> <p>Complete Silver layer configuration guide on Medium. Link in comments.</p> <p>CTA: Link to Article 5</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-6-dimensional-modeling-introduction","title":"Week 6: Dimensional Modeling Introduction","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-61-facts-vs-dimensions-monday","title":"Post 6.1 - Facts vs Dimensions (Monday)","text":"<p>Hook: Every data warehouse has two types of tables: facts and dimensions.</p> <p>Body: Facts = What happened (events, transactions, measurements) - Orders placed - Payments made - Items purchased</p> <p>Dimensions = Context about what happened (who, what, where, when) - Customers (who bought) - Products (what was bought) - Dates (when it happened) - Locations (where)</p> <p>Facts have numbers you sum/count/average. Dimensions have attributes you filter/group by.</p> <p>In our e-commerce data: - <code>fact_orders</code> = order_id, customer_sk, product_sk, amount, quantity - <code>dim_customer</code> = customer_sk, customer_id, city, state - <code>dim_product</code> = product_sk, product_id, category, weight - <code>dim_date</code> = date_sk, full_date, month, quarter, year</p> <p>The \"sk\" = surrogate key. More on that Wednesday.</p> <p>CTA: Facts = numbers, Dimensions = context</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-62-whats-a-surrogate-key-wednesday","title":"Post 6.2 - What's a Surrogate Key? (Wednesday)","text":"<p>Hook: A surrogate key is an ID that means nothing-and that's the point.</p> <p>Body: Source systems have natural keys: - customer_id = \"abc123xyz\" - product_id = \"PROD-7890\"</p> <p>These are fine until: - The source system changes its ID format - Two source systems use the same ID for different things - You need to track history (same customer, different versions)</p> <p>Surrogate keys solve this:</p> <pre><code>customer_sk = 1, 2, 3, 4...\n</code></pre> <p>Just integers. Generated by your warehouse. Never change.</p> <p>Your fact table uses surrogate keys:</p> <pre><code>order_id | customer_sk | product_sk | amount\n1001     | 42          | 156        | 99.00\n</code></pre> <p>To get customer name, join to dim_customer on customer_sk.</p> <p>Why integers? - Joins are faster - Storage is smaller - No format surprises</p> <p>I generate them like this:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n</code></pre> <p>CTA: More on dimension building Friday</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-63-building-dim_customer-friday","title":"Post 6.3 - Building dim_customer (Friday)","text":"<p>Hook: Let's build our first dimension table.</p> <p>Body: Source: silver_customers (99,441 unique customers)</p> <p>Target: dim_customer with: - customer_sk (generated) - customer_id (from source) - city - state - created_at</p> <pre><code>- name: dim_customer\n  read:\n    connection: silver\n    path: customers\n\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 1\n      unknown_member: true\n      audit:\n        load_timestamp: true\n\n  write:\n    connection: gold\n    table: dim_customer\n    format: delta\n</code></pre> <p>What this does: 1. Generates customer_sk (1, 2, 3...) 2. Adds row with customer_sk = 0 for unknown/missing customers 3. Adds load_timestamp for auditing 4. SCD Type 1 = overwrite on change (no history)</p> <p>Result: 99,442 rows (99,441 customers + 1 unknown member)</p> <p>Why unknown member? So fact tables can still join even when customer_id is null.</p> <p>Full guide on facts vs dimensions with code examples on Medium. Link in comments.</p> <p>CTA: Link to Article 6</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-7-more-dimensions","title":"Week 7: More Dimensions","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-71-building-dim_product-monday","title":"Post 7.1 - Building dim_product (Monday)","text":"<p>Hook: Products have categories. Categories have hierarchies. Let's model it.</p> <p>Body: Products table has: - product_id - category_name (in Portuguese) - weight, length, height, width</p> <p>I want dim_product with: - product_sk (generated) - product_id - category (cleaned up) - physical attributes</p> <pre><code>- name: dim_product\n  read:\n    connection: silver\n    path: products\n\n  pattern:\n    type: dimension\n    params:\n      natural_key: product_id\n      surrogate_key: product_sk\n      scd_type: 1\n      unknown_member: true\n</code></pre> <p>Cleaning step-category names need work:</p> <pre><code>transform:\n  steps:\n    - function: clean_text\n      params:\n        columns: [product_category_name]\n        trim: true\n        lowercase: true\n    - function: fill_nulls\n      params:\n        columns: [product_category_name]\n        value: \"unknown\"\n</code></pre> <p>32,951 products \u2192 dim_product ready.</p> <p>CTA: Simple pattern, reusable</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-72-building-dim_date-wednesday","title":"Post 7.2 - Building dim_date (Wednesday)","text":"<p>Hook: Every data warehouse needs a date dimension. Here's how to generate one.</p> <p>Body: Date dimensions aren't loaded from source-they're generated.</p> <p>I need dates from 2016-01-01 to 2025-12-31:</p> <pre><code>- name: dim_date\n  pattern:\n    type: date_dimension\n    params:\n      start_date: \"2016-01-01\"\n      end_date: \"2025-12-31\"\n      fiscal_year_start_month: 1\n      unknown_member: true\n\n  write:\n    connection: gold\n    table: dim_date\n    format: delta\n</code></pre> <p>This generates: - date_sk (20160101, 20160102, ...) - full_date - day_of_week, day_of_month, day_of_year - week_of_year - month, month_name - quarter, quarter_name - year - fiscal_year, fiscal_quarter - is_weekend, is_month_end</p> <p>3,653 rows. Zero source data needed.</p> <p>Now fact tables can join on date_sk for any time-based analysis.</p> <p>I wrote a complete guide on building a date dimension from scratch on Medium. Link in comments.</p> <p>CTA: Link to Article 7</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-73-building-dim_seller-friday","title":"Post 7.3 - Building dim_seller (Friday)","text":"<p>Hook: 3,095 sellers. Same pattern. 5 minutes.</p> <p>Body:</p> <pre><code>- name: dim_seller\n  read:\n    connection: silver\n    path: sellers\n\n  pattern:\n    type: dimension\n    params:\n      natural_key: seller_id\n      surrogate_key: seller_sk\n      scd_type: 1\n      unknown_member: true\n\n  write:\n    connection: gold\n    table: dim_seller\n</code></pre> <p>That's it.</p> <p>Once you understand the pattern, dimensions are fast.</p> <p>Checklist: - [x] dim_customer (99,442 rows) - [x] dim_product (32,952 rows) - [x] dim_date (3,653 rows) - [x] dim_seller (3,096 rows)</p> <p>Next week: The fact table. That's where it gets interesting.</p> <p>CTA: Fact table coming Monday</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-8-fact-tables","title":"Week 8: Fact Tables","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-81-anatomy-of-a-fact-table-monday","title":"Post 8.1 - Anatomy of a Fact Table (Monday)","text":"<p>Hook: Fact tables are where the numbers live.</p> <p>Body: A fact table has: 1. Surrogate keys pointing to dimensions 2. Degenerate dimensions (order_id-no separate dim needed) 3. Measures (amounts, quantities, counts)</p> <p>Our fact_orders will have:</p> <pre><code>order_sk          -- Generated PK\norder_id          -- Degenerate dimension\ncustomer_sk       -- FK to dim_customer\nproduct_sk        -- FK to dim_product\nseller_sk         -- FK to dim_seller\norder_date_sk     -- FK to dim_date\ndelivery_date_sk  -- FK to dim_date\nquantity          -- Measure\nprice             -- Measure\nfreight_value     -- Measure\n</code></pre> <p>The pattern: 1. Read order items (the grain-one row per item) 2. Look up surrogate keys from dimensions 3. Handle orphans (what if customer doesn't exist?) 4. Add measures 5. Write</p> <p>Let's build it.</p> <p>CTA: Tomorrow-the tricky parts</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-82-surrogate-key-lookups-wednesday","title":"Post 8.2 - Surrogate Key Lookups (Wednesday)","text":"<p>Hook: The hardest part of fact tables: looking up dimension keys.</p> <p>Body: Source has: customer_id = \"abc123\" Fact needs: customer_sk = 42</p> <p>I need to join to dim_customer and get the surrogate key.</p> <pre><code>pattern:\n  type: fact\n  params:\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n\n      - source_column: seller_id\n        dimension_table: dim_seller\n        dimension_key: seller_id\n        surrogate_key: seller_sk\n\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n</code></pre> <p>This automatically: 1. Joins to each dimension 2. Retrieves the surrogate key 3. Adds it to the fact row</p> <p>But what about orphans?</p> <p>CTA: Orphans tomorrow</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-83-handling-orphan-records-friday","title":"Post 8.3 - Handling Orphan Records (Friday)","text":"<p>Hook: An order references customer_id \"xyz\"-but that customer doesn't exist in dim_customer. Now what?</p> <p>Body: Options: 1. Reject - Fail the pipeline (strict but painful) 2. Unknown - Assign to customer_sk = 0 (the unknown member) 3. Quarantine - Route to a holding table for review</p> <p>I use \"unknown\" for most cases:</p> <pre><code>pattern:\n  type: fact\n  params:\n    orphan_handling: unknown\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n</code></pre> <p>Now orphan orders get customer_sk = 0.</p> <p>I can query later:</p> <pre><code>SELECT COUNT(*) \nFROM fact_orders \nWHERE customer_sk = 0\n</code></pre> <p>Found 8 orphans. Now I can investigate without blocking the pipeline.</p> <p>This is why dim tables have unknown member rows-they catch orphans cleanly.</p> <p>Full guide on fact table patterns including lookups, orphans, and measures on Medium. Link in comments.</p> <p>CTA: Link to Article 8</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-9-putting-it-all-together","title":"Week 9: Putting It All Together","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-91-the-complete-fact-table-monday","title":"Post 9.1 - The Complete Fact Table (Monday)","text":"<p>Hook: Here's fact_orders-the center of our star schema.</p> <p>Body:</p> <pre><code>- name: fact_orders\n  read:\n    connection: silver\n    path: order_items\n\n  pattern:\n    type: fact\n    params:\n      grain: [order_id, product_id, seller_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          dimension_key: customer_id\n          surrogate_key: customer_sk\n        - source_column: product_id\n          dimension_table: dim_product\n          dimension_key: product_id\n          surrogate_key: product_sk\n        - source_column: seller_id\n          dimension_table: dim_seller\n          dimension_key: seller_id\n          surrogate_key: seller_sk\n      orphan_handling: unknown\n      measures:\n        - quantity: \"order_item_id\"\n        - price\n        - freight_value\n        - total_amount: \"price + freight_value\"\n\n  write:\n    connection: gold\n    table: fact_orders\n    format: delta\n</code></pre> <p>112,650 order items \u2192 fact_orders ready for analysis.</p> <p>CTA: Star schema complete</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-92-the-star-schema-wednesday","title":"Post 9.2 - The Star Schema (Wednesday)","text":"<p>Hook: Here's what we built:</p> <p>Body:</p> <pre><code>                    dim_customer\n                         |\n    dim_product ---- fact_orders ---- dim_date\n                         |\n                    dim_seller\n</code></pre> <p>From 8 messy CSV files to a clean, queryable warehouse.</p> <p>Total: - dim_customer: 99,442 rows - dim_product: 32,952 rows - dim_seller: 3,096 rows - dim_date: 3,653 rows - fact_orders: 112,650 rows</p> <p>Now I can answer: - Revenue by product category? - Orders by customer state? - Delivery performance by month? - Top sellers by quarter?</p> <p>All with simple SQL joins.</p> <p>I wrote a complete walkthrough from CSV to Star Schema on Medium. Link in comments.</p> <p>CTA: Link to Article 9 + Sample queries Friday</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-93-queries-that-just-work-friday","title":"Post 9.3 - Queries That Just Work (Friday)","text":"<p>Hook: The whole point: analytics that are fast and simple.</p> <p>Body: Revenue by product category:</p> <pre><code>SELECT \n  p.product_category_name,\n  SUM(f.total_amount) as revenue\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.product_category_name\nORDER BY revenue DESC\n</code></pre> <p>Orders by customer state:</p> <pre><code>SELECT \n  c.customer_state,\n  COUNT(DISTINCT f.order_id) as order_count\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nGROUP BY c.customer_state\n</code></pre> <p>Monthly trend:</p> <pre><code>SELECT \n  d.year,\n  d.month,\n  SUM(f.total_amount) as revenue\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nGROUP BY d.year, d.month\nORDER BY d.year, d.month\n</code></pre> <p>Clean data + good model = simple queries.</p> <p>CTA: Next week-the tool I built to make this easier</p>"},{"location":"marketing/campaign_phase2_building_in_public/#week-10-introducing-odibi","title":"Week 10: Introducing Odibi","text":""},{"location":"marketing/campaign_phase2_building_in_public/#post-101-why-i-built-a-framework-monday","title":"Post 10.1 - Why I Built a Framework (Monday)","text":"<p>Hook: I got tired of writing the same pipeline code over and over.</p> <p>Body: Every project: - Same Bronze layer patterns - Same deduplication logic - Same SCD2 handling - Same dimension building - Same fact lookups</p> <p>Copy-paste. Tweak. Debug. Repeat.</p> <p>So I built a framework.</p> <p>Declare WHAT you want. Let the tool handle HOW.</p> <p>Instead of 200 lines of Python:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n</code></pre> <p>Instead of manual joins for SK lookups:</p> <pre><code>pattern:\n  type: fact\n  params:\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        surrogate_key: customer_sk\n</code></pre> <p>It's called Odibi.</p> <p>It's what I've been using for this entire series.</p> <p>CTA: More tomorrow</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-102-what-odibi-does-wednesday","title":"Post 10.2 - What Odibi Does (Wednesday)","text":"<p>Hook: YAML in. Data warehouse out.</p> <p>Body: Odibi handles:</p> <p>Patterns: - Dimensions (SCD0, SCD1, SCD2) - Facts (with automatic SK lookups) - Aggregations - Date dimensions</p> <p>Data Quality: - Contracts (not_null, accepted_values, row_count) - Validation before load - Quarantine for bad records</p> <p>Transformations: - 30+ built-in functions - SQL steps - Custom Python</p> <p>Operations: - Incremental loading - Delta Lake support - Spark, Pandas, or Polars engines</p> <p>One config file. One command. Full pipeline.</p> <p>I'm open-sourcing it because I want feedback and collaborators.</p> <p>I wrote a full introduction to Odibi on Medium explaining the why and how. Link in comments along with GitHub.</p> <p>CTA: Link to Article 10 + GitHub</p>"},{"location":"marketing/campaign_phase2_building_in_public/#post-103-get-started-friday","title":"Post 10.3 - Get Started (Friday)","text":"<p>Hook: Everything from this series-in one repo.</p> <p>Body: The complete Brazilian E-Commerce warehouse:</p> <p>\ud83d\udcc1 configs/ - All YAML configs \ud83d\udcc1 docs/ - How it works \ud83d\udcc1 examples/ - Ready-to-run pipelines</p> <pre><code>pip install odibi\nodibi run ecommerce_warehouse.yaml\n</code></pre> <p>Bronze \u2192 Silver \u2192 Gold in one command.</p> <p>What's next: - More patterns - Better docs - Community feedback</p> <p>If you followed along, thank you. </p> <p>If you try it and find bugs-tell me. I'm still learning and building.</p> <p>GitHub: [link] Docs: [link]</p> <p>CTA: Links to repo and docs</p>"},{"location":"marketing/campaign_phase2_building_in_public/#medium-articles-for-phase-2","title":"Medium Articles for Phase 2","text":"Week Article 3 \"Setting Up a Bronze Layer with Delta Lake\" 4 \"Data Quality Contracts: Catching Problems Before Production\" 5 \"Complete Silver Layer Configuration Guide\" 6 \"Facts vs Dimensions: A Practical Guide\" 7 \"Building a Date Dimension from Scratch\" 8 \"Fact Table Patterns: Lookups, Orphans, and Measures\" 9 \"From CSV to Star Schema: Complete Walkthrough\" 10 \"Introducing Odibi: Declarative Data Pipelines\""},{"location":"marketing/campaign_phase3_pattern_deep_dives/","title":"LinkedIn Campaign - Phase 3: Pattern Deep Dives","text":"<p>Duration: Weeks 11-16 (18 posts) Goal: Establish expertise through deep technical content on data patterns Tone: Teaching, authoritative but humble, practical examples</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#week-11-scd2-deep-dive","title":"Week 11: SCD2 Deep Dive","text":""},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-111-what-is-scd2-monday","title":"Post 11.1 - What is SCD2? (Monday)","text":"<p>Hook: Your customer moved from California to New York. Do you update their record or keep both addresses?</p> <p>Body: This is the SCD2 question.</p> <p>SCD = Slowly Changing Dimension</p> <p>Type 1: Overwrite. Customer now lives in NY. CA is forgotten.</p> <p>Type 2: Keep history. Customer lived in CA from Jan-Jun, now lives in NY.</p> <p>Why does this matter?</p> <p>Imagine an order from March. Which address was valid then? CA.</p> <p>If you overwrote with NY, you'd show the wrong state in your March reports.</p> <p>SCD2 tracks versions:</p> <pre><code>customer_id | address | valid_from | valid_to   | is_current\n101         | CA      | 2024-01-01 | 2024-06-30 | false\n101         | NY      | 2024-07-01 | NULL       | true\n</code></pre> <p>Now you can time-travel. \"What was the address on March 15th?\"</p> <p>This is how enterprises track history.</p> <p>CTA: Config tomorrow</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-112-scd2-configuration-wednesday","title":"Post 11.2 - SCD2 Configuration (Wednesday)","text":"<p>Hook: Here's how to implement SCD2 without writing complex merge logic.</p> <p>Body:</p> <pre><code>- name: dim_customer\n  read:\n    connection: silver\n    path: customers\n\n  transformer: scd2\n  params:\n    target: gold.dim_customer\n    keys: [customer_id]\n    track_cols: [address, city, state, tier]\n    effective_time_col: updated_at\n    end_time_col: valid_to\n    current_flag_col: is_current\n\n  write:\n    connection: gold\n    table: dim_customer\n    format: delta\n    mode: overwrite\n</code></pre> <p>What this does: 1. Compare incoming data to existing dim_customer 2. If track_cols changed \u2192 close old row, insert new row 3. If no changes \u2192 do nothing 4. New customers \u2192 insert with is_current = true</p> <p>The framework handles: - Row versioning - Date management - Duplicate prevention</p> <p>You just declare what to track.</p> <p>I wrote a complete SCD2 guide on Medium covering when to use it, how it works, and common gotchas. Link in comments.</p> <p>CTA: Link to Article 11</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-113-scd2-gotchas-friday","title":"Post 11.3 - SCD2 Gotchas (Friday)","text":"<p>Hook: 3 ways SCD2 will blow up your table if you're not careful.</p> <p>Body: 1. Duplicate source data</p> <p>If your source has the same customer twice:</p> <pre><code>customer_id | address  | updated_at\n101         | CA       | 2024-01-01 10:00:00\n101         | CA       | 2024-01-01 10:00:01  \u2190 duplicate!\n</code></pre> <p>SCD2 sees two \"changes\" \u2192 creates two versions.</p> <p>Fix: Deduplicate BEFORE SCD2.</p> <p>2. Tracking volatile columns</p> <p>If you track <code>last_login_timestamp</code>: - Customer logs in 50 times - 50 versions created - Table explodes</p> <p>Fix: Only track slowly changing attributes (address, tier, status).</p> <p>3. Running without deduplication</p> <p>Each pipeline run processes ALL source data. If you don't dedupe, old records create new versions.</p> <p>Fix: Use incremental loading or dedupe on keys.</p> <p>SCD2 is powerful. But it amplifies bad data hygiene.</p> <p>CTA: Dedupe first, always</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#week-12-dimensions-advanced","title":"Week 12: Dimensions Advanced","text":""},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-121-unknown-member-pattern-monday","title":"Post 12.1 - Unknown Member Pattern (Monday)","text":"<p>Hook: Customer ID is null. The join fails. Your dashboard breaks.</p> <p>Body: This happens constantly: - Source system has gaps - Foreign key wasn't populated - Data arrived out of order</p> <p>Solution: Unknown member row.</p> <p>Every dimension has a row with surrogate_key = 0:</p> <pre><code>customer_sk | customer_id | name\n0           | UNKNOWN     | Unknown Customer\n1           | abc123      | Alice\n2           | def456      | Bob\n</code></pre> <p>When fact table has null/invalid customer_id, it joins to customer_sk = 0.</p> <p>Dashboard still works. Query still runs. You just see \"Unknown Customer.\"</p> <pre><code>pattern:\n  type: dimension\n  params:\n    unknown_member: true\n</code></pre> <p>One line. Problem solved.</p> <p>Later, you can investigate:</p> <pre><code>SELECT COUNT(*) \nFROM fact_orders \nWHERE customer_sk = 0\n</code></pre> <p>Now you know how many orphans you have.</p> <p>CTA: Defense against bad data</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-122-surrogate-key-generation-wednesday","title":"Post 12.2 - Surrogate Key Generation (Wednesday)","text":"<p>Hook: How do you generate 100,000 unique IDs efficiently?</p> <p>Body: Options:</p> <p>1. Hash-based (deterministic)</p> <pre><code>generate_surrogate_key:\n  strategy: hash\n  source_columns: [customer_id]\n</code></pre> <p>Same input \u2192 same key. Always. Pro: Idempotent. Con: Collisions possible (rare).</p> <p>2. Sequence-based (incremental)</p> <pre><code>generate_surrogate_key:\n  strategy: sequence\n</code></pre> <p>1, 2, 3, 4... Pro: Simple, small. Con: Gaps if rows deleted.</p> <p>3. UUID</p> <pre><code>generate_surrogate_key:\n  strategy: uuid\n</code></pre> <p>Random globally unique IDs. Pro: No coordination needed. Con: Large, not human-readable.</p> <p>My default: Sequence for dimensions (small, fast joins), Hash when I need idempotency.</p> <p>CTA: Pick based on use case</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-123-audit-columns-friday","title":"Post 12.3 - Audit Columns (Friday)","text":"<p>Hook: \"When was this row loaded?\" \"From what source?\"</p> <p>Body: Every dimension should have: - <code>load_timestamp</code> - When the pipeline ran - <code>source_system</code> - Where the data came from</p> <pre><code>pattern:\n  type: dimension\n  params:\n    audit:\n      load_timestamp: true\n      source_system: \"ecommerce_api\"\n</code></pre> <p>Result:</p> <pre><code>customer_sk | name  | load_timestamp      | source_system\n1           | Alice | 2024-12-01 08:00:00 | ecommerce_api\n</code></pre> <p>Why? 1. Debugging - \"This value looks wrong. When did it arrive?\" 2. Lineage - \"Which system is the source of truth?\" 3. SLA tracking - \"Is data fresh?\"</p> <p>Tiny addition. Massive debugging value.</p> <p>Full guide on dimension table patterns including unknown members, keys, and auditing on Medium. Link in comments.</p> <p>CTA: Link to Article 12</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#week-13-fact-tables-advanced","title":"Week 13: Fact Tables Advanced","text":""},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-131-fact-table-grain-monday","title":"Post 13.1 - Fact Table Grain (Monday)","text":"<p>Hook: The grain determines everything about your fact table.</p> <p>Body: Grain = what does one row represent?</p> <p>Order grain:</p> <pre><code>order_id | customer_sk | total_amount\n</code></pre> <p>One row per order. Can't see individual products.</p> <p>Order-item grain:</p> <pre><code>order_id | product_sk | customer_sk | quantity | price\n</code></pre> <p>One row per product in each order. More detail.</p> <p>Daily aggregate grain:</p> <pre><code>date_sk | product_sk | total_quantity | total_revenue\n</code></pre> <p>One row per product per day. Less storage, less detail.</p> <p>Choose based on questions: - \"Total revenue by month?\" \u2192 Any grain works - \"Which products were bought together?\" \u2192 Need order-item grain - \"What time was each order placed?\" \u2192 Need order grain</p> <p>Grain is the first decision. Everything else follows.</p> <p>CTA: Grain = detail level</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-132-calculated-measures-wednesday","title":"Post 13.2 - Calculated Measures (Wednesday)","text":"<p>Hook: Don't store what you can calculate. Except when you should.</p> <p>Body: Option 1: Calculate at query time</p> <pre><code>SELECT quantity * unit_price as line_total\nFROM fact_orders\n</code></pre> <p>Pro: Always correct. Con: Slower queries.</p> <p>Option 2: Store pre-calculated</p> <pre><code>measures:\n  - line_total: \"quantity * unit_price\"\n  - margin: \"(unit_price - cost) * quantity\"\n</code></pre> <p>Pro: Fast queries. Con: Must recalculate if formula changes.</p> <p>My rule: - Store stable calculations (price * quantity) - Calculate complex/changing formulas at query time</p> <p>For this warehouse:</p> <pre><code>measures:\n  - quantity\n  - price\n  - freight_value\n  - total_amount: \"price + freight_value\"\n</code></pre> <p>Simple math = store it. Complex business logic = calculate it.</p> <p>CTA: Balance storage vs compute</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-133-grain-validation-friday","title":"Post 13.3 - Grain Validation (Friday)","text":"<p>Hook: Duplicate grain rows corrupt your metrics. Catch them before they happen.</p> <p>Body: If your grain is [order_id, product_sk] and you have:</p> <pre><code>order_id | product_sk | quantity\n1001     | 42         | 1\n1001     | 42         | 1  \u2190 DUPLICATE GRAIN\n</code></pre> <p>Your revenue will be 2x the actual value.</p> <p>Validate grain before load:</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, product_sk, seller_sk]\n</code></pre> <p>This checks: is each combination unique?</p> <p>If not, pipeline fails with:</p> <pre><code>GrainValidationError: 23 duplicate grain rows found\n</code></pre> <p>Better to fail loudly than silently corrupt reports.</p> <p>I wrote a complete guide on fact table design covering grain, measures, and validation on Medium. Link in comments.</p> <p>CTA: Link to Article 13</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#week-14-aggregation-patterns","title":"Week 14: Aggregation Patterns","text":""},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-141-why-pre-aggregate-monday","title":"Post 14.1 - Why Pre-Aggregate? (Monday)","text":"<p>Hook: 100 million fact rows. Dashboard needs to load in 3 seconds.</p> <p>Body: Options:</p> <p>1. Query raw facts</p> <pre><code>SELECT month, SUM(revenue)\nFROM fact_orders\nGROUP BY month\n</code></pre> <p>Works. But 100M rows = slow.</p> <p>2. Pre-aggregate</p> <pre><code>-- Pre-built table\nSELECT month, revenue\nFROM agg_monthly_sales\n</code></pre> <p>10 rows = instant.</p> <p>Pre-aggregation trades storage for speed.</p> <p>When to use: - Dashboard queries are slow - Same GROUP BY runs repeatedly - Users can't wait for complex queries</p> <p>When not to use: - Exploratory analysis (need detail) - Data changes frequently (aggregates go stale)</p> <p>CTA: Aggregates = speed</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-142-aggregation-pattern-wednesday","title":"Post 14.2 - Aggregation Pattern (Wednesday)","text":"<p>Hook: Here's how to build an aggregate table declaratively.</p> <p>Body:</p> <pre><code>- name: agg_daily_sales\n  read:\n    connection: gold\n    path: fact_orders\n\n  pattern:\n    type: aggregation\n    params:\n      grain: [order_date_sk, product_sk]\n      measures:\n        - name: total_revenue\n          expr: \"SUM(total_amount)\"\n        - name: order_count\n          expr: \"COUNT(DISTINCT order_id)\"\n        - name: avg_order_value\n          expr: \"AVG(total_amount)\"\n\n  write:\n    connection: gold\n    table: agg_daily_sales\n    format: delta\n</code></pre> <p>Input: 112,650 fact rows Output: ~30,000 aggregate rows (one per date + product)</p> <p>Queries on aggregates: &lt;100ms Queries on raw facts: 5-10 seconds</p> <p>10-100x faster for common questions.</p> <p>Full guide on pre-aggregation strategies for fast dashboards on Medium. Link in comments.</p> <p>CTA: Link to Article 14</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-143-aggregate-refresh-strategy-friday","title":"Post 14.3 - Aggregate Refresh Strategy (Friday)","text":"<p>Hook: Aggregates go stale. Here's how to keep them fresh.</p> <p>Body: Option 1: Full rebuild</p> <pre><code>mode: overwrite\n</code></pre> <p>Drop and recreate every run. Simple. Safe. Expensive.</p> <p>Option 2: Incremental</p> <pre><code>incremental:\n  timestamp_column: order_date_sk\n  merge_strategy: replace\n</code></pre> <p>Only update rows that changed. Faster. More complex.</p> <p>Option 3: Append + dedupe Add new aggregates, then dedupe on grain.</p> <p>My recommendation: - Daily aggregates \u2192 Full rebuild (cheap enough) - Hourly/real-time \u2192 Incremental - Monthly summaries \u2192 Full rebuild monthly</p> <p>Start simple. Optimize when it hurts.</p> <p>CTA: Start with full rebuild</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#week-15-data-quality-patterns","title":"Week 15: Data Quality Patterns","text":""},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-151-contracts-vs-validation-monday","title":"Post 15.1 - Contracts vs Validation (Monday)","text":"<p>Hook: Contracts run BEFORE load. Validation runs AFTER load. Both matter.</p> <p>Body: Contracts = Pre-conditions \"Is this data safe to load?\" - Required columns exist? - Types are correct? - Values in expected range?</p> <p>If contract fails \u2192 pipeline stops \u2192 no bad data enters.</p> <p>Validation = Post-conditions \"Did the load produce good results?\" - Row counts make sense? - Aggregates balance? - No orphan keys?</p> <p>If validation fails \u2192 alert \u2192 investigate.</p> <pre><code># Contracts (pre-load)\ncontracts:\n  - type: not_null\n    columns: [order_id]\n  - type: row_count\n    min: 1000\n\n# Validation (post-load)\nvalidation:\n  rules:\n    - type: freshness\n      column: load_timestamp\n      max_age: 24h\n</code></pre> <p>Both are guardrails. Use them.</p> <p>CTA: Pre + post checks</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-152-quarantine-pattern-wednesday","title":"Post 15.2 - Quarantine Pattern (Wednesday)","text":"<p>Hook: Bad rows don't have to kill your pipeline. Route them somewhere else.</p> <p>Body: Instead of failing on bad data: 1. Route bad rows to quarantine table 2. Load good rows normally 3. Review quarantine later</p> <pre><code>pattern:\n  type: fact\n  params:\n    orphan_handling: quarantine\n    quarantine:\n      connection: silver\n      path: fact_orders_quarantine\n      add_columns:\n        _rejection_reason: true\n        _rejected_at: true\n</code></pre> <p>Result: - <code>fact_orders</code> - 112,000 good rows - <code>fact_orders_quarantine</code> - 650 problem rows</p> <p>Query quarantine:</p> <pre><code>SELECT _rejection_reason, COUNT(*)\nFROM fact_orders_quarantine\nGROUP BY _rejection_reason\n</code></pre> <p>Pipeline keeps running. You investigate at your pace.</p> <p>Complete guide on data quality patterns covering contracts, validation, and quarantine on Medium. Link in comments.</p> <p>CTA: Link to Article 15</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-153-volume-drop-detection-friday","title":"Post 15.3 - Volume Drop Detection (Friday)","text":"<p>Hook: Yesterday: 100,000 rows. Today: 500 rows. Something is wrong.</p> <p>Body: A sudden drop in data volume usually means: - Source system failed - Extract job broke - Filter accidentally added</p> <p>Catch it:</p> <pre><code>contracts:\n  - type: volume_drop\n    threshold: 0.5  # Alert if &lt; 50% of average\n    baseline: last_7_days\n</code></pre> <p>If today's count is less than 50% of the 7-day average \u2192 pipeline fails.</p> <p>Better to fail and investigate than to report incomplete data.</p> <p>Real story: Source system had an outage. Sent 0 rows. Dashboard showed \"$0 revenue.\"</p> <p>CFO called. Not a good day.</p> <p>Volume checks would have caught it before it hit the dashboard.</p> <p>CTA: Volume = first warning sign</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#week-16-incremental-patterns","title":"Week 16: Incremental Patterns","text":""},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-161-why-incremental-monday","title":"Post 16.1 - Why Incremental? (Monday)","text":"<p>Hook: 5 years of history. 500GB of data. Do you reload it all every day?</p> <p>Body: Full load: - Read 500GB - Process 500GB - Write 500GB - Time: hours, cost: $$$</p> <p>Incremental load: - Read 1GB (today's data) - Process 1GB - Merge with existing - Time: minutes, cost: $</p> <p>When to use: - Large historical tables - Daily/frequent refreshes - Append-mostly data (events, transactions)</p> <p>When NOT to use: - Small tables (full load is fine) - Complete snapshots (need full picture) - First run (nothing to increment from)</p> <p>CTA: Big data = incremental</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-162-watermark-pattern-wednesday","title":"Post 16.2 - Watermark Pattern (Wednesday)","text":"<p>Hook: \"Only load rows newer than what I already have.\"</p> <p>Body: Watermark = highest value already loaded.</p> <pre><code>read:\n  incremental:\n    mode: stateful\n    column: updated_at\n    watermark_lag: 2h  # Safety overlap\n</code></pre> <p>How it works: 1. Check: What's the max updated_at in target? \u2192 2024-12-01 08:00 2. Subtract lag: 2024-12-01 06:00 3. Query source: WHERE updated_at &gt;= '2024-12-01 06:00' 4. Load only those rows</p> <p>Why the lag? Late-arriving data. A row might be created at 07:55 but arrive at 08:05. The 2-hour overlap catches stragglers.</p> <p>After load, update watermark to new max.</p> <p>Next run: Only new rows.</p> <p>CTA: Watermarks save compute</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#post-163-merge-vs-append-friday","title":"Post 16.3 - Merge vs Append (Friday)","text":"<p>Hook: Incremental data can be appended or merged. Choose wisely.</p> <p>Body: Append</p> <pre><code>mode: append\n</code></pre> <p>New rows added to table. No updates to existing. Use for: Events, logs, immutable facts.</p> <p>Merge</p> <pre><code>transformer: merge\nparams:\n  keys: [order_id]\n</code></pre> <p>New rows inserted. Existing rows updated. Use for: Dimensions, correctable facts.</p> <p>Example:</p> <p>Order #1001 arrives on Day 1 with status \"shipped\". Order #1001 updated on Day 2 with status \"delivered\".</p> <p>Append: Two rows for #1001 (confusing) Merge: One row for #1001 with latest status (correct)</p> <p>Facts are usually append-only. Dimensions are usually merge.</p> <p>Aggregates depend on your refresh strategy.</p> <p>Complete guide on incremental loading covering watermarks, merge, and append on Medium. Link in comments.</p> <p>CTA: Link to Article 16</p>"},{"location":"marketing/campaign_phase3_pattern_deep_dives/#medium-articles-for-phase-3","title":"Medium Articles for Phase 3","text":"Week Article 11 \"SCD2 Complete Guide: When and How to Track History\" 12 \"Dimension Table Patterns: Unknown Members, Keys, and Auditing\" 13 \"Fact Table Design: Grain, Measures, and Validation\" 14 \"Pre-Aggregation Strategies for Fast Dashboards\" 15 \"Data Quality Patterns: Contracts, Validation, and Quarantine\" 16 \"Incremental Loading: Watermarks, Merge, and Append\""},{"location":"marketing/campaign_phase4_antipatterns/","title":"LinkedIn Campaign - Phase 4: Anti-Patterns &amp; Troubleshooting","text":"<p>Duration: Weeks 17-22 (18 posts) Goal: Build credibility by sharing mistakes and solutions Tone: Humble, \"learned the hard way,\" practical fixes</p>"},{"location":"marketing/campaign_phase4_antipatterns/#week-17-bronze-layer-anti-patterns","title":"Week 17: Bronze Layer Anti-Patterns","text":""},{"location":"marketing/campaign_phase4_antipatterns/#post-171-dont-transform-in-bronze-monday","title":"Post 17.1 - Don't Transform in Bronze (Monday)","text":"<p>Hook: I lost 6 months of data because I \"cleaned\" it during ingest.</p> <p>Body: The mistake:</p> <pre><code># BAD - transforming in Bronze\n- name: bronze_orders\n  read:\n    path: orders.csv\n  transform:\n    steps:\n      - sql: \"SELECT * WHERE status != 'canceled'\"\n  write:\n    connection: bronze\n</code></pre> <p>I filtered out canceled orders. \"We don't need those.\"</p> <p>6 months later: \"Can you analyze canceled orders?\"</p> <p>Me: \"They're... gone.\"</p> <p>The lesson: Bronze layer has ONE job: preserve raw data exactly as received.</p> <pre><code># GOOD - no transformation\n- name: bronze_orders\n  read:\n    path: orders.csv\n  write:\n    connection: bronze\n    mode: append\n</code></pre> <p>Transform in Silver. Filter in Silver. Clean in Silver.</p> <p>Bronze is your undo button. Don't break it.</p> <p>CTA: Have you made this mistake?</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-172-missing-extraction-timestamps-wednesday","title":"Post 17.2 - Missing Extraction Timestamps (Wednesday)","text":"<p>Hook: \"When did this data arrive?\" \"I... don't know.\"</p> <p>Body: The mistake:</p> <pre><code># BAD - no metadata\n- name: bronze_orders\n  read:\n    path: orders.csv\n  write:\n    connection: bronze\n</code></pre> <p>The problem: - Can't tell when rows were loaded - Can't debug timing issues - Can't identify duplicate loads</p> <p>The fix:</p> <pre><code># GOOD - add metadata\n- name: bronze_orders\n  read:\n    path: orders.csv\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            _extracted_at: \"current_timestamp()\"\n            _source_file: \"'orders.csv'\"\n  write:\n    connection: bronze\n</code></pre> <p>Now every row has: - When it arrived - Where it came from</p> <p>Debugging time: minutes instead of hours.</p> <p>I wrote about the 3 Bronze layer mistakes that will haunt you on Medium. Link in comments.</p> <p>CTA: Link to Article 17</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-173-append-without-deduplication-friday","title":"Post 17.3 - Append Without Deduplication (Friday)","text":"<p>Hook: Pipeline failed. I reran it. Revenue doubled overnight.</p> <p>Body: The mistake:</p> <pre><code># BAD - blind append\nmode: append\n</code></pre> <p>What happened: 1. Pipeline loaded 100k rows 2. Failed at a later step 3. I reran the whole pipeline 4. Another 100k rows appended 5. Now I have duplicates</p> <p>Dashboard showed 2x revenue. Finance panicked.</p> <p>The fix:</p> <pre><code># GOOD - deduplicate downstream\n# Bronze: append (keep everything)\nbronze:\n  mode: append\n\n# Silver: deduplicate\nsilver:\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: \"_extracted_at DESC\"\n</code></pre> <p>Or use merge mode for idempotency:</p> <pre><code># Alternative - merge is idempotent\ntransformer: merge\nparams:\n  keys: [order_id]\n</code></pre> <p>Rerunning a merge gives the same result.</p> <p>CTA: Make pipelines rerunnable</p>"},{"location":"marketing/campaign_phase4_antipatterns/#week-18-silver-layer-anti-patterns","title":"Week 18: Silver Layer Anti-Patterns","text":""},{"location":"marketing/campaign_phase4_antipatterns/#post-181-business-logic-everywhere-monday","title":"Post 18.1 - Business Logic Everywhere (Monday)","text":"<p>Hook: \"Why is revenue $1,100?\" \"Check Bronze. No wait, check Silver. Actually...\"</p> <p>Body: The mistake: Business logic scattered across layers.</p> <p>Bronze:</p> <pre><code># Applied 8% discount here\ntransform:\n  - sql: \"SELECT *, amount * 0.92 as net\"\n</code></pre> <p>Silver:</p> <pre><code># Added 10% markup here\ntransform:\n  - sql: \"SELECT *, net * 1.1 as projected\"\n</code></pre> <p>Gold:</p> <pre><code># More calculations here\ntransform:\n  - sql: \"SELECT *, projected * tax_rate as final\"\n</code></pre> <p>Debugging = archaeology.</p> <p>The fix: Centralize logic in ONE place.</p> <pre><code># GOOD - all business logic in Silver\nsilver_orders:\n  transform:\n    steps:\n      - sql: |\n          SELECT *,\n            amount * 0.92 as net_amount,\n            amount * 0.92 * 1.1 as projected,\n            amount * 0.92 * 1.1 * 1.08 as final_with_tax\n          FROM df\n</code></pre> <p>One place to check. One place to fix.</p> <p>CTA: Centralize business logic</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-182-skipping-silver-layer-wednesday","title":"Post 18.2 - Skipping Silver Layer (Wednesday)","text":"<p>Hook: \"Let's just go Bronze \u2192 Gold. Fewer steps.\"</p> <p>Body: The temptation:</p> <pre><code># BAD - skipping Silver\ngold_report:\n  read:\n    connection: bronze\n    path: raw_orders\n  transform:\n    - deduplicate\n    - clean\n    - validate\n    - aggregate\n</code></pre> <p>The problems: 1. Every Gold table repeats cleaning logic 2. Cleaning done 5 different ways 3. No single source of truth 4. Reports don't match</p> <p>The fix:</p> <pre><code># GOOD - Silver is the source of truth\nsilver_orders:\n  read:\n    connection: bronze\n  transform:\n    - deduplicate\n    - clean\n    - validate\n  write:\n    connection: silver\n\ngold_report_a:\n  read:\n    connection: silver\n    path: orders\n  # Already clean!\n\ngold_report_b:\n  read:\n    connection: silver\n    path: orders\n  # Same source, same numbers\n</code></pre> <p>Silver = cleaned once, used everywhere.</p> <p>Full guide on Silver layer best practices on Medium. Link in comments.</p> <p>CTA: Link to Article 18</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-183-ignoring-null-keys-friday","title":"Post 18.3 - Ignoring NULL Keys (Friday)","text":"<p>Hook: 1000 orders. 900 in the report. \"Where did 100 go?\"</p> <p>Body: The mistake:</p> <pre><code># Orders with NULL customer_id\n# Joined to dim_customer on customer_id\n# NULL != NULL in SQL\n# 100 orders dropped silently\n</code></pre> <p>NULL values in join keys = silent data loss.</p> <p>The fix:</p> <pre><code># GOOD - handle NULLs explicitly\ntransform:\n  steps:\n    - function: fill_nulls\n      params:\n        columns: [customer_id]\n        value: \"UNKNOWN\"\n</code></pre> <p>Or use the fact pattern with unknown handling:</p> <pre><code>pattern:\n  type: fact\n  params:\n    orphan_handling: unknown\n</code></pre> <p>Rows with NULL/invalid keys \u2192 customer_sk = 0 (unknown member).</p> <p>All 1000 orders in the report. 100 flagged as \"unknown customer.\"</p> <p>Visible problems are fixable. Silent problems are dangerous.</p> <p>CTA: NULL = silent killer</p>"},{"location":"marketing/campaign_phase4_antipatterns/#week-19-scd2-anti-patterns","title":"Week 19: SCD2 Anti-Patterns","text":""},{"location":"marketing/campaign_phase4_antipatterns/#post-191-scd2-without-deduplication-monday","title":"Post 19.1 - SCD2 Without Deduplication (Monday)","text":"<p>Hook: dim_customer: 10,000 rows. After SCD2: 10,000,000 rows.</p> <p>Body: The mistake: Source had duplicates:</p> <pre><code>customer_id | name  | updated_at\n101         | Alice | 2024-01-01 08:00:00\n101         | Alice | 2024-01-01 08:00:01\n101         | Alice | 2024-01-01 08:00:02\n</code></pre> <p>SCD2 saw 3 \"versions\" \u2192 created 3 history rows.</p> <p>Every customer \u00d7 every duplicate = explosion.</p> <p>The fix:</p> <pre><code># GOOD - dedupe before SCD2\n- name: prep_customers\n  transformer: deduplicate\n  params:\n    keys: [customer_id]\n    order_by: \"updated_at DESC\"\n\n- name: dim_customer\n  depends_on: [prep_customers]\n  transformer: scd2\n  params:\n    keys: [customer_id]\n</code></pre> <p>Deduplicate FIRST. Then SCD2.</p> <p>CTA: Dedupe before SCD2, always</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-192-tracking-volatile-columns-wednesday","title":"Post 19.2 - Tracking Volatile Columns (Wednesday)","text":"<p>Hook: Tracking last_login in SCD2. Customer logs in 50 times. 50 history rows created.</p> <p>Body: The mistake:</p> <pre><code># BAD - tracking rapidly changing column\ntrack_cols:\n  - name\n  - email\n  - last_login_timestamp  # Changes constantly!\n</code></pre> <p>Result: - Customer logs in Monday \u2192 new version - Logs in Tuesday \u2192 new version - Logs in 50 times \u2192 50 versions</p> <p>Dimension explodes. Storage costs spike. Queries slow.</p> <p>The fix: Only track SLOWLY changing attributes.</p> <pre><code># GOOD - track stable attributes only\ntrack_cols:\n  - name\n  - email\n  - address\n  - tier\n  # NOT: last_login, last_order, session_count\n</code></pre> <p>SCD2 is for slowly changing dimensions. Not rapidly changing ones.</p> <p>If you need to track logins, use a fact table.</p> <p>CTA: Slowly. Changing. Dimensions.</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-193-scd2-on-fact-tables-friday","title":"Post 19.3 - SCD2 on Fact Tables (Friday)","text":"<p>Hook: Using SCD2 on an orders table. Don't.</p> <p>Body: The mistake:</p> <pre><code># BAD - SCD2 on facts\n- name: fact_orders\n  transformer: scd2\n  params:\n    keys: [order_id]\n    track_cols: [status, amount]\n</code></pre> <p>Why it's wrong: Facts are events. Events happened. They don't \"change.\"</p> <p>Order #1001 was placed on Jan 15 for $99. That's a historical fact.</p> <p>If the status changes (shipped \u2192 delivered), that's a NEW event, not a change to the old one.</p> <p>The fix:</p> <pre><code># GOOD - append facts, track status changes separately\nmode: append\n\n# Or use a status history table\n- name: order_status_events\n  mode: append\n  # order_id, old_status, new_status, changed_at\n</code></pre> <p>SCD2 = dimensions only. Facts = append or merge.</p> <p>I wrote about SCD2 done wrong and how to prevent history explosion on Medium. Link in comments.</p> <p>CTA: Link to Article 19</p>"},{"location":"marketing/campaign_phase4_antipatterns/#week-20-performance-anti-patterns","title":"Week 20: Performance Anti-Patterns","text":""},{"location":"marketing/campaign_phase4_antipatterns/#post-201-reading-too-much-data-monday","title":"Post 20.1 - Reading Too Much Data (Monday)","text":"<p>Hook: 5 years of data. I only need today. Still loading all 5 years.</p> <p>Body: The mistake:</p> <pre><code># BAD - full table scan\nread:\n  connection: source\n  table: orders\n  # No filter = load everything\n</code></pre> <p>Every run: 500GB loaded. 499GB ignored. $$$$ wasted.</p> <p>The fix:</p> <pre><code># GOOD - incremental with watermark\nread:\n  connection: source\n  table: orders\n  incremental:\n    mode: stateful\n    column: updated_at\n</code></pre> <p>Only load rows newer than last run.</p> <p>Or explicit filter:</p> <pre><code>read:\n  connection: source\n  table: orders\n  filter: \"order_date &gt;= '2024-01-01'\"\n</code></pre> <p>5 years of history matters for initial load. Daily runs should be incremental.</p> <p>CTA: Don't reload the world</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-202-no-partition-strategy-wednesday","title":"Post 20.2 - No Partition Strategy (Wednesday)","text":"<p>Hook: Query scans 100 million rows. Answer requires 10,000.</p> <p>Body: The mistake:</p> <pre><code># BAD - no partitioning\nwrite:\n  table: fact_orders\n  format: delta\n</code></pre> <p>Every query scans entire table. Slow and expensive.</p> <p>The fix:</p> <pre><code># GOOD - partition by date\nwrite:\n  table: fact_orders\n  format: delta\n  partition_by: [order_date]\n</code></pre> <p>Now:</p> <pre><code>SELECT * FROM fact_orders \nWHERE order_date = '2024-12-01'\n</code></pre> <p>Only scans one partition. 1000x faster.</p> <p>Partition by: - Date (most common) - Region - Customer segment - Whatever you filter by most</p> <p>Full guide on performance anti-patterns and why your pipeline takes hours on Medium. Link in comments.</p> <p>CTA: Link to Article 20</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-203-joining-large-tables-in-memory-friday","title":"Post 20.3 - Joining Large Tables in Memory (Friday)","text":"<p>Hook: Two 10GB tables. Join them. Computer crashes.</p> <p>Body: The mistake: Using Pandas for data that doesn't fit in memory.</p> <pre><code># BAD - Pandas with 10GB tables\ndf1 = pd.read_parquet(\"orders.parquet\")  # 10GB\ndf2 = pd.read_parquet(\"customers.parquet\")  # 5GB\nresult = df1.merge(df2)  # Memory error\n</code></pre> <p>The fix: Use the right engine.</p> <pre><code># GOOD - Spark for large data\nengine: spark\n\n# Or Polars for medium data\nengine: polars\n</code></pre> <p>Guidelines: - Pandas: &lt; 1GB (fits in memory with room to spare) - Polars: 1-10GB (efficient single-machine) - Spark: &gt; 10GB (distributed processing)</p> <p>Don't force a kitchen blender to do industrial work.</p> <p>CTA: Right tool, right job</p>"},{"location":"marketing/campaign_phase4_antipatterns/#week-21-configuration-anti-patterns","title":"Week 21: Configuration Anti-Patterns","text":""},{"location":"marketing/campaign_phase4_antipatterns/#post-211-hardcoded-paths-monday","title":"Post 21.1 - Hardcoded Paths (Monday)","text":"<p>Hook: Pipeline works in dev. Breaks in prod. Paths are different.</p> <p>Body: The mistake:</p> <pre><code># BAD - hardcoded paths\nread:\n  path: \"abfss://raw@devaccount.dfs.core.windows.net/orders\"\nwrite:\n  path: \"abfss://bronze@devaccount.dfs.core.windows.net/orders\"\n</code></pre> <p>Moving to prod requires editing every path.</p> <p>The fix:</p> <pre><code># GOOD - use connections\nconnections:\n  landing:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}  # From environment\n    container: raw\n  bronze:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    container: bronze\n\n# In pipeline\nread:\n  connection: landing\n  path: orders\nwrite:\n  connection: bronze\n  path: orders\n</code></pre> <p>Same YAML. Different environments. Just change env vars.</p> <p>CTA: Connections = portability</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-212-no-error-handling-wednesday","title":"Post 21.2 - No Error Handling (Wednesday)","text":"<p>Hook: Pipeline failed at step 47. You have to rerun all 47 steps.</p> <p>Body: The mistake:</p> <pre><code># BAD - no checkpointing, no retry\nnodes:\n  - name: step_1\n  - name: step_2\n  # ... 47 more steps\n</code></pre> <p>One failure = start from scratch.</p> <p>The fix:</p> <pre><code># GOOD - retry with backoff\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\n# GOOD - graceful failure handling\nnodes:\n  - name: critical_step\n    on_error: fail  # Stop everything\n\n  - name: optional_step\n    on_error: warn  # Log and continue\n</code></pre> <p>And design for restart: - Idempotent writes (merge, not append) - Checkpoints between major stages - State tracking</p> <p>CTA: Plan for failure</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-213-ignoring-schema-evolution-friday","title":"Post 21.3 - Ignoring Schema Evolution (Friday)","text":"<p>Hook: Source added a column. Pipeline broke at 2am.</p> <p>Body: The mistake:</p> <pre><code># BAD - rigid schema expectations\nwrite:\n  format: delta\n  # No schema handling\n</code></pre> <p>Source adds <code>phone_number</code> column. Delta rejects it. Pipeline fails.</p> <p>The fix:</p> <pre><code># GOOD - allow schema evolution\nwrite:\n  format: delta\n  delta_options:\n    mergeSchema: true\n\n# Or explicit policy\nschema_policy:\n  on_new_column: add\n  on_missing_column: warn\n  on_type_mismatch: error\n</code></pre> <p>Sources change. Plan for it.</p> <p>Full guide on configuration patterns for multi-environment pipelines on Medium. Link in comments.</p> <p>CTA: Link to Article 21</p>"},{"location":"marketing/campaign_phase4_antipatterns/#week-22-debugging-troubleshooting","title":"Week 22: Debugging &amp; Troubleshooting","text":""},{"location":"marketing/campaign_phase4_antipatterns/#post-221-column-name-case-sensitivity-monday","title":"Post 22.1 - Column Name Case Sensitivity (Monday)","text":"<p>Hook: Error: \"Column 'customer_id' not found\" Me: looks at table \"It's right there!\"</p> <p>Body: The problem:</p> <pre><code>Table columns: CustomerID, Name, Email\nConfig: customer_id\n</code></pre> <p>Case doesn't match. Column \"not found.\"</p> <p>The fix: Normalize early.</p> <pre><code># In Bronze or Silver\ntransform:\n  steps:\n    - function: rename_columns\n      params:\n        lowercase: true\n</code></pre> <p>Now everything is lowercase. Forever. No more case bugs.</p> <p>Or match exact case in config:</p> <pre><code>keys: [CustomerID]  # Match exactly\n</code></pre> <p>I normalize in Silver. Saves headaches forever.</p> <p>CTA: Normalize column names</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-222-debugging-pipeline-failures-wednesday","title":"Post 22.2 - Debugging Pipeline Failures (Wednesday)","text":"<p>Hook: \"Pipeline failed\" tells you nothing. Here's how to actually debug.</p> <p>Body: Step 1: Check the error message</p> <pre><code>KeyError: 'updated_at'\n</code></pre> <p>Translation: Column doesn't exist.</p> <p>Step 2: Print columns at failure point</p> <pre><code>- name: debug_node\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df LIMIT 1\"\n  # Check logs for columns\n</code></pre> <p>Step 3: Trace column through pipeline - Where was it created? - Was it renamed? - Was it dropped?</p> <p>Step 4: Check data, not just schema</p> <pre><code>SELECT DISTINCT column_name \nFROM df \nLIMIT 10\n</code></pre> <p>Maybe it exists but is NULL everywhere.</p> <p>Step 5: Run with verbose logging</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>Most errors are: - Column name mismatch (case, spelling) - Column dropped upstream - Type mismatch</p> <p>Full guide on debugging data pipelines with a systematic approach on Medium. Link in comments.</p> <p>CTA: Link to Article 22</p>"},{"location":"marketing/campaign_phase4_antipatterns/#post-223-when-row-counts-explode-friday","title":"Post 22.3 - When Row Counts Explode (Friday)","text":"<p>Hook: Before: 100,000 rows. After: 10,000,000 rows. What happened?</p> <p>Body: Common causes:</p> <p>1. Bad join</p> <pre><code>-- Accidental cross join\nSELECT * FROM orders, customers\n-- Should be\nSELECT * FROM orders JOIN customers ON ...\n</code></pre> <p>1M \u00d7 1M = 1 trillion rows.</p> <p>2. Duplicate keys in join</p> <pre><code>orders.customer_id | customers.customer_id\n101                | 101\n101                | 101  \u2190 duplicate!\n</code></pre> <p>Each duplicate multiplies rows.</p> <p>3. SCD2 without dedup Covered earlier-every duplicate creates a version.</p> <p>How to debug:</p> <pre><code>-- Check for duplicates on join keys\nSELECT customer_id, COUNT(*) \nFROM customers \nGROUP BY customer_id \nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If duplicates exist, dedupe before join.</p> <p>CTA: Row explosion = join or dupe issue</p>"},{"location":"marketing/campaign_phase4_antipatterns/#medium-articles-for-phase-4","title":"Medium Articles for Phase 4","text":"Week Article 17 \"Bronze Layer Anti-Patterns: 3 Mistakes That Will Haunt You\" 18 \"Silver Layer Best Practices: Centralize, Validate, Deduplicate\" 19 \"SCD2 Done Wrong: History Explosion and How to Prevent It\" 20 \"Performance Anti-Patterns: Why Your Pipeline Takes Hours\" 21 \"Configuration Patterns for Multi-Environment Pipelines\" 22 \"Debugging Data Pipelines: A Systematic Approach\""},{"location":"marketing/campaign_phase5_advanced_community/","title":"LinkedIn Campaign - Phase 5: Advanced Topics &amp; Community Building","text":"<p>Duration: Weeks 23-26 (12 posts) Goal: Establish thought leadership, build community around Odibi Tone: Expert but approachable, inviting collaboration</p>"},{"location":"marketing/campaign_phase5_advanced_community/#week-23-semantic-layer-self-service","title":"Week 23: Semantic Layer &amp; Self-Service","text":""},{"location":"marketing/campaign_phase5_advanced_community/#post-231-what-is-a-semantic-layer-monday","title":"Post 23.1 - What is a Semantic Layer? (Monday)","text":"<p>Hook: Business user: \"What's our revenue by region?\" Data team: \"Which revenue? Gross? Net? After returns? Which region table?\"</p> <p>Body: This conversation happens 10 times a week.</p> <p>A semantic layer solves it.</p> <p>Define metrics ONCE:</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre> <p>Now everyone queries the same \"revenue\":</p> <pre><code>project.query(\"revenue BY region\")\n</code></pre> <p>No ambiguity. No \"which table.\" No \"did you use the right join?\"</p> <p>The semantic layer is the contract between data team and business.</p> <p>I've built this into Odibi. More this week.</p> <p>CTA: Semantic layer = single source of truth</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-232-defining-metrics-wednesday","title":"Post 23.2 - Defining Metrics (Wednesday)","text":"<p>Hook: Here's how to define metrics that everyone can query.</p> <p>Body:</p> <pre><code>semantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: gold.fact_orders\n      filters:\n        - \"status = 'completed'\"\n      description: \"Total revenue from completed orders\"\n\n    - name: order_count\n      expr: \"COUNT(DISTINCT order_id)\"\n      source: gold.fact_orders\n\n    - name: avg_order_value\n      expr: \"revenue / order_count\"\n      type: derived  # Calculated from other metrics\n</code></pre> <p>Now in Python:</p> <pre><code>project = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue, order_count BY month\")\nprint(result.df)\n</code></pre> <p>Or in SQL:</p> <pre><code>SELECT month, SUM(total_amount) as revenue\nFROM fact_orders\nWHERE status = 'completed'\nGROUP BY month\n</code></pre> <p>Define once. Query anywhere.</p> <p>Full guide on building a semantic layer that everyone can trust on Medium. Link in comments.</p> <p>CTA: Link to Article 23</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-233-materializations-friday","title":"Post 23.3 - Materializations (Friday)","text":"<p>Hook: Querying raw metrics = slow. Pre-computing them = fast.</p> <p>Body: Materializations pre-aggregate metrics at specific grain:</p> <pre><code>materializations:\n  - name: monthly_revenue\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold.agg_monthly_revenue\n    schedule: \"0 6 * * *\"  # 6am daily\n</code></pre> <p>This creates a table:</p> <pre><code>region | month   | revenue    | order_count\nNorth  | 2024-01 | 1,234,567  | 12,345\nSouth  | 2024-01 | 987,654    | 9,876\n</code></pre> <p>Dashboard queries this \u2192 instant.</p> <p>Without materialization: compute on the fly \u2192 seconds/minutes. With materialization: read pre-computed \u2192 milliseconds.</p> <p>CTA: Pre-compute common queries</p>"},{"location":"marketing/campaign_phase5_advanced_community/#week-24-production-operations","title":"Week 24: Production Operations","text":""},{"location":"marketing/campaign_phase5_advanced_community/#post-241-monitoring-alerting-monday","title":"Post 24.1 - Monitoring &amp; Alerting (Monday)","text":"<p>Hook: Pipeline failed at 3am. Nobody knew until the 9am meeting.</p> <p>Body: Production pipelines need: 1. Alerting on failure 2. Metrics on runtime 3. Data quality checks</p> <pre><code>alerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK}\n    on_events: [on_failure]\n\n  - type: email\n    to: data-team@company.com\n    on_events: [on_failure, on_quality_warning]\n</code></pre> <p>What to monitor: - Did it run? (job success/failure) - How long? (runtime trending) - Is data fresh? (freshness checks) - Is data complete? (row count checks)</p> <p>Silence is dangerous. Configure alerts before you go to production.</p> <p>CTA: No silent failures</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-242-retry-recovery-wednesday","title":"Post 24.2 - Retry &amp; Recovery (Wednesday)","text":"<p>Hook: Network blip. API timeout. Pipeline failed. Do you restart from scratch?</p> <p>Body: Transient failures are normal. Plan for them.</p> <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n  initial_delay: 5  # seconds\n</code></pre> <p>This means: - Attempt 1: Try immediately - Attempt 2: Wait 5 seconds, retry - Attempt 3: Wait 10 seconds, retry</p> <p>Most transient errors resolve with a retry.</p> <p>But also design for restart: - Idempotent writes: Merge instead of append - Checkpoints: Save progress between stages - State tracking: Know what succeeded</p> <p>If everything is idempotent, \"restart from scratch\" just re-does work, doesn't corrupt data.</p> <p>CTA: Design for restart</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-243-testing-pipelines-friday","title":"Post 24.3 - Testing Pipelines (Friday)","text":"<p>Hook: \"It works on my laptop\" is not a test strategy.</p> <p>Body: What to test:</p> <p>1. Unit tests for transformations</p> <pre><code>def test_clean_text():\n    df = pd.DataFrame({\"name\": [\" ALICE \", \"bob\"]})\n    result = clean_text(df, columns=[\"name\"])\n    assert result[\"name\"].tolist() == [\"alice\", \"bob\"]\n</code></pre> <p>2. Contract tests for schema</p> <pre><code>contracts:\n  - type: schema\n    columns:\n      - name: order_id\n        type: string\n        nullable: false\n</code></pre> <p>3. Integration tests with sample data</p> <pre><code>def test_full_pipeline():\n    result = run_pipeline(\"test_config.yaml\")\n    assert result.success\n    assert result.row_count &gt; 0\n</code></pre> <p>4. Data quality tests</p> <pre><code>validation:\n  rules:\n    - type: uniqueness\n      columns: [order_id]\n</code></pre> <p>Tests catch bugs before production. Production catches bugs after damage.</p> <p>Full guide on production data pipelines covering monitoring, retry, and testing on Medium. Link in comments.</p> <p>CTA: Link to Article 24</p>"},{"location":"marketing/campaign_phase5_advanced_community/#week-25-lessons-reflections","title":"Week 25: Lessons &amp; Reflections","text":""},{"location":"marketing/campaign_phase5_advanced_community/#post-251-what-i-learned-building-odibi-monday","title":"Post 25.1 - What I Learned Building Odibi (Monday)","text":"<p>Hook: Building a data framework taught me more than using one ever could.</p> <p>Body: Lessons from building Odibi:</p> <p>1. Patterns &gt; Code Writing the same code 10 times \u2192 abstract into a pattern. SCD2, dimensions, facts-they're all patterns.</p> <p>2. Configuration &gt; Scripts YAML is more maintainable than Python notebooks. Declarative &gt; imperative for data pipelines.</p> <p>3. Constraints breed creativity No direct database access? Build around it. Limited resources? Optimize ruthlessly.</p> <p>4. Documentation is product If people can't understand it, they won't use it. I spent as much time on docs as code.</p> <p>5. Ship early, iterate First version was embarrassing. Tenth version is useful. Feedback &gt; perfection.</p> <p>Building something forces you to deeply understand the problem.</p> <p>CTA: What have you built that taught you?</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-252-the-hard-parts-wednesday","title":"Post 25.2 - The Hard Parts (Wednesday)","text":"<p>Hook: Building a framework is 20% code, 80% everything else.</p> <p>Body: The hard parts nobody talks about:</p> <p>Naming things - Should it be \"transformer\" or \"transform\"? - \"keys\" or \"key_columns\"? - Naming decisions haunt you forever.</p> <p>Edge cases - What if the table doesn't exist? - What if columns have spaces? - What if there are zero rows? - Edge cases multiply infinitely.</p> <p>Backward compatibility - Changing a parameter name breaks existing configs. - Deprecation is painful.</p> <p>Documentation - Code changes, docs get stale. - Keeping them in sync is constant work.</p> <p>Testing across engines - Works in Pandas. Breaks in Spark. - Behavior differences are subtle.</p> <p>Building tools is humbling. Respect the maintainers.</p> <p>I wrote about everything I learned building an open source data framework on Medium. Link in comments.</p> <p>CTA: Link to Article 25</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-253-whats-next-friday","title":"Post 25.3 - What's Next (Friday)","text":"<p>Hook: Odibi is just the beginning.</p> <p>Body: What I'm working on:</p> <p>Immediate: - More patterns (CDC, snapshots) - Better error messages - Performance optimization</p> <p>Medium-term: - Web UI for config editing - Lineage visualization - dbt integration</p> <p>Long-term: - AI-assisted config generation - Automated testing suggestions - Self-healing pipelines</p> <p>But I can't do it alone.</p> <p>If you've found value in these posts, consider: - Trying Odibi and giving feedback - Reporting issues you find - Suggesting features you need - Contributing if you're interested</p> <p>Open source is a community effort.</p> <p>GitHub: [link]</p> <p>CTA: Join the journey</p>"},{"location":"marketing/campaign_phase5_advanced_community/#week-26-community-call-to-action","title":"Week 26: Community &amp; Call to Action","text":""},{"location":"marketing/campaign_phase5_advanced_community/#post-261-how-to-contribute-monday","title":"Post 26.1 - How to Contribute (Monday)","text":"<p>Hook: You don't need to write code to contribute to open source.</p> <p>Body: Ways to help Odibi (and any open source project):</p> <p>No code required: - Report bugs you find - Improve documentation - Answer questions from new users - Write tutorials or blog posts - Share on social media</p> <p>Some code: - Fix typos in docs - Add tests for existing features - Fix small bugs</p> <p>More code: - Add new transformers - Implement new patterns - Improve performance</p> <p>Every contribution matters. The person who fixes a typo makes the project better for the next user.</p> <p>Start small. Build from there.</p> <p>CTA: Link to contributing guide</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-262-thank-you-wednesday","title":"Post 26.2 - Thank You (Wednesday)","text":"<p>Hook: 26 weeks. 78 posts. 100+ people who engaged along the way.</p> <p>Body: When I started this series, I didn't know if anyone would care.</p> <p>A solo data engineer building a framework in public? Who wants to see that?</p> <p>Turns out: a lot of you.</p> <p>Thank you for: - Reading these posts - Asking questions - Pointing out mistakes - Trying the tool - Sharing feedback</p> <p>I started this to learn in public. I learned more from your comments than from building alone.</p> <p>What's next? - I'll keep posting (less frequently) - Odibi keeps improving - The community keeps growing</p> <p>If you ever want to chat data engineering, DMs are open.</p> <p>I also published a complete index of all 6 months of content on Medium. Link in comments.</p> <p>CTA: Link to Article 26 + Genuine gratitude</p>"},{"location":"marketing/campaign_phase5_advanced_community/#post-263-the-summary-post-friday","title":"Post 26.3 - The Summary Post (Friday)","text":"<p>Hook: 6 months of data engineering content in one post.</p> <p>Body: If you're just joining, here's what we covered:</p> <p>Weeks 1-2: My story Self-taught. Night shifts. 1000+ hours of learning.</p> <p>Weeks 3-10: Building a warehouse Bronze \u2192 Silver \u2192 Gold with real data. - [Link to key posts]</p> <p>Weeks 11-16: Patterns SCD2, dimensions, facts, aggregations. - [Link to key posts]</p> <p>Weeks 17-22: Anti-patterns What NOT to do. Mistakes I made. - [Link to key posts]</p> <p>Weeks 23-25: Advanced Semantic layer, production ops, reflections. - [Link to key posts]</p> <p>The tool: Odibi Open source. Free. Feedback welcome. - [GitHub link]</p> <p>If you learned one thing from this series, my job is done.</p> <p>Thank you for following along.</p> <p>CTA: Links to everything</p>"},{"location":"marketing/campaign_phase5_advanced_community/#medium-articles-for-phase-5","title":"Medium Articles for Phase 5","text":"Week Article 23 \"Building a Semantic Layer: Metrics Everyone Can Trust\" 24 \"Production Data Pipelines: Monitoring, Retry, and Testing\" 25 \"What I Learned Building an Open Source Data Framework\" 26 \"6 Months of Data Engineering Content: A Complete Index\""},{"location":"marketing/campaign_phase5_advanced_community/#campaign-summary","title":"Campaign Summary","text":"Phase Weeks Posts Focus 1 1-2 6 Credibility building 2 3-10 24 Building in public series 3 11-16 18 Pattern deep dives 4 17-22 18 Anti-patterns &amp; troubleshooting 5 23-26 12 Advanced &amp; community Total 26 78 6 months of content"},{"location":"marketing/campaign_phase5_advanced_community/#medium-articles-summary","title":"Medium Articles Summary","text":"<p>~24 articles total, roughly 1-2 per week.</p>"},{"location":"marketing/campaign_phase5_advanced_community/#execution-tips","title":"Execution Tips","text":"<ol> <li>Batch create: Write a week's posts on Sunday</li> <li>Schedule: Use LinkedIn's scheduler or Buffer</li> <li>Engage: Respond to every comment within 24 hours</li> <li>Track: Note which topics get most engagement</li> <li>Iterate: Double down on what works</li> <li>Cross-post: Share Medium articles on LinkedIn and Twitter</li> </ol>"},{"location":"marketing/campaign_phase5_advanced_community/#final-notes","title":"Final Notes","text":"<p>This campaign positions you as: - A practitioner who learned the hard way - Someone who builds, not just talks - Humble and open to feedback - A genuine contributor to the community</p> <p>The goal isn't just followers-it's credibility, opportunities, and a community around Odibi.</p> <p>Good luck. You've got this.</p>"},{"location":"marketing/medium_articles/","title":"Medium Article Outlines","text":"<p>Companion articles for the LinkedIn campaign. Each article expands on LinkedIn posts with full code, diagrams, and deep explanations.</p>"},{"location":"marketing/medium_articles/#phase-1-articles-weeks-1-2","title":"Phase 1 Articles (Weeks 1-2)","text":""},{"location":"marketing/medium_articles/#article-1-how-i-taught-myself-data-engineering-while-working-night-shifts","title":"Article 1: \"How I Taught Myself Data Engineering While Working Night Shifts\"","text":"<p>Target length: 1,500 words LinkedIn tie-in: Post 1.1 (Origin Story)</p> <p>Outline: 1. The setup: 2020 grad, pandemic, night shifts 2. The turning point: Being asked to do data work 3. The learning journey    - YouTube, Udemy, LinkedIn Learning    - What worked, what didn't    - Time management while working full-time 4. The progression: Excel \u2192 Power BI \u2192 Azure \u2192 Python \u2192 Databricks 5. Key milestones and projects 6. Advice for others in similar situations    - You don't need permission    - Start with a real problem    - 1000 hours is just commitment 7. Where I am now</p> <p>Code examples: None (story-focused)</p>"},{"location":"marketing/medium_articles/#article-2-the-bronze-layer-mistake-that-cost-me-6-months-of-data","title":"Article 2: \"The Bronze Layer Mistake That Cost Me 6 Months of Data\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Post 1.2 (Biggest Mistake)</p> <p>Outline: 1. The story: What I did and why 2. What is the Bronze layer?    - Medallion architecture diagram    - Purpose: preservation, not transformation 3. The correct pattern    - Full YAML example    - Code walkthrough 4. What to add in Bronze (metadata only)    - <code>_extracted_at</code>    - <code>_source_file</code>    - <code>_batch_id</code> 5. What NOT to do in Bronze    - No filtering    - No transformations    - No deduplication 6. The Silver layer: Where cleaning belongs 7. Takeaway: Bronze = undo button</p> <p>Code examples: - Bad Bronze config - Good Bronze config - Metadata column additions</p>"},{"location":"marketing/medium_articles/#phase-2-articles-weeks-3-10","title":"Phase 2 Articles (Weeks 3-10)","text":""},{"location":"marketing/medium_articles/#article-3-setting-up-a-bronze-layer-with-delta-lake","title":"Article 3: \"Setting Up a Bronze Layer with Delta Lake\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 3 posts</p> <p>Outline: 1. Dataset introduction (Brazilian E-Commerce) 2. Project structure 3. Connections configuration 4. Bronze nodes for all 8 files    - Full YAML for each 5. Adding extraction metadata 6. Running the pipeline 7. Verifying results 8. Next steps</p> <p>Code examples: - Full project YAML - All Bronze node configs - Verification queries</p>"},{"location":"marketing/medium_articles/#article-4-data-quality-contracts-catching-problems-before-production","title":"Article 4: \"Data Quality Contracts: Catching Problems Before Production\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Week 4-5 posts</p> <p>Outline: 1. What are data contracts? 2. Types of contracts    - not_null    - accepted_values    - row_count    - uniqueness    - custom SQL 3. Severity levels (error vs warn) 4. Full Silver layer example with contracts 5. What happens when contracts fail 6. Quarantine pattern for failed records 7. Best practices</p> <p>Code examples: - Contract configurations - Quarantine setup - Error handling</p>"},{"location":"marketing/medium_articles/#article-5-complete-silver-layer-configuration-guide","title":"Article 5: \"Complete Silver Layer Configuration Guide\"","text":"<p>Target length: 2,500 words LinkedIn tie-in: Week 5 posts</p> <p>Outline: 1. Purpose of Silver layer 2. Common transformations    - Deduplication    - Type casting    - Null handling    - Text cleaning 3. Full config for each table    - orders    - customers    - products    - etc. 4. Validation rules 5. Testing your Silver layer 6. Before/after data examples</p> <p>Code examples: - All Silver node configs - Transform step examples - Validation rules</p>"},{"location":"marketing/medium_articles/#article-6-facts-vs-dimensions-a-practical-guide","title":"Article 6: \"Facts vs Dimensions: A Practical Guide\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 6 posts</p> <p>Outline: 1. The restaurant analogy    - Facts = receipts    - Dimensions = menus, customer lists 2. Identifying facts and dimensions in your data 3. Fact table characteristics    - Keys (surrogate + degenerate)    - Measures    - Grain 4. Dimension table characteristics    - Natural key    - Surrogate key    - Attributes 5. Star schema diagram 6. The e-commerce example 7. Common mistakes</p> <p>Code examples: - Fact table design - Dimension table design - Star schema SQL</p>"},{"location":"marketing/medium_articles/#article-7-building-a-date-dimension-from-scratch","title":"Article 7: \"Building a Date Dimension from Scratch\"","text":"<p>Target length: 1,500 words LinkedIn tie-in: Week 7 posts</p> <p>Outline: 1. Why every warehouse needs a date dimension 2. What columns to include 3. Generating with Odibi 4. Fiscal year handling 5. Unknown member row 6. Common queries using date dimension 7. Complete configuration</p> <p>Code examples: - Date dimension config - Generated column list - Query examples</p>"},{"location":"marketing/medium_articles/#article-8-fact-table-patterns-lookups-orphans-and-measures","title":"Article 8: \"Fact Table Patterns: Lookups, Orphans, and Measures\"","text":"<p>Target length: 2,200 words LinkedIn tie-in: Week 8 posts</p> <p>Outline: 1. Anatomy of a fact table 2. Surrogate key lookups    - How it works    - Configuration 3. Orphan handling strategies    - unknown (default to 0)    - reject (fail pipeline)    - quarantine (route to table) 4. Defining measures    - Passthrough    - Calculated    - Renamed 5. Grain validation 6. Complete fact table config 7. Testing your fact table</p> <p>Code examples: - Full fact pattern config - Lookup configuration - Quarantine setup</p>"},{"location":"marketing/medium_articles/#article-9-from-csv-to-star-schema-complete-walkthrough","title":"Article 9: \"From CSV to Star Schema: Complete Walkthrough\"","text":"<p>Target length: 3,000 words LinkedIn tie-in: Week 9 posts</p> <p>Outline: 1. Starting point: 8 CSV files 2. The journey    - Bronze: Raw landing    - Silver: Cleaning    - Gold: Dimensional model 3. Complete architecture diagram 4. Full configuration files 5. Running the full pipeline 6. Query examples 7. Results and metrics 8. What we built</p> <p>Code examples: - Complete project YAML - All pipeline configs - Query examples</p>"},{"location":"marketing/medium_articles/#article-10-introducing-odibi-declarative-data-pipelines","title":"Article 10: \"Introducing Odibi: Declarative Data Pipelines\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 10 posts</p> <p>Outline: 1. Why I built Odibi 2. The problem with script-based pipelines 3. Declarative vs imperative 4. Core concepts    - Nodes    - Transformers    - Patterns    - Validation 5. Getting started 6. The e-commerce example 7. What's next 8. How to contribute</p> <p>Code examples: - Installation - Basic usage - Pattern examples</p>"},{"location":"marketing/medium_articles/#phase-3-articles-weeks-11-16","title":"Phase 3 Articles (Weeks 11-16)","text":""},{"location":"marketing/medium_articles/#article-11-scd2-complete-guide-when-and-how-to-track-history","title":"Article 11: \"SCD2 Complete Guide: When and How to Track History\"","text":"<p>Target length: 2,500 words LinkedIn tie-in: Week 11 posts</p> <p>Outline: 1. What is SCD2? 2. When to use it (and when not to) 3. How it works (with diagrams) 4. Configuration deep dive 5. Common gotchas    - Deduplication    - Volatile columns    - Performance 6. Debugging SCD2 issues 7. Full example</p>"},{"location":"marketing/medium_articles/#article-12-dimension-table-patterns-unknown-members-keys-and-auditing","title":"Article 12: \"Dimension Table Patterns: Unknown Members, Keys, and Auditing\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 12 posts</p>"},{"location":"marketing/medium_articles/#article-13-fact-table-design-grain-measures-and-validation","title":"Article 13: \"Fact Table Design: Grain, Measures, and Validation\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 13 posts</p>"},{"location":"marketing/medium_articles/#article-14-pre-aggregation-strategies-for-fast-dashboards","title":"Article 14: \"Pre-Aggregation Strategies for Fast Dashboards\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Week 14 posts</p>"},{"location":"marketing/medium_articles/#article-15-data-quality-patterns-contracts-validation-and-quarantine","title":"Article 15: \"Data Quality Patterns: Contracts, Validation, and Quarantine\"","text":"<p>Target length: 2,200 words LinkedIn tie-in: Week 15 posts</p>"},{"location":"marketing/medium_articles/#article-16-incremental-loading-watermarks-merge-and-append","title":"Article 16: \"Incremental Loading: Watermarks, Merge, and Append\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 16 posts</p>"},{"location":"marketing/medium_articles/#phase-4-articles-weeks-17-22","title":"Phase 4 Articles (Weeks 17-22)","text":""},{"location":"marketing/medium_articles/#article-17-bronze-layer-anti-patterns-3-mistakes-that-will-haunt-you","title":"Article 17: \"Bronze Layer Anti-Patterns: 3 Mistakes That Will Haunt You\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Week 17 posts</p>"},{"location":"marketing/medium_articles/#article-18-silver-layer-best-practices-centralize-validate-deduplicate","title":"Article 18: \"Silver Layer Best Practices: Centralize, Validate, Deduplicate\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Week 18 posts</p>"},{"location":"marketing/medium_articles/#article-19-scd2-done-wrong-history-explosion-and-how-to-prevent-it","title":"Article 19: \"SCD2 Done Wrong: History Explosion and How to Prevent It\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 19 posts</p>"},{"location":"marketing/medium_articles/#article-20-performance-anti-patterns-why-your-pipeline-takes-hours","title":"Article 20: \"Performance Anti-Patterns: Why Your Pipeline Takes Hours\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Week 20 posts</p>"},{"location":"marketing/medium_articles/#article-21-configuration-patterns-for-multi-environment-pipelines","title":"Article 21: \"Configuration Patterns for Multi-Environment Pipelines\"","text":"<p>Target length: 1,800 words LinkedIn tie-in: Week 21 posts</p>"},{"location":"marketing/medium_articles/#article-22-debugging-data-pipelines-a-systematic-approach","title":"Article 22: \"Debugging Data Pipelines: A Systematic Approach\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 22 posts</p>"},{"location":"marketing/medium_articles/#phase-5-articles-weeks-23-26","title":"Phase 5 Articles (Weeks 23-26)","text":""},{"location":"marketing/medium_articles/#article-23-building-a-semantic-layer-metrics-everyone-can-trust","title":"Article 23: \"Building a Semantic Layer: Metrics Everyone Can Trust\"","text":"<p>Target length: 2,200 words LinkedIn tie-in: Week 23 posts</p>"},{"location":"marketing/medium_articles/#article-24-production-data-pipelines-monitoring-retry-and-testing","title":"Article 24: \"Production Data Pipelines: Monitoring, Retry, and Testing\"","text":"<p>Target length: 2,000 words LinkedIn tie-in: Week 24 posts</p>"},{"location":"marketing/medium_articles/#article-25-what-i-learned-building-an-open-source-data-framework","title":"Article 25: \"What I Learned Building an Open Source Data Framework\"","text":"<p>Target length: 1,500 words LinkedIn tie-in: Week 25 posts</p>"},{"location":"marketing/medium_articles/#article-26-6-months-of-data-engineering-content-a-complete-index","title":"Article 26: \"6 Months of Data Engineering Content: A Complete Index\"","text":"<p>Target length: 1,000 words LinkedIn tie-in: Week 26 posts</p> <p>This is a summary/index article linking to everything.</p>"},{"location":"marketing/medium_articles/#publishing-schedule","title":"Publishing Schedule","text":"Week Article LinkedIn Posts 1 Article 1 1.1, 1.2, 1.3 2 Article 2 2.1, 2.2, 2.3 3 Article 3 3.1, 3.2, 3.3 4 Article 4 4.1, 4.2, 4.3 5 Article 5 5.1, 5.2, 5.3 ... ... ... 26 Article 26 26.1, 26.2, 26.3"},{"location":"marketing/medium_articles/#medium-tips","title":"Medium Tips","text":"<ol> <li>Publish on Tuesday-Thursday for best reach</li> <li>Use code blocks with syntax highlighting</li> <li>Add diagrams (use Mermaid or draw.io)</li> <li>Include a \"TL;DR\" at the top for skimmers</li> <li>Link to related articles you've written</li> <li>End with a CTA (follow, clap, comment)</li> <li>Cross-post to LinkedIn after publishing</li> </ol>"},{"location":"marketing/articles/article_01_origin_story/","title":"How I Taught Myself Data Engineering While Working in Operations","text":"<p>From night shifts to building data platforms-a self-taught journey</p>"},{"location":"marketing/articles/article_01_origin_story/#tldr","title":"TL;DR","text":"<p>I graduated during COVID, joined a rotational leadership program in operations, and taught myself data engineering through 1000+ hours of self-study. No CS degree. No formal training. Just real problems and the willingness to figure things out. Here's how it happened and what I learned.</p>"},{"location":"marketing/articles/article_01_origin_story/#the-beginning-2020","title":"The Beginning: 2020","text":"<p>I graduated from Texas Tech University in 2020, right as the world shut down.</p> <p>Like many graduates that year, my plans changed overnight. Job offers evaporated. The economy was uncertain. But I had an opportunity-a rotational leadership development program at a global manufacturing company.</p> <p>The catch? It started in operations.</p>"},{"location":"marketing/articles/article_01_origin_story/#rotation-1-production-supervision","title":"Rotation 1: Production Supervision","text":"<p>My first assignment was supervising a production line. Night shifts. 12 hours at a time. Three nights on, three nights off.</p> <p>At first, I didn't see the connection to where I wanted to end up. I was there to learn leadership, not to become a plant manager.</p> <p>But looking back, those 18 months taught me things I couldn't have learned anywhere else:</p> <ul> <li>How businesses actually run. Not from a PowerPoint, but from the floor.</li> <li>How to lead people. Real people, with real problems, at 3am.</li> <li>How to operate in ambiguity. Production doesn't wait for perfect information.</li> </ul> <p>I didn't know it yet, but this operational perspective would become my superpower in data.</p>"},{"location":"marketing/articles/article_01_origin_story/#rotation-2-the-pivot","title":"Rotation 2: The Pivot","text":"<p>After 18 months, I moved to an engineering role at our largest global facility. The assignment: map energy usage across the entire plant.</p> <p>Nobody handed me a tool or a method. Just a problem.</p> <p>I started in Excel. Built some charts. They were... fine.</p> <p>But I wanted more. I started exploring Power BI. Then I realized the data I needed wasn't easily accessible. That led me to Azure. Which led to SQL. Which led to Python.</p> <p>One problem unlocked the next skill.</p>"},{"location":"marketing/articles/article_01_origin_story/#the-learning-phase-1000-hours","title":"The Learning Phase: 1000+ Hours","text":"<p>Here's the truth about self-teaching: it's not glamorous.</p> <p>I watched YouTube tutorials during lunch. Took Udemy courses on weekends. Did LinkedIn Learning modules whenever I had time. Stayed up way too late reading documentation.</p> <p>In one year, I logged over 1000 hours of learning outside of work.</p> <p>Not because I'm special. Because I was hungry to learn. And because the problems in front of me were real-I couldn't half-solve them.</p>"},{"location":"marketing/articles/article_01_origin_story/#what-i-studied-in-rough-order","title":"What I Studied (In Rough Order)","text":"<ol> <li>Excel - Where everyone starts</li> <li>Power BI - Visualization and basic data modeling</li> <li>SQL - The foundation I should have learned first</li> <li>Azure fundamentals - Cloud storage, basic services</li> <li>Azure Data Factory - Building pipelines</li> <li>Python - Scripting, automation, data manipulation</li> <li>Databricks/Spark - Large-scale processing</li> </ol> <p>Each skill unlocked the next problem I could solve.</p>"},{"location":"marketing/articles/article_01_origin_story/#resources-that-helped","title":"Resources That Helped","text":"<ul> <li>YouTube - Free, visual, endless content. Great for getting started.</li> <li>Udemy - Structured courses, cheap during sales. Good for going deeper.</li> <li>LinkedIn Learning - Solid for fundamentals, often free through employers.</li> <li>Documentation - The hardest but most valuable. Microsoft Docs, Databricks docs, etc.</li> <li>AI assistants - ChatGPT became invaluable for debugging and learning faster.</li> </ul>"},{"location":"marketing/articles/article_01_origin_story/#rotation-3-corporate-engineering","title":"Rotation 3: Corporate Engineering","text":"<p>My third rotation took me to the global engineering team at corporate headquarters.</p> <p>Same pattern: they needed data work, I figured it out.</p> <p>I built: - A global cycle time report - A global energy report - A global yield report</p> <p>Each project was harder than the last. Each one taught me something new.</p> <p>I also did a 9-month training program that touched on AI/ML. But I quickly realized: the foundations weren't solid enough. I needed to focus on data engineering before data science.</p>"},{"location":"marketing/articles/article_01_origin_story/#the-current-role-analytics-data-engineering","title":"The Current Role: Analytics &amp; Data Engineering","text":"<p>After my rotational program ended, I joined a global digital team focused on analytics.</p> <p>My official title was \"data analyst.\" But I told my boss: I'd add more value on the backend, managing the data.</p> <p>So that's what I did.</p> <p>I took over a report with terrible performance. The ask: push complexity to the backend so dashboards load faster.</p> <p>The challenge: I didn't have direct access to the source databases. Security requirements meant I had to get creative.</p>"},{"location":"marketing/articles/article_01_origin_story/#getting-creative-with-constraints","title":"Getting Creative With Constraints","text":"<p>In large organizations, data governance exists for good reason. Security matters. Compliance matters.</p> <p>But I still needed to deliver value.</p> <p>So I found solutions: - Queried semantic models instead of source tables - Used Power Automate to extract data in chunks - Built pipelines in Azure Data Factory - Partnered with the IT team, built trust, and eventually got my own Databricks workspace</p> <p>The Azure admin who helped me became a key ally. That relationship opened doors.</p> <p>Lesson learned: Constraints force creativity. And relationships open doors that tools can't.</p>"},{"location":"marketing/articles/article_01_origin_story/#what-i-do-now","title":"What I Do Now","text":"<p>Today, I'm the go-to data person for global operations at my company.</p> <p>I manage the data infrastructure for our team. I build pipelines, maintain data models, and support reporting across multiple regions.</p> <p>I'm not in IT. I'm in operations. Which gives me a unique perspective: I understand both the business problems and the technical solutions.</p>"},{"location":"marketing/articles/article_01_origin_story/#what-i-learned-along-the-way","title":"What I Learned Along The Way","text":""},{"location":"marketing/articles/article_01_origin_story/#1-you-dont-need-permission-to-learn","title":"1. You Don't Need Permission to Learn","text":"<p>Nobody asked me to learn Python. Nobody assigned me Azure training. I just started.</p> <p>The most valuable career decisions I've made were things I chose to do, not things I was told to do.</p>"},{"location":"marketing/articles/article_01_origin_story/#2-sql-is-80-of-the-job","title":"2. SQL is 80% of the Job","text":"<p>I spent months learning fancy tools. I should have mastered SQL first.</p> <p>Everything-Power BI, Spark, dbt, whatever-eventually talks to data through SQL or something SQL-like. Learn it deeply.</p>"},{"location":"marketing/articles/article_01_origin_story/#3-the-hard-part-isnt-technical","title":"3. The Hard Part Isn't Technical","text":"<p>Getting access to data is hard. Understanding what the business actually needs is hard. Communicating with non-technical stakeholders is hard.</p> <p>The code is the easy part.</p>"},{"location":"marketing/articles/article_01_origin_story/#4-rewriting-is-normal","title":"4. Rewriting is Normal","text":"<p>I've rebuilt my pipelines four or five times. Each time because I learned a better approach.</p> <p>That's not failure. That's growth.</p>"},{"location":"marketing/articles/article_01_origin_story/#5-documentation-is-not-optional","title":"5. Documentation is Not Optional","text":"<p>Future you will forget why you did something. Write it down.</p> <p>When you're the only one who understands a system, documentation is your safety net.</p>"},{"location":"marketing/articles/article_01_origin_story/#6-build-systems-not-scripts","title":"6. Build Systems, Not Scripts","text":"<p>A script solves one problem one time. A system solves many problems many times.</p> <p>When you're solo, you can't maintain 50 one-off notebooks. You need reusable patterns.</p>"},{"location":"marketing/articles/article_01_origin_story/#why-im-sharing-this","title":"Why I'm Sharing This","text":"<p>I learned data engineering the hard way-piecing together YouTube videos, documentation, and trial-and-error at 2am.</p> <p>There's no single resource that covers the practical, end-to-end journey from \"I have no idea what I'm doing\" to \"I can build production data pipelines.\"</p> <p>So I'm creating that resource.</p> <p>Over the coming months, I'll be sharing: - How I approach data engineering problems - Patterns and anti-patterns I've learned - Code and configurations you can reuse - A framework I built to make this easier</p> <p>If you're on a similar journey-or thinking about starting one-I hope this helps.</p>"},{"location":"marketing/articles/article_01_origin_story/#the-bottom-line","title":"The Bottom Line","text":"<p>You don't need a CS degree to become a data engineer.</p> <p>You need: - A real problem to solve - The willingness to look stupid while you figure it out - 1000 hours of focus</p> <p>The path isn't linear. The learning never stops. But if you're hungry enough, you can teach yourself almost anything.</p> <p>I'm Henry-a data engineer who started in operations. I write about data engineering, building in public, and the tools I create along the way. Follow along if that's interesting to you.</p>"},{"location":"marketing/articles/article_01_origin_story/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/","title":"The Bronze Layer Mistake That Cost Me 6 Months of Data","text":"<p>Why you should never transform data during ingestion-and what to do instead</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#tldr","title":"TL;DR","text":"<p>I \"cleaned\" data during ingestion to the Bronze layer-removing duplicates, filtering records, transforming dates. Six months later, the business needed the original values. They were gone. The lesson: Bronze layer has one job-preserve raw data exactly as received. All transformations belong in Silver.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-mistake","title":"The Mistake","text":"<p>It seemed like a good idea at the time.</p> <p>I was building a data pipeline for operational reporting. The source data was messy: - Duplicate records - Inconsistent date formats - Records with statuses we \"didn't need\" - Columns with strange characters</p> <p>So I cleaned it up. Right there, during the ingest process.</p> <pre><code># What I did (DON'T DO THIS)\n- name: bronze_orders\n  read:\n    connection: source\n    path: orders.csv\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status != 'canceled'\"\n      - function: deduplicate\n        params:\n          keys: [order_id]\n      - function: standardize_dates\n        params:\n          columns: [order_date]\n  write:\n    connection: bronze\n    path: orders\n</code></pre> <p>Clean data in, clean data stored. Efficient, right?</p> <p>Wrong.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-consequences","title":"The Consequences","text":"<p>Six months later, the business came with a request:</p> <p>\"We need to analyze canceled orders. Can you show us the original cancellation patterns?\"</p> <p>Me: \"Sure, let me pull that from... oh.\"</p> <p>The canceled orders were gone. I had filtered them out during ingestion.</p> <p>\"Also, we noticed some date discrepancies. Can you show us what the original dates looked like before transformation?\"</p> <p>Me: \"I... no. I can't.\"</p> <p>I had transformed the dates. The originals were overwritten.</p> <p>Six months of historical data. Unrecoverable.</p> <p>Not because of a system failure. Because of a design decision I made on day one.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#what-is-the-bronze-layer","title":"What is the Bronze Layer?","text":"<p>To understand why this was such a big mistake, you need to understand the Medallion Architecture.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-medallion-architecture","title":"The Medallion Architecture","text":"<p>Modern data platforms typically organize data into three layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      YOUR DATA LAKE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   BRONZE    \u2502     SILVER      \u2502           GOLD              \u2502\n\u2502   (Raw)     \u2502    (Cleaned)    \u2502       (Business-Ready)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 As-is     \u2502 \u2022 Deduplicated  \u2502 \u2022 Aggregated                \u2502\n\u2502 \u2022 Untouched \u2502 \u2022 Validated     \u2502 \u2022 Joined                    \u2502\n\u2502 \u2022 Archived  \u2502 \u2022 Typed         \u2502 \u2022 Ready for dashboards      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Bronze: Raw data, exactly as received from the source. No transformations.</p> <p>Silver: Cleaned, validated, deduplicated. Business logic applied. Single source of truth.</p> <p>Gold: Aggregated, joined, optimized for consumption. Ready for dashboards and reports.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#why-bronze-exists","title":"Why Bronze Exists","text":"<p>Bronze is your insurance policy.</p> <p>When (not if) something goes wrong downstream, Bronze lets you: - See what the source actually sent - Reprocess with different logic - Debug data quality issues - Answer questions you didn't anticipate</p> <p>If you transform in Bronze, you lose all of this.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-right-way","title":"The Right Way","text":"<p>Here's what I should have done:</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#step-1-bronze-land-raw-data","title":"Step 1: Bronze - Land Raw Data","text":"<pre><code>- name: bronze_orders\n  read:\n    connection: source\n    path: orders.csv\n\n  # Only add metadata - no transformations!\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            _extracted_at: \"current_timestamp()\"\n            _source_file: \"'orders.csv'\"\n\n  write:\n    connection: bronze\n    path: orders\n    format: delta\n    mode: append\n</code></pre> <p>That's it. Read it. Add metadata. Write it.</p> <p>The only columns I add in Bronze: - <code>_extracted_at</code> - When this data was loaded - <code>_source_file</code> - Where it came from - <code>_batch_id</code> - Optional, for tracking pipeline runs</p> <p>These are metadata columns that don't alter the source data. They help with debugging but preserve everything from the source.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#step-2-silver-clean-and-transform","title":"Step 2: Silver - Clean and Transform","text":"<pre><code>- name: silver_orders\n  read:\n    connection: bronze\n    path: orders\n\n  # NOW we transform\n  transform:\n    steps:\n      - function: deduplicate\n        params:\n          keys: [order_id]\n          order_by: \"_extracted_at DESC\"\n\n      - function: cast_columns\n        params:\n          columns:\n            order_date: timestamp\n            amount: decimal\n\n      - function: clean_text\n        params:\n          columns: [customer_name]\n          trim: true\n          lowercase: true\n\n  # Validation contracts\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: accepted_values\n      column: status\n      values: [pending, shipped, delivered, canceled]\n\n  write:\n    connection: silver\n    path: orders\n    format: delta\n</code></pre> <p>Notice: I'm NOT filtering out canceled orders. I'm validating that the status values are expected, but I'm keeping all records.</p> <p>If I want to exclude canceled orders, I do that in Gold or in specific reports-not in the core data layer.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#step-3-gold-aggregate-and-optimize","title":"Step 3: Gold - Aggregate and Optimize","text":"<pre><code>- name: gold_daily_orders\n  read:\n    connection: silver\n    path: orders\n\n  # Filter if needed for this specific use case\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status = 'delivered'\"\n\n  # Aggregate\n  pattern:\n    type: aggregation\n    params:\n      grain: [order_date, region]\n      measures:\n        - name: order_count\n          expr: \"COUNT(*)\"\n        - name: total_revenue\n          expr: \"SUM(amount)\"\n\n  write:\n    connection: gold\n    table: daily_orders_summary\n</code></pre> <p>Now if the business wants canceled orders, I can: 1. Go back to Bronze (raw data is intact) 2. Create a different Gold table with different filters 3. Answer questions I didn't anticipate</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-rules","title":"The Rules","text":""},{"location":"marketing/articles/article_02_bronze_layer_mistake/#what-belongs-in-bronze","title":"What Belongs in Bronze","text":"<p>\u2705 Do: - Store data exactly as received - Add extraction metadata (_extracted_at, _source_file) - Use append mode (preserve history) - Use a durable format (Delta Lake, Parquet)</p> <p>\u274c Don't: - Filter rows - Transform values - Deduplicate - Apply business logic - Cast types</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#what-belongs-in-silver","title":"What Belongs in Silver","text":"<p>\u2705 Do: - Deduplicate - Cast types - Standardize formats - Validate data quality - Apply business rules - Handle nulls</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#what-belongs-in-gold","title":"What Belongs in Gold","text":"<p>\u2705 Do: - Aggregate for specific use cases - Join across domains - Optimize for query patterns - Build dimensional models</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#real-world-implications","title":"Real-World Implications","text":""},{"location":"marketing/articles/article_02_bronze_layer_mistake/#debugging-is-easier","title":"Debugging is Easier","text":"<p>When a dashboard shows wrong numbers, the first question is: \"What did the source send?\"</p> <p>With Bronze intact, you can answer that in seconds.</p> <p>Without Bronze, you're guessing.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#reprocessing-is-possible","title":"Reprocessing is Possible","text":"<p>Business logic changes. What was \"correct\" six months ago might be wrong today.</p> <p>With Bronze, you can reprocess all historical data with new logic.</p> <p>Without Bronze, you're stuck with decisions made months ago.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#auditing-is-simple","title":"Auditing is Simple","text":"<p>Auditors ask: \"Show me the original data.\"</p> <p>With Bronze, you show them.</p> <p>Without Bronze, you show them transformed data and hope they don't ask follow-up questions.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-cost-of-getting-it-wrong","title":"The Cost of Getting it Wrong","text":"<p>In my case: - Six months of canceled order data: unrecoverable - Original date formats: lost - Audit trail: compromised - Trust with business stakeholders: damaged</p> <p>All because I wanted to be \"efficient\" by cleaning data early.</p> <p>The storage cost of keeping raw data? Negligible.</p> <p>The cost of losing it? Significant.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#how-i-fixed-it","title":"How I Fixed It","text":"<p>I couldn't recover the lost data. What I could do:</p> <ol> <li>Redesign the pipeline with proper Bronze/Silver/Gold separation</li> <li>Document the incident so others don't repeat it</li> <li>Build better habits for future projects</li> </ol> <p>From that point forward, Bronze has been sacred. No transformations. Ever.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#takeaways","title":"Takeaways","text":"<ol> <li> <p>Bronze is your undo button. Don't break it.</p> </li> <li> <p>Store everything. Disk is cheap. Lost data is expensive.</p> </li> <li> <p>Transform in Silver. That's what it's for.</p> </li> <li> <p>Filter in Gold. Specific use cases get specific views.</p> </li> <li> <p>Add metadata. <code>_extracted_at</code> and <code>_source_file</code> cost nothing and save hours of debugging.</p> </li> </ol>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#the-framework","title":"The Framework","text":"<p>After making this mistake (and others), I started building patterns to prevent them.</p> <p>That became Odibi-a framework for declarative data pipelines that enforces good practices by default.</p> <p>If you're interested in how I approach these problems now, follow along. I'll be sharing more patterns, anti-patterns, and lessons learned.</p> <p>I'm Henry-a data engineer who learned most of these lessons the hard way. I write about data engineering, building in public, and the tools I create along the way.</p>"},{"location":"marketing/articles/article_02_bronze_layer_mistake/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]  </li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_03_bronze_layer_setup/","title":"Setting Up a Bronze Layer with Delta Lake","text":"<p>A complete walkthrough using the Brazilian E-Commerce dataset</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#tldr","title":"TL;DR","text":"<p>This article walks through setting up a Bronze layer for 8 CSV files using Delta Lake. We'll configure connections, create nodes for each file, add extraction metadata, and run the pipeline. By the end, you'll have a complete Bronze layer ready for Silver processing.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#what-were-building","title":"What We're Building","text":"<p>We're using the Brazilian E-Commerce dataset from Kaggle. It contains:</p> File Description Rows olist_orders_dataset.csv Order headers ~100k olist_order_items_dataset.csv Line items ~113k olist_customers_dataset.csv Customer info ~100k olist_products_dataset.csv Product catalog ~33k olist_sellers_dataset.csv Seller info ~3k olist_order_payments_dataset.csv Payment details ~104k olist_order_reviews_dataset.csv Customer reviews ~100k olist_geolocation_dataset.csv Zip code data ~1M <p>Our goal: Land all of these in Bronze, exactly as-is, with metadata for debugging.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#project-structure","title":"Project Structure","text":"<pre><code>ecommerce_warehouse/\n\u251c\u2500\u2500 odibi.yaml              # Main config\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 landing/            # Raw CSV files go here\n\u251c\u2500\u2500 bronze/                 # Bronze layer output\n\u251c\u2500\u2500 silver/                 # Silver layer output (later)\n\u2514\u2500\u2500 gold/                   # Gold layer output (later)\n</code></pre>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#step-1-download-the-dataset","title":"Step 1: Download the Dataset","text":"<ol> <li>Go to Kaggle</li> <li>Download and extract to <code>data/landing/</code></li> <li>You should have 8 CSV files</li> </ol>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#step-2-configure-connections","title":"Step 2: Configure Connections","text":"<p>First, we define where data lives:</p> <pre><code># odibi.yaml\n\nproject: \"ecommerce_warehouse\"\nengine: \"pandas\"  # Use pandas for local dev, spark for production\n\nconnections:\n  # Where raw CSVs are\n  landing:\n    type: local\n    base_path: \"./data/landing\"\n\n  # Where Bronze layer goes\n  bronze:\n    type: local\n    base_path: \"./bronze\"\n\n  # Where Silver layer goes (for later)\n  silver:\n    type: local\n    base_path: \"./silver\"\n\n  # Where Gold layer goes (for later)\n  gold:\n    type: local\n    base_path: \"./gold\"\n</code></pre> <p>For production, you'd use cloud connections:</p> <pre><code>connections:\n  landing:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    container: landing\n    credential: ${STORAGE_KEY}\n\n  bronze:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    container: bronze\n    credential: ${STORAGE_KEY}\n</code></pre>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#step-3-create-bronze-nodes","title":"Step 3: Create Bronze Nodes","text":"<p>Each source file gets its own node. The pattern is identical for all:</p> <ol> <li>Read from landing</li> <li>Add extraction metadata</li> <li>Write to Bronze as Delta</li> </ol>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#orders-node","title":"Orders Node","text":"<pre><code>pipelines:\n  - pipeline: bronze_ecommerce\n    layer: bronze\n    description: \"Land raw e-commerce data\"\n\n    nodes:\n      - name: bronze_orders\n        description: \"Raw order headers\"\n\n        read:\n          connection: landing\n          path: olist_orders_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_orders_dataset.csv'\"\n\n        write:\n          connection: bronze\n          path: orders\n          format: delta\n          mode: append\n</code></pre>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#all-other-nodes","title":"All Other Nodes","text":"<p>The same pattern repeats. Here's the complete Bronze pipeline:</p> <pre><code>pipelines:\n  - pipeline: bronze_ecommerce\n    layer: bronze\n    description: \"Land raw e-commerce data from Kaggle dataset\"\n\n    nodes:\n      # Orders\n      - name: bronze_orders\n        description: \"Raw order headers\"\n        read:\n          connection: landing\n          path: olist_orders_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_orders_dataset.csv'\"\n        write:\n          connection: bronze\n          path: orders\n          format: delta\n          mode: append\n\n      # Order Items\n      - name: bronze_order_items\n        description: \"Raw order line items\"\n        read:\n          connection: landing\n          path: olist_order_items_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_order_items_dataset.csv'\"\n        write:\n          connection: bronze\n          path: order_items\n          format: delta\n          mode: append\n\n      # Customers\n      - name: bronze_customers\n        description: \"Raw customer data\"\n        read:\n          connection: landing\n          path: olist_customers_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_customers_dataset.csv'\"\n        write:\n          connection: bronze\n          path: customers\n          format: delta\n          mode: append\n\n      # Products\n      - name: bronze_products\n        description: \"Raw product catalog\"\n        read:\n          connection: landing\n          path: olist_products_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_products_dataset.csv'\"\n        write:\n          connection: bronze\n          path: products\n          format: delta\n          mode: append\n\n      # Sellers\n      - name: bronze_sellers\n        description: \"Raw seller data\"\n        read:\n          connection: landing\n          path: olist_sellers_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_sellers_dataset.csv'\"\n        write:\n          connection: bronze\n          path: sellers\n          format: delta\n          mode: append\n\n      # Payments\n      - name: bronze_payments\n        description: \"Raw payment data\"\n        read:\n          connection: landing\n          path: olist_order_payments_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_order_payments_dataset.csv'\"\n        write:\n          connection: bronze\n          path: payments\n          format: delta\n          mode: append\n\n      # Reviews\n      - name: bronze_reviews\n        description: \"Raw review data\"\n        read:\n          connection: landing\n          path: olist_order_reviews_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_order_reviews_dataset.csv'\"\n        write:\n          connection: bronze\n          path: reviews\n          format: delta\n          mode: append\n\n      # Geolocation\n      - name: bronze_geolocation\n        description: \"Raw geolocation data\"\n        read:\n          connection: landing\n          path: olist_geolocation_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_geolocation_dataset.csv'\"\n        write:\n          connection: bronze\n          path: geolocation\n          format: delta\n          mode: append\n</code></pre>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#step-4-run-the-pipeline","title":"Step 4: Run the Pipeline","text":"<pre><code># Install odibi if you haven't\npip install odibi\n\n# Run the bronze pipeline\nodibi run odibi.yaml --pipeline bronze_ecommerce\n</code></pre> <p>You should see output like:</p> <pre><code>[INFO] Starting pipeline: bronze_ecommerce\n[INFO] Executing node: bronze_orders\n[INFO]   Read 99,441 rows from olist_orders_dataset.csv\n[INFO]   Added metadata columns\n[INFO]   Written to bronze/orders (delta format)\n[INFO] Executing node: bronze_order_items\n[INFO]   Read 112,650 rows from olist_order_items_dataset.csv\n...\n[INFO] Pipeline bronze_ecommerce completed successfully\n</code></pre>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#step-5-verify-the-results","title":"Step 5: Verify the Results","text":"<p>Check that Delta tables were created:</p> <pre><code>import pandas as pd\nfrom deltalake import DeltaTable\n\n# Read a Bronze table\ndt = DeltaTable(\"./bronze/orders\")\ndf = dt.to_pandas()\n\nprint(f\"Rows: {len(df)}\")\nprint(f\"Columns: {df.columns.tolist()}\")\nprint(df.head())\n</code></pre> <p>You should see: - All original columns from the CSV - <code>_extracted_at</code> with the load timestamp - <code>_source_file</code> with the source filename</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#why-delta-lake","title":"Why Delta Lake?","text":"<p>We write to Delta format instead of CSV or Parquet because:</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#1-acid-transactions","title":"1. ACID Transactions","text":"<p>Delta Lake provides atomic writes. Either the whole write succeeds or nothing changes. No partial files.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#2-time-travel","title":"2. Time Travel","text":"<p>Delta keeps history. You can query previous versions:</p> <pre><code># Read version from 5 writes ago\ndt = DeltaTable(\"./bronze/orders\")\ndf = dt.load_as_version(5).to_pandas()\n</code></pre>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#3-schema-enforcement","title":"3. Schema Enforcement","text":"<p>Delta validates schema on write. If source columns change, you'll know.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#4-efficient-updates","title":"4. Efficient Updates","text":"<p>When we get to Silver, Delta enables efficient upserts and deletes.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#what-we-achieved","title":"What We Achieved","text":"<p>After running this pipeline:</p> Table Rows Columns bronze.orders 99,441 10 (8 + 2 metadata) bronze.order_items 112,650 9 (7 + 2 metadata) bronze.customers 99,441 7 (5 + 2 metadata) bronze.products 32,951 11 (9 + 2 metadata) bronze.sellers 3,095 6 (4 + 2 metadata) bronze.payments 103,886 7 (5 + 2 metadata) bronze.reviews 100,000 7 (5 + 2 metadata) bronze.geolocation 1,000,163 7 (5 + 2 metadata) <p>Total: ~1.5 million rows landed in Bronze, with full metadata and Delta Lake durability.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#key-principles-followed","title":"Key Principles Followed","text":"<ol> <li>No transformations in Bronze - Data is stored exactly as received</li> <li>Metadata columns added - <code>_extracted_at</code> and <code>_source_file</code> for debugging</li> <li>Append mode - Each run adds to history, never overwrites</li> <li>Delta format - ACID guarantees, time travel, schema enforcement</li> <li>One node per source - Clear lineage and easier debugging</li> </ol>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#next-steps","title":"Next Steps","text":"<p>With Bronze complete, we're ready for Silver:</p> <ul> <li>Deduplicate records</li> <li>Cast types</li> <li>Validate data quality</li> <li>Standardize formats</li> </ul> <p>That's the next article.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#complete-configuration-file","title":"Complete Configuration File","text":"<p>Here's the full <code>odibi.yaml</code> for reference:</p> <pre><code>project: \"ecommerce_warehouse\"\nengine: \"pandas\"\nversion: \"1.0.0\"\ndescription: \"Brazilian E-Commerce Data Warehouse\"\n\nconnections:\n  landing:\n    type: local\n    base_path: \"./data/landing\"\n\n  bronze:\n    type: local\n    base_path: \"./bronze\"\n\n  silver:\n    type: local\n    base_path: \"./silver\"\n\n  gold:\n    type: local\n    base_path: \"./gold\"\n\nstory:\n  connection: bronze\n  path: \"_stories\"\n  retention_days: 30\n\nsystem:\n  connection: bronze\n  path: \"_system\"\n\npipelines:\n  - pipeline: bronze_ecommerce\n    layer: bronze\n    description: \"Land raw e-commerce data from Kaggle dataset\"\n\n    nodes:\n      # ... (all 8 nodes from above)\n</code></pre> <p>Next article: Building the Silver Layer - cleaning, validating, and deduplicating our Bronze data.</p>"},{"location":"marketing/articles/article_03_bronze_layer_setup/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_04_data_quality_contracts/","title":"Data Quality Contracts: Catching Problems Before Production","text":"<p>Stop bad data before it ruins your dashboards</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#tldr","title":"TL;DR","text":"<p>Data contracts are pre-conditions that run before your pipeline loads data. If they fail, the pipeline stops-or routes bad records to quarantine. This article shows how to configure contracts for null checks, value validation, row counts, uniqueness, and custom SQL. We'll apply them to our Silver layer and set up quarantine for failed records.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#the-problem-with-late-discovery","title":"The Problem With Late Discovery","text":"<p>You load data at 6 AM. The dashboard updates. At 9 AM, the CFO notices revenue is $0.</p> <p>What happened? A null crept into <code>order_status</code> and filtered out every order. You didn't find out until someone looked at the dashboard.</p> <p>This is the late discovery problem. By the time you catch bad data, it's already in production. The fix requires:</p> <ol> <li>Finding the bad records</li> <li>Understanding how they got in</li> <li>Reloading corrected data</li> <li>Rebuilding downstream tables</li> <li>Apologizing to stakeholders</li> </ol> <p>There's a better way: stop bad data at the gate.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#contracts-vs-validation","title":"Contracts vs Validation","text":"<p>Before we dive in, let's clarify two related concepts:</p> Concept When It Runs What It Does Contracts Before transformation Pre-conditions on input data Validation After transformation Post-conditions on output data <p>Think of it this way: - Contracts = \"I won't process garbage\" - Validation = \"I didn't produce garbage\"</p> <p>You need both. This article focuses on contracts.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#types-of-contracts","title":"Types of Contracts","text":"<p>Odibi supports several contract types out of the box:</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#1-not-null","title":"1. Not Null","text":"<p>The most common contract. Ensures critical columns have values.</p> <pre><code>contracts:\n  - type: not_null\n    column: customer_id\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#2-accepted-values","title":"2. Accepted Values","text":"<p>Ensures a column contains only expected values.</p> <pre><code>contracts:\n  - type: accepted_values\n    column: order_status\n    values:\n      - created\n      - approved\n      - shipped\n      - delivered\n      - canceled\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#3-uniqueness","title":"3. Uniqueness","text":"<p>Ensures no duplicate values in a column or combination of columns.</p> <pre><code>contracts:\n  - type: unique\n    columns:\n      - order_id\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#4-row-count","title":"4. Row Count","text":"<p>Ensures the source has enough rows. Catches truncated files.</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000\n    severity: error\n</code></pre> <p>Or detect suspiciously small files:</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000\n    max: 10000000  # Suspiciously large\n    severity: warn\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#5-range","title":"5. Range","text":"<p>Ensures numeric values fall within bounds.</p> <pre><code>contracts:\n  - type: range\n    column: price\n    min: 0\n    max: 100000\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#6-regex-match","title":"6. Regex Match","text":"<p>Validates string patterns.</p> <pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n    severity: warn\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#7-custom-sql","title":"7. Custom SQL","text":"<p>For complex business rules.</p> <pre><code>contracts:\n  - type: custom_sql\n    name: \"order_items_have_orders\"\n    sql: |\n      SELECT COUNT(*) = 0 \n      FROM df oi\n      LEFT JOIN bronze_orders o ON oi.order_id = o.order_id\n      WHERE o.order_id IS NULL\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#severity-levels","title":"Severity Levels","text":"<p>Each contract has a severity:</p> Severity Behavior <code>error</code> Pipeline fails immediately <code>warn</code> Log warning, continue processing <p>Use <code>error</code> for critical data issues that would break downstream systems.</p> <p>Use <code>warn</code> for issues you want to monitor but not stop the pipeline.</p> <pre><code>contracts:\n  # Critical: can't have orders without IDs\n  - type: not_null\n    column: order_id\n    severity: error\n\n  # Nice to have: reviews can be empty\n  - type: not_null\n    column: review_comment\n    severity: warn\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#full-example-silver-layer-with-contracts","title":"Full Example: Silver Layer with Contracts","text":"<p>Let's apply contracts to our Brazilian E-Commerce Silver layer:</p> <pre><code>pipelines:\n  - pipeline: silver_ecommerce\n    layer: silver\n    description: \"Cleaned and validated e-commerce data\"\n\n    nodes:\n      - name: silver_orders\n        description: \"Validated order data\"\n\n        read:\n          connection: bronze\n          path: orders\n          format: delta\n\n        # Pre-conditions: validate BEFORE processing\n        contracts:\n          # Critical: every order needs an ID\n          - type: not_null\n            column: order_id\n            severity: error\n\n          # Critical: must have a customer\n          - type: not_null\n            column: customer_id\n            severity: error\n\n          # Critical: order IDs should be unique\n          - type: unique\n            columns: [order_id]\n            severity: error\n\n          # Business rule: only valid statuses\n          - type: accepted_values\n            column: order_status\n            values:\n              - created\n              - approved\n              - invoiced\n              - processing\n              - shipped\n              - delivered\n              - canceled\n              - unavailable\n            severity: error\n\n          # Sanity check: should have data\n          - type: row_count\n            min: 1000\n            severity: warn\n\n        # Now transform (runs only if contracts pass)\n        transform:\n          steps:\n            - function: clean_text\n              params:\n                columns: [order_status]\n                case: lower\n                trim: true\n\n            - function: derive_columns\n              params:\n                columns:\n                  order_purchase_date: \"TO_DATE(order_purchase_timestamp)\"\n                  order_delivered_date: \"TO_DATE(order_delivered_customer_date)\"\n\n        write:\n          connection: silver\n          path: orders\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#what-happens-when-contracts-fail","title":"What Happens When Contracts Fail","text":"<p>When a contract with <code>severity: error</code> fails:</p> <pre><code>[ERROR] Contract failed: not_null on customer_id\n[ERROR]   Found 23 null values\n[ERROR]   Sample failing rows:\n[ERROR]     Row 4521: order_id=abc123, customer_id=NULL\n[ERROR]     Row 8932: order_id=def456, customer_id=NULL\n[ERROR] Pipeline stopped. No data written.\n</code></pre> <p>The pipeline halts. No data is written. You can investigate before bad data spreads.</p> <p>When <code>severity: warn</code>:</p> <pre><code>[WARN] Contract warning: row_count\n[WARN]   Expected min 1000, got 847\n[WARN] Continuing with pipeline...\n</code></pre> <p>The pipeline continues, but you're notified.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#quarantine-pattern","title":"Quarantine Pattern","text":"<p>Sometimes you don't want to fail the entire pipeline. You want to route bad records somewhere else and process the good ones.</p> <p>That's the quarantine pattern.</p> <pre><code>- name: silver_orders\n  description: \"Validated orders with quarantine\"\n\n  read:\n    connection: bronze\n    path: orders\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: order_id\n      severity: error  # Still fail on null IDs\n\n    - type: not_null\n      column: customer_id\n      severity: quarantine  # Route to quarantine\n\n    - type: accepted_values\n      column: order_status\n      values: [created, approved, shipped, delivered, canceled]\n      severity: quarantine  # Route to quarantine\n\n  # Where to send failed records\n  validation:\n    quarantine:\n      connection: silver\n      path: orders_quarantine\n      add_columns:\n        _rejection_reason: true\n        _rejected_at: true\n\n  write:\n    connection: silver\n    path: orders\n    format: delta\n</code></pre> <p>With this config: - Null <code>order_id</code> \u2192 Pipeline fails (critical) - Null <code>customer_id</code> \u2192 Record goes to quarantine - Invalid <code>order_status</code> \u2192 Record goes to quarantine - Good records \u2192 Written to Silver</p> <p>The quarantine table includes: - All original columns - <code>_rejection_reason</code>: Which contract failed - <code>_rejected_at</code>: When it was quarantined</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#quarantine-review-process","title":"Quarantine Review Process","text":"<p>Don't just quarantine and forget. Set up a process:</p> <ol> <li>Daily review: Check quarantine tables for patterns</li> <li>Fix upstream: Work with source systems to fix root causes</li> <li>Reprocess: Once fixed, reload from Bronze</li> <li>Track metrics: Count quarantine rate over time</li> </ol> <pre><code>-- Daily quarantine report\nSELECT \n  DATE(_rejected_at) as rejection_date,\n  _rejection_reason,\n  COUNT(*) as rejected_count\nFROM silver.orders_quarantine\nWHERE _rejected_at &gt;= CURRENT_DATE - 7\nGROUP BY 1, 2\nORDER BY 1 DESC, 3 DESC\n</code></pre> <p>If quarantine rates spike, something changed upstream.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#contracts-for-each-table","title":"Contracts for Each Table","text":"<p>Here's a contract checklist for the e-commerce dataset:</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#orders","title":"Orders","text":"<pre><code>contracts:\n  - type: not_null\n    column: order_id\n  - type: not_null\n    column: customer_id\n  - type: unique\n    columns: [order_id]\n  - type: accepted_values\n    column: order_status\n    values: [created, approved, invoiced, processing, shipped, delivered, canceled, unavailable]\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#order-items","title":"Order Items","text":"<pre><code>contracts:\n  - type: not_null\n    column: order_id\n  - type: not_null\n    column: product_id\n  - type: not_null\n    column: seller_id\n  - type: range\n    column: price\n    min: 0\n  - type: range\n    column: freight_value\n    min: 0\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#customers","title":"Customers","text":"<pre><code>contracts:\n  - type: not_null\n    column: customer_id\n  - type: unique\n    columns: [customer_id]\n  - type: not_null\n    column: customer_zip_code_prefix\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#products","title":"Products","text":"<pre><code>contracts:\n  - type: not_null\n    column: product_id\n  - type: unique\n    columns: [product_id]\n  - type: range\n    column: product_weight_g\n    min: 0\n    max: 100000  # 100kg max seems reasonable\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#payments","title":"Payments","text":"<pre><code>contracts:\n  - type: not_null\n    column: order_id\n  - type: accepted_values\n    column: payment_type\n    values: [credit_card, boleto, voucher, debit_card, not_defined]\n  - type: range\n    column: payment_value\n    min: 0\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#best-practices","title":"Best Practices","text":""},{"location":"marketing/articles/article_04_data_quality_contracts/#1-start-strict-loosen-as-needed","title":"1. Start Strict, Loosen as Needed","text":"<p>Begin with <code>severity: error</code> on everything. When you find legitimate edge cases, loosen specific contracts.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#2-document-why","title":"2. Document Why","text":"<p>Add descriptions to contracts:</p> <pre><code>contracts:\n  - type: not_null\n    column: customer_id\n    description: \"Orders must have customers for dimension lookups\"\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#3-layer-your-checks","title":"3. Layer Your Checks","text":"<ul> <li>Bronze: Minimal (just metadata)</li> <li>Silver: Heavy contracts (this is your validation layer)</li> <li>Gold: Business rule validation</li> </ul>"},{"location":"marketing/articles/article_04_data_quality_contracts/#4-track-contract-failures","title":"4. Track Contract Failures","text":"<p>Log contract failures to a metrics table:</p> <pre><code>system:\n  connection: bronze\n  path: _system\n\n  # Automatically logs contract results\n  metrics:\n    enabled: true\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#5-test-your-contracts","title":"5. Test Your Contracts","text":"<p>Create test files with intentional bad data:</p> <pre><code># test_contracts.py\ndef test_null_customer_rejects():\n    \"\"\"Null customer_id should fail contract\"\"\"\n    bad_data = pd.DataFrame({\n        'order_id': ['123'],\n        'customer_id': [None]\n    })\n\n    result = run_node('silver_orders', input_df=bad_data)\n    assert result.status == 'failed'\n    assert 'not_null' in result.failure_reason\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#common-mistakes","title":"Common Mistakes","text":""},{"location":"marketing/articles/article_04_data_quality_contracts/#1-too-many-warnings","title":"1. Too Many Warnings","text":"<p>If you have 50 warnings, you'll ignore them all. Keep warnings rare and actionable.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#2-contracts-without-context","title":"2. Contracts Without Context","text":"<p>\"23 rows failed\" tells you nothing. Include sample data in failure messages.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#3-quarantine-without-cleanup","title":"3. Quarantine Without Cleanup","text":"<p>Quarantine tables grow forever. Set up retention:</p> <pre><code>quarantine:\n  connection: silver\n  path: orders_quarantine\n  retention_days: 90\n</code></pre>"},{"location":"marketing/articles/article_04_data_quality_contracts/#4-validating-in-bronze","title":"4. Validating in Bronze","text":"<p>Bronze should be untouched. Move all validation to Silver.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#what-we-learned","title":"What We Learned","text":"<ol> <li>Contracts catch problems early - Before bad data spreads</li> <li>Severity levels give control - Error stops, warn notifies</li> <li>Quarantine keeps pipelines running - Process good data, investigate bad</li> <li>Each table needs its own contracts - Based on business rules</li> <li>Track and trend failures - Spikes indicate upstream changes</li> </ol>"},{"location":"marketing/articles/article_04_data_quality_contracts/#next-steps","title":"Next Steps","text":"<p>With contracts protecting our Silver layer, we're ready to:</p> <ol> <li>Build the complete Silver layer configuration</li> <li>Add complex transformations</li> <li>Deduplicate and clean records</li> </ol> <p>That's the next article.</p>"},{"location":"marketing/articles/article_04_data_quality_contracts/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_05_silver_layer_guide/","title":"Complete Silver Layer Configuration Guide","text":"<p>From raw Bronze to clean, validated, analysis-ready data</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#tldr","title":"TL;DR","text":"<p>The Silver layer is where data gets cleaned, deduplicated, type-casted, and validated. This article provides complete configurations for all 8 tables in our Brazilian E-Commerce dataset. We cover deduplication, null handling, text cleaning, date parsing, and data contracts. By the end, you'll have a production-ready Silver layer.</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#what-silver-does","title":"What Silver Does","text":"<p>Silver is the Single Source of Truth (SSOT) for your organization. It's where:</p> Task Why Deduplication Remove duplicate records from source systems Type casting Convert strings to proper dates, numbers, etc. Null handling Replace nulls with defaults or quarantine Text cleaning Trim whitespace, standardize case Validation Ensure data meets business rules Standardization Consistent formats across all tables <p>Bronze is your backup. Silver is what everyone queries.</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#project-structure","title":"Project Structure","text":"<p>We continue from the Bronze layer setup:</p> <pre><code>ecommerce_warehouse/\n\u251c\u2500\u2500 odibi.yaml              # Main config\n\u251c\u2500\u2500 data/landing/           # Raw CSVs\n\u251c\u2500\u2500 bronze/                 # Bronze layer (done)\n\u251c\u2500\u2500 silver/                 # Silver layer (this article)\n\u2514\u2500\u2500 gold/                   # Gold layer (next)\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#common-transformation-patterns","title":"Common Transformation Patterns","text":"<p>Before we configure each table, let's review the patterns we'll use:</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#deduplication","title":"Deduplication","text":"<p>Source systems often send duplicates. Remove them:</p> <pre><code>transformer: deduplicate\nparams:\n  keys: [order_id]\n  order_by: _extracted_at DESC  # Keep latest\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#type-casting","title":"Type Casting","text":"<p>Bronze infers types. Silver enforces them:</p> <pre><code>transform:\n  steps:\n    - function: cast_columns\n      params:\n        columns:\n          price: double\n          quantity: integer\n          order_date: date\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#null-handling","title":"Null Handling","text":"<p>Replace nulls with sensible defaults:</p> <pre><code>transform:\n  steps:\n    - function: fill_nulls\n      params:\n        columns:\n          review_score: 0\n          review_comment: \"\"\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#text-cleaning","title":"Text Cleaning","text":"<p>Standardize text fields:</p> <pre><code>transform:\n  steps:\n    - function: clean_text\n      params:\n        columns: [customer_city, customer_state]\n        case: upper\n        trim: true\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#date-parsing","title":"Date Parsing","text":"<p>Convert timestamp strings to proper dates:</p> <pre><code>transform:\n  steps:\n    - function: derive_columns\n      params:\n        columns:\n          order_date: \"TO_DATE(order_purchase_timestamp)\"\n          delivery_date: \"TO_DATE(order_delivered_customer_date)\"\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#silver-layer-configuration","title":"Silver Layer Configuration","text":"<p>Now let's configure each table.</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#1-silver-orders","title":"1. Silver Orders","text":"<p>The orders table is central to everything. It needs careful cleaning:</p> <pre><code>- name: silver_orders\n  description: \"Cleaned order headers\"\n\n  read:\n    connection: bronze\n    path: orders\n    format: delta\n\n  # Pre-validation contracts\n  contracts:\n    - type: not_null\n      column: order_id\n      severity: error\n\n    - type: not_null\n      column: customer_id\n      severity: error\n\n    - type: unique\n      columns: [order_id]\n      severity: error\n\n    - type: accepted_values\n      column: order_status\n      values: [created, approved, invoiced, processing, shipped, delivered, canceled, unavailable]\n      severity: error\n\n  # Deduplicate first\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: _extracted_at DESC\n\n  # Then clean and transform\n  transform:\n    steps:\n      # Standardize status\n      - function: clean_text\n        params:\n          columns: [order_status]\n          case: lower\n          trim: true\n\n      # Parse timestamps to dates\n      - function: derive_columns\n        params:\n          columns:\n            order_purchase_date: \"TO_DATE(order_purchase_timestamp)\"\n            order_approved_date: \"TO_DATE(order_approved_at)\"\n            order_delivered_carrier_date: \"TO_DATE(order_delivered_carrier_date)\"\n            order_delivered_customer_date: \"TO_DATE(order_delivered_customer_date)\"\n            order_estimated_delivery_date: \"TO_DATE(order_estimated_delivery_date)\"\n\n      # Calculate derived metrics\n      - function: derive_columns\n        params:\n          columns:\n            days_to_delivery: \"DATEDIFF(order_delivered_customer_date, order_purchase_date)\"\n            delivery_delay_days: \"DATEDIFF(order_delivered_customer_date, order_estimated_delivery_date)\"\n            is_late: \"CASE WHEN order_delivered_customer_date &gt; order_estimated_delivery_date THEN true ELSE false END\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            order_id,\n            customer_id,\n            order_status,\n            order_purchase_date,\n            order_approved_date,\n            order_delivered_carrier_date,\n            order_delivered_customer_date,\n            order_estimated_delivery_date,\n            days_to_delivery,\n            delivery_delay_days,\n            is_late,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: orders\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#2-silver-order-items","title":"2. Silver Order Items","text":"<p>Line items need price validation and seller linkage:</p> <pre><code>- name: silver_order_items\n  description: \"Cleaned order line items\"\n\n  read:\n    connection: bronze\n    path: order_items\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: order_id\n      severity: error\n\n    - type: not_null\n      column: product_id\n      severity: error\n\n    - type: not_null\n      column: seller_id\n      severity: error\n\n    - type: range\n      column: price\n      min: 0\n      severity: error\n\n    - type: range\n      column: freight_value\n      min: 0\n      severity: error\n\n  transformer: deduplicate\n  params:\n    keys: [order_id, order_item_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      # Cast numeric columns\n      - function: cast_columns\n        params:\n          columns:\n            price: double\n            freight_value: double\n            order_item_id: integer\n\n      # Calculate line total\n      - function: derive_columns\n        params:\n          columns:\n            line_total: \"price + freight_value\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            order_id,\n            order_item_id,\n            product_id,\n            seller_id,\n            shipping_limit_date,\n            price,\n            freight_value,\n            line_total,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: order_items\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#3-silver-customers","title":"3. Silver Customers","text":"<p>Customers need text standardization for geolocation matching:</p> <pre><code>- name: silver_customers\n  description: \"Cleaned customer data\"\n\n  read:\n    connection: bronze\n    path: customers\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: customer_id\n      severity: error\n\n    - type: unique\n      columns: [customer_id]\n      severity: error\n\n    - type: not_null\n      column: customer_zip_code_prefix\n      severity: warn\n\n  transformer: deduplicate\n  params:\n    keys: [customer_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      # Standardize location text\n      - function: clean_text\n        params:\n          columns: [customer_city]\n          case: upper\n          trim: true\n\n      - function: clean_text\n        params:\n          columns: [customer_state]\n          case: upper\n          trim: true\n\n      # Pad zip code if needed\n      - function: derive_columns\n        params:\n          columns:\n            customer_zip_code_prefix: \"LPAD(CAST(customer_zip_code_prefix AS STRING), 5, '0')\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            customer_id,\n            customer_unique_id,\n            customer_zip_code_prefix,\n            customer_city,\n            customer_state,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: customers\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#4-silver-products","title":"4. Silver Products","text":"<p>Products need weight and dimension validation:</p> <pre><code>- name: silver_products\n  description: \"Cleaned product catalog\"\n\n  read:\n    connection: bronze\n    path: products\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: product_id\n      severity: error\n\n    - type: unique\n      columns: [product_id]\n      severity: error\n\n    - type: range\n      column: product_weight_g\n      min: 0\n      max: 100000\n      severity: warn\n\n  transformer: deduplicate\n  params:\n    keys: [product_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      # Cast dimensions\n      - function: cast_columns\n        params:\n          columns:\n            product_weight_g: double\n            product_length_cm: double\n            product_height_cm: double\n            product_width_cm: double\n            product_photos_qty: integer\n\n      # Fill nulls for dimensions\n      - function: fill_nulls\n        params:\n          columns:\n            product_weight_g: 0.0\n            product_length_cm: 0.0\n            product_height_cm: 0.0\n            product_width_cm: 0.0\n            product_photos_qty: 0\n\n      # Calculate volume\n      - function: derive_columns\n        params:\n          columns:\n            product_volume_cm3: \"product_length_cm * product_height_cm * product_width_cm\"\n\n      # Clean category name\n      - function: clean_text\n        params:\n          columns: [product_category_name]\n          case: lower\n          trim: true\n\n      # Fill null category\n      - function: fill_nulls\n        params:\n          columns:\n            product_category_name: \"unknown\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            product_id,\n            product_category_name,\n            product_name_length,\n            product_description_length,\n            product_photos_qty,\n            product_weight_g,\n            product_length_cm,\n            product_height_cm,\n            product_width_cm,\n            product_volume_cm3,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: products\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#5-silver-sellers","title":"5. Silver Sellers","text":"<p>Sellers need location standardization:</p> <pre><code>- name: silver_sellers\n  description: \"Cleaned seller data\"\n\n  read:\n    connection: bronze\n    path: sellers\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: seller_id\n      severity: error\n\n    - type: unique\n      columns: [seller_id]\n      severity: error\n\n  transformer: deduplicate\n  params:\n    keys: [seller_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      # Standardize location\n      - function: clean_text\n        params:\n          columns: [seller_city]\n          case: upper\n          trim: true\n\n      - function: clean_text\n        params:\n          columns: [seller_state]\n          case: upper\n          trim: true\n\n      # Pad zip code\n      - function: derive_columns\n        params:\n          columns:\n            seller_zip_code_prefix: \"LPAD(CAST(seller_zip_code_prefix AS STRING), 5, '0')\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            seller_id,\n            seller_zip_code_prefix,\n            seller_city,\n            seller_state,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: sellers\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#6-silver-payments","title":"6. Silver Payments","text":"<p>Payments need type validation and amount checks:</p> <pre><code>- name: silver_payments\n  description: \"Cleaned payment data\"\n\n  read:\n    connection: bronze\n    path: payments\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: order_id\n      severity: error\n\n    - type: accepted_values\n      column: payment_type\n      values: [credit_card, boleto, voucher, debit_card, not_defined]\n      severity: error\n\n    - type: range\n      column: payment_value\n      min: 0\n      severity: error\n\n  # Note: No dedupe on order_id - orders can have multiple payments\n\n  transform:\n    steps:\n      # Cast amounts\n      - function: cast_columns\n        params:\n          columns:\n            payment_sequential: integer\n            payment_installments: integer\n            payment_value: double\n\n      # Standardize payment type\n      - function: clean_text\n        params:\n          columns: [payment_type]\n          case: lower\n          trim: true\n\n      # Select final columns\n      - sql: |\n          SELECT\n            order_id,\n            payment_sequential,\n            payment_type,\n            payment_installments,\n            payment_value,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: payments\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#7-silver-reviews","title":"7. Silver Reviews","text":"<p>Reviews need score validation and text cleanup:</p> <pre><code>- name: silver_reviews\n  description: \"Cleaned review data\"\n\n  read:\n    connection: bronze\n    path: reviews\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: review_id\n      severity: error\n\n    - type: not_null\n      column: order_id\n      severity: error\n\n    - type: range\n      column: review_score\n      min: 1\n      max: 5\n      severity: error\n\n  transformer: deduplicate\n  params:\n    keys: [review_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      # Cast score\n      - function: cast_columns\n        params:\n          columns:\n            review_score: integer\n\n      # Parse dates\n      - function: derive_columns\n        params:\n          columns:\n            review_creation_date: \"TO_DATE(review_creation_date)\"\n            review_answer_timestamp: \"TO_TIMESTAMP(review_answer_timestamp)\"\n\n      # Clean text fields (but preserve content)\n      - function: fill_nulls\n        params:\n          columns:\n            review_comment_title: \"\"\n            review_comment_message: \"\"\n\n      # Flag reviews with comments\n      - function: derive_columns\n        params:\n          columns:\n            has_comment: \"CASE WHEN LENGTH(review_comment_message) &gt; 0 THEN true ELSE false END\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            review_id,\n            order_id,\n            review_score,\n            review_comment_title,\n            review_comment_message,\n            has_comment,\n            review_creation_date,\n            review_answer_timestamp,\n            _extracted_at,\n            _source_file\n          FROM df\n\n  write:\n    connection: silver\n    path: reviews\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#8-silver-geolocation","title":"8. Silver Geolocation","text":"<p>Geolocation needs careful deduplication (multiple entries per zip):</p> <pre><code>- name: silver_geolocation\n  description: \"Cleaned geolocation data\"\n\n  read:\n    connection: bronze\n    path: geolocation\n    format: delta\n\n  contracts:\n    - type: not_null\n      column: geolocation_zip_code_prefix\n      severity: error\n\n  # Aggregate to one row per zip code\n  transform:\n    steps:\n      # Cast coordinates\n      - function: cast_columns\n        params:\n          columns:\n            geolocation_lat: double\n            geolocation_lng: double\n\n      # Standardize location text\n      - function: clean_text\n        params:\n          columns: [geolocation_city]\n          case: upper\n          trim: true\n\n      - function: clean_text\n        params:\n          columns: [geolocation_state]\n          case: upper\n          trim: true\n\n      # Pad zip code\n      - function: derive_columns\n        params:\n          columns:\n            geolocation_zip_code_prefix: \"LPAD(CAST(geolocation_zip_code_prefix AS STRING), 5, '0')\"\n\n      # Aggregate to one record per zip (take average coordinates)\n      - sql: |\n          SELECT\n            geolocation_zip_code_prefix,\n            AVG(geolocation_lat) as geolocation_lat,\n            AVG(geolocation_lng) as geolocation_lng,\n            FIRST(geolocation_city) as geolocation_city,\n            FIRST(geolocation_state) as geolocation_state,\n            COUNT(*) as source_record_count\n          FROM df\n          GROUP BY geolocation_zip_code_prefix\n\n  write:\n    connection: silver\n    path: geolocation\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#complete-pipeline-configuration","title":"Complete Pipeline Configuration","text":"<p>Here's the complete Silver pipeline:</p> <pre><code>pipelines:\n  - pipeline: silver_ecommerce\n    layer: silver\n    description: \"Cleaned and validated e-commerce data\"\n\n    nodes:\n      # Include all 8 node configurations from above\n      - name: silver_orders\n        # ... (config from above)\n\n      - name: silver_order_items\n        # ... (config from above)\n\n      - name: silver_customers\n        # ... (config from above)\n\n      - name: silver_products\n        # ... (config from above)\n\n      - name: silver_sellers\n        # ... (config from above)\n\n      - name: silver_payments\n        # ... (config from above)\n\n      - name: silver_reviews\n        # ... (config from above)\n\n      - name: silver_geolocation\n        # ... (config from above)\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#running-the-pipeline","title":"Running the Pipeline","text":"<pre><code># Run Silver pipeline\nodibi run odibi.yaml --pipeline silver_ecommerce\n\n# Or run a specific node\nodibi run odibi.yaml --node silver_orders\n</code></pre> <p>Expected output:</p> <pre><code>[INFO] Starting pipeline: silver_ecommerce\n[INFO] Executing node: silver_orders\n[INFO]   Contracts passed: 4/4\n[INFO]   Deduplicated: 99,441 \u2192 99,441 rows (0 duplicates)\n[INFO]   Transformed: Added 3 derived columns\n[INFO]   Written to silver/orders\n[INFO] Executing node: silver_order_items\n...\n[INFO] Pipeline silver_ecommerce completed successfully\n</code></pre>"},{"location":"marketing/articles/article_05_silver_layer_guide/#before-and-after","title":"Before and After","text":"<p>Let's see what Silver accomplished:</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#orders","title":"Orders","text":"Metric Bronze Silver Rows 99,441 99,441 Columns 10 13 Duplicates Unknown 0 (verified) Date format String DATE Derived columns 0 3 (days_to_delivery, etc.)"},{"location":"marketing/articles/article_05_silver_layer_guide/#products","title":"Products","text":"Metric Bronze Silver Rows 32,951 32,951 Null weights 610 0 (filled with 0) Category format Mixed case Lowercase Volume column No Yes (calculated)"},{"location":"marketing/articles/article_05_silver_layer_guide/#geolocation","title":"Geolocation","text":"Metric Bronze Silver Rows 1,000,163 ~19,000 Duplicates Many per zip One per zip Coordinates Raw Averaged"},{"location":"marketing/articles/article_05_silver_layer_guide/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Contracts first, transforms second - Validate before processing</li> <li>Deduplicate early - Remove duplicates before calculations</li> <li>Cast types explicitly - Don't rely on inference</li> <li>Standardize text - Consistent case, trimmed whitespace</li> <li>Fill nulls intentionally - Document defaults</li> <li>Calculate derived columns - Silver is where business logic lives</li> </ol>"},{"location":"marketing/articles/article_05_silver_layer_guide/#next-steps","title":"Next Steps","text":"<p>With a clean Silver layer, we're ready for the Gold layer:</p> <ol> <li>Build dimension tables (Customers, Products, Sellers, Dates)</li> <li>Build fact tables (Orders, Order Items)</li> <li>Create aggregations for reporting</li> </ol> <p>That's the next article: Facts vs Dimensions: A Practical Guide.</p>"},{"location":"marketing/articles/article_05_silver_layer_guide/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/","title":"Facts vs Dimensions: A Practical Guide","text":"<p>The building blocks of every data warehouse</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#tldr","title":"TL;DR","text":"<p>Facts are the \"how much\" - transactions, events, measurements. Dimensions are the \"who, what, where, when\" - customers, products, dates, locations. Facts have measures and foreign keys. Dimensions have attributes and surrogate keys. This article explains how to identify each in your data and design a star schema for the Brazilian E-Commerce dataset.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#the-restaurant-analogy","title":"The Restaurant Analogy","text":"<p>Imagine a restaurant.</p> <p>Receipts are facts: - Table 5, $47.50, 3 items, 8:15 PM - Table 12, $82.00, 5 items, 8:32 PM - Table 5, $12.00, 1 item, 9:01 PM</p> <p>Each receipt is a transaction with numbers you can add up.</p> <p>Reference lists are dimensions: - Menu (items, prices, categories) - Staff list (servers, cooks, hostess) - Table map (table numbers, sections, capacity) - Calendar (days, holidays, seasons)</p> <p>Reference lists describe the context around transactions. They change slowly (or never).</p> <p>That's the core distinction: - Facts = Events that happen. Additive numbers. - Dimensions = Context around events. Descriptive attributes.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#identifying-facts","title":"Identifying Facts","text":"<p>Ask yourself: \"Is this an event or transaction?\"</p> <p>Signs of a fact table: 1. Timestamps - When did it happen? 2. Measures - Numbers you'd SUM, COUNT, or AVG 3. Foreign keys - References to other tables 4. Many rows - Facts accumulate over time 5. Immutable - Once recorded, facts don't change</p> <p>In our e-commerce dataset:</p> Table Fact? Why orders \u2705 Yes Each order is an event order_items \u2705 Yes Each line item is a transaction payments \u2705 Yes Each payment is a transaction reviews \u2705 Yes Each review is an event"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#identifying-dimensions","title":"Identifying Dimensions","text":"<p>Ask yourself: \"Does this describe something else?\"</p> <p>Signs of a dimension table: 1. Descriptive attributes - Names, categories, addresses 2. Relatively static - Changes infrequently 3. Used for grouping - \"Sales by region,\" \"Orders by category\" 4. Used for filtering - \"Show only premium customers\" 5. Natural keys - Business identifiers like customer_id</p> <p>In our e-commerce dataset:</p> Table Dimension? Why customers \u2705 Yes Describes who bought products \u2705 Yes Describes what was sold sellers \u2705 Yes Describes who fulfilled geolocation \u2705 Yes Describes where (date) \u2705 Yes Describes when (we'll create this)"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#anatomy-of-a-fact-table","title":"Anatomy of a Fact Table","text":"<p>A well-designed fact table has:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     FACT_ORDERS                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 order_sk          (surrogate key - optional)                \u2502\n\u2502 order_id          (degenerate dimension - the original ID)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_sk       (foreign key \u2192 dim_customer)              \u2502\n\u2502 date_sk           (foreign key \u2192 dim_date)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 order_count       (measure: always 1, for counting)         \u2502\n\u2502 total_amount      (measure: sum of order value)             \u2502\n\u2502 shipping_cost     (measure: sum of freight)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 load_timestamp    (audit column)                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#key-concepts","title":"Key Concepts","text":"<p>Surrogate Key (SK): A meaningless integer generated by the warehouse. Used for joins. Not from the source system.</p> <p>Degenerate Dimension: A dimension value stored directly in the fact (like <code>order_id</code>). No separate dimension table needed.</p> <p>Foreign Keys: References to dimension tables. These are surrogate keys, not natural keys.</p> <p>Measures: Numbers you aggregate. Must be additive at the grain.</p> <p>Grain: The level of detail. \"One row per order\" or \"one row per order line.\"</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#anatomy-of-a-dimension-table","title":"Anatomy of a Dimension Table","text":"<p>A well-designed dimension table has:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     DIM_CUSTOMER                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_sk       (surrogate key - generated)                \u2502\n\u2502 customer_id       (natural key - from source system)         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 customer_city     (attribute)                                \u2502\n\u2502 customer_state    (attribute)                                \u2502\n\u2502 customer_region   (attribute - derived)                      \u2502\n\u2502 customer_segment  (attribute)                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 is_current        (SCD2: true for current record)            \u2502\n\u2502 valid_from        (SCD2: when this version started)          \u2502\n\u2502 valid_to          (SCD2: when this version ended)            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 load_timestamp    (audit column)                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#key-concepts_1","title":"Key Concepts","text":"<p>Surrogate Key (SK): The primary key of the dimension. An auto-incremented integer.</p> <p>Natural Key (NK): The business identifier from the source system. Used for matching incoming records.</p> <p>Attributes: Descriptive fields. These are what users filter and group by.</p> <p>SCD Columns: For tracking history (Type 2). <code>is_current</code>, <code>valid_from</code>, <code>valid_to</code>.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#star-schema-design","title":"Star Schema Design","text":"<p>When facts connect to dimensions, you get a star schema:</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   dim_date    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dim_customer  \u2502\u2500\u2500\u2500\u2502  fact_orders  \u2502\u2500\u2500\u2500\u2502  dim_product  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  dim_seller   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why \"star\"? The fact table sits in the center with dimensions radiating outward like points of a star.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#our-e-commerce-star-schema","title":"Our E-Commerce Star Schema","text":"<p>Let's design the star schema for our dataset:</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#fact-tables","title":"Fact Tables","text":"<p>fact_orders (grain: one row per order) - order_id (degenerate) - customer_sk \u2192 dim_customer - order_date_sk \u2192 dim_date - delivery_date_sk \u2192 dim_date - order_count (always 1) - is_late (0 or 1)</p> <p>fact_order_items (grain: one row per line item) - order_id (degenerate) - order_item_id (degenerate) - product_sk \u2192 dim_product - seller_sk \u2192 dim_seller - order_date_sk \u2192 dim_date - quantity (always 1 in this dataset) - price - freight_value - line_total</p> <p>fact_payments (grain: one row per payment) - order_id (degenerate) - payment_type (degenerate) - payment_date_sk \u2192 dim_date - payment_count (always 1) - payment_amount</p> <p>fact_reviews (grain: one row per review) - order_id (degenerate) - review_date_sk \u2192 dim_date - review_count (always 1) - review_score</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#dimension-tables","title":"Dimension Tables","text":"<p>dim_date - date_sk (YYYYMMDD format) - full_date - day_of_week - month, quarter, year - fiscal_year, fiscal_quarter - is_weekend, is_holiday</p> <p>dim_customer - customer_sk - customer_id (natural key) - customer_city - customer_state - customer_region</p> <p>dim_product - product_sk - product_id (natural key) - product_category - product_weight_g - product_volume_cm3</p> <p>dim_seller - seller_sk - seller_id (natural key) - seller_city - seller_state - seller_region</p> <p>dim_geolocation (optional, can denormalize into customer/seller) - zip_code (natural key) - latitude - longitude - city - state</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#why-surrogate-keys","title":"Why Surrogate Keys?","text":"<p>You might wonder: \"Why not just use <code>customer_id</code>?\"</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#problem-1-natural-keys-change","title":"Problem 1: Natural Keys Change","text":"<p>Business keys can be reassigned, reformatted, or merged. Surrogate keys are stable.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#problem-2-multi-source-integration","title":"Problem 2: Multi-Source Integration","text":"<p>If you have two source systems with different customer ID formats, surrogate keys unify them.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#problem-3-scd2-history","title":"Problem 3: SCD2 History","text":"<p>With SCD2, the same <code>customer_id</code> can have multiple rows (historical versions). You need a unique key per row.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#problem-4-join-performance","title":"Problem 4: Join Performance","text":"<p>Integer joins are faster than string joins. <code>customer_sk</code> (INT) beats <code>customer_id</code> (VARCHAR).</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#odibi-configuration-dimensions","title":"Odibi Configuration: Dimensions","text":"<p>Here's how to build <code>dim_customer</code> with Odibi:</p> <pre><code>- name: dim_customer\n  description: \"Customer dimension with surrogate key\"\n\n  depends_on: [silver_customers]\n\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 1  # Overwrite (no history)\n      unknown_member: true  # Add SK=0 for orphans\n      audit:\n        load_timestamp: true\n        source_system: \"ecommerce\"\n\n  # Add derived attributes\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            customer_region: |\n              CASE \n                WHEN customer_state IN ('SP', 'RJ', 'MG', 'ES') THEN 'Southeast'\n                WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n                WHEN customer_state IN ('BA', 'SE', 'AL', 'PE', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n                WHEN customer_state IN ('MT', 'MS', 'GO', 'DF') THEN 'Central-West'\n                ELSE 'North'\n              END\n\n  write:\n    connection: gold\n    path: dim_customer\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#odibi-configuration-facts","title":"Odibi Configuration: Facts","text":"<p>Here's how to build <code>fact_order_items</code>:</p> <pre><code>- name: fact_order_items\n  description: \"Order line items fact table\"\n\n  depends_on: \n    - silver_order_items\n    - dim_customer\n    - dim_product\n    - dim_seller\n    - dim_date\n\n  pattern:\n    type: fact\n    params:\n      grain: [order_id, order_item_id]\n\n      dimensions:\n        - source_column: product_id\n          dimension_table: dim_product\n          dimension_key: product_id\n          surrogate_key: product_sk\n\n        - source_column: seller_id\n          dimension_table: dim_seller\n          dimension_key: seller_id\n          surrogate_key: seller_sk\n\n      orphan_handling: unknown  # Use SK=0 for missing dimensions\n\n      measures:\n        - quantity: \"1\"  # Each row is one item\n        - price\n        - freight_value\n        - line_total: \"price + freight_value\"\n\n      audit:\n        load_timestamp: true\n\n  write:\n    connection: gold\n    path: fact_order_items\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#common-mistakes","title":"Common Mistakes","text":""},{"location":"marketing/articles/article_06_facts_vs_dimensions/#1-using-natural-keys-in-facts","title":"1. Using Natural Keys in Facts","text":"<p>\u274c Wrong:</p> <pre><code>SELECT customer_id, SUM(amount)\nFROM fact_orders\nJOIN silver_customers ON ...\n</code></pre> <p>\u2705 Right:</p> <pre><code>SELECT d.customer_id, SUM(f.amount)\nFROM fact_orders f\nJOIN dim_customer d ON f.customer_sk = d.customer_sk\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#2-putting-measures-in-dimensions","title":"2. Putting Measures in Dimensions","text":"<p>\u274c Wrong: <code>dim_customer.total_orders</code> (this is a measure)</p> <p>\u2705 Right: Calculate from fact table when needed</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#3-missing-unknown-member","title":"3. Missing Unknown Member","text":"<p>\u274c Wrong: Orphan records break joins</p> <p>\u2705 Right: Add row with <code>SK=0</code> and \"Unknown\" attributes</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#4-wrong-grain","title":"4. Wrong Grain","text":"<p>\u274c Wrong: <code>fact_orders</code> with one row per customer (too high)</p> <p>\u2705 Right: <code>fact_orders</code> with one row per order (transaction grain)</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#5-non-additive-measures","title":"5. Non-Additive Measures","text":"<p>\u274c Wrong: Storing percentages in facts (can't SUM percentages)</p> <p>\u2705 Right: Store numerator and denominator, calculate percentage in reporting</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#querying-the-star-schema","title":"Querying the Star Schema","text":"<p>Once built, the star schema enables clean queries:</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#total-revenue-by-region-and-month","title":"Total Revenue by Region and Month","text":"<pre><code>SELECT \n  dc.customer_region,\n  dd.month_name,\n  SUM(f.line_total) as revenue\nFROM fact_order_items f\nJOIN dim_customer dc ON f.customer_sk = dc.customer_sk\nJOIN dim_date dd ON f.order_date_sk = dd.date_sk\nWHERE dd.year = 2018\nGROUP BY dc.customer_region, dd.month_name\nORDER BY dc.customer_region, dd.month_name\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#top-10-products-by-revenue","title":"Top 10 Products by Revenue","text":"<pre><code>SELECT \n  dp.product_category,\n  dp.product_id,\n  SUM(f.line_total) as revenue,\n  COUNT(*) as order_count\nFROM fact_order_items f\nJOIN dim_product dp ON f.product_sk = dp.product_sk\nGROUP BY dp.product_category, dp.product_id\nORDER BY revenue DESC\nLIMIT 10\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#late-delivery-rate-by-seller-state","title":"Late Delivery Rate by Seller State","text":"<pre><code>SELECT \n  ds.seller_state,\n  SUM(f.is_late) as late_orders,\n  COUNT(*) as total_orders,\n  ROUND(100.0 * SUM(f.is_late) / COUNT(*), 2) as late_pct\nFROM fact_orders f\nJOIN dim_seller ds ON f.seller_sk = ds.seller_sk\nGROUP BY ds.seller_state\nORDER BY late_pct DESC\n</code></pre>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Facts are events - Transactions with measures and timestamps</li> <li>Dimensions are context - Descriptive attributes for filtering and grouping</li> <li>Surrogate keys enable history - Stable identifiers across time</li> <li>Grain defines uniqueness - \"One row per ___\"</li> <li>Star schema simplifies queries - Fact in center, dimensions around</li> </ol>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#next-steps","title":"Next Steps","text":"<p>Now that we understand facts and dimensions, we'll build them:</p> <ol> <li>Date Dimension - Every warehouse needs one</li> <li>Dimension Tables - Customer, Product, Seller</li> <li>Fact Tables - Orders, Order Items, Reviews</li> </ol> <p>Next article: Building a Date Dimension from Scratch.</p>"},{"location":"marketing/articles/article_06_facts_vs_dimensions/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_07_date_dimension/","title":"Building a Date Dimension from Scratch","text":"<p>The one dimension every data warehouse needs</p>"},{"location":"marketing/articles/article_07_date_dimension/#tldr","title":"TL;DR","text":"<p>Every data warehouse needs a date dimension. It enables filtering by month, quarter, fiscal year, day of week, and more-without complex date functions in every query. This article shows how to generate a complete date dimension with Odibi, including fiscal calendars, holiday flags, and the critical unknown member row.</p>"},{"location":"marketing/articles/article_07_date_dimension/#why-every-warehouse-needs-a-date-dimension","title":"Why Every Warehouse Needs a Date Dimension","text":"<p>You could join fact tables directly on dates:</p> <pre><code>SELECT \n  EXTRACT(MONTH FROM order_date) as month,\n  SUM(amount)\nFROM fact_orders\nGROUP BY EXTRACT(MONTH FROM order_date)\n</code></pre> <p>But this approach has problems:</p> <ol> <li>Repetitive: Every query repeats date extraction logic</li> <li>Inconsistent: Different analysts calculate \"fiscal quarter\" differently</li> <li>Slow: Date functions run on every row</li> <li>Limited: Can't filter on \"is_holiday\" or \"is_business_day\"</li> </ol> <p>A date dimension solves all of this:</p> <pre><code>SELECT \n  d.month_name,\n  d.fiscal_quarter,\n  SUM(f.amount)\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nWHERE d.is_business_day = true\nGROUP BY d.month_name, d.fiscal_quarter\n</code></pre> <p>Pre-calculate once, query forever.</p>"},{"location":"marketing/articles/article_07_date_dimension/#what-columns-to-include","title":"What Columns to Include","text":"<p>A complete date dimension has these categories:</p>"},{"location":"marketing/articles/article_07_date_dimension/#core-date-fields","title":"Core Date Fields","text":"Column Example Description date_sk 20180315 Surrogate key (YYYYMMDD) full_date 2018-03-15 The actual date day_of_week Thursday Day name day_of_week_num 4 1=Sunday, 7=Saturday day_of_month 15 1-31 day_of_year 74 1-366"},{"location":"marketing/articles/article_07_date_dimension/#week-fields","title":"Week Fields","text":"Column Example Description week_of_year 11 ISO week number week_start_date 2018-03-12 Monday of that week week_end_date 2018-03-18 Sunday of that week"},{"location":"marketing/articles/article_07_date_dimension/#month-fields","title":"Month Fields","text":"Column Example Description month 3 1-12 month_name March Full name month_short Mar Abbreviated month_start_date 2018-03-01 First of month month_end_date 2018-03-31 Last of month"},{"location":"marketing/articles/article_07_date_dimension/#quarter-fields","title":"Quarter Fields","text":"Column Example Description quarter 1 1-4 quarter_name Q1 Label quarter_start_date 2018-01-01 First of quarter quarter_end_date 2018-03-31 Last of quarter"},{"location":"marketing/articles/article_07_date_dimension/#year-fields","title":"Year Fields","text":"Column Example Description year 2018 Calendar year year_month 2018-03 For sorting year_quarter 2018-Q1 For sorting"},{"location":"marketing/articles/article_07_date_dimension/#fiscal-fields","title":"Fiscal Fields","text":"Column Example Description fiscal_year 2018 Based on fiscal calendar fiscal_quarter FQ3 Fiscal quarter fiscal_month 9 Month within fiscal year fiscal_week 37 Week within fiscal year"},{"location":"marketing/articles/article_07_date_dimension/#flags","title":"Flags","text":"Column Example Description is_weekend true Saturday or Sunday is_weekday false Monday-Friday is_holiday false Company holiday is_month_start false First of month is_month_end false Last of month is_quarter_start false First of quarter is_quarter_end true Last of quarter is_year_start false January 1 is_year_end false December 31"},{"location":"marketing/articles/article_07_date_dimension/#generating-with-odibi","title":"Generating with Odibi","text":"<p>Odibi has a built-in pattern for date dimensions:</p> <pre><code>pipelines:\n  - pipeline: gold_dimensions\n    layer: gold\n    description: \"Gold layer dimensions\"\n\n    nodes:\n      - name: dim_date\n        description: \"Date dimension spanning 2015-2030\"\n\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2015-01-01\"\n            end_date: \"2030-12-31\"\n            date_key_format: \"yyyyMMdd\"  # Creates 20180315 format\n            fiscal_year_start_month: 7    # Fiscal year starts in July\n            unknown_member: true          # Adds date_sk=0 row\n\n        write:\n          connection: gold\n          path: dim_date\n          format: delta\n</code></pre> <p>This generates: - 5,844 rows (16 years of dates) - 20+ columns - Unknown member row with <code>date_sk=0</code></p>"},{"location":"marketing/articles/article_07_date_dimension/#generated-columns","title":"Generated Columns","text":"<p>Running the pattern generates these columns:</p> <pre><code>date_sk              INTEGER    (20180315)\nfull_date            DATE       (2018-03-15)\nday_of_week          STRING     (Thursday)\nday_of_week_num      INTEGER    (4)\nday_of_month         INTEGER    (15)\nday_of_year          INTEGER    (74)\nis_weekend           BOOLEAN    (false)\nweek_of_year         INTEGER    (11)\nmonth                INTEGER    (3)\nmonth_name           STRING     (March)\nquarter              INTEGER    (1)\nquarter_name         STRING     (Q1)\nyear                 INTEGER    (2018)\nyear_month           STRING     (2018-03)\nfiscal_year          INTEGER    (2018)\nfiscal_quarter       STRING     (FQ3)\nis_month_start       BOOLEAN    (false)\nis_month_end         BOOLEAN    (false)\nis_quarter_start     BOOLEAN    (false)\nis_quarter_end       BOOLEAN    (false)\nis_year_start        BOOLEAN    (false)\nis_year_end          BOOLEAN    (false)\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#fiscal-year-handling","title":"Fiscal Year Handling","text":"<p>Many organizations don't follow the calendar year. </p> <p>If your fiscal year starts in July: - July 2018 = Fiscal Month 1 of FY2019 - June 2019 = Fiscal Month 12 of FY2019</p> <p>Configure this with <code>fiscal_year_start_month</code>:</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    fiscal_year_start_month: 7  # July\n</code></pre> <p>The pattern calculates: - <code>fiscal_year</code>: The fiscal year (e.g., 2019 for July 2018) - <code>fiscal_quarter</code>: FQ1-FQ4 based on fiscal months - <code>fiscal_month</code>: 1-12 within fiscal year - <code>fiscal_week</code>: 1-52 within fiscal year</p>"},{"location":"marketing/articles/article_07_date_dimension/#the-unknown-member-row","title":"The Unknown Member Row","text":"<p>What happens when a fact record has a null date? Or a date that falls outside your dimension range?</p> <p>The unknown member pattern adds a special row:</p> date_sk full_date day_of_week month_name ... 0 NULL Unknown Unknown ... <p>Configure it:</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    unknown_member: true\n</code></pre> <p>When building fact tables, orphan date lookups map to <code>date_sk=0</code>:</p> <pre><code># In fact table configuration\ndimensions:\n  - source_column: order_date\n    dimension_table: dim_date\n    dimension_key: full_date\n    surrogate_key: date_sk\n# Orphans get date_sk=0 instead of NULL or failure\n</code></pre> <p>This ensures: - All facts have valid foreign keys - Queries always return results - You can track \"unknown date\" separately</p>"},{"location":"marketing/articles/article_07_date_dimension/#adding-holidays","title":"Adding Holidays","text":"<p>The base pattern doesn't include holidays (they're company-specific). Add them after generation:</p> <pre><code>- name: dim_date\n  description: \"Date dimension with holidays\"\n\n  pattern:\n    type: date_dimension\n    params:\n      start_date: \"2015-01-01\"\n      end_date: \"2030-12-31\"\n      unknown_member: true\n\n  # Add holiday flags\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            is_us_holiday: |\n              CASE\n                -- New Year's Day\n                WHEN month = 1 AND day_of_month = 1 THEN true\n                -- Independence Day\n                WHEN month = 7 AND day_of_month = 4 THEN true\n                -- Thanksgiving (4th Thursday of November)\n                WHEN month = 11 AND day_of_week = 'Thursday' \n                     AND day_of_month BETWEEN 22 AND 28 THEN true\n                -- Christmas\n                WHEN month = 12 AND day_of_month = 25 THEN true\n                ELSE false\n              END\n\n            is_business_day: |\n              CASE\n                WHEN is_weekend = true THEN false\n                WHEN is_us_holiday = true THEN false\n                ELSE true\n              END\n\n  write:\n    connection: gold\n    path: dim_date\n    format: delta\n</code></pre> <p>For complex holiday logic (Easter, floating holidays), you might generate a holiday table separately and join it.</p>"},{"location":"marketing/articles/article_07_date_dimension/#date-key-format-choices","title":"Date Key Format Choices","text":"<p>The <code>date_key_format</code> parameter controls how <code>date_sk</code> is generated:</p> Format Example Pros Cons yyyyMMdd 20180315 Human readable, sortable Large integers yyyy-MM-dd 2018-03-15 Very readable String, slower joins Sequential 1, 2, 3... Compact Not readable <p>Most warehouses use <code>yyyyMMdd</code> because: - You can read it: \"20180315 = March 15, 2018\" - It sorts correctly numerically - It's widely understood</p>"},{"location":"marketing/articles/article_07_date_dimension/#common-queries","title":"Common Queries","text":"<p>Once you have a date dimension, these queries become trivial:</p>"},{"location":"marketing/articles/article_07_date_dimension/#sales-by-day-of-week","title":"Sales by Day of Week","text":"<pre><code>SELECT \n  d.day_of_week,\n  SUM(f.amount) as total_sales\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nGROUP BY d.day_of_week, d.day_of_week_num\nORDER BY d.day_of_week_num\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#month-over-month-growth","title":"Month-over-Month Growth","text":"<pre><code>SELECT \n  d.year_month,\n  SUM(f.amount) as monthly_sales,\n  LAG(SUM(f.amount)) OVER (ORDER BY d.year_month) as prev_month,\n  ROUND(100.0 * (SUM(f.amount) - LAG(SUM(f.amount)) OVER (ORDER BY d.year_month)) \n        / LAG(SUM(f.amount)) OVER (ORDER BY d.year_month), 2) as growth_pct\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nGROUP BY d.year_month\nORDER BY d.year_month\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#fiscal-year-comparison","title":"Fiscal Year Comparison","text":"<pre><code>SELECT \n  d.fiscal_year,\n  d.fiscal_quarter,\n  SUM(f.amount) as sales\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nGROUP BY d.fiscal_year, d.fiscal_quarter\nORDER BY d.fiscal_year, d.fiscal_quarter\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#business-days-only","title":"Business Days Only","text":"<pre><code>SELECT \n  COUNT(*) as orders,\n  AVG(f.amount) as avg_order_value\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nWHERE d.is_business_day = true\n  AND d.year = 2018\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#weekend-vs-weekday","title":"Weekend vs Weekday","text":"<pre><code>SELECT \n  CASE WHEN d.is_weekend THEN 'Weekend' ELSE 'Weekday' END as day_type,\n  COUNT(*) as order_count,\n  SUM(f.amount) as total_sales\nFROM fact_orders f\nJOIN dim_date d ON f.order_date_sk = d.date_sk\nGROUP BY d.is_weekend\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#complete-configuration","title":"Complete Configuration","text":"<p>Here's the full date dimension configuration:</p> <pre><code>pipelines:\n  - pipeline: gold_dimensions\n    layer: gold\n    description: \"Gold layer dimension tables\"\n\n    nodes:\n      - name: dim_date\n        description: \"Date dimension 2015-2030 with fiscal calendar and holidays\"\n\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2015-01-01\"\n            end_date: \"2030-12-31\"\n            date_key_format: \"yyyyMMdd\"\n            fiscal_year_start_month: 7\n            unknown_member: true\n\n        transform:\n          steps:\n            # Add holiday flags (customize for your company)\n            - function: derive_columns\n              params:\n                columns:\n                  is_holiday: |\n                    CASE\n                      WHEN month = 1 AND day_of_month = 1 THEN true\n                      WHEN month = 7 AND day_of_month = 4 THEN true\n                      WHEN month = 12 AND day_of_month = 25 THEN true\n                      ELSE false\n                    END\n\n                  is_business_day: |\n                    CASE\n                      WHEN is_weekend = true THEN false\n                      WHEN month = 1 AND day_of_month = 1 THEN false\n                      WHEN month = 7 AND day_of_month = 4 THEN false\n                      WHEN month = 12 AND day_of_month = 25 THEN false\n                      ELSE true\n                    END\n\n                  # Relative date flags (useful for dashboards)\n                  is_current_month: |\n                    CASE \n                      WHEN year = YEAR(CURRENT_DATE()) \n                       AND month = MONTH(CURRENT_DATE()) \n                      THEN true ELSE false \n                    END\n\n                  is_previous_month: |\n                    CASE \n                      WHEN full_date &gt;= DATE_TRUNC('month', ADD_MONTHS(CURRENT_DATE(), -1))\n                       AND full_date &lt; DATE_TRUNC('month', CURRENT_DATE())\n                      THEN true ELSE false \n                    END\n\n        write:\n          connection: gold\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#verification","title":"Verification","text":"<p>After running, verify your date dimension:</p> <pre><code>from deltalake import DeltaTable\nimport pandas as pd\n\ndt = DeltaTable(\"./gold/dim_date\")\ndf = dt.to_pandas()\n\nprint(f\"Total rows: {len(df)}\")\nprint(f\"Date range: {df['full_date'].min()} to {df['full_date'].max()}\")\nprint(f\"Has unknown member: {(df['date_sk'] == 0).any()}\")\nprint(f\"\\nSample:\")\nprint(df.head(10))\n</code></pre> <p>Expected output:</p> <pre><code>Total rows: 5845\nDate range: 2015-01-01 to 2030-12-31\nHas unknown member: True\n\nSample:\n   date_sk  full_date day_of_week  month month_name  ...\n0        0       None     Unknown      0    Unknown  ...\n1 20150101 2015-01-01    Thursday      1    January  ...\n2 20150102 2015-01-02      Friday      1    January  ...\n</code></pre>"},{"location":"marketing/articles/article_07_date_dimension/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Pre-calculate, don't recalculate - Date logic belongs in the dimension, not queries</li> <li>Fiscal calendars matter - Configure for your organization's fiscal year</li> <li>Unknown member is essential - Handles null and out-of-range dates</li> <li>Add business context - Holidays, business days, relative flags</li> <li>Generate once, use forever - 10+ years of dates is tiny (&lt;10K rows)</li> </ol>"},{"location":"marketing/articles/article_07_date_dimension/#next-steps","title":"Next Steps","text":"<p>With the date dimension complete, we'll build the remaining dimensions and fact tables:</p> <ol> <li>Customer, Product, Seller dimensions</li> <li>Order facts with surrogate key lookups</li> <li>Complete star schema assembly</li> </ol> <p>Next article: Fact Table Patterns: Lookups, Orphans, and Measures.</p>"},{"location":"marketing/articles/article_07_date_dimension/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_08_fact_table_patterns/","title":"Fact Table Patterns: Lookups, Orphans, and Measures","text":"<p>Building robust fact tables that handle the real world</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#tldr","title":"TL;DR","text":"<p>Fact tables need surrogate key lookups from dimensions, strategies for handling orphan records (missing dimension matches), and clear measure definitions. This article covers the complete fact pattern: configuring lookups with SCD2 support, choosing between unknown/reject/quarantine for orphans, defining passthrough and calculated measures, and validating grain. We'll build <code>fact_order_items</code> step by step.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#anatomy-of-a-fact-table","title":"Anatomy of a Fact Table","text":"<p>A fact table has three types of columns:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    FACT_ORDER_ITEMS                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 KEYS                                                         \u2502\n\u2502   order_id            (degenerate dimension)                 \u2502\n\u2502   order_item_id       (degenerate dimension)                 \u2502\n\u2502   customer_sk         (FK \u2192 dim_customer)                    \u2502\n\u2502   product_sk          (FK \u2192 dim_product)                     \u2502\n\u2502   seller_sk           (FK \u2192 dim_seller)                      \u2502\n\u2502   order_date_sk       (FK \u2192 dim_date)                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 MEASURES                                                     \u2502\n\u2502   quantity            (additive)                             \u2502\n\u2502   price               (additive)                             \u2502\n\u2502   freight_value       (additive)                             \u2502\n\u2502   line_total          (calculated: price + freight)          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 AUDIT                                                        \u2502\n\u2502   load_timestamp      (when loaded)                          \u2502\n\u2502   source_system       (where from)                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Let's break down each part.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#surrogate-key-lookups","title":"Surrogate Key Lookups","text":"<p>The source data has natural keys like <code>customer_id</code>. Fact tables need surrogate keys like <code>customer_sk</code>.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#the-challenge","title":"The Challenge","text":"<pre><code>Source (Silver):          Dimension (Gold):\norder_id: ABC123          customer_sk: 42\ncustomer_id: CUST-789 --&gt; customer_id: CUST-789\nproduct_id: PROD-456      \n</code></pre> <p>We need to look up <code>customer_sk = 42</code> using <code>customer_id = CUST-789</code>.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#basic-lookup-configuration","title":"Basic Lookup Configuration","text":"<pre><code>pattern:\n  type: fact\n  params:\n    dimensions:\n      - source_column: customer_id      # Column in source fact\n        dimension_table: dim_customer   # Dimension to look up\n        dimension_key: customer_id      # Natural key in dimension\n        surrogate_key: customer_sk      # SK to retrieve\n</code></pre> <p>This generates a LEFT JOIN:</p> <pre><code>SELECT \n  f.*,\n  d.customer_sk\nFROM source_fact f\nLEFT JOIN dim_customer d ON f.customer_id = d.customer_id\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#scd2-lookups","title":"SCD2 Lookups","text":"<p>If your dimension uses SCD2 (history tracking), you need to join only to current records:</p> <pre><code>dimensions:\n  - source_column: customer_id\n    dimension_table: dim_customer\n    dimension_key: customer_id\n    surrogate_key: customer_sk\n    scd2: true  # Only join where is_current = true\n</code></pre> <p>This adds a filter:</p> <pre><code>LEFT JOIN dim_customer d \n  ON f.customer_id = d.customer_id \n  AND d.is_current = true\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#point-in-time-lookups","title":"Point-in-Time Lookups","text":"<p>For historical analysis, join based on when the fact occurred:</p> <pre><code>dimensions:\n  - source_column: customer_id\n    dimension_table: dim_customer\n    dimension_key: customer_id\n    surrogate_key: customer_sk\n    scd2: true\n    point_in_time: order_date  # Look up SK valid at order_date\n</code></pre> <p>This joins to the dimension version that was active at <code>order_date</code>:</p> <pre><code>LEFT JOIN dim_customer d \n  ON f.customer_id = d.customer_id \n  AND f.order_date &gt;= d.valid_from \n  AND f.order_date &lt; d.valid_to\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#orphan-handling","title":"Orphan Handling","text":"<p>What happens when a fact record has a <code>customer_id</code> that doesn't exist in <code>dim_customer</code>?</p> <p>This is an orphan record. You have three choices:</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#option-1-unknown-default-to-sk0","title":"Option 1: Unknown (Default to SK=0)","text":"<p>Map orphans to the unknown member row in the dimension:</p> <pre><code>pattern:\n  type: fact\n  params:\n    orphan_handling: unknown\n</code></pre> <p>Result: - Orphan records get <code>customer_sk = 0</code> - They join to the \"Unknown Customer\" row - Pipeline continues - No data loss</p> <p>This is the safest default.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#option-2-reject-fail-the-pipeline","title":"Option 2: Reject (Fail the Pipeline)","text":"<p>If orphans indicate a serious data quality issue:</p> <pre><code>pattern:\n  type: fact\n  params:\n    orphan_handling: reject\n</code></pre> <p>Result: - Pipeline fails immediately - Error message shows which records failed - No data written - Forces upstream fix</p> <p>Use this when orphans should never happen.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#option-3-quarantine-route-to-separate-table","title":"Option 3: Quarantine (Route to Separate Table)","text":"<p>Process good records, save bad records for later review:</p> <pre><code>pattern:\n  type: fact\n  params:\n    orphan_handling: quarantine\n    quarantine:\n      connection: silver\n      path: fact_order_items_orphans\n      add_columns:\n        _rejection_reason: true    # Which lookup failed\n        _rejected_at: true         # Timestamp\n        _source_dimension: true    # Which dimension\n</code></pre> <p>Result: - Good records \u2192 <code>fact_order_items</code> - Bad records \u2192 <code>fact_order_items_orphans</code> - Pipeline succeeds - Review orphans later</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#choosing-your-strategy","title":"Choosing Your Strategy","text":"Scenario Strategy Why New data sources, still building dimensions unknown Keep pipeline running Production, dimensions should be complete reject Fail fast on data issues Production, but need resilience quarantine Keep pipeline running, track issues Historical loads with known gaps unknown Accept that some old data has missing refs"},{"location":"marketing/articles/article_08_fact_table_patterns/#defining-measures","title":"Defining Measures","text":"<p>Measures are the numbers you aggregate. Three types:</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#passthrough-measures","title":"Passthrough Measures","text":"<p>Copy the column as-is:</p> <pre><code>measures:\n  - price\n  - freight_value\n  - quantity\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#calculated-measures","title":"Calculated Measures","text":"<p>Create new columns with expressions:</p> <pre><code>measures:\n  - line_total: \"price + freight_value\"\n  - discount_amount: \"list_price - price\"\n  - unit_price: \"price / quantity\"\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#renamed-measures","title":"Renamed Measures","text":"<p>Rename for clarity:</p> <pre><code>measures:\n  - order_amount: price  # Rename price to order_amount\n  - shipping_cost: freight_value\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#complex-example","title":"Complex Example","text":"<pre><code>measures:\n  # Passthrough\n  - quantity\n  - price\n  - freight_value\n\n  # Calculated\n  - line_total: \"price + freight_value\"\n  - is_free_shipping: \"CASE WHEN freight_value = 0 THEN 1 ELSE 0 END\"\n  - weight_per_item: \"product_weight_g / quantity\"\n\n  # Renamed\n  - order_line_count: \"1\"  # Always 1 for counting lines\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#grain-validation","title":"Grain Validation","text":"<p>The grain is the level of detail in your fact table. It defines \"one row per ___.\"</p> <p>For <code>fact_order_items</code>: \"One row per order line item\" \u2192 <code>[order_id, order_item_id]</code></p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#why-grain-matters","title":"Why Grain Matters","text":"<p>If you accidentally load duplicates, measures get double-counted. Grain validation catches this.</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, order_item_id]\n</code></pre> <p>This adds a check:</p> <pre><code>SELECT order_id, order_item_id, COUNT(*) as cnt\nFROM source_data\nGROUP BY order_id, order_item_id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If duplicates exist: - With <code>on_violation: error</code> \u2192 Pipeline fails - With <code>on_violation: warn</code> \u2192 Log warning, continue (risky!)</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#common-grain-patterns","title":"Common Grain Patterns","text":"Fact Table Grain fact_orders [order_id] fact_order_items [order_id, order_item_id] fact_payments [order_id, payment_sequential] fact_daily_sales [date_sk, product_sk, store_sk] fact_inventory [date_sk, product_sk, warehouse_sk]"},{"location":"marketing/articles/article_08_fact_table_patterns/#deduplication","title":"Deduplication","text":"<p>Sometimes source data has duplicates that need removal before processing:</p> <pre><code>pattern:\n  type: fact\n  params:\n    deduplicate: true\n    keys: [order_id, order_item_id]\n    order_by: _extracted_at DESC  # Keep latest\n</code></pre> <p>This runs before lookups and measures:</p> <pre><code>SELECT * FROM (\n  SELECT *, ROW_NUMBER() OVER (\n    PARTITION BY order_id, order_item_id \n    ORDER BY _extracted_at DESC\n  ) as rn\n  FROM source_data\n)\nWHERE rn = 1\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#complete-fact-configuration","title":"Complete Fact Configuration","text":"<p>Here's the full <code>fact_order_items</code> configuration:</p> <pre><code>- name: fact_order_items\n  description: \"Order line items fact table\"\n\n  depends_on:\n    - silver_order_items\n    - silver_orders  # For order date\n    - dim_customer\n    - dim_product\n    - dim_seller\n    - dim_date\n\n  # Join order items with orders to get dates and customer\n  inputs:\n    order_items: $silver_ecommerce.silver_order_items\n    orders: $silver_ecommerce.silver_orders\n\n  transform:\n    steps:\n      # Join to get order-level data\n      - sql: |\n          SELECT \n            oi.order_id,\n            oi.order_item_id,\n            oi.product_id,\n            oi.seller_id,\n            o.customer_id,\n            o.order_purchase_date,\n            oi.price,\n            oi.freight_value,\n            oi.line_total\n          FROM order_items oi\n          JOIN orders o ON oi.order_id = o.order_id\n\n  pattern:\n    type: fact\n    params:\n      grain: [order_id, order_item_id]\n\n      dimensions:\n        # Customer lookup\n        - source_column: customer_id\n          dimension_table: dim_customer\n          dimension_key: customer_id\n          surrogate_key: customer_sk\n          scd2: true\n\n        # Product lookup\n        - source_column: product_id\n          dimension_table: dim_product\n          dimension_key: product_id\n          surrogate_key: product_sk\n\n        # Seller lookup\n        - source_column: seller_id\n          dimension_table: dim_seller\n          dimension_key: seller_id\n          surrogate_key: seller_sk\n\n        # Date lookup\n        - source_column: order_purchase_date\n          dimension_table: dim_date\n          dimension_key: full_date\n          surrogate_key: date_sk\n          target_column: order_date_sk  # Rename the FK\n\n      orphan_handling: unknown\n\n      measures:\n        - quantity: \"1\"\n        - price\n        - freight_value\n        - line_total\n        - item_count: \"1\"\n\n      audit:\n        load_timestamp: true\n        source_system: \"ecommerce\"\n\n  write:\n    connection: gold\n    path: fact_order_items\n    format: delta\n    mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#testing-your-fact-table","title":"Testing Your Fact Table","text":"<p>After building, run these checks:</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#1-row-count-validation","title":"1. Row Count Validation","text":"<pre><code>-- Source count should match fact count (after dedup)\nSELECT \n  (SELECT COUNT(*) FROM silver.order_items) as source_count,\n  (SELECT COUNT(*) FROM gold.fact_order_items) as fact_count\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#2-orphan-check","title":"2. Orphan Check","text":"<pre><code>-- Check for unknown members (SK=0)\nSELECT \n  'customer' as dimension,\n  COUNT(*) as unknown_count\nFROM gold.fact_order_items\nWHERE customer_sk = 0\nUNION ALL\nSELECT \n  'product',\n  COUNT(*)\nFROM gold.fact_order_items\nWHERE product_sk = 0\n</code></pre> <p>If unknown counts are high, investigate dimension coverage.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#3-measure-validation","title":"3. Measure Validation","text":"<pre><code>-- Verify totals match source\nSELECT \n  SUM(price) as fact_total,\n  (SELECT SUM(price) FROM silver.order_items) as source_total\nFROM gold.fact_order_items\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#4-grain-validation","title":"4. Grain Validation","text":"<pre><code>-- Should return 0 rows\nSELECT order_id, order_item_id, COUNT(*)\nFROM gold.fact_order_items\nGROUP BY order_id, order_item_id\nHAVING COUNT(*) &gt; 1\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#common-mistakes","title":"Common Mistakes","text":""},{"location":"marketing/articles/article_08_fact_table_patterns/#1-missing-dimension-dependencies","title":"1. Missing Dimension Dependencies","text":"<p>\u274c Wrong:</p> <pre><code>depends_on: [silver_order_items]  # Missing dimensions!\npattern:\n  type: fact\n  params:\n    dimensions:\n      - dimension_table: dim_customer  # Will fail!\n</code></pre> <p>\u2705 Right:</p> <pre><code>depends_on: \n  - silver_order_items\n  - dim_customer\n  - dim_product\n  - dim_seller\n  - dim_date\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#2-forgetting-scd2-filter","title":"2. Forgetting SCD2 Filter","text":"<p>\u274c Wrong:</p> <pre><code>dimensions:\n  - source_column: customer_id\n    dimension_table: dim_customer  # SCD2 dimension\n    dimension_key: customer_id\n    surrogate_key: customer_sk\n    # Missing scd2: true!\n</code></pre> <p>This joins to ALL versions, creating duplicates.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#3-wrong-grain","title":"3. Wrong Grain","text":"<p>\u274c Wrong:</p> <pre><code>grain: [order_id]  # Should be [order_id, order_item_id]\n</code></pre> <p>Grain validation will fail or you'll lose line items.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#4-non-additive-measures","title":"4. Non-Additive Measures","text":"<p>\u274c Wrong:</p> <pre><code>measures:\n  - discount_pct: \"(list_price - price) / list_price * 100\"\n</code></pre> <p>You can't SUM percentages. Store components instead:</p> <p>\u2705 Right:</p> <pre><code>measures:\n  - list_price\n  - price\n  - discount_amount: \"list_price - price\"\n# Calculate percentage in reporting layer\n</code></pre>"},{"location":"marketing/articles/article_08_fact_table_patterns/#quarantine-management","title":"Quarantine Management","text":"<p>If using quarantine, set up a review process:</p> <pre><code>-- Daily quarantine summary\nSELECT \n  DATE(_rejected_at) as rejection_date,\n  _source_dimension,\n  _rejection_reason,\n  COUNT(*) as orphan_count\nFROM silver.fact_order_items_orphans\nWHERE _rejected_at &gt;= CURRENT_DATE - 7\nGROUP BY 1, 2, 3\nORDER BY 1 DESC, 4 DESC\n</code></pre> <p>Common fixes: - Missing dimension record \u2192 Load the dimension first - Typo in source data \u2192 Fix upstream - Timing issue \u2192 Reload after dimension update</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Surrogate key lookups connect facts to dimensions - Configure with SCD2 awareness</li> <li>Orphan handling protects your pipeline - Choose unknown/reject/quarantine based on needs</li> <li>Measures must be additive - Store components, not percentages</li> <li>Grain validation catches duplicates - Define it, enforce it</li> <li>Test your fact tables - Row counts, orphan counts, measure totals</li> </ol>"},{"location":"marketing/articles/article_08_fact_table_patterns/#next-steps","title":"Next Steps","text":"<p>With all the pieces in place: - Bronze layer \u2713 - Silver layer \u2713 - Date dimension \u2713 - Fact table pattern \u2713</p> <p>We're ready to put it all together:</p> <p>Next article: From CSV to Star Schema: Complete Walkthrough.</p>"},{"location":"marketing/articles/article_08_fact_table_patterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_09_csv_to_star_schema/","title":"From CSV to Star Schema: Complete Walkthrough","text":"<p>Building a complete data warehouse from raw files</p>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#tldr","title":"TL;DR","text":"<p>This article ties everything together. We take 8 CSV files from the Brazilian E-Commerce dataset and build a complete star schema: Bronze (raw landing), Silver (cleaned data), and Gold (dimensional model). You'll get the complete configuration files, execution commands, and verification queries. By the end, you'll have a working data warehouse.</p>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#what-were-building","title":"What We're Building","text":"<p>Starting point: 8 CSV files from Kaggle</p> <pre><code>data/landing/\n\u251c\u2500\u2500 olist_orders_dataset.csv\n\u251c\u2500\u2500 olist_order_items_dataset.csv\n\u251c\u2500\u2500 olist_customers_dataset.csv\n\u251c\u2500\u2500 olist_products_dataset.csv\n\u251c\u2500\u2500 olist_sellers_dataset.csv\n\u251c\u2500\u2500 olist_order_payments_dataset.csv\n\u251c\u2500\u2500 olist_order_reviews_dataset.csv\n\u2514\u2500\u2500 olist_geolocation_dataset.csv\n</code></pre> <p>End result: A star schema</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     GOLD LAYER                               \u2502\n\u2502                                                              \u2502\n\u2502    dim_date \u2500\u2500\u2500\u2500\u2500\u2510                                          \u2502\n\u2502                  \u2502                                          \u2502\n\u2502    dim_customer \u2500\u253c\u2500\u2500\u2500\u2500 fact_order_items \u2500\u2500\u2500\u2500 dim_product   \u2502\n\u2502                  \u2502                                          \u2502\n\u2502    dim_seller \u2500\u2500\u2500\u2518                                          \u2502\n\u2502                                                              \u2502\n\u2502    fact_orders \u2500\u2500\u2500\u2500 fact_payments \u2500\u2500\u2500\u2500 fact_reviews        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#project-structure","title":"Project Structure","text":"<pre><code>ecommerce_warehouse/\n\u251c\u2500\u2500 odibi.yaml              # Main configuration\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 landing/            # Raw CSV files\n\u251c\u2500\u2500 bronze/                 # Bronze layer (Delta tables)\n\u251c\u2500\u2500 silver/                 # Silver layer (Delta tables)\n\u251c\u2500\u2500 gold/                   # Gold layer (Star schema)\n\u2514\u2500\u2500 logs/                   # Pipeline logs\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   LANDING   \u2502     \u2502   BRONZE    \u2502     \u2502   SILVER    \u2502\n\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u2502  8 CSV      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  8 Delta    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  8 Delta    \u2502\n\u2502  files      \u2502     \u2502  tables     \u2502     \u2502  tables     \u2502\n\u2502             \u2502     \u2502  (raw +     \u2502     \u2502  (cleaned,  \u2502\n\u2502             \u2502     \u2502  metadata)  \u2502     \u2502  validated) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                               \u2502\n                                               \u25bc\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2502    GOLD     \u2502\n                                        \u2502             \u2502\n                                        \u2502  4 Dims     \u2502\n                                        \u2502  4 Facts    \u2502\n                                        \u2502  (star      \u2502\n                                        \u2502  schema)    \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#complete-configuration","title":"Complete Configuration","text":"<p>Here's the full <code>odibi.yaml</code>:</p> <pre><code>project: \"ecommerce_warehouse\"\nengine: \"pandas\"\nversion: \"1.0.0\"\ndescription: \"Brazilian E-Commerce Data Warehouse\"\nowner: \"data-team\"\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONNECTIONS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nconnections:\n  landing:\n    type: local\n    base_path: \"./data/landing\"\n\n  bronze:\n    type: local\n    base_path: \"./bronze\"\n\n  silver:\n    type: local\n    base_path: \"./silver\"\n\n  gold:\n    type: local\n    base_path: \"./gold\"\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# SYSTEM CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nstory:\n  connection: bronze\n  path: \"_stories\"\n  retention_days: 30\n\nsystem:\n  connection: bronze\n  path: \"_system\"\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# PIPELINES\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\npipelines:\n\n  # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  # BRONZE LAYER - Raw Data Landing\n  # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n  - pipeline: bronze_ecommerce\n    layer: bronze\n    description: \"Land raw e-commerce data from CSV files\"\n\n    nodes:\n      - name: bronze_orders\n        description: \"Raw order headers\"\n        read:\n          connection: landing\n          path: olist_orders_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_orders_dataset.csv'\"\n        write:\n          connection: bronze\n          path: orders\n          format: delta\n          mode: append\n\n      - name: bronze_order_items\n        description: \"Raw order line items\"\n        read:\n          connection: landing\n          path: olist_order_items_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_order_items_dataset.csv'\"\n        write:\n          connection: bronze\n          path: order_items\n          format: delta\n          mode: append\n\n      - name: bronze_customers\n        description: \"Raw customer data\"\n        read:\n          connection: landing\n          path: olist_customers_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_customers_dataset.csv'\"\n        write:\n          connection: bronze\n          path: customers\n          format: delta\n          mode: append\n\n      - name: bronze_products\n        description: \"Raw product catalog\"\n        read:\n          connection: landing\n          path: olist_products_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_products_dataset.csv'\"\n        write:\n          connection: bronze\n          path: products\n          format: delta\n          mode: append\n\n      - name: bronze_sellers\n        description: \"Raw seller data\"\n        read:\n          connection: landing\n          path: olist_sellers_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_sellers_dataset.csv'\"\n        write:\n          connection: bronze\n          path: sellers\n          format: delta\n          mode: append\n\n      - name: bronze_payments\n        description: \"Raw payment data\"\n        read:\n          connection: landing\n          path: olist_order_payments_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_order_payments_dataset.csv'\"\n        write:\n          connection: bronze\n          path: payments\n          format: delta\n          mode: append\n\n      - name: bronze_reviews\n        description: \"Raw review data\"\n        read:\n          connection: landing\n          path: olist_order_reviews_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_order_reviews_dataset.csv'\"\n        write:\n          connection: bronze\n          path: reviews\n          format: delta\n          mode: append\n\n      - name: bronze_geolocation\n        description: \"Raw geolocation data\"\n        read:\n          connection: landing\n          path: olist_geolocation_dataset.csv\n          format: csv\n          options:\n            header: true\n            inferSchema: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  _extracted_at: \"current_timestamp()\"\n                  _source_file: \"'olist_geolocation_dataset.csv'\"\n        write:\n          connection: bronze\n          path: geolocation\n          format: delta\n          mode: append\n\n  # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  # SILVER LAYER - Cleaned and Validated Data\n  # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n  - pipeline: silver_ecommerce\n    layer: silver\n    description: \"Cleaned and validated e-commerce data\"\n\n    nodes:\n      - name: silver_orders\n        description: \"Validated order data\"\n        read:\n          connection: bronze\n          path: orders\n          format: delta\n        contracts:\n          - type: not_null\n            column: order_id\n            severity: error\n          - type: not_null\n            column: customer_id\n            severity: error\n          - type: unique\n            columns: [order_id]\n            severity: error\n        transformer: deduplicate\n        params:\n          keys: [order_id]\n          order_by: _extracted_at DESC\n        transform:\n          steps:\n            - function: clean_text\n              params:\n                columns: [order_status]\n                case: lower\n                trim: true\n            - function: derive_columns\n              params:\n                columns:\n                  order_purchase_date: \"TO_DATE(order_purchase_timestamp)\"\n                  order_delivered_date: \"TO_DATE(order_delivered_customer_date)\"\n                  order_estimated_date: \"TO_DATE(order_estimated_delivery_date)\"\n            - sql: |\n                SELECT\n                  order_id,\n                  customer_id,\n                  order_status,\n                  order_purchase_date,\n                  order_delivered_date,\n                  order_estimated_date,\n                  DATEDIFF(order_delivered_date, order_purchase_date) as days_to_delivery,\n                  CASE WHEN order_delivered_date &gt; order_estimated_date THEN 1 ELSE 0 END as is_late,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: orders\n          format: delta\n          mode: overwrite\n\n      - name: silver_order_items\n        description: \"Validated order line items\"\n        read:\n          connection: bronze\n          path: order_items\n          format: delta\n        contracts:\n          - type: not_null\n            column: order_id\n            severity: error\n          - type: not_null\n            column: product_id\n            severity: error\n          - type: range\n            column: price\n            min: 0\n            severity: error\n        transformer: deduplicate\n        params:\n          keys: [order_id, order_item_id]\n          order_by: _extracted_at DESC\n        transform:\n          steps:\n            - function: cast_columns\n              params:\n                columns:\n                  price: double\n                  freight_value: double\n            - function: derive_columns\n              params:\n                columns:\n                  line_total: \"price + freight_value\"\n            - sql: |\n                SELECT\n                  order_id,\n                  order_item_id,\n                  product_id,\n                  seller_id,\n                  price,\n                  freight_value,\n                  line_total,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: order_items\n          format: delta\n          mode: overwrite\n\n      - name: silver_customers\n        description: \"Validated customer data\"\n        read:\n          connection: bronze\n          path: customers\n          format: delta\n        contracts:\n          - type: not_null\n            column: customer_id\n            severity: error\n          - type: unique\n            columns: [customer_id]\n            severity: error\n        transformer: deduplicate\n        params:\n          keys: [customer_id]\n          order_by: _extracted_at DESC\n        transform:\n          steps:\n            - function: clean_text\n              params:\n                columns: [customer_city, customer_state]\n                case: upper\n                trim: true\n            - sql: |\n                SELECT\n                  customer_id,\n                  customer_unique_id,\n                  customer_zip_code_prefix,\n                  customer_city,\n                  customer_state,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: customers\n          format: delta\n          mode: overwrite\n\n      - name: silver_products\n        description: \"Validated product catalog\"\n        read:\n          connection: bronze\n          path: products\n          format: delta\n        contracts:\n          - type: not_null\n            column: product_id\n            severity: error\n          - type: unique\n            columns: [product_id]\n            severity: error\n        transformer: deduplicate\n        params:\n          keys: [product_id]\n          order_by: _extracted_at DESC\n        transform:\n          steps:\n            - function: fill_nulls\n              params:\n                columns:\n                  product_category_name: \"unknown\"\n                  product_weight_g: 0\n            - function: clean_text\n              params:\n                columns: [product_category_name]\n                case: lower\n                trim: true\n            - sql: |\n                SELECT\n                  product_id,\n                  product_category_name,\n                  product_weight_g,\n                  product_length_cm,\n                  product_height_cm,\n                  product_width_cm,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: products\n          format: delta\n          mode: overwrite\n\n      - name: silver_sellers\n        description: \"Validated seller data\"\n        read:\n          connection: bronze\n          path: sellers\n          format: delta\n        contracts:\n          - type: not_null\n            column: seller_id\n            severity: error\n          - type: unique\n            columns: [seller_id]\n            severity: error\n        transformer: deduplicate\n        params:\n          keys: [seller_id]\n          order_by: _extracted_at DESC\n        transform:\n          steps:\n            - function: clean_text\n              params:\n                columns: [seller_city, seller_state]\n                case: upper\n                trim: true\n            - sql: |\n                SELECT\n                  seller_id,\n                  seller_zip_code_prefix,\n                  seller_city,\n                  seller_state,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: sellers\n          format: delta\n          mode: overwrite\n\n      - name: silver_payments\n        description: \"Validated payment data\"\n        read:\n          connection: bronze\n          path: payments\n          format: delta\n        contracts:\n          - type: not_null\n            column: order_id\n            severity: error\n          - type: range\n            column: payment_value\n            min: 0\n            severity: error\n        transform:\n          steps:\n            - function: clean_text\n              params:\n                columns: [payment_type]\n                case: lower\n                trim: true\n            - sql: |\n                SELECT\n                  order_id,\n                  payment_sequential,\n                  payment_type,\n                  payment_installments,\n                  payment_value,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: payments\n          format: delta\n          mode: overwrite\n\n      - name: silver_reviews\n        description: \"Validated review data\"\n        read:\n          connection: bronze\n          path: reviews\n          format: delta\n        contracts:\n          - type: not_null\n            column: review_id\n            severity: error\n          - type: range\n            column: review_score\n            min: 1\n            max: 5\n            severity: error\n        transformer: deduplicate\n        params:\n          keys: [review_id]\n          order_by: _extracted_at DESC\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  review_date: \"TO_DATE(review_creation_date)\"\n            - sql: |\n                SELECT\n                  review_id,\n                  order_id,\n                  review_score,\n                  review_date,\n                  _extracted_at\n                FROM df\n        write:\n          connection: silver\n          path: reviews\n          format: delta\n          mode: overwrite\n\n  # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  # GOLD LAYER - Dimensional Model (Star Schema)\n  # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n  - pipeline: gold_dimensions\n    layer: gold\n    description: \"Dimension tables for star schema\"\n\n    nodes:\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # DATE DIMENSION\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: dim_date\n        description: \"Date dimension 2016-2025\"\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2016-01-01\"\n            end_date: \"2025-12-31\"\n            date_key_format: \"yyyyMMdd\"\n            fiscal_year_start_month: 1\n            unknown_member: true\n        write:\n          connection: gold\n          path: dim_date\n          format: delta\n          mode: overwrite\n\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # CUSTOMER DIMENSION\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: dim_customer\n        description: \"Customer dimension with surrogate key\"\n        depends_on: [silver_customers]\n        read:\n          connection: silver\n          path: customers\n          format: delta\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 1\n            unknown_member: true\n            audit:\n              load_timestamp: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  customer_region: |\n                    CASE \n                      WHEN customer_state IN ('SP', 'RJ', 'MG', 'ES') THEN 'Southeast'\n                      WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n                      WHEN customer_state IN ('BA', 'SE', 'AL', 'PE', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n                      WHEN customer_state IN ('MT', 'MS', 'GO', 'DF') THEN 'Central-West'\n                      ELSE 'North'\n                    END\n        write:\n          connection: gold\n          path: dim_customer\n          format: delta\n          mode: overwrite\n\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # PRODUCT DIMENSION\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: dim_product\n        description: \"Product dimension with surrogate key\"\n        depends_on: [silver_products]\n        read:\n          connection: silver\n          path: products\n          format: delta\n        pattern:\n          type: dimension\n          params:\n            natural_key: product_id\n            surrogate_key: product_sk\n            scd_type: 1\n            unknown_member: true\n            audit:\n              load_timestamp: true\n        write:\n          connection: gold\n          path: dim_product\n          format: delta\n          mode: overwrite\n\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # SELLER DIMENSION\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: dim_seller\n        description: \"Seller dimension with surrogate key\"\n        depends_on: [silver_sellers]\n        read:\n          connection: silver\n          path: sellers\n          format: delta\n        pattern:\n          type: dimension\n          params:\n            natural_key: seller_id\n            surrogate_key: seller_sk\n            scd_type: 1\n            unknown_member: true\n            audit:\n              load_timestamp: true\n        transform:\n          steps:\n            - function: derive_columns\n              params:\n                columns:\n                  seller_region: |\n                    CASE \n                      WHEN seller_state IN ('SP', 'RJ', 'MG', 'ES') THEN 'Southeast'\n                      WHEN seller_state IN ('PR', 'SC', 'RS') THEN 'South'\n                      WHEN seller_state IN ('BA', 'SE', 'AL', 'PE', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n                      WHEN seller_state IN ('MT', 'MS', 'GO', 'DF') THEN 'Central-West'\n                      ELSE 'North'\n                    END\n        write:\n          connection: gold\n          path: dim_seller\n          format: delta\n          mode: overwrite\n\n  - pipeline: gold_facts\n    layer: gold\n    description: \"Fact tables for star schema\"\n\n    nodes:\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # FACT ORDER ITEMS\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: fact_order_items\n        description: \"Order line items fact table\"\n        depends_on:\n          - silver_order_items\n          - silver_orders\n          - dim_customer\n          - dim_product\n          - dim_seller\n          - dim_date\n        inputs:\n          order_items: $silver_ecommerce.silver_order_items\n          orders: $silver_ecommerce.silver_orders\n        transform:\n          steps:\n            - sql: |\n                SELECT \n                  oi.order_id,\n                  oi.order_item_id,\n                  oi.product_id,\n                  oi.seller_id,\n                  o.customer_id,\n                  o.order_purchase_date,\n                  oi.price,\n                  oi.freight_value,\n                  oi.line_total\n                FROM order_items oi\n                JOIN orders o ON oi.order_id = o.order_id\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, order_item_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n              - source_column: seller_id\n                dimension_table: dim_seller\n                dimension_key: seller_id\n                surrogate_key: seller_sk\n              - source_column: order_purchase_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n                target_column: order_date_sk\n            orphan_handling: unknown\n            measures:\n              - item_count: \"1\"\n              - price\n              - freight_value\n              - line_total\n            audit:\n              load_timestamp: true\n        write:\n          connection: gold\n          path: fact_order_items\n          format: delta\n          mode: overwrite\n\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # FACT ORDERS (Summary)\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: fact_orders\n        description: \"Order summary fact table\"\n        depends_on:\n          - silver_orders\n          - dim_customer\n          - dim_date\n        read:\n          connection: silver\n          path: orders\n          format: delta\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n              - source_column: order_purchase_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n                target_column: order_date_sk\n            orphan_handling: unknown\n            measures:\n              - order_count: \"1\"\n              - is_late\n              - days_to_delivery\n            audit:\n              load_timestamp: true\n        write:\n          connection: gold\n          path: fact_orders\n          format: delta\n          mode: overwrite\n\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # FACT REVIEWS\n      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: fact_reviews\n        description: \"Review fact table\"\n        depends_on:\n          - silver_reviews\n          - dim_date\n        read:\n          connection: silver\n          path: reviews\n          format: delta\n        pattern:\n          type: fact\n          params:\n            grain: [review_id]\n            dimensions:\n              - source_column: review_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n                target_column: review_date_sk\n            orphan_handling: unknown\n            measures:\n              - review_count: \"1\"\n              - review_score\n            audit:\n              load_timestamp: true\n        write:\n          connection: gold\n          path: fact_reviews\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#running-the-full-pipeline","title":"Running the Full Pipeline","text":"<p>Execute in order:</p> <pre><code># 1. Bronze Layer\nodibi run odibi.yaml --pipeline bronze_ecommerce\n\n# 2. Silver Layer\nodibi run odibi.yaml --pipeline silver_ecommerce\n\n# 3. Gold Dimensions (must run before facts)\nodibi run odibi.yaml --pipeline gold_dimensions\n\n# 4. Gold Facts\nodibi run odibi.yaml --pipeline gold_facts\n</code></pre> <p>Or run everything:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Expected output:</p> <pre><code>[INFO] Starting pipeline: bronze_ecommerce\n[INFO]   Completed 8 nodes in 12.3s\n[INFO] Starting pipeline: silver_ecommerce\n[INFO]   Completed 7 nodes in 8.7s\n[INFO] Starting pipeline: gold_dimensions\n[INFO]   Completed 4 nodes in 3.2s\n[INFO] Starting pipeline: gold_facts\n[INFO]   Completed 3 nodes in 5.1s\n[INFO] All pipelines completed successfully\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#results","title":"Results","text":"<p>After running all pipelines:</p>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#bronze-layer","title":"Bronze Layer","text":"Table Rows Description bronze.orders 99,441 Raw orders + metadata bronze.order_items 112,650 Raw line items + metadata bronze.customers 99,441 Raw customers + metadata bronze.products 32,951 Raw products + metadata bronze.sellers 3,095 Raw sellers + metadata bronze.payments 103,886 Raw payments + metadata bronze.reviews 100,000 Raw reviews + metadata bronze.geolocation 1,000,163 Raw geo data + metadata"},{"location":"marketing/articles/article_09_csv_to_star_schema/#silver-layer","title":"Silver Layer","text":"Table Rows Key Transformations silver.orders 99,441 Dates parsed, delivery metrics calculated silver.order_items 112,650 Prices validated, line_total added silver.customers 99,441 Text standardized silver.products 32,951 Nulls filled, categories cleaned silver.sellers 3,095 Text standardized silver.payments 103,886 Types standardized silver.reviews 100,000 Dates parsed, scores validated"},{"location":"marketing/articles/article_09_csv_to_star_schema/#gold-layer","title":"Gold Layer","text":"Table Rows Description dim_date 3,653 10 years of dates + unknown dim_customer 99,442 Customers + unknown member dim_product 32,952 Products + unknown member dim_seller 3,096 Sellers + unknown member fact_order_items 112,650 Line items with SKs fact_orders 99,441 Orders with SKs fact_reviews 100,000 Reviews with SKs"},{"location":"marketing/articles/article_09_csv_to_star_schema/#query-examples","title":"Query Examples","text":"<p>Now you can run star schema queries:</p>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#revenue-by-region-and-month","title":"Revenue by Region and Month","text":"<pre><code>SELECT \n  dc.customer_region,\n  dd.month_name,\n  dd.year,\n  SUM(f.line_total) as revenue,\n  COUNT(*) as order_count\nFROM gold.fact_order_items f\nJOIN gold.dim_customer dc ON f.customer_sk = dc.customer_sk\nJOIN gold.dim_date dd ON f.order_date_sk = dd.date_sk\nWHERE dd.year = 2018\nGROUP BY dc.customer_region, dd.month_name, dd.year\nORDER BY dc.customer_region, dd.year, dd.month\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#top-products-by-revenue","title":"Top Products by Revenue","text":"<pre><code>SELECT \n  dp.product_category_name,\n  SUM(f.line_total) as revenue,\n  SUM(f.item_count) as items_sold\nFROM gold.fact_order_items f\nJOIN gold.dim_product dp ON f.product_sk = dp.product_sk\nGROUP BY dp.product_category_name\nORDER BY revenue DESC\nLIMIT 10\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#late-delivery-analysis","title":"Late Delivery Analysis","text":"<pre><code>SELECT \n  dc.customer_region,\n  SUM(f.is_late) as late_orders,\n  SUM(f.order_count) as total_orders,\n  ROUND(100.0 * SUM(f.is_late) / SUM(f.order_count), 2) as late_pct\nFROM gold.fact_orders f\nJOIN gold.dim_customer dc ON f.customer_sk = dc.customer_sk\nGROUP BY dc.customer_region\nORDER BY late_pct DESC\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#review-score-distribution","title":"Review Score Distribution","text":"<pre><code>SELECT \n  review_score,\n  SUM(review_count) as count,\n  ROUND(100.0 * SUM(review_count) / (SELECT SUM(review_count) FROM gold.fact_reviews), 2) as pct\nFROM gold.fact_reviews\nGROUP BY review_score\nORDER BY review_score\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#what-we-built","title":"What We Built","text":"<pre><code>LANDING (8 CSV files, ~1.5M rows)\n    \u2502\n    \u25bc\nBRONZE (8 Delta tables, raw + metadata)\n    \u2502\n    \u25bc\nSILVER (8 Delta tables, cleaned + validated)\n    \u2502\n    \u25bc\nGOLD (Star Schema)\n    \u251c\u2500\u2500 dim_date (3,653 rows)\n    \u251c\u2500\u2500 dim_customer (99,442 rows)\n    \u251c\u2500\u2500 dim_product (32,952 rows)\n    \u251c\u2500\u2500 dim_seller (3,096 rows)\n    \u251c\u2500\u2500 fact_order_items (112,650 rows)\n    \u251c\u2500\u2500 fact_orders (99,441 rows)\n    \u2514\u2500\u2500 fact_reviews (100,000 rows)\n</code></pre>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Layers serve different purposes - Bronze preserves, Silver cleans, Gold models</li> <li>Configuration is documentation - YAML describes the entire pipeline</li> <li>Patterns reduce code - Dimension, Fact, Date patterns are declarative</li> <li>Dependencies matter - Dimensions before facts</li> <li>Validation catches problems early - Contracts stop bad data</li> </ol>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#next-steps","title":"Next Steps","text":"<p>With a complete star schema, you can:</p> <ol> <li>Connect BI tools (Power BI, Tableau)</li> <li>Build aggregation tables for dashboards</li> <li>Add more facts (payments, inventory snapshots)</li> <li>Implement incremental loading</li> <li>Add a semantic layer for metrics</li> </ol> <p>Next article: Introducing Odibi (Article 10, already complete)</p>"},{"location":"marketing/articles/article_09_csv_to_star_schema/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_10_introducing_odibi/","title":"Introducing Odibi: Declarative Data Pipelines","text":"<p>YAML in. Data warehouse out.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#tldr","title":"TL;DR","text":"<p>I built Odibi because I was tired of rewriting the same pipeline code for every project. It's an open-source framework that lets you define data pipelines declaratively-describe WHAT you want, let the tool handle HOW. Patterns for dimensions, facts, SCD2, aggregations, and more. Works with Spark, Pandas, or Polars.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#why-i-built-this","title":"Why I Built This","text":"<p>Every data engineering project I worked on had the same patterns:</p> <ul> <li>Bronze layer ingestion</li> <li>Deduplication</li> <li>SCD2 history tracking</li> <li>Dimension table management</li> <li>Fact table surrogate key lookups</li> <li>Aggregations</li> <li>Data quality validation</li> </ul> <p>And every project, I rewrote them from scratch.</p> <p>Copy-paste from the last project. Tweak for the new schema. Debug the same edge cases. Repeat.</p> <p>After doing this for the fifth time, I thought: Why am I writing the same code over and over?</p> <p>So I stopped.</p> <p>I started abstracting the patterns into reusable components. That became Odibi.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#what-is-odibi","title":"What is Odibi?","text":"<p>Odibi is a declarative data pipeline framework.</p> <p>Declarative means you describe what you want, not how to do it.</p> <p>Instead of writing 200 lines of Python for SCD2:</p> <pre><code># Traditional approach\ndef apply_scd2(source_df, target_df, keys, track_cols, effective_col):\n    # Match records\n    matched = source_df.join(target_df, on=keys, how=\"left\")\n\n    # Find changes\n    changed = matched.filter(/* complex comparison */)\n\n    # Close old records\n    closed = target_df.filter(/* is_current and has_changes */)\n    closed = closed.withColumn(\"valid_to\", /* new value */)\n    closed = closed.withColumn(\"is_current\", lit(False))\n\n    # Insert new records\n    new_records = changed.withColumn(\"valid_from\", col(effective_col))\n    new_records = new_records.withColumn(\"valid_to\", lit(None))\n    new_records = new_records.withColumn(\"is_current\", lit(True))\n\n    # Union everything\n    result = unchanged.union(closed).union(new_records)\n\n    # Handle first run\n    # Handle nulls\n    # Handle duplicates\n    # ... 150 more lines\n\n    return result\n</code></pre> <p>You write 10 lines of YAML:</p> <pre><code>- name: dim_customer\n  transformer: scd2\n  params:\n    target: silver.dim_customer\n    keys: [customer_id]\n    track_cols: [name, email, address]\n    effective_time_col: updated_at\n</code></pre> <p>The framework handles the complexity. You focus on the business logic.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#core-concepts","title":"Core Concepts","text":""},{"location":"marketing/articles/article_10_introducing_odibi/#nodes","title":"Nodes","text":"<p>A node is a single step in your pipeline. It reads data, optionally transforms it, and writes the result.</p> <pre><code>nodes:\n  - name: bronze_orders\n    read:\n      connection: landing\n      path: orders.csv\n    write:\n      connection: bronze\n      path: orders\n</code></pre>"},{"location":"marketing/articles/article_10_introducing_odibi/#transformers","title":"Transformers","text":"<p>A transformer is a reusable operation. Odibi has 30+ built-in:</p> <p>Data Engineering Patterns: - <code>scd2</code> - Slowly changing dimensions type 2 - <code>merge</code> - Upsert/merge operations - <code>deduplicate</code> - Remove duplicates</p> <p>Relational Operations: - <code>join</code> - Combine datasets - <code>union</code> - Stack datasets - <code>aggregate</code> - Group and summarize - <code>pivot</code> / <code>unpivot</code> - Reshape data</p> <p>Data Quality: - <code>validate_and_flag</code> - Check rules, flag violations - <code>filter_rows</code> - SQL-based filtering - <code>fill_nulls</code> - Handle missing values</p> <p>Feature Engineering: - <code>derive_columns</code> - Create calculated columns - <code>case_when</code> - Conditional logic - <code>generate_surrogate_key</code> - Create unique keys</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#patterns","title":"Patterns","text":"<p>A pattern is a higher-level abstraction for common data modeling tasks:</p> <pre><code># Dimension pattern\npattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_cols: [name, email]\n\n# Fact pattern\npattern:\n  type: fact\n  params:\n    grain: [order_id, product_sk]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        surrogate_key: customer_sk\n    orphan_handling: unknown\n\n# Aggregation pattern\npattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk]\n    measures:\n      - name: revenue\n        expr: \"SUM(amount)\"\n</code></pre>"},{"location":"marketing/articles/article_10_introducing_odibi/#contracts","title":"Contracts","text":"<p>Contracts are data quality checks that run before loading:</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id]\n\n  - type: accepted_values\n    column: status\n    values: [pending, shipped, delivered]\n\n  - type: row_count\n    min: 1000\n</code></pre> <p>If a contract fails, the pipeline stops before bad data enters.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#example-complete-pipeline","title":"Example: Complete Pipeline","text":"<p>Here's a simplified e-commerce pipeline:</p> <pre><code>project: \"ecommerce\"\nengine: \"spark\"\n\nconnections:\n  bronze:\n    type: delta\n    catalog: spark_catalog\n    schema_name: bronze\n\n  silver:\n    type: delta\n    catalog: spark_catalog\n    schema_name: silver\n\n  gold:\n    type: delta\n    catalog: spark_catalog\n    schema_name: gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      # Silver: Clean customers\n      - name: silver_customers\n        read:\n          connection: bronze\n          table: customers\n        transformer: deduplicate\n        params:\n          keys: [customer_id]\n        write:\n          connection: silver\n          table: customers\n\n      # Gold: Dimension\n      - name: dim_customer\n        depends_on: [silver_customers]\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols: [name, email, city]\n        write:\n          connection: gold\n          table: dim_customer\n\n      # Gold: Fact\n      - name: fact_orders\n        read:\n          connection: silver\n          table: orders\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n            orphan_handling: unknown\n        write:\n          connection: gold\n          table: fact_orders\n</code></pre> <p>Run it:</p> <pre><code>odibi run config.yaml\n</code></pre> <p>That's it. Bronze \u2192 Silver \u2192 Gold in one command.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#multi-engine-support","title":"Multi-Engine Support","text":"<p>Odibi works with three engines:</p> Engine Best For Scale Pandas Development, small data &lt; 1GB Polars Medium data, fast single-machine 1-10GB Spark Production, large data 10GB+ <p>Same YAML works with all three:</p> <pre><code># Development\nengine: pandas\n\n# Production\nengine: spark\n</code></pre> <p>Switch engines without rewriting pipelines.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#what-odibi-handles-for-you","title":"What Odibi Handles For You","text":""},{"location":"marketing/articles/article_10_introducing_odibi/#scd2-complexity","title":"SCD2 Complexity","text":"<ul> <li>First run detection</li> <li>Change detection</li> <li>Row versioning</li> <li>Date management</li> <li>Duplicate handling</li> </ul>"},{"location":"marketing/articles/article_10_introducing_odibi/#surrogate-key-lookups","title":"Surrogate Key Lookups","text":"<ul> <li>Join to dimension tables</li> <li>Handle nulls (unknown member)</li> <li>Orphan detection and routing</li> </ul>"},{"location":"marketing/articles/article_10_introducing_odibi/#data-quality","title":"Data Quality","text":"<ul> <li>Pre-load validation</li> <li>Quarantine for failures</li> <li>Volume monitoring</li> </ul>"},{"location":"marketing/articles/article_10_introducing_odibi/#operations","title":"Operations","text":"<ul> <li>Incremental loading</li> <li>Schema evolution</li> <li>Retry with backoff</li> <li>Logging and alerting</li> </ul>"},{"location":"marketing/articles/article_10_introducing_odibi/#what-odibi-doesnt-do","title":"What Odibi Doesn't Do","text":"<p>Odibi is not:</p> <ul> <li>An orchestrator - Use Airflow, Databricks Workflows, or Prefect to schedule</li> <li>A data catalog - Use Unity Catalog, Datahub, or Amundsen</li> <li>A BI tool - Use Power BI, Tableau, or Superset</li> </ul> <p>Odibi does one thing: transform data through well-defined patterns.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#getting-started","title":"Getting Started","text":""},{"location":"marketing/articles/article_10_introducing_odibi/#installation","title":"Installation","text":"<pre><code>pip install odibi\n</code></pre>"},{"location":"marketing/articles/article_10_introducing_odibi/#minimal-example","title":"Minimal Example","text":"<pre><code># odibi.yaml\nproject: \"my_project\"\nengine: \"pandas\"\n\nconnections:\n  local:\n    type: local\n    base_path: \"./data\"\n\nstory:\n  connection: local\n  path: \"_stories\"\n\nsystem:\n  connection: local\n  path: \"_system\"\n\npipelines:\n  - pipeline: example\n    nodes:\n      - name: process_data\n        read:\n          connection: local\n          path: input.csv\n          format: csv\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE amount &gt; 0\"\n        write:\n          connection: local\n          path: output\n          format: parquet\n</code></pre> <pre><code>odibi run odibi.yaml\n</code></pre>"},{"location":"marketing/articles/article_10_introducing_odibi/#documentation","title":"Documentation","text":"<p>Full docs: [link to docs]</p> <p>Includes: - Configuration reference - Pattern guides - Transformer catalog - Example projects</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#why-open-source","title":"Why Open Source?","text":"<p>I built Odibi to solve my own problems. But the problems aren't unique to me.</p> <p>Every data engineer I know has rebuilt SCD2 from scratch. Every team has their own dimension pattern code. Every project reinvents the wheel.</p> <p>By open-sourcing Odibi, I hope to:</p> <ol> <li>Save others time - Don't rewrite what's already built</li> <li>Get feedback - Find edge cases I haven't hit</li> <li>Build community - Data engineering patterns should be shared, not siloed</li> </ol>"},{"location":"marketing/articles/article_10_introducing_odibi/#contributing","title":"Contributing","text":"<p>Odibi is early. There are bugs. There are missing features. There are rough edges.</p> <p>If you try it and find issues, I want to know.</p> <p>Ways to contribute: - Report bugs - GitHub issues - Improve docs - Pull requests welcome - Add transformers - Extend the catalog - Share feedback - What's missing? What's confusing?</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#whats-next","title":"What's Next","text":"<p>Over the coming weeks, I'll be sharing:</p> <ul> <li>Deep dives on each pattern</li> <li>Anti-patterns and common mistakes</li> <li>Production deployment guides</li> <li>Performance optimization</li> </ul> <p>Follow along if you're interested.</p>"},{"location":"marketing/articles/article_10_introducing_odibi/#links","title":"Links","text":"<ul> <li>GitHub: https://github.com/henryodibi11/odibi</li> <li>Documentation: [link to docs]</li> <li>LinkedIn: [Your LinkedIn URL]</li> </ul> <p>I'm Henry-a data engineer building in public. I created Odibi to solve problems I kept hitting. If it helps you too, that's a win.</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/","title":"SCD2 Complete Guide: When and How to Track History","text":"<p>The most powerful (and misused) pattern in dimensional modeling</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#tldr","title":"TL;DR","text":"<p>SCD Type 2 tracks the complete history of dimension changes by creating new rows with effective dates. It's essential for \"as-was\" reporting but dangerous when overused-leading to history explosion and performance issues. This article covers when to use SCD2, how it works internally, proper configuration, and common gotchas including volatile columns, deduplication, and performance tuning.</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#what-is-scd2","title":"What is SCD2?","text":"<p>SCD stands for Slowly Changing Dimension. Type 2 is one of several strategies for handling changes to dimension data:</p> Type Strategy Use Case Type 0 Never update Static reference data Type 1 Overwrite Current state only, no history Type 2 Add new row Full history tracking Type 3 Add column Previous + current only Type 6 Hybrid (1+2+3) Complex scenarios <p>SCD2 creates a new row every time a tracked attribute changes, preserving the complete history.</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#how-scd2-works","title":"How SCD2 Works","text":"<p>Let's say a customer moves from S\u00e3o Paulo to Rio de Janeiro:</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#before-one-row","title":"Before (One Row)","text":"customer_sk customer_id city is_current 42 CUST-123 S\u00e3o Paulo true"},{"location":"marketing/articles/article_11_scd2_complete_guide/#after-scd2-update-two-rows","title":"After SCD2 Update (Two Rows)","text":"customer_sk customer_id city is_current valid_from valid_to 42 CUST-123 S\u00e3o Paulo false 2018-01-15 2023-06-01 89 CUST-123 Rio de Janeiro true 2023-06-01 9999-12-31 <p>Key points: - Old row closed: <code>is_current = false</code>, <code>valid_to = 2023-06-01</code> - New row created: <code>is_current = true</code>, new <code>customer_sk</code> - Natural key unchanged: <code>customer_id = CUST-123</code> for both rows - Surrogate key changes: New row gets new <code>customer_sk</code></p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#when-to-use-scd2","title":"When to Use SCD2","text":"<p>Use SCD2 when you need to answer questions about historical state:</p> <p>\u2705 Good Use Cases</p> Question Why SCD2 Helps \"What was the customer's address when they placed this order?\" Point-in-time lookup \"How long was this product in the 'Electronics' category?\" Duration analysis \"What was the sales rep assigned to this territory in Q3 2022?\" Historical attribution <p>\u274c Bad Use Cases</p> Scenario Problem Better Approach Tracking every login timestamp Creates billions of rows Event log / fact table Recording stock price changes 1000+ changes per day Time series, not SCD2 System timestamp fields Changes on every load Exclude from tracking"},{"location":"marketing/articles/article_11_scd2_complete_guide/#the-rule-of-thumb","title":"The Rule of Thumb","text":"<p>SCD2 is for slowly changing data-things that change infrequently but meaningfully: - Customer address changes: Maybe once a year - Product category: Maybe never - Employee department: A few times in a career</p> <p>If it changes frequently (daily, hourly), it's not \"slowly changing\"-use a different pattern.</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#configuration-deep-dive","title":"Configuration Deep Dive","text":""},{"location":"marketing/articles/article_11_scd2_complete_guide/#basic-scd2-configuration","title":"Basic SCD2 Configuration","text":"<pre><code>- name: dim_customer\n  description: \"Customer dimension with history tracking\"\n\n  read:\n    connection: silver\n    path: customers\n    format: delta\n\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols:\n        - customer_city\n        - customer_state\n        - customer_segment\n      target: gold.dim_customer  # Existing table to compare against\n      unknown_member: true\n      audit:\n        load_timestamp: true\n        source_system: \"crm\"\n\n  write:\n    connection: gold\n    path: dim_customer\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#configuration-options","title":"Configuration Options","text":"Parameter Required Description <code>natural_key</code> Yes Business key for matching (e.g., customer_id) <code>surrogate_key</code> Yes Name of SK column to generate <code>scd_type</code> Yes Set to <code>2</code> for history tracking <code>track_cols</code> Yes (for SCD2) Columns to monitor for changes <code>target</code> Yes (for SCD2) Path to existing dimension table <code>unknown_member</code> No Add SK=0 row for orphan handling <code>hash_column</code> No Column name for change hash (default: <code>_row_hash</code>)"},{"location":"marketing/articles/article_11_scd2_complete_guide/#generated-columns","title":"Generated Columns","text":"<p>SCD2 automatically adds these columns:</p> Column Type Description <code>{surrogate_key}</code> INT Auto-generated surrogate key <code>is_current</code> BOOLEAN True for current version <code>valid_from</code> TIMESTAMP When this version became active <code>valid_to</code> TIMESTAMP When this version was superseded <code>_row_hash</code> STRING Hash of tracked columns (for change detection)"},{"location":"marketing/articles/article_11_scd2_complete_guide/#the-scd2-algorithm","title":"The SCD2 Algorithm","text":"<p>Here's what happens during an SCD2 load:</p> <pre><code>1. READ new data from source\n2. HASH tracked columns for each row\n3. COMPARE hashes with existing current records\n4. For UNCHANGED records: Keep as-is\n5. For NEW records: Insert with new SK, is_current=true\n6. For CHANGED records:\n   a. Close old row: is_current=false, valid_to=now\n   b. Insert new row: new SK, is_current=true, valid_from=now\n7. For DELETED records (optional): Close row, valid_to=now\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#visual-example","title":"Visual Example","text":"<pre><code>SOURCE DATA:                      EXISTING DIMENSION:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2502 city    \u2502        \u2502 sk \u2502 customer_id \u2502 city    \u2502 current \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 CUST-001    \u2502 NYC     \u2502  --&gt;   \u2502 1  \u2502 CUST-001    \u2502 NYC     \u2502 true    \u2502 (unchanged)\n\u2502 CUST-002    \u2502 LA      \u2502  --&gt;   \u2502 2  \u2502 CUST-002    \u2502 SF      \u2502 true    \u2502 (changed!)\n\u2502 CUST-003    \u2502 Chicago \u2502  --&gt;   \u2502    \u2502             \u2502         \u2502         \u2502 (new!)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nRESULT:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sk \u2502 customer_id \u2502 city    \u2502 current \u2502 valid_from \u2502 valid_to   \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1  \u2502 CUST-001    \u2502 NYC     \u2502 true    \u2502 2023-01-01 \u2502 9999-12-31 \u2502 (kept)\n\u2502 2  \u2502 CUST-002    \u2502 SF      \u2502 false   \u2502 2023-01-01 \u2502 2023-06-15 \u2502 (closed)\n\u2502 3  \u2502 CUST-002    \u2502 LA      \u2502 true    \u2502 2023-06-15 \u2502 9999-12-31 \u2502 (new version)\n\u2502 4  \u2502 CUST-003    \u2502 Chicago \u2502 true    \u2502 2023-06-15 \u2502 9999-12-31 \u2502 (new record)\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#common-gotchas","title":"Common Gotchas","text":""},{"location":"marketing/articles/article_11_scd2_complete_guide/#1-volatile-columns-history-explosion","title":"1. Volatile Columns (History Explosion)","text":"<p>Problem: Including columns that change frequently</p> <pre><code># \u274c WRONG - updated_at changes on every load!\ntrack_cols:\n  - customer_name\n  - customer_city\n  - updated_at  # This creates a new row every time!\n</code></pre> <p>Result: New SCD2 row every load = millions of rows for no business value</p> <p>Solution: Only track columns with business meaning</p> <pre><code># \u2705 RIGHT - only track meaningful changes\ntrack_cols:\n  - customer_name\n  - customer_city\n  - customer_segment\n# updated_at is NOT tracked\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#2-source-duplicates","title":"2. Source Duplicates","text":"<p>Problem: Source data has duplicate records for same natural key</p> <pre><code>SOURCE:\nCUST-001, New York\nCUST-001, New York  (duplicate!)\n</code></pre> <p>If not handled, SCD2 creates multiple \"current\" rows.</p> <p>Solution: Deduplicate before SCD2</p> <pre><code>- name: dim_customer\n  read:\n    connection: silver\n    path: customers\n\n  # Deduplicate FIRST\n  transformer: deduplicate\n  params:\n    keys: [customer_id]\n    order_by: _extracted_at DESC\n\n  # Then apply SCD2\n  pattern:\n    type: dimension\n    params:\n      scd_type: 2\n      # ...\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#3-performance-on-large-tables","title":"3. Performance on Large Tables","text":"<p>Problem: SCD2 compares every incoming row against existing table</p> <p>For a 10M row dimension with 100K daily updates: - Hash 100K incoming rows - Lookup 100K hashes in 10M row table - Update/Insert operations</p> <p>Solution: Partition and filter</p> <pre><code>pattern:\n  type: dimension\n  params:\n    scd_type: 2\n    partition_by: customer_region  # Partition for performance\n    incremental:\n      column: _extracted_at\n      lookback: \"7 days\"  # Only process recent records\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#4-null-handling-in-tracked-columns","title":"4. NULL Handling in Tracked Columns","text":"<p>Problem: NULL \u2192 \"value\" or \"value\" \u2192 NULL triggers false changes</p> <pre><code>Day 1: customer_segment = NULL\nDay 2: customer_segment = NULL  (NULLs are \"different\" in hashing)\n</code></pre> <p>Solution: Fill NULLs with known values before SCD2</p> <pre><code>transform:\n  steps:\n    - function: fill_nulls\n      params:\n        columns:\n          customer_segment: \"Unknown\"\n          customer_tier: \"Standard\"\n\npattern:\n  type: dimension\n  params:\n    scd_type: 2\n    track_cols: [customer_segment, customer_tier]\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#5-case-sensitivity","title":"5. Case Sensitivity","text":"<p>Problem: \"new york\" vs \"New York\" creates new history</p> <pre><code>Day 1: city = \"New York\"\nDay 2: city = \"NEW YORK\"  (uppercase in source)\n</code></pre> <p>Solution: Standardize case before SCD2</p> <pre><code>transform:\n  steps:\n    - function: clean_text\n      params:\n        columns: [customer_city]\n        case: upper\n        trim: true\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#debugging-scd2-issues","title":"Debugging SCD2 Issues","text":""},{"location":"marketing/articles/article_11_scd2_complete_guide/#check-for-multiple-current-rows","title":"Check for Multiple Current Rows","text":"<pre><code>-- Should return 0 rows\nSELECT customer_id, COUNT(*) as current_count\nFROM gold.dim_customer\nWHERE is_current = true\nGROUP BY customer_id\nHAVING COUNT(*) &gt; 1\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#check-history-growth-rate","title":"Check History Growth Rate","text":"<pre><code>-- History versions per natural key\nSELECT \n  customer_id,\n  COUNT(*) as version_count,\n  MIN(valid_from) as first_version,\n  MAX(valid_from) as latest_version\nFROM gold.dim_customer\nGROUP BY customer_id\nHAVING COUNT(*) &gt; 5  -- Suspiciously many versions\nORDER BY version_count DESC\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#audit-column-changes","title":"Audit Column Changes","text":"<pre><code>-- What changed?\nSELECT \n  a.customer_id,\n  a.customer_city as old_city,\n  b.customer_city as new_city,\n  a.valid_to as change_date\nFROM gold.dim_customer a\nJOIN gold.dim_customer b \n  ON a.customer_id = b.customer_id \n  AND a.valid_to = b.valid_from\nWHERE a.customer_city != b.customer_city\nORDER BY a.valid_to DESC\nLIMIT 100\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#scd2-in-fact-tables-dont-do-it","title":"SCD2 in Fact Tables: Don't Do It","text":"<p>A common mistake: applying SCD2 to fact tables.</p> <pre><code># \u274c NEVER DO THIS\npattern:\n  type: fact\n  params:\n    scd_type: 2  # Facts don't need SCD2!\n</code></pre> <p>Facts are immutable events. Once an order is placed, it doesn't change. If you need to track order updates:</p> <ol> <li>Create an <code>order_events</code> fact table (append-only)</li> <li>Use SCD2 on dimensions only</li> <li>The latest <code>order_status</code> lives in a dimension, not the fact</li> </ol>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#full-example","title":"Full Example","text":"<p>Here's a complete SCD2 dimension configuration:</p> <pre><code>- name: dim_customer\n  description: \"Customer dimension with SCD2 history tracking\"\n\n  read:\n    connection: silver\n    path: customers\n    format: delta\n\n  # Pre-processing: clean and deduplicate\n  transformer: deduplicate\n  params:\n    keys: [customer_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      # Standardize text to prevent false changes\n      - function: clean_text\n        params:\n          columns: [customer_city, customer_state]\n          case: upper\n          trim: true\n\n      # Fill NULLs to prevent hash issues\n      - function: fill_nulls\n        params:\n          columns:\n            customer_segment: \"Unknown\"\n            customer_tier: \"Standard\"\n\n      # Add derived attributes\n      - function: derive_columns\n        params:\n          columns:\n            customer_region: |\n              CASE \n                WHEN customer_state IN ('SP', 'RJ', 'MG', 'ES') THEN 'Southeast'\n                WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n                ELSE 'Other'\n              END\n\n  # Apply SCD2 pattern\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols:\n        - customer_city\n        - customer_state\n        - customer_region\n        - customer_segment\n        - customer_tier\n      target: gold.dim_customer\n      unknown_member: true\n      audit:\n        load_timestamp: true\n        source_system: \"ecommerce\"\n\n  write:\n    connection: gold\n    path: dim_customer\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>SCD2 is for slowly changing data - Not for high-frequency changes</li> <li>Choose tracked columns carefully - Exclude volatile fields</li> <li>Deduplicate before SCD2 - Prevent multiple current rows</li> <li>Standardize data first - Case, NULLs, whitespace</li> <li>Monitor history growth - Excessive versions indicate problems</li> <li>Never use SCD2 on facts - Facts are immutable events</li> </ol>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#next-steps","title":"Next Steps","text":"<p>With SCD2 understood, we'll explore more dimension patterns:</p> <ul> <li>Unknown members and orphan handling</li> <li>Dimension keys and auditing</li> <li>Role-playing dimensions</li> </ul> <p>Next article: Dimension Table Patterns: Unknown Members, Keys, and Auditing.</p>"},{"location":"marketing/articles/article_11_scd2_complete_guide/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_12_dimension_patterns/","title":"Dimension Table Patterns: Unknown Members, Keys, and Auditing","text":"<p>Building dimensions that handle real-world messiness</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#tldr","title":"TL;DR","text":"<p>Dimension tables need more than just surrogate keys. They need unknown member rows for orphan handling, proper key management for consistency, and audit columns for debugging. This article covers the essential patterns: unknown members, natural vs surrogate keys, conformed dimensions, role-playing dimensions, and comprehensive auditing.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#the-unknown-member-pattern","title":"The Unknown Member Pattern","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#the-problem","title":"The Problem","text":"<p>Your fact table has <code>customer_id = CUST-999</code>. But <code>dim_customer</code> has no record for <code>CUST-999</code>.</p> <p>What happens?</p> Strategy Result Problem NULL foreign key <code>customer_sk = NULL</code> Joins fail, counts wrong Inner join in ETL Row dropped Data loss Fail pipeline Nothing loads Downstream blocked"},{"location":"marketing/articles/article_12_dimension_patterns/#the-solution-unknown-member","title":"The Solution: Unknown Member","text":"<p>Add a row with <code>surrogate_key = 0</code>:</p> customer_sk customer_id customer_name customer_city 0 UNKNOWN Unknown Customer Unknown 1 CUST-001 John Smith S\u00e3o Paulo 2 CUST-002 Maria Silva Rio de Janeiro <p>Now orphan facts map to <code>customer_sk = 0</code>:</p> <pre><code>SELECT \n  d.customer_name,\n  SUM(f.amount)\nFROM fact_orders f\nJOIN dim_customer d ON f.customer_sk = d.customer_sk\nGROUP BY d.customer_name\n</code></pre> <p>Results include \"Unknown Customer\" for orphan orders. No data loss, no NULL issues.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#configuration","title":"Configuration","text":"<pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    unknown_member: true  # Adds SK=0 row\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#customizing-unknown-member","title":"Customizing Unknown Member","text":"<p>Default unknown member has generic values. Customize with explicit attributes:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    unknown_member:\n      customer_id: \"-1\"\n      customer_name: \"Unknown Customer\"\n      customer_city: \"N/A\"\n      customer_state: \"N/A\"\n      customer_region: \"Unassigned\"\n      is_active: false\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#key-management","title":"Key Management","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#natural-keys-vs-surrogate-keys","title":"Natural Keys vs Surrogate Keys","text":"Concept Definition Example Natural Key Business identifier from source <code>customer_id = \"CUST-12345\"</code> Surrogate Key Warehouse-generated integer <code>customer_sk = 42</code>"},{"location":"marketing/articles/article_12_dimension_patterns/#why-use-both","title":"Why Use Both?","text":"<p>Natural Key (<code>customer_id</code>): - Match incoming records to existing dimension rows - Lookup for humans (\"find customer CUST-12345\") - Cross-reference with source systems</p> <p>Surrogate Key (<code>customer_sk</code>): - Stable join key (natural keys can change format) - Enable SCD2 history (same natural key, multiple rows) - Faster integer joins - Multi-source integration (two systems, same natural key format)</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#key-generation-strategies","title":"Key Generation Strategies","text":"<p>Odibi generates surrogate keys using <code>MAX(existing) + ROW_NUMBER</code>:</p> <pre><code>-- For new records:\nSELECT \n  (SELECT COALESCE(MAX(customer_sk), 0) FROM dim_customer) + ROW_NUMBER() as customer_sk,\n  customer_id,\n  ...\nFROM incoming_data\nWHERE customer_id NOT IN (SELECT customer_id FROM dim_customer)\n</code></pre> <p>Configuration:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    # SK starts at 1 (0 reserved for unknown member)\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#composite-natural-keys","title":"Composite Natural Keys","text":"<p>Some dimensions have compound business keys:</p> <pre><code># Product in category hierarchy\npattern:\n  type: dimension\n  params:\n    natural_key: [category_id, product_id]  # Compound key\n    surrogate_key: product_sk\n</code></pre> <p>This generates one SK per unique combination.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#conformed-dimensions","title":"Conformed Dimensions","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#the-problem_1","title":"The Problem","text":"<p>You have three fact tables: - <code>fact_sales</code> with <code>customer_sk</code> - <code>fact_returns</code> with <code>customer_sk</code> - <code>fact_support_tickets</code> with <code>customer_key</code>  \u2190 Different name!</p> <p>And two customer dimensions: - Sales team uses <code>dim_customer_sales</code> - Support team uses <code>dim_customer_support</code></p> <p>Result: Can't join sales to support tickets. Can't get \"360 view\" of customer.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#the-solution-conformed-dimensions","title":"The Solution: Conformed Dimensions","text":"<p>One dimension serves all facts:</p> <pre><code>              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502   dim_customer   \u2502\n              \u2502  (single source) \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u25bc             \u25bc             \u25bc\n   fact_sales    fact_returns   fact_support\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#implementation","title":"Implementation","text":"<ol> <li>Build once: Create <code>dim_customer</code> in Gold layer</li> <li>Reference everywhere: All facts use the same dimension</li> <li>Consistent naming: <code>customer_sk</code> in all facts</li> </ol> <pre><code># Gold dimension (single source of truth)\n- name: dim_customer\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n  write:\n    connection: gold\n    path: dim_customer\n\n# All facts reference the same dimension\n- name: fact_sales\n  pattern:\n    type: fact\n    params:\n      dimensions:\n        - dimension_table: dim_customer\n          surrogate_key: customer_sk\n\n- name: fact_returns\n  pattern:\n    type: fact\n    params:\n      dimensions:\n        - dimension_table: dim_customer\n          surrogate_key: customer_sk\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#role-playing-dimensions","title":"Role-Playing Dimensions","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#the-problem_2","title":"The Problem","text":"<p>An order has three dates: - <code>order_date</code> - when placed - <code>ship_date</code> - when shipped - <code>delivery_date</code> - when delivered</p> <p>One <code>dim_date</code>, three foreign keys in the fact:</p> <pre><code>SELECT \n  -- Which dim_date row?\n  d.month_name\nFROM fact_orders f\nJOIN dim_date d ON f.??? = d.date_sk  -- order_date? ship_date? delivery_date?\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#the-solution-role-playing","title":"The Solution: Role-Playing","text":"<p>The same dimension \"plays different roles\":</p> <pre><code>- name: fact_orders\n  pattern:\n    type: fact\n    params:\n      dimensions:\n        # Same dimension, different roles\n        - source_column: order_date\n          dimension_table: dim_date\n          surrogate_key: date_sk\n          target_column: order_date_sk  # Role: order date\n\n        - source_column: ship_date\n          dimension_table: dim_date\n          surrogate_key: date_sk\n          target_column: ship_date_sk   # Role: ship date\n\n        - source_column: delivery_date\n          dimension_table: dim_date\n          surrogate_key: date_sk\n          target_column: delivery_date_sk  # Role: delivery date\n</code></pre> <p>Result in fact table:</p> order_id order_date_sk ship_date_sk delivery_date_sk ORD-001 20180315 20180317 20180322 <p>Query with explicit role:</p> <pre><code>SELECT \n  d_order.month_name as order_month,\n  d_ship.month_name as ship_month,\n  COUNT(*) as order_count\nFROM fact_orders f\nJOIN dim_date d_order ON f.order_date_sk = d_order.date_sk\nJOIN dim_date d_ship ON f.ship_date_sk = d_ship.date_sk\nGROUP BY d_order.month_name, d_ship.month_name\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#audit-columns","title":"Audit Columns","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#why-audit","title":"Why Audit?","text":"<p>When something goes wrong: - \"When was this record loaded?\" - \"Where did this data come from?\" - \"Why does this row exist?\"</p> <p>Audit columns answer these questions.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#standard-audit-columns","title":"Standard Audit Columns","text":"Column Type Purpose <code>load_timestamp</code> TIMESTAMP When record was loaded <code>source_system</code> STRING Which system provided data <code>source_file</code> STRING Which file (if file-based) <code>batch_id</code> STRING Which pipeline run <code>record_hash</code> STRING Hash for change detection"},{"location":"marketing/articles/article_12_dimension_patterns/#configuration_1","title":"Configuration","text":"<pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n      batch_id: true\n</code></pre> <p>Generated columns:</p> <pre><code>customer_sk       INT\ncustomer_id       STRING\ncustomer_name     STRING\n...\nload_timestamp    TIMESTAMP    (when loaded)\nsource_system     STRING       ('crm')\nbatch_id          STRING       (pipeline run ID)\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#using-audit-columns","title":"Using Audit Columns","text":"<pre><code>-- Find records loaded today\nSELECT * FROM dim_customer \nWHERE DATE(load_timestamp) = CURRENT_DATE\n\n-- Find records from specific batch\nSELECT * FROM dim_customer \nWHERE batch_id = 'run_20231215_093045'\n\n-- Count records by source\nSELECT source_system, COUNT(*) \nFROM dim_customer \nGROUP BY source_system\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#data-dictionary-column-metadata","title":"Data Dictionary / Column Metadata","text":"<p>Document your dimensions with column metadata:</p> <pre><code>- name: dim_customer\n  columns:\n    customer_sk:\n      description: \"Surrogate key for customer dimension\"\n      tags: [surrogate_key]\n\n    customer_id:\n      description: \"Natural key from CRM system\"\n      tags: [natural_key, business_key]\n\n    customer_email:\n      description: \"Customer email address\"\n      pii: true  # Marks as sensitive\n\n    customer_segment:\n      description: \"Customer value segment (Premium, Standard, Basic)\"\n      tags: [business_attribute]\n</code></pre> <p>This metadata: - Documents the schema - Flags PII for compliance - Enables automated data catalogs - Helps new team members understand the model</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#junk-dimensions","title":"Junk Dimensions","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#the-problem_3","title":"The Problem","text":"<p>Fact table has many low-cardinality flags:</p> <pre><code>is_online_order: true/false\nis_gift: true/false\nis_rush: true/false\npayment_status: pending/approved/declined\n</code></pre> <p>Each flag could be a dimension, but that's excessive.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#the-solution-junk-dimension","title":"The Solution: Junk Dimension","text":"<p>Combine flags into one dimension:</p> junk_sk is_online is_gift is_rush payment_status 1 true false false approved 2 true true false approved 3 false false true pending <p>Fact table stores single <code>junk_sk</code> instead of multiple columns.</p> <pre><code>- name: dim_order_attributes\n  description: \"Junk dimension for order flags\"\n\n  # Generate all combinations\n  transform:\n    steps:\n      - sql: |\n          SELECT DISTINCT\n            is_online_order,\n            is_gift,\n            is_rush_delivery,\n            payment_status\n          FROM silver.orders\n\n  pattern:\n    type: dimension\n    params:\n      natural_key: [is_online_order, is_gift, is_rush_delivery, payment_status]\n      surrogate_key: order_attribute_sk\n      unknown_member: true\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#mini-dimensions","title":"Mini-Dimensions","text":""},{"location":"marketing/articles/article_12_dimension_patterns/#the-problem_4","title":"The Problem","text":"<p>Customer dimension has 10 million rows. A subset of columns changes frequently:</p> <ul> <li><code>customer_segment</code> - changes quarterly</li> <li><code>loyalty_tier</code> - changes monthly</li> <li><code>credit_score_band</code> - changes frequently</li> </ul> <p>SCD2 on 10M rows with frequent changes = explosion.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#the-solution-mini-dimension","title":"The Solution: Mini-Dimension","text":"<p>Extract volatile attributes into separate small dimension:</p> <p>Main dimension (SCD1 or SCD2 on stable attributes): | customer_sk | customer_id | name | address | ... |</p> <p>Mini-dimension (SCD2 on volatile attributes): | customer_demo_sk | segment | loyalty_tier | credit_band |</p> <p>Fact table has both keys: | order_id | customer_sk | customer_demo_sk | ... |</p> <pre><code># Main customer dimension (stable)\n- name: dim_customer\n  pattern:\n    type: dimension\n    params:\n      scd_type: 1\n      track_cols: [customer_name, customer_address]\n\n# Mini-dimension (volatile)\n- name: dim_customer_demographics\n  pattern:\n    type: dimension\n    params:\n      scd_type: 2\n      track_cols: [customer_segment, loyalty_tier, credit_band]\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive dimension configuration:</p> <pre><code>- name: dim_customer\n  description: \"Customer dimension with full pattern implementation\"\n\n  read:\n    connection: silver\n    path: customers\n    format: delta\n\n  # Pre-processing\n  transformer: deduplicate\n  params:\n    keys: [customer_id]\n    order_by: updated_at DESC\n\n  transform:\n    steps:\n      # Standardize\n      - function: clean_text\n        params:\n          columns: [customer_name, customer_city, customer_state]\n          case: upper\n          trim: true\n\n      # Fill NULLs\n      - function: fill_nulls\n        params:\n          columns:\n            customer_segment: \"Unknown\"\n            customer_tier: \"Standard\"\n\n      # Derive attributes\n      - function: derive_columns\n        params:\n          columns:\n            customer_region: |\n              CASE \n                WHEN customer_state IN ('SP', 'RJ') THEN 'Southeast'\n                ELSE 'Other'\n              END\n            customer_name_length: \"LENGTH(customer_name)\"\n\n  # Column documentation\n  columns:\n    customer_sk:\n      description: \"Surrogate key\"\n      tags: [surrogate_key]\n    customer_id:\n      description: \"Natural key from source\"\n      tags: [natural_key]\n    customer_email:\n      description: \"Email address\"\n      pii: true\n    customer_segment:\n      description: \"Value segment\"\n      tags: [business_attribute]\n\n  # Dimension pattern\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols:\n        - customer_city\n        - customer_state\n        - customer_segment\n      target: gold.dim_customer\n      unknown_member:\n        customer_id: \"-1\"\n        customer_name: \"Unknown\"\n        customer_city: \"N/A\"\n        customer_state: \"N/A\"\n        customer_region: \"Unknown\"\n        customer_segment: \"Unknown\"\n      audit:\n        load_timestamp: true\n        source_system: \"crm\"\n\n  write:\n    connection: gold\n    path: dim_customer\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_12_dimension_patterns/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Unknown members prevent join failures - Always add SK=0 row</li> <li>Natural keys for matching, surrogate keys for joining - Use both</li> <li>Conform dimensions across facts - One dimension, multiple consumers</li> <li>Role-playing enables multiple relationships - Same dimension, different contexts</li> <li>Audit columns enable debugging - load_timestamp, source_system, batch_id</li> <li>Document with column metadata - PII flags, descriptions, tags</li> </ol>"},{"location":"marketing/articles/article_12_dimension_patterns/#next-steps","title":"Next Steps","text":"<p>With dimension patterns solid, we'll look at fact table design in more depth:</p> <ul> <li>Grain decisions</li> <li>Measure types (additive, semi-additive, non-additive)</li> <li>Factless facts</li> </ul> <p>Next article: Fact Table Design: Grain, Measures, and Validation.</p>"},{"location":"marketing/articles/article_12_dimension_patterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_13_fact_table_design/","title":"Fact Table Design: Grain, Measures, and Validation","text":"<p>The art of choosing the right level of detail</p>"},{"location":"marketing/articles/article_13_fact_table_design/#tldr","title":"TL;DR","text":"<p>Fact table design starts with grain-the level of detail each row represents. Get it wrong and everything downstream breaks. This article covers grain decisions, measure types (additive, semi-additive, non-additive), factless facts for tracking events without measures, and validation strategies to ensure data integrity.</p>"},{"location":"marketing/articles/article_13_fact_table_design/#grain-the-foundation","title":"Grain: The Foundation","text":"<p>Grain is the answer to: \"What does one row represent?\"</p> Fact Table Grain fact_orders One row per order fact_order_items One row per order line item fact_daily_inventory One row per product per day per warehouse fact_page_views One row per page view event"},{"location":"marketing/articles/article_13_fact_table_design/#why-grain-matters","title":"Why Grain Matters","text":"<p>Wrong grain leads to: - Double-counting: If grain is too coarse, you lose detail - Explosion: If grain is too fine, you have billions of rows - Confusion: Users don't know what numbers mean</p>"},{"location":"marketing/articles/article_13_fact_table_design/#choosing-your-grain","title":"Choosing Your Grain","text":"<p>Ask these questions:</p> <ol> <li>What is the lowest level of detail in the source?</li> <li>If source has line items, that's your natural grain</li> <li> <p>Don't aggregate before the fact table</p> </li> <li> <p>What questions will users ask?</p> </li> <li>\"Total orders by month\" \u2192 Order grain works</li> <li>\"Average items per order\" \u2192 Need line item grain</li> <li> <p>\"Revenue by product\" \u2192 Need line item grain</p> </li> <li> <p>How much data volume can you handle?</p> </li> <li>1 billion rows per day? Consider aggregation</li> <li>10 million per day? Full transaction grain is fine</li> </ol>"},{"location":"marketing/articles/article_13_fact_table_design/#example-order-grain-vs-item-grain","title":"Example: Order Grain vs Item Grain","text":"<p>Order grain (<code>fact_orders</code>): | order_id | customer_sk | order_date_sk | order_total | |----------|-------------|---------------|-------------| | ORD-001 | 42 | 20180315 | 150.00 |</p> <ul> <li>\u2705 Query: \"How many orders per month?\"</li> <li>\u274c Query: \"Revenue by product?\" (can't break down by item)</li> </ul> <p>Item grain (<code>fact_order_items</code>): | order_id | item_id | product_sk | price | |----------|---------|------------|-------| | ORD-001 | 1 | 101 | 75.00 | | ORD-001 | 2 | 102 | 75.00 |</p> <ul> <li>\u2705 Query: \"Revenue by product?\"</li> <li>\u2705 Query: \"How many orders per month?\" (COUNT DISTINCT order_id)</li> </ul> <p>Rule: When in doubt, go with the finer grain. You can always aggregate up.</p>"},{"location":"marketing/articles/article_13_fact_table_design/#grain-declaration","title":"Grain Declaration","text":"<p>In Odibi, declare grain explicitly:</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, order_item_id]  # Explicit grain declaration\n</code></pre> <p>This enables: 1. Documentation: Clear understanding of what each row means 2. Validation: Detect duplicate rows that violate grain 3. Debugging: Easier to track data issues</p>"},{"location":"marketing/articles/article_13_fact_table_design/#grain-validation","title":"Grain Validation","text":"<p>If you declare grain, Odibi validates it:</p> <pre><code>-- Check for duplicates\nSELECT order_id, order_item_id, COUNT(*)\nFROM fact_order_items\nGROUP BY order_id, order_item_id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If duplicates exist, the pipeline fails (or warns, depending on config):</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, order_item_id]\n    grain_validation:\n      on_violation: error  # error | warn | ignore\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#measure-types","title":"Measure Types","text":"<p>Measures are the numbers you aggregate. But not all measures work the same way.</p>"},{"location":"marketing/articles/article_13_fact_table_design/#additive-measures","title":"Additive Measures","text":"<p>Can be summed across all dimensions.</p> Measure Example Why Additive Revenue $100 + $200 = $300 Totals make sense Quantity 5 + 10 = 15 Counts add up Discount Amount $10 + $20 = $30 Dollar amounts add <pre><code>measures:\n  - revenue\n  - quantity\n  - discount_amount\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#semi-additive-measures","title":"Semi-Additive Measures","text":"<p>Can be summed across some dimensions, but not all.</p> Measure Additive Over Non-Additive Over Inventory Count Product, Location Time (snapshot) Account Balance Customer Time (snapshot) Headcount Department Time (snapshot) <p>You can sum inventory across products (total items in warehouse). But summing inventory across time gives nonsense.</p> <pre><code># Inventory fact needs special handling\nmeasures:\n  - inventory_quantity  # Semi-additive\n\n# Query must use MAX/AVG for time\n# SUM(inventory_quantity) across dates = WRONG\n# MAX(inventory_quantity) for latest = RIGHT\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#non-additive-measures","title":"Non-Additive Measures","text":"<p>Cannot be summed across any dimension.</p> Measure Why Non-Additive Use Instead Unit Price Average, not sum AVG, or store revenue + quantity Percentage Can't sum percentages Store numerator + denominator Ratio Can't sum ratios Store components separately <pre><code># \u274c WRONG - storing calculated ratio\nmeasures:\n  - profit_margin_pct: \"profit / revenue * 100\"  # Can't sum!\n\n# \u2705 RIGHT - store components\nmeasures:\n  - profit\n  - revenue\n# Calculate margin in reporting layer\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#best-practice-store-components","title":"Best Practice: Store Components","text":"<p>For any derived metric, store the components:</p> Instead of Store <code>avg_order_value</code> <code>order_total</code>, <code>order_count</code> <code>conversion_rate</code> <code>conversions</code>, <code>visitors</code> <code>profit_margin</code> <code>profit</code>, <code>revenue</code> <code>fill_rate</code> <code>fulfilled_qty</code>, <code>ordered_qty</code> <pre><code>measures:\n  - order_total     # Additive\n  - order_count: \"1\"  # Additive (always 1)\n  - profit          # Additive\n  - revenue         # Additive\n  # Calculate ratios in BI layer\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#factless-facts","title":"Factless Facts","text":""},{"location":"marketing/articles/article_13_fact_table_design/#what-are-factless-facts","title":"What Are Factless Facts?","text":"<p>Fact tables without measures. They track events or relationships.</p>"},{"location":"marketing/articles/article_13_fact_table_design/#type-1-event-tracking","title":"Type 1: Event Tracking","text":"<p>\"Student attended class on this date\"</p> student_sk class_sk date_sk 42 101 20180315 42 102 20180316 <p>No measure columns-the row's existence IS the fact.</p> <p>Query:</p> <pre><code>-- How many classes did each student attend?\nSELECT student_sk, COUNT(*) as classes_attended\nFROM fact_class_attendance\nGROUP BY student_sk\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#type-2-coverageeligibility","title":"Type 2: Coverage/Eligibility","text":"<p>\"Which promotions applied to which products on which dates?\"</p> promotion_sk product_sk date_sk 1 101 20180315 1 101 20180316 2 102 20180315 <p>Query:</p> <pre><code>-- What products were on promotion this week?\nSELECT DISTINCT product_sk\nFROM fact_promotion_coverage\nWHERE date_sk BETWEEN 20180315 AND 20180321\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#configuration","title":"Configuration","text":"<pre><code>- name: fact_class_attendance\n  description: \"Factless fact tracking student attendance\"\n\n  pattern:\n    type: fact\n    params:\n      grain: [student_id, class_id, attendance_date]\n      dimensions:\n        - source_column: student_id\n          dimension_table: dim_student\n          surrogate_key: student_sk\n        - source_column: class_id\n          dimension_table: dim_class\n          surrogate_key: class_sk\n        - source_column: attendance_date\n          dimension_table: dim_date\n          surrogate_key: date_sk\n\n      # No measures - factless!\n      measures: []\n\n      # Add count measure for convenience\n      # measures:\n      #   - attendance_count: \"1\"\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#degenerate-dimensions","title":"Degenerate Dimensions","text":"<p>Some dimension values live directly in the fact table:</p> Concept Example Why Not Separate Dimension? Order ID <code>ORD-12345</code> 1:1 with fact row, no extra attributes Invoice Number <code>INV-9999</code> Just a reference number Transaction ID <code>TXN-ABC</code> Unique per row <p>These are degenerate dimensions-dimension values without a dimension table.</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, order_item_id]\n\n    # Degenerate dimensions stay in fact\n    # No lookup needed\n\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer  # Real dimension\n      - source_column: product_id\n        dimension_table: dim_product   # Real dimension\n\n    # order_id and order_item_id remain as columns\n    # They're part of grain, not looked up\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#fact-table-validation","title":"Fact Table Validation","text":""},{"location":"marketing/articles/article_13_fact_table_design/#1-grain-validation","title":"1. Grain Validation","text":"<p>No duplicates at grain level:</p> <pre><code>-- Must return 0 rows\nSELECT grain_columns, COUNT(*)\nFROM fact_table\nGROUP BY grain_columns\nHAVING COUNT(*) &gt; 1\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#2-referential-integrity","title":"2. Referential Integrity","text":"<p>All foreign keys should resolve:</p> <pre><code>-- Check for orphans\nSELECT COUNT(*)\nFROM fact_order_items f\nWHERE f.customer_sk NOT IN (SELECT customer_sk FROM dim_customer)\n</code></pre> <p>With unknown member pattern, this should return 0 (orphans map to SK=0).</p>"},{"location":"marketing/articles/article_13_fact_table_design/#3-measure-reasonableness","title":"3. Measure Reasonableness","text":"<p>Sanity checks on measures:</p> <pre><code>-- No negative quantities\nSELECT COUNT(*) FROM fact_order_items WHERE quantity &lt; 0\n\n-- No insane prices\nSELECT COUNT(*) FROM fact_order_items WHERE price &gt; 1000000\n\n-- Totals make sense\nSELECT \n  SUM(price) as fact_total,\n  (SELECT SUM(price) FROM silver.order_items) as source_total\nFROM fact_order_items\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#4-date-range-validation","title":"4. Date Range Validation","text":"<p>Facts should fall within expected date range:</p> <pre><code>-- Check date bounds\nSELECT \n  MIN(order_date_sk) as min_date,\n  MAX(order_date_sk) as max_date,\n  COUNT(CASE WHEN order_date_sk = 0 THEN 1 END) as unknown_dates\nFROM fact_order_items\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#configuration_1","title":"Configuration","text":"<pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, order_item_id]\n\n    validation:\n      grain_check: true\n      orphan_check: true\n      measure_checks:\n        - column: price\n          min: 0\n          max: 100000\n        - column: quantity\n          min: 1\n          max: 1000\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#accumulating-snapshot-facts","title":"Accumulating Snapshot Facts","text":"<p>Some facts track a process with multiple milestones:</p> <p>Order Lifecycle: | order_sk | order_date_sk | ship_date_sk | delivery_date_sk | return_date_sk | |----------|---------------|--------------|------------------|----------------| | 1 | 20180315 | 20180317 | 20180322 | NULL |</p> <p>Each milestone gets its own date key. The row is updated as the order progresses.</p> <pre><code>- name: fact_order_lifecycle\n  description: \"Accumulating snapshot of order milestones\"\n\n  pattern:\n    type: fact\n    params:\n      grain: [order_id]\n\n      # Multiple date roles\n      dimensions:\n        - source_column: order_date\n          dimension_table: dim_date\n          target_column: order_date_sk\n        - source_column: ship_date\n          dimension_table: dim_date\n          target_column: ship_date_sk\n        - source_column: delivery_date\n          dimension_table: dim_date\n          target_column: delivery_date_sk\n        - source_column: return_date\n          dimension_table: dim_date\n          target_column: return_date_sk\n\n      # Lag measures\n      measures:\n        - days_to_ship: \"DATEDIFF(ship_date, order_date)\"\n        - days_to_deliver: \"DATEDIFF(delivery_date, order_date)\"\n        - days_ship_to_deliver: \"DATEDIFF(delivery_date, ship_date)\"\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive fact table configuration:</p> <pre><code>- name: fact_order_items\n  description: \"Order line items at transaction grain\"\n\n  depends_on:\n    - silver_order_items\n    - silver_orders\n    - dim_customer\n    - dim_product\n    - dim_seller\n    - dim_date\n\n  # Combine sources\n  inputs:\n    order_items: $silver_ecommerce.silver_order_items\n    orders: $silver_ecommerce.silver_orders\n\n  transform:\n    steps:\n      - sql: |\n          SELECT \n            oi.order_id,\n            oi.order_item_id,\n            oi.product_id,\n            oi.seller_id,\n            o.customer_id,\n            o.order_purchase_date,\n            oi.price,\n            oi.freight_value,\n            oi.price + oi.freight_value as line_total\n          FROM order_items oi\n          JOIN orders o ON oi.order_id = o.order_id\n\n  pattern:\n    type: fact\n    params:\n      # Explicit grain\n      grain: [order_id, order_item_id]\n      grain_validation:\n        on_violation: error\n\n      # Dimension lookups\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          dimension_key: customer_id\n          surrogate_key: customer_sk\n        - source_column: product_id\n          dimension_table: dim_product\n          dimension_key: product_id\n          surrogate_key: product_sk\n        - source_column: seller_id\n          dimension_table: dim_seller\n          dimension_key: seller_id\n          surrogate_key: seller_sk\n        - source_column: order_purchase_date\n          dimension_table: dim_date\n          dimension_key: full_date\n          surrogate_key: date_sk\n          target_column: order_date_sk\n\n      # Orphan handling\n      orphan_handling: unknown\n\n      # Measures (all additive)\n      measures:\n        - item_count: \"1\"  # For counting items\n        - price\n        - freight_value\n        - line_total\n\n      # Validation\n      validation:\n        orphan_check: true\n        measure_checks:\n          - column: price\n            min: 0\n          - column: freight_value\n            min: 0\n\n      # Audit\n      audit:\n        load_timestamp: true\n        source_system: \"ecommerce\"\n\n  write:\n    connection: gold\n    path: fact_order_items\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_13_fact_table_design/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Grain first - Define what each row represents before anything else</li> <li>Finer grain is safer - You can always aggregate up, can't disaggregate down</li> <li>Know your measure types - Additive, semi-additive, non-additive</li> <li>Store components, not ratios - Calculate derived metrics in reporting</li> <li>Factless facts track events - The row existence IS the measure</li> <li>Validate everything - Grain, referential integrity, measure bounds</li> </ol>"},{"location":"marketing/articles/article_13_fact_table_design/#next-steps","title":"Next Steps","text":"<p>With solid fact table design, we'll look at performance optimization:</p> <ul> <li>Pre-aggregation for dashboard speed</li> <li>Aggregation patterns and strategies</li> <li>Choosing the right grain for aggregates</li> </ul> <p>Next article: Pre-Aggregation Strategies for Fast Dashboards.</p>"},{"location":"marketing/articles/article_13_fact_table_design/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_14_pre_aggregation/","title":"Pre-Aggregation Strategies for Fast Dashboards","text":"<p>Trade storage for speed</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#tldr","title":"TL;DR","text":"<p>Dashboards that query billions of rows are slow. Pre-aggregation creates summary tables at coarser grains, trading storage space for query speed. This article covers when to aggregate, how to choose the right grain, incremental aggregation patterns, and the Odibi aggregation pattern for declarative configuration.</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#the-problem-slow-dashboards","title":"The Problem: Slow Dashboards","text":"<p>Your fact table has 500 million rows. The dashboard query:</p> <pre><code>SELECT \n  d.month_name,\n  r.region,\n  SUM(f.revenue)\nFROM fact_sales f\nJOIN dim_date d ON f.date_sk = d.date_sk\nJOIN dim_region r ON f.region_sk = r.region_sk\nWHERE d.year = 2023\nGROUP BY d.month_name, r.region\n</code></pre> <p>Takes 45 seconds. Users give up after 10.</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#the-solution-pre-aggregation","title":"The Solution: Pre-Aggregation","text":"<p>Create a summary table at monthly + region grain:</p> month_sk region_sk total_revenue order_count 202301 1 1,250,000 15,234 202301 2 890,000 11,567 202302 1 1,380,000 16,891 <p>Now the query:</p> <pre><code>SELECT \n  d.month_name,\n  r.region,\n  a.total_revenue\nFROM agg_monthly_sales_by_region a\nJOIN dim_date d ON a.month_sk = d.month_sk\nJOIN dim_region r ON a.region_sk = r.region_sk\nWHERE d.year = 2023\n</code></pre> <p>Scans 24 rows instead of 500 million. Instant.</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#when-to-aggregate","title":"When to Aggregate","text":""},{"location":"marketing/articles/article_14_pre_aggregation/#aggregate-when","title":"Aggregate When:","text":"Scenario Why Dashboard loads slowly Direct query on fact takes too long Same query runs repeatedly Cache the result Known reporting grain \"Monthly by region\" is a standard view Detail not needed Users never drill to transaction level"},{"location":"marketing/articles/article_14_pre_aggregation/#dont-aggregate-when","title":"Don't Aggregate When:","text":"Scenario Why Data changes frequently Aggregate staleness issues Users need detail Can't drill through pre-aggregated data Unknown query patterns You don't know what grain to use Small data volumes 1M rows is fast enough without aggregation"},{"location":"marketing/articles/article_14_pre_aggregation/#choosing-aggregation-grain","title":"Choosing Aggregation Grain","text":"<p>The aggregation grain determines: - Storage: Finer grain = more rows - Flexibility: Finer grain = more query options - Speed: Coarser grain = faster queries</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#common-aggregation-grains","title":"Common Aggregation Grains","text":"Aggregate Table Grain Rows (approx) Daily by product by store day \u00d7 product \u00d7 store 365 \u00d7 10K \u00d7 100 = 365M Daily by product day \u00d7 product 365 \u00d7 10K = 3.65M Monthly by category by region month \u00d7 category \u00d7 region 12 \u00d7 50 \u00d7 10 = 6K Yearly by category year \u00d7 category 5 \u00d7 50 = 250"},{"location":"marketing/articles/article_14_pre_aggregation/#rule-of-thumb","title":"Rule of Thumb","text":"<p>Start with the grain that matches your most common dashboard query. Add more aggregates as needed.</p> <pre><code>Fact Table (500M rows)\n    \u2502\n    \u25bc\nDaily Aggregate (3.65M rows)      \u2190 For daily trend charts\n    \u2502\n    \u25bc\nMonthly Aggregate (36K rows)      \u2190 For monthly reports\n    \u2502\n    \u25bc\nYearly Aggregate (500 rows)       \u2190 For YoY comparison\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#the-aggregation-pattern","title":"The Aggregation Pattern","text":"<p>Odibi provides a declarative aggregation pattern:</p> <pre><code>- name: agg_monthly_sales_by_region\n  description: \"Monthly sales aggregated by region\"\n\n  depends_on: [fact_order_items]\n\n  read:\n    connection: gold\n    path: fact_order_items\n    format: delta\n\n  pattern:\n    type: aggregation\n    params:\n      grain: [month_sk, region_sk]\n\n      measures:\n        - name: total_revenue\n          expr: \"SUM(line_total)\"\n\n        - name: total_orders\n          expr: \"COUNT(DISTINCT order_id)\"\n\n        - name: total_items\n          expr: \"SUM(item_count)\"\n\n        - name: avg_order_value\n          expr: \"SUM(line_total) / COUNT(DISTINCT order_id)\"\n\n      audit:\n        load_timestamp: true\n\n  write:\n    connection: gold\n    path: agg_monthly_sales_by_region\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#how-it-works","title":"How It Works","text":"<ol> <li>Reads from fact table</li> <li>Groups by grain columns</li> <li>Applies aggregate expressions</li> <li>Writes summary table</li> </ol> <p>Generated SQL:</p> <pre><code>SELECT \n  month_sk,\n  region_sk,\n  SUM(line_total) as total_revenue,\n  COUNT(DISTINCT order_id) as total_orders,\n  SUM(item_count) as total_items,\n  SUM(line_total) / COUNT(DISTINCT order_id) as avg_order_value,\n  CURRENT_TIMESTAMP() as load_timestamp\nFROM fact_order_items\nGROUP BY month_sk, region_sk\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#joining-dimensions-for-aggregation","title":"Joining Dimensions for Aggregation","text":"<p>Often you need dimension attributes for grouping:</p> <pre><code>- name: agg_monthly_sales_by_category\n  description: \"Monthly sales by product category\"\n\n  depends_on: \n    - fact_order_items\n    - dim_product\n    - dim_date\n\n  inputs:\n    facts: $gold_facts.fact_order_items\n    products: $gold_dimensions.dim_product\n    dates: $gold_dimensions.dim_date\n\n  transform:\n    steps:\n      # Join to get dimension attributes\n      - sql: |\n          SELECT \n            d.year,\n            d.month,\n            d.month_name,\n            p.product_category_name,\n            f.line_total,\n            f.item_count,\n            f.order_id\n          FROM facts f\n          JOIN products p ON f.product_sk = p.product_sk\n          JOIN dates d ON f.order_date_sk = d.date_sk\n\n  pattern:\n    type: aggregation\n    params:\n      grain: [year, month, product_category_name]\n\n      measures:\n        - name: total_revenue\n          expr: \"SUM(line_total)\"\n        - name: total_items\n          expr: \"SUM(item_count)\"\n        - name: unique_orders\n          expr: \"COUNT(DISTINCT order_id)\"\n\n  write:\n    connection: gold\n    path: agg_monthly_sales_by_category\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#incremental-aggregation","title":"Incremental Aggregation","text":""},{"location":"marketing/articles/article_14_pre_aggregation/#the-problem","title":"The Problem","text":"<p>Full aggregation reprocesses all history every run. For 500M rows, that's expensive.</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#the-solution-incremental-updates","title":"The Solution: Incremental Updates","text":"<p>Only aggregate new/changed data, then merge with existing aggregate.</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk]\n\n    incremental:\n      timestamp_column: load_timestamp\n      lookback: \"7 days\"  # Only process last 7 days\n      merge_strategy: replace  # Replace matching grain rows\n\n    measures:\n      - name: total_revenue\n        expr: \"SUM(line_total)\"\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#merge-strategies","title":"Merge Strategies","text":"Strategy Behavior Use Case <code>replace</code> Replace existing rows for grain Most aggregates <code>sum</code> Add to existing values Cumulative totals (dangerous!) <code>max</code> Keep larger value Latest snapshot"},{"location":"marketing/articles/article_14_pre_aggregation/#how-replace-works","title":"How Replace Works","text":"<pre><code>Existing Aggregate:\n| date_sk  | product_sk | revenue |\n|----------|------------|---------|\n| 20230115 | 101        | 1000    |\n| 20230116 | 101        | 1200    |\n\nNew Data (for 20230116):\n| date_sk  | product_sk | revenue |\n|----------|------------|---------|\n| 20230116 | 101        | 1500    |  \u2190 Corrected value\n\nAfter Merge (replace):\n| date_sk  | product_sk | revenue |\n|----------|------------|---------|\n| 20230115 | 101        | 1000    |  \u2190 Unchanged\n| 20230116 | 101        | 1500    |  \u2190 Replaced\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#having-clause","title":"HAVING Clause","text":"<p>Filter aggregates after grouping:</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [customer_sk]\n\n    measures:\n      - name: total_spend\n        expr: \"SUM(line_total)\"\n      - name: order_count\n        expr: \"COUNT(DISTINCT order_id)\"\n\n    having: \"COUNT(DISTINCT order_id) &gt; 5\"  # Only customers with 5+ orders\n</code></pre> <p>Useful for: - Top N customers - Products with significant volume - Filtering noise from aggregates</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#multiple-aggregation-levels","title":"Multiple Aggregation Levels","text":"<p>Build a hierarchy of aggregates:</p> <pre><code># Level 1: Daily grain\n- name: agg_daily_sales\n  pattern:\n    type: aggregation\n    params:\n      grain: [date_sk, product_sk, store_sk]\n      measures:\n        - name: revenue\n          expr: \"SUM(line_total)\"\n\n# Level 2: Monthly grain (built from daily)\n- name: agg_monthly_sales\n  depends_on: [agg_daily_sales]\n  read:\n    connection: gold\n    path: agg_daily_sales\n  pattern:\n    type: aggregation\n    params:\n      grain: [month_sk, product_sk, store_sk]\n      measures:\n        - name: revenue\n          expr: \"SUM(revenue)\"  # Aggregate from daily\n\n# Level 3: Yearly grain (built from monthly)\n- name: agg_yearly_sales\n  depends_on: [agg_monthly_sales]\n  read:\n    connection: gold\n    path: agg_monthly_sales\n  pattern:\n    type: aggregation\n    params:\n      grain: [year, product_sk, store_sk]\n      measures:\n        - name: revenue\n          expr: \"SUM(revenue)\"  # Aggregate from monthly\n</code></pre> <p>Benefits: - Each level is smaller and faster to compute - Can run incrementally at each level - Clear audit trail</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#aggregate-awareness-in-bi-tools","title":"Aggregate Awareness in BI Tools","text":"<p>Some BI tools automatically route queries to the right aggregate:</p> <pre><code>User asks: \"Revenue by month by region\"\n\u2193\nBI tool checks: agg_monthly_sales_by_region exists?\n\u2193\nYes \u2192 Query aggregate (fast)\nNo \u2192 Query fact table (slow)\n</code></pre> <p>For tools without aggregate awareness, create views:</p> <pre><code>CREATE VIEW v_sales_summary AS\nSELECT \n  d.year,\n  d.month_name,\n  r.region_name,\n  a.total_revenue\nFROM agg_monthly_sales_by_region a\nJOIN dim_date d ON a.month_sk = d.month_sk\nJOIN dim_region r ON a.region_sk = r.region_sk\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#materialized-views-vs-aggregation-tables","title":"Materialized Views vs Aggregation Tables","text":"Approach Pros Cons Aggregation Tables Full control, cross-database Manual refresh Materialized Views Auto-refresh in some DBs Database-specific <p>Odibi uses aggregation tables for portability across Spark, Databricks, Snowflake, etc.</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#complete-example","title":"Complete Example","text":"<p>Here's a full aggregation pipeline:</p> <pre><code>pipelines:\n  - pipeline: gold_aggregates\n    layer: gold\n    description: \"Pre-aggregated summary tables\"\n\n    nodes:\n      # Daily by product and region\n      - name: agg_daily_product_region\n        description: \"Daily sales by product and region\"\n        depends_on: [fact_order_items, dim_product, dim_customer, dim_date]\n\n        inputs:\n          facts: $gold_facts.fact_order_items\n          products: $gold_dimensions.dim_product\n          customers: $gold_dimensions.dim_customer\n          dates: $gold_dimensions.dim_date\n\n        transform:\n          steps:\n            - sql: |\n                SELECT \n                  d.date_sk,\n                  d.year,\n                  d.month,\n                  d.day_of_week,\n                  p.product_category_name,\n                  c.customer_region,\n                  f.line_total,\n                  f.item_count,\n                  f.order_id\n                FROM facts f\n                JOIN products p ON f.product_sk = p.product_sk\n                JOIN customers c ON f.customer_sk = c.customer_sk\n                JOIN dates d ON f.order_date_sk = d.date_sk\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_category_name, customer_region]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: total_items\n                expr: \"SUM(item_count)\"\n              - name: order_count\n                expr: \"COUNT(DISTINCT order_id)\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: gold\n          path: agg_daily_product_region\n          format: delta\n\n      # Monthly summary (from daily)\n      - name: agg_monthly_summary\n        description: \"Monthly sales summary\"\n        depends_on: [agg_daily_product_region]\n\n        read:\n          connection: gold\n          path: agg_daily_product_region\n          format: delta\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [year, month, product_category_name, customer_region]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(total_revenue)\"\n              - name: total_items\n                expr: \"SUM(total_items)\"\n              - name: order_count\n                expr: \"SUM(order_count)\"\n              - name: days_with_sales\n                expr: \"COUNT(DISTINCT date_sk)\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: gold\n          path: agg_monthly_summary\n          format: delta\n</code></pre>"},{"location":"marketing/articles/article_14_pre_aggregation/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Pre-aggregation trades storage for speed - Worth it for slow dashboards</li> <li>Choose grain based on query patterns - Match your most common reports</li> <li>Incremental aggregation saves time - Don't reprocess all history</li> <li>Build hierarchies - Daily \u2192 Monthly \u2192 Yearly</li> <li>Store components, calculate ratios - Aggregates should be additive</li> <li>Consider BI tool integration - Views or aggregate awareness</li> </ol>"},{"location":"marketing/articles/article_14_pre_aggregation/#next-steps","title":"Next Steps","text":"<p>With fast aggregates in place, we'll look at data quality across the entire pipeline:</p> <ul> <li>Contract patterns for each layer</li> <li>Validation strategies</li> <li>Quarantine management</li> </ul> <p>Next article: Data Quality Patterns: Contracts, Validation, and Quarantine.</p>"},{"location":"marketing/articles/article_14_pre_aggregation/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_15_data_quality_patterns/","title":"Data Quality Patterns: Contracts, Validation, and Quarantine","text":"<p>Building trust in your data pipeline</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#tldr","title":"TL;DR","text":"<p>Data quality isn't one thing-it's a system of checks at every layer. Contracts validate input before processing. Validation checks output after transformation. Quarantine routes bad data for review without stopping the pipeline. This article covers the complete data quality framework: what to check at each layer, severity strategies, quarantine management, and quality metrics.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#the-data-quality-framework","title":"The Data Quality Framework","text":"<p>Data quality has three components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     DATA QUALITY                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  CONTRACTS          VALIDATION         QUARANTINE           \u2502\n\u2502  (Pre-conditions)   (Post-conditions)  (Bad data routing)  \u2502\n\u2502                                                              \u2502\n\u2502  \"Is input valid?\"  \"Is output valid?\" \"What do we do with \u2502\n\u2502                                         invalid data?\"       \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#contracts","title":"Contracts","text":"<p>Run before transformation on input data: - \"Does the source have data?\" - \"Are required columns non-null?\" - \"Are values in expected ranges?\"</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#validation","title":"Validation","text":"<p>Run after transformation on output data: - \"Did we produce the right row count?\" - \"Are calculated fields reasonable?\" - \"Did referential integrity hold?\"</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#quarantine","title":"Quarantine","text":"<p>Routes bad records for investigation: - \"What failed the check?\" - \"When did it fail?\" - \"What was the original data?\"</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#layer-by-layer-strategy","title":"Layer-by-Layer Strategy","text":"<p>Different layers need different checks:</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#bronze-layer","title":"Bronze Layer","text":"<p>Minimal checks-preserve everything:</p> <pre><code># Bronze contracts: Just make sure we have data\ncontracts:\n  - type: row_count\n    min: 1\n    severity: error\n    description: \"Source file must not be empty\"\n</code></pre> <p>Why minimal? - Bronze is your safety net - You can't \"fix\" data you didn't keep - Source problems should be visible, not hidden</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#silver-layer","title":"Silver Layer","text":"<p>Heavy validation-this is your filter:</p> <pre><code># Silver: The quality gate\ncontracts:\n  # Structural\n  - type: not_null\n    column: customer_id\n    severity: error\n\n  # Domain\n  - type: accepted_values\n    column: order_status\n    values: [created, shipped, delivered, canceled]\n    severity: error\n\n  # Reasonableness\n  - type: range\n    column: order_total\n    min: 0\n    max: 100000\n    severity: warn\n\n  # Uniqueness\n  - type: unique\n    columns: [order_id]\n    severity: error\n</code></pre> <p>Why heavy? - Silver is Single Source of Truth - Bad data stops here, not in reports - This is where cleaning happens</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#gold-layer","title":"Gold Layer","text":"<p>Business rule validation:</p> <pre><code># Gold: Business logic checks\ncontracts:\n  # Referential integrity\n  - type: custom_sql\n    name: \"all_orders_have_customers\"\n    sql: |\n      SELECT COUNT(*) = 0\n      FROM df f\n      WHERE f.customer_sk NOT IN (SELECT customer_sk FROM dim_customer)\n    severity: error\n\n  # Grain validation\n  - type: unique\n    columns: [order_id, order_item_id]\n    severity: error\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#contract-types-reference","title":"Contract Types Reference","text":""},{"location":"marketing/articles/article_15_data_quality_patterns/#structural-contracts","title":"Structural Contracts","text":"<p>Check data shape:</p> Contract Purpose Example <code>not_null</code> Column has values PK columns <code>unique</code> No duplicates Natural keys <code>row_count</code> Expected volume Min/max bounds <code>schema</code> Expected columns Required fields <pre><code>contracts:\n  - type: not_null\n    column: order_id\n\n  - type: unique\n    columns: [order_id]\n\n  - type: row_count\n    min: 1000\n    max: 10000000\n\n  - type: schema\n    required_columns: [order_id, customer_id, order_date]\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#domain-contracts","title":"Domain Contracts","text":"<p>Check business rules:</p> Contract Purpose Example <code>accepted_values</code> Valid categories Status codes <code>range</code> Numeric bounds Prices, quantities <code>regex_match</code> String patterns Email, phone <code>custom_sql</code> Complex logic Business rules <pre><code>contracts:\n  - type: accepted_values\n    column: payment_type\n    values: [credit_card, debit_card, cash, voucher]\n\n  - type: range\n    column: discount_pct\n    min: 0\n    max: 100\n\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n\n  - type: custom_sql\n    name: \"order_date_not_future\"\n    sql: \"SELECT COUNT(*) = 0 FROM df WHERE order_date &gt; CURRENT_DATE()\"\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#freshness-contracts","title":"Freshness Contracts","text":"<p>Check data timeliness:</p> <pre><code>contracts:\n  - type: freshness\n    column: _extracted_at\n    max_age: \"24 hours\"\n    severity: warn\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#volume-change-contracts","title":"Volume Change Contracts","text":"<p>Detect anomalies:</p> <pre><code>contracts:\n  - type: volume_drop\n    baseline: \"7 day average\"\n    threshold: 0.5  # Alert if &lt; 50% of baseline\n    severity: warn\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#severity-strategies","title":"Severity Strategies","text":"<p>Choose severity based on impact:</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#error-pipeline-fails","title":"Error (Pipeline Fails)","text":"<p>Use for: - Primary key violations - Null critical fields - Invalid data that would corrupt downstream</p> <pre><code>- type: not_null\n  column: order_id\n  severity: error  # Can't proceed without this\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#warn-log-and-continue","title":"Warn (Log and Continue)","text":"<p>Use for: - Data quality issues to monitor - Edge cases that don't break processing - Metrics you want to track</p> <pre><code>- type: range\n  column: order_total\n  max: 50000\n  severity: warn  # Unusual but not impossible\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#quarantine-route-bad-data","title":"Quarantine (Route Bad Data)","text":"<p>Use for: - Keep pipeline running - Isolate problems for investigation - Process good data, review bad data later</p> <pre><code>- type: not_null\n  column: customer_id\n  severity: quarantine  # Route to quarantine table\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#quarantine-pattern","title":"Quarantine Pattern","text":""},{"location":"marketing/articles/article_15_data_quality_patterns/#configuration","title":"Configuration","text":"<pre><code>- name: silver_orders\n  contracts:\n    - type: not_null\n      column: order_id\n      severity: error  # Critical - fail pipeline\n\n    - type: not_null\n      column: customer_id\n      severity: quarantine  # Route to quarantine\n\n    - type: accepted_values\n      column: order_status\n      values: [created, shipped, delivered]\n      severity: quarantine  # Route to quarantine\n\n  validation:\n    quarantine:\n      connection: silver\n      path: orders_quarantine\n      add_columns:\n        _rejection_reason: true\n        _rejected_at: true\n        _source_contract: true\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#quarantine-table-schema","title":"Quarantine Table Schema","text":"Column Type Description (all source columns) varies Original data <code>_rejection_reason</code> STRING Which contract failed <code>_rejected_at</code> TIMESTAMP When it was quarantined <code>_source_contract</code> STRING Contract name <code>_batch_id</code> STRING Pipeline run ID"},{"location":"marketing/articles/article_15_data_quality_patterns/#example-quarantine-data","title":"Example Quarantine Data","text":"order_id customer_id order_status _rejection_reason _rejected_at ORD-123 NULL created not_null: customer_id 2023-12-15 09:30:00 ORD-456 CUST-789 pending accepted_values: order_status 2023-12-15 09:30:00"},{"location":"marketing/articles/article_15_data_quality_patterns/#quarantine-management","title":"Quarantine Management","text":""},{"location":"marketing/articles/article_15_data_quality_patterns/#daily-review-query","title":"Daily Review Query","text":"<pre><code>-- Quarantine summary by reason\nSELECT \n  DATE(_rejected_at) as rejection_date,\n  _rejection_reason,\n  COUNT(*) as rejected_count\nFROM silver.orders_quarantine\nWHERE _rejected_at &gt;= CURRENT_DATE - 7\nGROUP BY 1, 2\nORDER BY 1 DESC, 3 DESC\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#trend-analysis","title":"Trend Analysis","text":"<pre><code>-- Quarantine rate over time\nSELECT \n  DATE(load_timestamp) as load_date,\n  COUNT(*) as total_loaded,\n  (SELECT COUNT(*) FROM quarantine q \n   WHERE DATE(q._rejected_at) = DATE(m.load_timestamp)) as quarantined,\n  ROUND(100.0 * quarantined / total_loaded, 2) as quarantine_rate_pct\nFROM silver.orders m\nGROUP BY load_date\nORDER BY load_date DESC\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#reprocessing-quarantined-data","title":"Reprocessing Quarantined Data","text":"<p>After fixing upstream issues:</p> <pre><code># Read quarantine\nquarantined = spark.read.delta(\"silver/orders_quarantine\")\n\n# Filter for specific issue (now fixed)\nto_reprocess = quarantined.filter(\n    F.col(\"_rejection_reason\") == \"not_null: customer_id\"\n)\n\n# Remove quarantine columns\nclean_data = to_reprocess.drop(\"_rejection_reason\", \"_rejected_at\", \"_source_contract\")\n\n# Rerun through pipeline\n# ...\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#retention-policy","title":"Retention Policy","text":"<p>Don't keep quarantine data forever:</p> <pre><code>validation:\n  quarantine:\n    connection: silver\n    path: orders_quarantine\n    retention_days: 90  # Auto-cleanup after 90 days\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#quality-metrics","title":"Quality Metrics","text":"<p>Track data quality over time:</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#core-metrics","title":"Core Metrics","text":"Metric Formula Target Completeness (Non-null values / Total values) \u00d7 100 &gt; 99% Uniqueness (Distinct values / Total values) \u00d7 100 100% for PKs Validity (Valid values / Total values) \u00d7 100 &gt; 99% Quarantine Rate (Quarantined / Total) \u00d7 100 &lt; 1%"},{"location":"marketing/articles/article_15_data_quality_patterns/#metrics-table","title":"Metrics Table","text":"<pre><code>system:\n  connection: bronze\n  path: _system\n\n  metrics:\n    enabled: true\n    table: data_quality_metrics\n    retention_days: 365\n</code></pre> <p>Generated metrics:</p> metric_date pipeline node metric_name metric_value 2023-12-15 silver orders row_count 99441 2023-12-15 silver orders null_rate_customer_id 0.02 2023-12-15 silver orders quarantine_count 23"},{"location":"marketing/articles/article_15_data_quality_patterns/#dashboard-query","title":"Dashboard Query","text":"<pre><code>-- Quality dashboard data\nSELECT \n  metric_date,\n  node,\n  SUM(CASE WHEN metric_name = 'row_count' THEN metric_value END) as rows_loaded,\n  SUM(CASE WHEN metric_name = 'quarantine_count' THEN metric_value END) as quarantined,\n  ROUND(100.0 * quarantine_count / rows_loaded, 2) as quarantine_pct\nFROM system.data_quality_metrics\nWHERE metric_date &gt;= CURRENT_DATE - 30\nGROUP BY metric_date, node\nORDER BY metric_date DESC\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#testing-contracts","title":"Testing Contracts","text":"<p>Create test cases for your contracts:</p> <pre><code># test_contracts.py\nimport pytest\nfrom odibi import Pipeline\n\ndef test_null_customer_fails():\n    \"\"\"Null customer_id should fail contract\"\"\"\n    bad_data = pd.DataFrame({\n        'order_id': ['ORD-001'],\n        'customer_id': [None],\n        'order_status': ['created']\n    })\n\n    result = Pipeline.run_node(\n        'silver_orders',\n        input_df=bad_data,\n        dry_run=True\n    )\n\n    assert result.contracts_passed == False\n    assert 'not_null' in result.failed_contracts[0].type\n\ndef test_invalid_status_quarantines():\n    \"\"\"Invalid status should route to quarantine\"\"\"\n    bad_data = pd.DataFrame({\n        'order_id': ['ORD-001'],\n        'customer_id': ['CUST-001'],\n        'order_status': ['invalid_status']\n    })\n\n    result = Pipeline.run_node(\n        'silver_orders',\n        input_df=bad_data\n    )\n\n    assert result.quarantine_count == 1\n    assert result.output_count == 0\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive data quality configuration:</p> <pre><code>- name: silver_orders\n  description: \"Validated orders with full quality checks\"\n\n  read:\n    connection: bronze\n    path: orders\n    format: delta\n\n  # Pre-processing contracts\n  contracts:\n    # Structural - fail on violations\n    - type: row_count\n      min: 1\n      severity: error\n      description: \"Source must have data\"\n\n    - type: not_null\n      column: order_id\n      severity: error\n      description: \"Order ID is required\"\n\n    - type: unique\n      columns: [order_id]\n      severity: error\n      description: \"Order IDs must be unique\"\n\n    # Domain - quarantine violations\n    - type: not_null\n      column: customer_id\n      severity: quarantine\n      description: \"Orders need customers\"\n\n    - type: accepted_values\n      column: order_status\n      values: [created, approved, shipped, delivered, canceled]\n      severity: quarantine\n      description: \"Status must be valid\"\n\n    # Reasonableness - warn on violations\n    - type: freshness\n      column: order_purchase_timestamp\n      max_age: \"30 days\"\n      severity: warn\n      description: \"Data should be recent\"\n\n    - type: volume_drop\n      baseline: \"7 day average\"\n      threshold: 0.5\n      severity: warn\n      description: \"Alert on volume drop\"\n\n  # Transformations\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: _extracted_at DESC\n\n  transform:\n    steps:\n      - function: clean_text\n        params:\n          columns: [order_status]\n          case: lower\n\n  # Post-processing validation\n  validation:\n    # Output checks\n    checks:\n      - type: row_count_comparison\n        source: input\n        threshold: 0.95  # Output should be &gt;= 95% of input\n        severity: warn\n\n    # Quarantine configuration\n    quarantine:\n      connection: silver\n      path: orders_quarantine\n      add_columns:\n        _rejection_reason: true\n        _rejected_at: true\n        _source_contract: true\n        _batch_id: true\n      retention_days: 90\n\n  write:\n    connection: silver\n    path: orders\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#best-practices","title":"Best Practices","text":""},{"location":"marketing/articles/article_15_data_quality_patterns/#1-start-strict-loosen-as-needed","title":"1. Start Strict, Loosen as Needed","text":"<p>Begin with <code>error</code> severity, move to <code>warn</code> or <code>quarantine</code> for known edge cases.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#2-document-every-contract","title":"2. Document Every Contract","text":"<pre><code>- type: not_null\n  column: customer_id\n  description: \"Orders must have customers for dimension lookups\"\n  owner: \"data-team@company.com\"\n</code></pre>"},{"location":"marketing/articles/article_15_data_quality_patterns/#3-alert-on-trends-not-incidents","title":"3. Alert on Trends, Not Incidents","text":"<p>One bad record isn't news. A 10x spike in quarantine rate is.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#4-review-quarantine-weekly","title":"4. Review Quarantine Weekly","text":"<p>Don't just collect bad data-investigate patterns and fix upstream.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#5-track-quality-over-time","title":"5. Track Quality Over Time","text":"<p>Dashboards showing quality trends catch degradation early.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#6-test-your-contracts","title":"6. Test Your Contracts","text":"<p>Write unit tests that verify contracts catch bad data.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Contracts validate input - Before transformation</li> <li>Validation checks output - After transformation</li> <li>Quarantine keeps pipelines running - Route bad data, process good data</li> <li>Layer strategy matters - Light on Bronze, heavy on Silver</li> <li>Metrics enable improvement - Track quality over time</li> <li>Test your quality checks - Verify they catch real issues</li> </ol>"},{"location":"marketing/articles/article_15_data_quality_patterns/#next-steps","title":"Next Steps","text":"<p>With data quality solid, we'll cover incremental loading:</p> <ul> <li>Watermarks and high-water marks</li> <li>Merge strategies</li> <li>Append vs overwrite patterns</li> </ul> <p>Next article: Incremental Loading: Watermarks, Merge, and Append.</p>"},{"location":"marketing/articles/article_15_data_quality_patterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_16_incremental_loading/","title":"Incremental Loading: Watermarks, Merge, and Append","text":"<p>Stop reprocessing your entire history every day</p>"},{"location":"marketing/articles/article_16_incremental_loading/#tldr","title":"TL;DR","text":"<p>Full loads reprocess everything. Incremental loads process only what's new or changed. This article covers the three incremental patterns: append (add new rows), watermark (filter by timestamp), and merge (upsert changed records). You'll learn when to use each, how to configure them in Odibi, and how to handle late-arriving data.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#the-problem-full-loads-dont-scale","title":"The Problem: Full Loads Don't Scale","text":"<p>You have 1 billion rows in your fact table. Every day, 1 million new records arrive.</p> <p>Full load approach: - Read 1 billion rows - Transform 1 billion rows - Write 1 billion rows - Time: 4 hours</p> <p>Incremental approach: - Read 1 million new rows - Transform 1 million new rows - Append 1 million rows - Time: 10 minutes</p> <p>Same result, 24x faster.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#three-incremental-patterns","title":"Three Incremental Patterns","text":"Pattern How It Works Use Case Append Add new rows only Immutable event data Watermark Filter by timestamp New records with monotonic timestamp Merge (Upsert) Update existing + insert new Changing records with business key"},{"location":"marketing/articles/article_16_incremental_loading/#pattern-1-append","title":"Pattern 1: Append","text":"<p>The simplest pattern. Just add new rows.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#when-to-use","title":"When to Use","text":"<ul> <li>Event logs (clicks, transactions, sensor readings)</li> <li>Facts that never change</li> <li>Source provides only new records</li> </ul>"},{"location":"marketing/articles/article_16_incremental_loading/#configuration","title":"Configuration","text":"<pre><code>- name: bronze_events\n  description: \"Append new events\"\n\n  read:\n    connection: landing\n    path: events/*.parquet\n    format: parquet\n\n  write:\n    connection: bronze\n    path: events\n    format: delta\n    mode: append  # Key setting\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#how-it-works","title":"How It Works","text":"<pre><code>Before:\n| event_id | timestamp           | value |\n|----------|---------------------|-------|\n| 1        | 2023-12-14 10:00:00 | 100   |\n| 2        | 2023-12-14 11:00:00 | 200   |\n\nNew Data:\n| event_id | timestamp           | value |\n|----------|---------------------|-------|\n| 3        | 2023-12-15 10:00:00 | 150   |\n\nAfter (Append):\n| event_id | timestamp           | value |\n|----------|---------------------|-------|\n| 1        | 2023-12-14 10:00:00 | 100   |\n| 2        | 2023-12-14 11:00:00 | 200   |\n| 3        | 2023-12-15 10:00:00 | 150   |  \u2190 Added\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#gotcha-duplicate-prevention","title":"Gotcha: Duplicate Prevention","text":"<p>If the source resends data, append creates duplicates.</p> <p>Solution 1: Deduplicate after append</p> <pre><code>- name: bronze_events\n  write:\n    mode: append\n\n- name: silver_events\n  depends_on: [bronze_events]\n  transformer: deduplicate\n  params:\n    keys: [event_id]\n    order_by: _extracted_at DESC\n</code></pre> <p>Solution 2: Merge instead of append (if source resends frequently)</p>"},{"location":"marketing/articles/article_16_incremental_loading/#pattern-2-watermark-high-water-mark","title":"Pattern 2: Watermark (High-Water Mark)","text":"<p>Track the maximum timestamp processed, only read newer records.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#when-to-use_1","title":"When to Use","text":"<ul> <li>Source has monotonically increasing timestamp</li> <li>You want to filter at read time (efficiency)</li> <li>Source is large, but new data is small</li> </ul>"},{"location":"marketing/articles/article_16_incremental_loading/#configuration_1","title":"Configuration","text":"<pre><code>- name: silver_orders\n  description: \"Incremental load using watermark\"\n\n  read:\n    connection: bronze\n    path: orders\n    format: delta\n    incremental:\n      mode: stateful       # Remember last high-water mark\n      column: order_date   # Timestamp column to track\n      lookback: \"2 days\"   # Safety buffer for late data\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#how-it-works_1","title":"How It Works","text":"<pre><code>Run 1 (First Load):\n- Target doesn't exist\n- Read ALL data (ignore watermark)\n- Write all rows\n- Save HWM = MAX(order_date) = 2023-12-14\n\nRun 2 (Incremental):\n- Read HWM = 2023-12-14\n- Filter: order_date &gt; 2023-12-14 - 2 days\n- Process only filtered rows\n- Update HWM = MAX(order_date) from new data\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#the-lookback-buffer","title":"The Lookback Buffer","text":"<p>Why <code>lookback: \"2 days\"</code>?</p> <p>Data can arrive late. An order from December 13 might arrive on December 15.</p> <p>Without lookback: - HWM = 2023-12-14 - Filter: order_date &gt; 2023-12-14 - Late order (2023-12-13) is missed!</p> <p>With <code>lookback: \"2 days\"</code>: - Filter: order_date &gt; 2023-12-12 - Late order is captured - Merge handles duplicates</p>"},{"location":"marketing/articles/article_16_incremental_loading/#stateful-vs-rolling","title":"Stateful vs Rolling","text":"Mode Behavior Use Case Stateful Track HWM in system table Production pipelines Rolling Always use NOW() - lookback Simple, no state <pre><code># Stateful (recommended)\nincremental:\n  mode: stateful\n  column: updated_at\n\n# Rolling (simpler)\nincremental:\n  mode: rolling\n  column: updated_at\n  lookback: \"7 days\"  # Always process last 7 days\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#pattern-3-merge-upsert","title":"Pattern 3: Merge (Upsert)","text":"<p>Update existing rows, insert new ones. The most flexible pattern.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#when-to-use_2","title":"When to Use","text":"<ul> <li>Source data can change (updates, corrections)</li> <li>You need exactly one row per business key</li> <li>SCD1 dimensions</li> </ul>"},{"location":"marketing/articles/article_16_incremental_loading/#configuration_2","title":"Configuration","text":"<pre><code>- name: silver_customers\n  description: \"Merge customer updates\"\n\n  read:\n    connection: bronze\n    path: customers\n    format: delta\n\n  transformer: merge\n  params:\n    target: silver.customers\n    keys: [customer_id]  # Match on business key\n\n    # Optional: Only merge if something changed\n    condition: |\n      source.updated_at &gt; target.updated_at\n\n    # What to do on match\n    when_matched: update\n\n    # What to do on no match\n    when_not_matched: insert\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#how-it-works_2","title":"How It Works","text":"<pre><code>Target (Before):\n| customer_id | name      | city       | updated_at |\n|-------------|-----------|------------|------------|\n| CUST-001    | John      | S\u00e3o Paulo  | 2023-12-01 |\n| CUST-002    | Maria     | Rio        | 2023-12-01 |\n\nSource (New Data):\n| customer_id | name      | city       | updated_at |\n|-------------|-----------|------------|------------|\n| CUST-001    | John      | Bras\u00edlia   | 2023-12-15 |  \u2190 Changed\n| CUST-003    | Pedro     | Salvador   | 2023-12-15 |  \u2190 New\n\nTarget (After Merge):\n| customer_id | name      | city       | updated_at |\n|-------------|-----------|------------|------------|\n| CUST-001    | John      | Bras\u00edlia   | 2023-12-15 |  \u2190 Updated\n| CUST-002    | Maria     | Rio        | 2023-12-01 |  \u2190 Unchanged\n| CUST-003    | Pedro     | Salvador   | 2023-12-15 |  \u2190 Inserted\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#merge-strategies","title":"Merge Strategies","text":"<pre><code># Update all columns on match\nwhen_matched: update\n\n# Update only specific columns\nwhen_matched:\n  update:\n    columns: [name, city, updated_at]\n\n# Update only if condition met\nwhen_matched:\n  update:\n    condition: \"source.updated_at &gt; target.updated_at\"\n\n# Delete matching rows\nwhen_matched: delete\n\n# Do nothing (keep target)\nwhen_matched: ignore\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#soft-deletes","title":"Soft Deletes","text":"<p>Detect and mark deleted records:</p> <pre><code>transformer: merge\nparams:\n  target: silver.customers\n  keys: [customer_id]\n\n  when_matched: update\n  when_not_matched: insert\n\n  # Mark records in target that aren't in source\n  when_not_matched_by_source:\n    update:\n      columns:\n        is_deleted: true\n        deleted_at: \"current_timestamp()\"\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#combining-patterns","title":"Combining Patterns","text":"<p>Often you need multiple patterns together:</p>"},{"location":"marketing/articles/article_16_incremental_loading/#watermark-merge","title":"Watermark + Merge","text":"<pre><code>- name: silver_orders\n  description: \"Incremental watermark with merge for updates\"\n\n  read:\n    connection: bronze\n    path: orders\n    format: delta\n    incremental:\n      mode: stateful\n      column: updated_at\n      lookback: \"3 days\"\n\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    when_matched:\n      update:\n        condition: \"source.updated_at &gt; target.updated_at\"\n    when_not_matched: insert\n</code></pre> <p>This: 1. Reads only recent records (watermark) 2. Updates existing orders that changed (merge) 3. Inserts new orders (merge)</p>"},{"location":"marketing/articles/article_16_incremental_loading/#watermark-append-dedupe","title":"Watermark + Append + Dedupe","text":"<pre><code># Bronze: Append everything\n- name: bronze_events\n  read:\n    connection: landing\n    path: events/\n  write:\n    mode: append\n\n# Silver: Deduplicate\n- name: silver_events\n  read:\n    connection: bronze\n    path: events\n    incremental:\n      mode: stateful\n      column: _extracted_at\n      lookback: \"1 day\"\n\n  transformer: deduplicate\n  params:\n    keys: [event_id]\n    order_by: event_timestamp DESC\n\n  transformer: merge\n  params:\n    target: silver.events\n    keys: [event_id]\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#first-run-vs-subsequent-runs","title":"First Run vs Subsequent Runs","text":"<p>Odibi handles the first run automatically:</p>"},{"location":"marketing/articles/article_16_incremental_loading/#first-run-target-doesnt-exist","title":"First Run (Target Doesn't Exist)","text":"<pre><code>read:\n  incremental:\n    mode: stateful\n    column: order_date\n    lookback: \"2 days\"\n</code></pre> <p>Behavior: 1. Target table doesn't exist 2. Ignore incremental filter 3. Read ALL source data 4. Create target table 5. Initialize HWM</p>"},{"location":"marketing/articles/article_16_incremental_loading/#first-run-override","title":"First Run Override","text":"<p>For large historical data, limit the first run:</p> <pre><code>read:\n  incremental:\n    mode: stateful\n    column: order_date\n    lookback: \"2 days\"\n    first_run_query: |\n      SELECT * FROM source\n      WHERE order_date &gt;= '2023-01-01'  # Only load 1 year\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#late-arriving-data","title":"Late-Arriving Data","text":"<p>Data arrives late when: - Batch files are delayed - Source systems have sync lag - Time zone issues</p>"},{"location":"marketing/articles/article_16_incremental_loading/#strategy-1-lookback-window","title":"Strategy 1: Lookback Window","text":"<pre><code>incremental:\n  lookback: \"3 days\"  # Reprocess last 3 days every run\n</code></pre> <p>Pros: Simple, catches late data Cons: Reprocesses data unnecessarily</p>"},{"location":"marketing/articles/article_16_incremental_loading/#strategy-2-restatement-table","title":"Strategy 2: Restatement Table","text":"<p>Track late arrivals explicitly:</p> <pre><code>- name: detect_late_arrivals\n  description: \"Find records that arrived late\"\n\n  transform:\n    steps:\n      - sql: |\n          SELECT *\n          FROM bronze.orders\n          WHERE _extracted_at &gt; order_date + INTERVAL '2 days'\n\n  write:\n    connection: silver\n    path: late_arrivals_log\n    mode: append\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#strategy-3-partition-based-reprocessing","title":"Strategy 3: Partition-Based Reprocessing","text":"<pre><code>read:\n  incremental:\n    mode: stateful\n    column: order_date\n    partition_column: order_date  # Reprocess entire partition\n</code></pre> <p>When late data arrives for Dec 13, reprocess the entire Dec 13 partition.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#state-management","title":"State Management","text":"<p>Watermark state is stored in the system catalog:</p> <pre><code>system:\n  connection: bronze\n  path: _system\n</code></pre> <p>State table structure:</p> pipeline node state_key state_value updated_at silver orders high_water_mark 2023-12-15 2023-12-15 09:30:00 silver customers high_water_mark 2023-12-15 2023-12-15 09:35:00"},{"location":"marketing/articles/article_16_incremental_loading/#resetting-state","title":"Resetting State","text":"<p>Force a full reload by resetting the watermark:</p> <pre><code># Reset watermark\nodibi reset-state --pipeline silver --node orders\n\n# Or delete from state table\nspark.sql(\"\"\"\n  DELETE FROM system._state\n  WHERE pipeline = 'silver' AND node = 'orders'\n\"\"\")\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#performance-optimization","title":"Performance Optimization","text":""},{"location":"marketing/articles/article_16_incremental_loading/#partition-pruning","title":"Partition Pruning","text":"<p>Structure data to enable partition pruning:</p> <pre><code>write:\n  connection: silver\n  path: orders\n  format: delta\n  partition_by: [order_year, order_month]\n</code></pre> <p>Incremental reads then only scan relevant partitions.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#z-ordering-delta-lake","title":"Z-Ordering (Delta Lake)","text":"<p>Optimize for common filter columns:</p> <pre><code>write:\n  connection: silver\n  path: orders\n  format: delta\n  z_order_by: [customer_id, order_date]\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#skip-full-scans","title":"Skip Full Scans","text":"<p>Use file statistics to skip unnecessary files:</p> <pre><code>read:\n  connection: bronze\n  path: orders\n  format: delta\n  incremental:\n    use_file_stats: true  # Skip files without matching data\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#complete-example","title":"Complete Example","text":"<p>Here's a production-ready incremental pipeline:</p> <pre><code>pipelines:\n  - pipeline: silver_incremental\n    layer: silver\n    description: \"Incremental silver layer processing\"\n\n    nodes:\n      # Append-only events\n      - name: silver_page_views\n        description: \"Append new page view events\"\n        read:\n          connection: bronze\n          path: page_views\n          format: delta\n          incremental:\n            mode: stateful\n            column: event_timestamp\n            lookback: \"1 hour\"\n        write:\n          connection: silver\n          path: page_views\n          format: delta\n          mode: append\n          partition_by: [event_date]\n\n      # Merge with updates\n      - name: silver_customers\n        description: \"Merge customer updates\"\n        read:\n          connection: bronze\n          path: customers\n          format: delta\n          incremental:\n            mode: stateful\n            column: updated_at\n            lookback: \"2 days\"\n        transformer: merge\n        params:\n          target: silver.customers\n          keys: [customer_id]\n          when_matched:\n            update:\n              condition: \"source.updated_at &gt; target.updated_at\"\n          when_not_matched: insert\n        write:\n          connection: silver\n          path: customers\n          format: delta\n\n      # Watermark with dedup\n      - name: silver_orders\n        description: \"Incremental orders with deduplication\"\n        read:\n          connection: bronze\n          path: orders\n          format: delta\n          incremental:\n            mode: stateful\n            column: _extracted_at\n            lookback: \"3 days\"\n        transformer: deduplicate\n        params:\n          keys: [order_id]\n          order_by: _extracted_at DESC\n        transformer: merge\n        params:\n          target: silver.orders\n          keys: [order_id]\n          when_matched:\n            update:\n              condition: \"source._extracted_at &gt; target._extracted_at\"\n          when_not_matched: insert\n        write:\n          connection: silver\n          path: orders\n          format: delta\n          partition_by: [order_year, order_month]\n</code></pre>"},{"location":"marketing/articles/article_16_incremental_loading/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Append for immutable data - Events, logs, transactions</li> <li>Watermark for efficient filtering - Read only new data</li> <li>Merge for changing data - Update + insert in one operation</li> <li>Lookback handles late data - Safety buffer for stragglers</li> <li>Combine patterns as needed - Watermark + Merge is common</li> <li>Monitor state - Know your high-water marks</li> </ol>"},{"location":"marketing/articles/article_16_incremental_loading/#next-steps","title":"Next Steps","text":"<p>We've covered the core patterns. Now we'll explore anti-patterns and troubleshooting:</p> <ul> <li>Common mistakes in Bronze, Silver, Gold layers</li> <li>Performance anti-patterns</li> <li>Debugging strategies</li> </ul> <p>Next article: Bronze Layer Anti-Patterns: 3 Mistakes That Will Haunt You.</p>"},{"location":"marketing/articles/article_16_incremental_loading/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_17_bronze_antipatterns/","title":"Bronze Layer Anti-Patterns: 3 Mistakes That Will Haunt You","text":"<p>How to ruin your data lake before it starts</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#tldr","title":"TL;DR","text":"<p>The Bronze layer is your safety net-the one place you can always go back to. But three common mistakes destroy this safety: transforming data before landing, overwriting instead of appending, and skipping metadata. This article shows what goes wrong and how to fix it.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#what-bronze-is-supposed-to-do","title":"What Bronze Is Supposed to Do","text":"<p>Bronze has one job: preserve the source data exactly as received.</p> <p>That's it. No cleaning. No transforming. No filtering. Just land it and add metadata.</p> <p>When you mess this up, you lose your ability to recover from mistakes made in Silver and Gold.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#anti-pattern-1-transforming-in-bronze","title":"Anti-Pattern #1: Transforming in Bronze","text":""},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-mistake","title":"The Mistake","text":"<p>\"Let's just clean up the dates while we're loading...\"</p> <pre><code># \u274c WRONG: Transforming in Bronze\n- name: bronze_orders\n  read:\n    connection: landing\n    path: orders.csv\n\n  transform:\n    steps:\n      # NO! This is Silver's job!\n      - function: derive_columns\n        params:\n          columns:\n            order_date: \"TO_DATE(order_timestamp)\"\n      - sql: \"SELECT * FROM df WHERE order_status != 'test'\"\n\n  write:\n    connection: bronze\n    path: orders\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#why-its-wrong","title":"Why It's Wrong","text":"<p>You discarded test orders. Six months later, you discover \"test\" orders were actually legitimate orders from a test environment that went to production.</p> <p>Those orders are gone. Forever.</p> <p>You converted timestamps to dates. Later you realize you needed the time component for delivery SLA calculations.</p> <p>That precision is gone. Forever.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-correct-pattern","title":"The Correct Pattern","text":"<pre><code># \u2705 RIGHT: Bronze preserves everything\n- name: bronze_orders\n  read:\n    connection: landing\n    path: orders.csv\n\n  transform:\n    steps:\n      # ONLY metadata - nothing from source changes\n      - function: derive_columns\n        params:\n          columns:\n            _extracted_at: \"current_timestamp()\"\n            _source_file: \"'orders.csv'\"\n            _batch_id: \"'run_20231215_0900'\"\n\n  write:\n    connection: bronze\n    path: orders\n    mode: append\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-rule","title":"The Rule","text":"<p>In Bronze, you may only add columns. Never change or remove source columns.</p> Allowed in Bronze Not Allowed in Bronze <code>_extracted_at</code> <code>UPPER(customer_name)</code> <code>_source_file</code> <code>TO_DATE(timestamp)</code> <code>_batch_id</code> <code>WHERE status != 'test'</code>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#anti-pattern-2-overwriting-instead-of-appending","title":"Anti-Pattern #2: Overwriting Instead of Appending","text":""},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-mistake_1","title":"The Mistake","text":"<p>\"We don't need history, just keep the latest data...\"</p> <pre><code># \u274c WRONG: Overwriting Bronze\n- name: bronze_orders\n  read:\n    connection: landing\n    path: orders.csv\n\n  write:\n    connection: bronze\n    path: orders\n    mode: overwrite  # DANGER!\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#why-its-wrong_1","title":"Why It's Wrong","text":"<p>Monday: Source sends 100,000 orders Tuesday: Source sends 95,000 orders (5,000 missing due to source bug) Wednesday: You notice revenue is down</p> <p>With <code>overwrite</code>, Monday's data is gone. You can't: - Identify the 5,000 missing orders - Compare Tuesday to Monday - Recover the lost data</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-correct-pattern_1","title":"The Correct Pattern","text":"<pre><code># \u2705 RIGHT: Always append in Bronze\n- name: bronze_orders\n  write:\n    connection: bronze\n    path: orders\n    mode: append  # Always append!\n</code></pre> <p>Now you have: - Monday's full load (100K rows, batch_1) - Tuesday's load (95K rows, batch_2) - Evidence of the problem - Ability to recover</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#but-what-about-disk-space","title":"But What About Disk Space?","text":"<p>\"Appending forever will use too much storage!\"</p> <p>True. But storage is cheap. Lost data is expensive.</p> <p>Solutions: 1. Retention policy: Keep 90 days in Bronze, archive older 2. Compression: Delta Lake compresses well 3. Tiered storage: Cold storage for older Bronze data</p> <pre><code># Retention policy\nsystem:\n  bronze_retention:\n    enabled: true\n    days: 90\n    archive_connection: cold_storage\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#when-overwrite-is-okay","title":"When Overwrite Is Okay","text":"<p>Never in production Bronze. Maybe for: - Local development testing - Scratch/experimental pipelines - Non-production environments</p> <p>Even then, think twice.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#anti-pattern-3-skipping-metadata","title":"Anti-Pattern #3: Skipping Metadata","text":""},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-mistake_2","title":"The Mistake","text":"<p>\"We'll just load the raw files, we can figure out where they came from later...\"</p> <pre><code># \u274c WRONG: No metadata\n- name: bronze_orders\n  read:\n    connection: landing\n    path: orders.csv\n\n  write:\n    connection: bronze\n    path: orders\n    mode: append\n    # No metadata columns added\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#why-its-wrong_2","title":"Why It's Wrong","text":"<p>You have 10 million rows in Bronze. Something is wrong with orders from December 13.</p> <p>Without metadata: - Which file did those rows come from? - When were they loaded? - Which pipeline run loaded them?</p> <p>You're stuck.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-correct-pattern_2","title":"The Correct Pattern","text":"<pre><code># \u2705 RIGHT: Always add metadata\n- name: bronze_orders\n  read:\n    connection: landing\n    path: orders.csv\n\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            _extracted_at: \"current_timestamp()\"\n            _source_file: \"'orders_20231215.csv'\"\n            _batch_id: \"${BATCH_ID}\"\n            _pipeline_version: \"'1.2.3'\"\n\n  write:\n    connection: bronze\n    path: orders\n    mode: append\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#essential-metadata-columns","title":"Essential Metadata Columns","text":"Column Purpose Example <code>_extracted_at</code> When loaded 2023-12-15 09:30:00 <code>_source_file</code> Which file orders_20231215.csv <code>_batch_id</code> Which run run_20231215_0930 <code>_source_system</code> Which system salesforce_prod"},{"location":"marketing/articles/article_17_bronze_antipatterns/#debugging-with-metadata","title":"Debugging With Metadata","text":"<p>Now you can:</p> <pre><code>-- Find December 13 data\nSELECT * FROM bronze.orders\nWHERE DATE(_extracted_at) = '2023-12-13'\n\n-- Find data from specific file\nSELECT * FROM bronze.orders\nWHERE _source_file = 'orders_20231213.csv'\n\n-- Compare batches\nSELECT _batch_id, COUNT(*) as row_count\nFROM bronze.orders\nGROUP BY _batch_id\nORDER BY _batch_id DESC\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-complete-bronze-pattern","title":"The Complete Bronze Pattern","text":"<p>Here's the right way to do Bronze:</p> <pre><code>- name: bronze_orders\n  description: \"Raw orders - NO transformations\"\n\n  read:\n    connection: landing\n    path: orders_*.csv  # Support wildcards\n    format: csv\n    options:\n      header: true\n      inferSchema: true\n\n  # Minimal contract - just verify we have data\n  contracts:\n    - type: row_count\n      min: 1\n      severity: error\n\n  # ONLY metadata columns\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            _extracted_at: \"current_timestamp()\"\n            _source_file: \"input_file_name()\"  # Gets actual filename\n            _batch_id: \"'${RUN_ID}'\"\n\n  write:\n    connection: bronze\n    path: orders\n    format: delta\n    mode: append  # Always append!\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#how-to-audit-your-bronze-layer","title":"How to Audit Your Bronze Layer","text":"<p>Check for these problems:</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#1-are-you-transforming","title":"1. Are You Transforming?","text":"<pre><code>-- Compare Bronze to source\n-- If columns are different (except _metadata), you're transforming\nSELECT column_name \nFROM information_schema.columns\nWHERE table_name = 'bronze_orders'\n  AND column_name NOT LIKE '_%'  -- Exclude metadata\n\n-- Should exactly match source file columns\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#2-are-you-appending","title":"2. Are You Appending?","text":"<pre><code>-- Check for multiple batches\nSELECT _batch_id, COUNT(*) as rows, MIN(_extracted_at), MAX(_extracted_at)\nFROM bronze.orders\nGROUP BY _batch_id\nORDER BY MIN(_extracted_at)\n\n-- If only one batch, you might be overwriting\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#3-do-you-have-metadata","title":"3. Do You Have Metadata?","text":"<pre><code>-- Check for metadata columns\nSELECT column_name\nFROM information_schema.columns\nWHERE table_name = 'bronze_orders'\n  AND column_name LIKE '_%'\n\n-- Should have _extracted_at, _source_file, _batch_id at minimum\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#recovery-stories","title":"Recovery Stories","text":""},{"location":"marketing/articles/article_17_bronze_antipatterns/#story-1-the-lost-orders","title":"Story 1: The Lost Orders","text":"<p>Problem: Source system bug sent empty file on December 14.</p> <p>Without proper Bronze:  - Overwrite replaced December 13 data with empty - December 13 orders lost - Revenue reports wrong</p> <p>With proper Bronze: - Append added empty batch - December 13 data still there - Quick fix: exclude empty batch in Silver</p> <pre><code>-- Recovery query\nSELECT * FROM bronze.orders\nWHERE _batch_id != 'batch_20231214_empty'\n</code></pre>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#story-2-the-bad-transform","title":"Story 2: The Bad Transform","text":"<p>Problem: Developer accidentally filtered out orders &lt; $10.</p> <p>Without proper Bronze: - Bronze already filtered - Small orders lost forever - No way to recover</p> <p>With proper Bronze: - Bronze has all orders - Fix Silver query - Reload from Bronze</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#story-3-the-missing-precision","title":"Story 3: The Missing Precision","text":"<p>Problem: Timestamp truncated to date in Bronze.</p> <p>Without proper Bronze: - Time component lost - Can't calculate delivery SLA to the hour - Months of data affected</p> <p>With proper Bronze: - Full timestamp preserved - Create new Silver column with hour precision - No data loss</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#key-takeaways","title":"Key Takeaways","text":"Anti-Pattern Problem Fix Transforming in Bronze Lose original data Only add metadata columns Overwriting Lose history Always append Skipping metadata Can't debug Add _extracted_at, _source_file, _batch_id"},{"location":"marketing/articles/article_17_bronze_antipatterns/#the-bronze-mantra","title":"The Bronze Mantra","text":"<p>Bronze is your undo button. Don't break it.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#next-steps","title":"Next Steps","text":"<p>With Bronze protected, let's look at Silver layer best practices:</p> <ul> <li>Centralization strategies</li> <li>Validation patterns</li> <li>Deduplication approaches</li> </ul> <p>Next article: Silver Layer Best Practices: Centralize, Validate, Deduplicate.</p>"},{"location":"marketing/articles/article_17_bronze_antipatterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_18_silver_best_practices/","title":"Silver Layer Best Practices: Centralize, Validate, Deduplicate","text":"<p>Building the single source of truth</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#tldr","title":"TL;DR","text":"<p>Silver is where data becomes trustworthy. Three principles guide Silver layer design: centralize (one authoritative version per entity), validate (stop bad data here), and deduplicate (resolve duplicates before downstream processing). This article covers each principle with concrete patterns and configurations.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#silvers-role","title":"Silver's Role","text":"<p>Silver sits between raw and refined:</p> <pre><code>Bronze (Raw)          Silver (Clean)         Gold (Modeled)\n\u251c\u2500\u2500 orders_raw   --&gt;  \u251c\u2500\u2500 orders        --&gt;  \u251c\u2500\u2500 dim_customer\n\u251c\u2500\u2500 customers_raw --&gt;  \u251c\u2500\u2500 customers    --&gt;  \u251c\u2500\u2500 fact_orders\n\u251c\u2500\u2500 products_raw  --&gt;  \u251c\u2500\u2500 products     --&gt;  \u2514\u2500\u2500 agg_daily_sales\n</code></pre> <p>Silver's responsibilities: 1. Clean - Standardize formats, fix data quality issues 2. Validate - Enforce business rules, quarantine violations 3. Deduplicate - One record per entity 4. Centralize - Single source for all downstream consumers</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#principle-1-centralize","title":"Principle 1: Centralize","text":""},{"location":"marketing/articles/article_18_silver_best_practices/#the-problem-data-silos","title":"The Problem: Data Silos","text":"<p>Marketing team creates their own customer table:</p> <pre><code>gold/marketing/customers_marketing\n</code></pre> <p>Finance creates their own:</p> <pre><code>gold/finance/customers_finance\n</code></pre> <p>Now you have: - Different customer counts - Different attribute values - Conflicting reports - No source of truth</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#the-solution-silver-as-ssot","title":"The Solution: Silver as SSOT","text":"<p>All teams consume from one Silver layer:</p> <pre><code>silver/customers  &lt;--  Marketing queries\n                  &lt;--  Finance queries\n                  &lt;--  Operations queries\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#implementation","title":"Implementation","text":"<pre><code># Central customer table in Silver\n- name: silver_customers\n  description: \"Single Source of Truth for customer data\"\n\n  # Pull from Bronze\n  read:\n    connection: bronze\n    path: customers\n\n  # Clean and validate\n  contracts:\n    - type: not_null\n      column: customer_id\n      severity: error\n    - type: unique\n      columns: [customer_id]\n      severity: error\n\n  transformer: deduplicate\n  params:\n    keys: [customer_id]\n    order_by: updated_at DESC\n\n  transform:\n    steps:\n      - function: clean_text\n        params:\n          columns: [customer_name, customer_city]\n          case: upper\n          trim: true\n\n  write:\n    connection: silver\n    path: customers\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#governance","title":"Governance","text":"<p>Document who owns Silver tables:</p> <pre><code>- name: silver_customers\n  metadata:\n    owner: data-team@company.com\n    domain: customer\n    sla: \"Updated daily by 8am\"\n    consumers:\n      - marketing\n      - finance\n      - operations\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#principle-2-validate","title":"Principle 2: Validate","text":""},{"location":"marketing/articles/article_18_silver_best_practices/#when-to-validate","title":"When to Validate","text":"<p>Silver is the validation layer. Bronze preserves; Silver protects.</p> Layer Validation Level Bronze Minimal (just verify data exists) Silver Heavy (enforce all business rules) Gold Light (mostly referential integrity)"},{"location":"marketing/articles/article_18_silver_best_practices/#validation-categories","title":"Validation Categories","text":"<p>Structural: Does the data have the right shape? - Required columns exist - Primary keys are unique - Data types are correct</p> <p>Domain: Does the data make business sense? - Values are in expected ranges - Categories are valid - Dates are reasonable</p> <p>Freshness: Is the data current? - Not stale - Recent timestamp</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#configuration-pattern","title":"Configuration Pattern","text":"<pre><code>- name: silver_orders\n  contracts:\n    # Structural\n    - type: not_null\n      column: order_id\n      severity: error\n\n    - type: unique\n      columns: [order_id]\n      severity: error\n\n    # Domain\n    - type: accepted_values\n      column: order_status\n      values: [created, approved, shipped, delivered, canceled]\n      severity: quarantine\n\n    - type: range\n      column: order_total\n      min: 0\n      max: 100000\n      severity: warn\n\n    - type: custom_sql\n      name: \"order_date_not_future\"\n      sql: \"SELECT COUNT(*) = 0 FROM df WHERE order_date &gt; CURRENT_DATE()\"\n      severity: error\n\n    # Freshness\n    - type: freshness\n      column: order_timestamp\n      max_age: \"48 hours\"\n      severity: warn\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#quarantine-strategy","title":"Quarantine Strategy","text":"<p>For domain violations, quarantine instead of fail:</p> <pre><code>validation:\n  quarantine:\n    connection: silver\n    path: orders_quarantine\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n    retention_days: 90\n</code></pre> <p>This keeps the pipeline running while tracking issues.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#principle-3-deduplicate","title":"Principle 3: Deduplicate","text":""},{"location":"marketing/articles/article_18_silver_best_practices/#why-duplicates-happen","title":"Why Duplicates Happen","text":"<p>Source systems send duplicates for many reasons: - Retry logic creates duplicate events - Batch files overlap - Source bugs - Late-arriving data restatement</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#the-deduplication-pattern","title":"The Deduplication Pattern","text":"<pre><code>transformer: deduplicate\nparams:\n  keys: [order_id]           # Natural key for matching\n  order_by: updated_at DESC  # Keep the latest version\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#how-it-works","title":"How It Works","text":"<pre><code>SELECT * FROM (\n  SELECT \n    *,\n    ROW_NUMBER() OVER (\n      PARTITION BY order_id \n      ORDER BY updated_at DESC\n    ) as rn\n  FROM source_data\n)\nWHERE rn = 1\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#deduplication-strategies","title":"Deduplication Strategies","text":"Strategy When to Use Keep latest (<code>order_by: updated_at DESC</code>) Source has update timestamp Keep first (<code>order_by: created_at ASC</code>) First occurrence is correct Keep richest (<code>order_by: field_count DESC</code>) Row with most data is best Merge fields Different rows have different fields"},{"location":"marketing/articles/article_18_silver_best_practices/#merge-deduplication","title":"Merge Deduplication","text":"<p>When duplicates have complementary data:</p> <pre><code># Row 1: {id: 1, name: \"John\", email: null}\n# Row 2: {id: 1, name: null, email: \"john@email.com\"}\n# Desired: {id: 1, name: \"John\", email: \"john@email.com\"}\n\ntransform:\n  steps:\n    - sql: |\n        SELECT \n          id,\n          FIRST_VALUE(name IGNORE NULLS) OVER (\n            PARTITION BY id ORDER BY updated_at DESC\n          ) as name,\n          FIRST_VALUE(email IGNORE NULLS) OVER (\n            PARTITION BY id ORDER BY updated_at DESC\n          ) as email\n        FROM source_data\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#deduplication-order","title":"Deduplication Order","text":"<p>Deduplicate before other transformations:</p> <pre><code>- name: silver_orders\n  # 1. First deduplicate\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: updated_at DESC\n\n  # 2. Then transform clean data\n  transform:\n    steps:\n      - function: clean_text\n        params:\n          columns: [status]\n          case: lower\n</code></pre> <p>This prevents wasted computation on duplicate rows.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#complete-silver-pattern","title":"Complete Silver Pattern","text":"<p>Here's a comprehensive Silver layer configuration:</p> <pre><code>- name: silver_orders\n  description: \"Clean, validated, deduplicated orders\"\n\n  # Source\n  read:\n    connection: bronze\n    path: orders\n    format: delta\n\n  # Pre-validation contracts\n  contracts:\n    # Structural (fail on violation)\n    - type: row_count\n      min: 1\n      severity: error\n\n    - type: not_null\n      column: order_id\n      severity: error\n\n    - type: unique\n      columns: [order_id]\n      severity: error\n\n    # Domain (quarantine violations)\n    - type: not_null\n      column: customer_id\n      severity: quarantine\n\n    - type: accepted_values\n      column: order_status\n      values: [created, approved, shipped, delivered, canceled]\n      severity: quarantine\n\n    - type: range\n      column: order_total\n      min: 0\n      severity: quarantine\n\n    # Reasonableness (warn only)\n    - type: range\n      column: order_total\n      max: 50000\n      severity: warn\n\n    - type: freshness\n      column: order_timestamp\n      max_age: \"24 hours\"\n      severity: warn\n\n  # Deduplicate first\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: _extracted_at DESC\n\n  # Then clean\n  transform:\n    steps:\n      # Standardize text\n      - function: clean_text\n        params:\n          columns: [order_status]\n          case: lower\n          trim: true\n\n      # Parse dates\n      - function: derive_columns\n        params:\n          columns:\n            order_date: \"TO_DATE(order_timestamp)\"\n            order_year: \"YEAR(order_date)\"\n            order_month: \"MONTH(order_date)\"\n\n      # Calculate derived fields\n      - function: derive_columns\n        params:\n          columns:\n            days_to_delivery: \"DATEDIFF(delivered_at, order_timestamp)\"\n            is_late: \"CASE WHEN delivered_at &gt; estimated_delivery THEN 1 ELSE 0 END\"\n\n      # Select final columns\n      - sql: |\n          SELECT\n            order_id,\n            customer_id,\n            order_status,\n            order_date,\n            order_year,\n            order_month,\n            order_total,\n            days_to_delivery,\n            is_late,\n            _extracted_at\n          FROM df\n\n  # Quarantine configuration\n  validation:\n    quarantine:\n      connection: silver\n      path: orders_quarantine\n      add_columns:\n        _rejection_reason: true\n        _rejected_at: true\n        _batch_id: true\n      retention_days: 90\n\n  # Output\n  write:\n    connection: silver\n    path: orders\n    format: delta\n    mode: overwrite\n    partition_by: [order_year, order_month]\n</code></pre>"},{"location":"marketing/articles/article_18_silver_best_practices/#common-silver-mistakes","title":"Common Silver Mistakes","text":""},{"location":"marketing/articles/article_18_silver_best_practices/#mistake-1-transforming-too-much","title":"Mistake 1: Transforming Too Much","text":"<p>Silver should clean, not model.</p> <p>\u274c Wrong:</p> <pre><code># Creating dimensions in Silver\ntransform:\n  steps:\n    - function: derive_columns\n      params:\n        columns:\n          customer_sk: \"HASH(customer_id)\"  # Wrong layer!\n</code></pre> <p>\u2705 Right: Surrogate keys belong in Gold.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#mistake-2-not-deduplicating","title":"Mistake 2: Not Deduplicating","text":"<p>\u274c Wrong:</p> <pre><code># Assuming source has no duplicates\n- name: silver_orders\n  read: ...\n  write: ...  # No deduplication!\n</code></pre> <p>\u2705 Right: Always deduplicate, even if you think source is clean.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#mistake-3-inconsistent-cleaning","title":"Mistake 3: Inconsistent Cleaning","text":"<p>\u274c Wrong:</p> <pre><code># Different case rules in different tables\nsilver_customers:\n  clean_text: upper\n\nsilver_orders:\n  clean_text: lower  # Inconsistent!\n</code></pre> <p>\u2705 Right: Establish standards and apply consistently.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#testing-silver-quality","title":"Testing Silver Quality","text":""},{"location":"marketing/articles/article_18_silver_best_practices/#row-count-comparison","title":"Row Count Comparison","text":"<pre><code>SELECT \n  (SELECT COUNT(*) FROM bronze.orders) as bronze_count,\n  (SELECT COUNT(*) FROM silver.orders) as silver_count,\n  (SELECT COUNT(*) FROM silver.orders_quarantine) as quarantine_count,\n  bronze_count - silver_count - quarantine_count as missing\n</code></pre> <p><code>missing</code> should be 0.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#uniqueness-check","title":"Uniqueness Check","text":"<pre><code>SELECT order_id, COUNT(*) as cnt\nFROM silver.orders\nGROUP BY order_id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>Should return 0 rows.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#referential-integrity-preview","title":"Referential Integrity Preview","text":"<pre><code>-- Check if customer_ids in orders exist in customers\nSELECT COUNT(*)\nFROM silver.orders o\nWHERE o.customer_id NOT IN (SELECT customer_id FROM silver.customers)\n</code></pre> <p>Prepare for Gold layer requirements.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#key-takeaways","title":"Key Takeaways","text":"Principle Implementation Centralize One Silver table per entity, all teams consume from it Validate Heavy contracts in Silver, quarantine violations Deduplicate Dedupe before transforms, use clear ordering"},{"location":"marketing/articles/article_18_silver_best_practices/#the-silver-mantra","title":"The Silver Mantra","text":"<p>Silver is the single source of truth. Make it clean, make it trusted, make it central.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#next-steps","title":"Next Steps","text":"<p>With Silver solid, let's examine a specific anti-pattern in detail:</p> <ul> <li>SCD2 misuse</li> <li>History explosion</li> <li>When NOT to use SCD2</li> </ul> <p>Next article: SCD2 Done Wrong: History Explosion and How to Prevent It.</p>"},{"location":"marketing/articles/article_18_silver_best_practices/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_19_scd2_antipatterns/","title":"SCD2 Done Wrong: History Explosion and How to Prevent It","text":"<p>When change tracking becomes a disaster</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#tldr","title":"TL;DR","text":"<p>SCD2 is powerful but dangerous when misused. The most common mistake is tracking volatile columns that change frequently, causing \"history explosion\"-millions of unnecessary rows. This article covers the warning signs, common causes, and how to fix an exploding dimension.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#the-history-explosion-problem","title":"The History Explosion Problem","text":"<p>You implement SCD2 on your customer dimension. Day 1: 100,000 rows.</p> <p>Six months later: 50,000,000 rows.</p> <p>What happened?</p> <pre><code>Day 1:    100,000 customers \u00d7 1 version = 100,000 rows\nMonth 1:  100,000 customers \u00d7 3 versions = 300,000 rows\nMonth 6:  100,000 customers \u00d7 500 versions = 50,000,000 rows\n</code></pre> <p>You're creating new versions so fast that your dimension is growing exponentially.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#warning-signs","title":"Warning Signs","text":""},{"location":"marketing/articles/article_19_scd2_antipatterns/#1-dimension-growing-faster-than-source","title":"1. Dimension Growing Faster Than Source","text":"<pre><code>-- Check growth rate\nSELECT \n  DATE(load_timestamp) as load_date,\n  COUNT(*) as rows_added,\n  SUM(COUNT(*)) OVER (ORDER BY DATE(load_timestamp)) as cumulative\nFROM dim_customer\nGROUP BY DATE(load_timestamp)\nORDER BY load_date\n</code></pre> <p>If <code>rows_added</code> is consistently high, something is wrong.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#2-many-versions-per-entity","title":"2. Many Versions Per Entity","text":"<pre><code>-- Average versions per customer\nSELECT \n  AVG(version_count) as avg_versions,\n  MAX(version_count) as max_versions\nFROM (\n  SELECT customer_id, COUNT(*) as version_count\n  FROM dim_customer\n  GROUP BY customer_id\n)\n</code></pre> <p>Healthy: 1-5 versions average Problematic: 50+ versions average</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#3-same-day-version-changes","title":"3. Same-Day Version Changes","text":"<pre><code>-- Multiple versions on same day\nSELECT customer_id, DATE(valid_from), COUNT(*)\nFROM dim_customer\nGROUP BY customer_id, DATE(valid_from)\nHAVING COUNT(*) &gt; 1\nORDER BY COUNT(*) DESC\n</code></pre> <p>Multiple versions per day is a red flag.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#common-causes","title":"Common Causes","text":""},{"location":"marketing/articles/article_19_scd2_antipatterns/#cause-1-tracking-volatile-columns","title":"Cause 1: Tracking Volatile Columns","text":"<p>The #1 cause. You track columns that change frequently but have no business value.</p> <pre><code># \u274c WRONG: Tracking updated_at\npattern:\n  type: dimension\n  params:\n    scd_type: 2\n    track_cols:\n      - customer_name\n      - customer_city\n      - updated_at  # EXPLODES! Changes every load!\n</code></pre> <p><code>updated_at</code> changes on every source system update. That triggers a new SCD2 version every time.</p> <p>Fix: Never track system timestamps.</p> <pre><code># \u2705 RIGHT: Only track business columns\npattern:\n  type: dimension\n  params:\n    scd_type: 2\n    track_cols:\n      - customer_name\n      - customer_city\n      # updated_at excluded!\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#cause-2-tracking-derivedcalculated-columns","title":"Cause 2: Tracking Derived/Calculated Columns","text":"<pre><code># \u274c WRONG: Tracking calculated fields\ntrack_cols:\n  - customer_name\n  - customer_lifetime_value  # Recalculated daily!\n  - days_since_last_order    # Changes daily!\n</code></pre> <p>Calculated fields change by definition. Don't track them.</p> <p>Fix: Keep calculated metrics in facts or separate tables.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#cause-3-casewhitespace-sensitivity","title":"Cause 3: Case/Whitespace Sensitivity","text":"<pre><code>Day 1: customer_name = \"John Smith\"\nDay 2: customer_name = \"John Smith \"  # Trailing space!\nDay 3: customer_name = \"JOHN SMITH\"   # Different case!\n</code></pre> <p>Each creates a new SCD2 version.</p> <p>Fix: Standardize before SCD2.</p> <pre><code>- name: dim_customer\n  # Clean BEFORE SCD2\n  transform:\n    steps:\n      - function: clean_text\n        params:\n          columns: [customer_name]\n          case: upper\n          trim: true\n\n  pattern:\n    type: dimension\n    params:\n      scd_type: 2\n      track_cols: [customer_name]\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#cause-4-null-variations","title":"Cause 4: NULL Variations","text":"<pre><code>Day 1: segment = NULL\nDay 2: segment = NULL  # NULLs hash differently!\n</code></pre> <p>NULL handling in hashing can cause false changes.</p> <p>Fix: Fill NULLs before SCD2.</p> <pre><code>transform:\n  steps:\n    - function: fill_nulls\n      params:\n        columns:\n          segment: \"Unknown\"\n          tier: \"Standard\"\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#cause-5-floating-point-precision","title":"Cause 5: Floating Point Precision","text":"<pre><code>Day 1: latitude = 40.7127999999\nDay 2: latitude = 40.7128000001  # Precision change!\n</code></pre> <p>Floating point representation can vary.</p> <p>Fix: Round numeric columns.</p> <pre><code>transform:\n  steps:\n    - sql: |\n        SELECT \n          *,\n          ROUND(latitude, 4) as latitude,\n          ROUND(longitude, 4) as longitude\n        FROM df\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#diagnosis-which-column-is-exploding","title":"Diagnosis: Which Column Is Exploding?","text":"<p>Find the culprit:</p> <pre><code>-- Compare consecutive versions\nWITH versions AS (\n  SELECT \n    customer_id,\n    customer_name,\n    customer_city,\n    customer_segment,\n    load_timestamp,\n    LAG(customer_name) OVER (PARTITION BY customer_id ORDER BY valid_from) as prev_name,\n    LAG(customer_city) OVER (PARTITION BY customer_id ORDER BY valid_from) as prev_city,\n    LAG(customer_segment) OVER (PARTITION BY customer_id ORDER BY valid_from) as prev_segment\n  FROM dim_customer\n)\nSELECT \n  SUM(CASE WHEN customer_name != prev_name THEN 1 ELSE 0 END) as name_changes,\n  SUM(CASE WHEN customer_city != prev_city THEN 1 ELSE 0 END) as city_changes,\n  SUM(CASE WHEN customer_segment != prev_segment THEN 1 ELSE 0 END) as segment_changes\nFROM versions\nWHERE prev_name IS NOT NULL\n</code></pre> <p>The column with the most changes is your problem.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#fixing-an-exploded-dimension","title":"Fixing an Exploded Dimension","text":""},{"location":"marketing/articles/article_19_scd2_antipatterns/#step-1-identify-true-changes","title":"Step 1: Identify True Changes","text":"<p>Rebuild with only meaningful columns:</p> <pre><code>- name: dim_customer_rebuild\n  read:\n    connection: gold\n    path: dim_customer\n\n  transform:\n    steps:\n      # Dedupe to first occurrence of each meaningful state\n      - sql: |\n          SELECT *\n          FROM (\n            SELECT *,\n              ROW_NUMBER() OVER (\n                PARTITION BY customer_id, customer_name, customer_city\n                ORDER BY valid_from\n              ) as rn\n            FROM df\n          )\n          WHERE rn = 1\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#step-2-recalculate-valid-ranges","title":"Step 2: Recalculate Valid Ranges","text":"<pre><code>WITH deduped AS (\n  -- Your deduped data\n),\nwith_next AS (\n  SELECT \n    *,\n    LEAD(valid_from) OVER (\n      PARTITION BY customer_id \n      ORDER BY valid_from\n    ) as next_valid_from\n  FROM deduped\n)\nSELECT \n  customer_sk,\n  customer_id,\n  customer_name,\n  customer_city,\n  valid_from,\n  COALESCE(next_valid_from, '9999-12-31') as valid_to,\n  CASE WHEN next_valid_from IS NULL THEN true ELSE false END as is_current\nFROM with_next\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#step-3-replace-production-table","title":"Step 3: Replace Production Table","text":"<pre><code>- name: dim_customer_fixed\n  # ... rebuild logic ...\n\n  write:\n    connection: gold\n    path: dim_customer_new  # Write to new table first\n\n# Then swap:\n# 1. Rename dim_customer to dim_customer_backup\n# 2. Rename dim_customer_new to dim_customer\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#prevention-the-scd2-checklist","title":"Prevention: The SCD2 Checklist","text":"<p>Before enabling SCD2, ask:</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#does-this-column-change-slowly","title":"\u2705 Does this column change slowly?","text":"<ul> <li>Customer address: Yes (maybe once a year)</li> <li>Last login timestamp: No (changes constantly)</li> </ul>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#is-the-change-business-meaningful","title":"\u2705 Is the change business-meaningful?","text":"<ul> <li>Customer segment change: Yes (affects pricing)</li> <li>Record update timestamp: No (just bookkeeping)</li> </ul>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#will-users-query-by-this-column","title":"\u2705 Will users query by this column?","text":"<ul> <li>\"Show me this customer's address history\": Yes</li> <li>\"Show me timestamp changes\": No</li> </ul>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#is-the-data-already-clean","title":"\u2705 Is the data already clean?","text":"<ul> <li>Standardized case? \u2713</li> <li>Trimmed whitespace? \u2713</li> <li>NULLs handled? \u2713</li> <li>Precision controlled? \u2713</li> </ul>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#when-not-to-use-scd2","title":"When NOT to Use SCD2","text":"Scenario Use Instead Column changes daily SCD1 (overwrite) Column changes hourly Event log / fact table Column is calculated Store in separate table Column is a timestamp Don't track at all No one needs history SCD1"},{"location":"marketing/articles/article_19_scd2_antipatterns/#example-use-scd1-instead","title":"Example: Use SCD1 Instead","text":"<pre><code># Customer loyalty points - changes too often for SCD2\npattern:\n  type: dimension\n  params:\n    scd_type: 1  # Just overwrite\n    track_cols: []  # No history tracking\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#example-use-event-log-instead","title":"Example: Use Event Log Instead","text":"<pre><code># Status changes - every change matters\n- name: fact_customer_status_changes\n  pattern:\n    type: fact\n    params:\n      grain: [customer_id, change_timestamp]\n      measures:\n        - status_before\n        - status_after\n        - change_reason\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#the-right-way-to-configure-scd2","title":"The Right Way to Configure SCD2","text":"<pre><code>- name: dim_customer\n  description: \"Customer dimension with controlled SCD2\"\n\n  # Clean data first\n  transform:\n    steps:\n      # Standardize case\n      - function: clean_text\n        params:\n          columns: [customer_name, customer_city]\n          case: upper\n          trim: true\n\n      # Fill NULLs\n      - function: fill_nulls\n        params:\n          columns:\n            customer_segment: \"Unknown\"\n\n      # Round floats\n      - sql: |\n          SELECT *, ROUND(latitude, 4) as latitude\n          FROM df\n\n  # Then apply SCD2 with minimal tracked columns\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols:\n        - customer_name       # Rarely changes\n        - customer_city       # Rarely changes\n        - customer_segment    # Changes quarterly\n        # NOT: updated_at, login_count, points_balance\n      target: gold.dim_customer\n</code></pre>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#monitoring-scd2-health","title":"Monitoring SCD2 Health","text":"<p>Set up alerts:</p> <pre><code># In your pipeline\npost_sql:\n  - |\n    INSERT INTO system.metrics\n    SELECT \n      'dim_customer' as table_name,\n      'avg_versions' as metric,\n      AVG(cnt) as value,\n      CURRENT_TIMESTAMP() as measured_at\n    FROM (\n      SELECT customer_id, COUNT(*) as cnt\n      FROM dim_customer\n      GROUP BY customer_id\n    )\n</code></pre> <p>Alert if <code>avg_versions &gt; 10</code>.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#key-takeaways","title":"Key Takeaways","text":"Problem Cause Fix Dimension exploding Tracking volatile columns Remove from track_cols Daily version changes Tracking timestamps Never track system fields Same-row re-versioning Case/whitespace differences Standardize before SCD2 NULL changes Inconsistent NULL handling Fill NULLs before SCD2"},{"location":"marketing/articles/article_19_scd2_antipatterns/#the-scd2-rule","title":"The SCD2 Rule","text":"<p>Only track columns that: 1. Change slowly (yearly, not daily) 2. Have business meaning 3. Users actually query historically</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#next-steps","title":"Next Steps","text":"<p>Beyond SCD2, there are other performance killers. Let's cover:</p> <ul> <li>Query anti-patterns</li> <li>Shuffle explosions</li> <li>Join disasters</li> </ul> <p>Next article: Performance Anti-Patterns: Why Your Pipeline Takes Hours.</p>"},{"location":"marketing/articles/article_19_scd2_antipatterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_20_performance_antipatterns/","title":"Performance Anti-Patterns: Why Your Pipeline Takes Hours","text":"<p>Common mistakes that kill data pipeline performance</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#tldr","title":"TL;DR","text":"<p>Data pipelines slow down for predictable reasons: unnecessary shuffles, wrong join strategies, unpartitioned data, and collect operations that bring everything to a single node. This article covers the top performance killers and how to fix them.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-1-shuffle-everything","title":"Anti-Pattern 1: Shuffle Everything","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem","title":"The Problem","text":"<p>Every <code>GROUP BY</code>, <code>JOIN</code>, or <code>DISTINCT</code> operation can trigger a shuffle-redistributing data across the cluster. Unnecessary shuffles kill performance.</p> <pre><code># \u274c Multiple shuffles\ntransform:\n  steps:\n    - sql: \"SELECT DISTINCT customer_id FROM df\"  # Shuffle 1\n    - sql: \"SELECT * FROM df GROUP BY region\"     # Shuffle 2\n    - sql: \"SELECT * FROM df ORDER BY date\"       # Shuffle 3\n</code></pre> <p>Three shuffles when you might need zero.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix","title":"The Fix","text":"<p>Combine operations to minimize shuffles:</p> <pre><code># \u2705 Single shuffle\ntransform:\n  steps:\n    - sql: |\n        SELECT region, COUNT(DISTINCT customer_id) as customers\n        FROM df\n        GROUP BY region\n        ORDER BY region\n</code></pre> <p>And pre-partition data:</p> <pre><code>write:\n  partition_by: [region]  # Future reads filter efficiently\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-2-cartesian-joins","title":"Anti-Pattern 2: Cartesian Joins","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_1","title":"The Problem","text":"<p>A join without a proper key creates a Cartesian product:</p> <pre><code># \u274c Missing join key\ntransform:\n  steps:\n    - sql: |\n        SELECT *\n        FROM orders o, customers c  # Cartesian product!\n        WHERE o.region = c.region   # This is a filter, not a join key!\n</code></pre> <p>10,000 orders \u00d7 10,000 customers = 100,000,000 row intermediate result.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_1","title":"The Fix","text":"<p>Always use explicit join syntax with proper keys:</p> <pre><code># \u2705 Proper join\ntransform:\n  steps:\n    - sql: |\n        SELECT *\n        FROM orders o\n        JOIN customers c ON o.customer_id = c.customer_id\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-3-skewed-joins","title":"Anti-Pattern 3: Skewed Joins","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_2","title":"The Problem","text":"<p>When one join key has many more values than others:</p> <pre><code>Customer A: 10,000,000 orders\nCustomer B: 100 orders\nCustomer C: 50 orders\n</code></pre> <p>All Customer A data goes to one partition. That partition takes forever while others finish instantly.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix-salting","title":"The Fix: Salting","text":"<p>Add a random component to spread the hot key:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        -- Add salt to fact table\n        SELECT \n          *,\n          CONCAT(customer_id, '_', FLOOR(RAND() * 10)) as customer_id_salted\n        FROM orders\n\n    - sql: |\n        -- Explode dimension to match salted keys\n        SELECT \n          c.*,\n          EXPLODE(ARRAY(0,1,2,3,4,5,6,7,8,9)) as salt\n        FROM customers c\n\n    - sql: |\n        -- Join on salted keys\n        SELECT o.*, c.*\n        FROM orders_salted o\n        JOIN customers_exploded c \n          ON o.customer_id_salted = CONCAT(c.customer_id, '_', c.salt)\n</code></pre> <p>Or use Spark's broadcast hint for small dimensions:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        SELECT /*+ BROADCAST(c) */ *\n        FROM orders o\n        JOIN customers c ON o.customer_id = c.customer_id\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-4-collect-to-driver","title":"Anti-Pattern 4: Collect to Driver","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_3","title":"The Problem","text":"<p>Bringing all data to a single node:</p> <pre><code># \u274c Collects entire dataset to memory\ndf = spark.read.parquet(\"data/\")\nall_data = df.collect()  # BOOM: OutOfMemory\nfor row in all_data:\n    process(row)\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_2","title":"The Fix","text":"<p>Keep data distributed:</p> <pre><code># \u2705 Process in parallel\ndf = spark.read.parquet(\"data/\")\ndf.foreach(process_row)  # Distributed\n\n# Or write results back to storage\ndf.write.parquet(\"output/\")\n</code></pre> <p>In Odibi, avoid transforms that require collection:</p> <pre><code># \u2705 All operations stay distributed\ntransform:\n  steps:\n    - sql: \"SELECT * FROM df WHERE ...\"  # Distributed\n    - function: aggregate                   # Distributed\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-5-reading-unpartitioned-data","title":"Anti-Pattern 5: Reading Unpartitioned Data","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_4","title":"The Problem","text":"<pre><code>read:\n  path: \"data/orders\"  # 10 billion rows, not partitioned\n  filter: \"WHERE order_date = '2023-12-15'\"\n</code></pre> <p>Spark reads ALL 10 billion rows, then filters. Slow.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_3","title":"The Fix","text":"<p>Partition on common filter columns:</p> <pre><code>write:\n  path: \"data/orders\"\n  partition_by: [order_year, order_month, order_day]\n</code></pre> <p>Now:</p> <pre><code>read:\n  path: \"data/orders\"\n  filter: \"WHERE order_year = 2023 AND order_month = 12 AND order_day = 15\"\n</code></pre> <p>Reads only the relevant partition.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-6-too-many-small-files","title":"Anti-Pattern 6: Too Many Small Files","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_5","title":"The Problem","text":"<p>1,000,000 tiny files (1MB each) instead of 1,000 reasonable files (1GB each).</p> <p>Each file = overhead for: - File listing - Opening/closing - Metadata reads</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_4","title":"The Fix","text":"<p>Coalesce on write:</p> <pre><code>write:\n  coalesce: 100  # Reduce to 100 files\n</code></pre> <p>Or repartition:</p> <pre><code>transform:\n  steps:\n    - sql: \"SELECT /*+ REPARTITION(100) */ * FROM df\"\n</code></pre> <p>For Delta Lake, use OPTIMIZE:</p> <pre><code>post_sql:\n  - \"OPTIMIZE gold.fact_orders\"\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-7-full-table-scans-in-lookups","title":"Anti-Pattern 7: Full Table Scans in Lookups","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_6","title":"The Problem","text":"<p>SCD2 dimension lookup scans entire dimension for every fact row:</p> <pre><code># Implicit nested loop join\npattern:\n  type: fact\n  params:\n    dimensions:\n      - dimension_table: dim_customer  # 10M rows\n        # Scanned for every fact row!\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_5","title":"The Fix","text":"<p>Broadcast small dimensions:</p> <pre><code>pattern:\n  type: fact\n  params:\n    dimensions:\n      - dimension_table: dim_customer\n        broadcast: true  # If &lt; 1GB\n</code></pre> <p>Or pre-filter dimensions:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        -- Only current records for lookup\n        SELECT * FROM dim_customer WHERE is_current = true\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-8-wrong-file-format","title":"Anti-Pattern 8: Wrong File Format","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_7","title":"The Problem","text":"<p>Using row-oriented formats for analytical queries:</p> Format Good For Bad For CSV Human reading Everything else JSON Semi-structured Analytical queries Parquet Analytics Row-level updates Delta Analytics + Updates Simple use cases <pre><code># \u274c CSV for analytics\nread:\n  format: csv  # No columnar pushdown, no compression\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_6","title":"The Fix","text":"<pre><code># \u2705 Delta for analytics\nwrite:\n  format: delta\n\npost_sql:\n  - \"OPTIMIZE table ZORDER BY (customer_id)\"\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-9-processing-all-history-every-run","title":"Anti-Pattern 9: Processing All History Every Run","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_8","title":"The Problem","text":"<pre><code># \u274c Full load every day\nread:\n  path: \"bronze/orders\"  # All history\n\nwrite:\n  mode: overwrite  # Rewrite everything\n</code></pre> <p>Processing 5 years of history when you only need today's data.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_7","title":"The Fix","text":"<p>Incremental loading:</p> <pre><code>read:\n  path: \"bronze/orders\"\n  incremental:\n    mode: stateful\n    column: _extracted_at\n    lookback: \"3 days\"\n\nwrite:\n  mode: append  # Or merge\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#anti-pattern-10-n1-query-pattern","title":"Anti-Pattern 10: N+1 Query Pattern","text":""},{"location":"marketing/articles/article_20_performance_antipatterns/#the-problem_9","title":"The Problem","text":"<p>Processing each record with a separate query:</p> <pre><code># \u274c N+1 queries\nfor order_id in order_ids:  # 1M iterations\n    customer = spark.sql(f\"SELECT * FROM customers WHERE id = {order_id}\")\n    # 1M separate queries!\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#the-fix_8","title":"The Fix","text":"<p>Batch operations:</p> <pre><code># \u2705 Single join\norders_with_customers = orders.join(customers, \"customer_id\")\n</code></pre> <p>In YAML:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        SELECT o.*, c.*\n        FROM orders o\n        LEFT JOIN customers c ON o.customer_id = c.customer_id\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#quick-performance-checklist","title":"Quick Performance Checklist","text":"<p>Before running a slow pipeline, check:</p> Check Fix Is data partitioned? Add partition_by Using columnar format? Switch to Parquet/Delta Broadcasting small tables? Add broadcast hint Processing incrementally? Add watermark Too many small files? Coalesce or OPTIMIZE Shuffling unnecessarily? Combine operations"},{"location":"marketing/articles/article_20_performance_antipatterns/#monitoring-performance","title":"Monitoring Performance","text":"<p>Add timing to your nodes:</p> <pre><code>- name: silver_orders\n  metrics:\n    enabled: true\n    include:\n      - execution_time\n      - rows_processed\n      - rows_per_second\n</code></pre> <p>Query metrics:</p> <pre><code>SELECT \n  node_name,\n  AVG(execution_seconds) as avg_time,\n  AVG(rows_processed) as avg_rows,\n  AVG(rows_processed / execution_seconds) as rows_per_sec\nFROM system.metrics\nWHERE run_date &gt;= CURRENT_DATE - 7\nGROUP BY node_name\nORDER BY avg_time DESC\n</code></pre>"},{"location":"marketing/articles/article_20_performance_antipatterns/#key-takeaways","title":"Key Takeaways","text":"Anti-Pattern Impact Fix Unnecessary shuffles 10x slowdown Combine operations Cartesian joins Exponential explosion Use proper join keys Skewed data One partition takes forever Salting or broadcast Collect to driver OutOfMemory Keep distributed Unpartitioned data Full scans Partition on filter columns Too many small files File overhead Coalesce/OPTIMIZE Wrong file format No pushdown Use Parquet/Delta Full loads Wasted reprocessing Incremental loading"},{"location":"marketing/articles/article_20_performance_antipatterns/#next-steps","title":"Next Steps","text":"<p>Beyond performance, configuration management matters for production:</p> <ul> <li>Environment-specific settings</li> <li>Secret management</li> <li>Config validation</li> </ul> <p>Next article: Configuration Patterns for Multi-Environment Pipelines.</p>"},{"location":"marketing/articles/article_20_performance_antipatterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_21_config_patterns/","title":"Configuration Patterns for Multi-Environment Pipelines","text":"<p>Same pipeline, different environments</p>"},{"location":"marketing/articles/article_21_config_patterns/#tldr","title":"TL;DR","text":"<p>Production pipelines run in multiple environments: dev, staging, production. Each needs different connection strings, credentials, and behaviors. This article covers patterns for managing environment-specific configuration: variable substitution, environment overrides, and secret management.</p>"},{"location":"marketing/articles/article_21_config_patterns/#the-problem-hardcoded-configs","title":"The Problem: Hardcoded Configs","text":"<pre><code># \u274c Works in dev, breaks everywhere else\nconnections:\n  bronze:\n    type: azure_blob\n    account: devstorageaccount123\n    container: bronze-dev\n    credential: abc123secret  # Committed to git!\n</code></pre> <p>Problems: - Different storage accounts per environment - Secrets in version control - Manual changes for each deployment</p>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-1-environment-variables","title":"Pattern 1: Environment Variables","text":"<p>Use <code>${VAR}</code> syntax for environment-specific values:</p> <pre><code>connections:\n  bronze:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    container: ${BRONZE_CONTAINER}\n    credential: ${STORAGE_KEY}\n</code></pre> <p>Set differently per environment:</p> <pre><code># Development\nexport STORAGE_ACCOUNT=devstorageaccount\nexport BRONZE_CONTAINER=bronze-dev\nexport STORAGE_KEY=dev-key-123\n\n# Production\nexport STORAGE_ACCOUNT=prodstorageaccount\nexport BRONZE_CONTAINER=bronze-prod\nexport STORAGE_KEY=prod-key-secure\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-2-environment-overrides","title":"Pattern 2: Environment Overrides","text":"<p>Define base config with environment-specific overrides:</p> <pre><code># Base configuration\nproject: \"ecommerce_warehouse\"\nengine: \"pandas\"\n\nconnections:\n  bronze:\n    type: local\n    base_path: \"./bronze\"\n\n# Environment overrides\nenvironments:\n  dev:\n    connections:\n      bronze:\n        base_path: \"./dev/bronze\"\n\n  staging:\n    connections:\n      bronze:\n        type: azure_blob\n        account: ${STAGING_ACCOUNT}\n        container: bronze-staging\n\n  production:\n    engine: \"spark\"  # Use Spark in prod\n    connections:\n      bronze:\n        type: azure_blob\n        account: ${PROD_ACCOUNT}\n        container: bronze-prod\n</code></pre> <p>Run with environment flag:</p> <pre><code>odibi run odibi.yaml --env production\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-3-global-variables","title":"Pattern 3: Global Variables","text":"<p>Define reusable variables:</p> <pre><code>vars:\n  project_name: \"ecommerce\"\n  team: \"data-platform\"\n  retention_days: 90\n\nconnections:\n  bronze:\n    base_path: \"./${vars.project_name}/bronze\"\n\npipelines:\n  - pipeline: bronze_${vars.project_name}\n    description: \"Bronze layer for ${vars.project_name}\"\n</code></pre> <p>Override per environment:</p> <pre><code>environments:\n  production:\n    vars:\n      retention_days: 365  # Keep longer in prod\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-4-secret-management","title":"Pattern 4: Secret Management","text":"<p>Never store secrets in YAML. Reference them:</p>"},{"location":"marketing/articles/article_21_config_patterns/#azure-key-vault","title":"Azure Key Vault","text":"<pre><code>connections:\n  bronze:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    credential:\n      type: keyvault\n      vault: ${KEYVAULT_NAME}\n      secret: storage-key\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#environment-variables","title":"Environment Variables","text":"<pre><code>connections:\n  bronze:\n    credential: ${STORAGE_KEY}  # Set in environment\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#databricks-secrets","title":"Databricks Secrets","text":"<pre><code>connections:\n  bronze:\n    credential:\n      type: databricks_secret\n      scope: data-platform\n      key: storage-key\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-5-separate-config-files","title":"Pattern 5: Separate Config Files","text":"<p>Split configuration by concern:</p> <pre><code>config/\n\u251c\u2500\u2500 odibi.yaml           # Main entry point\n\u251c\u2500\u2500 connections.yaml     # Connection definitions\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 bronze.yaml      # Bronze pipeline\n\u2502   \u251c\u2500\u2500 silver.yaml      # Silver pipeline\n\u2502   \u2514\u2500\u2500 gold.yaml        # Gold pipeline\n\u2514\u2500\u2500 environments/\n    \u251c\u2500\u2500 dev.yaml         # Dev overrides\n    \u251c\u2500\u2500 staging.yaml     # Staging overrides\n    \u2514\u2500\u2500 prod.yaml        # Prod overrides\n</code></pre> <p>Main config references others:</p> <pre><code># odibi.yaml\nproject: \"ecommerce\"\nengine: \"pandas\"\n\ninclude:\n  - connections.yaml\n  - pipelines/bronze.yaml\n  - pipelines/silver.yaml\n  - pipelines/gold.yaml\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-6-feature-flags","title":"Pattern 6: Feature Flags","text":"<p>Enable/disable features per environment:</p> <pre><code>vars:\n  enable_quality_checks: true\n  enable_notifications: false\n  sample_data: false\n\npipelines:\n  - pipeline: silver\n    nodes:\n      - name: silver_orders\n        enabled: ${vars.enable_quality_checks}\n        contracts:\n          # ...\n\nenvironments:\n  dev:\n    vars:\n      enable_quality_checks: false  # Skip in dev\n      sample_data: true             # Use sample data\n\n  production:\n    vars:\n      enable_quality_checks: true\n      enable_notifications: true\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#pattern-7-conditional-node-execution","title":"Pattern 7: Conditional Node Execution","text":"<p>Enable nodes based on environment:</p> <pre><code>- name: send_slack_notification\n  enabled: ${vars.enable_notifications}\n  # Only runs in production\n\n- name: sample_data\n  enabled: ${vars.sample_data}\n  # Only runs in dev\n</code></pre> <p>Or use tags:</p> <pre><code>- name: expensive_validation\n  tags: [production]\n  # Run: odibi run --tag production\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#complete-multi-environment-setup","title":"Complete Multi-Environment Setup","text":"<pre><code># odibi.yaml\n\nproject: \"ecommerce_warehouse\"\nengine: ${ENGINE}\nversion: \"1.0.0\"\n\n# Global variables\nvars:\n  env: ${ENVIRONMENT:-dev}\n  retention_days: 30\n  enable_quality_checks: true\n  enable_notifications: false\n\n# Connections with variable substitution\nconnections:\n  landing:\n    type: ${LANDING_TYPE:-local}\n    base_path: ${LANDING_PATH:-./data/landing}\n\n  bronze:\n    type: ${BRONZE_TYPE:-local}\n    base_path: ${BRONZE_PATH:-./bronze}\n\n  silver:\n    type: ${SILVER_TYPE:-local}\n    base_path: ${SILVER_PATH:-./silver}\n\n  gold:\n    type: ${GOLD_TYPE:-local}\n    base_path: ${GOLD_PATH:-./gold}\n\n# System config\nsystem:\n  connection: bronze\n  path: _system\n\nstory:\n  connection: bronze\n  path: _stories\n  retention_days: ${vars.retention_days}\n\n# Environment-specific overrides\nenvironments:\n  dev:\n    engine: pandas\n    vars:\n      enable_quality_checks: false\n\n  staging:\n    engine: spark\n    connections:\n      landing:\n        type: azure_blob\n        account: ${STAGING_STORAGE}\n        container: landing\n        credential: ${STAGING_KEY}\n      bronze:\n        type: azure_blob\n        account: ${STAGING_STORAGE}\n        container: bronze\n        credential: ${STAGING_KEY}\n\n  production:\n    engine: spark\n    vars:\n      retention_days: 90\n      enable_quality_checks: true\n      enable_notifications: true\n    connections:\n      landing:\n        type: azure_blob\n        account: ${PROD_STORAGE}\n        container: landing\n        credential:\n          type: keyvault\n          vault: ${KEYVAULT_NAME}\n          secret: storage-key\n      bronze:\n        type: azure_blob\n        account: ${PROD_STORAGE}\n        container: bronze\n        credential:\n          type: keyvault\n          vault: ${KEYVAULT_NAME}\n          secret: storage-key\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"marketing/articles/article_21_config_patterns/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/deploy.yaml\nname: Deploy Pipeline\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy-staging:\n    runs-on: ubuntu-latest\n    environment: staging\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Run Pipeline\n        env:\n          ENVIRONMENT: staging\n          STAGING_STORAGE: ${{ secrets.STAGING_STORAGE }}\n          STAGING_KEY: ${{ secrets.STAGING_KEY }}\n        run: |\n          odibi run odibi.yaml --env staging\n\n  deploy-production:\n    needs: deploy-staging\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Run Pipeline\n        env:\n          ENVIRONMENT: production\n          PROD_STORAGE: ${{ secrets.PROD_STORAGE }}\n          KEYVAULT_NAME: ${{ secrets.KEYVAULT_NAME }}\n        run: |\n          odibi run odibi.yaml --env production\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#azure-devops","title":"Azure DevOps","text":"<pre><code># azure-pipelines.yaml\nstages:\n  - stage: Staging\n    variables:\n      - group: staging-variables\n    jobs:\n      - job: RunPipeline\n        steps:\n          - script: |\n              odibi run odibi.yaml --env staging\n            env:\n              STAGING_STORAGE: $(STAGING_STORAGE)\n              STAGING_KEY: $(STAGING_KEY)\n\n  - stage: Production\n    dependsOn: Staging\n    variables:\n      - group: production-variables\n    jobs:\n      - job: RunPipeline\n        steps:\n          - script: |\n              odibi run odibi.yaml --env production\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#validation","title":"Validation","text":"<p>Validate configuration before running:</p> <pre><code># Check config syntax\nodibi validate odibi.yaml\n\n# Check with environment\nodibi validate odibi.yaml --env production\n\n# Check variables are set\nodibi validate odibi.yaml --env production --check-vars\n</code></pre> <p>Output:</p> <pre><code>\u2713 YAML syntax valid\n\u2713 All nodes have unique names\n\u2713 All dependencies exist\n\u2713 All connections defined\n\u2717 Missing variable: PROD_STORAGE\n\u2717 Missing variable: KEYVAULT_NAME\n</code></pre>"},{"location":"marketing/articles/article_21_config_patterns/#key-takeaways","title":"Key Takeaways","text":"Pattern Use Case Environment variables Secrets, account names Environment overrides Engine, paths, feature flags Global variables Shared values, DRY Secret management Never store secrets in YAML Separate files Large configurations Feature flags Environment-specific behavior"},{"location":"marketing/articles/article_21_config_patterns/#next-steps","title":"Next Steps","text":"<p>With configuration solid, let's cover debugging when things go wrong:</p> <ul> <li>Log analysis</li> <li>Common error patterns</li> <li>Step-by-step debugging</li> </ul> <p>Next article: Debugging Data Pipelines: A Systematic Approach.</p>"},{"location":"marketing/articles/article_21_config_patterns/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_22_debugging_pipelines/","title":"Debugging Data Pipelines: A Systematic Approach","text":"<p>When your pipeline fails at 3 AM</p>"},{"location":"marketing/articles/article_22_debugging_pipelines/#tldr","title":"TL;DR","text":"<p>Data pipeline debugging is systematic, not random. This article covers a repeatable process: identify the failure point, check the logs, reproduce the issue, find the root cause, and fix it. We'll cover common error patterns, log analysis, and debugging techniques for Bronze, Silver, and Gold layers.</p>"},{"location":"marketing/articles/article_22_debugging_pipelines/#the-debugging-process","title":"The Debugging Process","text":"<pre><code>1. IDENTIFY    \u2192 Which node failed?\n     \u2193\n2. LOGS        \u2192 What does the error say?\n     \u2193\n3. DATA        \u2192 What data caused the problem?\n     \u2193\n4. REPRODUCE   \u2192 Can you recreate it locally?\n     \u2193\n5. ROOT CAUSE  \u2192 Why did it happen?\n     \u2193\n6. FIX         \u2192 Implement and test solution\n     \u2193\n7. PREVENT     \u2192 Add tests/contracts to prevent recurrence\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-1-identify-the-failure-point","title":"Step 1: Identify the Failure Point","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#check-pipeline-status","title":"Check Pipeline Status","text":"<pre><code># View recent runs\nodibi status --last 10\n\n# Output:\n# Run ID          Status    Started              Duration   Failed Node\n# run_20231215    FAILED    2023-12-15 09:30:00  15m        silver_orders\n# run_20231214    SUCCESS   2023-12-14 09:30:00  12m        -\n# run_20231213    SUCCESS   2023-12-13 09:30:00  11m        -\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#identify-failed-node","title":"Identify Failed Node","text":"<pre><code># Details for specific run\nodibi status run_20231215\n\n# Output:\n# Pipeline: silver_ecommerce\n# Status: FAILED\n# \n# Nodes:\n# \u2713 silver_customers    SUCCESS   2m 15s\n# \u2713 silver_products     SUCCESS   1m 45s  \n# \u2717 silver_orders       FAILED    5m 30s    \u2190 HERE\n# \u25cb silver_payments     SKIPPED   (depends on silver_orders)\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-2-check-the-logs","title":"Step 2: Check the Logs","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#view-node-logs","title":"View Node Logs","text":"<pre><code># Full logs for failed node\nodibi logs run_20231215 --node silver_orders\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#common-error-patterns","title":"Common Error Patterns","text":"<p>Pattern 1: Contract Failure</p> <pre><code>[ERROR] Contract failed: not_null on customer_id\n[ERROR]   Found 47 null values in 99,441 rows\n[ERROR]   Sample rows with null customer_id:\n[ERROR]     Row 4521: order_id=abc123, customer_id=NULL\n[ERROR] Pipeline halted due to contract failure\n</code></pre> <p>Pattern 2: Schema Mismatch</p> <pre><code>[ERROR] Column 'order_total' not found in source\n[ERROR] Expected columns: [order_id, customer_id, order_total]\n[ERROR] Found columns: [order_id, customer_id, total_amount]\n[ERROR] Source schema may have changed\n</code></pre> <p>Pattern 3: Data Type Error</p> <pre><code>[ERROR] Cannot cast 'TBD' to DOUBLE for column 'price'\n[ERROR] Row 8932: order_id=xyz789, price='TBD'\n[ERROR] Consider using TRY_CAST or filtering invalid values\n</code></pre> <p>Pattern 4: Join Failure</p> <pre><code>[ERROR] Join produced 0 rows\n[ERROR] Left table: 50,000 rows\n[ERROR] Right table: 10,000 rows\n[ERROR] Join condition: left.customer_id = right.customer_id\n[ERROR] Possible cause: No matching keys between tables\n</code></pre> <p>Pattern 5: Out of Memory</p> <pre><code>[ERROR] java.lang.OutOfMemoryError: Java heap space\n[ERROR] Task failed while processing partition 47 of 100\n[ERROR] Consider increasing executor memory or reducing partition size\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-3-examine-the-data","title":"Step 3: Examine the Data","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#find-problem-records","title":"Find Problem Records","text":"<pre><code>-- Find rows that would fail the contract\nSELECT *\nFROM bronze.orders\nWHERE customer_id IS NULL\nORDER BY _extracted_at DESC\nLIMIT 100\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#check-for-pattern","title":"Check for Pattern","text":"<pre><code>-- Are nulls from a specific batch?\nSELECT \n  _batch_id,\n  COUNT(*) as total_rows,\n  SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customers\nFROM bronze.orders\nGROUP BY _batch_id\nORDER BY _batch_id DESC\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#compare-to-previous-run","title":"Compare to Previous Run","text":"<pre><code>-- What changed since last successful run?\nSELECT \n  DATE(_extracted_at) as extract_date,\n  COUNT(*) as rows,\n  SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as nulls\nFROM bronze.orders\nGROUP BY DATE(_extracted_at)\nORDER BY extract_date DESC\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-4-reproduce-locally","title":"Step 4: Reproduce Locally","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#extract-failing-data","title":"Extract Failing Data","text":"<pre><code># Save problem records\nodibi sample run_20231215 --node silver_orders --filter \"customer_id IS NULL\" --output failing_data.parquet\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#run-locally","title":"Run Locally","text":"<pre><code># Run with sample data\nodibi run odibi.yaml --node silver_orders --input failing_data.parquet --dry-run\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#debug-interactively","title":"Debug Interactively","text":"<pre><code># Python debugging\nfrom odibi import Pipeline\n\npipeline = Pipeline.load(\"odibi.yaml\")\nnode = pipeline.get_node(\"silver_orders\")\n\n# Read input\ndf = node.read_input()\n\n# Check data\nprint(df.filter(df.customer_id.isNull()).count())\n\n# Run transformations step by step\ndf_step1 = node.run_step(0, df)\ndf_step2 = node.run_step(1, df_step1)\n# ...\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-5-root-cause-analysis","title":"Step 5: Root Cause Analysis","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#common-root-causes","title":"Common Root Causes","text":"Symptom Likely Cause Investigation Null values Source system bug Check source system logs Schema change Upstream deployment Check schema history Missing data Extraction failure Check Bronze for gaps Duplicates Retry without dedup Check for duplicate batches Wrong values Business logic change Check with data owners"},{"location":"marketing/articles/article_22_debugging_pipelines/#the-5-whys","title":"The 5 Whys","text":"<pre><code>Problem: customer_id is NULL\n\nWhy? Source file has NULL customer_id\nWhy? CRM export job had partial failure\nWhy? CRM database timeout during export\nWhy? Large query without pagination\nWhy? Export job was never optimized\n\nRoot Cause: CRM export job needs pagination\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-6-fix-the-issue","title":"Step 6: Fix the Issue","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#immediate-fix-stop-the-bleeding","title":"Immediate Fix (Stop the Bleeding)","text":"<pre><code># Add quarantine for immediate recovery\ncontracts:\n  - type: not_null\n    column: customer_id\n    severity: quarantine  # Changed from error\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#proper-fix","title":"Proper Fix","text":"<pre><code># After source is fixed, validate it doesn't recur\ncontracts:\n  - type: not_null\n    column: customer_id\n    severity: error\n    description: \"Fixed upstream in ticket DATA-1234\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#reprocess-failed-data","title":"Reprocess Failed Data","text":"<pre><code># Rerun failed node\nodibi run odibi.yaml --node silver_orders --reprocess run_20231215\n\n# Or reprocess specific batches\nodibi run odibi.yaml --node silver_orders --filter \"_batch_id = 'batch_20231215'\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#step-7-prevent-recurrence","title":"Step 7: Prevent Recurrence","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#add-contracts","title":"Add Contracts","text":"<pre><code># New contract based on failure\ncontracts:\n  - type: not_null\n    column: customer_id\n    severity: error\n    description: \"Added after incident INC-2023-1215\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#add-monitoring","title":"Add Monitoring","text":"<pre><code>alerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK}\n    on_events: [on_failure, on_warning]\n    message: \"Pipeline ${pipeline} node ${node} failed: ${error}\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#add-tests","title":"Add Tests","text":"<pre><code># test_silver_orders.py\ndef test_null_customer_rejected():\n    \"\"\"Null customer_id should fail contract\"\"\"\n    bad_data = pd.DataFrame({\n        'order_id': ['ORD-001'],\n        'customer_id': [None]\n    })\n\n    result = run_node('silver_orders', input_df=bad_data, dry_run=True)\n    assert result.status == 'failed'\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#layer-specific-debugging","title":"Layer-Specific Debugging","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#bronze-debugging","title":"Bronze Debugging","text":"<p>Common issues: - Empty source files - Schema drift - Encoding problems</p> <pre><code># Check source file\nhead -n 5 data/landing/orders.csv\n\n# Verify Bronze landed correctly\nodibi query \"SELECT COUNT(*), MIN(_extracted_at), MAX(_extracted_at) FROM bronze.orders\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#silver-debugging","title":"Silver Debugging","text":"<p>Common issues: - Contract failures - Transform errors - Deduplication issues</p> <pre><code># Check pre/post row counts\nodibi query \"\n  SELECT \n    (SELECT COUNT(*) FROM bronze.orders) as bronze,\n    (SELECT COUNT(*) FROM silver.orders) as silver,\n    (SELECT COUNT(*) FROM silver.orders_quarantine) as quarantine\n\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#gold-debugging","title":"Gold Debugging","text":"<p>Common issues: - Dimension lookup failures - Grain violations - Aggregation errors</p> <pre><code># Check for orphans\nodibi query \"\n  SELECT customer_sk, COUNT(*) \n  FROM gold.fact_orders \n  WHERE customer_sk = 0  -- Unknown member\n  GROUP BY customer_sk\n\"\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#debugging-toolkit","title":"Debugging Toolkit","text":""},{"location":"marketing/articles/article_22_debugging_pipelines/#useful-queries","title":"Useful Queries","text":"<pre><code>-- Data freshness\nSELECT MAX(_extracted_at) as latest FROM bronze.orders;\n\n-- Row counts by batch\nSELECT _batch_id, COUNT(*) FROM bronze.orders GROUP BY _batch_id;\n\n-- Contract violations\nSELECT _rejection_reason, COUNT(*) \nFROM silver.orders_quarantine \nGROUP BY _rejection_reason;\n\n-- Duplicate check\nSELECT order_id, COUNT(*) \nFROM silver.orders \nGROUP BY order_id \nHAVING COUNT(*) &gt; 1;\n\n-- Schema comparison\nDESCRIBE bronze.orders;\nDESCRIBE silver.orders;\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#log-analysis-commands","title":"Log Analysis Commands","text":"<pre><code># Search logs for errors\nodibi logs run_20231215 | grep -i error\n\n# Count errors by type\nodibi logs run_20231215 | grep ERROR | cut -d: -f3 | sort | uniq -c\n\n# Find slow operations\nodibi logs run_20231215 | grep \"completed in\" | sort -t' ' -k4 -rn | head\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#emergency-playbook","title":"Emergency Playbook","text":"<p>When the pipeline fails at 3 AM:</p>"},{"location":"marketing/articles/article_22_debugging_pipelines/#1-quick-assessment-5-min","title":"1. Quick Assessment (5 min)","text":"<pre><code>odibi status --last 1\nodibi logs $RUN_ID | tail -50\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#2-decide-fix-or-skip-5-min","title":"2. Decide: Fix or Skip (5 min)","text":"<ul> <li>Data issue: Quarantine and continue</li> <li>Code issue: Rollback last change</li> <li>Infrastructure: Alert on-call</li> </ul>"},{"location":"marketing/articles/article_22_debugging_pipelines/#3-quick-fix-10-min","title":"3. Quick Fix (10 min)","text":"<pre><code># Emergency quarantine\ncontracts:\n  - severity: quarantine  # Was: error\n</code></pre> <pre><code>odibi run --node failed_node\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#4-document-5-min","title":"4. Document (5 min)","text":"<pre><code>Incident: Pipeline failed 2023-12-15 03:00\nCause: NULL customer_id in batch_20231215\nFix: Quarantined 47 records, pipeline completed\nFollow-up: Ticket DATA-1234 for source fix\n</code></pre>"},{"location":"marketing/articles/article_22_debugging_pipelines/#key-takeaways","title":"Key Takeaways","text":"Step Action Identify Which node, which run Logs Read the error message carefully Data Find the specific bad records Reproduce Test locally with sample data Root cause 5 Whys, find upstream issue Fix Immediate + proper solution Prevent Contracts, monitoring, tests"},{"location":"marketing/articles/article_22_debugging_pipelines/#next-steps","title":"Next Steps","text":"<p>We've covered the core patterns. Now for advanced topics:</p> <ul> <li>Semantic layer for metrics</li> <li>Production monitoring</li> <li>Reflections on open source</li> </ul> <p>Next article: Building a Semantic Layer: Metrics Everyone Can Trust.</p>"},{"location":"marketing/articles/article_22_debugging_pipelines/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_23_semantic_layer/","title":"Building a Semantic Layer: Metrics Everyone Can Trust","text":"<p>Define once, query everywhere</p>"},{"location":"marketing/articles/article_23_semantic_layer/#tldr","title":"TL;DR","text":"<p>A semantic layer defines business metrics centrally so everyone gets the same numbers. Instead of each analyst writing their own \"revenue\" calculation, you define it once: <code>SUM(line_total) WHERE status = 'completed'</code>. This article covers building a semantic layer with Odibi: defining metrics, dimensions, and materializations.</p>"},{"location":"marketing/articles/article_23_semantic_layer/#the-problem-metric-chaos","title":"The Problem: Metric Chaos","text":"<p>Marketing says revenue is $10M. Finance says revenue is $9.5M. Sales says revenue is $11M.</p> <p>Why? - Marketing includes pending orders - Finance excludes refunds - Sales includes forecasted orders</p> <p>Everyone has their own definition. Nobody trusts the numbers.</p>"},{"location":"marketing/articles/article_23_semantic_layer/#the-solution-semantic-layer","title":"The Solution: Semantic Layer","text":"<p>One authoritative definition:</p> <pre><code>metrics:\n  - name: revenue\n    description: \"Total completed order revenue, net of refunds\"\n    expr: \"SUM(line_total)\"\n    source: gold.fact_orders\n    filters:\n      - \"status = 'completed'\"\n      - \"refunded = false\"\n</code></pre> <p>Now everyone queries the same metric:</p> <pre><code>project.query(\"revenue BY region\")\nproject.query(\"revenue BY month\")\nproject.query(\"revenue BY product_category\")\n</code></pre> <p>Same definition. Same numbers. Every time.</p>"},{"location":"marketing/articles/article_23_semantic_layer/#core-concepts","title":"Core Concepts","text":""},{"location":"marketing/articles/article_23_semantic_layer/#metrics","title":"Metrics","text":"<p>Measurable values you aggregate:</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(line_total)\"\n    source: gold.fact_order_items\n\n  - name: order_count\n    expr: \"COUNT(DISTINCT order_id)\"\n    source: gold.fact_orders\n\n  - name: avg_order_value\n    expr: \"SUM(line_total) / COUNT(DISTINCT order_id)\"\n    source: gold.fact_order_items\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#dimensions","title":"Dimensions","text":"<p>Attributes you group by:</p> <pre><code>dimensions:\n  - name: region\n    source: gold.dim_customer\n    column: customer_region\n\n  - name: product_category\n    source: gold.dim_product\n    column: product_category_name\n\n  - name: order_date\n    source: gold.dim_date\n    column: full_date\n    hierarchy: [year, quarter, month, day]\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#materializations","title":"Materializations","text":"<p>Pre-computed aggregates for performance:</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold.agg_monthly_region\n    schedule: \"0 6 * * *\"  # Daily at 6 AM\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#configuration","title":"Configuration","text":""},{"location":"marketing/articles/article_23_semantic_layer/#complete-semantic-layer-setup","title":"Complete Semantic Layer Setup","text":"<pre><code>project: \"ecommerce_warehouse\"\nengine: \"spark\"\n\nconnections:\n  gold:\n    type: delta\n    catalog: spark_catalog\n    schema_name: gold\n\n# Semantic layer configuration\nsemantic:\n  metrics:\n    # Simple metric\n    - name: revenue\n      description: \"Total revenue from completed orders\"\n      expr: \"SUM(line_total)\"\n      source: gold.fact_order_items\n      filters:\n        - \"order_status = 'completed'\"\n\n    # Count metric\n    - name: order_count\n      description: \"Number of unique orders\"\n      expr: \"COUNT(DISTINCT order_id)\"\n      source: gold.fact_orders\n\n    # Derived metric (references other metrics)\n    - name: avg_order_value\n      description: \"Average order value\"\n      expr: \"revenue / order_count\"\n      type: derived\n\n    # Metric with time filter\n    - name: revenue_last_30_days\n      description: \"Revenue in last 30 days\"\n      expr: \"SUM(line_total)\"\n      source: gold.fact_order_items\n      filters:\n        - \"order_date &gt;= CURRENT_DATE - 30\"\n\n  dimensions:\n    # Simple dimension\n    - name: region\n      description: \"Customer region\"\n      source: gold.dim_customer\n      column: customer_region\n\n    # Dimension with hierarchy\n    - name: date\n      description: \"Order date\"\n      source: gold.dim_date\n      column: full_date\n      hierarchy: [year, quarter, month, week, day]\n\n    # Product dimension\n    - name: product_category\n      description: \"Product category\"\n      source: gold.dim_product\n      column: product_category_name\n\n    # Dimension from same table as metric\n    - name: order_status\n      description: \"Order status\"\n      source: gold.fact_orders\n      column: order_status\n\n  materializations:\n    # Daily refresh\n    - name: daily_revenue\n      metrics: [revenue, order_count]\n      dimensions: [date, region]\n      output: gold.agg_daily_revenue\n      schedule: \"0 6 * * *\"\n\n    # Weekly refresh\n    - name: weekly_category_performance\n      metrics: [revenue, order_count, avg_order_value]\n      dimensions: [product_category, region, month]\n      output: gold.agg_category_performance\n      schedule: \"0 8 * * 1\"  # Monday 8 AM\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#querying-the-semantic-layer","title":"Querying the Semantic Layer","text":""},{"location":"marketing/articles/article_23_semantic_layer/#python-api","title":"Python API","text":"<pre><code>from odibi import Project\n\n# Load project with semantic layer\nproject = Project.load(\"odibi.yaml\")\n\n# Simple query\nresult = project.query(\"revenue\")\nprint(f\"Total Revenue: ${result.value:,.2f}\")\n\n# Query by dimension\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n#   region      revenue\n#   Southeast   5,234,567\n#   South       2,123,456\n#   Northeast   1,567,890\n\n# Multiple dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\nprint(result.df)\n\n# With filters\nresult = project.query(\n    \"revenue BY product_category\",\n    filters=[\"region = 'Southeast'\"]\n)\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#sql-generation","title":"SQL Generation","text":"<p>The semantic layer generates SQL:</p> <pre><code># Get generated SQL\nsql = project.query(\"revenue BY region\", return_sql=True)\nprint(sql)\n</code></pre> <p>Output:</p> <pre><code>SELECT \n  dc.customer_region as region,\n  SUM(f.line_total) as revenue\nFROM gold.fact_order_items f\nJOIN gold.dim_customer dc ON f.customer_sk = dc.customer_sk\nWHERE f.order_status = 'completed'\nGROUP BY dc.customer_region\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#metric-types","title":"Metric Types","text":""},{"location":"marketing/articles/article_23_semantic_layer/#simple-metrics","title":"Simple Metrics","text":"<p>Direct aggregation:</p> <pre><code>- name: revenue\n  expr: \"SUM(line_total)\"\n  source: gold.fact_order_items\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#filtered-metrics","title":"Filtered Metrics","text":"<p>With WHERE clause:</p> <pre><code>- name: completed_orders\n  expr: \"COUNT(*)\"\n  source: gold.fact_orders\n  filters:\n    - \"status = 'completed'\"\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#derived-metrics","title":"Derived Metrics","text":"<p>Calculations from other metrics:</p> <pre><code>- name: avg_order_value\n  expr: \"revenue / order_count\"\n  type: derived\n\n- name: profit_margin\n  expr: \"(revenue - cost) / revenue * 100\"\n  type: derived\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#time-relative-metrics","title":"Time-Relative Metrics","text":"<p>Relative to query time:</p> <pre><code>- name: mtd_revenue\n  description: \"Month-to-date revenue\"\n  expr: \"SUM(line_total)\"\n  filters:\n    - \"order_date &gt;= DATE_TRUNC('month', CURRENT_DATE)\"\n    - \"order_date &lt;= CURRENT_DATE\"\n\n- name: yoy_growth\n  description: \"Year-over-year growth %\"\n  expr: |\n    (SUM(CASE WHEN order_year = YEAR(CURRENT_DATE) THEN line_total END) - \n     SUM(CASE WHEN order_year = YEAR(CURRENT_DATE) - 1 THEN line_total END)) /\n    SUM(CASE WHEN order_year = YEAR(CURRENT_DATE) - 1 THEN line_total END) * 100\n  type: derived\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#dimension-hierarchies","title":"Dimension Hierarchies","text":"<p>Enable drill-down:</p> <pre><code>dimensions:\n  - name: date\n    source: gold.dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter\n      - month\n      - week\n      - day\n</code></pre> <p>Query at different levels:</p> <pre><code># By year\nproject.query(\"revenue BY date.year\")\n\n# By month\nproject.query(\"revenue BY date.month\")\n\n# Drill down\nproject.query(\"revenue BY date.year, date.month\")\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#materializations_1","title":"Materializations","text":""},{"location":"marketing/articles/article_23_semantic_layer/#why-materialize","title":"Why Materialize?","text":"<p>Semantic queries can be expensive: - Multiple joins - Large aggregations - Complex filters</p> <p>Pre-compute common queries for speed.</p>"},{"location":"marketing/articles/article_23_semantic_layer/#configuration_1","title":"Configuration","text":"<pre><code>materializations:\n  - name: monthly_summary\n    metrics: [revenue, order_count, avg_order_value]\n    dimensions: [region, product_category, month]\n    output: gold.agg_monthly_summary\n    schedule: \"0 6 1 * *\"  # 1st of month, 6 AM\n\n    # Incremental refresh\n    incremental:\n      timestamp_column: order_date\n      lookback: \"7 days\"\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#automatic-materialization-routing","title":"Automatic Materialization Routing","text":"<p>When a query matches a materialization, it uses the pre-computed table:</p> <pre><code># This hits agg_monthly_summary, not fact tables\nresult = project.query(\"revenue BY region, month\")\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#governance","title":"Governance","text":""},{"location":"marketing/articles/article_23_semantic_layer/#metric-ownership","title":"Metric Ownership","text":"<pre><code>metrics:\n  - name: revenue\n    owner: finance-team@company.com\n    certified: true\n    certification_date: 2023-12-01\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#documentation","title":"Documentation","text":"<pre><code>metrics:\n  - name: revenue\n    description: |\n      Total revenue from completed orders.\n\n      Calculation:\n      - Includes: All order line items with status = 'completed'\n      - Excludes: Refunded orders, canceled orders\n      - Currency: USD (all currencies converted at order date rate)\n\n      Owner: Finance Team\n      Last reviewed: 2023-12-01\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#lineage","title":"Lineage","text":"<p>Track where metrics come from:</p> <pre><code># Show metric lineage\nlineage = project.metric_lineage(\"revenue\")\nprint(lineage)\n# revenue\n#   \u2514\u2500\u2500 fact_order_items.line_total\n#       \u2514\u2500\u2500 silver.order_items.price + freight_value\n#           \u2514\u2500\u2500 bronze.order_items\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#bi-tool-integration","title":"BI Tool Integration","text":""},{"location":"marketing/articles/article_23_semantic_layer/#power-bi-tableau","title":"Power BI / Tableau","text":"<p>Generate a view layer for BI tools:</p> <pre><code>materializations:\n  - name: bi_revenue_report\n    metrics: [revenue, order_count, avg_order_value]\n    dimensions: [region, product_category, date.month]\n    output: gold.vw_bi_revenue  # View for BI tool\n    format: view\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#sql-interface","title":"SQL Interface","text":"<p>Direct SQL access to semantic layer:</p> <pre><code>-- BI tool connects via SQL\nSELECT * FROM gold.vw_bi_revenue\nWHERE month &gt;= '2023-01'\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#complete-example","title":"Complete Example","text":"<pre><code># odibi.yaml\n\nproject: \"ecommerce_analytics\"\nengine: \"spark\"\n\nconnections:\n  gold:\n    type: delta\n    catalog: spark_catalog\n    schema_name: gold\n\nsemantic:\n  # Core business metrics\n  metrics:\n    - name: revenue\n      description: \"Net revenue from completed orders\"\n      expr: \"SUM(line_total)\"\n      source: gold.fact_order_items\n      filters:\n        - \"order_status = 'completed'\"\n      owner: finance@company.com\n      certified: true\n\n    - name: orders\n      description: \"Count of unique orders\"\n      expr: \"COUNT(DISTINCT order_id)\"\n      source: gold.fact_orders\n      filters:\n        - \"order_status != 'canceled'\"\n\n    - name: customers\n      description: \"Count of unique customers\"\n      expr: \"COUNT(DISTINCT customer_id)\"\n      source: gold.fact_orders\n\n    - name: aov\n      description: \"Average Order Value\"\n      expr: \"revenue / orders\"\n      type: derived\n\n    - name: late_delivery_rate\n      description: \"Percentage of late deliveries\"\n      expr: \"SUM(is_late) * 100.0 / COUNT(*)\"\n      source: gold.fact_orders\n\n  # Dimensions for slicing\n  dimensions:\n    - name: region\n      source: gold.dim_customer\n      column: customer_region\n\n    - name: category\n      source: gold.dim_product\n      column: product_category_name\n\n    - name: seller_state\n      source: gold.dim_seller\n      column: seller_state\n\n    - name: date\n      source: gold.dim_date\n      column: full_date\n      hierarchy: [year, quarter, month, week, day]\n\n  # Pre-computed aggregates\n  materializations:\n    - name: daily_kpis\n      metrics: [revenue, orders, customers, aov]\n      dimensions: [date.day, region]\n      output: gold.agg_daily_kpis\n      schedule: \"0 5 * * *\"\n\n    - name: category_performance\n      metrics: [revenue, orders]\n      dimensions: [category, region, date.month]\n      output: gold.agg_category_monthly\n      schedule: \"0 6 * * *\"\n</code></pre>"},{"location":"marketing/articles/article_23_semantic_layer/#key-takeaways","title":"Key Takeaways","text":"Concept Purpose Metrics Centralized aggregation definitions Dimensions Consistent grouping attributes Materializations Pre-computed for performance Governance Ownership, certification, documentation"},{"location":"marketing/articles/article_23_semantic_layer/#the-semantic-layer-mantra","title":"The Semantic Layer Mantra","text":"<p>Define once. Trust always. Query anywhere.</p>"},{"location":"marketing/articles/article_23_semantic_layer/#next-steps","title":"Next Steps","text":"<p>With analytics in place, let's cover production operations:</p> <ul> <li>Monitoring</li> <li>Alerting</li> <li>Retry strategies</li> </ul> <p>Next article: Production Data Pipelines: Monitoring, Retry, and Testing.</p>"},{"location":"marketing/articles/article_23_semantic_layer/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_24_production_pipelines/","title":"Production Data Pipelines: Monitoring, Retry, and Testing","text":"<p>Keeping the lights on</p>"},{"location":"marketing/articles/article_24_production_pipelines/#tldr","title":"TL;DR","text":"<p>Production pipelines need more than correct code. They need monitoring (know when things fail), retry logic (handle transient failures), alerting (notify the right people), and testing (catch issues before production). This article covers operational patterns for reliable data pipelines.</p>"},{"location":"marketing/articles/article_24_production_pipelines/#the-production-checklist","title":"The Production Checklist","text":"<p>Before going live, verify:</p> Category Check Reliability Retry on transient failures Monitoring Metrics collected and dashboarded Alerting Failures notify on-call Testing Unit + integration tests pass Documentation Runbooks exist Recovery Can reprocess from any point"},{"location":"marketing/articles/article_24_production_pipelines/#monitoring","title":"Monitoring","text":""},{"location":"marketing/articles/article_24_production_pipelines/#what-to-monitor","title":"What to Monitor","text":"Metric Why Execution time Detect slowdowns Row counts Catch data volume changes Error counts Track failures Quarantine rate Data quality trending Late runs SLA compliance"},{"location":"marketing/articles/article_24_production_pipelines/#configuration","title":"Configuration","text":"<pre><code># Enable metrics collection\nsystem:\n  connection: bronze\n  path: _system\n\n  metrics:\n    enabled: true\n    table: pipeline_metrics\n    retention_days: 90\n\nlogging:\n  level: INFO\n  structured: true  # JSON for log aggregation\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#metrics-table","title":"Metrics Table","text":"<pre><code>SELECT \n  run_id,\n  pipeline,\n  node,\n  status,\n  rows_read,\n  rows_written,\n  rows_quarantined,\n  execution_seconds,\n  started_at,\n  completed_at\nFROM system.pipeline_metrics\nWHERE started_at &gt;= CURRENT_DATE - 7\nORDER BY started_at DESC\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#dashboard-queries","title":"Dashboard Queries","text":"<pre><code>-- Daily success rate\nSELECT \n  DATE(started_at) as run_date,\n  COUNT(*) as total_runs,\n  SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successes,\n  ROUND(100.0 * successes / total_runs, 2) as success_rate\nFROM system.pipeline_metrics\nGROUP BY DATE(started_at)\nORDER BY run_date DESC\n\n-- Slowest nodes this week\nSELECT \n  node,\n  AVG(execution_seconds) as avg_seconds,\n  MAX(execution_seconds) as max_seconds\nFROM system.pipeline_metrics\nWHERE started_at &gt;= CURRENT_DATE - 7\nGROUP BY node\nORDER BY avg_seconds DESC\n\n-- Quarantine trends\nSELECT \n  DATE(started_at) as run_date,\n  SUM(rows_quarantined) as quarantined,\n  SUM(rows_written) as written,\n  ROUND(100.0 * quarantined / (written + quarantined), 4) as quarantine_rate\nFROM system.pipeline_metrics\nGROUP BY DATE(started_at)\nORDER BY run_date DESC\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#retry-logic","title":"Retry Logic","text":""},{"location":"marketing/articles/article_24_production_pipelines/#transient-failures","title":"Transient Failures","text":"<p>Some failures are temporary: - Network timeout - Rate limiting - Service unavailable - Lock conflicts</p> <p>These should retry automatically.</p>"},{"location":"marketing/articles/article_24_production_pipelines/#configuration_1","title":"Configuration","text":"<pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n  initial_delay: 30  # seconds\n  max_delay: 300     # 5 minutes\n\n  # Which errors to retry\n  retryable_errors:\n    - \"Connection timed out\"\n    - \"Service temporarily unavailable\"\n    - \"Rate limit exceeded\"\n    - \"Lock acquisition failed\"\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#backoff-strategies","title":"Backoff Strategies","text":"Strategy Behavior <code>fixed</code> Wait same time each retry (30s, 30s, 30s) <code>linear</code> Increase linearly (30s, 60s, 90s) <code>exponential</code> Double each time (30s, 60s, 120s)"},{"location":"marketing/articles/article_24_production_pipelines/#node-level-retry","title":"Node-Level Retry","text":"<p>Override for specific nodes:</p> <pre><code>- name: load_from_api\n  description: \"External API - needs extra retries\"\n\n  retry:\n    max_attempts: 5\n    backoff: exponential\n    initial_delay: 10\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#alerting","title":"Alerting","text":""},{"location":"marketing/articles/article_24_production_pipelines/#alert-configuration","title":"Alert Configuration","text":"<pre><code>alerts:\n  # Slack notification\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    channel: \"#data-alerts\"\n    on_events: [on_failure]\n\n  # Email for critical failures\n  - type: email\n    to: [data-team@company.com, oncall@company.com]\n    on_events: [on_failure]\n    severity: critical\n\n  # PagerDuty for SLA breaches\n  - type: pagerduty\n    service_key: ${PAGERDUTY_KEY}\n    on_events: [on_sla_breach]\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#alert-events","title":"Alert Events","text":"Event Trigger <code>on_failure</code> Pipeline fails <code>on_warning</code> Contract warning <code>on_success</code> Pipeline succeeds (optional) <code>on_sla_breach</code> Runs past deadline <code>on_quarantine_spike</code> High quarantine rate"},{"location":"marketing/articles/article_24_production_pipelines/#sla-configuration","title":"SLA Configuration","text":"<pre><code>pipelines:\n  - pipeline: silver_ecommerce\n    sla:\n      deadline: \"08:00\"  # Must complete by 8 AM\n      timezone: \"America/Sao_Paulo\"\n      alert_before: 30   # Alert if not done 30 min before\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#testing","title":"Testing","text":""},{"location":"marketing/articles/article_24_production_pipelines/#test-levels","title":"Test Levels","text":"Level What When Unit Individual functions Every commit Contract Data quality rules Every run Integration End-to-end pipeline Pre-deploy Regression Known issues Weekly"},{"location":"marketing/articles/article_24_production_pipelines/#unit-tests","title":"Unit Tests","text":"<pre><code># tests/test_transforms.py\nimport pytest\nfrom odibi.transforms import clean_text, derive_columns\n\ndef test_clean_text_uppercase():\n    \"\"\"clean_text should uppercase properly\"\"\"\n    df = pd.DataFrame({'name': ['john', 'Jane', ' BOB ']})\n    result = clean_text(df, columns=['name'], case='upper', trim=True)\n\n    assert result['name'].tolist() == ['JOHN', 'JANE', 'BOB']\n\ndef test_derive_columns_null_handling():\n    \"\"\"derive_columns should handle NULLs\"\"\"\n    df = pd.DataFrame({'a': [1, None, 3], 'b': [10, 20, None]})\n    result = derive_columns(df, columns={'c': 'a + b'})\n\n    assert result['c'].tolist() == [11, None, None]\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#contract-tests","title":"Contract Tests","text":"<pre><code># tests/test_contracts.py\ndef test_not_null_contract_catches_nulls():\n    \"\"\"not_null contract should fail on NULL values\"\"\"\n    bad_data = pd.DataFrame({\n        'order_id': ['ORD-001', 'ORD-002'],\n        'customer_id': ['CUST-001', None]\n    })\n\n    result = run_contract('not_null', column='customer_id', df=bad_data)\n\n    assert result.passed == False\n    assert result.failure_count == 1\n\ndef test_accepted_values_contract():\n    \"\"\"accepted_values should reject invalid values\"\"\"\n    bad_data = pd.DataFrame({\n        'status': ['active', 'inactive', 'bogus']\n    })\n\n    result = run_contract(\n        'accepted_values',\n        column='status',\n        values=['active', 'inactive'],\n        df=bad_data\n    )\n\n    assert result.passed == False\n    assert 'bogus' in result.invalid_values\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#integration-tests","title":"Integration Tests","text":"<pre><code># tests/test_integration.py\n@pytest.fixture\ndef test_data():\n    \"\"\"Create test data in Bronze\"\"\"\n    create_test_tables()\n    yield\n    cleanup_test_tables()\n\ndef test_silver_pipeline_end_to_end(test_data):\n    \"\"\"Full Silver pipeline should complete\"\"\"\n    result = run_pipeline('silver_ecommerce', test_mode=True)\n\n    assert result.status == 'SUCCESS'\n    assert result.nodes_completed == 7\n    assert result.total_rows &gt; 0\n\ndef test_gold_dimensions_populated(test_data):\n    \"\"\"Gold dimensions should have expected rows\"\"\"\n    run_pipeline('gold_dimensions', test_mode=True)\n\n    assert count_rows('dim_customer') &gt; 0\n    assert count_rows('dim_product') &gt; 0\n    assert count_rows('dim_date') &gt; 0\n\n    # Check unknown member exists\n    assert exists_row('dim_customer', 'customer_sk = 0')\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_contracts.py -v\n\n# Run with coverage\npytest tests/ --cov=odibi --cov-report=html\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#recovery-patterns","title":"Recovery Patterns","text":""},{"location":"marketing/articles/article_24_production_pipelines/#reprocessing","title":"Reprocessing","text":"<pre><code># Rerun failed node\nodibi run --node silver_orders --reprocess\n\n# Rerun from specific date\nodibi run --node silver_orders --from-date 2023-12-01\n\n# Rerun specific batches\nodibi run --node silver_orders --filter \"_batch_id = 'batch_20231215'\"\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#rollback","title":"Rollback","text":"<pre><code># List table versions\nodibi history gold.dim_customer\n\n# Rollback to previous version\nodibi rollback gold.dim_customer --version 5\n\n# Or restore to timestamp\nodibi rollback gold.dim_customer --timestamp \"2023-12-14 09:00:00\"\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#backfill","title":"Backfill","text":"<pre><code># Historical reprocessing\n- name: silver_orders\n  read:\n    incremental:\n      mode: stateful\n      column: order_date\n\n      # Override for backfill\n      first_run_query: |\n        SELECT * FROM bronze.orders\n        WHERE order_date BETWEEN '2023-01-01' AND '2023-12-31'\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#runbooks","title":"Runbooks","text":"<p>Document operational procedures:</p>"},{"location":"marketing/articles/article_24_production_pipelines/#example-runbook","title":"Example Runbook","text":"<pre><code># Silver Pipeline Failure Runbook\n\n## Overview\nThis runbook covers troubleshooting for `silver_ecommerce` pipeline failures.\n\n## Quick Diagnosis\n\n1. Check pipeline status:\n   ```bash\n   odibi status --last 5\n   ```\n\n2. View failed node logs:\n   ```bash\n   odibi logs $RUN_ID --node $FAILED_NODE\n   ```\n\n## Common Issues\n\n### Contract Failure: not_null\n**Symptom**: \"Contract failed: not_null on customer_id\"\n\n**Resolution**:\n1. Check Bronze for null values:\n   ```sql\n   SELECT COUNT(*) FROM bronze.orders WHERE customer_id IS NULL\n   ```\n2. If source issue: Contact upstream team\n3. If temporary: Enable quarantine, rerun\n\n### Schema Change\n**Symptom**: \"Column 'X' not found\"\n\n**Resolution**:\n1. Compare schemas:\n   ```bash\n   odibi schema bronze.orders\n   ```\n2. Update pipeline config for new schema\n3. Rerun\n\n## Escalation\n- Level 1: Data team on-call\n- Level 2: Platform team\n- Level 3: Vendor support\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#operational-configuration","title":"Operational Configuration","text":"<pre><code># odibi.yaml - Production configuration\n\nproject: \"ecommerce_warehouse\"\nengine: \"spark\"\nversion: \"1.0.0\"\n\n# Reliability\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\n# Observability\nlogging:\n  level: INFO\n  structured: true\n\nsystem:\n  connection: bronze\n  path: _system\n  metrics:\n    enabled: true\n    retention_days: 90\n\n# Alerting\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK}\n    on_events: [on_failure, on_warning]\n\n  - type: pagerduty\n    service_key: ${PAGERDUTY_KEY}\n    on_events: [on_failure]\n    severity: critical\n\n# Performance\nperformance:\n  max_concurrent_nodes: 4\n  checkpoint_interval: 100000\n  spill_to_disk: true\n\n# Quality gates\nquality:\n  quarantine_threshold: 0.05  # Alert if &gt; 5% quarantined\n  row_count_variance: 0.20     # Alert if count varies &gt; 20%\n</code></pre>"},{"location":"marketing/articles/article_24_production_pipelines/#key-takeaways","title":"Key Takeaways","text":"Area Implementation Monitoring Collect metrics, build dashboards Retry Exponential backoff for transient failures Alerting Right people, right time, right channel Testing Unit, contract, integration tests Recovery Reprocess, rollback, backfill Documentation Runbooks for common issues"},{"location":"marketing/articles/article_24_production_pipelines/#next-steps","title":"Next Steps","text":"<p>With production patterns covered, let's reflect on the journey:</p> <ul> <li>Lessons learned</li> <li>Community building</li> <li>Open source reflections</li> </ul> <p>Next article: What I Learned Building an Open Source Data Framework.</p>"},{"location":"marketing/articles/article_24_production_pipelines/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul>"},{"location":"marketing/articles/article_25_lessons_learned/","title":"What I Learned Building an Open Source Data Framework","text":"<p>Reflections on a year of building in public</p>"},{"location":"marketing/articles/article_25_lessons_learned/#tldr","title":"TL;DR","text":"<p>Building Odibi taught me more than technical skills. It taught me about constraints driving creativity, the value of documentation, the importance of real-world testing, and the power of community. This article shares the lessons learned-both technical and personal.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#why-i-built-it","title":"Why I Built It","text":"<p>I'm a solo data engineer on an analytics team. No IT support for my specific needs. No budget for enterprise tools. Just me, a laptop, and problems to solve.</p> <p>Every pipeline I built followed the same patterns: - SCD2 dimensions - Fact tables with lookups - Data quality contracts - Medallion architecture</p> <p>I was writing the same code over and over. Different projects, same patterns.</p> <p>So I extracted the patterns into a framework. Not because I wanted to build a product-because I was tired of repeating myself.</p> <p>That's how Odibi started. A tool to solve my own problems.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-1-constraints-drive-creativity","title":"Lesson 1: Constraints Drive Creativity","text":"<p>I couldn't access production databases directly. I had to work with CSV exports and API calls.</p> <p>At first, I saw this as a limitation. Now I see it as a feature.</p> <p>Because I couldn't rely on database features, I built: - Engine-agnostic patterns (Spark, Pandas, Polars) - File-based state management - Configuration-driven everything</p> <p>The constraints forced better architecture.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#the-takeaway","title":"The Takeaway","text":"<p>Don't fight constraints. Work with them. They often lead to better solutions.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-2-documentation-is-the-product","title":"Lesson 2: Documentation Is the Product","text":"<p>Early versions of Odibi had minimal docs. \"The code is self-explanatory.\"</p> <p>Nobody used it.</p> <p>Then I wrote the YAML reference. Then the pattern guides. Then the tutorials.</p> <p>Usage jumped. Not because the code changed-because people could finally understand it.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#what-i-learned","title":"What I Learned","text":"<ul> <li>Good docs = more users</li> <li>Bad docs = great code nobody uses</li> <li>Docstrings are the source of truth</li> <li>Auto-generate where possible</li> <li>Examples matter more than explanations</li> </ul>"},{"location":"marketing/articles/article_25_lessons_learned/#the-docstring-pattern","title":"The Docstring Pattern","text":"<pre><code>def dimension_pattern(\n    df: DataFrame,\n    natural_key: str,\n    surrogate_key: str,\n    scd_type: int = 1,\n    **kwargs\n) -&gt; DataFrame:\n    \"\"\"\n    Build a dimension table with surrogate key generation.\n\n    Parameters\n    ----------\n    df : DataFrame\n        Input data with dimension attributes\n    natural_key : str\n        Business key column (e.g., 'customer_id')\n    surrogate_key : str\n        Name of surrogate key to generate (e.g., 'customer_sk')\n    scd_type : int\n        0=static, 1=overwrite, 2=history tracking\n\n    Returns\n    -------\n    DataFrame\n        Dimension table with surrogate key\n\n    Example\n    -------\n    &gt;&gt;&gt; result = dimension_pattern(\n    ...     df=customers,\n    ...     natural_key='customer_id',\n    ...     surrogate_key='customer_sk',\n    ...     scd_type=2\n    ... )\n    \"\"\"\n</code></pre>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-3-real-world-testing-reveals-everything","title":"Lesson 3: Real-World Testing Reveals Everything","text":"<p>Unit tests passed. Integration tests passed. Production failed.</p> <p>Why? Because real data is weird: - NULLs where you don't expect them - Unicode characters that break parsing - Timestamps in unexpected formats - Values that exceed reasonable bounds</p>"},{"location":"marketing/articles/article_25_lessons_learned/#what-i-changed","title":"What I Changed","text":"<ol> <li>Test with production-like data: Not just happy path</li> <li>Add chaos: Inject NULLs, duplicates, out-of-range values</li> <li>Fuzz testing: Random data reveals edge cases</li> <li>Production monitoring: Track what actually fails</li> </ol>"},{"location":"marketing/articles/article_25_lessons_learned/#the-testing-pyramid","title":"The Testing Pyramid","text":"<pre><code>         /\\\n        /  \\\n       / E2E \\        \u2190 Few: Full pipeline tests\n      /--------\\\n     /          \\\n    / Integration \\   \u2190 Some: Component tests\n   /--------------\\\n  /                \\\n /    Unit Tests    \\ \u2190 Many: Function-level tests\n/--------------------\\\n</code></pre>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-4-yaml-is-a-user-interface","title":"Lesson 4: YAML Is a User Interface","text":"<p>Configuration is user experience. Bad YAML = bad UX.</p> <p>Early versions were verbose:</p> <pre><code># Old: Too verbose\ntransform:\n  steps:\n    - type: function\n      name: clean_text\n      parameters:\n        columns:\n          - name: customer_name\n            case: upper\n            trim: true\n          - name: customer_city\n            case: upper\n            trim: true\n</code></pre> <p>Current version is cleaner:</p> <pre><code># New: Readable\ntransform:\n  steps:\n    - function: clean_text\n      params:\n        columns: [customer_name, customer_city]\n        case: upper\n        trim: true\n</code></pre>"},{"location":"marketing/articles/article_25_lessons_learned/#yaml-design-principles","title":"YAML Design Principles","text":"<ol> <li>Minimize nesting: 3 levels max</li> <li>Sensible defaults: Only specify what differs</li> <li>Consistent naming: <code>params</code>, not <code>parameters</code> or <code>config</code></li> <li>Clear errors: Show exactly what's wrong</li> </ol>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-5-start-with-one-engine-expand-later","title":"Lesson 5: Start With One Engine, Expand Later","text":"<p>I wanted to support Spark, Pandas, Polars, and DuckDB from day one.</p> <p>Mistake.</p> <p>Each engine has quirks: - Spark: Lazy evaluation, distributed - Pandas: Eager, single-node - Polars: Lazy by default, different API - DuckDB: SQL-first</p> <p>I spent weeks on compatibility instead of features.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#better-approach","title":"Better Approach","text":"<ol> <li>Pick one engine: I chose Pandas for dev speed</li> <li>Build all patterns: Get it working first</li> <li>Add engines incrementally: Spark second, Polars third</li> <li>Abstract at boundaries: Engine-specific code isolated</li> </ol>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-6-patterns-over-features","title":"Lesson 6: Patterns Over Features","text":"<p>I kept wanting to add features: - Support for streaming - Real-time ingestion - ML model serving - ...</p> <p>But the core patterns weren't solid yet.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#what-i-focused-on-instead","title":"What I Focused On Instead","text":"<ol> <li>Dimension pattern: Works perfectly</li> <li>Fact pattern: Handles all edge cases</li> <li>SCD2: Robust and tested</li> <li>Contracts: Catches real issues</li> </ol> <p>Features without solid patterns = shaky foundation.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-7-build-for-yourself-first","title":"Lesson 7: Build for Yourself First","text":"<p>Some advice says \"talk to users before building.\"</p> <p>I was my own user. I knew exactly what I needed: - Less boilerplate - Declarative configuration - Patterns I could trust</p> <p>Only after solving my problems did I share it.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#the-result","title":"The Result","text":"<p>The framework works because it solves real problems I actually had. Not theoretical problems. Real ones.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-8-community-matters","title":"Lesson 8: Community Matters","text":"<p>When I shared Odibi publicly: - Strangers filed bug reports - Contributors suggested improvements - Users shared use cases I hadn't imagined</p> <p>The framework improved faster with community input than it ever did in isolation.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#what-helped","title":"What Helped","text":"<ul> <li>Clear contribution guide</li> <li>Responsive to issues</li> <li>Public roadmap</li> <li>Welcoming attitude</li> </ul>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-9-incremental-progress-compounds","title":"Lesson 9: Incremental Progress Compounds","text":"<p>I couldn't work on Odibi full-time. Maybe an hour a day. Sometimes less.</p> <p>But an hour a day, every day, for a year = 365 hours.</p> <p>That's enough to build something real.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#the-compounding-effect","title":"The Compounding Effect","text":"Month Cumulative Hours State 1 30 Basic structure 3 90 Core patterns working 6 180 Documentation complete 12 365 Production-ready <p>Small, consistent effort beats sporadic bursts.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#lesson-10-sharing-is-learning","title":"Lesson 10: Sharing Is Learning","text":"<p>Writing this article series forced me to: - Organize my knowledge - Fill gaps in my understanding - Explain things clearly</p> <p>Teaching is learning.</p> <p>Every article I wrote, I learned something new about my own work.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#what-id-do-differently","title":"What I'd Do Differently","text":""},{"location":"marketing/articles/article_25_lessons_learned/#start-with-tests","title":"Start With Tests","text":"<p>I added tests after the fact. Some patterns were hard to test because they weren't designed for it.</p> <p>Next time: Test-first design.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#document-as-i-build","title":"Document As I Build","text":"<p>I documented after building. Details were forgotten.</p> <p>Next time: Docstrings before implementation.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#release-earlier","title":"Release Earlier","text":"<p>I waited until it was \"ready.\" Ready is never.</p> <p>Next time: Release as soon as it solves one problem.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#advice-for-solo-builders","title":"Advice for Solo Builders","text":"<ol> <li>Solve your own problem first: You understand it best</li> <li>Document obsessively: Future you (and others) will thank you</li> <li>Test with real data: Happy path tests lie</li> <li>Share early: Feedback is gold</li> <li>Compound progress: Small daily effort adds up</li> <li>Embrace constraints: They force creativity</li> </ol>"},{"location":"marketing/articles/article_25_lessons_learned/#whats-next","title":"What's Next","text":"<p>Odibi is far from done. The roadmap includes: - Streaming support - More engine integrations - Better error messages - Visual pipeline designer</p> <p>But the core is solid. It solves real problems. It's used in production.</p> <p>That's enough for now.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#thank-you","title":"Thank You","text":"<p>To everyone who: - Read these articles - Filed issues - Contributed code - Shared feedback</p> <p>You made this better.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#final-thoughts","title":"Final Thoughts","text":"<p>Building in public is scary. Your mistakes are visible. Your progress is slow. Your code is imperfect.</p> <p>But the alternative-building in private-means no feedback, no community, no improvement.</p> <p>I'd rather build imperfectly in public than perfectly in private.</p>"},{"location":"marketing/articles/article_25_lessons_learned/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for more articles in this series</li> </ul> <p>Next article: The final summary-a complete index of everything we covered.</p>"},{"location":"marketing/articles/article_26_complete_index/","title":"6 Months of Data Engineering Content: A Complete Index","text":"<p>Everything we covered, organized for reference</p>"},{"location":"marketing/articles/article_26_complete_index/#tldr","title":"TL;DR","text":"<p>This article indexes all 26 articles in the series. Use it to find specific topics, revisit concepts, or share with colleagues. Every article is linked with a one-line summary.</p>"},{"location":"marketing/articles/article_26_complete_index/#the-journey","title":"The Journey","text":"<p>Over 6 months, we built a complete data warehouse from scratch:</p> <pre><code>Week 1-2:   Credibility (who I am, why I built this)\nWeek 3-10:  Building (Bronze \u2192 Silver \u2192 Gold)\nWeek 11-16: Deep Dives (patterns, anti-patterns)\nWeek 17-22: Troubleshooting (debugging, performance)\nWeek 23-26: Advanced (semantic layer, production, reflections)\n</code></pre>"},{"location":"marketing/articles/article_26_complete_index/#phase-1-credibility-weeks-1-2","title":"Phase 1: Credibility (Weeks 1-2)","text":"<p>Building trust before building audience.</p> # Article Summary 1 How I Taught Myself Data Engineering While Working Night Shifts Origin story: 1000 hours of self-study, rotational program background 2 The Bronze Layer Mistake That Cost Me 6 Months of Data Why Bronze should never transform data 3 Setting Up a Bronze Layer with Delta Lake Complete Bronze config for 8 CSV files"},{"location":"marketing/articles/article_26_complete_index/#phase-2-building-in-public-weeks-3-10","title":"Phase 2: Building in Public (Weeks 3-10)","text":"<p>The core technical series.</p> # Article Summary 4 Data Quality Contracts: Catching Problems Before Production Pre-conditions, severity levels, quarantine pattern 5 Complete Silver Layer Configuration Guide Full configs for all 8 tables with dedup, validation 6 Facts vs Dimensions: A Practical Guide Restaurant analogy, identifying facts and dimensions 7 Building a Date Dimension from Scratch Fiscal calendars, unknown member, generated columns 8 Fact Table Patterns: Lookups, Orphans, and Measures Surrogate key lookups, orphan handling, measure definitions 9 From CSV to Star Schema: Complete Walkthrough End-to-end: 8 CSVs \u2192 Bronze \u2192 Silver \u2192 Gold 10 Introducing Odibi: Declarative Data Pipelines Framework overview, core concepts, getting started"},{"location":"marketing/articles/article_26_complete_index/#phase-3-pattern-deep-dives-weeks-11-16","title":"Phase 3: Pattern Deep Dives (Weeks 11-16)","text":"<p>Mastering the patterns.</p> # Article Summary 11 SCD2 Complete Guide: When and How to Track History When to use SCD2, configuration, common gotchas 12 Dimension Table Patterns: Unknown Members, Keys, and Auditing Conformed dimensions, role-playing, junk dimensions 13 Fact Table Design: Grain, Measures, and Validation Grain decisions, measure types, factless facts 14 Pre-Aggregation Strategies for Fast Dashboards When to aggregate, incremental aggregation 15 Data Quality Patterns: Contracts, Validation, and Quarantine Layer-by-layer strategy, metrics, testing 16 Incremental Loading: Watermarks, Merge, and Append Three patterns, late-arriving data, state management"},{"location":"marketing/articles/article_26_complete_index/#phase-4-anti-patterns-troubleshooting-weeks-17-22","title":"Phase 4: Anti-Patterns &amp; Troubleshooting (Weeks 17-22)","text":"<p>Learning from mistakes.</p> # Article Summary 17 Bronze Layer Anti-Patterns: 3 Mistakes That Will Haunt You Transforming, overwriting, skipping metadata 18 Silver Layer Best Practices: Centralize, Validate, Deduplicate SSOT, validation strategy, dedup patterns 19 SCD2 Done Wrong: History Explosion and How to Prevent It Volatile columns, diagnosis, fixing exploded dimensions 20 Performance Anti-Patterns: Why Your Pipeline Takes Hours Shuffles, skew, collect, partitioning 21 Configuration Patterns for Multi-Environment Pipelines Variables, environment overrides, secret management 22 Debugging Data Pipelines: A Systematic Approach 7-step process, log analysis, recovery"},{"location":"marketing/articles/article_26_complete_index/#phase-5-advanced-community-weeks-23-26","title":"Phase 5: Advanced &amp; Community (Weeks 23-26)","text":"<p>Taking it to production.</p> # Article Summary 23 Building a Semantic Layer: Metrics Everyone Can Trust Define metrics once, query everywhere 24 Production Data Pipelines: Monitoring, Retry, and Testing Operational patterns for reliability 25 What I Learned Building an Open Source Data Framework Lessons learned, advice for builders 26 6 Months of Data Engineering Content: A Complete Index This article-the complete reference"},{"location":"marketing/articles/article_26_complete_index/#topic-index","title":"Topic Index","text":"<p>Find articles by topic:</p>"},{"location":"marketing/articles/article_26_complete_index/#architecture","title":"Architecture","text":"<ul> <li>Medallion architecture: Articles 2, 3, 5, 9</li> <li>Star schema: Articles 6, 7, 8, 9</li> <li>Layered approach: Articles 17, 18</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#bronze-layer","title":"Bronze Layer","text":"<ul> <li>Setup: Article 3</li> <li>Anti-patterns: Article 17</li> <li>Metadata: Articles 2, 3, 17</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#silver-layer","title":"Silver Layer","text":"<ul> <li>Complete guide: Article 5</li> <li>Best practices: Article 18</li> <li>Validation: Articles 4, 5, 15</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#gold-layer","title":"Gold Layer","text":"<ul> <li>Facts vs dimensions: Article 6</li> <li>Date dimension: Article 7</li> <li>Fact patterns: Articles 8, 13</li> <li>Aggregation: Article 14</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#dimensions","title":"Dimensions","text":"<ul> <li>Basics: Article 6</li> <li>Date dimension: Article 7</li> <li>SCD2: Articles 11, 19</li> <li>Patterns: Article 12</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#facts","title":"Facts","text":"<ul> <li>Basics: Article 6</li> <li>Patterns: Article 8</li> <li>Design: Article 13</li> <li>Factless facts: Article 13</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#data-quality","title":"Data Quality","text":"<ul> <li>Contracts: Articles 4, 15</li> <li>Validation: Articles 5, 15</li> <li>Quarantine: Articles 4, 15</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#scd2","title":"SCD2","text":"<ul> <li>Complete guide: Article 11</li> <li>Anti-patterns: Article 19</li> <li>When NOT to use: Articles 11, 19</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#performance","title":"Performance","text":"<ul> <li>Aggregation: Article 14</li> <li>Anti-patterns: Article 20</li> <li>Incremental loading: Article 16</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#operations","title":"Operations","text":"<ul> <li>Configuration: Article 21</li> <li>Debugging: Article 22</li> <li>Production: Article 24</li> <li>Monitoring: Article 24</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#advanced","title":"Advanced","text":"<ul> <li>Semantic layer: Article 23</li> <li>Multi-environment: Article 21</li> </ul>"},{"location":"marketing/articles/article_26_complete_index/#configuration-quick-reference","title":"Configuration Quick Reference","text":"<p>Common patterns you can copy:</p>"},{"location":"marketing/articles/article_26_complete_index/#bronze-node","title":"Bronze Node","text":"<pre><code>- name: bronze_orders\n  read:\n    connection: landing\n    path: orders.csv\n    format: csv\n  transform:\n    steps:\n      - function: derive_columns\n        params:\n          columns:\n            _extracted_at: \"current_timestamp()\"\n            _source_file: \"'orders.csv'\"\n  write:\n    connection: bronze\n    path: orders\n    format: delta\n    mode: append\n</code></pre>"},{"location":"marketing/articles/article_26_complete_index/#silver-node-with-contracts","title":"Silver Node with Contracts","text":"<pre><code>- name: silver_orders\n  contracts:\n    - type: not_null\n      column: order_id\n      severity: error\n    - type: unique\n      columns: [order_id]\n      severity: error\n  transformer: deduplicate\n  params:\n    keys: [order_id]\n    order_by: _extracted_at DESC\n  write:\n    connection: silver\n    path: orders\n    format: delta\n</code></pre>"},{"location":"marketing/articles/article_26_complete_index/#dimension-with-scd2","title":"Dimension with SCD2","text":"<pre><code>- name: dim_customer\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols: [customer_city, customer_state]\n      target: gold.dim_customer\n      unknown_member: true\n</code></pre>"},{"location":"marketing/articles/article_26_complete_index/#fact-with-lookups","title":"Fact with Lookups","text":"<pre><code>- name: fact_order_items\n  pattern:\n    type: fact\n    params:\n      grain: [order_id, order_item_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          surrogate_key: customer_sk\n      orphan_handling: unknown\n      measures:\n        - price\n        - quantity\n</code></pre>"},{"location":"marketing/articles/article_26_complete_index/#date-dimension","title":"Date Dimension","text":"<pre><code>- name: dim_date\n  pattern:\n    type: date_dimension\n    params:\n      start_date: \"2020-01-01\"\n      end_date: \"2030-12-31\"\n      fiscal_year_start_month: 7\n      unknown_member: true\n</code></pre>"},{"location":"marketing/articles/article_26_complete_index/#key-concepts","title":"Key Concepts","text":"Concept Definition Article Medallion Architecture Bronze \u2192 Silver \u2192 Gold layering 2, 3 SCD2 Slowly Changing Dimension Type 2 (history tracking) 11 Surrogate Key Warehouse-generated integer for joins 6, 8 Natural Key Business identifier from source 6 Grain Level of detail in a fact table 13 Contracts Pre-conditions on data quality 4 Quarantine Routing bad data for review 4, 15 Unknown Member Dimension row for orphan handling 7, 12 Conformed Dimension Single dimension used across facts 12 Semantic Layer Centralized metric definitions 23"},{"location":"marketing/articles/article_26_complete_index/#the-brazilian-e-commerce-dataset","title":"The Brazilian E-Commerce Dataset","text":"<p>Throughout the series, we used the Olist Brazilian E-Commerce dataset:</p> Table Rows Purpose orders 99,441 Order headers order_items 112,650 Line items customers 99,441 Customer info products 32,951 Product catalog sellers 3,095 Seller info payments 103,886 Payment details reviews 100,000 Customer reviews geolocation 1,000,163 Zip code data"},{"location":"marketing/articles/article_26_complete_index/#whats-next","title":"What's Next?","text":"<p>This series covered the fundamentals. Future content might explore:</p> <ul> <li>Streaming: Real-time data pipelines</li> <li>ML Integration: Feature stores and model serving</li> <li>Data Mesh: Decentralized data ownership</li> <li>Advanced Spark: Optimization deep dives</li> <li>Cloud Patterns: AWS, Azure, GCP specifics</li> </ul> <p>Follow for updates.</p>"},{"location":"marketing/articles/article_26_complete_index/#thank-you","title":"Thank You","text":"<p>26 articles. 6 months. One complete data engineering education.</p> <p>Thank you for reading, sharing, and building with me.</p>"},{"location":"marketing/articles/article_26_complete_index/#connect","title":"Connect","text":"<ul> <li>LinkedIn: [Your LinkedIn URL]</li> <li>GitHub: [Odibi Repository URL]</li> <li>Medium: Follow for future content</li> </ul> <p>This completes the 6-month series. See you in the next one.</p>"},{"location":"patterns/","title":"Odibi Data Patterns","text":"<p>This directory contains documentation for common data pipeline patterns used in Odibi. Each pattern solves a specific problem and includes step-by-step examples.</p>"},{"location":"patterns/#patterns","title":"Patterns","text":""},{"location":"patterns/#1-append-only-raw-layer","title":"1. Append-Only Raw Layer","text":"<p>Problem: How do I safely ingest data without losing audit trails?</p> <p>Pattern: All data from sources is appended to the Raw layer without modification. Raw is immutable and append-only.</p> <p>When to use: Always, for all source ingestion. Raw is your safety net.</p>"},{"location":"patterns/#2-high-water-mark-smart-read","title":"2. High Water Mark (Smart Read)","text":"<p>Problem: My source table has millions of rows. How do I efficiently read only new/changed data?</p> <p>Pattern: Use the <code>incremental</code> configuration (Smart Read) to automatically filter the source query. Odibi manages the state for you: First Run = Full Load, Subsequent Runs = Incremental Load.</p> <p>When to use: When your source has timestamps (created_at, updated_at) and you want incremental reads. Essential for daily incremental loads.</p> <p>See also: Manual HWM Guide - understanding the underlying SQL pattern.</p>"},{"location":"patterns/#3-mergeupsert-silver-layer","title":"3. Merge/Upsert (Silver Layer)","text":"<p>Problem: How do I deduplicate and keep the latest version of each record?</p> <p>Pattern: Use Delta Lake's MERGE operation (or Merge Transformer) to upsert records by key, with audit columns tracking created/updated timestamps.</p> <p>When to use: Refining Raw \u2192 Silver. Always use for stateful transformations.</p>"},{"location":"patterns/#4-scd-type-2-history-tracking","title":"4. SCD Type 2 (History Tracking)","text":"<p>Problem: \"I need to know what the address was last month, not just now.\"</p> <p>Pattern: Track full history. Old records are closed (valid_to set), new records are opened (valid_to NULL). Preserves point-in-time accuracy.</p> <p>When to use: Slowly Changing Dimensions (Customer address, Product category).</p>"},{"location":"patterns/#5-windowed-reprocess-gold-layer-aggregates","title":"5. Windowed Reprocess (Gold Layer Aggregates)","text":"<p>Problem: Late-arriving data can break my aggregates. How do I fix them without double-counting?</p> <p>Pattern: Instead of patching aggregates with updates, recalculate the entire time window and overwrite that partition.</p> <p>When to use: Building Gold-layer aggregates (KPIs, star schemas). Ensures idempotency and correctness.</p>"},{"location":"patterns/#6-skip-if-unchanged-snapshot-optimization","title":"6. Skip If Unchanged (Snapshot Optimization)","text":"<p>Problem: My hourly pipeline appends identical data 24 times/day when the source hasn't changed.</p> <p>Pattern: Compute a hash of the DataFrame content before writing. If hash matches previous write, skip the append entirely.</p> <p>When to use: Snapshot tables without timestamps, reference data that changes infrequently, or when change frequency is unknown.</p>"},{"location":"patterns/#dimensional-modeling-patterns","title":"Dimensional Modeling Patterns","text":"<p>These patterns are designed for building star schemas and data warehouses. Use them via <code>transformer: pattern_name</code> in your node config.</p>"},{"location":"patterns/#7-dimension-pattern","title":"7. Dimension Pattern","text":"<p>Problem: How do I build dimension tables with surrogate keys and SCD support?</p> <p>Pattern: Use <code>transformer: dimension</code> to auto-generate surrogate keys and handle SCD Type 0/1/2 with optional unknown member rows.</p> <p>When to use: Building any dimension table (dim_customer, dim_product, etc.)</p> <pre><code>transformer: dimension\nparams:\n  natural_key: customer_id\n  surrogate_key: customer_sk\n  scd_type: 2\n  track_cols: [name, email, address]\n</code></pre>"},{"location":"patterns/#8-date-dimension-pattern","title":"8. Date Dimension Pattern","text":"<p>Problem: How do I generate a complete date dimension with fiscal calendars?</p> <p>Pattern: Use <code>transformer: date_dimension</code> to generate dates with 19 pre-calculated columns including fiscal year/quarter.</p> <p>When to use: Every data warehouse needs a date dimension. Generate once with a wide range (2015-2035).</p> <pre><code>transformer: date_dimension\nparams:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 7\n  unknown_member: true\n</code></pre>"},{"location":"patterns/#9-fact-pattern","title":"9. Fact Pattern","text":"<p>Problem: How do I build fact tables with automatic surrogate key lookups?</p> <p>Pattern: Use <code>transformer: fact</code> to join source data to dimensions, retrieve SKs, handle orphans, and validate grain.</p> <p>When to use: Building any fact table that references dimensions.</p> <pre><code>transformer: fact\nparams:\n  grain: [order_id, line_item_id]\n  dimensions:\n    - source_column: customer_id\n      dimension_table: dim_customer\n      dimension_key: customer_id\n      surrogate_key: customer_sk\n  orphan_handling: unknown\n</code></pre>"},{"location":"patterns/#10-aggregation-pattern","title":"10. Aggregation Pattern","text":"<p>Problem: How do I build aggregate tables with declarative GROUP BY and incremental refresh?</p> <p>Pattern: Use <code>transformer: aggregation</code> with grain (GROUP BY) and measure expressions.</p> <p>When to use: Building aggregate/summary tables, KPI tables, or materializing metrics.</p> <pre><code>transformer: aggregation\nparams:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n</code></pre>"},{"location":"patterns/#design-principles","title":"Design Principles","text":"<p>These patterns are built on the Odibi Architecture Manifesto:</p> <ol> <li>Robots Remember, Humans Forget \u2192 Use checkpoint bookkeeping, not manual state tracking</li> <li>Raw is Sacred \u2192 Append-only, immutable history. Never destroy original data.</li> <li>Rebuild the Bucket, Don't Patch the Hole \u2192 Reprocess entire time windows, don't patch aggregates</li> <li>SQL is for Humans, ADLS is for Robots \u2192 ADLS stores everything; SQL serves BI</li> <li>No Duplication \u2192 Test against production data; don't duplicate datasets</li> </ol>"},{"location":"patterns/#quick-reference","title":"Quick Reference","text":"Pattern Input Output Write Mode Idempotent? Append-Only Raw Source Raw <code>append</code> Yes (duplicates OK) High Water Mark Source + Timestamp Raw <code>append</code> Yes (filtered by timestamp) Smart Read Source + Timestamp Raw <code>append</code> Yes (auto-managed) Merge/Upsert Raw (micro-batch) Silver <code>merge</code> Yes (by key) SCD Type 2 Raw (micro-batch) Silver/Gold <code>overwrite</code> Yes (full history) Windowed Reprocess Silver (window) Gold <code>overwrite</code> (partition) Yes (recalculated) Skip If Unchanged Snapshot Source Raw <code>append</code> (conditional) Yes (hash-based) Dimension Staging Gold (dim_*) <code>overwrite</code> Yes (SK-based) Date Dimension Generated Gold (dim_date) <code>overwrite</code> Yes (no input) Fact Staging + Dims Gold (fact_*) <code>overwrite</code> Yes (grain-based) Aggregation Fact Gold (agg_*) <code>overwrite</code> Yes (grain-based)"},{"location":"patterns/#further-reading","title":"Further Reading","text":"<ul> <li>Databricks: \"Incremental Processing\" documentation</li> <li>Book: Fundamentals of Data Engineering by Joe Reis &amp; Matt Housley</li> </ul>"},{"location":"patterns/aggregation/","title":"Aggregation Pattern","text":"<p>The <code>aggregation</code> pattern provides declarative aggregation with configurable grain, measures, and incremental merge strategies.</p>"},{"location":"patterns/aggregation/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The aggregation pattern works on data from the read block or a dependency, applies GROUP BY aggregation, and writes the results.</p> <pre><code>project: sales_analytics\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    nodes:\n      - name: agg_daily_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n\n        transformer: aggregation\n        params:\n          grain: [date_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          having: \"COUNT(*) &gt; 0\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#features","title":"Features","text":"<ul> <li>Declarative grain (GROUP BY columns)</li> <li>Flexible measure expressions (SUM, COUNT, AVG, etc.)</li> <li>Incremental aggregation (merge new data with existing)</li> <li>HAVING clause support</li> <li>Audit columns</li> </ul>"},{"location":"patterns/aggregation/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>grain</code> list Yes - Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes - Measure definitions with name and expr <code>having</code> str No - Optional HAVING clause <code>incremental</code> dict No - Incremental merge configuration <code>target</code> str For incremental - Target table for incremental merge <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/aggregation/#measure-definition","title":"Measure Definition","text":"<pre><code>params:\n  measures:\n    - name: total_revenue      # Output column name\n      expr: \"SUM(line_total)\"  # SQL aggregation expression\n</code></pre>"},{"location":"patterns/aggregation/#incremental-config","title":"Incremental Config","text":"<pre><code>params:\n  incremental:\n    timestamp_column: order_date  # Column to identify new data\n    merge_strategy: replace       # \"replace\" or \"sum\"\n  target: warehouse.agg_daily_sales\n</code></pre>"},{"location":"patterns/aggregation/#measure-expressions","title":"Measure Expressions","text":"<p>Use standard SQL aggregation functions:</p> <pre><code>params:\n  measures:\n    # Basic aggregations\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n\n    - name: max_order\n      expr: \"MAX(line_total)\"\n\n    # Complex expressions\n    - name: total_with_discount\n      expr: \"SUM(line_total - discount_amount)\"\n\n    - name: discount_rate\n      expr: \"SUM(discount_amount) / SUM(line_total)\"\n</code></pre>"},{"location":"patterns/aggregation/#incremental-merge-strategies","title":"Incremental Merge Strategies","text":""},{"location":"patterns/aggregation/#replace-strategy","title":"Replace Strategy","text":"<p>New aggregates overwrite existing for matching grain keys:</p> <pre><code>nodes:\n  - name: agg_daily_sales\n    read:\n      connection: warehouse\n      path: fact_orders\n    transformer: aggregation\n    params:\n      grain: [date_sk, product_sk]\n      measures:\n        - name: total_revenue\n          expr: \"SUM(line_total)\"\n      incremental:\n        timestamp_column: order_date\n        merge_strategy: replace\n      target: warehouse.agg_daily_sales\n    write:\n      connection: warehouse\n      path: agg_daily_sales\n      mode: overwrite\n</code></pre> <p>Use case: Full recalculation of affected grains (idempotent, handles late data)</p>"},{"location":"patterns/aggregation/#sum-strategy","title":"Sum Strategy","text":"<p>Add new measure values to existing aggregates:</p> <pre><code>params:\n  incremental:\n    timestamp_column: order_date\n    merge_strategy: sum\n</code></pre> <p>Use case: Additive metrics only (counts, sums) where data is append-only</p> <p>Warning: Don't use for AVG, DISTINCT counts, or ratios.</p>"},{"location":"patterns/aggregation/#time-rollups","title":"Time Rollups","text":"<p>Build aggregate hierarchies at multiple time grains:</p> <pre><code>pipelines:\n  - pipeline: build_aggregates\n    nodes:\n      # Daily aggregate (from fact)\n      - name: agg_daily_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n        transformer: aggregation\n        params:\n          grain: [date_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          mode: overwrite\n\n      # Monthly rollup (from daily aggregate)\n      - name: agg_monthly_sales\n        depends_on: [agg_daily_sales]\n        transform:\n          steps:\n            - sql: |\n                SELECT \n                  FLOOR(date_sk / 100) AS month_sk,\n                  product_sk,\n                  total_revenue,\n                  order_count\n                FROM df\n        transformer: aggregation\n        params:\n          grain: [month_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(total_revenue)\"\n            - name: order_count\n              expr: \"SUM(order_count)\"\n        write:\n          connection: warehouse\n          path: agg_monthly_sales\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#full-yaml-example","title":"Full YAML Example","text":"<p>Complete aggregation pipeline with incremental refresh:</p> <pre><code>project: sales_analytics\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_aggregates\n    description: \"Build daily and monthly sales aggregates\"\n    nodes:\n      - name: agg_daily_product_sales\n        description: \"Daily product sales aggregate\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n        transformer: aggregation\n        params:\n          grain:\n            - date_sk\n            - product_sk\n            - region\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: total_cost\n              expr: \"SUM(cost_amount)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: units_sold\n              expr: \"SUM(quantity)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n            - name: avg_unit_price\n              expr: \"AVG(unit_price)\"\n          having: \"SUM(line_total) &gt; 0\"\n          incremental:\n            timestamp_column: load_timestamp\n            merge_strategy: replace\n          target: warehouse.agg_daily_product_sales\n          audit:\n            load_timestamp: true\n            source_system: \"aggregation_pipeline\"\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#see-also","title":"See Also","text":"<ul> <li>Fact Pattern - Build fact tables</li> <li>Semantic Layer - Define metrics for ad-hoc queries</li> <li>Materializing Metrics - Schedule metric materialization</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/anti_patterns/","title":"\u274c Anti-Patterns: What NOT to Do","text":"<p>This guide documents common mistakes when building data pipelines. For each anti-pattern, we show what NOT to do, why it's bad, and the correct approach.</p> <p>Learning what NOT to do is just as important as learning what TO do.</p>"},{"location":"patterns/anti_patterns/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Transforming in Bronze Layer</li> <li>Not Deduplicating Before SCD2</li> <li>Using SCD2 for Fact Tables</li> <li>Hardcoding Paths Instead of Connections</li> <li>Not Handling NULLs in Key Columns</li> <li>Mixing Business Logic Across Layers</li> <li>Skipping the Silver Layer</li> <li>Not Adding Extracted Timestamps in Bronze</li> <li>Using Append Mode Without Deduplication</li> <li>Ignoring Schema Evolution</li> </ol>"},{"location":"patterns/anti_patterns/#1-transforming-in-bronze-layer","title":"1. Transforming in Bronze Layer","text":""},{"location":"patterns/anti_patterns/#what-not-to-do","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Cleaning data in the Bronze layer\npipelines:\n  - pipeline: \"bronze_customers\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_customers\"\n        read:\n          connection: landing\n          path: customers.csv\n\n        # \u274c DON'T transform in Bronze!\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE status != 'inactive'\"\n            - function: \"clean_text\"\n              params: { columns: [\"name\", \"email\"] }\n\n        write:\n          connection: bronze\n          path: customers\n          format: delta\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad","title":"Why It's Bad","text":"<p>You lose the original data forever. </p> <p>Imagine this scenario: 1. You filter out \"inactive\" customers in Bronze 2. 6 months later, business says \"We need to analyze inactive customers too\" 3. You can't\u2014because you threw that data away</p> <p>Bronze is your \"undo button.\" If you transform data there, you lose the ability to go back to the source.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Keep Bronze raw, transform in Silver\npipelines:\n  # Step 1: Bronze - Store raw data exactly as received\n  - pipeline: \"bronze_customers\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_customers\"\n        read:\n          connection: landing\n          path: customers.csv\n\n        # No transformations! Just land the data\n        write:\n          connection: bronze\n          path: customers\n          format: delta\n          mode: append\n\n  # Step 2: Silver - Now you can transform\n  - pipeline: \"silver_customers\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_customers\"\n        read:\n          connection: bronze\n          path: customers\n\n        # \u2705 Transform in Silver\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE status != 'inactive'\"\n            - function: \"clean_text\"\n              params: { columns: [\"name\", \"email\"] }\n\n        write:\n          connection: silver\n          path: dim_customers\n          format: delta\n</code></pre>"},{"location":"patterns/anti_patterns/#the-one-exception","title":"\ud83d\udca1 The One Exception","text":"<p>You MAY add metadata columns in Bronze (they don't alter original data):</p> <pre><code># OK: Adding metadata in Bronze\ntransform:\n  steps:\n    - function: \"derive_columns\"\n      params:\n        columns:\n          _extracted_at: \"current_timestamp()\"\n          _source_file: \"'customers.csv'\"\n</code></pre>"},{"location":"patterns/anti_patterns/#2-not-deduplicating-before-scd2","title":"2. Not Deduplicating Before SCD2","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_1","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Source has duplicates, feeding directly to SCD2\nnodes:\n  - name: \"dim_customers\"\n    read:\n      connection: bronze\n      path: customers  # Contains duplicate customer_id rows!\n\n    # \u274c SCD2 without deduplication\n    transformer: \"scd2\"\n    params:\n      target: silver.dim_customers\n      keys: [\"customer_id\"]\n      track_cols: [\"name\", \"email\", \"address\"]\n      effective_time_col: \"updated_at\"\n\n    write:\n      connection: silver\n      table: dim_customers\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_1","title":"Why It's Bad","text":"<p>Your history table explodes with duplicate versions.</p> <p>If your source has:</p> <pre><code>customer_id | name  | updated_at\n101         | Alice | 2024-01-01 10:00:00\n101         | Alice | 2024-01-01 10:00:01  &lt;- Duplicate from same extract\n101         | Alice | 2024-01-01 10:00:02  &lt;- Another duplicate\n</code></pre> <p>SCD2 sees three \"changes\" and creates three history rows, even though nothing actually changed:</p> <pre><code>customer_id | name  | effective_time          | end_time               | is_current\n101         | Alice | 2024-01-01 10:00:00     | 2024-01-01 10:00:01    | false\n101         | Alice | 2024-01-01 10:00:01     | 2024-01-01 10:00:02    | false  \n101         | Alice | 2024-01-01 10:00:02     | NULL                   | true\n</code></pre> <p>Your dimension table grows 3x faster than it should, wasting storage and slowing queries.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_1","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Deduplicate first, then SCD2\nnodes:\n  - name: \"dedup_customers\"\n    read:\n      connection: bronze\n      path: customers\n\n    # \u2705 Deduplicate first - keep most recent per customer\n    transformer: \"deduplicate\"\n    params:\n      keys: [\"customer_id\"]\n      order_by: \"updated_at DESC\"\n\n    write:\n      connection: staging\n      path: customers_deduped\n\n  - name: \"dim_customers\"\n    depends_on: [\"dedup_customers\"]\n\n    # \u2705 Now SCD2 sees clean data\n    transformer: \"scd2\"\n    params:\n      target: silver.dim_customers\n      keys: [\"customer_id\"]\n      track_cols: [\"name\", \"email\", \"address\"]\n      effective_time_col: \"updated_at\"\n\n    write:\n      connection: silver\n      table: dim_customers\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/anti_patterns/#3-using-scd2-for-fact-tables","title":"3. Using SCD2 for Fact Tables","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_2","title":"\u274c What NOT to Do","text":"<pre><code># BAD: SCD2 on a fact table\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: bronze\n      path: orders\n\n    # \u274c DON'T use SCD2 for facts!\n    transformer: \"scd2\"\n    params:\n      target: silver.fact_orders\n      keys: [\"order_id\"]\n      track_cols: [\"quantity\", \"total_amount\"]\n      effective_time_col: \"order_date\"\n\n    write:\n      connection: silver\n      table: fact_orders\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_2","title":"Why It's Bad","text":"<p>Facts don't change\u2014they happen.</p> <p>An order is an event. Customer 101 placed order #5001 on January 15th for $99.00. That's a historical fact. It doesn't \"change.\"</p> <p>If the source shows a different amount for the same order, that's either: 1. A correction (handle with a correction fact, not by changing history) 2. A data quality issue (should be caught by validation)</p> <p>Using SCD2 on facts: - Bloats your table unnecessarily - Creates confusing history for immutable events - Slows down analytical queries</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_2","title":"\u2705 What to Do Instead","text":"<p>For fact tables, use append mode (for new records) or merge mode (for late-arriving corrections):</p> <pre><code># GOOD: Append mode for facts\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: bronze\n      path: orders\n\n    # \u2705 Just append new orders\n    write:\n      connection: silver\n      table: fact_orders\n      format: delta\n      mode: append\n\n# OR: Merge mode if you expect corrections\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: bronze\n      path: orders\n\n    # \u2705 Merge handles late corrections\n    transformer: \"merge\"\n    params:\n      target: silver.fact_orders\n      keys: [\"order_id\"]\n      # Updates existing, inserts new\n\n    write:\n      connection: silver\n      table: fact_orders\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/anti_patterns/#when-to-use-each","title":"\ud83d\udca1 When to Use Each","text":"Scenario Pattern New orders arriving daily <code>append</code> Orders may be corrected later <code>merge</code> Customer info changes over time <code>scd2</code>"},{"location":"patterns/anti_patterns/#4-hardcoding-paths-instead-of-connections","title":"4. Hardcoding Paths Instead of Connections","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_3","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Hardcoded paths everywhere\nnodes:\n  - name: \"load_sales\"\n    read:\n      # \u274c Hardcoded path - breaks when moving to prod\n      path: \"abfss://raw@devstorageaccount.dfs.core.windows.net/sales/2024/\"\n\n    write:\n      # \u274c Another hardcoded path\n      path: \"abfss://bronze@devstorageaccount.dfs.core.windows.net/sales\"\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_3","title":"Why It's Bad","text":"<p>Your pipeline breaks when moving between environments.</p> <p>Development, staging, and production have different: - Storage account names - Credentials - Base paths</p> <p>If you hardcode paths, you need to edit the YAML for every environment. This leads to: - Copy-paste errors - Secrets accidentally committed to git - \"It works on my machine\" syndrome</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_3","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Use connections with environment variables\nconnections:\n  landing:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}  # From environment\n    container: raw\n    credential: ${STORAGE_KEY}   # Secret from Key Vault\n\n  bronze:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    container: bronze\n    credential: ${STORAGE_KEY}\n\npipelines:\n  - pipeline: \"load_sales\"\n    nodes:\n      - name: \"ingest_sales\"\n        read:\n          # \u2705 Use connection name + relative path\n          connection: landing\n          path: sales/2024/\n\n        write:\n          # \u2705 Portable across environments\n          connection: bronze\n          path: sales\n</code></pre> <p>Now the same YAML works in dev, staging, and prod\u2014just change the environment variables.</p>"},{"location":"patterns/anti_patterns/#5-not-handling-nulls-in-key-columns","title":"5. Not Handling NULLs in Key Columns","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_4","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Joining on columns that might be NULL\nnodes:\n  - name: \"enrich_orders\"\n    read:\n      connection: silver\n      path: fact_orders  # customer_id can be NULL!\n\n    # \u274c Join without NULL handling\n    transformer: \"join\"\n    params:\n      right: silver.dim_customer\n      on: [\"customer_id\"]\n      how: \"left\"\n</code></pre> <p>Source data:</p> <pre><code>order_id | customer_id | amount\n1001     | 101         | 99.00\n1002     | NULL        | 45.00   &lt;- Guest checkout, no customer\n1003     | 102         | 150.00\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_4","title":"Why It's Bad","text":"<p>NULL never equals NULL in SQL.</p> <p>When you join on <code>customer_id</code>: - <code>101 = 101</code> \u2705 Match - <code>NULL = NULL</code> \u274c No match! (NULL is \"unknown\", and unknown \u2260 unknown)</p> <p>Your orders with NULL customer_id get dropped or get incorrect dimension values.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_4","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Handle NULLs before joining\nnodes:\n  - name: \"prep_orders\"\n    read:\n      connection: silver\n      path: fact_orders\n\n    # \u2705 Option 1: Fill NULLs with a placeholder that maps to \"unknown\" customer\n    transform:\n      steps:\n        - function: \"fill_nulls\"\n          params:\n            columns: [\"customer_id\"]\n            value: 0  # Maps to unknown member in dim_customer\n\n    write:\n      connection: staging\n      path: orders_with_valid_keys\n\n  - name: \"enrich_orders\"\n    depends_on: [\"prep_orders\"]\n\n    transformer: \"join\"\n    params:\n      right: silver.dim_customer  # Has customer_id=0 as unknown member\n      on: [\"customer_id\"]\n      how: \"left\"\n</code></pre> <p>Or use the fact pattern with <code>orphan_handling: unknown</code>:</p> <pre><code># GOOD: Use the fact pattern for automatic NULL handling\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: silver\n      path: orders_clean\n\n    pattern:\n      type: fact\n      params:\n        dimensions:\n          - source_column: customer_id\n            dimension_table: dim_customer\n            dimension_key: customer_id\n            surrogate_key: customer_sk\n        # \u2705 NULLs get SK=0 (unknown member)\n        orphan_handling: unknown\n</code></pre>"},{"location":"patterns/anti_patterns/#6-mixing-business-logic-across-layers","title":"6. Mixing Business Logic Across Layers","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_5","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Business logic scattered everywhere\npipelines:\n  - pipeline: \"bronze_sales\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_sales\"\n        transform:\n          steps:\n            # \u274c Business calculation in Bronze?!\n            - sql: \"SELECT *, quantity * unit_price * 0.92 as net_amount FROM df\"\n        write:\n          connection: bronze\n          path: sales\n\n  - pipeline: \"silver_sales\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_sales\"\n        transform:\n          steps:\n            # \u274c More business logic here\n            - sql: \"SELECT *, CASE WHEN net_amount &gt; 1000 THEN 'high' ELSE 'low' END as tier FROM df\"\n        write:\n          connection: silver\n          path: sales\n\n  - pipeline: \"gold_sales\"\n    layer: \"gold\"\n    nodes:\n      - name: \"report_sales\"\n        transform:\n          steps:\n            # \u274c And here too!\n            - sql: \"SELECT *, net_amount * 1.1 as projected_amount FROM df\"\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_5","title":"Why It's Bad","text":"<p>Debugging becomes a nightmare.</p> <p>When someone asks \"Why is projected_amount $1,100?\", you have to trace through: 1. Bronze: <code>quantity * unit_price * 0.92 = net_amount</code> (8% discount) 2. Silver: No change to amounts 3. Gold: <code>net_amount * 1.1 = projected_amount</code> (10% markup)</p> <p>The business logic is hidden in three different places. Any change requires editing multiple pipelines.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_5","title":"\u2705 What to Do Instead","text":"<p>Keep business logic in ONE place\u2014Silver or Gold, not both.</p> <pre><code># GOOD: Clear separation of concerns\npipelines:\n  # Bronze: Raw data only\n  - pipeline: \"bronze_sales\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_sales\"\n        read:\n          connection: landing\n          path: sales.csv\n        # \u2705 No transformations in Bronze\n        write:\n          connection: bronze\n          path: sales\n\n  # Silver: Cleaning + business logic\n  - pipeline: \"silver_sales\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_sales\"\n        read:\n          connection: bronze\n          path: sales\n        transform:\n          steps:\n            # \u2705 All business calculations in ONE place\n            - sql: |\n                SELECT \n                  *,\n                  quantity * unit_price as gross_amount,\n                  quantity * unit_price * 0.92 as net_amount,\n                  quantity * unit_price * 0.92 * 1.1 as projected_amount,\n                  CASE WHEN quantity * unit_price * 0.92 &gt; 1000 THEN 'high' ELSE 'low' END as tier\n                FROM df\n        write:\n          connection: silver\n          path: fact_sales\n\n  # Gold: Aggregation only (no new business logic)\n  - pipeline: \"gold_sales\"\n    layer: \"gold\"\n    nodes:\n      - name: \"daily_summary\"\n        read:\n          connection: silver\n          path: fact_sales\n        # \u2705 Gold just aggregates what Silver prepared\n        pattern:\n          type: aggregation\n          params:\n            grain: [sale_date, region]\n            measures:\n              - name: total_net\n                expr: \"SUM(net_amount)\"\n              - name: total_projected\n                expr: \"SUM(projected_amount)\"\n</code></pre>"},{"location":"patterns/anti_patterns/#7-skipping-the-silver-layer","title":"7. Skipping the Silver Layer","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_6","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Going directly from Bronze to Gold\npipelines:\n  - pipeline: \"bronze_orders\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: landing\n          path: orders.csv\n        write:\n          connection: bronze\n          path: orders\n\n  - pipeline: \"gold_summary\"\n    layer: \"gold\"\n    nodes:\n      - name: \"daily_sales\"\n        read:\n          connection: bronze\n          path: orders  # \u274c Reading raw Bronze directly!\n\n        # Trying to do EVERYTHING in one step\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_id IS NOT NULL\"\n            - function: \"deduplicate\"\n              params: { keys: [\"order_id\"] }\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [order_date]\n            measures:\n              - name: total_sales\n                expr: \"SUM(amount)\"\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_6","title":"Why It's Bad","text":"<p>Every downstream consumer has to repeat the cleaning.</p> <p>If you have 5 Gold tables that all read from Bronze: 1. Each one cleans duplicates (same code x5) 2. Each one handles nulls (same code x5) 3. Each one applies business rules (same code x5)</p> <p>Any cleaning bug must be fixed in 5 places. And if different teams make slightly different cleaning decisions, your reports don't match.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_6","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Silver is your \"single source of truth\"\npipelines:\n  - pipeline: \"bronze_orders\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: landing\n          path: orders.csv\n        write:\n          connection: bronze\n          path: orders\n\n  # \u2705 Silver: Clean ONCE, use EVERYWHERE\n  - pipeline: \"silver_orders\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_orders\"\n        read:\n          connection: bronze\n          path: orders\n\n        # All cleaning happens here, once\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_id IS NOT NULL\"\n            - function: \"deduplicate\"\n              params: { keys: [\"order_id\"] }\n\n        validation:\n          contracts:\n            - type: not_null\n              columns: [order_id, customer_id, amount]\n\n        write:\n          connection: silver\n          path: fact_orders\n\n  # \u2705 Gold: Just aggregate clean data\n  - pipeline: \"gold_daily\"\n    layer: \"gold\"\n    nodes:\n      - name: \"daily_sales\"\n        read:\n          connection: silver\n          path: fact_orders  # \u2705 Reading from Silver\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [order_date]\n            measures:\n              - name: total_sales\n                expr: \"SUM(amount)\"\n\n  # \u2705 Another Gold table reads the same Silver\n  - pipeline: \"gold_regional\"\n    layer: \"gold\"\n    nodes:\n      - name: \"regional_sales\"\n        read:\n          connection: silver\n          path: fact_orders  # \u2705 Same Silver source\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [region, month]\n            measures:\n              - name: total_sales\n                expr: \"SUM(amount)\"\n</code></pre>"},{"location":"patterns/anti_patterns/#8-not-adding-extracted-timestamps-in-bronze","title":"8. Not Adding Extracted Timestamps in Bronze","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_7","title":"\u274c What NOT to Do","text":"<pre><code># BAD: No extraction timestamp\nnodes:\n  - name: \"ingest_sales\"\n    read:\n      connection: landing\n      path: sales/\n\n    # \u274c No metadata about when this was loaded\n    write:\n      connection: bronze\n      path: sales\n      mode: append\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_7","title":"Why It's Bad","text":"<p>You can't debug timing issues.</p> <p>Scenario: Data looks wrong for January 15th.</p> <p>Questions you can't answer: - When was the January 15th data loaded? - Was it loaded multiple times? - Did the source file change between loads?</p> <p>Without timestamps, your Bronze table is just a pile of data with no history of how it got there.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_7","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Add extraction metadata\nnodes:\n  - name: \"ingest_sales\"\n    read:\n      connection: landing\n      path: sales/\n\n    # \u2705 Add metadata columns\n    transform:\n      steps:\n        - function: \"derive_columns\"\n          params:\n            columns:\n              _extracted_at: \"current_timestamp()\"\n              _source_file: \"input_file_name()\"\n              _batch_id: \"'${BATCH_ID}'\"  # From orchestrator\n\n    write:\n      connection: bronze\n      path: sales\n      mode: append\n</code></pre> <p>Now your Bronze data includes:</p> <pre><code>order_id | amount | _extracted_at       | _source_file        | _batch_id\n1001     | 99.00  | 2024-01-16 06:00:00 | sales_20240115.csv  | batch_42\n1002     | 45.00  | 2024-01-16 06:00:00 | sales_20240115.csv  | batch_42\n1001     | 99.00  | 2024-01-16 18:00:00 | sales_20240115.csv  | batch_43  &lt;- Aha! Loaded twice!\n</code></pre>"},{"location":"patterns/anti_patterns/#9-using-append-mode-without-deduplication","title":"9. Using Append Mode Without Deduplication","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_8","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Append mode on a table that gets reprocessed\nnodes:\n  - name: \"load_daily_sales\"\n    read:\n      connection: landing\n      path: sales/${YESTERDAY}/  # Same file every rerun\n\n    # \u274c Append without deduplication\n    write:\n      connection: bronze\n      path: sales\n      mode: append\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_8","title":"Why It's Bad","text":"<p>Re-running the pipeline doubles your data.</p> <ul> <li>First run: 1,000 rows appended \u2705</li> <li>Pipeline fails later, you rerun from scratch</li> <li>Second run: Same 1,000 rows appended again \u274c</li> <li>Now you have 2,000 rows (1,000 duplicates)</li> </ul> <p>Your aggregations now show 2x the real sales.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_8","title":"\u2705 What to Do Instead","text":"<p>Option 1: Use merge mode for idempotent writes</p> <pre><code># GOOD: Merge mode is idempotent\nnodes:\n  - name: \"load_daily_sales\"\n    read:\n      connection: landing\n      path: sales/${YESTERDAY}/\n\n    # \u2705 Merge inserts new, updates existing\n    transformer: \"merge\"\n    params:\n      target: bronze.sales\n      keys: [\"order_id\"]\n\n    write:\n      connection: bronze\n      table: sales\n      format: delta\n      mode: overwrite\n</code></pre> <p>Option 2: Deduplicate in Silver</p> <pre><code># GOOD: Bronze appends, Silver deduplicates\nnodes:\n  - name: \"load_sales_bronze\"\n    write:\n      connection: bronze\n      path: sales\n      mode: append  # OK because Silver deduplicates\n\n  - name: \"clean_sales_silver\"\n    read:\n      connection: bronze\n      path: sales\n\n    # \u2705 Deduplicate the appended data\n    transformer: \"deduplicate\"\n    params:\n      keys: [\"order_id\"]\n      order_by: \"_extracted_at DESC\"  # Keep most recent if duplicated\n\n    write:\n      connection: silver\n      path: fact_sales\n      mode: overwrite  # Full refresh\n</code></pre>"},{"location":"patterns/anti_patterns/#10-ignoring-schema-evolution","title":"10. Ignoring Schema Evolution","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_9","title":"\u274c What NOT to Do","text":"<pre><code># BAD: No schema handling\nnodes:\n  - name: \"load_api_data\"\n    read:\n      connection: api\n      endpoint: /customers\n\n    # \u274c Just writing whatever comes from the API\n    write:\n      connection: bronze\n      path: customers\n      format: delta\n      # No schema_mode specified\n</code></pre> <p>What happens when the API adds a new field:</p> <pre><code>Day 1: {\"id\": 1, \"name\": \"Alice\"}\nDay 2: {\"id\": 2, \"name\": \"Bob\", \"loyalty_points\": 500}  &lt;- New field!\n</code></pre> <p>Pipeline fails with:</p> <pre><code>SchemaEvolutionException: Found new column 'loyalty_points' \nnot present in the target schema\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_9","title":"Why It's Bad","text":"<p>APIs and source systems change without warning.</p> <p>Vendors add fields, rename columns, or change types. Without explicit schema handling, your pipeline breaks whenever this happens\u2014usually at 3 AM on a weekend.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_9","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Explicit schema evolution handling\nnodes:\n  - name: \"load_api_data\"\n    read:\n      connection: api\n      endpoint: /customers\n\n    write:\n      connection: bronze\n      path: customers\n      format: delta\n      # \u2705 Allow schema to grow\n      delta_options:\n        mergeSchema: true\n\n    # \u2705 Or use schema_policy for fine control\n    schema_policy:\n      on_new_column: add       # Add new columns automatically\n      on_missing_column: warn  # Log warning but continue\n      on_type_mismatch: error  # Fail on type changes (dangerous!)\n</code></pre> <p>For production, consider schema contracts:</p> <pre><code># GOOD: Schema contract catches unexpected changes\ncontracts:\n  - type: schema\n    expected:\n      - column: id\n        type: integer\n        nullable: false\n      - column: name\n        type: string\n        nullable: false\n      - column: loyalty_points\n        type: integer\n        nullable: true  # We know about this field\n    on_extra_columns: warn  # New fields trigger warning, not failure\n</code></pre>"},{"location":"patterns/anti_patterns/#quick-reference-anti-patterns-cheat-sheet","title":"Quick Reference: Anti-Patterns Cheat Sheet","text":"Anti-Pattern Why It's Bad Do This Instead Transforming in Bronze Lose original data Keep Bronze raw, transform in Silver No dedup before SCD2 History table explodes Deduplicate first, then SCD2 SCD2 on fact tables Facts are immutable events Use append or merge Hardcoded paths Breaks across environments Use connections + env vars NULLs in join keys NULL \u2260 NULL, rows get dropped Handle NULLs with placeholders Business logic everywhere Can't debug, inconsistent Centralize in Silver Bronze \u2192 Gold directly Cleaning repeated everywhere Use Silver as single source of truth No _extracted_at Can't debug timing Add metadata columns Append without dedup Reruns create duplicates Use merge or deduplicate downstream Ignore schema changes Pipeline breaks on changes Use mergeSchema or schema_policy"},{"location":"patterns/anti_patterns/#next-steps","title":"Next Steps","text":"<ul> <li>SCD2 Troubleshooting</li> <li>Validation Patterns</li> <li>Best Practices Guide</li> <li>Troubleshooting Guide</li> </ul>"},{"location":"patterns/append_only_raw/","title":"Pattern: Append-Only Raw Layer (Landing \u2192 Raw)","text":"<p>Status: Core Pattern Layer: Raw (Bronze) Engine: Spark Structured Streaming or Batch Write Mode: <code>append</code> </p>"},{"location":"patterns/append_only_raw/#problem","title":"Problem","text":"<p>You have source data coming from multiple places (SQL databases, APIs, files). You need to: - Preserve the complete history of what arrived and when - Enable replay/reconstruction if downstream logic breaks - Avoid data loss from overwriting mistakes</p>"},{"location":"patterns/append_only_raw/#solution","title":"Solution","text":"<p>Append every piece of data you receive to the Raw layer without modification. Raw is the immutable audit log.</p>"},{"location":"patterns/append_only_raw/#key-principles","title":"Key Principles","text":"<ol> <li>Raw is Sacred \u2192 Never delete or overwrite Raw data</li> <li>Append-Only \u2192 Each run appends new rows (possibly duplicates)</li> <li>One-to-One Copy \u2192 Raw mirrors the source schema, nothing more</li> <li>Permanent Retention \u2192 Keep forever; storage is cheap</li> </ol>"},{"location":"patterns/append_only_raw/#pattern-flow","title":"Pattern Flow","text":"<pre><code>Source System (SQL, API, Files)\n         \u2193\n    Read Data\n         \u2193\n   Append to Raw\n    (append mode)\n         \u2193\nRaw Layer (history preserved)\n         \u2193\n[Next: Merge into Silver for dedup/cleanup]\n</code></pre>"},{"location":"patterns/append_only_raw/#example-sql-source-raw-layer","title":"Example: SQL Source \u2192 Raw Layer","text":""},{"location":"patterns/append_only_raw/#scenario","title":"Scenario","text":"<p>You have a SQL table <code>dbo.orders</code> with new/updated rows arriving daily.</p> <p>First Run (Day 1):</p> <p>Source (<code>dbo.orders</code>):</p> <pre><code>order_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n</code></pre> <p>After Append to Raw:</p> <pre><code>Raw.orders (Delta Table)\norder_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n</code></pre> <p>Second Run (Day 2):</p> <p>Source now has 2 new rows + 1 duplicate:</p> <pre><code>order_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00  \u2190 Already seen\n2        | Widget B     | 5   | 2025-11-01 11:00  \u2190 Already seen\n3        | Widget A     | 2   | 2025-11-01 12:00  \u2190 Already seen\n4        | Widget C     | 8   | 2025-11-02 09:00  \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00  \u2190 NEW\n</code></pre> <p>After Append to Raw:</p> <pre><code>Raw.orders (all 8 rows)\norder_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n1        | Widget A     | 10  | 2025-11-01 10:00  \u2190 DUPLICATE (OK)\n2        | Widget B     | 5   | 2025-11-01 11:00  \u2190 DUPLICATE (OK)\n3        | Widget A     | 2   | 2025-11-01 12:00  \u2190 DUPLICATE (OK)\n4        | Widget C     | 8   | 2025-11-02 09:00\n5        | Widget B     | 12  | 2025-11-02 10:00\n</code></pre> <p>Note: Duplicates in Raw are OK. Silver's merge will deduplicate them.</p>"},{"location":"patterns/append_only_raw/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/append_only_raw/#using-high-water-mark-incremental","title":"Using High Water Mark (Incremental)","text":"<pre><code>pipelines:\n  - pipeline: sql_to_raw\n    layer: bronze\n    nodes:\n      - id: load_orders_raw\n        name: \"Load Orders to Raw (Bronze)\"\n        read:\n          connection: sql_prod\n          format: sql\n          table: dbo.orders\n          options:\n            query: |\n              SELECT * FROM dbo.orders\n              WHERE COALESCE(updated_at, created_at) &gt; (\n                SELECT COALESCE(MAX(COALESCE(updated_at, created_at)), '1900-01-01')\n                FROM raw.orders\n              )\n        write:\n          connection: adls_prod\n          format: delta\n          table: raw.orders\n          mode: append\n</code></pre>"},{"location":"patterns/append_only_raw/#using-scheduled-batch-full-rescan","title":"Using Scheduled Batch (Full Rescan)","text":"<p>If your source doesn't have timestamps, rescan everything and append:</p> <pre><code>      - id: load_orders_raw\n        name: \"Load Orders to Raw\"\n        read:\n          connection: sql_prod\n          format: sql\n          table: dbo.orders\n        write:\n          connection: adls_prod\n          format: delta\n          table: raw.orders\n          mode: append\n</code></pre> <p>Warning: This will append the entire table each run, creating duplicates. Use High Water Mark pattern when possible.</p>"},{"location":"patterns/append_only_raw/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/append_only_raw/#advantages","title":"Advantages","text":"<p>\u2713 Complete audit trail (when did each row arrive?) \u2713 Safe replay/reconstruction if Silver breaks \u2713 Simple logic (no deduplication, no logic) \u2713 Idempotent (rerun is safe; creates duplicates but that's OK)  </p>"},{"location":"patterns/append_only_raw/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Storage grows unbounded (mitigated by cheap cloud storage) \u2717 Duplicates must be handled downstream (Silver's job) \u2717 Full rescans can be slow without timestamp filtering  </p>"},{"location":"patterns/append_only_raw/#when-to-use","title":"When to Use","text":"<p>Always use Append-Only for Raw ingestion. No exceptions.</p>"},{"location":"patterns/append_only_raw/#when-not-to-use","title":"When NOT to Use","text":"<p>Never overwrite Raw. Never delete from Raw. Never use <code>merge</code> in Raw.</p>"},{"location":"patterns/append_only_raw/#related-patterns","title":"Related Patterns","text":"<ul> <li>High Water Mark \u2192 Efficiently filter SQL sources for incremental reads</li> <li>Merge/Upsert \u2192 Deduplicate Raw data in Silver</li> </ul>"},{"location":"patterns/date_dimension/","title":"Date Dimension Pattern","text":"<p>The <code>date_dimension</code> pattern generates a complete date dimension table with pre-calculated attributes useful for BI/reporting.</p>"},{"location":"patterns/date_dimension/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The date dimension pattern is unique - it generates data rather than transforming it. No <code>read:</code> block is needed.</p> <pre><code>project: my_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    nodes:\n      - name: dim_date\n        # No read block - pattern generates data\n        transformer: date_dimension\n        params:\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n          fiscal_year_start_month: 7  # July fiscal year\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/date_dimension/#features","title":"Features","text":"<ul> <li>Date range generation from start_date to end_date</li> <li>Fiscal calendar support with configurable fiscal year start month</li> <li>19 pre-calculated columns for flexible analysis</li> <li>Unknown member row (date_sk=0) for orphan FK handling</li> <li>Works with both Spark and Pandas</li> </ul>"},{"location":"patterns/date_dimension/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>start_date</code> str Yes - Start date in YYYY-MM-DD format <code>end_date</code> str Yes - End date in YYYY-MM-DD format <code>fiscal_year_start_month</code> int No 1 Month when fiscal year starts (1-12) <code>unknown_member</code> bool No false Add unknown date row with date_sk=0"},{"location":"patterns/date_dimension/#generated-columns","title":"Generated Columns","text":"<p>The pattern generates 19 columns automatically:</p> Column Type Description Example <code>date_sk</code> int Surrogate key (YYYYMMDD format) 20240115 <code>full_date</code> date The actual date 2024-01-15 <code>day_of_week</code> str Day name Monday <code>day_of_week_num</code> int Day number (1=Monday, 7=Sunday) 1 <code>day_of_month</code> int Day of month (1-31) 15 <code>day_of_year</code> int Day of year (1-366) 15 <code>is_weekend</code> bool Weekend flag false <code>week_of_year</code> int ISO week number (1-53) 3 <code>month</code> int Month number (1-12) 1 <code>month_name</code> str Month name January <code>quarter</code> int Calendar quarter (1-4) 1 <code>quarter_name</code> str Quarter name Q1 <code>year</code> int Calendar year 2024 <code>fiscal_year</code> int Fiscal year 2024 <code>fiscal_quarter</code> int Fiscal quarter (1-4) 3 <code>is_month_start</code> bool First day of month false <code>is_month_end</code> bool Last day of month false <code>is_year_start</code> bool First day of year false <code>is_year_end</code> bool Last day of year false"},{"location":"patterns/date_dimension/#fiscal-calendar-configuration","title":"Fiscal Calendar Configuration","text":"<p>Configure fiscal year start month for companies with non-calendar fiscal years:</p> <pre><code>nodes:\n  - name: dim_date\n    transformer: date_dimension\n    params:\n      start_date: \"2020-01-01\"\n      end_date: \"2030-12-31\"\n      fiscal_year_start_month: 7  # July 1st = FY start\n    write:\n      connection: warehouse\n      path: dim_date\n      mode: overwrite\n</code></pre> <p>Fiscal Year Calculation: - If <code>fiscal_year_start_month = 7</code> (July) - July 2024 \u2192 FY 2025 - June 2024 \u2192 FY 2024</p> <p>Fiscal Quarter Calculation: - Fiscal Q1: July, August, September - Fiscal Q2: October, November, December - Fiscal Q3: January, February, March - Fiscal Q4: April, May, June</p>"},{"location":"patterns/date_dimension/#unknown-member-row","title":"Unknown Member Row","text":"<p>Enable <code>unknown_member: true</code> to add a special row for orphan FK handling:</p> Column Value date_sk 0 full_date 1900-01-01 day_of_week Unknown day_of_week_num 0 month_name Unknown quarter_name Unknown year 0"},{"location":"patterns/date_dimension/#full-yaml-example","title":"Full YAML Example","text":"<p>Complete date dimension in a warehouse pipeline:</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_reference_dimensions\n    description: \"Build date and other reference dimensions\"\n    nodes:\n      - name: dim_date\n        description: \"Standard date dimension with fiscal calendar\"\n        transformer: date_dimension\n        params:\n          start_date: \"2015-01-01\"\n          end_date: \"2035-12-31\"\n          fiscal_year_start_month: 10  # October fiscal year (retail)\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n          partition_by: [year]  # Optional: partition by year\n</code></pre>"},{"location":"patterns/date_dimension/#common-fiscal-year-configurations","title":"Common Fiscal Year Configurations","text":""},{"location":"patterns/date_dimension/#retail-calendar-october-fy","title":"Retail Calendar (October FY)","text":"<pre><code>fiscal_year_start_month: 10\n</code></pre>"},{"location":"patterns/date_dimension/#government-calendar-october-fy","title":"Government Calendar (October FY)","text":"<pre><code>fiscal_year_start_month: 10\n</code></pre>"},{"location":"patterns/date_dimension/#education-calendar-july-fy","title":"Education Calendar (July FY)","text":"<pre><code>fiscal_year_start_month: 7\n</code></pre>"},{"location":"patterns/date_dimension/#standard-calendar-year","title":"Standard Calendar Year","text":"<pre><code>fiscal_year_start_month: 1  # Default\n</code></pre>"},{"location":"patterns/date_dimension/#complete-configuration-reference","title":"Complete Configuration Reference","text":""},{"location":"patterns/date_dimension/#all-parameters","title":"All Parameters","text":"Parameter Type Required Default Description <code>start_date</code> str Yes - Start date in YYYY-MM-DD format <code>end_date</code> str Yes - End date in YYYY-MM-DD format <code>fiscal_year_start_month</code> int No 1 Month when fiscal year starts (1-12) <code>unknown_member</code> bool No false Add unknown date row with date_sk=0 <code>date_format</code> str No \"%Y-%m-%d\" Format string for date parsing <code>week_start_day</code> int No 1 First day of week (1=Monday, 7=Sunday) <code>include_holidays</code> bool No false Generate is_holiday column (requires holiday_country) <code>holiday_country</code> str No \"US\" Country code for holiday calendar"},{"location":"patterns/date_dimension/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code>nodes:\n  - name: dim_date\n    transformer: date_dimension\n    params:\n      # Required\n      start_date: \"2015-01-01\"\n      end_date: \"2035-12-31\"\n\n      # Fiscal calendar\n      fiscal_year_start_month: 7      # July fiscal year\n\n      # Unknown member\n      unknown_member: true            # Add SK=0 row for orphans\n\n      # Week configuration\n      week_start_day: 1               # Monday (ISO standard)\n\n      # Holiday support (if holidays package installed)\n      include_holidays: true\n      holiday_country: \"US\"\n    write:\n      connection: warehouse\n      path: dim_date\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/date_dimension/#output-column-customization","title":"Output Column Customization","text":"<p>To customize output columns, add a SQL step after generation:</p> <pre><code>nodes:\n  - name: dim_date_raw\n    transformer: date_dimension\n    params:\n      start_date: \"2020-01-01\"\n      end_date: \"2030-12-31\"\n      fiscal_year_start_month: 10\n      unknown_member: true\n\n  - name: dim_date\n    depends_on: [dim_date_raw]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              date_sk,\n              full_date,\n              day_of_week,\n              month_name,\n              quarter_name,\n              year,\n              fiscal_year,\n              fiscal_quarter,\n              is_weekend,\n              -- Custom columns\n              CONCAT(year, '-', LPAD(month, 2, '0')) AS year_month,\n              CASE WHEN month IN (11, 12) THEN true ELSE false END AS is_holiday_season\n            FROM dim_date_raw\n    write:\n      connection: warehouse\n      path: dim_date\n</code></pre>"},{"location":"patterns/date_dimension/#see-also","title":"See Also","text":"<ul> <li>Dimension Pattern - Build regular dimensions</li> <li>Fact Pattern - Build fact tables with SK lookups</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/dimension/","title":"Dimension Pattern","text":"<p>The <code>dimension</code> pattern builds complete dimension tables with automatic surrogate key generation and SCD (Slowly Changing Dimension) support.</p>"},{"location":"patterns/dimension/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>Patterns are used via the <code>transformer:</code> field in a node config. The pattern name goes in <code>transformer:</code> and configuration goes in <code>params:</code>.</p> <pre><code>project: my_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dimensions\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n          format: delta\n\n        # Use dimension pattern via transformer\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols: [name, email, address]\n          target: warehouse.dim_customer\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#features","title":"Features","text":"<ul> <li>Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows)</li> <li>SCD Type 0 (static - never update existing records)</li> <li>SCD Type 1 (overwrite - update in place, no history)</li> <li>SCD Type 2 (history tracking - full audit trail)</li> <li>Unknown member row (SK=0) for orphan FK handling</li> <li>Audit columns (load_timestamp, source_system)</li> </ul>"},{"location":"patterns/dimension/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>natural_key</code> str Yes - Natural/business key column name <code>surrogate_key</code> str Yes - Surrogate key column name to generate <code>scd_type</code> int No 1 0=static, 1=overwrite, 2=history tracking <code>track_cols</code> list For SCD1/2 - Columns to track for changes <code>target</code> str For SCD2 - Target table path (required to read existing history) <code>unknown_member</code> bool No false Insert a row with SK=0 for orphan FK handling <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/dimension/#audit-config","title":"Audit Config","text":"<pre><code>params:\n  # ... other params ...\n  audit:\n    load_timestamp: true      # Add load_timestamp column\n    source_system: \"pos\"      # Add source_system column with this value\n</code></pre>"},{"location":"patterns/dimension/#scd-type-0-static","title":"SCD Type 0 (Static)","text":"<p>Static dimensions never update existing records. Only new records (not matching natural key) are inserted.</p> <p>Use case: Reference data that never changes (ISO country codes, fixed lookup values).</p> <pre><code>nodes:\n  - name: dim_country\n    read:\n      connection: staging\n      path: countries\n    transformer: dimension\n    params:\n      natural_key: country_code\n      surrogate_key: country_sk\n      scd_type: 0\n      target: warehouse.dim_country\n    write:\n      connection: warehouse\n      path: dim_country\n      mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#scd-type-1-overwrite","title":"SCD Type 1 (Overwrite)","text":"<p>Overwrite dimensions update existing records in place. No history is kept.</p> <p>Use case: Attributes where you only care about the current value (customer email, product price).</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: staging\n      path: customers\n    transformer: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 1\n      track_cols: [name, email, address]\n      target: warehouse.dim_customer\n      audit:\n        load_timestamp: true\n    write:\n      connection: warehouse\n      path: dim_customer\n      mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#scd-type-2-history-tracking","title":"SCD Type 2 (History Tracking)","text":"<p>History-tracking dimensions preserve full audit trail. Old records are closed, new versions are opened.</p> <p>Use case: Slowly changing attributes where historical accuracy matters (customer address for point-in-time reporting).</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: staging\n      path: customers\n    transformer: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols: [name, email, address, city, state]\n      target: warehouse.dim_customer\n      valid_from_col: valid_from     # Optional, default: valid_from\n      valid_to_col: valid_to         # Optional, default: valid_to\n      is_current_col: is_current     # Optional, default: is_current\n      unknown_member: true\n      audit:\n        load_timestamp: true\n        source_system: \"crm\"\n    write:\n      connection: warehouse\n      path: dim_customer\n      mode: overwrite\n</code></pre> <p>Generated Columns: - <code>valid_from</code>: Timestamp when this version became active - <code>valid_to</code>: Timestamp when this version was superseded (NULL for current) - <code>is_current</code>: Boolean flag (true for current version)</p>"},{"location":"patterns/dimension/#unknown-member-handling","title":"Unknown Member Handling","text":"<p>Enable <code>unknown_member: true</code> to automatically insert a row with SK=0. This allows fact tables to reference unknown dimensions without FK violations.</p> <p>Generated Unknown Member Row:</p> customer_sk customer_id name email valid_from is_current 0 -1 Unknown Unknown 1900-01-01 true"},{"location":"patterns/dimension/#full-star-schema-example","title":"Full Star Schema Example","text":"<p>Complete pipeline building dimensions for a star schema:</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_dimensions\n    nodes:\n      # Customer dimension with SCD2\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n          format: delta\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols:\n            - name\n            - email\n            - phone\n            - address_line_1\n            - city\n            - state\n            - postal_code\n          target: warehouse.dim_customer\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"salesforce\"\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n          mode: overwrite\n\n      # Product dimension with SCD1 (no history)\n      - name: dim_product\n        read:\n          connection: staging\n          path: products\n          format: delta\n        transformer: dimension\n        params:\n          natural_key: product_id\n          surrogate_key: product_sk\n          scd_type: 1\n          track_cols: [name, category, price, status]\n          target: warehouse.dim_product\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_product\n          format: delta\n          mode: overwrite\n\n      # Date dimension (generated, no source read needed)\n      - name: dim_date\n        transformer: date_dimension\n        params:\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n          fiscal_year_start_month: 7\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#python-api","title":"Python API","text":"<pre><code>from odibi.patterns.dimension import DimensionPattern\nfrom odibi.context import EngineContext\n\n# Create pattern instance\npattern = DimensionPattern(\n    engine=my_engine,\n    config=node_config  # NodeConfig with params\n)\n\n# Or directly with params dict\nfrom odibi.patterns.dimension import DimensionPattern\n\npattern = DimensionPattern(params={\n    \"natural_key\": \"customer_id\",\n    \"surrogate_key\": \"customer_sk\",\n    \"scd_type\": 2,\n    \"track_cols\": [\"name\", \"email\", \"address\"],\n    \"target\": \"gold.dim_customer\",\n    \"unknown_member\": True,\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"crm\"\n    }\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern\ncontext = EngineContext(df=source_df, engine_type=EngineType.SPARK)\nresult_df = pattern.execute(context)\n</code></pre>"},{"location":"patterns/dimension/#see-also","title":"See Also","text":"<ul> <li>Date Dimension Pattern - Generate date dimensions</li> <li>Fact Pattern - Build fact tables with SK lookups</li> <li>Aggregation Pattern - Build aggregate tables</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/fact/","title":"Fact Pattern","text":"<p>The <code>fact</code> pattern builds fact tables with automatic surrogate key lookups from dimension tables, orphan handling, grain validation, and measure calculations.</p>"},{"location":"patterns/fact/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The fact pattern looks up dimension tables from context - dimensions must be registered (either by running dimension nodes in the same pipeline with <code>depends_on</code>, or by reading them from storage).</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_star_schema\n    nodes:\n      # First, build or load dimensions\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n        # Just loading - no transform needed\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: delta\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: delta\n\n      # Then build fact table with SK lookups\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: staging\n          path: orders\n          format: delta\n\n        transformer: fact\n        params:\n          grain: [order_id, line_item_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer  # References node name\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/fact/#features","title":"Features","text":"<ul> <li>Automatic SK lookups from dimension tables</li> <li>Orphan handling (unknown member, reject, or quarantine)</li> <li>Grain validation (detect duplicates at PK level)</li> <li>Deduplication support</li> <li>Measure calculations and renaming</li> <li>Audit columns (load_timestamp, source_system)</li> <li>SCD2 dimension support (filter to is_current=true)</li> </ul>"},{"location":"patterns/fact/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>grain</code> list No - Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No [] Dimension lookup configurations <code>orphan_handling</code> str No \"unknown\" \"unknown\", \"reject\", or \"quarantine\" <code>measures</code> list No [] Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No false Remove duplicates before insert <code>keys</code> list Required if deduplicate - Keys for deduplication <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/fact/#dimension-lookup-config","title":"Dimension Lookup Config","text":"<pre><code>params:\n  dimensions:\n    - source_column: customer_id     # Column in source data\n      dimension_table: dim_customer  # Node name in context\n      dimension_key: customer_id     # Natural key column in dimension\n      surrogate_key: customer_sk     # Surrogate key to retrieve\n      scd2: true                     # If true, filter is_current=true\n</code></pre>"},{"location":"patterns/fact/#measures-config","title":"Measures Config","text":"<pre><code>params:\n  measures:\n    - quantity                           # Passthrough\n    - revenue: total_amount              # Rename\n    - line_total: \"quantity * unit_price\" # Calculate\n</code></pre>"},{"location":"patterns/fact/#orphan-handling","title":"Orphan Handling","text":"<p>Three strategies for handling source records that don't match any dimension:</p>"},{"location":"patterns/fact/#1-unknown-default","title":"1. Unknown (Default)","text":"<p>Map orphans to the unknown member (SK=0):</p> <pre><code>orphan_handling: unknown\n</code></pre>"},{"location":"patterns/fact/#2-reject","title":"2. Reject","text":"<p>Fail the pipeline if any orphans exist:</p> <pre><code>orphan_handling: reject\n</code></pre>"},{"location":"patterns/fact/#3-quarantine","title":"3. Quarantine","text":"<p>Route orphans to quarantine table:</p> <pre><code>orphan_handling: quarantine\n</code></pre>"},{"location":"patterns/fact/#grain-validation","title":"Grain Validation","text":"<p>Define the fact table grain to detect duplicate records:</p> <pre><code>params:\n  grain: [order_id, line_item_id]\n</code></pre> <p>If duplicates exist, the pattern raises an error with details.</p>"},{"location":"patterns/fact/#full-star-schema-example","title":"Full Star Schema Example","text":"<p>Complete pipeline building dimensions AND fact tables:</p> <pre><code>project: sales_star_schema\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  # Pipeline 1: Build dimensions\n  - pipeline: build_dimensions\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols: [name, email, region]\n          target: warehouse.dim_customer\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_customer\n          mode: overwrite\n\n      - name: dim_product\n        read:\n          connection: staging\n          path: products\n        transformer: dimension\n        params:\n          natural_key: product_id\n          surrogate_key: product_sk\n          scd_type: 1\n          track_cols: [name, category, price]\n          target: warehouse.dim_product\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_product\n          mode: overwrite\n\n      - name: dim_date\n        transformer: date_dimension\n        params:\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          mode: overwrite\n\n  # Pipeline 2: Build fact table (depends on dimensions existing)\n  - pipeline: build_facts\n    nodes:\n      # Load dimensions into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n\n      # Build fact table\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: staging\n          path: orders\n        transformer: fact\n        params:\n          grain: [order_id, line_item_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - discount_amount\n            - line_total: \"quantity * unit_price\"\n            - net_amount: \"quantity * unit_price - discount_amount\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n        write:\n          connection: warehouse\n          path: fact_orders\n          mode: overwrite\n</code></pre>"},{"location":"patterns/fact/#see-also","title":"See Also","text":"<ul> <li>Dimension Pattern - Build dimensions with SCD support</li> <li>Date Dimension Pattern - Generate date dimensions</li> <li>Aggregation Pattern - Build aggregate tables</li> <li>FK Validation - Additional FK validation</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/incremental_stateful/","title":"Stateful Incremental Loading","text":"<p>Stateful Incremental Loading is the \"Auto-Pilot\" mode for ingestion. Unlike Smart Read (Rolling Window) which blindly looks back X days, Stateful Mode remembers exactly where it left off.</p> <p>It tracks the High Water Mark (HWM)\u2014the maximum value of a column (e.g., <code>updated_at</code> or <code>id</code>) seen in the previous run\u2014and only fetches records greater than that value.</p>"},{"location":"patterns/incremental_stateful/#when-to-use-this","title":"When to use this?","text":"<ul> <li>CDC-like Ingestion: You want to sync a large table and only get new rows.</li> <li>Exactness: You don't want to guess a lookback window (e.g., \"3 days just to be safe\").</li> <li>Performance: You want to query the absolute minimum data required.</li> </ul>"},{"location":"patterns/incremental_stateful/#configuration","title":"Configuration","text":"<p>Enable it by setting <code>mode: stateful</code> in the <code>incremental</code> block.</p> <pre><code>- name: \"ingest_orders\"\n  read:\n    connection: \"postgres_prod\"\n    format: \"sql\"\n    table: \"public.orders\"\n\n    incremental:\n      mode: \"stateful\"              # Enable State Tracking\n      column: \"updated_at\"      # Column to track (max value is saved)\n      fallback_column: \"created_at\" # Optional: Use this if key_column is NULL\n      watermark_lag: \"30m\"          # Safety buffer (overlaps the window)\n      state_key: \"orders_ingest\"    # Optional: Custom ID for the state file\n\n  write:\n    connection: \"bronze\"\n    format: \"delta\"\n    table: \"orders_bronze\"\n    mode: \"append\"\n</code></pre>"},{"location":"patterns/incremental_stateful/#how-it-works","title":"How It Works","text":"<ol> <li> <p>First Run (Bootstrap)</p> <ul> <li>Odibi checks the state backend (Delta table or local JSON).</li> <li>No state found? $\\rightarrow$ Full Load (<code>SELECT * FROM table</code>).</li> <li>After success, it saves <code>MAX(updated_at)</code> as the HWM.</li> </ul> </li> <li> <p>Subsequent Runs (Incremental)</p> <ul> <li>Odibi retrieves the last HWM (e.g., <code>2023-10-25 10:00:00</code>).</li> <li>It subtracts the <code>watermark_lag</code> (e.g., 30 mins) $\\rightarrow$ <code>09:30:00</code>.</li> <li>Generates query: <code>SELECT * FROM table WHERE updated_at &gt; '2023-10-25 09:30:00'</code>.</li> <li>After success, it updates the HWM with the new maximum from the fetched batch.</li> </ul> </li> </ol>"},{"location":"patterns/incremental_stateful/#key-features","title":"Key Features","text":""},{"location":"patterns/incremental_stateful/#watermark-lag","title":"\ud83c\udf0a Watermark Lag","text":"<p>Data often arrives late or out of order. If you run your pipeline at 10:00, you might miss a record timestamped 09:59 that gets committed at 10:01.</p> <p>The <code>watermark_lag</code> creates a safety overlap. *   Lag: \"30m\" implies: \"Give me everything since the last run, but re-read the last 30 minutes just in case.\" *   This ensures At-Least-Once delivery. *   Note: This causes duplicates in the Bronze layer. This is expected! Your Silver layer (Merge/Upsert) handles deduplication.</p>"},{"location":"patterns/incremental_stateful/#state-backends","title":"\ud83d\udee1\ufe0f State Backends","text":"<p>Odibi automatically chooses the best backend: *   Spark/Databricks: Uses a Delta table (<code>odibi_meta.state</code>) to track HWMs. This is robust and supports concurrency. *   Pandas/Local: Uses a local JSON file (<code>.odibi/state.json</code>).</p>"},{"location":"patterns/incremental_stateful/#resets","title":"\ud83d\udd04 Resets","text":"<p>To reset the state and force a full reload: 1.  Delete the target table/file. 2.  Clear the state entry (manually or via CLI - CLI command coming soon).</p>"},{"location":"patterns/incremental_stateful/#comparison-rolling-window-vs-stateful","title":"Comparison: Rolling Window vs. Stateful","text":"Feature Rolling Window (<code>smart_read</code>) Stateful (<code>stateful</code>) Logic <code>NOW() - lookback</code> <code>&gt; Last HWM</code> State Stateless (Time-based) Stateful (Persisted) Best For Reporting windows (\"Last 30 days\") Ingestion / Replication (\"Sync table\") Complexity Low Medium Safety Good (if lookback is large) Excellent (Exact tracking)"},{"location":"patterns/incremental_stateful/#example-cdc-ingestion-pipeline","title":"Example: CDC Ingestion Pipeline","text":"<p>Here is a robust pattern for database replication:</p> <pre><code>nodes:\n  # 1. Ingest (Bronze) - Accumulates history with duplicates\n  - name: \"ingest_users\"\n    read:\n      connection: \"db_prod\"\n      table: \"users\"\n      incremental:\n        mode: \"stateful\"\n        key_column: \"updated_at\"\n        watermark_lag: \"15m\"\n    write:\n      connection: \"lake\"\n      format: \"delta\"\n      table: \"bronze_users\"\n      mode: \"append\"\n\n  # 2. Merge (Silver) - Deduplicates and keeps current state\n  - name: \"dim_users\"\n    depends_on: [\"ingest_users\"] # Reads ONLY the new batch\n    transformer: \"merge\"\n    params:\n      keys: [\"user_id\"]\n      order_by: \"updated_at DESC\"\n    write:\n      connection: \"lake\"\n      format: \"delta\"\n      table: \"silver_users\"\n      mode: \"upsert\"\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/","title":"[Legacy] Manual High Water Mark (HWM)","text":"<p>\u26a0\ufe0f Deprecated Pattern</p> <p>This manual pattern is no longer recommended. Please use the new Stateful Incremental Loading feature which handles this automatically.</p>"},{"location":"patterns/legacy_hwm_manual/#what-is-hwm","title":"What Is HWM?","text":"<p>A pattern for incremental data loading: load all data once on the first run, then load only new/changed data on each subsequent run.</p> <p>Day 1: Load 10 years of history Day 2+: Load only today's new records</p>"},{"location":"patterns/legacy_hwm_manual/#the-pattern","title":"The Pattern","text":""},{"location":"patterns/legacy_hwm_manual/#configuration","title":"Configuration","text":"<pre><code>nodes:\n  load_orders:\n    read:\n      connection: my_sql_server\n      format: sql_server\n      query: |\n        SELECT\n          order_id,\n          customer_id,\n          amount,\n          created_at,\n          updated_at\n        FROM dbo.orders\n        WHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n\n    write:\n      connection: adls\n      format: delta\n      path: bronze/orders\n      mode: append\n      first_run_query: |\n        SELECT\n          order_id,\n          customer_id,\n          amount,\n          created_at,\n          updated_at\n        FROM dbo.orders\n      options:\n        cluster_by: [created_at]\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#how-it-works","title":"How It Works","text":"<p>Day 1 (table doesn't exist): - Odibi runs <code>first_run_query</code> - Loads ALL orders from source - Creates table with clustering - Write mode: OVERWRITE</p> <p>Day 2+ (table exists): - Odibi runs regular <code>query</code> - Loads only orders modified in last 1 day - Appends to table - Write mode: APPEND</p>"},{"location":"patterns/legacy_hwm_manual/#the-two-queries","title":"The Two Queries","text":""},{"location":"patterns/legacy_hwm_manual/#first_run_query","title":"<code>first_run_query</code>","text":"<pre><code>SELECT * FROM dbo.orders\n</code></pre> <ul> <li>Loads everything</li> <li>Runs once on day 1</li> <li>Takes longer, but happens only once</li> </ul>"},{"location":"patterns/legacy_hwm_manual/#query-incremental","title":"<code>query</code> (incremental)","text":"<pre><code>SELECT * FROM dbo.orders\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n</code></pre> <ul> <li>Loads only new/updated records</li> <li>Runs every day from day 2 onward</li> <li>Takes seconds</li> </ul>"},{"location":"patterns/legacy_hwm_manual/#key-concepts","title":"Key Concepts","text":""},{"location":"patterns/legacy_hwm_manual/#1-track-changes","title":"1. Track Changes","text":"<p>Use <code>updated_at</code> if available. If not, use <code>created_at</code>.</p> <pre><code>-- Prefer updated_at (catches changes)\nWHERE updated_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- Fallback to created_at (new records only)\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- Use both (catches new + modified)\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#2-time-ranges","title":"2. Time Ranges","text":"<p>Use overlapping time ranges (2 days instead of 1) to catch late-arriving data:</p> <pre><code>-- 1 day: misses records that arrived late\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- 2 days: safer, catches late arrivals\nWHERE created_at &gt;= DATEADD(DAY, -2, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#3-clustering","title":"3. Clustering","text":"<p>On first run, apply clustering for query performance:</p> <pre><code>options:\n  cluster_by: [created_at]  # Applied day 1, speeds up incremental queries\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#setup-steps","title":"Setup Steps","text":"<ol> <li>Identify HWM column (<code>updated_at</code> or <code>created_at</code>)</li> <li>Write first_run_query (<code>SELECT * FROM table</code>)</li> <li>Write incremental query (<code>SELECT * WHERE timestamp &gt;= date_function</code>)</li> <li>Add options (<code>cluster_by</code> for performance)</li> <li>Deploy and let Odibi handle the rest</li> </ol>"},{"location":"patterns/legacy_hwm_manual/#example-orders-table","title":"Example: Orders Table","text":""},{"location":"patterns/legacy_hwm_manual/#source-data-dboorders","title":"Source Data (dbo.orders)","text":"<pre><code>order_id | customer_id | amount | created_at          | updated_at\n1        | 100         | 99.99  | 2025-01-20 10:00:00 | NULL\n2        | 101         | 49.99  | 2025-01-21 14:30:00 | 2025-01-21 15:00:00\n3        | 102         | 199.99 | 2025-01-23 09:00:00 | NULL\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#odibi-configuration","title":"Odibi Configuration","text":"<pre><code>nodes:\n  load_orders:\n    read:\n      connection: sql_server\n      format: sql_server\n      query: |\n        SELECT order_id, customer_id, amount, created_at, updated_at\n        FROM dbo.orders\n        WHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n\n    write:\n      connection: adls\n      format: delta\n      path: bronze/orders\n      mode: append\n      first_run_query: |\n        SELECT order_id, customer_id, amount, created_at, updated_at\n        FROM dbo.orders\n      options:\n        cluster_by: [created_at]\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#execution-timeline","title":"Execution Timeline","text":"<p>Day 1 (2025-01-20):</p> <pre><code>Run: first_run_query\nLoads: All 3 orders\nWrite: OVERWRITE (creates table)\nBronze table: 3 rows\n</code></pre> <p>Day 2 (2025-01-21):</p> <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-20)\nLoads: Order 1 (updated), Order 2 (new)\nWrite: APPEND\nBronze table: 5 rows (with duplicates of 1, 2)\n</code></pre> <p>Day 3 (2025-01-22):</p> <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-21)\nLoads: Order 2 (updated timestamp)\nWrite: APPEND\nBronze table: 6 rows\n</code></pre> <p>Day 4 (2025-01-23):</p> <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-22)\nLoads: Order 3 (new)\nWrite: APPEND\nBronze table: 7 rows\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#handling-duplicates","title":"Handling Duplicates","text":"<p>Since we append each day, you'll get duplicates in Bronze (which is fine\u2014that's what Raw/Bronze is for):</p> <pre><code>order_id | created_at          | load_date\n1        | 2025-01-20 10:00:00 | 2025-01-20  \u2190 Day 1\n1        | 2025-01-20 10:00:00 | 2025-01-21  \u2190 Day 2 (duplicate)\n2        | 2025-01-21 14:30:00 | 2025-01-21  \u2190 Day 2\n2        | 2025-01-21 14:30:00 | 2025-01-22  \u2190 Day 3 (duplicate)\n3        | 2025-01-23 09:00:00 | 2025-01-23  \u2190 Day 4\n</code></pre> <p>Silver layer (merge/upsert) deduplicates later using the merge transformer.</p>"},{"location":"patterns/legacy_hwm_manual/#sql-date-functions-by-database","title":"SQL Date Functions (by Database)","text":"Database Syntax SQL Server <code>DATEADD(DAY, -1, CAST(GETDATE() AS DATE))</code> PostgreSQL <code>CURRENT_DATE - INTERVAL '1 day'</code> MySQL <code>DATE_SUB(CURDATE(), INTERVAL 1 DAY)</code> Snowflake <code>DATEADD(day, -1, CURRENT_DATE())</code> BigQuery <code>DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)</code>"},{"location":"patterns/legacy_hwm_manual/#common-mistakes","title":"Common Mistakes","text":""},{"location":"patterns/legacy_hwm_manual/#single-query-for-everything","title":"\u274c Single query for everything","text":"<pre><code># Wrong: Won't capture first-run history\nquery: WHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#two-queries","title":"\u2705 Two queries","text":"<pre><code>first_run_query: SELECT * FROM table\nquery: WHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-instead-of","title":"\u274c Using <code>&gt;</code> instead of <code>&gt;=</code>","text":"<pre><code>-- Wrong: filters out today's data\nWHERE created_at &gt; DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-for-inclusive-range","title":"\u2705 Using <code>&gt;=</code> for inclusive range","text":"<pre><code>-- Right: includes today\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#only-using-created_at-misses-updates","title":"\u274c Only using created_at (misses updates)","text":"<pre><code>-- Wrong: updated records not captured\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-coalesceupdated_at-created_at","title":"\u2705 Using COALESCE(updated_at, created_at)","text":"<pre><code>-- Right: captures new AND updated\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#thats-it","title":"That's It","text":"<p>HWM is simple: two queries, Odibi chooses which one runs.</p> <ul> <li>First run: Odibi detects table doesn't exist, runs <code>first_run_query</code></li> <li>Subsequent runs: Odibi detects table exists, runs regular <code>query</code></li> <li>Automatic mode override: First run uses OVERWRITE, subsequent runs use APPEND</li> </ul> <p>Works great for loading from any SQL database into Bronze.</p>"},{"location":"patterns/merge_upsert/","title":"Pattern: Merge/Upsert (Raw \u2192 Silver)","text":"<p>Status: Core Pattern Layer: Silver (refined/cleaned) Engine: Spark (Delta) or Pandas Strategy: Merge (MERGE INTO in Spark) Idempotent: Yes (by key)  </p>"},{"location":"patterns/merge_upsert/#problem","title":"Problem","text":"<p>Raw contains duplicates and historical versions of records. You need: - One current version per key (deduplication) - Audit columns to track when each record was created/updated - Idempotency (rerunning doesn't create duplicates or double-count)</p> <p>How do you efficiently merge new/changed raw data into Silver while maintaining a clean current state?</p>"},{"location":"patterns/merge_upsert/#solution","title":"Solution","text":"<p>Use Delta Lake's MERGE operation to upsert records by key. Odibi provides the Merge Transformer to make this configuration-driven.</p>"},{"location":"patterns/merge_upsert/#how-it-works","title":"How It Works","text":"<p>MERGE Logic: 1. Read a batch of Raw data (new/changed rows) 2. Join with Silver by key columns 3. If matched: Update the Silver row with the new data 4. If not matched: Insert the new row 5. Auto-inject audit columns (created_at, updated_at)</p>"},{"location":"patterns/merge_upsert/#step-by-step-example","title":"Step-by-Step Example","text":""},{"location":"patterns/merge_upsert/#scenario-orders-table","title":"Scenario: Orders Table","text":"<p>Raw (after 2 runs; has duplicates):</p> <pre><code>order_id | product      | qty | created_at          | updated_at\n---------|--------------|-----|---------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL              \u2190 Duplicate\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 \u2190 Duplicate\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL\n</code></pre> <p>Silver (before merge):</p> <pre><code>order_id | product      | qty | created_at          | updated_at          | _created_ts        | _updated_ts\n---------|--------------|-----|---------------------|---------------------|--------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL                | 2025-11-01 10:30   | 2025-11-01 10:30\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL                | 2025-11-01 11:30   | 2025-11-01 11:30\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 | 2025-11-01 12:30   | 2025-11-01 13:30\n</code></pre>"},{"location":"patterns/merge_upsert/#merge-operation","title":"Merge Operation","text":"<p>New micro-batch from Raw (after dedup by timestamp):</p> <pre><code>order_id | product      | qty | created_at          | updated_at\n---------|--------------|-----|---------------------|-------------------\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL              \u2190 Duplicate, older timestamp\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL              \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL              \u2190 NEW\n</code></pre> <p>Merge By Key (<code>order_id</code>):</p> <pre><code>Row (order_id=2):\n  - Matches Silver row (order_id=2)\n  - Source timestamp: 2025-11-01 11:00\n  - Silver timestamp: 2025-11-01 13:30\n  - Source is older, skip? OR update anyway?\n  \u2192 MERGE strategy: UPDATE (keep latest by source created_at/updated_at)\n  \u2192 Actually: Insert latest version FIRST, then merge handles it\n  \u2192 Result: Silver row 2 unchanged (already has latest)\n\nRow (order_id=4):\n  - No match in Silver\n  - NEW row\n  \u2192 INSERT into Silver\n  \u2192 Set _created_ts = now(), _updated_ts = now()\n\nRow (order_id=5):\n  - No match in Silver\n  - NEW row\n  \u2192 INSERT into Silver\n  \u2192 Set _created_ts = now(), _updated_ts = now()\n</code></pre> <p>Silver (after merge):</p> <pre><code>order_id | product      | qty | created_at          | updated_at          | _created_ts        | _updated_ts\n---------|--------------|-----|---------------------|---------------------|--------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL                | 2025-11-01 10:30   | 2025-11-01 10:30\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL                | 2025-11-01 11:30   | 2025-11-01 11:30  (unchanged)\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 | 2025-11-01 12:30   | 2025-11-01 13:30  (unchanged)\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL                | 2025-11-02 09:30   | 2025-11-02 09:30  \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL                | 2025-11-02 10:30   | 2025-11-02 10:30  \u2190 NEW\n</code></pre> <p>Result: - Silver has exactly 5 unique orders (1 per key) - Duplicates from Raw are deduplicated - Audit columns track when Odibi processed each row - Idempotent: rerunning the same batch produces the same result</p>"},{"location":"patterns/merge_upsert/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/merge_upsert/#minimal-config","title":"Minimal Config","text":"<pre><code>- id: merge_orders_silver\n  name: \"Merge Orders to Silver\"\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.orders\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    strategy: upsert\n</code></pre>"},{"location":"patterns/merge_upsert/#full-config-with-audit-columns","title":"Full Config (with Audit Columns)","text":"<pre><code>- id: merge_orders_silver\n  name: \"Merge Orders: Raw \u2192 Silver\"\n  description: \"Deduplicate and upsert orders from raw layer\"\n  depends_on: [load_orders_raw]\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.orders\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    strategy: upsert\n    audit_cols:\n      created_col: _created_at\n      updated_col: _updated_at\n  validation:\n    not_empty: true\n    schema:\n      order_id:\n        type: integer\n        nullable: false\n      product:\n        type: string\n        nullable: false\n      qty:\n        type: integer\n        nullable: false\n</code></pre>"},{"location":"patterns/merge_upsert/#multi-key-example-composite-key","title":"Multi-Key Example (Composite Key)","text":"<pre><code>- id: merge_inventory_silver\n  name: \"Merge Inventory to Silver\"\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.inventory\n  transformer: merge\n  params:\n    target: silver.inventory\n    keys: [plant_id, material_id]  \u2190 Composite key\n    strategy: upsert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre>"},{"location":"patterns/merge_upsert/#merge-transformer-behavior","title":"Merge Transformer Behavior","text":""},{"location":"patterns/merge_upsert/#spark-delta","title":"Spark (Delta)","text":"<p>Uses native <code>DeltaTable.merge()</code>:</p> <pre><code># Pseudo-code\ndelta_table = DeltaTable.forName(\"silver.orders\")\ndelta_table.merge(\n    source_df,\n    condition=\"target.order_id = source.order_id\"\n) \\\n.whenMatchedUpdateAll() \\\n.whenNotMatchedInsertAll() \\\n.execute()\n\n# Auto-inject audit columns:\n# If insert: created_at = now(), updated_at = now()\n# If update: updated_at = now(), created_at unchanged\n</code></pre>"},{"location":"patterns/merge_upsert/#pandas","title":"Pandas","text":"<p>Loads, merges, overwrites:</p> <pre><code># Pseudo-code\ntarget = pd.read_parquet(\"silver/orders\")\nsource = df  # Input DataFrame\n\n# Merge indicator\nmerged = target.merge(source, on=['order_id'], how='outer', indicator=True)\n\n# Apply logic:\n# - Rows in target only: keep\n# - Rows in source only: insert\n# - Rows in both: update source values\n\n# Overwrite\nmerged.to_parquet(\"silver/orders\", mode=\"overwrite\")\n</code></pre>"},{"location":"patterns/merge_upsert/#strategy-options","title":"Strategy Options","text":"Strategy Behavior Best For <code>upsert</code> Insert new, update existing Standard use case (Raw \u2192 Silver) <code>append_only</code> Insert new, ignore duplicates Append-only tables (no updates) <code>delete_match</code> Delete matching rows Tombstones, soft deletes"},{"location":"patterns/merge_upsert/#audit-columns","title":"Audit Columns","text":""},{"location":"patterns/merge_upsert/#auto-injected-columns","title":"Auto-Injected Columns","text":"<p>When <code>audit_cols</code> is specified, Odibi adds two columns:</p> <pre><code># On INSERT\n_created_at = CURRENT_TIMESTAMP()\n_updated_at = CURRENT_TIMESTAMP()\n\n# On UPDATE\n_created_at = [unchanged]\n_updated_at = CURRENT_TIMESTAMP()\n</code></pre> <p>This lets you track when Odibi processed each record, separate from the source's created/updated columns.</p>"},{"location":"patterns/merge_upsert/#example","title":"Example","text":"<pre><code>audit_cols:\n  created_col: _sys_created_ts\n  updated_col: _sys_updated_ts\n</code></pre> <p>Result:</p> <pre><code>order_id | product | _sys_created_ts        | _sys_updated_ts\n---------|---------|------------------------|-------------------\n1        | Widget  | 2025-11-01 10:30:00    | 2025-11-01 10:30:00\n2        | Gadget  | 2025-11-01 11:30:00    | 2025-11-02 14:45:00  \u2190 Updated\n</code></pre>"},{"location":"patterns/merge_upsert/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/merge_upsert/#advantages","title":"Advantages","text":"<p>\u2713 Deduplicates Raw data automatically \u2713 Idempotent (safe to rerun) \u2713 Tracks data lineage (audit columns) \u2713 Handles both new and changed rows efficiently \u2713 Spark merge is fast (native Delta operation)  </p>"},{"location":"patterns/merge_upsert/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Requires primary key (what makes each row unique?) \u2717 Overwrites previous values (no history of all versions) \u2717 Pandas merge is slower than Spark (pandas mode not recommended for large tables)  </p>"},{"location":"patterns/merge_upsert/#common-patterns","title":"Common Patterns","text":""},{"location":"patterns/merge_upsert/#pattern-scd-type-1-current-state-only","title":"Pattern: SCD Type 1 (Current State Only)","text":"<p>Keep only the latest version of each record. This is the default merge pattern.</p> <pre><code>transformer: merge\nparams:\n  target: silver.customers\n  keys: [customer_id]\n  strategy: upsert\n</code></pre>"},{"location":"patterns/merge_upsert/#pattern-scd-type-2-full-history","title":"Pattern: SCD Type 2 (Full History)","text":"<p>Keep all historical versions with effective dates. NOT supported by standard merge. Use a separate <code>dim_customers</code> table with: - <code>customer_id</code> - <code>effective_from</code>, <code>effective_to</code> - <code>is_current</code> flag</p> <p>Then maintain it with a separate pipeline.</p>"},{"location":"patterns/merge_upsert/#pattern-append-only-no-duplicates","title":"Pattern: Append-Only (No Duplicates)","text":"<p>If your table should never have duplicates and you want to avoid updates:</p> <pre><code>transformer: merge\nparams:\n  target: silver.events\n  keys: [event_id]\n  strategy: append_only\n</code></pre> <p>This inserts new rows but ignores duplicates instead of updating.</p>"},{"location":"patterns/merge_upsert/#pattern-connection-based-path-with-table-registration","title":"Pattern: Connection-Based Path with Table Registration","text":"<p>Use a connection to resolve ADLS paths and register the table in Unity Catalog:</p> <pre><code>transform:\n  steps:\n    - sql_file: \"sql/clean_orders.sql\"\n    - function: merge\n      params:\n        connection: adls_prod\n        path: OEE/silver/orders\n        register_table: silver.orders\n        keys: [order_id]\n        strategy: upsert\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre> <p>This: 1. Resolves the full ADLS path via the <code>adls_prod</code> connection 2. Performs the merge operation 3. Registers the Delta table as <code>silver.orders</code> in the metastore</p>"},{"location":"patterns/merge_upsert/#debugging","title":"Debugging","text":""},{"location":"patterns/merge_upsert/#check-for-duplicates-in-silver","title":"Check for Duplicates in Silver","text":"<pre><code>SELECT order_id, COUNT(*) as count\nFROM silver.orders\nGROUP BY order_id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If you see duplicates, your merge key is wrong.</p>"},{"location":"patterns/merge_upsert/#check-merge-history","title":"Check Merge History","text":"<pre><code>DESCRIBE HISTORY silver.orders\n</code></pre> <p>Shows every merge operation, versions, and row counts.</p>"},{"location":"patterns/merge_upsert/#when-to-use","title":"When to Use","text":"<ul> <li>Always for Raw \u2192 Silver refinement</li> <li>Multiple sources merging into same table</li> <li>Need to track data lineage (audit columns)</li> <li>Want idempotent transformations</li> </ul>"},{"location":"patterns/merge_upsert/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Audit tables (keep appending)</li> <li>SCD Type 2 (need version history)</li> <li>Data that should be immutable (use append instead)</li> </ul>"},{"location":"patterns/merge_upsert/#related-patterns","title":"Related Patterns","text":"<ul> <li>Append-Only Raw \u2192 Source layer (unmerged, duplicates OK)</li> <li>High Water Mark \u2192 How to efficiently feed Raw with new data</li> </ul>"},{"location":"patterns/merge_upsert/#references","title":"References","text":"<ul> <li>Databricks: Delta Lake MERGE</li> <li>Fundamentals of Data Engineering: Chapter on SCD</li> </ul>"},{"location":"patterns/scd2/","title":"SCD Type 2 (Slowly Changing Dimensions)","text":"<p>The SCD Type 2 pattern allows you to track the full history of changes for a record over time. Unlike a simple update (which overwrites the old value), SCD2 keeps the old version and adds a new version, managing effective dates for you.</p>"},{"location":"patterns/scd2/#the-time-machine-concept","title":"The \"Time Machine\" Concept","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: Each record has an \"effective window\" (<code>effective_time</code> to <code>end_time</code>) and a flag (<code>is_current</code>) indicating if it is the latest version.</p>"},{"location":"patterns/scd2/#visual-example","title":"Visual Example","text":"<p>Input (Source Update): Customer 101 moved to NY on Feb 1st.</p> customer_id address tier txn_date 101 NY Gold 2024-02-01 <p>Target Table (Before): Customer 101 lived in CA since Jan 1st.</p> customer_id address tier txn_date valid_to is_active 101 CA Gold 2024-01-01 NULL true <p>Target Table (After SCD2): Old record CLOSED (valid_to set). New record OPEN (is_active=true).</p> customer_id address tier txn_date valid_to is_active 101 CA Gold 2024-01-01 2024-02-01 false 101 NY Gold 2024-02-01 NULL true"},{"location":"patterns/scd2/#configuration","title":"Configuration","text":"<p>Use the <code>scd2</code> transformer in your pipeline node.</p>"},{"location":"patterns/scd2/#option-1-using-table-name","title":"Option 1: Using Table Name","text":"<pre><code>nodes:\n  - name: \"dim_customers\"\n    # ... (read from source) ...\n\n    transformer: \"scd2\"\n    params:\n      target: \"silver.dim_customers\"   # Registered table name\n      keys: [\"customer_id\"]            # Unique ID\n      track_cols: [\"address\", \"tier\"]  # Changes here trigger a new version\n      effective_time_col: \"txn_date\"   # When the change happened\n\n    write:\n      table: \"silver.dim_customers\"\n      format: \"delta\"\n      mode: \"overwrite\"                # Important: SCD2 returns FULL history\n</code></pre>"},{"location":"patterns/scd2/#option-2-using-connection-path-adls","title":"Option 2: Using Connection + Path (ADLS)","text":"<pre><code>nodes:\n  - name: \"dim_customers\"\n    # ... (read from source) ...\n\n    transformer: \"scd2\"\n    params:\n      connection: adls_prod            # READ existing history from here\n      path: OEE/silver/dim_customers\n      keys: [\"customer_id\"]\n      track_cols: [\"address\", \"tier\"]\n      effective_time_col: \"txn_date\"\n\n    write:\n      connection: adls_prod            # WRITE result back (same location)\n      path: OEE/silver/dim_customers\n      format: \"delta\"\n      mode: \"overwrite\"\n</code></pre> <p>Why specify the path twice? - <code>params.connection/path</code> \u2192 Where to read existing history (to detect changes) - <code>write.connection/path</code> \u2192 Where to write the full result</p> <p>They should typically match. SCD2 returns a DataFrame (doesn't write directly), so the write phase handles persistence separately.</p>"},{"location":"patterns/scd2/#full-configuration","title":"Full Configuration","text":"<pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"       # OR use connection + path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\", \"email\"]\n\n  # Source column for start date\n  effective_time_col: \"updated_at\"\n\n  # Target columns to manage (optional defaults shown)\n  end_time_col: \"valid_to\"\n  current_flag_col: \"is_current\"\n</code></pre>"},{"location":"patterns/scd2/#how-it-works","title":"How It Works","text":"<p>The <code>scd2</code> transformer performs a complex set of operations automatically:</p> <ol> <li>Match: Finds existing records in the <code>target</code> table using <code>keys</code>.</li> <li>Compare: Checks <code>track_cols</code> to see if any data has changed.</li> <li>Close: If a record changed, it updates the old record's <code>end_time_col</code> to equal the new record's <code>effective_time_col</code>, and sets <code>is_current = false</code>.</li> <li>Insert: It adds the new record with <code>effective_time_col</code> as the start date, <code>NULL</code> as the end date, and <code>is_current = true</code>.</li> <li>Preserve: It keeps all unchanged history records as they are.</li> </ol>"},{"location":"patterns/scd2/#important-notes","title":"Important Notes","text":"<ul> <li>Write Mode: You must use <code>mode: overwrite</code> for the write operation following this transformer. The transformer constructs the complete new state of the history table (including old closed records and new open records).</li> <li>Target Existence: If the target table doesn't exist (first run), the transformer simply prepares the source data (adds valid_to/is_current columns) and returns it.</li> <li>Engine Support: Works on both Spark (Delta Lake) and Pandas (Parquet/CSV).</li> </ul>"},{"location":"patterns/scd2/#when-to-use","title":"When to Use","text":"<ul> <li>Dimension Tables: Customer dimensions, Product dimensions where attributes change slowly over time.</li> <li>Audit Trails: When you need exact historical state reconstruction.</li> </ul>"},{"location":"patterns/scd2/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Fact Tables: Events (Transactions, Logs) are immutable; they don't change state, they just occur. Use <code>append</code> instead.</li> <li>Rapidly Changing Data: If a record changes 100 times a day, SCD2 will explode your storage size. Use a snapshot or aggregate approach instead.</li> </ul>"},{"location":"patterns/scd2/#common-errors-and-debugging","title":"Common Errors and Debugging","text":"<p>This section covers the most common SCD2 errors and how to fix them.</p>"},{"location":"patterns/scd2/#error-effective_time_col-not-found","title":"Error: <code>effective_time_col</code> Not Found","text":"<p>Error Message:</p> <pre><code>KeyError: 'updated_at'\n# or\nAnalysisException: Column 'updated_at' does not exist\n</code></pre> <p>What It Means (Plain English): The <code>effective_time_col</code> you specified doesn't exist in your source DataFrame.</p> <p>Why It Happens: - The column name is misspelled - The column was renamed or dropped upstream - Case sensitivity mismatch (<code>Updated_At</code> vs <code>updated_at</code>)</p> <p>Step-by-Step Fix:</p> <ol> <li> <p>Check your source data columns: <code>python    # Add this before the SCD2 node to debug    df = spark.read.format(\"delta\").load(\"bronze/customers\")    print(df.columns)    # Output: ['customer_id', 'name', 'UpdatedAt', 'address']    # Aha! It's 'UpdatedAt', not 'updated_at'</code></p> </li> <li> <p>Fix the YAML:    ```yaml    # BEFORE (wrong)    params:      effective_time_col: \"updated_at\"  # \u274c Doesn't exist</p> </li> </ol> <p># AFTER (correct)    params:      effective_time_col: \"UpdatedAt\"  # \u2705 Matches actual column    ```</p> <p>Important: The <code>effective_time_col</code> must exist in the SOURCE data, not the target. After SCD2 processing, this column gets used to populate the history columns.</p>"},{"location":"patterns/scd2/#error-track_cols-column-mismatch","title":"Error: <code>track_cols</code> Column Mismatch","text":"<p>Error Message:</p> <pre><code>KeyError: 'email'\n# or\nColumn 'Email' not found in schema\n</code></pre> <p>What It Means: One of the columns in <code>track_cols</code> doesn't exist in your source data, or there's a case mismatch.</p> <p>Why It Happens: - Column names are case-sensitive - A column was renamed in the source system - You're tracking a column that doesn't exist yet</p> <p>Step-by-Step Fix:</p> <ol> <li> <p>List actual columns: <code>python    df = spark.read.format(\"delta\").load(\"bronze/customers\")    print(df.columns)    # ['customer_id', 'Name', 'Email', 'Address']</code></p> </li> <li> <p>Match case exactly:    ```yaml    # BEFORE (wrong - case doesn't match)    params:      track_cols: [\"name\", \"email\", \"address\"]</p> </li> </ol> <p># AFTER (correct - matches actual columns)    params:      track_cols: [\"Name\", \"Email\", \"Address\"]    ```</p> <p>\ud83d\udca1 Pro Tip: Consider normalizing column names to lowercase in Bronze/Silver to avoid case issues:</p> <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        lowercase: true\n</code></pre>"},{"location":"patterns/scd2/#error-schema-evolution-issues","title":"Error: Schema Evolution Issues","text":"<p>Error Message:</p> <pre><code>AnalysisException: A]chema mismatch detected:\n- Expected: customer_id: string, name: string, address: string, ...\n- Actual:   customer_id: string, name: string, phone: string, ...\n</code></pre> <p>What It Means: Your target table (from a previous run) has a different schema than the new data.</p> <p>Why It Happens: - Source added new columns (e.g., <code>phone</code>) - Source removed columns (e.g., dropped <code>address</code>) - Column types changed (e.g., <code>int</code> \u2192 <code>string</code>)</p> <p>Step-by-Step Fix:</p> <p>Option 1: Allow Schema Merging</p> <pre><code>write:\n  connection: silver\n  table: dim_customers\n  format: delta\n  mode: overwrite\n  delta_options:\n    mergeSchema: true  # \u2705 Allows new columns\n</code></pre> <p>Option 2: Handle in Transform</p> <pre><code># Add missing columns with defaults before SCD2\ntransform:\n  steps:\n    - function: \"derive_columns\"\n      params:\n        columns:\n          phone: \"COALESCE(phone, 'unknown')\"\n</code></pre> <p>Option 3: Full Schema Reset (Nuclear Option)</p> <pre><code># Delete target table and rerun from scratch\n# WARNING: Loses all history!\nspark.sql(\"DROP TABLE IF EXISTS silver.dim_customers\")\n</code></pre>"},{"location":"patterns/scd2/#why-did-my-row-count-explode","title":"\"Why Did My Row Count Explode?\"","text":"<p>Symptom:</p> <pre><code>Before: dim_customers had 10,000 rows\nAfter:  dim_customers has 50,000 rows\n</code></pre> <p>What's Happening: SCD2 is working correctly! Every time a tracked column changes, it creates a new version. If you ran it multiple times or have duplicates, you get multiple versions per record.</p> <p>Common Causes:</p> <ol> <li> <p>Duplicate source data: <code>customer_id | name  | updated_at    101         | Alice | 2024-01-01    101         | Alice | 2024-01-01  &lt;- Duplicate!    101         | Alice | 2024-01-01  &lt;- Another duplicate!</code> Fix: Deduplicate before SCD2 (see Anti-Patterns)</p> </li> <li> <p>Running SCD2 on append-mode source:    Each run sees ALL historical source data, creating versions for old changes again.    Fix: Use incremental loading or filter source to only new records.</p> </li> <li> <p>Tracking too many columns:    ```yaml    # Tracking every column = version explosion    track_cols: [\"*\"]  # \u274c Don't do this!</p> </li> </ol> <p># Track only meaningful business changes    track_cols: [\"tier\", \"status\", \"region\"]  # \u2705 Selective    ```</p> <p>Debugging Query:</p> <pre><code>-- Find customers with excessive versions\nSELECT customer_id, COUNT(*) as version_count\nFROM dim_customers\nGROUP BY customer_id\nHAVING COUNT(*) &gt; 10\nORDER BY version_count DESC\n</code></pre>"},{"location":"patterns/scd2/#debugging-checklist","title":"Debugging Checklist","text":"<p>Before running your SCD2 pipeline, verify these items:</p> <pre><code># \u2705 DEBUGGING CHECKLIST\n# Print this and check each box:\n\n# [ ] 1. Source Data Check\n#     - effective_time_col exists in source\n#     - All track_cols exist in source\n#     - Column names match case exactly\n\n# [ ] 2. Key Column Check  \n#     - keys columns exist in source\n#     - keys columns have no NULLs\n#     - keys columns uniquely identify records\n\n# [ ] 3. Target Table Check\n#     - Target exists (or this is first run)\n#     - Target schema is compatible\n#     - Target has end_time_col and current_flag_col\n\n# [ ] 4. Deduplication Check\n#     - Source has no duplicate keys\n#     - If duplicates exist, deduplicate BEFORE SCD2\n\n# [ ] 5. Write Mode Check\n#     - Using mode: overwrite (required for SCD2)\n</code></pre> <p>Python Debugging Script:</p> <pre><code># Add this before your SCD2 node to validate\ndef validate_scd2_input(df, config):\n    \"\"\"Validate data before SCD2 processing.\"\"\"\n    errors = []\n\n    # Check effective_time_col exists\n    if config['effective_time_col'] not in df.columns:\n        errors.append(f\"effective_time_col '{config['effective_time_col']}' not in columns: {df.columns}\")\n\n    # Check all track_cols exist\n    for col in config['track_cols']:\n        if col not in df.columns:\n            errors.append(f\"track_col '{col}' not in columns: {df.columns}\")\n\n    # Check for duplicate keys\n    key_cols = config['keys']\n    dup_count = df.groupBy(key_cols).count().filter(\"count &gt; 1\").count()\n    if dup_count &gt; 0:\n        errors.append(f\"Found {dup_count} duplicate keys! Deduplicate first.\")\n\n    # Check for NULL keys\n    for key in key_cols:\n        null_count = df.filter(df[key].isNull()).count()\n        if null_count &gt; 0:\n            errors.append(f\"Found {null_count} NULL values in key column '{key}'\")\n\n    if errors:\n        for e in errors:\n            print(f\"\u274c {e}\")\n        raise ValueError(\"SCD2 validation failed. Fix errors above.\")\n    else:\n        print(\"\u2705 SCD2 input validation passed\")\n</code></pre>"},{"location":"patterns/scd2/#quick-reference-scd2-error-cheat-sheet","title":"Quick Reference: SCD2 Error Cheat Sheet","text":"Error Likely Cause Quick Fix <code>effective_time_col not found</code> Column doesn't exist or wrong name Check source columns, fix spelling/case <code>track_col X not found</code> Column name mismatch Match exact column names including case Schema mismatch Target has different columns Use <code>mergeSchema: true</code> or reset target Row count explosion Duplicates or too many runs Deduplicate source first <code>merge_key not found</code> Key column missing Verify keys exist in both source and target NULL in key columns Missing business keys Handle NULLs before SCD2"},{"location":"patterns/scd2/#next-steps","title":"Next Steps","text":"<ul> <li>Anti-Patterns Guide - What NOT to do with SCD2</li> <li>Dimension Pattern - Full dimension table management</li> <li>Troubleshooting Guide - General debugging</li> </ul>"},{"location":"patterns/skip_if_unchanged/","title":"Skip If Unchanged Pattern","text":"<p>Use Case: Avoid redundant writes for snapshot tables that may not change between pipeline runs.</p>"},{"location":"patterns/skip_if_unchanged/#the-problem","title":"The Problem","text":"<p>When ingesting snapshot data (full table extracts without timestamps), an hourly pipeline will append the same 192k rows 24 times per day if the source data doesn't change. This wastes:</p> <ul> <li>Storage: 24\u00d7 the necessary data</li> <li>Compute: Unnecessary write operations</li> <li>Query performance: More files to scan</li> </ul>"},{"location":"patterns/skip_if_unchanged/#the-solution","title":"The Solution","text":"<p>The <code>skip_if_unchanged</code> feature computes a hash of the DataFrame content before writing. If the hash matches the previous write, the write is skipped entirely.</p> <pre><code>nodes:\n  - name: bronze_data\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.MySnapshotTable\n    write:\n      connection: bronze\n      format: delta\n      table: my_snapshot_table\n      mode: append\n      skip_if_unchanged: true\n      skip_hash_sort_columns: [P_ID, DateId]  # For deterministic ordering\n</code></pre>"},{"location":"patterns/skip_if_unchanged/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    A[Read source data] --&gt; B[Compute SHA256 hash of DataFrame]\n    B --&gt; C{Previous hash exists?}\n    C --&gt;|No| D[Write data]\n    D --&gt; E[Store hash in Delta metadata]\n    C --&gt;|Yes| F{Hashes match?}\n    F --&gt;|Yes| G[Skip write, log 'unchanged']\n    F --&gt;|No| D\n</code></pre> <ol> <li>Hash Computation: Before writing, the entire DataFrame is converted to CSV bytes and hashed with SHA256</li> <li>Hash Storage: The hash is stored in Delta table properties (<code>odibi.content_hash</code>)</li> <li>Comparison: On subsequent runs, the new hash is compared to the stored hash</li> <li>Skip or Write: If hashes match, the write is skipped; otherwise, data is written and hash updated</li> </ol>"},{"location":"patterns/skip_if_unchanged/#configuration-options","title":"Configuration Options","text":"Option Type Description <code>skip_if_unchanged</code> bool Enable hash-based skip detection <code>skip_hash_columns</code> list Subset of columns to hash (default: all) <code>skip_hash_sort_columns</code> list Columns to sort by before hashing (for determinism)"},{"location":"patterns/skip_if_unchanged/#when-to-use","title":"When to Use","text":"<p>\u2705 Good fit: - Snapshot tables without <code>updated_at</code> timestamps - Reference/dimension data that changes infrequently - Tables where you don't know the change frequency</p> <p>\u274c Not recommended: - Tables with reliable <code>updated_at</code> (use HWM instead) - Append-only fact tables (new data every run) - Very large tables (hash computation is expensive)</p>"},{"location":"patterns/skip_if_unchanged/#example-global-manufacturing-data","title":"Example: Global Manufacturing Data","text":"<pre><code># Pipeline runs hourly for global coverage\n# But this table only changes 1-2 times per day\n\nnodes:\n  - name: bronze_osmdssds_detail\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.vw_OSMDSSDSEDetail\n    write:\n      connection: bronze\n      format: delta\n      table: osmdssds_detail\n      mode: append\n      add_metadata: true\n      skip_if_unchanged: true\n      skip_hash_sort_columns: [P_ID, DateId]\n</code></pre> <p>Result: - Pipeline checks every hour (data is fresh when needed) - Only writes when data actually changes (storage efficient) - Logs show \"Skipping write - content unchanged\" for skipped runs</p>"},{"location":"patterns/skip_if_unchanged/#storage-impact","title":"Storage Impact","text":"Scenario Daily Writes Annual Rows (192k/snapshot) No skip (hourly) 24 1.7 billion With skip (2 changes/day) 2 140 million Savings 92% 92%"},{"location":"patterns/skip_if_unchanged/#limitations","title":"Limitations","text":"<ol> <li>Delta only: Currently only supported for Delta format</li> <li>Full DataFrame hash: Computes hash of entire DataFrame (not row-by-row)</li> <li>Memory: DataFrame must fit in driver memory for hashing</li> <li>First run: Always writes on first run (no previous hash to compare)</li> </ol>"},{"location":"patterns/skip_if_unchanged/#related-patterns","title":"Related Patterns","text":"<ul> <li>Append-Only Raw Layer - Bronze layer best practices</li> <li>Incremental Stateful - For tables with timestamps</li> <li>Smart Read - Full load pattern</li> </ul>"},{"location":"patterns/smart_read/","title":"Smart Read (Rolling Window)","text":"<p>The \"Smart Read\" feature simplifies incremental data loading by automatically generating the correct SQL query based on time windows.</p> <p>Note: This page describes the Rolling Window mode (Stateless). For exact state tracking (HWM), see Stateful Incremental Loading.</p> <p>It eliminates the need to write complex SQL with <code>first_run_query</code> and dialect-specific date math.</p> <p>!!! warning \"Requirement: Write Configuration\"     Smart Read requires a <code>write</code> block in the same node.</p> <pre><code>It determines whether to run a **Full Load** or **Incremental Load** by checking if the destination defined in `write` already exists.\n\n*   If you only want to read data (without writing), use the standard `query` option with explicit date filters instead.\n*   Ensure your `write` mode is set correctly (usually `append`) to preserve history.\n</code></pre>"},{"location":"patterns/smart_read/#write-modes-for-incremental","title":"Write Modes for Incremental","text":"Mode Suitability Why? <code>append</code> \u2705 Recommended Safely adds new records to the lake. Preserves history. <code>upsert</code> \u26a0\ufe0f Advanced Use only if you are merging directly into a Silver layer table and have defined keys. <code>overwrite</code> \u274c Dangerous Do NOT use. This would replace your entire historical dataset with just the latest batch (e.g., the last 3 days)."},{"location":"patterns/smart_read/#how-it-works","title":"How It Works","text":"<p>Odibi checks if your Write target exists:</p> <ol> <li> <p>Target Missing (First Run):</p> <ul> <li>It assumes this is a historical load.</li> <li>Generates: <code>SELECT * FROM source_table</code></li> <li>Result: Loads all history.</li> </ul> </li> <li> <p>Target Exists (Subsequent Runs):</p> <ul> <li>It assumes this is an incremental load.</li> <li>Generates: <code>SELECT * FROM source_table WHERE column &gt;= [Calculated Date]</code></li> <li>Result: Loads only new/changed data.</li> </ul> </li> </ol>"},{"location":"patterns/smart_read/#the-standard-pattern-ingest-to-bronze","title":"The Standard Pattern: \"Ingest to Bronze\"","text":"<p>The most common use case for Smart Read is the Ingestion Node. This node acts as a bridge between your external source (SQL, API) and your Data Lake (Bronze Layer).</p>"},{"location":"patterns/smart_read/#why-use-this-pattern","title":"Why use this pattern?","text":"<ol> <li>State Management: The node uses the Write Target (e.g., <code>bronze_orders</code>) as its state.<ul> <li>Target Empty? $\\rightarrow$ Run <code>SELECT *</code> (Full History)</li> <li>Target Exists? $\\rightarrow$ Run <code>SELECT * ... WHERE date &gt; X</code> (Incremental)</li> </ul> </li> <li>Efficiency: Downstream nodes (e.g., \"clean_orders\") can simply depend on this node. They will receive the dataframe containing only the data that was just ingested (the incremental batch), allowing your entire pipeline to process only new data efficiently.</li> </ol>"},{"location":"patterns/smart_read/#example-node","title":"Example Node","text":"<pre><code>- name: \"ingest_orders\"\n  description: \"Incrementally load orders from SQL to Delta\"\n\n  # 1. READ (Source)\n  read:\n    connection: \"sql_db\"\n    format: \"sql\"\n    table: \"orders\"\n    incremental:\n      column: \"updated_at\"\n      lookback: 3\n      unit: \"day\"\n\n  # 2. WRITE (Target - Required for state tracking)\n  write:\n    connection: \"data_lake\"\n    format: \"delta\"\n    table: \"bronze_orders\"\n    mode: \"append\"  # Append new rows from the incremental batch\n</code></pre>"},{"location":"patterns/smart_read/#configuration","title":"Configuration","text":"<p>Use the <code>incremental</code> block in your <code>read</code> configuration.</p>"},{"location":"patterns/smart_read/#example-handling-updates-inserts","title":"Example: Handling Updates &amp; Inserts","text":"<p>This pattern handles both new records (<code>created_at</code>) and updates (<code>updated_at</code>).</p> <pre><code>nodes:\n  - name: \"load_orders\"\n    read:\n      connection: \"sql_server_prod\"\n      format: \"sql\"\n      table: \"dbo.orders\"\n\n      incremental:\n        column: \"updated_at\"         # Primary check\n        fallback_column: \"created_at\" # If updated_at is NULL\n        lookback: 1\n        unit: \"day\"\n\n    write:\n      connection: \"bronze\"\n      format: \"delta\"\n      table: \"orders_raw\"\n      mode: \"append\"\n</code></pre> <p>This generates:</p> <pre><code>SELECT * FROM dbo.orders\nWHERE COALESCE(updated_at, created_at) &gt;= '2023-10-25 10:00:00'\n</code></pre>"},{"location":"patterns/smart_read/#example-simple-append-only","title":"Example: Simple Append-Only","text":"<p>Perfect for pipelines that run every hour but want a 4-hour safety window for late-arriving data.</p> <pre><code>    read:\n      connection: \"postgres_db\"\n      format: \"sql\"\n      table: \"public.events\"\n      incremental:\n        column: \"event_time\"\n        lookback: 4\n        unit: \"hour\"\n\n    write:\n      connection: \"bronze\"\n      format: \"delta\"\n      table: \"events_raw\"\n      mode: \"append\"\n</code></pre>"},{"location":"patterns/smart_read/#advanced-merging-directly-to-silver-upsert","title":"Advanced: Merging directly to Silver (Upsert)","text":"<p>If you are bypassing Bronze and merging directly into a Silver table, you can use <code>upsert</code>. Note: This requires defining the primary <code>keys</code> to match on.</p> <pre><code>    read:\n      connection: \"crm_db\"\n      format: \"sql\"\n      table: \"customers\"\n      incremental:\n        column: \"last_modified\"\n        lookback: 1\n        unit: \"day\"\n\n    write:\n      connection: \"silver\"\n      format: \"delta\"\n      table: \"dim_customers\"\n      mode: \"upsert\"\n      options:\n        keys: [\"customer_id\"]\n</code></pre>"},{"location":"patterns/smart_read/#supported-units","title":"Supported Units","text":"Unit Description <code>hour</code> Looks back N hours from <code>now()</code> <code>day</code> Looks back N days from <code>now()</code> <code>month</code> Looks back N * 30 days (approx) <code>year</code> Looks back N * 365 days (approx)"},{"location":"patterns/smart_read/#comparison-with-legacy-pattern","title":"Comparison with Legacy Pattern","text":""},{"location":"patterns/smart_read/#old-way-manual","title":"\u274c Old Way (Manual)","text":"<p>You had to write two queries and know the SQL dialect.</p> <pre><code>read:\n  query: \"SELECT * FROM orders WHERE updated_at &gt;= DATEADD(DAY, -1, GETDATE())\"\nwrite:\n  first_run_query: \"SELECT * FROM orders\"\n</code></pre>"},{"location":"patterns/smart_read/#new-way-smart-read","title":"\u2705 New Way (Smart Read)","text":"<p>Configuration is declarative and dialect-agnostic.</p> <pre><code>read:\n  table: \"orders\"\n  incremental:\n    column: \"updated_at\"\n    lookback: 1\n    unit: \"day\"\n</code></pre>"},{"location":"patterns/smart_read/#faq","title":"FAQ","text":"<p>Q: What if I want to reload all history manually? A: You can simply delete the target table (or folder) in your data lake. The next run will detect it's missing and trigger the full historical load.</p> <p>Q: Does this work with <code>depends_on</code>? A: This feature is for Ingestion Nodes (Node 1) that read from external systems. Downstream nodes automatically benefit because they receive the data frame produced by Node 1.</p> <p>Q: Can I mix this with custom SQL? A: No. If you provide a <code>query</code> in the <code>read</code> section, Odibi respects your manual query and ignores the <code>incremental</code> block.</p>"},{"location":"patterns/windowed_reprocess/","title":"Pattern: Windowed Reprocess (Silver \u2192 Gold Aggregates)","text":"<p>Status: Core Pattern Layer: Gold (aggregated/BI-ready) Engine: Spark Batch Write Mode: <code>overwrite</code> (partition-specific) Idempotent: Yes (recalculated)  </p>"},{"location":"patterns/windowed_reprocess/#problem","title":"Problem","text":"<p>You have a Gold aggregate table (e.g., daily sales summary). Late-arriving data in Silver invalidates yesterday's numbers. You need to: - Fix aggregates when new data arrives - Avoid double-counting (can't just add new rows) - Keep calculations simple (always correct, never patched)</p> <p>How do you maintain accurate aggregates without complex update logic?</p>"},{"location":"patterns/windowed_reprocess/#solution","title":"Solution","text":"<p>Instead of patching aggregates with updates (error-prone), recalculate the entire time window and replace it.</p> <p>Principle: \"Rebuild the Bucket, Don't Patch the Hole\"</p>"},{"location":"patterns/windowed_reprocess/#how-it-works","title":"How It Works","text":""},{"location":"patterns/windowed_reprocess/#the-reprocess-pattern","title":"The Reprocess Pattern","text":"<ol> <li>Identify the window (e.g., \"Last 7 days\", \"This month\")</li> <li>Read Silver filtered to that window</li> <li>Recalculate aggregate (SUM, COUNT, AVG, etc.)</li> <li>Write to Gold with Dynamic Partition Overwrite</li> </ol> <p>If late data arrives in the last 7 days, next run recalculates those days\u2014automatically fixing aggregates.</p>"},{"location":"patterns/windowed_reprocess/#step-by-step-example","title":"Step-by-Step Example","text":""},{"location":"patterns/windowed_reprocess/#scenario-daily-sales-summary","title":"Scenario: Daily Sales Summary","text":"<p>Silver Table (Orders, with timestamps):</p> <p>Day 1 (Initial load on 2025-11-01 at 10:00):</p> <pre><code>order_id | order_date | amount | created_at\n---------|------------|--------|-------------------\n1        | 2025-11-01 | 100    | 2025-11-01 10:00\n2        | 2025-11-01 | 50     | 2025-11-01 10:30\n3        | 2025-10-31 | 200    | 2025-11-01 10:45\n</code></pre> <p>Run 1 (Calculate last 7 days: 2025-10-25 to 2025-11-01):</p> <pre><code>SELECT\n  DATE(order_date) as order_date,\n  SUM(amount) as total_sales,\n  COUNT(*) as order_count\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n</code></pre> <p>Gold Result (Days 25-Oct to 1-Nov):</p> <pre><code>order_date | total_sales | order_count\n-----------|-------------|-------------\n2025-10-31 | 200         | 1\n2025-11-01 | 150         | 2\n</code></pre> <p>Partition written: <code>order_date=2025-10-31</code>, <code>order_date=2025-11-01</code></p>"},{"location":"patterns/windowed_reprocess/#day-2-late-data-arrives","title":"Day 2: Late Data Arrives","text":"<p>Silver Table (New data arrived at 14:00):</p> <pre><code>order_id | order_date | amount | created_at\n---------|------------|--------|-------------------\n1        | 2025-11-01 | 100    | 2025-11-01 10:00\n2        | 2025-11-01 | 50     | 2025-11-01 10:30\n3        | 2025-10-31 | 200    | 2025-11-01 10:45\n4        | 2025-11-01 | 75     | 2025-11-02 14:00  \u2190 LATE DATA (same day, arrived late)\n</code></pre> <p>Run 2 (Recalculate last 7 days: 2025-10-25 to 2025-11-02):</p> <pre><code>SELECT\n  DATE(order_date) as order_date,\n  SUM(amount) as total_sales,\n  COUNT(*) as order_count\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n</code></pre> <p>Gold Result (Days 25-Oct to 2-Nov):</p> <pre><code>order_date | total_sales | order_count\n-----------|-------------|-------------\n2025-10-31 | 200         | 1\n2025-11-01 | 225         | 3              \u2190 UPDATED (was 150, now 225)\n</code></pre> <p>Write Mode: Dynamic Partition Overwrite - Existing partition <code>order_date=2025-10-31</code> is untouched - Partition <code>order_date=2025-11-01</code> is replaced entirely (was 2 rows, now 3 rows)</p> <p>No double-counting: The aggregate is recalculated from scratch, not patched.</p>"},{"location":"patterns/windowed_reprocess/#why-this-works","title":"Why This Works","text":""},{"location":"patterns/windowed_reprocess/#without-windowed-reprocess-wrong","title":"Without Windowed Reprocess (WRONG)","text":"<pre><code>-- Don't do this\nUPDATE gold.sales\nSET total_sales = total_sales + 75,\n    order_count = order_count + 1\nWHERE order_date = '2025-11-01'\n</code></pre> <p>Problems: - If this query runs twice, you add 75 twice (double-counting) - If you run it out-of-order, you corrupt data - Requires tracking \"what did I update?\"</p>"},{"location":"patterns/windowed_reprocess/#with-windowed-reprocess-right","title":"With Windowed Reprocess (RIGHT)","text":"<pre><code>-- Recalculate the entire 7-day window\nSELECT\n  DATE(order_date),\n  SUM(amount),\n  COUNT(*)\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n\n-- Write with Dynamic Partition Overwrite\n-- Entire partition is replaced\n</code></pre> <p>Advantages: - Idempotent (run 10 times = same result) - No double-counting (always fresh calculation) - Simple logic (standard SQL aggregate)</p>"},{"location":"patterns/windowed_reprocess/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/windowed_reprocess/#simple-daily-aggregate","title":"Simple Daily Aggregate","text":"<pre><code>- id: gold_daily_sales\n  name: \"Daily Sales Summary (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            DATE(order_date) as order_date,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            MIN(order_date) as first_order_ts,\n            MAX(order_date) as last_order_ts\n          FROM silver.orders\n          WHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\n          GROUP BY DATE(order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.daily_sales\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#monthly-aggregate-wider-window","title":"Monthly Aggregate (Wider Window)","text":"<pre><code>- id: gold_monthly_sales\n  name: \"Monthly Sales Summary (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            DATE_TRUNC('month', order_date) as month,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            COUNT(DISTINCT customer_id) as unique_customers\n          FROM silver.orders\n          WHERE DATE_TRUNC('month', order_date) &gt;= DATE_TRUNC('month', DATE_SUB(CURRENT_DATE(), 90))\n          GROUP BY DATE_TRUNC('month', order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.monthly_sales\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#multi-grain-aggregates","title":"Multi-Grain Aggregates","text":"<pre><code>- id: gold_sales_by_region_day\n  name: \"Sales by Region &amp; Day (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            region,\n            DATE(order_date) as order_date,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            AVG(amount) as avg_order_value\n          FROM silver.orders\n          WHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 30)\n          GROUP BY region, DATE(order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.sales_by_region_day\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#window-size-strategy","title":"Window Size Strategy","text":""},{"location":"patterns/windowed_reprocess/#how-far-back-should-the-window-be","title":"How Far Back Should the Window Be?","text":"<p>Rule of Thumb: 2-3x your SLA for late data.</p> SLA Window Example Same-day delivery (next day processed) 3-7 days Daily aggregate 1-week SLA 14-21 days Weekly aggregate End-of-month close (3-5 days) 30-45 days Monthly aggregate <p>Conservative approach: Recalculate 30 days back, even if only aggregating daily. It costs minimal compute.</p>"},{"location":"patterns/windowed_reprocess/#dynamic-partition-overwrite","title":"Dynamic Partition Overwrite","text":""},{"location":"patterns/windowed_reprocess/#why-it-matters","title":"Why It Matters","text":"<p>Scenario: Your table is partitioned by <code>order_date</code>:</p> <pre><code>gold/sales/\n\u251c\u2500\u2500 order_date=2025-11-01/\n\u251c\u2500\u2500 order_date=2025-10-31/\n\u251c\u2500\u2500 order_date=2025-10-30/\n\u2514\u2500\u2500 ... (30 days of data)\n</code></pre> <p>If you use full overwrite (default): - Entire table is replaced - All 30 days are rewritten (slow) - Other columns lose their data</p> <p>If you use dynamic partition overwrite: - Only <code>order_date=2025-11-01</code> (and other affected dates) are replaced - Unaffected dates remain untouched - Much faster</p>"},{"location":"patterns/windowed_reprocess/#enabling-in-odibi","title":"Enabling in Odibi","text":"<pre><code>write:\n  connection: adls_prod\n  format: delta\n  table: gold.daily_sales\n  mode: overwrite\n  options:\n    partitionOverwriteMode: dynamic\n</code></pre> <p>This is automatically enabled by Odibi's safe defaults (per Architecture Manifesto).</p>"},{"location":"patterns/windowed_reprocess/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/windowed_reprocess/#problem-aggregate-is-still-wrong","title":"Problem: Aggregate is Still Wrong","text":"<p>Causes: 1. Window is too short \u2192 Late data arriving outside window. Increase window size. 2. Wrong grouping \u2192 Missing a dimension (e.g., region). Check Silver data. 3. Stale Silver data \u2192 No new orders merged in. Check merge pipeline.</p> <p>Debug:</p> <pre><code>-- Check what's in Silver for the window\nSELECT DATE(order_date), COUNT(*)\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\nORDER BY order_date DESC;\n\n-- Compare to Gold\nSELECT order_date, COUNT(*) as count\nFROM gold.daily_sales\nWHERE order_date &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY order_date\nORDER BY order_date DESC;\n</code></pre>"},{"location":"patterns/windowed_reprocess/#problem-slow-rewrites","title":"Problem: Slow Rewrites","text":"<p>Causes: 1. Window too large \u2192 Recalculating 365 days every run. Reduce window or run less frequently. 2. No partitioning \u2192 Entire table is scanned. Add <code>partition by order_date</code> to Silver.</p> <p>Solution:</p> <pre><code># Smaller window for frequent runs\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 3)\n\n# Larger window for nightly runs\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 30)\n</code></pre>"},{"location":"patterns/windowed_reprocess/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/windowed_reprocess/#advantages","title":"Advantages","text":"<p>\u2713 Always correct (fresh calculation, not patched) \u2713 Idempotent (run multiple times = same result) \u2713 Self-healing (late data automatically fixes aggregates) \u2713 Simple logic (standard SQL, no complex update logic) \u2713 Fast (recalculate 7 days vs. maintain entire history)  </p>"},{"location":"patterns/windowed_reprocess/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Requires recomputation (slower than patches, but worth it) \u2717 Assumes partitioning (without partitioning, rewrites entire table) \u2717 Assumes stateless logic (can't use row-level updates)  </p>"},{"location":"patterns/windowed_reprocess/#when-to-use","title":"When to Use","text":"<ul> <li>Always for Gold aggregates (KPIs, fact tables, summaries)</li> <li>Late-arriving data possible</li> <li>Queries can be re-executed without side effects</li> <li>Need guaranteed correctness over minimal compute</li> </ul>"},{"location":"patterns/windowed_reprocess/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Audit tables (use append)</li> <li>Streaming aggregates with sub-second latency (use Structured Streaming)</li> <li>Data with complex, stateful dependencies</li> </ul>"},{"location":"patterns/windowed_reprocess/#related-patterns","title":"Related Patterns","text":"<ul> <li>Merge/Upsert \u2192 Maintains clean Silver data that aggregates read from</li> <li>Append-Only Raw \u2192 Source of truth if aggregates need replay</li> </ul>"},{"location":"patterns/windowed_reprocess/#references","title":"References","text":"<ul> <li>Databricks: Dynamic Partition Overwrite</li> <li>Fundamentals of Data Engineering: Chapter on Aggregation</li> </ul>"},{"location":"plans/gold_layer_implementation_plan/","title":"ODIBI Gold Layer Enhancement Plan","text":""},{"location":"plans/gold_layer_implementation_plan/#executive-summary","title":"Executive Summary","text":"<p>Enhance ODIBI's Gold layer to support declarative dimensional modeling. Currently, users must write manual SQL for surrogate key lookups, aggregations, and FK validation. This plan adds patterns that make dimensional modeling as easy as Bronze/Silver.</p>"},{"location":"plans/gold_layer_implementation_plan/#current-state-vs-target-state","title":"Current State vs Target State","text":"Capability Current Target Fact table with SK lookups Manual SQL <code>FactPattern</code> auto-lookups Dimension with surrogate keys Manual + SCD2 <code>DimensionPattern</code> Date dimension Manual <code>DateDimensionPattern</code> Aggregations Manual SQL <code>AggregationPattern</code> FK validation None Built into FactPattern Semantic layer Storage only Execute + materialize"},{"location":"plans/gold_layer_implementation_plan/#implementation-phases","title":"Implementation Phases","text":""},{"location":"plans/gold_layer_implementation_plan/#phase-1-enhanced-dimensionpattern","title":"Phase 1: Enhanced DimensionPattern","text":"<p>Goal: Single pattern to build a complete dimension table with surrogate keys and SCD2.</p> <p>Location: <code>odibi/patterns/dimension.py</code></p> <p>Features: 1. Auto-generate surrogate key (sequential integer) 2. Support SCD Type 0, 1, 2 (configurable) 3. Add \"Unknown\" member automatically (SK = 0 or -1) 4. Add audit columns (load_timestamp, source_system) 5. Column aliasing for BI-friendly names</p> <p>Config Example:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id           # Business key (from Silver)\n    surrogate_key: customer_sk         # Generated (auto-increment)\n    scd_type: 2                        # 0=static, 1=overwrite, 2=history\n    track_cols: [name, email, city] # Columns to track for changes\n    unknown_member: true               # Add row for orphan FKs\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre> <p>Implementation Tasks: - [ ] Create <code>DimensionPattern</code> class extending <code>Pattern</code> - [ ] Implement surrogate key generation (max + row_number) - [ ] Integrate with existing SCD2 logic (reuse <code>scd.py</code>) - [ ] Add unknown member row insertion - [ ] Add audit column injection - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-2-datedimensionpattern","title":"Phase 2: DateDimensionPattern","text":"<p>Goal: Generate a complete date dimension table with pre-calculated attributes.</p> <p>Location: <code>odibi/patterns/date_dimension.py</code></p> <p>Features: 1. Generate date range (start_date to end_date) 2. Pre-calculate: day_of_week, is_weekend, month, quarter, year 3. Support fiscal calendar offset 4. Holiday integration (optional)</p> <p>Config Example:</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    date_key_format: \"yyyyMMdd\"        # 20240115\n    fiscal_year_start_month: 7          # July = FY start\n    include_time: false                 # Date only, no time grain\n</code></pre> <p>Generated Columns: - date_sk (INT, primary key) - full_date (DATE) - day_of_week (STRING: Monday, Tuesday...) - day_of_week_num (INT: 1-7) - is_weekend (BOOLEAN) - week_of_year (INT) - month (INT) - month_name (STRING) - quarter (INT: 1-4) - quarter_name (STRING: Q1, Q2...) - year (INT) - fiscal_year (INT) - fiscal_quarter (INT) - is_month_start (BOOLEAN) - is_month_end (BOOLEAN)</p> <p>Implementation Tasks: - [ ] Create <code>DateDimensionPattern</code> class - [ ] Date range generator (Pandas date_range / Spark sequence) - [ ] Attribute calculators - [ ] Fiscal calendar logic - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-3-enhanced-factpattern","title":"Phase 3: Enhanced FactPattern","text":"<p>Goal: Declarative fact table building with automatic SK lookups and validation.</p> <p>Location: <code>odibi/patterns/fact.py</code> (enhance existing)</p> <p>Features: 1. Declare dimension relationships 2. Auto-lookup surrogate keys from dimensions 3. Handle orphans (use unknown member or reject) 4. Validate grain (no duplicates at PK level) 5. Add audit columns 6. Enforce append-only mode</p> <p>Config Example:</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]                  # What makes a row unique\n\n    dimensions:\n      - source_column: customer_id     # Column in Silver data\n        dimension_table: dim_customer  # Gold dimension to lookup\n        dimension_key: customer_id     # Natural key in dimension\n        surrogate_key: customer_sk     # SK to retrieve\n        scd2: true                     # Filter is_current = true\n\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n\n      - source_column: order_date\n        dimension_table: dim_date\n        dimension_key: full_date\n        surrogate_key: date_sk\n\n    orphan_handling: unknown           # unknown | reject | quarantine\n\n    measures:\n      - quantity                       # Pass through\n      - unit_price: price              # Rename\n      - total_amount: \"quantity * price\"  # Calculated\n\n    audit:\n      load_timestamp: true\n      source_system: \"pos\"\n</code></pre> <p>Implementation Tasks: - [ ] Extend <code>FactPattern</code> class - [ ] Dimension lookup logic (generate JOIN SQL) - [ ] Orphan handling (COALESCE to unknown SK) - [ ] Grain validation (check for duplicates) - [ ] Measure calculation/renaming - [ ] Audit column injection - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-4-aggregationpattern","title":"Phase 4: AggregationPattern","text":"<p>Goal: Declarative aggregation with time-grain rollups.</p> <p>Location: <code>odibi/patterns/aggregation.py</code></p> <p>Features: 1. Declare grain (GROUP BY columns) 2. Declare measures with aggregation functions 3. Incremental aggregation (merge new data) 4. Time rollups (daily \u2192 weekly \u2192 monthly)</p> <p>Config Example:</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    source: fact_orders\n    grain: [date_sk, product_sk, region_sk]\n\n    measures:\n      - name: total_revenue\n        expr: \"SUM(total_amount)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n      - name: avg_order_value\n        expr: \"AVG(total_amount)\"\n      - name: unique_customers\n        expr: \"COUNT(DISTINCT customer_sk)\"\n\n    incremental:\n      timestamp_column: date_sk\n      merge_strategy: replace           # replace | sum\n\n    rollups:                            # Optional: generate multiple grains\n      - grain: [month, product_sk]\n        output: agg_monthly_product\n</code></pre> <p>Implementation Tasks: - [ ] Create <code>AggregationPattern</code> class - [ ] SQL generation for GROUP BY + aggregates - [ ] Incremental merge logic - [ ] Rollup generation - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-5-semantic-layer-execution","title":"Phase 5: Semantic Layer Execution","text":"<p>Goal: Execute metric definitions stored in meta_metrics.</p> <p>Location: <code>odibi/semantics/</code> (new module)</p> <p>Features: 1. Define metrics in YAML (not just SQL strings) 2. Query interface: \"revenue BY region, month\" 3. Materialize metrics on schedule 4. Serve via API (optional, future)</p> <p>Config Example (in odibi.yaml):</p> <pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"revenue / COUNT(*)\"\n    type: derived\n\ndimensions:\n  - name: order_date\n    source: dim_date\n    hierarchy: [year, quarter, month, full_date]\n\n  - name: product\n    source: dim_product\n    hierarchy: [category, subcategory, product_name]\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    schedule: \"0 2 1 * *\"              # 2am on 1st of month\n    output: gold/agg_monthly_revenue\n</code></pre> <p>Implementation Tasks: - [ ] Create <code>odibi/semantics/</code> module - [ ] Metric definition parser - [ ] Query generator (metrics BY dimensions) - [ ] Materialization executor - [ ] CLI: <code>odibi query \"revenue BY region\"</code> - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-6-relationship-registry-fk-validation","title":"Phase 6: Relationship Registry &amp; FK Validation","text":"<p>Goal: Declare and validate star schema relationships.</p> <p>Location: <code>odibi/catalog.py</code> (extend) + <code>odibi/validation/fk.py</code> (new)</p> <p>Features: 1. Declare relationships in YAML 2. Validate referential integrity on fact load 3. Detect orphan records 4. Generate lineage from relationships</p> <p>Config Example:</p> <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n</code></pre> <p>Implementation Tasks: - [ ] Relationship config schema - [ ] FK validation logic - [ ] Orphan detection reporting - [ ] Integration with FactPattern - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#implementation-order","title":"Implementation Order","text":"<pre><code>Phase 1: DimensionPattern     \u2190\u2500\u2500 Start here (foundation)\n    \u2193\nPhase 2: DateDimensionPattern \u2190\u2500\u2500 Quick win, very useful\n    \u2193\nPhase 3: Enhanced FactPattern \u2190\u2500\u2500 Core value (auto SK lookups)\n    \u2193\nPhase 4: AggregationPattern   \u2190\u2500\u2500 Performance optimization\n    \u2193\nPhase 5: Semantic Layer       \u2190\u2500\u2500 Advanced (can defer)\n    \u2193\nPhase 6: FK Validation        \u2190\u2500\u2500 Polish (can defer)\n</code></pre>"},{"location":"plans/gold_layer_implementation_plan/#file-structure","title":"File Structure","text":"<pre><code>odibi/\n\u251c\u2500\u2500 patterns/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py              # Existing\n\u2502   \u251c\u2500\u2500 scd2.py              # Existing\n\u2502   \u251c\u2500\u2500 fact.py              # Enhance\n\u2502   \u251c\u2500\u2500 merge.py             # Existing\n\u2502   \u251c\u2500\u2500 snapshot.py          # Existing\n\u2502   \u251c\u2500\u2500 dimension.py         # NEW (Phase 1)\n\u2502   \u251c\u2500\u2500 date_dimension.py    # NEW (Phase 2)\n\u2502   \u2514\u2500\u2500 aggregation.py       # NEW (Phase 4)\n\u251c\u2500\u2500 semantics/               # NEW (Phase 5)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 metrics.py\n\u2502   \u251c\u2500\u2500 query.py\n\u2502   \u2514\u2500\u2500 materialize.py\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 engine.py            # Existing\n\u2502   \u251c\u2500\u2500 gate.py              # Existing\n\u2502   \u2514\u2500\u2500 fk.py                # NEW (Phase 6)\n\u2514\u2500\u2500 config.py                # Add new config schemas\n</code></pre>"},{"location":"plans/gold_layer_implementation_plan/#testing-strategy","title":"Testing Strategy","text":"<p>Each pattern needs: 1. Unit tests with mock data (Pandas engine) 2. Integration tests (if Spark available) 3. Example YAML configs in <code>examples/</code> 4. Documentation in <code>docs/</code></p>"},{"location":"plans/gold_layer_implementation_plan/#success-criteria","title":"Success Criteria","text":"<p>After implementation, this YAML should work:</p> <pre><code>pipelines:\n  - pipeline: gold_dimensional\n    nodes:\n      # Date dimension (generated)\n      - name: dim_date\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2020-01-01\"\n            end_date: \"2030-12-31\"\n        write:\n          connection: gold\n          path: dim_date\n\n      # Customer dimension (from Silver)\n      - name: dim_customer\n        depends_on: [clean_customers]\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols: [name, city]\n            unknown_member: true\n        write:\n          connection: gold\n          path: dim_customer\n\n      # Fact table (auto SK lookups)\n      - name: fact_orders\n        depends_on: [clean_orders, dim_customer, dim_product, dim_date]\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - total_amount: \"quantity * price\"\n        write:\n          connection: gold\n          path: fact_orders\n          mode: append\n\n      # Daily aggregation\n      - name: agg_daily_sales\n        depends_on: [fact_orders]\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: revenue\n                expr: \"SUM(total_amount)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n        write:\n          connection: gold\n          path: agg_daily_sales\n</code></pre>"},{"location":"plans/gold_layer_implementation_plan/#timeline-estimate","title":"Timeline Estimate","text":"Phase Effort Priority Phase 1: DimensionPattern 2-3 days High Phase 2: DateDimensionPattern 1 day High Phase 3: Enhanced FactPattern 3-4 days High Phase 4: AggregationPattern 2 days Medium Phase 5: Semantic Layer 3-5 days Low (defer) Phase 6: FK Validation 1-2 days Low (defer) <p>Total for Phases 1-4: ~8-10 days</p>"},{"location":"plans/gold_layer_implementation_prompt/","title":"Prompt for Next Thread: Implement ODIBI Gold Layer Patterns","text":"<p>Copy and paste this into a new Amp thread:</p>"},{"location":"plans/gold_layer_implementation_prompt/#implement-phase-1-dimensionpattern-for-odibi","title":"Implement Phase 1: DimensionPattern for ODIBI","text":""},{"location":"plans/gold_layer_implementation_prompt/#context","title":"Context","text":"<p>I'm enhancing ODIBI's Gold layer to support declarative dimensional modeling. See the full plan at <code>docs/plans/gold_layer_implementation_plan.md</code>.</p> <p>ODIBI already has: - <code>odibi/patterns/scd2.py</code> - SCD Type 2 logic - <code>odibi/patterns/fact.py</code> - Basic fact pattern (just dedup) - <code>odibi/patterns/base.py</code> - Base Pattern class - <code>odibi/transformers/advanced.py</code> - <code>generate_surrogate_key</code> transformer</p>"},{"location":"plans/gold_layer_implementation_prompt/#task","title":"Task","text":"<p>Create <code>odibi/patterns/dimension.py</code> - a new <code>DimensionPattern</code> that builds a complete dimension table.</p>"},{"location":"plans/gold_layer_implementation_prompt/#requirements","title":"Requirements","text":"<ol> <li>Surrogate Key Generation</li> <li>Auto-generate integer surrogate key (e.g., <code>customer_sk</code>)</li> <li>Use <code>MAX(existing_sk) + ROW_NUMBER()</code> for new rows</li> <li> <p>Handle first run (no existing table)</p> </li> <li> <p>SCD Support</p> </li> <li><code>scd_type: 0</code> - Static (never update)</li> <li><code>scd_type: 1</code> - Overwrite (no history)</li> <li> <p><code>scd_type: 2</code> - Track history (reuse existing <code>scd2.py</code> logic)</p> </li> <li> <p>Unknown Member</p> </li> <li>If <code>unknown_member: true</code>, insert a row with SK=0 and \"Unknown\" values</li> <li> <p>This row is used for orphan FK handling in fact tables</p> </li> <li> <p>Audit Columns</p> </li> <li>Add <code>load_timestamp</code> (current timestamp)</li> <li> <p>Add <code>source_system</code> (from config)</p> </li> <li> <p>Config Schema <code>yaml    pattern:      type: dimension      params:        natural_key: customer_id         # Required        surrogate_key: customer_sk       # Required        scd_type: 2                      # 0, 1, or 2        track_cols: [name, city]      # For SCD1/2        target: gold/dim_customer        # For SCD2 (existing table)        unknown_member: true             # Optional        audit:          load_timestamp: true          source_system: \"crm\"</code></p> </li> </ol>"},{"location":"plans/gold_layer_implementation_prompt/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Create <code>odibi/patterns/dimension.py</code></li> <li>Create <code>DimensionPattern</code> class extending <code>Pattern</code></li> <li>Implement <code>validate()</code> method</li> <li>Implement <code>execute()</code> method with:</li> <li>SK generation logic</li> <li>SCD routing (0/1/2)</li> <li>Unknown member insertion</li> <li>Audit column injection</li> <li>Register pattern in <code>odibi/patterns/__init__.py</code></li> <li>Add config schema in <code>odibi/config.py</code> if needed</li> <li>Create unit tests in <code>tests/patterns/test_dimension.py</code></li> <li>Test with both Pandas and Spark engines</li> </ol>"},{"location":"plans/gold_layer_implementation_prompt/#reference-files","title":"Reference Files","text":"<p>Look at these for patterns to follow: - <code>odibi/patterns/scd2.py</code> - SCD2 logic to reuse - <code>odibi/patterns/fact.py</code> - Simple pattern structure - <code>odibi/patterns/base.py</code> - Base class - <code>odibi/transformers/advanced.py</code> - <code>generate_surrogate_key</code> for reference</p>"},{"location":"plans/gold_layer_implementation_prompt/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>This config should work:</p> <pre><code>- name: dim_customer\n  depends_on: [clean_customers]\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_cols: [name, email, city]\n      target: gold/dim_customer\n      unknown_member: true\n      audit:\n        load_timestamp: true\n        source_system: \"crm\"\n  write:\n    connection: gold\n    path: dim_customer\n</code></pre> <p>And produce:</p> customer_sk customer_id name email city valid_from valid_to is_current load_timestamp source_system 0 -1 Unknown unknown@unknown.com Unknown 1900-01-01 NULL true 2024-01-15 crm 1 abc123... John john@mail.com NYC 2024-01-15 NULL true 2024-01-15 crm <p>After Phase 1 is complete, we'll move to Phase 2 (DateDimensionPattern) and Phase 3 (Enhanced FactPattern).</p>"},{"location":"playbook/","title":"The Odibi Playbook","text":"<p>A problem-first guide to data engineering with Odibi. Find your problem, get the solution.</p>"},{"location":"playbook/#new-to-odibi","title":"New to Odibi?","text":"<p>If you're just getting started:</p> <ol> <li>Getting Started Tutorial \u2014 Build your first pipeline in 10 minutes</li> <li>Bronze Layer Tutorial \u2014 Common ingestion problems solved</li> <li>Silver Layer Tutorial \u2014 Cleaning and deduplication</li> <li>Gold Layer Tutorial \u2014 Building star schemas</li> </ol> <p>Then come back here when you need to solve a specific problem.</p>"},{"location":"playbook/#how-to-use-this-playbook","title":"How to Use This Playbook","text":"<ol> <li>Find your layer (Bronze \u2192 Silver \u2192 Gold)</li> <li>Find your problem</li> <li>Follow the link to implementation details</li> </ol>"},{"location":"playbook/#bronze-layer-ingestion","title":"Bronze Layer: Ingestion","text":"<p>\"Get data from sources into your lakehouse reliably.\"</p>"},{"location":"playbook/#file-based-ingestion","title":"File-Based Ingestion","text":"Problem Solution Odibi Feature Docs Load all files from a folder Append-only raw ingestion <code>read</code> \u2192 <code>write</code> Append-Only Raw Only process new files since last run Time-based lookback <code>incremental.mode: rolling_window</code> Incremental Loading Track exact high-water mark Stateful HWM tracking <code>incremental.mode: stateful</code> Incremental Loading Files have inconsistent schemas Schema evolution handling <code>write.options.mergeSchema: true</code> Schema Tracking Malformed records break the pipeline Bad record archiving <code>read.options.badRecordsPath</code> YAML Reference Need to reprocess a time window Windowed reprocessing Rolling window config Windowed Reprocess"},{"location":"playbook/#database-ingestion","title":"Database Ingestion","text":"Problem Solution Odibi Feature Docs Full table extract from SQL SQL read <code>read.format: jdbc</code> Getting Started Only extract new/changed rows Stateful HWM incremental <code>incremental.mode: stateful</code> Incremental Loading CDC / change data capture Merge incoming changes <code>transformer: merge</code> Merge/Upsert Source sends duplicates Deduplicate on ingest <code>transformer: deduplicate</code> YAML Reference"},{"location":"playbook/#ingestion-quality-contracts","title":"Ingestion Quality (Contracts)","text":"Problem Solution Odibi Feature Docs Fail fast if source is empty Row count contract <code>contracts: [{type: row_count, min: 1}]</code> YAML Reference Detect unexpected volume drops Volume drop check <code>contracts: [{type: volume_drop, threshold: 0.5}]</code> YAML Reference Ensure required columns exist Schema contract <code>contracts: [{type: schema, strict: true}]</code> YAML Reference Ensure no NULL values Not-null check <code>contracts: [{type: not_null, columns: [...]}]</code> YAML Reference Ensure unique keys Uniqueness check <code>contracts: [{type: unique, columns: [...]}]</code> YAML Reference Validate enum values Accepted values <code>contracts: [{type: accepted_values, column: status, values: [A,B,C]}]</code> YAML Reference Validate numeric range Range check <code>contracts: [{type: range, column: age, min: 0, max: 150}]</code> YAML Reference Validate format (email, ID) Regex match <code>contracts: [{type: regex_match, column: email, pattern: \"...\"}]</code> YAML Reference Custom SQL validation Custom SQL <code>contracts: [{type: custom_sql, condition: \"amount &gt; 0\"}]</code> YAML Reference Detect data drift Distribution check <code>contracts: [{type: distribution, column: price, metric: mean}]</code> YAML Reference Ensure data freshness Freshness check <code>contracts: [{type: freshness, column: updated_at, max_age: \"24h\"}]</code> YAML Reference"},{"location":"playbook/#silver-layer-transformation-conforming","title":"Silver Layer: Transformation &amp; Conforming","text":"<p>\"Clean, deduplicate, and model your data.\"</p>"},{"location":"playbook/#deduplication","title":"Deduplication","text":"Problem Solution Odibi Feature Docs Remove duplicates within a batch Deduplicate transformer <code>transformer: deduplicate</code> YAML Reference Keep latest record per key Dedupe with ordering <code>params: {keys: [...], order_by: \"updated_at DESC\"}</code> YAML Reference"},{"location":"playbook/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"Problem Solution Odibi Feature Docs Overwrite dimension changes (no history) SCD Type 1 <code>transformer: dimension</code> + <code>params.scd_type: 1</code> Dimension Pattern Track dimension changes over time SCD Type 2 <code>transformer: dimension</code> + <code>params.scd_type: 2</code> SCD2 Pattern Generate surrogate keys Dimension pattern <code>params.surrogate_key: customer_sk</code> Dimension Pattern Handle unknown dimension members Unknown member row <code>params.unknown_member: true</code> Dimension Pattern"},{"location":"playbook/#data-quality","title":"Data Quality","text":"Problem Solution Odibi Feature Docs Validate data after transformation Validation tests <code>validation.tests: [...]</code> Quality Gates Route bad rows for review Quarantine <code>validation.quarantine: {...}</code> Quarantine Require 95% pass rate Quality gate threshold <code>validation.gate.require_pass_rate: 0.95</code> Quality Gates Check referential integrity FK validation FK Validator API FK Validation"},{"location":"playbook/#merge-upsert","title":"Merge &amp; Upsert","text":"Problem Solution Odibi Feature Docs Upsert into target table Merge transformer <code>transformer: merge</code> Merge/Upsert Detect deleted records Delete detection <code>delete_detection.mode: sql_compare</code> YAML Reference Soft delete instead of hard delete Soft delete flag <code>delete_detection.soft_delete_col: is_deleted</code> YAML Reference"},{"location":"playbook/#custom-transformations","title":"Custom Transformations","text":"Problem Solution Odibi Feature Docs Apply SQL transformations SQL steps <code>transform.steps: [{sql: \"...\"}]</code> Writing Transformations Apply Python functions Function steps <code>transform.steps: [{function: \"...\"}]</code> Writing Transformations Chain multiple operations Transform pipeline Multiple steps in sequence Writing Transformations"},{"location":"playbook/#built-in-transformers-40","title":"Built-in Transformers (40+)","text":"<p>Odibi has 40+ built-in transformers. Use them in <code>transform.steps</code>:</p> <pre><code>transform:\n  steps:\n    - transformer: filter_rows\n      params: {condition: \"amount &gt; 0\"}\n    - transformer: derive_columns\n      params: {columns: {total: \"qty * price\"}}\n</code></pre> Category Transformers Filtering <code>filter_rows</code>, <code>distinct</code>, <code>sample</code>, <code>limit</code> Columns <code>derive_columns</code>, <code>select_columns</code>, <code>drop_columns</code>, <code>rename_columns</code>, <code>cast_columns</code> Text <code>clean_text</code>, <code>trim_whitespace</code>, <code>regex_replace</code>, <code>split_part</code>, <code>concat_columns</code> Dates <code>extract_date_parts</code>, <code>date_add</code>, <code>date_trunc</code>, <code>date_diff</code>, <code>convert_timezone</code> Nulls <code>fill_nulls</code>, <code>coalesce_columns</code> Relational <code>join</code>, <code>union</code>, <code>aggregate</code>, <code>pivot</code>, <code>unpivot</code> Window <code>window_calculation</code> (rank, sum, lag, lead, etc.) JSON/Nested <code>parse_json</code>, <code>normalize_json</code>, <code>explode_list_column</code>, <code>unpack_struct</code> Keys/Hashing <code>generate_surrogate_key</code>, <code>hash_columns</code> Advanced <code>sessionize</code>, <code>validate_and_flag</code>, <code>dict_based_mapping</code>, <code>split_events_by_period</code> Validation <code>cross_check</code> (compare two DataFrames) Schema <code>normalize_schema</code>, <code>normalize_column_names</code>, <code>add_prefix</code>, <code>add_suffix</code> Sorting <code>sort</code> Values <code>replace_values</code>, <code>case_when</code> <p>Full reference: YAML Schema - Transformers</p>"},{"location":"playbook/#gold-layer-consumption-analytics","title":"Gold Layer: Consumption &amp; Analytics","text":"<p>\"Build fact tables, aggregations, and semantic layers for BI.\"</p>"},{"location":"playbook/#fact-tables","title":"Fact Tables","text":"Problem Solution Odibi Feature Docs Build fact table with SK lookups Fact pattern <code>transformer: fact</code> Fact Pattern Handle orphan records (missing dims) Orphan handling <code>params.orphan_handling: unknown</code> Fact Pattern Quarantine orphans for review Orphan quarantine <code>params.orphan_handling: quarantine</code> Fact Pattern Validate grain (no duplicates) Grain validation <code>params.grain: [order_id]</code> Fact Pattern"},{"location":"playbook/#aggregations","title":"Aggregations","text":"Problem Solution Odibi Feature Docs Pre-aggregate metrics Aggregation pattern <code>transformer: aggregation</code> Aggregation Pattern Incremental aggregation Merge aggregates <code>params.incremental: true</code> Aggregation Pattern"},{"location":"playbook/#reference-data","title":"Reference Data","text":"Problem Solution Odibi Feature Docs Generate date dimension Date dimension pattern <code>transformer: date_dimension</code> Date Dimension Skip unchanged reference data Skip-if-unchanged <code>write.skip_if_unchanged: true</code> Skip If Unchanged"},{"location":"playbook/#semantic-layer","title":"Semantic Layer","text":"Problem Solution Odibi Feature Docs Define reusable metrics Semantic config <code>semantic.metrics: [...]</code> Semantics Define dimensions for BI Semantic config <code>semantic.dimensions: [...]</code> Semantics Query metrics dynamically Semantic Query <code>SemanticQuery.execute(\"revenue BY region\")</code> Query API Pre-compute aggregations Materialization <code>Materializer.execute(\"monthly_revenue\")</code> Materialization Incremental materialization Incremental materializer <code>IncrementalMaterializer</code> Materialization Schedule materializations Materialization schedules <code>materializations[].schedule: \"@daily\"</code> Materialization List defined materializations Materializer API <code>materializer.list_materializations()</code> Materialization"},{"location":"playbook/#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":""},{"location":"playbook/#performance","title":"Performance","text":"Problem Solution Odibi Feature Docs Pipeline is slow Performance tuning <code>performance</code> config Performance Tuning Cache hot DataFrames Caching <code>cache: true</code> YAML Reference Skip expensive profiling Skip null profiling <code>skip_null_profiling: true</code> Performance Tuning"},{"location":"playbook/#operations","title":"Operations","text":"Problem Solution Odibi Feature Docs Run only specific nodes Tag-based execution <code>tags: [daily]</code> + <code>odibi run --tag daily</code> CLI Guide Retry on transient failures Retry config <code>retry: {enabled: true}</code> YAML Reference Alert on failure Alerting <code>alerts: [{type: slack, ...}]</code> Alerting Track data lineage OpenLineage <code>lineage: {url: \"...\"}</code> Lineage Export traces to observability platform OpenTelemetry <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> env var Observability"},{"location":"playbook/#multi-pipeline","title":"Multi-Pipeline","text":"Problem Solution Odibi Feature Docs Reference output from another pipeline Cross-pipeline deps <code>$pipeline.node</code> syntax Cross-Pipeline Dependencies Environment-specific config Environments <code>environments: {prod: {...}}</code> Environments"},{"location":"playbook/#documentation-observability","title":"Documentation &amp; Observability","text":"Problem Solution Odibi Feature Docs Track pipeline execution history Stories <code>story: {connection: ..., path: stories/}</code> Stories Generate HTML/JSON/Markdown reports Story renderers Automatic after each run Stories See row counts, duration, schema changes Story metadata Auto-captured metrics Stories Track schema evolution over time System Catalog <code>system: {connection: ..., path: _system}</code> Catalog Query run history and stats Catalog API <code>odibi catalog runs</code>, <code>odibi catalog stats</code> Catalog View HWM state Catalog state <code>odibi catalog state config.yaml</code> Catalog Audit config changes Version hashing Auto-detects config drift Catalog Compare Delta table versions Delta diagnostics <code>get_delta_diff(table, v1, v2)</code> Diagnostics Detect data/schema drift Drift detection <code>detect_drift(table, threshold)</code> Diagnostics Compare pipeline runs Run comparison <code>diff_runs(run_a, run_b)</code> Diagnostics"},{"location":"playbook/#orchestration","title":"Orchestration","text":"Problem Solution Odibi Feature Docs Export to Airflow Airflow DAG generation <code>AirflowExporter</code> Orchestration Export to Dagster Dagster asset generation <code>DagsterExporter</code> Orchestration CLI for orchestration export Export command <code>odibi export airflow config.yaml</code> Orchestration"},{"location":"playbook/#cli-commands","title":"CLI Commands","text":"Problem Solution Command Docs Scaffold &amp; Setup Create new project Init wizard <code>odibi init-pipeline my_project</code> CLI Guide Use a template Template init <code>odibi init-pipeline my_project --template local-medallion</code> CLI Guide Generate API docs Docs generator <code>odibi docs</code> CLI Guide Execution Run a pipeline Execute nodes <code>odibi run config.yaml</code> CLI Guide Run specific pipeline Filter by name <code>odibi run config.yaml --pipeline orders</code> CLI Guide Run specific node Single node <code>odibi run config.yaml --node transform_orders</code> CLI Guide Dry run (no writes) Simulate <code>odibi run config.yaml --dry-run</code> CLI Guide Resume from failure Skip successful nodes <code>odibi run config.yaml --resume</code> CLI Guide Run nodes in parallel Parallel execution <code>odibi run config.yaml --parallel --workers 8</code> CLI Guide Validation &amp; Testing Validate config syntax Config check <code>odibi validate config.yaml</code> CLI Guide Run transform unit tests Test runner <code>odibi test tests/</code> Testing Update test snapshots Snapshot mode <code>odibi test --snapshot</code> Testing Diagnose issues Doctor check <code>odibi doctor config.yaml</code> CLI Guide Visualization &amp; Inspection Visualize dependencies DAG graph <code>odibi graph config.yaml</code> CLI Guide Output as Mermaid Mermaid format <code>odibi graph config.yaml --format mermaid</code> CLI Guide View schema history Schema diff <code>odibi schema history table --config config.yaml</code> Schema Tracking Compare schema versions Schema diff <code>odibi schema diff table --from-version 3 --to-version 5</code> Schema Tracking Stories &amp; Reporting Generate story report Story gen <code>odibi story generate config.yaml</code> Stories Compare two runs Story diff <code>odibi story diff run1.json run2.json</code> Stories List story files Story list <code>odibi story list</code> Stories Lineage Emit OpenLineage events Lineage emit <code>odibi lineage emit config.yaml</code> Lineage View lineage graph Lineage view <code>odibi lineage view config.yaml</code> Lineage Deployment &amp; Export Deploy to System Catalog Catalog deploy <code>odibi deploy config.yaml</code> Catalog Export to Airflow Airflow DAG <code>odibi export airflow config.yaml</code> Orchestration Export to Dagster Dagster assets <code>odibi export dagster config.yaml</code> Orchestration Secrets &amp; Config Manage secrets Secret commands <code>odibi secrets set KEY VALUE</code> Secrets List secrets Secret list <code>odibi secrets list</code> Secrets"},{"location":"playbook/#connections","title":"Connections","text":"Problem Solution Connection Type Docs Read/write local files Local filesystem <code>type: local</code> Connections Read/write Azure Blob/ADLS Azure storage <code>type: azure_blob</code> Azure Setup Read/write Delta tables Delta Lake <code>type: delta</code> Connections Read from SQL Server JDBC connection <code>type: sql_server</code> Connections Call HTTP APIs REST endpoints <code>type: http</code> Connections"},{"location":"playbook/#advanced-features","title":"Advanced Features","text":"Problem Solution Odibi Feature Docs Query old data versions Delta Time Travel <code>read.time_travel.version</code> or <code>timestamp</code> YAML Reference Add load metadata columns Write metadata <code>write.add_metadata: true</code> YAML Reference Add _extracted_at, _source_file Selective metadata <code>write.add_metadata: {extracted_at: true}</code> YAML Reference Partition output for performance Partition by columns <code>write.partition_by: [year, month]</code> YAML Reference Run SQL before transform Pre-SQL hooks <code>pre_sql: [\"CREATE TEMP VIEW...\"]</code> YAML Reference Run SQL after write Post-SQL hooks <code>post_sql: [\"OPTIMIZE table\", \"VACUUM\"]</code> YAML Reference Control error handling Error strategy <code>on_error: fail_fast|fail_later|ignore</code> YAML Reference Depend on other nodes Node dependencies <code>depends_on: [node1, node2]</code> Pipelines Enable/disable nodes Conditional execution <code>enabled: false</code> Pipelines Route bad rows to quarantine Quarantine config <code>validation.quarantine: {connection: ..., path: ...}</code> Quarantine Compare two DataFrames Cross-check validation <code>transformer: cross_check</code> YAML Reference"},{"location":"playbook/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>I need to...\n\u2502\n\u251c\u2500\u25ba INGEST data\n\u2502   \u251c\u2500\u25ba From files \u2192 read + write (append mode)\n\u2502   \u251c\u2500\u25ba From database \u2192 read.format: jdbc\n\u2502   \u251c\u2500\u25ba Only new data \u2192 incremental.mode: stateful\n\u2502   \u2514\u2500\u25ba Skip if unchanged \u2192 write.skip_if_unchanged: true\n\u2502\n\u251c\u2500\u25ba CLEAN/TRANSFORM data\n\u2502   \u251c\u2500\u25ba Remove duplicates \u2192 transformer: deduplicate\n\u2502   \u251c\u2500\u25ba Track history \u2192 transformer: dimension (scd_type: 2)\n\u2502   \u251c\u2500\u25ba Upsert/merge \u2192 transformer: merge\n\u2502   \u251c\u2500\u25ba Validate quality \u2192 validation.tests: [...]\n\u2502   \u251c\u2500\u25ba Custom logic \u2192 transform.steps: [{sql: ...}]\n\u2502   \u2514\u2500\u25ba Built-in ops \u2192 filter_rows, derive_columns, join, etc.\n\u2502\n\u251c\u2500\u25ba BUILD ANALYTICS\n\u2502   \u251c\u2500\u25ba Fact table \u2192 transformer: fact\n\u2502   \u251c\u2500\u25ba Aggregation \u2192 transformer: aggregation\n\u2502   \u251c\u2500\u25ba Date table \u2192 transformer: date_dimension\n\u2502   \u2514\u2500\u25ba Metrics layer \u2192 semantic.metrics: [...]\n\u2502\n\u251c\u2500\u25ba MONITOR &amp; DEBUG\n\u2502   \u251c\u2500\u25ba Execution history \u2192 story config (auto HTML reports)\n\u2502   \u251c\u2500\u25ba Compare runs \u2192 odibi story diff run1.json run2.json\n\u2502   \u251c\u2500\u25ba Schema history \u2192 system catalog (auto tracking)\n\u2502   \u251c\u2500\u25ba Run stats \u2192 odibi catalog stats\n\u2502   \u251c\u2500\u25ba Diagnose \u2192 odibi doctor\n\u2502   \u2514\u2500\u25ba View lineage \u2192 odibi lineage view\n\u2502\n\u251c\u2500\u25ba OPERATE\n\u2502   \u251c\u2500\u25ba Run subset \u2192 odibi run --tag daily\n\u2502   \u251c\u2500\u25ba Resume \u2192 odibi run --resume\n\u2502   \u251c\u2500\u25ba Parallelize \u2192 odibi run --parallel\n\u2502   \u251c\u2500\u25ba Handle failures \u2192 retry + alerts\n\u2502   \u251c\u2500\u25ba Deploy to catalog \u2192 odibi deploy config.yaml\n\u2502   \u2514\u2500\u25ba Export to Airflow \u2192 odibi export airflow\n\u2502\n\u251c\u2500\u25ba EXTEND\n\u2502   \u251c\u2500\u25ba Custom connection \u2192 register_connection_factory()\n\u2502   \u251c\u2500\u25ba Custom transform \u2192 @transform decorator\n\u2502   \u2514\u2500\u25ba Package plugins \u2192 pyproject.toml entry points\n\u2502\n\u251c\u2500\u25ba TEST\n\u2502   \u251c\u2500\u25ba Unit test node \u2192 pipeline.run_node(mock_data=...)\n\u2502   \u251c\u2500\u25ba CLI test runner \u2192 odibi test tests/\n\u2502   \u251c\u2500\u25ba Assert equality \u2192 assert_frame_equal()\n\u2502   \u251c\u2500\u25ba Generate data \u2192 generate_sample_data()\n\u2502   \u2514\u2500\u25ba Deterministic tests \u2192 SourcePoolConfig\n\u2502\n\u251c\u2500\u25ba AUTOMATE\n\u2502   \u251c\u2500\u25ba Python API \u2192 PipelineManager.from_yaml()\n\u2502   \u251c\u2500\u25ba Web dashboard \u2192 odibi ui config.yaml\n\u2502   \u2514\u2500\u25ba Access state \u2192 StateManager API\n\u2502\n\u2514\u2500\u25ba GET STARTED\n    \u251c\u2500\u25ba New project \u2192 odibi init-pipeline my_project\n    \u251c\u2500\u25ba From template \u2192 odibi init-pipeline --template local-medallion\n    \u2514\u2500\u25ba Generate docs \u2192 odibi docs\n</code></pre>"},{"location":"playbook/#developer-experience","title":"Developer Experience","text":"<p>\"Extend, test, and automate Odibi.\"</p>"},{"location":"playbook/#python-api","title":"Python API","text":"Problem Solution Odibi Feature Docs Run pipelines programmatically Python execution <code>PipelineManager.from_yaml()</code> Python API Run a specific pipeline Select pipeline <code>manager.run(\"pipeline_name\")</code> Python API Dry run without writing Simulation <code>manager.run(dry_run=True)</code> Python API Resume from failure Skip completed <code>manager.run(resume_from_failure=True)</code> Python API Access story metadata in code Story API <code>result.story_path</code> Python API Compare pipeline runs Diff API <code>diff_runs(run_a, run_b)</code> Python API Compare Delta table versions Delta diff <code>get_delta_diff(path, v1, v2)</code> Python API"},{"location":"playbook/#plugins-extensibility","title":"Plugins &amp; Extensibility","text":"Problem Solution Odibi Feature Docs Need custom connection (Snowflake, Postgres) Connection plugin <code>register_connection_factory()</code> Plugins Need custom transformation Transform plugin <code>@transform</code> decorator Plugins Validate transform parameters Pydantic models <code>@transform(param_model=MyParams)</code> Plugins Auto-load custom code Auto-discovery <code>transforms.py</code>, <code>plugins.py</code> files Plugins Package plugins for distribution Entry points <code>pyproject.toml</code> entry points Plugins List registered functions Registry API <code>FunctionRegistry.list_functions()</code> Plugins"},{"location":"playbook/#testing","title":"Testing","text":"Problem Solution Odibi Feature Docs Unit test a single node Mock input data <code>pipeline.run_node(mock_data=...)</code> Python API Assert DataFrame equality Test assertions <code>assert_frame_equal(left, right)</code> Testing Assert schema match Schema assertions <code>assert_schema_equal(left, right)</code> Testing Generate sample test data Fixtures <code>generate_sample_data(rows=100)</code> Testing Create temp directories Fixtures <code>with temp_directory() as d:</code> Testing Deterministic replay tests Source Pools <code>SourcePoolConfig</code> Source Pools Test data with nulls/unicode Data quality pools <code>DataQuality.MESSY</code> Source Pools Hash-verify frozen test data Integrity manifest <code>IntegrityManifest</code> Source Pools"},{"location":"playbook/#web-ui","title":"Web UI","text":"Problem Solution Odibi Feature Docs View pipeline status in browser Web dashboard <code>odibi ui config.yaml</code> UI Browse story reports visually Stories viewer <code>/stories</code> endpoint UI View current config Config viewer <code>/config</code> endpoint UI"},{"location":"playbook/#advanced-internals","title":"Advanced Internals","text":"<p>\"Low-level APIs for power users and contributors.\"</p>"},{"location":"playbook/#state-management","title":"State Management","text":"Problem Solution Odibi Feature Docs Choose state backend Backend config <code>LocalJSONStateBackend</code> vs <code>CatalogStateBackend</code> State Access HWM programmatically State API <code>state_mgr.get_hwm(key)</code> State Set HWM programmatically State API <code>state_mgr.set_hwm(key, value)</code> State Batch HWM updates (parallel pipelines) Efficient writes <code>state_mgr.set_hwm_batch(updates)</code> State Handle Delta concurrent writes Retry logic Auto-retry with exponential backoff State"},{"location":"playbook/#context-api","title":"Context API","text":"Problem Solution Odibi Feature Docs Access engine-specific context Context classes <code>PandasContext</code>, <code>SparkContext</code>, <code>PolarsContext</code> Context API Get current DataFrame in transform Context get <code>context.get(\"node_name\")</code> Context API Register DataFrame for downstream Context set <code>context.set(\"name\", df)</code> Context API Access Spark session Engine context <code>context.engine_context.spark</code> Context API Execute SQL in context SQL execution <code>context.sql(\"SELECT ...\")</code> Context API"},{"location":"playbook/#engines","title":"Engines","text":"<p>Odibi supports 3 execution engines with feature parity:</p> Engine Best For Config <code>pandas</code> Small data (&lt;1GB), local dev <code>engine: pandas</code> <code>polars</code> Medium data (1-10GB), fast local <code>engine: polars</code> <code>spark</code> Big data (&gt;10GB), Delta Lake, Databricks <code>engine: spark</code> <p>All patterns and transformers work on all engines.</p> <p>See: Engine Parity Table, Spark Tutorial</p>"},{"location":"playbook/#gaps-coming-soon","title":"Gaps / Coming Soon","text":"Pattern Status Notes Streaming ingestion Partial See Spark Tutorial Full CDC patterns Partial Merge covers basics, need CDC-specific guide Data vault patterns Not yet Hub/Link/Satellite patterns Slowly Changing Dimension Type 3 Not yet Current + previous value"},{"location":"playbook/#see-also","title":"See Also","text":"<ul> <li>Getting Started \u2014 First pipeline tutorial</li> <li>YAML Reference \u2014 Full configuration options</li> <li>Python API Guide \u2014 Programmatic usage</li> <li>Plugins &amp; Extensibility \u2014 Custom connections &amp; transforms</li> <li>Source Pools Design \u2014 Deterministic test data</li> <li>Glossary \u2014 Terminology reference</li> <li>Best Practices \u2014 Production patterns</li> </ul>"},{"location":"reference/PARITY_TABLE/","title":"Engine Parity Table","text":"<p>Comparison of Pandas, Spark, and Polars engine capabilities as of V3.</p>"},{"location":"reference/PARITY_TABLE/#write-modes","title":"Write Modes","text":"Mode Pandas Spark Polars Notes <code>overwrite</code> \u2705 \u2705 \u2705 Default mode. <code>append</code> \u2705 \u2705 \u2705 <code>error</code> \u2705 \u2705 \u2705 Fails if table exists. <code>ignore</code> \u2705 \u2705 \u2705 Skips if table exists. <code>upsert</code> \u2705 \u2705 \u2705 Requires <code>keys</code>. Spark: Delta Lake only. <code>append_once</code> \u2705 \u2705 \u2705 Requires <code>keys</code>. Spark: Delta Lake only."},{"location":"reference/PARITY_TABLE/#core-features","title":"Core Features","text":"Feature Pandas Spark Polars Notes Reading (CSV, Parquet, JSON) \u2705 \u2705 \u2705 Reading (Delta Lake) \u2705 \u2705 \u2705 Pandas/Polars require <code>deltalake</code> package. Reading (SQL Server) \u2705 \u2705 \u2705 SQL Transformations \u2705 \u2705 \u2705 Pandas uses DuckDB/SQLite. Polars uses native SQL context. PII Anonymization \u2705 \u2705 \u2705 Hash, Mask, Redact. Schema Validation \u2705 \u2705 \u2705 Data Contracts \u2705 \u2705 \u2705 Incremental Loading \u2705 \u2705 \u2705 Rolling Window &amp; Stateful. Time Travel \u2705 \u2705 \u2705 Delta Lake only."},{"location":"reference/PARITY_TABLE/#execution-context","title":"Execution Context","text":"Property Pandas Spark Polars Notes <code>df</code> \u2705 \u2705 \u2705 Native DataFrame. <code>columns</code> \u2705 \u2705 \u2705 <code>schema</code> \u2705 \u2705 \u2705 Dictionary of types. <code>pii_metadata</code> \u2705 \u2705 \u2705 Active PII columns."},{"location":"reference/cheatsheet/","title":"Odibi Cheatsheet","text":""},{"location":"reference/cheatsheet/#cli-commands","title":"CLI Commands","text":"Command Description <code>odibi run odibi.yaml</code> Run the pipeline. <code>odibi run odibi.yaml --dry-run</code> Validate connections without moving data. <code>odibi doctor odibi.yaml</code> Check environment and config health. <code>odibi stress odibi.yaml</code> Run fuzz tests (random data) to find bugs. <code>odibi story view --latest</code> Open the latest run report. <code>odibi secrets init odibi.yaml</code> Create .env template for secrets. <code>odibi graph odibi.yaml</code> Visualize pipeline dependencies. <code>odibi generate-project</code> Scaffold a new project from files."},{"location":"reference/cheatsheet/#odibiyaml-structure","title":"<code>odibi.yaml</code> Structure","text":"<pre><code>version: 1\nproject: My Project\nengine: pandas          # or 'spark'\n\nconnections:\n  raw_data:\n    type: local\n    base_path: ./data\n\nstory:\n  connection: raw_data\n  path: stories/\n\npipelines:\n  - pipeline: main_etl\n    nodes:\n      # 1. Read\n      - name: load_csv\n        read:\n          connection: raw_data\n          path: input.csv\n          format: csv\n\n      # 2. Transform (SQL)\n      - name: clean_data\n        depends_on: [load_csv]\n        transform:\n          steps:\n            - \"SELECT * FROM load_csv WHERE id IS NOT NULL\"\n\n      # 3. Transform (Python)\n      - name: advanced_clean\n        depends_on: [clean_data]\n        transform:\n          steps:\n            - operation: my_custom_func  # defined in python\n              params:\n                threshold: 10\n\n      # 4. Write\n      - name: save_parquet\n        depends_on: [advanced_clean]\n        write:\n          connection: raw_data\n          path: output.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"reference/cheatsheet/#python-transformation","title":"Python Transformation","text":"<pre><code>from odibi.transformations import transformation\n\n@transformation(\"my_custom_func\")\ndef my_func(df, threshold=10):\n    \"\"\"Docstrings are required!\"\"\"\n    return df[df['val'] &gt; threshold]\n</code></pre>"},{"location":"reference/cheatsheet/#cross-pipeline-dependencies","title":"Cross-Pipeline Dependencies","text":"<p>Reference outputs from other pipelines using <code>$pipeline.node</code> syntax:</p> <pre><code>nodes:\n  - name: enriched_data\n    inputs:\n      # Cross-pipeline reference\n      events: $read_bronze.shift_events\n\n      # Explicit read config\n      calendar:\n        connection: prod\n        path: \"bronze/calendar\"\n        format: delta\n    transform:\n      steps:\n        - operation: join\n          left: events\n          right: calendar\n          on: [date_id]\n</code></pre> Syntax Example Description <code>$pipeline.node</code> <code>$read_bronze.orders</code> Reference node output from another pipeline <p>Requirements: - Referenced pipeline must have run first - Referenced node must have a <code>write</code> block - Cannot use both <code>read</code> and <code>inputs</code> in same node</p>"},{"location":"reference/cheatsheet/#sql-templates","title":"SQL Templates","text":"Variable Value <code>${source}</code> The path of the source file (if reading). <code>${SELF}</code> The name of the current node."},{"location":"reference/configuration/","title":"ODIBI Configuration System Explained","text":"<p>Last Updated: 2025-11-21 Author: Henry Odibi Purpose: Demystify how YAML configs, Python classes, and execution flow together</p>"},{"location":"reference/configuration/#table-of-contents","title":"\ud83d\udcd1 Table of Contents","text":"<ol> <li>The Big Picture</li> <li>Configuration Flow (Concept \u2192 Execution)</li> <li>Three Layers of Configuration</li> <li>Example: Tracing a Pipeline from YAML to Execution</li> <li>Key Concepts Explained</li> <li>Common Confusion Points</li> <li>Decision Trees</li> <li>Quick Reference</li> </ol>"},{"location":"reference/configuration/#the-big-picture","title":"The Big Picture","text":"<p>ODIBI has three distinct layers that work together:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 1: YAML Configuration (What You Write)              \u2502\n\u2502  - Declarative syntax (project.yaml, pipelines/)            \u2502\n\u2502  - Human-readable, version-controlled                       \u2502\n\u2502  - Defines WHAT to do, not HOW                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Parsed by\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 2: Pydantic Models (config.py)                      \u2502\n\u2502  - Validates YAML structure                                 \u2502\n\u2502  - Enforces required fields, types, constraints             \u2502\n\u2502  - Converts YAML \u2192 Python objects                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Used by\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 3: Runtime Classes (pipeline.py, node.py, etc.)     \u2502\n\u2502  - Executes the actual work                                 \u2502\n\u2502  - Manages context, engines, connections                    \u2502\n\u2502  - Generates stories, handles errors                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Insight: You interact with Layer 1 (YAML), but under the hood, Pydantic (Layer 2) ensures correctness before Runtime (Layer 3) does the work.</p>"},{"location":"reference/configuration/#configuration-flow-concept-execution","title":"Configuration Flow (Concept \u2192 Execution)","text":"<p>Let's trace a real pipeline from file to execution:</p>"},{"location":"reference/configuration/#step-1-you-write-yaml-user","title":"Step 1: You Write YAML (User)","text":"<pre><code># examples/templates/example_local.yaml\nproject: My Pipeline\nengine: pandas\nconnections:\n  local:\n    type: local\n    base_path: ./data\npipelines:\n  - pipeline: bronze_to_silver\n    nodes:\n      - name: load_raw_sales\n        read:\n          connection: local\n          path: bronze/sales.csv\n          format: csv\n</code></pre>"},{"location":"reference/configuration/#step-2-yaml-pydantic-models-automatic","title":"Step 2: YAML \u2192 Pydantic Models (Automatic)","text":"<p>When you call <code>Pipeline.from_yaml(\"examples/templates/example_local.yaml\")</code>:</p> <pre><code># pipeline.py (line 317)\nwith open(yaml_path, \"r\") as f:\n    config = yaml.safe_load(f)  # Loads YAML as Python dict\n\n# Parse into Pydantic models (config.py)\nproject_config = ProjectConfig(**config)  # Validates project/engine/connections\npipeline_config = PipelineConfig(**config['pipelines'][0])  # Validates pipeline structure\n</code></pre> <p>What Pydantic does: - Checks that <code>project</code>, <code>engine</code>, <code>connections</code> exist - Ensures <code>engine</code> is \"pandas\", \"spark\", or \"polars\" (not \"panda\" or \"pands\") - Validates each node has required fields (<code>name</code>, at least one operation) - Converts strings to enums where needed (e.g., <code>engine: \"pandas\"</code> \u2192 <code>EngineType.PANDAS</code>)</p> <p>If validation fails:</p> <pre><code>ValidationError: 1 validation error for PipelineConfig\nnodes.0.read.connection\n  field required (type=value_error.missing)\n</code></pre> <p>This is Layer 2 catching errors before execution.</p>"},{"location":"reference/configuration/#step-3-create-connections-automatic","title":"Step 3: Create Connections (Automatic)","text":"<pre><code># pipeline.py (lines 342-351) - PipelineManager.from_yaml()\nconnections = {}\nfor conn_name, conn_config in config.get(\"connections\", {}).items():\n    if conn_config.get(\"type\") == \"local\":\n        connections[conn_name] = LocalConnection(\n            base_path=conn_config.get(\"base_path\", \"./data\")\n        )\n</code></pre> <p>Result: Python objects ready to use:</p> <pre><code>connections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n</code></pre>"},{"location":"reference/configuration/#step-4-create-pipelinemanager-automatic","title":"Step 4: Create PipelineManager (Automatic)","text":"<pre><code># pipeline.py (lines 280-314)\nmanager = PipelineManager(\n    yaml_config=config,\n    engine=\"pandas\",\n    connections=connections,\n    story_config=config.get(\"story\", {})\n)\n</code></pre> <p>What PipelineManager does: - Creates a <code>Pipeline</code> object for each pipeline in the YAML - Stores them in <code>self._pipelines</code> dictionary keyed by pipeline name - Example: <code>manager._pipelines = {\"bronze_to_silver\": Pipeline(...), \"silver_to_gold\": Pipeline(...)}</code></p>"},{"location":"reference/configuration/#step-5-run-pipelines-user","title":"Step 5: Run Pipelines (User)","text":"<pre><code># You call:\nresults = manager.run()  # Runs ALL pipelines\n\n# Or:\nresult = manager.run('bronze_to_silver')  # Runs specific pipeline\n</code></pre> <p>What happens: 1. <code>PipelineManager.run()</code> looks up the pipeline by name 2. Calls <code>Pipeline.run()</code> (line 134) 3. <code>Pipeline.run()</code> orchestrates node execution:    - Builds dependency graph    - Topologically sorts nodes    - Executes each node in order    - Passes data via Context    - Generates story</p>"},{"location":"reference/configuration/#three-layers-of-configuration","title":"Three Layers of Configuration","text":""},{"location":"reference/configuration/#layer-1-yaml-files-declarative","title":"Layer 1: YAML Files (Declarative)","text":"<p>Purpose: Human-readable, version-controlled pipeline definitions</p> <p>Key Files: - Project-level: <code>project.yaml</code> or any YAML with <code>project</code> + <code>connections</code> + <code>pipelines</code> - Pipeline-level: Individual YAML files with specific pipelines</p> <p>What You Define: | Section | Purpose | Required | |---------|---------|----------| | <code>project</code> | Project name | \u2705 Yes | | <code>engine</code> | Execution engine (pandas/spark/polars) | \u2705 Yes | | <code>connections</code> | Where data lives | \u2705 Yes | | <code>pipelines</code> | List of pipelines | \u2705 Yes | | <code>story</code> | Story generation config | \u2705 Yes |</p> <p>Example:</p> <pre><code>project: Sales Analytics\nengine: pandas\nconnections:\n  warehouse:\n    type: local\n    base_path: /data/warehouse\npipelines:\n  - pipeline: daily_sales\n    nodes: [...]\n</code></pre>"},{"location":"reference/configuration/#layer-2-pydantic-models-validation","title":"Layer 2: Pydantic Models (Validation)","text":"<p>Purpose: Type-safe, validated Python objects</p> <p>Key File: <code>odibi/config.py</code></p> <p>Main Models:</p>"},{"location":"reference/configuration/#projectconfig-line-266","title":"ProjectConfig (Line 266)","text":"<pre><code>class ProjectConfig(BaseModel):\n    project: str  # Required\n    version: str = \"1.0.0\"  # Default\n    engine: EngineType = EngineType.PANDAS  # Default\n    connections: Dict[str, Dict[str, Any]]  # Required\n    story: StoryConfig  # Required\n    pipelines: List[PipelineConfig]  # Required\n    retry: RetryConfig = RetryConfig()  # Default\n    logging: LoggingConfig = LoggingConfig()  # Default\n</code></pre> <p>Maps to YAML:</p> <pre><code>project: My Pipeline      # \u2192 project\nversion: \"2.0.0\"          # \u2192 version\nengine: pandas            # \u2192 engine (validated as EngineType.PANDAS)\nconnections:\n  data:                   # \u2192 connections[\"data\"]\n    type: local\n    base_path: ./data\n  outputs:                # \u2192 connections[\"outputs\"]\n    type: local\n    base_path: ./outputs\n  api_source:             # \u2192 connections[\"api_source\"]\n    type: http\n    base_url: \"https://api.example.com/v1\"\n    headers:\n      Authorization: \"Bearer ${API_TOKEN}\"\nstory:\n  connection: outputs     # \u2192 story.connection (required)\n  path: stories/          # \u2192 story.path\n  auto_generate: true     # \u2192 story.auto_generate\n  max_sample_rows: 10     # \u2192 story.max_sample_rows\n  retention_days: 30      # \u2192 story.retention_days\n  retention_count: 100    # \u2192 story.retention_count\nretry:\n  enabled: true           # \u2192 retry.enabled\n  max_attempts: 3         # \u2192 retry.max_attempts\n  backoff: exponential    # \u2192 retry.backoff\nlogging:\n  level: INFO             # \u2192 logging.level\npipelines:                # \u2192 pipelines (list of pipeline configs)\n  - pipeline: example\n    nodes: [...]\n</code></pre>"},{"location":"reference/configuration/#pipelineconfig-line-203","title":"PipelineConfig (Line 203)","text":"<pre><code>class PipelineConfig(BaseModel):\n    pipeline: str  # Required (pipeline name)\n    description: Optional[str] = None\n    layer: Optional[str] = None\n    nodes: List[NodeConfig]  # Required (at least one node)\n</code></pre> <p>Maps to YAML:</p> <pre><code>pipelines:\n  - pipeline: bronze_to_silver  # \u2192 pipeline\n    layer: transformation       # \u2192 layer\n    description: \"Clean data\"   # \u2192 description\n    nodes: [...]               # \u2192 nodes\n</code></pre>"},{"location":"reference/configuration/#nodeconfig-line-172","title":"NodeConfig (Line 172)","text":"<pre><code>class NodeConfig(BaseModel):\n    name: str  # Required (unique within pipeline)\n    depends_on: List[str] = []\n    read: Optional[ReadConfig] = None\n    inputs: Optional[Dict[str, Union[str, Dict[str, Any]]]] = None  # Cross-pipeline dependencies\n    transform: Optional[TransformConfig] = None\n    write: Optional[WriteConfig] = None\n    cache: bool = False\n    sensitive: Union[bool, List[str]] = False  # PII Masking\n</code></pre> <p>Maps to YAML:</p> <pre><code>nodes:\n  - name: load_raw_sales         # \u2192 name\n    depends_on: [prev_node]      # \u2192 depends_on\n    sensitive: [\"email\"]         # \u2192 sensitive (Redact email in reports)\n    read:                        # \u2192 read (ReadConfig)\n      connection: local\n      path: bronze/sales.csv\n      format: csv\n    cache: true                  # \u2192 cache\n</code></pre> <p>Cross-Pipeline Dependencies (<code>inputs</code> block):</p> <p>For multi-input nodes that read from other pipelines, use the <code>inputs</code> block instead of <code>read</code>:</p> <pre><code>nodes:\n  - name: enriched_data\n    inputs:\n      events: $read_bronze.shift_events      # Cross-pipeline reference\n      calendar:                               # Explicit read config\n        connection: goat_prod\n        path: \"bronze/calendar\"\n        format: delta\n    transform:\n      steps:\n        - operation: join\n          left: events\n          right: calendar\n          on: [date_id]\n</code></pre> <p>Reference Syntax: <code>$pipeline_name.node_name</code> - The <code>$</code> prefix indicates a cross-pipeline reference - References are resolved via the Odibi Catalog (<code>meta_outputs</code> table) - The referenced node must have a <code>write</code> block and the pipeline must have run previously</p> <p>Validation Rules: - Node must have at least one of: <code>read</code>, <code>inputs</code>, <code>transform</code>, <code>write</code> - All node names must be unique within a pipeline - Connections referenced in <code>read.connection</code> or <code>write.connection</code> should exist (warned, not enforced) - Cannot have both <code>read</code> and <code>inputs</code> \u2014 use <code>read</code> for single-source nodes or <code>inputs</code> for multi-source cross-pipeline dependencies</p>"},{"location":"reference/configuration/#layer-3-runtime-classes-execution","title":"Layer 3: Runtime Classes (Execution)","text":"<p>Purpose: Execute the actual work</p> <p>Key Files: - <code>odibi/pipeline.py</code> - Pipeline orchestration - <code>odibi/node.py</code> - Individual node execution - <code>odibi/context.py</code> - Data passing between nodes - <code>odibi/engine/pandas_engine.py</code> - Actual read/write/transform logic</p> <p>Main Classes:</p>"},{"location":"reference/configuration/#pipelinemanager-line-280","title":"PipelineManager (Line 280)","text":"<pre><code>class PipelineManager:\n    def __init__(self, yaml_config, engine, connections, story_config):\n        self._pipelines = {}  # Dict[pipeline_name -&gt; Pipeline]\n        for pipeline_config_dict in yaml_config[\"pipelines\"]:\n            pipeline_config = PipelineConfig(**pipeline_config_dict)\n            self._pipelines[pipeline_config.pipeline] = Pipeline(...)\n\n    def run(self, pipelines=None):\n        # Run all or specific pipelines\n</code></pre> <p>Responsibilities: - Load and validate YAML - Instantiate connections - Create Pipeline objects for each pipeline - Orchestrate multi-pipeline execution - Return results</p>"},{"location":"reference/configuration/#pipeline-line-63","title":"Pipeline (Line 63)","text":"<pre><code>class Pipeline:\n    def __init__(self, pipeline_config, engine, connections, ...):\n        self.config = pipeline_config  # PipelineConfig from Layer 2\n        self.engine = PandasEngine()   # Or SparkEngine\n        self.context = create_context(engine)\n        self.graph = DependencyGraph(pipeline_config.nodes)\n\n    def run(self):\n        execution_order = self.graph.topological_sort()\n        for node_name in execution_order:\n            node = Node(...)\n            node_result = node.execute()\n</code></pre> <p>Responsibilities: - Build dependency graph - Determine execution order - Execute nodes sequentially (or parallel in future) - Manage context for data passing - Generate stories</p>"},{"location":"reference/configuration/#node-odibinodepy","title":"Node (odibi/node.py)","text":"<pre><code>class Node:\n    def execute(self):\n        # 1. Read data (if read config exists)\n        if self.config.read:\n            data = self.engine.read(...)\n            self.context.register(self.config.name, data)\n\n        # 2. Transform data (if transform config exists)\n        if self.config.transform:\n            data = self.engine.execute_transform(...)\n            self.context.register(self.config.name, data)\n\n        # 3. Write data (if write config exists)\n        if self.config.write:\n            self.engine.write(...)\n</code></pre> <p>Responsibilities: - Execute read \u2192 transform \u2192 write for a single node - Use engine for actual operations - Register results in context - Return NodeResult</p>"},{"location":"reference/configuration/#example-tracing-a-pipeline-from-yaml-to-execution","title":"Example: Tracing a Pipeline from YAML to Execution","text":"<p>Let's trace this simple YAML:</p> <pre><code>project: Simple Pipeline\nengine: pandas\nconnections:\n  local:\n    type: local\n    base_path: ./data\n  outputs:\n    type: local\n    base_path: ./outputs\nstory:\n  connection: outputs\n  path: stories/\n  enabled: true\npipelines:\n  - pipeline: example\n    nodes:\n      - name: load_data\n        read:\n          connection: local\n          path: input.csv\n          format: csv\n        cache: true\n\n      - name: clean_data\n        depends_on: [load_data]\n        transform:\n          steps:\n            - \"SELECT * FROM load_data WHERE amount &gt; 0\"\n\n      - name: save_data\n        depends_on: [clean_data]\n        write:\n          connection: local\n          path: output.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"reference/configuration/#execution-trace","title":"Execution Trace","text":"<p>1. User calls:</p> <pre><code>from odibi.pipeline import Pipeline\nmanager = Pipeline.from_yaml(\"simple.yaml\")\n</code></pre> <p>2. <code>Pipeline.from_yaml()</code> delegates to <code>PipelineManager.from_yaml()</code> (line 109)</p> <p>3. <code>PipelineManager.from_yaml()</code> (line 317):</p> <pre><code># Load YAML\nwith open(\"simple.yaml\") as f:\n    config = yaml.safe_load(f)\n# config = {\n#     \"project\": \"Simple Pipeline\",\n#     \"engine\": \"pandas\",\n#     \"connections\": {\"local\": {\"type\": \"local\", \"base_path\": \"./data\"}},\n#     \"pipelines\": [{\"pipeline\": \"example\", \"nodes\": [...]}]\n# }\n\n# Validate project config (entire YAML - single source of truth)\nproject_config = ProjectConfig(**config)\n# \u2705 Validation passed - checks:\n#    - Required fields: project, connections, story, pipelines\n#    - story.connection exists in connections\n#    - engine is valid (pandas, spark, or polars)\n\n# Create connections\nconnections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n\n# Create PipelineManager\nmanager = PipelineManager(\n    project_config=project_config,\n    engine=\"pandas\",\n    connections=connections\n)\n</code></pre> <p>4. <code>PipelineManager.__init__()</code> (line 283):</p> <pre><code># Loop through pipelines in YAML\nfor pipeline_config_dict in config[\"pipelines\"]:\n    # Validate pipeline config\n    pipeline_config = PipelineConfig(\n        pipeline=\"example\",\n        nodes=[\n            NodeConfig(name=\"load_data\", read=ReadConfig(...), cache=True),\n            NodeConfig(name=\"clean_data\", depends_on=[\"load_data\"], transform=TransformConfig(...)),\n            NodeConfig(name=\"save_data\", depends_on=[\"clean_data\"], write=WriteConfig(...))\n        ]\n    )\n    # \u2705 Validation passed (all nodes have unique names, at least one operation each)\n\n    # Create Pipeline instance\n    self._pipelines[\"example\"] = Pipeline(\n        pipeline_config=pipeline_config,\n        engine=\"pandas\",\n        connections={\"local\": LocalConnection(...)},\n        story_config={}\n    )\n</code></pre> <p>5. User runs:</p> <pre><code>results = manager.run()  # Run all pipelines\n</code></pre> <p>6. <code>PipelineManager.run()</code> (line 363):</p> <pre><code># pipelines=None means run all\npipeline_names = list(self._pipelines.keys())  # [\"example\"]\n\n# Run each pipeline\nfor name in pipeline_names:\n    results[name] = self._pipelines[name].run()\n</code></pre> <p>7. <code>Pipeline.run()</code> (line 134):</p> <pre><code># Get execution order from dependency graph\nexecution_order = self.graph.topological_sort()\n# Returns: [\"load_data\", \"clean_data\", \"save_data\"]\n\n# Execute nodes in order\nfor node_name in execution_order:  # \"load_data\"\n    node_config = self.graph.nodes[\"load_data\"]\n    node = Node(\n        config=node_config,\n        context=self.context,\n        engine=self.engine,  # PandasEngine\n        connections={\"local\": LocalConnection(...)}\n    )\n    node_result = node.execute()\n</code></pre> <p>8. <code>Node.execute()</code> for \"load_data\" (odibi/node.py):</p> <pre><code># Node has read config\nif self.config.read:\n    # Get connection\n    conn = self.connections[\"local\"]  # LocalConnection(base_path=\"./data\")\n    full_path = conn.get_path(\"input.csv\")  # \"./data/input.csv\"\n\n    # Read using engine\n    data = self.engine.read(\n        path=full_path,\n        format=\"csv\",\n        options={}\n    )\n    # data = pandas.DataFrame(...)\n\n    # Register in context\n    self.context.register(\"load_data\", data)\n</code></pre> <p>9. <code>Node.execute()</code> for \"clean_data\":</p> <pre><code># Node has transform config\nif self.config.transform:\n    sql = \"SELECT * FROM load_data WHERE amount &gt; 0\"\n    data = self.engine.execute_sql(sql, self.context)\n    # Engine gets \"load_data\" DataFrame from context\n    # Executes SQL using pandasql or duckdb\n    # Returns filtered DataFrame\n\n    self.context.register(\"clean_data\", data)\n</code></pre> <p>10. <code>Node.execute()</code> for \"save_data\":</p> <pre><code># Node has write config\nif self.config.write:\n    data = self.context.get(\"clean_data\")  # Get from previous node\n    conn = self.connections[\"local\"]\n    full_path = conn.get_path(\"output.parquet\")  # \"./data/output.parquet\"\n\n    self.engine.write(\n        data=data,\n        path=full_path,\n        format=\"parquet\",\n        mode=\"overwrite\",\n        options={}\n    )\n    # Writes DataFrame to ./data/output.parquet\n</code></pre> <p>11. Story generation (if enabled):</p> <pre><code>story_path = self.story_generator.generate(\n    node_results={...},\n    completed=[\"load_data\", \"clean_data\", \"save_data\"],\n    failed=[],\n    ...\n)\n# Generates markdown story in ./stories/\n</code></pre> <p>12. Return results to user:</p> <pre><code>results = {\n    \"example\": PipelineResults(\n        pipeline_name=\"example\",\n        completed=[\"load_data\", \"clean_data\", \"save_data\"],\n        failed=[],\n        duration=2.3,\n        story_path=\"./stories/example_20251107_143025.md\"\n    )\n}\n</code></pre>"},{"location":"reference/configuration/#key-concepts-explained","title":"Key Concepts Explained","text":""},{"location":"reference/configuration/#1-config-vs-runtime","title":"1. Config vs Runtime","text":"<p>Config (Layer 1 + 2): - What you declare in YAML - Validated by Pydantic - Immutable once loaded - Example: <code>ReadConfig(connection=\"local\", path=\"input.csv\", format=\"csv\")</code></p> <p>Runtime (Layer 3): - What executes the work - Uses config to make decisions - Mutable state (context, results) - Example: <code>PandasEngine.read(path=\"./data/input.csv\", format=\"csv\")</code> \u2192 returns DataFrame</p> <p>Why separate? - Validation happens early (before execution) - Config is reusable (can run same config multiple times) - Easier testing (mock runtime, test config separately)</p>"},{"location":"reference/configuration/#2-connection-config-vs-object","title":"2. Connection: Config vs Object","text":"<p>Connection Config (YAML):</p> <pre><code>connections:\n  local:\n    type: local\n    base_path: ./data\n</code></pre> <p>Connection Object (Python):</p> <pre><code>connections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n</code></pre> <p>What's the difference? - Config is declarative (YAML dict) - Object is executable (Python class with methods like <code>.get_path()</code>)</p> <p>Why both? - YAML is portable (version controlled, shareable) - Objects are functional (can call methods, maintain state)</p>"},{"location":"reference/configuration/#3-pipeline-vs-pipelinemanager","title":"3. Pipeline vs PipelineManager","text":"<p>Pipeline: - Represents one pipeline - Has nodes, dependencies, execution logic - Example: <code>bronze_to_silver</code> pipeline</p> <p>PipelineManager: - Manages multiple pipelines - Loads YAML, creates connections, instantiates Pipelines - Provides unified API: <code>manager.run()</code> runs all, <code>manager.run('bronze_to_silver')</code> runs one</p> <p>Why <code>Pipeline.from_yaml()</code> returns PipelineManager? - Convenience: Most YAMLs have multiple pipelines - Backward compatible: Users can still call <code>Pipeline.from_yaml()</code> - Unified API: <code>manager.run()</code> works for 1 or 10 pipelines</p>"},{"location":"reference/configuration/#4-from_yaml-the-boilerplate-eliminator","title":"4. <code>from_yaml()</code> - The Boilerplate Eliminator","text":"<p>Before (manual setup):</p> <pre><code>import yaml\nfrom odibi.pipeline import Pipeline\nfrom odibi.config import PipelineConfig\nfrom odibi.connections import LocalConnection\n\nwith open(\"config.yaml\") as f:\n    config = yaml.safe_load(f)\n\npipeline_config = PipelineConfig(**config['pipelines'][0])\nconnections = {\n    'local': LocalConnection(base_path=config['connections']['local']['base_path'])\n}\npipeline = Pipeline(\n    pipeline_config=pipeline_config,\n    engine=\"pandas\",\n    connections=connections\n)\nresults = pipeline.run()\n</code></pre> <p>After (<code>from_yaml()</code>):</p> <pre><code>from odibi.pipeline import Pipeline\n\nmanager = Pipeline.from_yaml(\"config.yaml\")\nresults = manager.run()\n</code></pre> <p>What <code>from_yaml()</code> does: 1. Load YAML 2. Validate with Pydantic 3. Create connection objects 4. Instantiate PipelineManager 5. Return ready-to-run manager</p> <p>Result: 2 lines instead of 15!</p>"},{"location":"reference/configuration/#5-context-the-data-bus","title":"5. Context - The Data Bus","text":"<p>Purpose: Pass data between nodes without explicit function calls</p> <p>How it works:</p> <pre><code># Node 1: load_data\ndata = engine.read(...)\ncontext.register(\"load_data\", data)  # Store DataFrame\n\n# Node 2: clean_data (depends_on: [load_data])\ndata = context.get(\"load_data\")  # Retrieve DataFrame\ncleaned = engine.execute_sql(\"SELECT * FROM load_data WHERE ...\", context)\ncontext.register(\"clean_data\", cleaned)\n\n# Node 3: save_data (depends_on: [clean_data])\ndata = context.get(\"clean_data\")\nengine.write(data, ...)\n</code></pre> <p>Why not return values? - Nodes execute sequentially but independently - SQL transforms reference DataFrames by name (not variable) - Context provides unified API across Pandas and Spark</p>"},{"location":"reference/configuration/#common-confusion-points","title":"Common Confusion Points","text":""},{"location":"reference/configuration/#confusion-1-why-do-i-see-both-pipeline-and-name","title":"Confusion #1: \"Why do I see both <code>pipeline</code> and <code>name</code>?\"","text":"<p>Answer: Different levels of abstraction!</p> <pre><code>pipelines:                   # List of pipelines\n  - pipeline: bronze_to_silver  # \u2190 Pipeline NAME (identifies the pipeline)\n    nodes:                     # List of nodes in THIS pipeline\n      - name: load_data        # \u2190 Node NAME (identifies the node)\n</code></pre> <p>Analogy: - <code>pipeline</code> is like a book title (\"Harry Potter\") - <code>name</code> is like a chapter name (\"The Boy Who Lived\")</p> <p>In code: - <code>PipelineConfig.pipeline</code> \u2192 pipeline name - <code>NodeConfig.name</code> \u2192 node name</p>"},{"location":"reference/configuration/#confusion-2-whats-the-difference-between-connection-local-and-type-local","title":"Confusion #2: \"What's the difference between <code>connection: local</code> and <code>type: local</code>?\"","text":"<p>Answer: Different contexts!</p> <p>In <code>connections</code> section (defining connections):</p> <pre><code>connections:\n  local:           # \u2190 Connection NAME (you choose this)\n    type: local    # \u2190 Connection TYPE (system type: local, azure_adls, etc.)\n    base_path: ./data\n</code></pre> <p>In <code>read</code>/<code>write</code> section (using connections):</p> <pre><code>nodes:\n  - name: load_data\n    read:\n      connection: local  # \u2190 References the CONNECTION NAME from above\n      path: input.csv\n</code></pre> <p>Analogy: - <code>connections</code> section: \"Define a car named 'my_car' of type 'sedan'\" - <code>read.connection</code>: \"Use the car named 'my_car' to drive somewhere\"</p>"},{"location":"reference/configuration/#confusion-3-why-does-from_yaml-return-a-manager-instead-of-a-pipeline","title":"Confusion #3: \"Why does <code>from_yaml()</code> return a manager instead of a pipeline?\"","text":"<p>Answer: YAML files typically have multiple pipelines!</p> <pre><code>pipelines:\n  - pipeline: bronze_to_silver  # Pipeline 1\n    nodes: [...]\n  - pipeline: silver_to_gold    # Pipeline 2\n    nodes: [...]\n</code></pre> <p>If it returned a single Pipeline: - Which one? The first? All? - How to run specific pipelines?</p> <p>By returning PipelineManager: - Access all pipelines - Run all: <code>manager.run()</code> - Run one: <code>manager.run('bronze_to_silver')</code> - Run some: <code>manager.run(['bronze_to_silver', 'silver_to_gold'])</code></p> <p>For single pipeline YAMLs:</p> <pre><code>result = manager.run()  # If only 1 pipeline, returns PipelineResults (not dict)\n</code></pre>"},{"location":"reference/configuration/#confusion-4-whats-the-difference-between-options-and-params","title":"Confusion #4: \"What's the difference between <code>options</code> and <code>params</code>?\"","text":"<p>Answer: Different operation types!</p> <p><code>options</code> (in read/write):</p> <pre><code>read:\n  connection: local\n  path: data.csv\n  format: csv\n  options:           # \u2190 Format-specific options (passed to pandas.read_csv())\n    header: 0\n    dtype:\n      id: str\n</code></pre> <p>Maps to: <code>pandas.read_csv(path, header=0, dtype={\"id\": str})</code></p> <p><code>params</code> (in transform):</p> <pre><code>transform:\n  steps:\n    - function: my_custom_function\n      params:        # \u2190 Function arguments\n        threshold: 0.5\n        mode: strict\n</code></pre> <p>Maps to: <code>my_custom_function(context, threshold=0.5, mode='strict')</code></p> <p>Key difference: - <code>options</code> \u2192 passed to engine (Pandas/Spark I/O functions) - <code>params</code> \u2192 passed to your function</p>"},{"location":"reference/configuration/#confusion-5-where-do-stories-get-written","title":"Confusion #5: \"Where do stories get written?\"","text":"<p>Answer: Stories use the connection pattern, just like data!</p> <p>Story configuration (required):</p> <pre><code>connections:\n  outputs:\n    type: local\n    base_path: ./outputs\n\nstory:\n  connection: outputs  # \u2190 References connection name\n  path: stories/       # \u2190 Path within connection\n  enabled: true\n</code></pre> <p>Resolved path: <code>./outputs/stories/pipeline_name_20251107_143025.md</code></p> <p>Why this pattern? - Explicit: Clear where stories are written (no hidden defaults) - Traceable: Connection-based paths preserve truth - Consistent: Same pattern as <code>read.connection</code> and <code>write.connection</code> - Flexible: Stories can go to ADLS, DBFS, or local storage</p> <p>Before v1.1 (confusing):</p> <pre><code># Story path was implicit - where is \"stories/\" relative to?\nconnections:\n  local:\n    type: local\n    base_path: ./data\n</code></pre> <p>After v1.1 (explicit):</p> <pre><code>connections:\n  outputs:\n    type: local\n    base_path: ./outputs\n\nstory:\n  connection: outputs  # Required - must exist in connections\n  path: stories/\n</code></pre>"},{"location":"reference/configuration/#confusion-6-how-does-sql-find-the-dataframes","title":"Confusion #6: \"How does SQL find the DataFrames?\"","text":"<p>Answer: The engine looks them up in the context!</p> <p>Your SQL:</p> <pre><code>SELECT * FROM load_data WHERE amount &gt; 0\n</code></pre> <p>What the engine does (simplified):</p> <pre><code># PandasEngine.execute_sql()\ndef execute_sql(self, sql: str, context: Context):\n    # 1. Find all table references in SQL\n    tables = extract_table_names(sql)  # [\"load_data\"]\n\n    # 2. Get DataFrames from context\n    load_data = context.get(\"load_data\")  # The DataFrame from earlier node\n\n    # 3. Execute SQL using pandasql or duckdb\n    result = duckdb.query(sql).to_df()\n\n    return result\n</code></pre> <p>Key insight: Table names in SQL must match node names in the pipeline!</p>"},{"location":"reference/configuration/#decision-trees","title":"Decision Trees","text":""},{"location":"reference/configuration/#which-class-do-i-use","title":"\"Which class do I use?\"","text":"<pre><code>\u250c\u2500 Need to load and run a YAML file?\n\u2502  \u251c\u2500 YES \u2192 Use `Pipeline.from_yaml(\"config.yaml\")`\n\u2502  \u2502         Returns PipelineManager\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Are you building custom integrations?\n\u2502     \u251c\u2500 YES \u2192 Use `PipelineManager(...)` or `Pipeline(...)` directly\n\u2502     \u2514\u2500 NO \u2192 Use `Pipeline.from_yaml()` (recommended)\n</code></pre>"},{"location":"reference/configuration/#how-do-i-run-my-pipelines","title":"\"How do I run my pipelines?\"","text":"<pre><code>\u250c\u2500 How many pipelines in YAML?\n\u2502  \u251c\u2500 ONE \u2192 `manager.run()` returns PipelineResults\n\u2502  \u251c\u2500 MANY \u2192 `manager.run()` returns Dict[name -&gt; PipelineResults]\n\u2502  \u2502\n\u2502  \u2514\u2500 Want to run specific pipeline(s)?\n\u2502     \u251c\u2500 ONE \u2192 `manager.run('pipeline_name')` returns PipelineResults\n\u2502     \u2514\u2500 MULTIPLE \u2192 `manager.run(['pipe1', 'pipe2'])` returns Dict\n</code></pre>"},{"location":"reference/configuration/#where-does-my-configuration-live","title":"\"Where does my configuration live?\"","text":"<pre><code>\u250c\u2500 Is it about the OVERALL project?\n\u2502  \u251c\u2500 YES \u2192 Top level (project, engine, connections, story)\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Is it about a PIPELINE?\n\u2502     \u251c\u2500 YES \u2192 Under `pipelines:` (pipeline, layer, nodes)\n\u2502     \u2502\n\u2502     \u2514\u2500 NO \u2192 Is it about a NODE?\n\u2502        \u251c\u2500 YES \u2192 Under `nodes:` (name, read, transform, write)\n\u2502        \u2502\n\u2502        \u2514\u2500 NO \u2192 Is it about an OPERATION?\n\u2502           \u251c\u2500 READ \u2192 Under `read:` (connection, path, format, options)\n\u2502           \u251c\u2500 TRANSFORM \u2192 Under `transform:` (steps)\n\u2502           \u2514\u2500 WRITE \u2192 Under `write:` (connection, path, format, mode, options)\n</code></pre>"},{"location":"reference/configuration/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/configuration/#yaml-structure","title":"YAML Structure","text":"<pre><code># PROJECT LEVEL (required)\nproject: string               # Project name\nengine: pandas|spark|polars   # Execution engine\n\n# GLOBAL SETTINGS (optional)\nretry:\n  enabled: bool\n  max_attempts: int\n  backoff: exponential|linear|constant\nlogging:\n  level: DEBUG|INFO|WARNING|ERROR\n  structured: bool\n  metadata: dict\n\n# CONNECTIONS (required, at least one)\nconnections:\n  &lt;connection_name&gt;:          # Your choice of name\n    type: local|azure_blob|delta|sql_server|http\n    validation_mode: lazy|eager   # optional, defaults to 'lazy'\n    &lt;type-specific-config&gt;\n\n# ENVIRONMENTS (optional)\nenvironments:\n  &lt;env_name&gt;:\n    &lt;overrides&gt;: ...\n    # Or use external file: env.&lt;env_name&gt;.yaml\n\n# STORY (required)\nstory:\n  connection: string        # Name of connection to write stories\n  path: string              # Relative path under that connection\n  auto_generate: bool\n  max_sample_rows: int\n  retention_days: int (optional)\n  retention_count: int (optional)\n\n# PIPELINES (required, at least one)\npipelines:\n  - pipeline: string          # Pipeline name\n    layer: string (optional)\n    description: string (optional)\n    nodes:                    # At least one node\n      - name: string          # Unique node name\n        depends_on: [string]  # List of node names (optional)\n        cache: bool (optional)\n\n        # At least ONE of these:\n        read:\n          connection: string  # Connection name\n          path: string        # Relative to connection base_path (Required unless 'query' used)\n          table: string       # Table name (alternative to path)\n          format: csv|parquet|json|excel|avro|sql_server\n          options: dict       # Format-specific (optional)\n            query: string     # SQL query (substitutes for path/table in sql_server)\n\n        transform:\n          steps:              # List of SQL strings or function calls\n            - string (SQL)\n            - function: string\n              params: dict\n\n        write:\n          connection: string\n          path: string\n          table: string       # Table name (alternative to path)\n          register_table: string # Register file output as external table (Spark/Delta only)\n          format: csv|parquet|json|excel|avro|delta\n          mode: overwrite|append\n          options: dict (optional)\n</code></pre>"},{"location":"reference/configuration/#python-api-quick-reference","title":"Python API Quick Reference","text":"<pre><code># === RECOMMENDED: Simple Usage ===\nfrom odibi.pipeline import Pipeline\n\n# Load and run all pipelines\nmanager = Pipeline.from_yaml(\"examples/templates/template_full.yaml\")\nresults = manager.run()  # Dict[name -&gt; PipelineResults] or single PipelineResults\n\n# Run specific pipeline\nresult = manager.run('bronze_to_silver')\n\n# List available pipelines\nprint(manager.list_pipelines())  # ['bronze_to_silver', 'silver_to_gold']\n\n# === ADVANCED: Direct PipelineManager ===\nfrom odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\nresults = manager.run()\n\n# Access specific pipeline\npipeline = manager.get_pipeline('bronze_to_silver')\nresult = pipeline.run()\n\n# === ADVANCED: Manual Construction ===\nfrom odibi.pipeline import Pipeline\nfrom odibi.config import PipelineConfig\nfrom odibi.connections import LocalConnection\n\npipeline_config = PipelineConfig(\n    pipeline=\"my_pipeline\",\n    nodes=[...]\n)\nconnections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\npipeline = Pipeline(\n    pipeline_config=pipeline_config,\n    engine=\"pandas\",\n    connections=connections\n)\nresult = pipeline.run()\n</code></pre>"},{"location":"reference/configuration/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: Single pipeline in YAML</p> <pre><code>manager = Pipeline.from_yaml(\"simple.yaml\")\nresult = manager.run()  # Returns PipelineResults (not dict)\nprint(f\"Completed: {result.completed}\")\n</code></pre> <p>Pattern 2: Multiple pipelines, run all</p> <pre><code>manager = Pipeline.from_yaml(\"multi.yaml\")\nresults = manager.run()  # Returns Dict[name -&gt; PipelineResults]\nfor name, result in results.items():\n    print(f\"{name}: {len(result.completed)} nodes\")\n</code></pre> <p>Pattern 3: Multiple pipelines, run specific</p> <pre><code>manager = Pipeline.from_yaml(\"multi.yaml\")\nresult = manager.run('bronze_to_silver')  # Returns PipelineResults\nprint(result.to_dict())\n</code></pre>"},{"location":"reference/configuration/#summary","title":"Summary","text":"<p>The Three Layers: 1. YAML (Layer 1): What you write (declarative) 2. Pydantic Models (Layer 2): Validation (automatic) 3. Runtime Classes (Layer 3): Execution (automatic)</p> <p>The Flow:</p> <pre><code>YAML file\n  \u2192 yaml.safe_load()\n  \u2192 Pydantic validation\n  \u2192 PipelineManager/Pipeline creation\n  \u2192 manager.run()\n  \u2192 Node execution\n  \u2192 Results\n</code></pre> <p>Key Takeaways: - <code>Pipeline.from_yaml()</code> returns <code>PipelineManager</code> (not <code>Pipeline</code>) - <code>manager.run()</code> runs all pipelines (or specific ones by name) - Configs are validated before execution (fail fast) - Context passes data between nodes (SQL references node names) - Connections are defined once, referenced many times</p> <p>Questions? Confusion? Open an issue on GitHub or check the examples in the <code>examples/</code> directory for complete YAML references.</p> <p>This document evolves with the framework. Last updated: 2025-11-20</p>"},{"location":"reference/glossary/","title":"Quick Glossary","text":"<p>Standard terminology used across Odibi documentation.</p> <p>Looking for in-depth explanations? See the Learning Glossary for beginner-friendly definitions with examples and real-world analogies.</p>"},{"location":"reference/glossary/#data-quality-terms","title":"Data Quality Terms","text":"Term Definition YAML Key Contracts Pre-transform checks that always fail on violation. Use for input data validation. <code>contracts:</code> Validation Tests Post-transform row-level checks with configurable actions (fail/warn/quarantine). <code>validation.tests:</code> Quality Gates Batch-level thresholds (pass rate, row counts) evaluated after validation. <code>validation.gate:</code> Quarantine Routing invalid rows to a separate table for review instead of failing. <code>validation.quarantine:</code>"},{"location":"reference/glossary/#pipeline-terms","title":"Pipeline Terms","text":"Term Definition YAML Key Pipeline A collection of nodes that execute together as a logical unit. <code>pipelines:</code> Node A single unit of work: read \u2192 transform \u2192 validate \u2192 write. <code>nodes:</code> Transformer A pre-built \"app\" for major operations (scd2, merge, deduplicate). <code>transformer:</code> Transform Steps A chain of smaller operations (SQL, functions) for custom logic. <code>transform.steps:</code> Pattern A declarative dimensional modeling template (dimension, fact, aggregation). <code>pattern:</code>"},{"location":"reference/glossary/#dimensional-modeling-terms","title":"Dimensional Modeling Terms","text":"Term Definition Natural Key Business identifier from source system (e.g., <code>customer_id</code>). Surrogate Key System-generated integer key for joins (e.g., <code>customer_sk</code>). SCD Type 1 Overwrite dimension changes (no history). SCD Type 2 Track dimension changes with versioned rows (<code>is_current</code>, <code>valid_from</code>, <code>valid_to</code>). Grain The level of detail in a fact table (e.g., one row per order). Orphan A fact row with no matching dimension record."},{"location":"reference/glossary/#execution-terms","title":"Execution Terms","text":"Term Definition Story Execution report with lineage, metrics, and validation results. Connection Named data source/destination (local, Azure, Delta, SQL Server). Context Runtime environment holding registered DataFrames and engine state."},{"location":"reference/glossary/#actions-on-failure","title":"Actions on Failure","text":"Term Usage Context Behavior <code>fail</code> Contracts, Validation Stop execution immediately <code>warn</code> Validation Log warning, continue processing <code>quarantine</code> Validation Route bad rows to quarantine table <code>abort</code> Quality Gates Stop pipeline, write nothing <code>warn_and_write</code> Quality Gates Log warning, write all rows <code>write_valid_only</code> Quality Gates Write only rows that passed"},{"location":"reference/glossary/#see-also","title":"See Also","text":"<ul> <li>YAML Configuration Reference - Complete configuration options</li> <li>Validation Overview - Data quality framework</li> </ul>"},{"location":"reference/supported_formats/","title":"Supported File Formats","text":"<p>ODIBI's PandasEngine supports multiple file formats with both local and cloud storage (ADLS, S3, etc.).</p>"},{"location":"reference/supported_formats/#format-support-matrix","title":"Format Support Matrix","text":"Format Read Write ADLS S3 Local Dependencies CSV \u2705 \u2705 \u2705 \u2705 \u2705 pandas (built-in) Parquet \u2705 \u2705 \u2705 \u2705 \u2705 pyarrow or fastparquet JSON \u2705 \u2705 \u2705 \u2705 \u2705 pandas (built-in) Excel \u2705 \u2705 \u2705 \u2705 \u2705 openpyxl Avro \u2705 \u2705 \u2705 \u2705 \u2705 fastavro"},{"location":"reference/supported_formats/#format-details","title":"Format Details","text":""},{"location":"reference/supported_formats/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>Use Case: Simple tabular data, human-readable format</p> <p>Read Example:</p> <pre><code>- name: load_csv\n  read:\n    connection: bronze\n    path: data/sales.csv\n    format: csv\n    options:\n      sep: \",\"\n      header: true\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_csv\n  write:\n    connection: bronze\n    path: output/results.csv\n    format: csv\n    mode: overwrite\n</code></pre>"},{"location":"reference/supported_formats/#parquet-apache-parquet","title":"Parquet (Apache Parquet)","text":"<p>Use Case: Data lake storage, recommended for production</p> <p>Benefits: - Columnar format (efficient for analytics) - Built-in compression - Type preservation - Fast reads for column-based queries</p> <p>Read Example:</p> <pre><code>- name: load_parquet\n  read:\n    connection: bronze\n    path: data/sales.parquet\n    format: parquet\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_parquet\n  write:\n    connection: silver\n    path: output/sales.parquet\n    format: parquet\n    options:\n      compression: snappy  # or gzip, brotli\n</code></pre>"},{"location":"reference/supported_formats/#json-json-lines","title":"JSON (JSON Lines)","text":"<p>Use Case: Semi-structured data, nested objects</p> <p>Format: JSON lines (newline-delimited JSON objects)</p> <p>Read Example:</p> <pre><code>- name: load_json\n  read:\n    connection: bronze\n    path: data/events.json\n    format: json\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_json\n  write:\n    connection: bronze\n    path: output/events.json\n    format: json\n    options:\n      orient: records\n</code></pre>"},{"location":"reference/supported_formats/#excel-microsoft-excel","title":"Excel (Microsoft Excel)","text":"<p>Use Case: Business reports, spreadsheets</p> <p>Supported: <code>.xlsx</code> files</p> <p>Dependencies: Requires <code>openpyxl</code></p> <pre><code>pip install openpyxl\n</code></pre> <p>Read Example:</p> <pre><code>- name: load_excel\n  read:\n    connection: bronze\n    path: reports/sales.xlsx\n    format: excel\n    options:\n      sheet_name: \"Sheet1\"\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_excel\n  write:\n    connection: bronze\n    path: output/report.xlsx\n    format: excel\n</code></pre>"},{"location":"reference/supported_formats/#avro-apache-avro","title":"Avro (Apache Avro)","text":"<p>Use Case: Event streaming, schema evolution</p> <p>Benefits: - Binary format (compact) - Schema included in file - Supports schema evolution - Efficient for serialization</p> <p>Dependencies: Requires <code>fastavro</code></p> <pre><code>pip install fastavro\n</code></pre> <p>Read Example:</p> <pre><code>- name: load_avro\n  read:\n    connection: bronze\n    path: events/stream.avro\n    format: avro\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_avro\n  write:\n    connection: bronze\n    path: output/events.avro\n    format: avro\n</code></pre> <p>Note: Avro schema is automatically inferred from DataFrame dtypes.</p>"},{"location":"reference/supported_formats/#cloud-storage-support","title":"Cloud Storage Support","text":"<p>All formats work seamlessly with cloud storage:</p> <p>ADLS (Azure Data Lake Storage):</p> <pre><code>connections:\n  bronze:\n    type: azure_adls\n    account: mystorageaccount\n    container: bronze\n    auth_mode: key_vault\n    key_vault_name: my-vault\n    secret_name: storage-key\n\npipelines:\n  - pipeline: multi_format\n    nodes:\n      - name: read_csv_from_adls\n        read:\n          connection: bronze\n          path: data/sales.csv\n          format: csv\n\n      - name: write_avro_to_adls\n        depends_on: [read_csv_from_adls]\n        write:\n          connection: bronze\n          path: output/sales.avro\n          format: avro\n</code></pre> <p>All formats support: - \u2705 Multi-account connections - \u2705 Key Vault authentication - \u2705 Storage options pass-through - \u2705 Remote URI handling (<code>abfss://</code>, <code>s3://</code>)</p>"},{"location":"reference/supported_formats/#best-practices","title":"Best Practices","text":""},{"location":"reference/supported_formats/#for-data-lakes-recommended","title":"For Data Lakes (Recommended)","text":"<ol> <li>Use Parquet for production data</li> <li>Efficient storage</li> <li>Fast analytics</li> <li> <p>Type preservation</p> </li> <li> <p>Use CSV for human-readable data</p> </li> <li>Easy to inspect</li> <li>Compatible with all tools</li> <li> <p>Good for small datasets</p> </li> <li> <p>Use Avro for event streams</p> </li> <li>Schema evolution support</li> <li>Compact binary format</li> <li>Good for append-only logs</li> </ol>"},{"location":"reference/supported_formats/#performance-tips","title":"Performance Tips","text":"<p>Parquet: - Use <code>snappy</code> compression (good balance of speed/size) - Enable column pruning (read only needed columns) - Consider partitioning for large datasets (Phase 2B)</p> <p>CSV: - Use chunking for large files - Specify dtypes explicitly to avoid inference</p> <p>Avro: - Best for write-once, read-many workloads - Schema is embedded (no separate schema files needed)</p>"},{"location":"reference/supported_formats/#delta-lake-databricks-open-source","title":"Delta Lake (Databricks / Open Source)","text":"<p>Use Case: ACID transactions, time travel, data lakehouse</p> <p>Benefits: - ACID Transactions: No partial writes or corruption - Time Travel: Query previous versions of data - Schema Evolution: Safely evolve schema over time - Audit History: Track all changes to the table</p> <p>Dependencies: Requires <code>delta-spark</code> (for Spark engine) or <code>deltalake</code> (for Pandas engine).</p> <p>Read Example:</p> <pre><code>- name: load_delta\n  read:\n    connection: bronze\n    path: data/sales.delta\n    format: delta\n    options:\n      version_as_of: 5  # Time travel!\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_delta\n  write:\n    connection: silver\n    path: output/sales.delta\n    format: delta\n    mode: append  # or overwrite\n</code></pre> <p>Delta Lake supports advanced features like VACUUM and Restore.</p>"},{"location":"reference/yaml_schema/","title":"Odibi Configuration Reference","text":"<p>This manual details the YAML configuration schema for Odibi projects. Auto-generated from Pydantic models.</p>"},{"location":"reference/yaml_schema/#project-structure","title":"Project Structure","text":""},{"location":"reference/yaml_schema/#projectconfig","title":"<code>ProjectConfig</code>","text":"<p>Complete project configuration from YAML.</p>"},{"location":"reference/yaml_schema/#enterprise-setup-guide","title":"\ud83c\udfe2 \"Enterprise Setup\" Guide","text":"<p>Business Problem: \"We need a robust production environment with alerts, retries, and proper logging.\"</p> <p>Recipe: Production Ready</p> <pre><code>project: \"Customer360\"\nengine: \"spark\"\n\n# 1. Resilience\nretry:\n    enabled: true\n    max_attempts: 3\n    backoff: \"exponential\"\n\n# 2. Observability\nlogging:\n    level: \"INFO\"\n    structured: true  # JSON logs for Splunk/Datadog\n\n# 3. Alerting\nalerts:\n    - type: \"slack\"\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events: [\"on_failure\"]\n\n# ... connections and pipelines ...\n</code></pre> Field Type Required Default Description project str Yes - Project name engine EngineType No <code>EngineType.PANDAS</code> Execution engine connections Dict[str, LocalConnectionConfig | AzureBlobConnectionConfig | DeltaConnectionConfig | SQLServerConnectionConfig | HttpConnectionConfig | CustomConnectionConfig] Yes - Named connections (at least one required)Options: LocalConnectionConfig, AzureBlobConnectionConfig, DeltaConnectionConfig, SQLServerConnectionConfig, HttpConnectionConfig pipelines List[PipelineConfig] Yes - Pipeline definitions (at least one required) story StoryConfig Yes - Story generation configuration (mandatory) system SystemConfig Yes - System Catalog configuration (mandatory) lineage Optional[LineageConfig] No - OpenLineage configuration description Optional[str] No - Project description version str No <code>1.0.0</code> Project version owner Optional[str] No - Project owner/contact vars Dict[str, Any] No <code>PydanticUndefined</code> Global variables for substitution (e.g. ${vars.env}) retry RetryConfig No <code>PydanticUndefined</code> - logging LoggingConfig No <code>PydanticUndefined</code> - alerts List[AlertConfig] No <code>PydanticUndefined</code> Alert configurations performance PerformanceConfig No <code>PydanticUndefined</code> Performance tuning environments Optional[Dict[str, Dict[str, Any]]] No - Structure: same as ProjectConfig but with only overridden fields. Not yet validated strictly. semantic Optional[Dict[str, Any]] No - Semantic layer configuration. Can be inline or reference external file. Contains metrics, dimensions, and materializations for self-service analytics. Example: semantic: { config: 'semantic_config.yaml' } or inline definitions."},{"location":"reference/yaml_schema/#pipelineconfig","title":"<code>PipelineConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for a pipeline.</p> <p>Example:</p> <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"node1\"\n        ...\n</code></pre> Field Type Required Default Description pipeline str Yes - Pipeline name description Optional[str] No - Pipeline description layer Optional[str] No - Logical layer (bronze/silver/gold) nodes List[NodeConfig] Yes - List of nodes in this pipeline"},{"location":"reference/yaml_schema/#nodeconfig","title":"<code>NodeConfig</code>","text":"<p>Used in: PipelineConfig</p> <p>Configuration for a single node.</p>"},{"location":"reference/yaml_schema/#the-smart-node-pattern","title":"\ud83e\udde0 \"The Smart Node\" Pattern","text":"<p>Business Problem: \"We need complex dependencies, caching for heavy computations, and the ability to run only specific parts of the pipeline.\"</p> <p>The Solution: Nodes are the building blocks. They handle dependencies (<code>depends_on</code>), execution control (<code>tags</code>, <code>enabled</code>), and performance (<code>cache</code>).</p>"},{"location":"reference/yaml_schema/#dag-dependencies","title":"\ud83d\udd78\ufe0f DAG &amp; Dependencies","text":"<p>The Glue of the Pipeline. Nodes don't run in isolation. They form a Directed Acyclic Graph (DAG).</p> <ul> <li><code>depends_on</code>: Critical! If Node B reads from Node A (in memory), you MUST list <code>[\"Node A\"]</code>.<ul> <li>Implicit Data Flow: If a node has no <code>read</code> block, it automatically picks up the DataFrame from its first dependency.</li> </ul> </li> </ul>"},{"location":"reference/yaml_schema/#smart-read-incremental-loading","title":"\ud83e\udde0 Smart Read &amp; Incremental Loading","text":"<p>Automated History Management.</p> <p>Odibi intelligently determines whether to perform a Full Load or an Incremental Load based on the state of the target.</p> <p>The \"Smart Read\" Logic: 1.  First Run (Full Load): If the target table (defined in <code>write</code>) does not exist:     *   Incremental filtering rules are ignored.     *   The entire source dataset is read.     *   Use <code>write.first_run_query</code> (optional) to override the read query for this initial bootstrap (e.g., to backfill only 1 year of history instead of all time).</p> <ol> <li>Subsequent Runs (Incremental Load): If the target table exists:<ul> <li>Rolling Window: Filters source data where <code>column &gt;= NOW() - lookback</code>.</li> <li>Stateful: Filters source data where <code>column &gt; last_high_water_mark</code>.</li> </ul> </li> </ol> <p>This ensures you don't need separate \"init\" and \"update\" pipelines. One config handles both lifecycle states.</p>"},{"location":"reference/yaml_schema/#orchestration-tags","title":"\ud83c\udff7\ufe0f Orchestration Tags","text":"<p>Run What You Need. Tags allow you to execute slices of your pipeline. *   <code>odibi run --tag daily</code> -&gt; Runs all nodes with \"daily\" tag. *   <code>odibi run --tag critical</code> -&gt; Runs high-priority nodes.</p>"},{"location":"reference/yaml_schema/#choosing-your-logic-transformer-vs-transform","title":"\ud83e\udd16 Choosing Your Logic: Transformer vs. Transform","text":"<p>1. The \"Transformer\" (Top-Level) *   What it is: A pre-packaged, heavy-duty operation that defines the entire purpose of the node. *   When to use: When applying a standard Data Engineering pattern (e.g., SCD2, Merge, Deduplicate). *   Analogy: \"Run this App.\" *   Syntax: <code>transformer: \"scd2\"</code> + <code>params: {...}</code></p> <p>2. The \"Transform Steps\" (Process Chain) *   What it is: A sequence of smaller steps (SQL, functions, operations) executed in order. *   When to use: For custom business logic, data cleaning, or feature engineering pipelines. *   Analogy: \"Run this Script.\" *   Syntax: <code>transform: { steps: [...] }</code></p> <p>Note: You can use both! The <code>transformer</code> runs first, then <code>transform</code> steps refine the result.</p>"},{"location":"reference/yaml_schema/#chaining-operations","title":"\ud83d\udd17 Chaining Operations","text":"<p>You can mix and match! The execution order is always: 1.  Read (or Dependency Injection) 2.  Transformer (The \"App\" logic, e.g., Deduplicate) 3.  Transform Steps (The \"Script\" logic, e.g., cleanup) 4.  Validation 5.  Write</p> <p>Constraint: You must define at least one of <code>read</code>, <code>transformer</code>, <code>transform</code>, or <code>write</code>.</p>"},{"location":"reference/yaml_schema/#example-app-vs-script","title":"\u26a1 Example: App vs. Script","text":"<p>Scenario 1: The Full ETL Flow (Chained) Shows explicit Read, Transform Chain, and Write.</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]\n\n  # \"clean_text\" is a registered function from the Transformer Catalog\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Top-Level Transformer) Shows a node that applies a pattern (Deduplicate) to incoming data.</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication (From Transformer Catalog)\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner (Reporting) Shows how tags allow running specific slices (e.g., <code>odibi run --tag daily</code>).</p> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  depends_on: [\"deduped_users\"]\n\n  # Ad-hoc aggregation script\n  transform:\n    steps:\n      - sql: \"SELECT date_trunc('day', updated_at) as day, count(*) as total FROM df GROUP BY 1\"\n\n  write: { connection: \"local_data\", format: \"csv\", path: \"reports/daily_stats.csv\" }\n</code></pre> <p>Scenario 4: The \"Kitchen Sink\" (All Operations) Shows Read -&gt; Transformer -&gt; Transform -&gt; Write execution order.</p> <p>Why this works: 1.  Internal Chaining (<code>df</code>): In every step (Transformer or SQL), <code>df</code> refers to the output of the previous step. 2.  External Access (<code>depends_on</code>): If you added <code>depends_on: [\"other_node\"]</code>, you could also run <code>SELECT * FROM other_node</code> in your SQL steps!</p> <pre><code>- name: \"complex_flow\"\n  # 1. Read -&gt; Creates initial 'df'\n  read: { connection: \"bronze\", format: \"parquet\", path: \"users\" }\n\n  # 2. Transformer (The \"App\": Deduplicate first)\n  # Takes 'df' (from Read), dedups it, returns new 'df'\n  transformer: \"deduplicate\"\n  params: { keys: [\"user_id\"], order_by: \"updated_at DESC\" }\n\n  # 3. Transform Steps (The \"Script\": Filter AFTER deduplication)\n  # SQL sees the deduped data as 'df'\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n  # 4. Write -&gt; Saves the final filtered 'df'\n  write: { connection: \"silver\", format: \"delta\", table: \"active_unique_users\" }\n</code></pre>"},{"location":"reference/yaml_schema/#transformer-catalog","title":"\ud83d\udcda Transformer Catalog","text":"<p>These are the built-in functions you can use in two ways:</p> <ol> <li>As a Top-Level Transformer: <code>transformer: \"name\"</code> (Defines the node's main logic)</li> <li>As a Step in a Chain: <code>transform: { steps: [{ function: \"name\" }] }</code> (Part of a sequence)</li> </ol> <p>Note: <code>merge</code> and <code>scd2</code> are special \"Heavy Lifters\" and should generally be used as Top-Level Transformers.</p> <p>Data Engineering Patterns *   <code>merge</code>: Upsert/Merge into target (Delta/SQL). (Params) *   <code>scd2</code>: Slowly Changing Dimensions Type 2. (Params) *   <code>deduplicate</code>: Remove duplicates using window functions. (Params)</p> <p>Relational Algebra *   <code>join</code>: Join two datasets. (Params) *   <code>union</code>: Stack datasets vertically. (Params) *   <code>pivot</code>: Rotate rows to columns. (Params) *   <code>unpivot</code>: Rotate columns to rows (melt). (Params) *   <code>aggregate</code>: Group by and sum/count/avg. (Params)</p> <p>Data Quality &amp; Cleaning *   <code>validate_and_flag</code>: Check rules and flag invalid rows. (Params) *   <code>clean_text</code>: Trim and normalize case. (Params) *   <code>filter_rows</code>: SQL-based filtering. (Params) *   <code>fill_nulls</code>: Replace NULLs with defaults. (Params)</p> <p>Feature Engineering *   <code>derive_columns</code>: Create new cols via SQL expressions. (Params) *   <code>case_when</code>: Conditional logic (if-else). (Params) *   <code>generate_surrogate_key</code>: Create MD5 keys from columns. (Params) *   <code>date_diff</code>, <code>date_add</code>, <code>date_trunc</code>: Date arithmetic.</p> <p>Scenario 1: The Full ETL Flow (Show two nodes: one loader, one processor)</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]  # &lt;--- Explicit dependency\n\n  # Explicit Transformation Steps\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Transformer) (Show a node that is a Transformer, no read needed if it picks up from dependency)</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner Run only this with <code>odibi run --tag daily</code></p> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  # ...\n</code></pre> <p>Scenario 4: Pre/Post SQL Hooks Setup and cleanup with SQL statements.</p> <pre><code>- name: \"optimize_sales\"\n  depends_on: [\"load_sales\"]\n  pre_sql:\n    - \"SET spark.sql.shuffle.partitions = 200\"\n    - \"CREATE TEMP VIEW staging AS SELECT * FROM bronze.raw_sales\"\n  transform:\n    steps:\n      - sql: \"SELECT * FROM staging WHERE amount &gt; 0\"\n  post_sql:\n    - \"OPTIMIZE gold.fact_sales ZORDER BY (customer_id)\"\n    - \"VACUUM gold.fact_sales RETAIN 168 HOURS\"\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sales\"\n</code></pre> <p>Scenario 5: Materialization Strategies Choose how output is persisted.</p> <pre><code># Option 1: View (no physical storage, logical model)\n- name: \"vw_active_customers\"\n  materialized: \"view\"  # Creates SQL view instead of table\n  transform:\n    steps:\n      - sql: \"SELECT * FROM customers WHERE status = 'active'\"\n  write:\n    connection: \"gold\"\n    table: \"vw_active_customers\"\n\n# Option 2: Incremental (append to existing Delta table)\n- name: \"fact_events\"\n  materialized: \"incremental\"  # Uses APPEND mode\n  read:\n    connection: \"bronze\"\n    table: \"raw_events\"\n    incremental:\n      mode: \"stateful\"\n      column: \"event_time\"\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"fact_events\"\n\n# Option 3: Table (default - full overwrite)\n- name: \"dim_products\"\n  materialized: \"table\"  # Default behavior\n  # ...\n</code></pre> Field Type Required Default Description name str Yes - Unique node name description Optional[str] No - Human-readable description enabled bool No <code>True</code> If False, node is skipped during execution tags List[str] No <code>PydanticUndefined</code> Operational tags for selective execution (e.g., 'daily', 'critical'). Use with <code>odibi run --tag</code>. depends_on List[str] No <code>PydanticUndefined</code> List of parent nodes that must complete before this node runs. The output of these nodes is available for reading. columns Dict[str, ColumnMetadata] No <code>PydanticUndefined</code> Data Dictionary defining the output schema. Used for documentation, PII tagging, and validation. read Optional[ReadConfig] No - Input operation (Load). If missing, data is taken from the first dependency. inputs Optional[Dict[str, str | Dict[str, Any]]] No - Multi-input support for cross-pipeline dependencies. Map input names to either: (a) $pipeline.node reference (e.g., '$read_bronze.shift_events') (b) Explicit read config dict. Cannot be used with 'read'. Example: inputs: {events: '$read_bronze.events', calendar: {connection: 'goat', path: 'cal'}} transform Optional[TransformConfig] No - Chain of fine-grained transformation steps (SQL, functions). Runs after 'transformer' if both are present. write Optional[WriteConfig] No - Output operation (Save to file/table). streaming bool No <code>False</code> Enable streaming execution for this node (Spark only) transformer Optional[str] No - Name of the 'App' logic to run (e.g., 'deduplicate', 'scd2'). See Transformer Catalog for options. params Dict[str, Any] No <code>PydanticUndefined</code> Parameters for transformer pre_sql List[str] No <code>PydanticUndefined</code> List of SQL statements to execute before node runs. Use for setup: temp tables, variable initialization, grants. Example: ['SET spark.sql.shuffle.partitions=200', 'CREATE TEMP VIEW src AS SELECT * FROM raw'] post_sql List[str] No <code>PydanticUndefined</code> List of SQL statements to execute after node completes. Use for cleanup, optimization, or audit logging. Example: ['OPTIMIZE gold.fact_sales', 'VACUUM gold.fact_sales RETAIN 168 HOURS'] materialized Optional[Literal['table', 'view', 'incremental']] No - Materialization strategy. Options: 'table' (default physical write), 'view' (creates SQL view instead of table), 'incremental' (uses append mode for Delta tables). Views are useful for Gold layer logical models. cache bool No <code>False</code> Cache result for reuse log_level Optional[LogLevel] No - Override log level for this node on_error ErrorStrategy No <code>ErrorStrategy.FAIL_LATER</code> Failure handling strategy validation Optional[ValidationConfig] No - - contracts List[TestConfig] No <code>PydanticUndefined</code> Pre-condition contracts (Circuit Breakers). Runs on input data before transformation.Options: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract schema_policy Optional[SchemaPolicyConfig] No - Schema drift handling policy privacy Optional[PrivacyConfig] No - Privacy Suite: PII anonymization settings sensitive bool | List[str] No <code>False</code> If true or list of columns, masks sample data in stories source_yaml Optional[str] No - Internal: source YAML file path for sql_file resolution"},{"location":"reference/yaml_schema/#columnmetadata","title":"<code>ColumnMetadata</code>","text":"<p>Used in: NodeConfig</p> <p>Metadata for a column in the data dictionary. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | description | Optional[str] | No | - | Column description | | pii | bool | No | <code>False</code> | Contains PII? | | tags | List[str] | No | <code>PydanticUndefined</code> | Tags (e.g. 'business_key', 'measure') |</p>"},{"location":"reference/yaml_schema/#systemconfig","title":"<code>SystemConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for the Odibi System Catalog (The Brain).</p> <p>Stores metadata, state, and pattern configurations. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection to store system tables (e.g., 'adls_bronze') | | path | str | No | <code>_odibi_system</code> | Path relative to connection root |</p>"},{"location":"reference/yaml_schema/#connections","title":"Connections","text":""},{"location":"reference/yaml_schema/#localconnectionconfig","title":"<code>LocalConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Local filesystem connection.</p> <p>When to Use: Development, testing, small datasets, local processing.</p> <p>See Also: AzureBlobConnectionConfig for cloud alternatives.</p> <p>Example:</p> <pre><code>local_data:\n  type: \"local\"\n  base_path: \"./data\"\n</code></pre> Field Type Required Default Description type Literal['local'] No <code>ConnectionType.LOCAL</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - base_path str No <code>./data</code> Base directory path"},{"location":"reference/yaml_schema/#deltaconnectionconfig","title":"<code>DeltaConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Delta Lake connection for ACID-compliant data lakes.</p> <p>When to Use: - Production data lakes on Azure/AWS/GCP - Need time travel, ACID transactions, schema evolution - Upsert/merge operations</p> <p>See Also: WriteConfig for Delta write options</p> <p>Scenario 1: Delta via metastore</p> <pre><code>delta_silver:\n  type: \"delta\"\n  catalog: \"spark_catalog\"\n  schema: \"silver_db\"\n</code></pre> <p>Scenario 2: Direct path + Node usage</p> <pre><code>delta_local:\n  type: \"local\"\n  base_path: \"dbfs:/mnt/delta\"\n\n# In pipeline:\n# read:\n#   connection: \"delta_local\"\n#   format: \"delta\"\n#   path: \"bronze/orders\"\n</code></pre> Field Type Required Default Description type Literal['delta'] No <code>ConnectionType.DELTA</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - catalog str Yes - Spark catalog name (e.g. 'spark_catalog') schema_name str Yes - Database/schema name table Optional[str] No - Optional default table name for this connection (used by story/pipeline helpers)"},{"location":"reference/yaml_schema/#azureblobconnectionconfig","title":"<code>AzureBlobConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Azure Blob Storage / ADLS Gen2 connection.</p> <p>When to Use: Azure-based data lakes, landing zones, raw data storage.</p> <p>See Also: DeltaConnectionConfig for Delta-specific options</p> <p>Scenario 1: Prod with Key Vault-managed key</p> <pre><code>adls_bronze:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"key_vault\"\n    key_vault: \"kv-data\"\n    secret: \"adls-account-key\"\n</code></pre> <p>Scenario 2: Local dev with inline account key</p> <pre><code>adls_dev:\n  type: \"azure_blob\"\n  account_name: \"devaccount\"\n  container: \"sandbox\"\n  auth:\n    mode: \"account_key\"\n    account_key: \"${ADLS_ACCOUNT_KEY}\"\n</code></pre> <p>Scenario 3: MSI (no secrets)</p> <pre><code>adls_msi:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"aad_msi\"\n    # optional: client_id for user-assigned identity\n    client_id: \"00000000-0000-0000-0000-000000000000\"\n</code></pre> Field Type Required Default Description type Literal['azure_blob'] No <code>ConnectionType.AZURE_BLOB</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - account_name str Yes - - container str Yes - - auth AzureBlobAuthConfig No <code>PydanticUndefined</code> Options: AzureBlobKeyVaultAuth, AzureBlobAccountKeyAuth, AzureBlobSasAuth, AzureBlobConnectionStringAuth, AzureBlobMsiAuth"},{"location":"reference/yaml_schema/#sqlserverconnectionconfig","title":"<code>SQLServerConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>SQL Server / Azure SQL Database connection.</p> <p>When to Use: Reading from SQL Server sources, Azure SQL DB, Azure Synapse.</p> <p>See Also: ReadConfig for query options</p> <p>Scenario 1: Managed identity (AAD MSI)</p> <pre><code>sql_dw_msi:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"aad_msi\"\n</code></pre> <p>Scenario 2: SQL login</p> <pre><code>sql_dw_login:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"sql_login\"\n    username: \"dw_writer\"\n    password: \"${DW_PASSWORD}\"\n</code></pre> Field Type Required Default Description type Literal['sql_server'] No <code>ConnectionType.SQL_SERVER</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - host str Yes - - database str Yes - - port int No <code>1433</code> - auth SQLServerAuthConfig No <code>PydanticUndefined</code> Options: SQLLoginAuth, SQLAadPasswordAuth, SQLMsiAuth, SQLConnectionStringAuth"},{"location":"reference/yaml_schema/#httpconnectionconfig","title":"<code>HttpConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>HTTP connection.</p> <p>Scenario: Bearer token via env var</p> <pre><code>api_source:\n  type: \"http\"\n  base_url: \"https://api.example.com\"\n  headers:\n    User-Agent: \"odibi-pipeline\"\n  auth:\n    mode: \"bearer\"\n    token: \"${API_TOKEN}\"\n</code></pre> Field Type Required Default Description type Literal['http'] No <code>ConnectionType.HTTP</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - base_url str Yes - - headers Dict[str, str] No <code>PydanticUndefined</code> - auth HttpAuthConfig No <code>PydanticUndefined</code> Options: HttpNoAuth, HttpBasicAuth, HttpBearerAuth, HttpApiKeyAuth"},{"location":"reference/yaml_schema/#node-operations","title":"Node Operations","text":""},{"location":"reference/yaml_schema/#readconfig","title":"<code>ReadConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for reading data into a node.</p> <p>When to Use: First node in a pipeline, or any node that reads from storage.</p> <p>Key Concepts: - <code>connection</code>: References a named connection from <code>connections:</code> section - <code>format</code>: File format (csv, parquet, delta, json, sql) - <code>incremental</code>: Enable incremental loading (only new data)</p> <p>See Also: - Incremental Loading - HWM-based loading - IncrementalConfig - Incremental loading options</p>"},{"location":"reference/yaml_schema/#universal-reader-guide","title":"\ud83d\udcd6 \"Universal Reader\" Guide","text":"<p>Business Problem: \"I need to read from files, databases, streams, and even travel back in time to see how data looked yesterday.\"</p> <p>Recipe 1: The Time Traveler (Delta/Iceberg) Reproduce a bug by seeing the data exactly as it was.</p> <pre><code>read:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  time_travel:\n    as_of_timestamp: \"2023-10-25T14:00:00Z\"\n</code></pre> <p>Recipe 2: The Streamer Process data in real-time.</p> <pre><code>read:\n  connection: \"event_hub\"\n  format: \"json\"\n  streaming: true\n</code></pre> <p>Recipe 3: The SQL Query Push down filtering to the source database.</p> <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  # Use the query option to filter at source!\n  query: \"SELECT * FROM huge_table WHERE date &gt;= '2024-01-01'\"\n</code></pre> <p>Recipe 4: Archive Bad Records (Spark) Capture malformed records for later inspection.</p> <pre><code>read:\n  connection: \"landing\"\n  format: \"json\"\n  path: \"events/*.json\"\n  archive_options:\n    badRecordsPath: \"/mnt/quarantine/bad_records\"\n</code></pre> <p>Recipe 5: Optimize JDBC Parallelism (Spark) Control partition count for SQL sources to reduce task overhead.</p> <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  table: \"small_lookup_table\"\n  options:\n    numPartitions: 1  # Single partition for small tables\n</code></pre> <p>Performance Tip: For small tables (&lt;100K rows), use <code>numPartitions: 1</code> to avoid excessive Spark task scheduling overhead. For large tables, increase partitions to enable parallel reads (requires partitionColumn, lowerBound, upperBound). | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name from project.yaml | | format | ReadFormat | str | Yes | - | Data format (csv, parquet, delta, etc.) | | table | Optional[str] | No | - | Table name for SQL/Delta | | path | Optional[str] | No | - | Path for file-based sources | | streaming | bool | No | <code>False</code> | Enable streaming read (Spark only) | | schema_ddl | Optional[str] | No | - | Schema for streaming reads from file sources (required for Avro, JSON, CSV). Use Spark DDL format: 'col1 STRING, col2 INT, col3 TIMESTAMP'. Not required for Delta (schema is inferred from table metadata). | | query | Optional[str] | No | - | SQL query to filter at source (pushdown). Mutually exclusive with table/path if supported by connector. | | filter | Optional[str] | No | - | SQL WHERE clause filter (pushed down to source for SQL formats). Example: \"DAY &gt; '2022-12-31'\" | | incremental | Optional[IncrementalConfig] | No | - | Automatic incremental loading strategy (CDC-like). If set, generates query based on target state (HWM). | | time_travel | Optional[TimeTravelConfig] | No | - | Time travel options (Delta only) | | archive_options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Options for archiving bad records (e.g. badRecordsPath for Spark) | | options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Format-specific options |</p>"},{"location":"reference/yaml_schema/#incrementalconfig","title":"<code>IncrementalConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for automatic incremental loading.</p> <p>When to Use: Load only new/changed data instead of full table scans.</p> <p>See Also: ReadConfig</p> <p>Modes: 1. Rolling Window (Default): Uses a time-based lookback from NOW().    Good for: Stateless loading where you just want \"recent\" data.    Args: <code>lookback</code>, <code>unit</code></p> <ol> <li>Stateful: Tracks the High-Water Mark (HWM) of the key column.    Good for: Exact incremental ingestion (e.g. CDC-like).    Args: <code>state_key</code> (optional), <code>watermark_lag</code> (optional)</li> </ol> <p>Generates SQL: - Rolling: <code>WHERE column &gt;= NOW() - lookback</code> - Stateful: <code>WHERE column &gt; :last_hwm</code></p> <p>Example (Rolling Window):</p> <pre><code>incremental:\n  mode: \"rolling_window\"\n  column: \"updated_at\"\n  lookback: 3\n  unit: \"day\"\n</code></pre> <p>Example (Stateful HWM):</p> <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"id\"\n  # Optional: track separate column for HWM state\n  state_key: \"last_processed_id\"\n</code></pre> <p>Example (Stateful with Watermark Lag):</p> <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"updated_at\"\n  # Handle late-arriving data: look back 2 hours from HWM\n  watermark_lag: \"2h\"\n</code></pre> Field Type Required Default Description mode IncrementalMode No <code>IncrementalMode.ROLLING_WINDOW</code> Incremental strategy: 'rolling_window' or 'stateful' column str Yes - Primary column to filter on (e.g., updated_at) fallback_column Optional[str] No - Backup column if primary is NULL (e.g., created_at). Generates COALESCE(col, fallback) &gt;= ... lookback Optional[int] No - Time units to look back (Rolling Window only) unit Optional[IncrementalUnit] No - Time unit for lookback (Rolling Window only). Options: 'hour', 'day', 'month', 'year' state_key Optional[str] No - Unique ID for state tracking. Defaults to node name if not provided. watermark_lag Optional[str] No - Safety buffer for late-arriving data in stateful mode. Subtracts this duration from the stored HWM when filtering. Format: '' where unit is 's', 'm', 'h', or 'd'. Examples: '2h' (2 hours), '30m' (30 minutes), '1d' (1 day). Use when source has replication lag or eventual consistency."},{"location":"reference/yaml_schema/#timetravelconfig","title":"<code>TimeTravelConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for time travel reading (Delta/Iceberg).</p> <p>Example:</p> <pre><code>time_travel:\n  as_of_version: 10\n  # OR\n  as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre> Field Type Required Default Description as_of_version Optional[int] No - Version number to time travel to as_of_timestamp Optional[str] No - Timestamp string to time travel to"},{"location":"reference/yaml_schema/#transformconfig","title":"<code>TransformConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for transformation steps within a node.</p> <p>When to Use: Custom business logic, data cleaning, SQL transformations.</p> <p>Key Concepts: - <code>steps</code>: Ordered list of operations (SQL, functions, or both) - Each step receives the DataFrame from the previous step - Steps execute in order: step1 \u2192 step2 \u2192 step3</p> <p>See Also: Transformer Catalog</p> <p>Transformer vs Transform: - <code>transformer</code>: Single heavy operation (scd2, merge, deduplicate) - <code>transform.steps</code>: Chain of lighter operations</p>"},{"location":"reference/yaml_schema/#transformation-pipeline-guide","title":"\ud83d\udd27 \"Transformation Pipeline\" Guide","text":"<p>Business Problem: \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"</p> <p>The Solution: Chain multiple steps together. Output of Step 1 becomes input of Step 2.</p> <p>Function Registry: The <code>function</code> step type looks up functions registered with <code>@transform</code> (or <code>@register</code>). This allows you to use the same registered functions as both top-level Transformers and steps in a chain.</p> <p>Recipe: The Mix-and-Match</p> <pre><code>transform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n</code></pre> Field Type Required Default Description steps List[str | TransformStep] Yes - List of transformation steps (SQL strings or TransformStep configs)"},{"location":"reference/yaml_schema/#deletedetectionconfig","title":"<code>DeleteDetectionConfig</code>","text":"<p>Configuration for delete detection in Silver layer.</p>"},{"location":"reference/yaml_schema/#cdc-without-cdc-guide","title":"\ud83d\udd0d \"CDC Without CDC\" Guide","text":"<p>Business Problem: \"Records are deleted in our Azure SQL source, but our Silver tables still show them.\"</p> <p>The Solution: Use delete detection to identify and flag records that no longer exist in the source.</p> <p>Recipe 1: SQL Compare (Recommended for HWM)</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n</code></pre> <p>Recipe 2: Snapshot Diff (For Full Snapshot Sources) Use ONLY with full snapshot ingestion, NOT with HWM incremental. Requires <code>connection</code> and <code>path</code> to specify the target Delta table for comparison.</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [customer_id]\n        connection: silver_conn    # Required: connection to target Delta table\n        path: \"silver/customers\"   # Required: path to target Delta table\n</code></pre> <p>Recipe 3: Conservative Threshold</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: erp\n        source_table: dbo.Customers\n        max_delete_percent: 20.0\n        on_threshold_breach: error\n</code></pre> <p>Recipe 4: Hard Delete (Remove Rows)</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n        soft_delete_col: null  # removes rows instead of flagging\n</code></pre> Field Type Required Default Description mode DeleteDetectionMode No <code>DeleteDetectionMode.NONE</code> Delete detection strategy: none, snapshot_diff, sql_compare keys List[str] No <code>PydanticUndefined</code> Business key columns for comparison connection Optional[str] No - For snapshot_diff: connection name to target Delta table (required for snapshot_diff) path Optional[str] No - For snapshot_diff: path to target Delta table (required for snapshot_diff) soft_delete_col Optional[str] No <code>_is_deleted</code> Column to flag deletes (True = deleted). Set to null for hard-delete (removes rows). source_connection Optional[str] No - For sql_compare: connection name to query live source source_table Optional[str] No - For sql_compare: table to query for current keys source_query Optional[str] No - For sql_compare: custom SQL query for keys (overrides source_table) snapshot_column Optional[str] No - For snapshot_diff on non-Delta: column to identify snapshots. If None, uses Delta time travel (default). on_first_run FirstRunBehavior No <code>FirstRunBehavior.SKIP</code> Behavior when no previous version exists for snapshot_diff max_delete_percent Optional[float] No <code>50.0</code> Safety threshold: warn/error if more than X% of rows would be deleted on_threshold_breach ThresholdBreachAction No <code>ThresholdBreachAction.WARN</code> Behavior when delete percentage exceeds max_delete_percent"},{"location":"reference/yaml_schema/#validationconfig","title":"<code>ValidationConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for data validation (post-transform checks).</p> <p>When to Use: Output data quality checks that run after transformation but before writing.</p> <p>See Also: Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)</p>"},{"location":"reference/yaml_schema/#the-indestructible-pipeline-pattern","title":"\ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern","text":"<p>Business Problem: \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it before it lands.\"</p> <p>The Solution: A Quality Gate that runs after transformation but before writing.</p> <p>Recipe: The Quality Gate</p> <pre><code>validation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges &amp; Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at &lt;= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n</code></pre> <p>Recipe: Quarantine + Gate</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n</code></pre> Field Type Required Default Description mode ValidationAction No <code>ValidationAction.FAIL</code> Execution mode: 'fail' (stop pipeline) or 'warn' (log only) on_fail OnFailAction No <code>OnFailAction.ALERT</code> Action on failure: 'alert' (send notification) or 'ignore' tests List[TestConfig] No <code>PydanticUndefined</code> List of validation testsOptions: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract quarantine Optional[QuarantineConfig] No - Quarantine configuration for failed rows gate Optional[GateConfig] No - Quality gate configuration for batch-level validation fail_fast bool No <code>False</code> Stop validation on first failure. Skips remaining tests for faster feedback. cache_df bool No <code>False</code> Cache DataFrame before validation (Spark only). Improves performance with many tests."},{"location":"reference/yaml_schema/#quarantineconfig","title":"<code>QuarantineConfig</code>","text":"<p>Used in: ValidationConfig</p> <p>Configuration for quarantine table routing.</p> <p>When to Use: Capture invalid records for review/reprocessing instead of failing the pipeline.</p> <p>See Also: Quarantine Guide, ValidationConfig</p> <p>Routes rows that fail validation tests to a quarantine table with rejection metadata for later analysis/reprocessing.</p> <p>Example:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n    max_rows: 10000\n    sample_fraction: 0.1\n</code></pre> Field Type Required Default Description connection str Yes - Connection for quarantine writes path Optional[str] No - Path for quarantine data table Optional[str] No - Table name for quarantine add_columns QuarantineColumnsConfig No <code>PydanticUndefined</code> Metadata columns to add to quarantined rows retention_days Optional[int] No <code>90</code> Days to retain quarantined data (auto-cleanup) max_rows Optional[int] No - Maximum number of rows to quarantine per run. Limits storage for high-failure batches. sample_fraction Optional[float] No - Sample fraction of invalid rows to quarantine (0.0-1.0). Use for sampling large invalid sets."},{"location":"reference/yaml_schema/#quarantinecolumnsconfig","title":"<code>QuarantineColumnsConfig</code>","text":"<p>Used in: QuarantineConfig</p> <p>Columns added to quarantined rows for debugging and reprocessing.</p> <p>Example:</p> <pre><code>quarantine:\n  connection: silver\n  path: customers_quarantine\n  add_columns:\n    _rejection_reason: true\n    _rejected_at: true\n    _source_batch_id: true\n    _failed_tests: true\n    _original_node: false\n</code></pre> Field Type Required Default Description rejection_reason bool No <code>True</code> Add _rejection_reason column with test failure description rejected_at bool No <code>True</code> Add _rejected_at column with UTC timestamp source_batch_id bool No <code>True</code> Add _source_batch_id column with run ID for traceability failed_tests bool No <code>True</code> Add _failed_tests column with comma-separated list of failed test names original_node bool No <code>False</code> Add _original_node column with source node name"},{"location":"reference/yaml_schema/#gateconfig","title":"<code>GateConfig</code>","text":"<p>Used in: EnvironmentConfig, ValidationConfig</p> <p>Gate requirements for promoting changes to Master.</p> <p>All gates must pass before changes can be promoted. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | require_ruff_clean | bool | No | <code>True</code> | Require ruff linting to pass with no errors | | require_pytest_pass | bool | No | <code>True</code> | Require all pytest tests to pass | | require_odibi_validate | bool | No | <code>True</code> | Require odibi validate to pass on modified configs | | require_golden_projects | bool | No | <code>True</code> | Require all learning harness configs to pass |</p>"},{"location":"reference/yaml_schema/#gateconfig_1","title":"<code>GateConfig</code>","text":"<p>Used in: EnvironmentConfig, ValidationConfig</p> <p>Quality gate configuration for batch-level validation.</p> <p>When to Use: Pipeline-level pass/fail thresholds, row count limits, change detection.</p> <p>See Also: Quality Gates, ValidationConfig</p> <p>Gates evaluate the entire batch before writing, ensuring data quality thresholds are met.</p> <p>Example:</p> <pre><code>gate:\n  require_pass_rate: 0.95\n  on_fail: abort\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n  row_count:\n    min: 100\n    change_threshold: 0.5\n</code></pre> Field Type Required Default Description require_pass_rate float No <code>0.95</code> Minimum percentage of rows passing ALL tests on_fail GateOnFail No <code>GateOnFail.ABORT</code> Action when gate fails thresholds List[GateThreshold] No <code>PydanticUndefined</code> Per-test thresholds (overrides global require_pass_rate) row_count Optional[RowCountGate] No - Row count anomaly detection"},{"location":"reference/yaml_schema/#gatethreshold","title":"<code>GateThreshold</code>","text":"<p>Used in: GateConfig</p> <p>Per-test threshold configuration for quality gates.</p> <p>Allows setting different pass rate requirements for specific tests.</p> <p>Example:</p> <pre><code>gate:\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n    - test: unique\n      min_pass_rate: 1.0\n</code></pre> Field Type Required Default Description test str Yes - Test name or type to apply threshold to min_pass_rate float Yes - Minimum pass rate required (0.0-1.0, e.g., 0.99 = 99%)"},{"location":"reference/yaml_schema/#rowcountgate","title":"<code>RowCountGate</code>","text":"<p>Used in: GateConfig</p> <p>Row count anomaly detection for quality gates.</p> <p>Validates that batch size falls within expected bounds and detects significant changes from previous runs.</p> <p>Example:</p> <pre><code>gate:\n  row_count:\n    min: 100\n    max: 1000000\n    change_threshold: 0.5\n</code></pre> Field Type Required Default Description min Optional[int] No - Minimum expected row count max Optional[int] No - Maximum expected row count change_threshold Optional[float] No - Max allowed change vs previous run (e.g., 0.5 = 50% change triggers failure)"},{"location":"reference/yaml_schema/#writeconfig","title":"<code>WriteConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for writing data from a node.</p> <p>When to Use: Any node that persists data to storage.</p> <p>Key Concepts: - <code>mode</code>: How to handle existing data (overwrite, append, upsert) - <code>keys</code>: Required for upsert mode - columns that identify unique records - <code>partition_by</code>: Columns to partition output by (improves query performance)</p> <p>See Also: - Performance Tuning - Partitioning strategies</p>"},{"location":"reference/yaml_schema/#big-data-performance-guide","title":"\ud83d\ude80 \"Big Data Performance\" Guide","text":"<p>Business Problem: \"My dashboards are slow because the query scans terabytes of data just to find one day's sales.\"</p> <p>The Solution: Use Partitioning for coarse filtering (skipping huge chunks) and Z-Ordering for fine-grained skipping (colocating related data).</p> <p>Recipe: Lakehouse Optimized</p> <pre><code>write:\n  connection: \"gold_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  mode: \"append\"\n\n  # 1. Partitioning: Physical folders.\n  # Use for low-cardinality columns often used in WHERE clauses.\n  # WARNING: Do NOT partition by high-cardinality cols like ID or Timestamp!\n  partition_by: [\"country_code\", \"txn_year_month\"]\n\n  # 2. Z-Ordering: Data clustering.\n  # Use for high-cardinality columns often used in JOINs or predicates.\n  zorder_by: [\"customer_id\", \"product_id\"]\n\n  # 3. Table Properties: Engine tuning.\n  table_properties:\n    \"delta.autoOptimize.optimizeWrite\": \"true\"\n    \"delta.autoOptimize.autoCompact\": \"true\"\n</code></pre> Field Type Required Default Description connection str Yes - Connection name from project.yaml format ReadFormat | str Yes - Output format (csv, parquet, delta, etc.) table Optional[str] No - Table name for SQL/Delta path Optional[str] No - Path for file-based outputs register_table Optional[str] No - Register file output as external table (Spark/Delta only) mode WriteMode No <code>WriteMode.OVERWRITE</code> Write mode. Options: 'overwrite', 'append', 'upsert', 'append_once' partition_by List[str] No <code>PydanticUndefined</code> List of columns to physically partition the output by (folder structure). Use for low-cardinality columns (e.g. date, country). zorder_by List[str] No <code>PydanticUndefined</code> List of columns to Z-Order by. Improves read performance for high-cardinality columns used in filters/joins (Delta only). table_properties Dict[str, str] No <code>PydanticUndefined</code> Delta table properties. Overrides global performance.delta_table_properties. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. merge_schema bool No <code>False</code> Allow schema evolution (mergeSchema option in Delta) first_run_query Optional[str] No - SQL query for full-load on first run (High Water Mark pattern). If set, uses this query when target table doesn't exist, then switches to incremental. Only applies to SQL reads. options Dict[str, Any] No <code>PydanticUndefined</code> Format-specific options auto_optimize bool | AutoOptimizeConfig No - Auto-run OPTIMIZE and VACUUM after write (Delta only) add_metadata bool | WriteMetadataConfig No - Add metadata columns for Bronze layer lineage. Set to <code>true</code> to add all applicable columns, or provide a WriteMetadataConfig for selective columns. Columns: _extracted_at, _source_file (file sources), _source_connection, _source_table (SQL sources). skip_if_unchanged bool No <code>False</code> Skip write if DataFrame content is identical to previous write. Computes SHA256 hash of entire DataFrame and compares to stored hash in Delta table metadata. Useful for snapshot tables without timestamps to avoid redundant appends. Only supported for Delta format. skip_hash_columns Optional[List[str]] No - Columns to include in hash computation for skip_if_unchanged. If None, all columns are used. Specify a subset to ignore volatile columns like timestamps. skip_hash_sort_columns Optional[List[str]] No - Columns to sort by before hashing for deterministic comparison. Required if row order may vary between runs. Typically your business key columns. streaming Optional[StreamingWriteConfig] No - Streaming write configuration for Spark Structured Streaming. When set, uses writeStream instead of batch write. Requires a streaming DataFrame from a streaming read source."},{"location":"reference/yaml_schema/#writemetadataconfig","title":"<code>WriteMetadataConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for metadata columns added during Bronze writes.</p>"},{"location":"reference/yaml_schema/#bronze-metadata-guide","title":"\ud83d\udccb Bronze Metadata Guide","text":"<p>Business Problem: \"We need lineage tracking and debugging info for our Bronze layer data.\"</p> <p>The Solution: Add metadata columns during ingestion for traceability.</p> <p>Recipe 1: Add All Metadata (Recommended)</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n</code></pre> <p>Recipe 2: Selective Metadata</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: false\n    source_table: false\n</code></pre> <p>Available Columns: - <code>_extracted_at</code>: Pipeline execution timestamp (all sources) - <code>_source_file</code>: Source filename/path (file sources only) - <code>_source_connection</code>: Connection name used (all sources) - <code>_source_table</code>: Table or query name (SQL sources only) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | extracted_at | bool | No | <code>True</code> | Add _extracted_at column with pipeline execution timestamp | | source_file | bool | No | <code>True</code> | Add _source_file column with source filename (file sources only) | | source_connection | bool | No | <code>False</code> | Add _source_connection column with connection name | | source_table | bool | No | <code>False</code> | Add _source_table column with table/query name (SQL sources only) |</p>"},{"location":"reference/yaml_schema/#streamingwriteconfig","title":"<code>StreamingWriteConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Spark Structured Streaming writes.</p>"},{"location":"reference/yaml_schema/#real-time-pipeline-guide","title":"\ud83d\ude80 \"Real-Time Pipeline\" Guide","text":"<p>Business Problem: \"I need to process data continuously as it arrives from Kafka/Event Hubs and write it to Delta Lake in near real-time.\"</p> <p>The Solution: Configure streaming write with checkpoint location for fault tolerance and trigger interval for processing frequency.</p> <p>Recipe: Streaming Ingestion</p> <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_stream\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_stream\"\n    trigger:\n      processing_time: \"10 seconds\"\n</code></pre> <p>Recipe: One-Time Streaming (Batch-like)</p> <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_batch\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_batch\"\n    trigger:\n      available_now: true\n</code></pre> Field Type Required Default Description output_mode Literal['append', 'update', 'complete'] No <code>append</code> Output mode for streaming writes. 'append' - Only new rows. 'update' - Updated rows only. 'complete' - Entire result table (requires aggregation). checkpoint_location str Yes - Path for streaming checkpoints. Required for fault tolerance. Must be a reliable storage location (e.g., cloud storage, DBFS). trigger Optional[TriggerConfig] No - Trigger configuration. If not specified, processes data as fast as possible. Use 'processing_time' for micro-batch intervals, 'once' for single batch, 'available_now' for processing all available data then stopping. query_name Optional[str] No - Name for the streaming query (useful for monitoring and debugging) await_termination Optional[bool] No <code>False</code> Wait for the streaming query to terminate. Set to True for batch-like streaming with 'once' or 'available_now' triggers. timeout_seconds Optional[int] No - Timeout in seconds when await_termination is True. If None, waits indefinitely."},{"location":"reference/yaml_schema/#triggerconfig","title":"<code>TriggerConfig</code>","text":"<p>Used in: StreamingWriteConfig</p> <p>Configuration for streaming trigger intervals.</p> <p>Specify exactly one of the trigger options.</p> <p>Example:</p> <pre><code>trigger:\n  processing_time: \"10 seconds\"\n</code></pre> <p>Or for one-time processing:</p> <pre><code>trigger:\n  once: true\n</code></pre> Field Type Required Default Description processing_time Optional[str] No - Trigger interval as duration string (e.g., '10 seconds', '1 minute') once Optional[bool] No - Process all available data once and stop available_now Optional[bool] No - Process all available data in multiple batches, then stop continuous Optional[str] No - Continuous processing with checkpoint interval (e.g., '1 second')"},{"location":"reference/yaml_schema/#autooptimizeconfig","title":"<code>AutoOptimizeConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Delta Lake automatic optimization.</p> <p>Example:</p> <pre><code>auto_optimize:\n  enabled: true\n  vacuum_retention_hours: 168\n</code></pre> Field Type Required Default Description enabled bool No <code>True</code> Enable auto optimization vacuum_retention_hours int No <code>168</code> Hours to retain history for VACUUM (default 7 days). Set to 0 to disable VACUUM."},{"location":"reference/yaml_schema/#privacyconfig","title":"<code>PrivacyConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for PII anonymization.</p>"},{"location":"reference/yaml_schema/#privacy-pii-protection","title":"\ud83d\udd10 Privacy &amp; PII Protection","text":"<p>How It Works: 1. Mark columns as <code>pii: true</code> in the <code>columns</code> metadata 2. Configure a <code>privacy</code> block with the anonymization method 3. During node execution, all columns marked as PII (and inherited from dependencies) are anonymized 4. Upstream PII markings are inherited by downstream nodes</p> <p>Example:</p> <pre><code>columns:\n  customer_email:\n    pii: true  # Mark as PII\n  customer_id:\n    pii: false\n\nprivacy:\n  method: hash       # hash, mask, or redact\n  salt: \"secret_key\" # Optional: makes hash unique/secure\n  declassify: []     # Remove columns from PII protection\n</code></pre> <p>Methods: - <code>hash</code>: SHA256 hash (length 64). With salt, prevents pre-computed rainbow tables. - <code>mask</code>: Show only last 4 chars, replace rest with <code>*</code>. Example: <code>john@email.com</code> \u2192 <code>****@email.com</code> - <code>redact</code>: Replace entire value with <code>[REDACTED]</code></p> <p>Important: - <code>pii: true</code> alone does NOTHING. You must set a <code>privacy.method</code> to actually mask data. - PII inheritance: If dependency outputs PII columns, this node inherits them unless declassified. - Salt is optional but recommended for hash to prevent attacks. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | method | PrivacyMethod | Yes | - | Anonymization method: 'hash' (SHA256), 'mask' (show last 4), or 'redact' ([REDACTED]) | | salt | Optional[str] | No | - | Salt for hashing (optional but recommended). Appended before hashing to create unique hashes. Example: 'company_secret_key_2025' | | declassify | List[str] | No | <code>PydanticUndefined</code> | List of columns to remove from PII protection (stops inheritance from upstream). Example: ['customer_id'] |</p>"},{"location":"reference/yaml_schema/#transformstep","title":"<code>TransformStep</code>","text":"<p>Used in: TransformConfig</p> <p>Single transformation step.</p> <p>Supports four step types (exactly one required):</p> <ul> <li><code>sql</code> - Inline SQL query string</li> <li><code>sql_file</code> - Path to external .sql file (relative to the YAML file defining the node)</li> <li><code>function</code> - Registered Python function name</li> <li><code>operation</code> - Built-in operation (e.g., drop_duplicates)</li> </ul> <p>sql_file Example:</p> <p>If your project structure is:</p> <pre><code>project.yaml              # imports pipelines/silver/silver.yaml\npipelines/\n  silver/\n    silver.yaml           # defines the node\n    sql/\n      transform.sql       # your SQL file\n</code></pre> <p>In <code>silver.yaml</code>, use a path relative to <code>silver.yaml</code>:</p> <pre><code>transform:\n  steps:\n    - sql_file: sql/transform.sql   # relative to silver.yaml\n</code></pre> <p>Important: The path is resolved relative to the YAML file where the node is defined, NOT the project.yaml that imports it. Do NOT use absolute paths like <code>/pipelines/silver/sql/...</code>. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | sql | Optional[str] | No | - | Inline SQL query. Use <code>df</code> to reference the current DataFrame. | | sql_file | Optional[str] | No | - | Path to external .sql file, relative to the YAML file defining the node. Example: 'sql/transform.sql' resolves relative to the node's source YAML. | | function | Optional[str] | No | - | Name of a registered Python function (@transform or @register). | | operation | Optional[str] | No | - | Built-in operation name (e.g., drop_duplicates, fill_na). | | params | Dict[str, Any] | No | <code>PydanticUndefined</code> | Parameters to pass to function or operation. |</p>"},{"location":"reference/yaml_schema/#contracts-data-quality-gates","title":"Contracts (Data Quality Gates)","text":""},{"location":"reference/yaml_schema/#contracts-pre-transform-checks","title":"Contracts (Pre-Transform Checks)","text":"<p>Contracts are fail-fast data quality checks that run on input data before transformation. They always halt execution on failure - use them to prevent bad data from entering the pipeline.</p> <p>Contracts vs Validation vs Quality Gates:</p> Feature When it Runs On Failure Use Case Contracts Before transform Always fails Input data quality (not-null, unique keys) Validation After transform Configurable (fail/warn/quarantine) Output data quality (ranges, formats) Quality Gates After validation Configurable (abort/warn) Pipeline-level thresholds (pass rate, row counts) Quarantine With validation Routes bad rows Capture invalid records for review <p>See Also: - Validation Guide - Full validation configuration - Quarantine Guide - Quarantine setup and review - Getting Started: Validation</p> <p>Example:</p> <pre><code>- name: \"process_orders\"\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read:\n    source: raw_orders\n</code></pre>"},{"location":"reference/yaml_schema/#acceptedvaluestest","title":"<code>AcceptedValuesTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures a column only contains values from an allowed list.</p> <p>When to Use: Enum-like fields, status columns, categorical data validation.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected]\n</code></pre> Field Type Required Default Description type Literal['accepted_values'] No <code>TestType.ACCEPTED_VALUES</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check values List[Any] Yes - Allowed values"},{"location":"reference/yaml_schema/#customsqltest","title":"<code>CustomSQLTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Runs a custom SQL condition and fails if too many rows violate it.</p> <pre><code>contracts:\n  - type: custom_sql\n    condition: \"amount &gt; 0\"\n    threshold: 0.01  # Allow up to 1% failures\n</code></pre> Field Type Required Default Description type Literal['custom_sql'] No <code>TestType.CUSTOM_SQL</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure condition str Yes - SQL condition that should be true for valid rows threshold float No <code>0.0</code> Failure rate threshold (0.0 = strictly no failures allowed)"},{"location":"reference/yaml_schema/#distributioncontract","title":"<code>DistributionContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if a column's statistical distribution is within expected bounds.</p> <p>When to Use: Detect data drift, anomaly detection, statistical monitoring.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \"&gt;100\"  # Mean must be &gt; 100\n    on_fail: warn\n</code></pre> Field Type Required Default Description type Literal['distribution'] No <code>TestType.DISTRIBUTION</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.WARN</code> - column str Yes - Column to analyze metric Literal['mean', 'min', 'max', 'null_percentage'] Yes - Statistical metric to check threshold str Yes - Threshold expression (e.g., '&gt;100', '&lt;0.05')"},{"location":"reference/yaml_schema/#freshnesscontract","title":"<code>FreshnessContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that data is not stale by checking a timestamp column.</p> <p>When to Use: Source systems that should update regularly, SLA monitoring.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n</code></pre> Field Type Required Default Description type Literal['freshness'] No <code>TestType.FRESHNESS</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> - column str No <code>updated_at</code> Timestamp column to check max_age str Yes - Maximum allowed age (e.g., '24h', '7d')"},{"location":"reference/yaml_schema/#notnulltest","title":"<code>NotNullTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns contain no NULL values.</p> <p>When to Use: Primary keys, required fields, foreign keys that must resolve.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n</code></pre> Field Type Required Default Description type Literal['not_null'] No <code>TestType.NOT_NULL</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure columns List[str] Yes - Columns that must not contain nulls"},{"location":"reference/yaml_schema/#rangetest","title":"<code>RangeTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values fall within a specified range.</p> <p>When to Use: Numeric bounds validation (ages, prices, quantities), date ranges.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n</code></pre> Field Type Required Default Description type Literal['range'] No <code>TestType.RANGE</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check min int | float | str No - Minimum value (inclusive) max int | float | str No - Maximum value (inclusive)"},{"location":"reference/yaml_schema/#regexmatchtest","title":"<code>RegexMatchTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values match a regex pattern.</p> <p>When to Use: Format validation (emails, phone numbers, IDs, codes).</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n</code></pre> Field Type Required Default Description type Literal['regex_match'] No <code>TestType.REGEX_MATCH</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check pattern str Yes - Regex pattern to match"},{"location":"reference/yaml_schema/#rowcounttest","title":"<code>RowCountTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that row count falls within expected bounds.</p> <p>When to Use: Ensure minimum data completeness, detect truncated loads, cap batch sizes.</p> <p>See Also: Contracts Overview, GateConfig</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000\n    max: 100000\n</code></pre> Field Type Required Default Description type Literal['row_count'] No <code>TestType.ROW_COUNT</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure min Optional[int] No - Minimum row count max Optional[int] No - Maximum row count"},{"location":"reference/yaml_schema/#schemacontract","title":"<code>SchemaContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that the DataFrame schema matches expected columns.</p> <p>When to Use: Enforce schema stability, detect upstream schema drift, ensure column presence.</p> <p>See Also: Contracts Overview, SchemaPolicyConfig</p> <p>Uses the <code>columns</code> metadata from NodeConfig to verify schema.</p> <pre><code>contracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n</code></pre> Field Type Required Default Description type Literal['schema'] No <code>TestType.SCHEMA</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> - strict bool No <code>True</code> If true, fail on unexpected columns"},{"location":"reference/yaml_schema/#uniquetest","title":"<code>UniqueTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns (or combination) contain unique values.</p> <p>When to Use: Primary keys, natural keys, deduplication verification.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: unique\n    columns: [order_id]  # Single column\n  # OR composite key:\n  - type: unique\n    columns: [customer_id, order_date]  # Composite uniqueness\n</code></pre> Field Type Required Default Description type Literal['unique'] No <code>TestType.UNIQUE</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure columns List[str] Yes - Columns that must be unique (composite key if multiple)"},{"location":"reference/yaml_schema/#volumedroptest","title":"<code>VolumeDropTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if row count dropped significantly compared to history.</p> <p>When to Use: Detect source outages, partial loads, or data pipeline issues.</p> <p>See Also: Contracts Overview, RowCountTest</p> <p>Formula: <code>(current - avg) / avg &lt; -threshold</code></p> <pre><code>contracts:\n  - type: volume_drop\n    threshold: 0.5  # Fail if &gt; 50% drop from 7-day average\n    lookback_days: 7\n</code></pre> Field Type Required Default Description type Literal['volume_drop'] No <code>TestType.VOLUME_DROP</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure threshold float No <code>0.5</code> Max allowed drop (0.5 = 50% drop) lookback_days int No <code>7</code> Days of history to average"},{"location":"reference/yaml_schema/#global-settings","title":"Global Settings","text":""},{"location":"reference/yaml_schema/#lineageconfig","title":"<code>LineageConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for OpenLineage integration.</p> <p>Example:</p> <pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n</code></pre> Field Type Required Default Description url Optional[str] No - OpenLineage API URL namespace str No <code>odibi</code> Namespace for jobs api_key Optional[str] No - API Key"},{"location":"reference/yaml_schema/#alertconfig","title":"<code>AlertConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for alerts with throttling support.</p> <p>Supports Slack, Teams, and generic webhooks with event-specific payloads.</p> <p>Available Events: - <code>on_start</code> - Pipeline started - <code>on_success</code> - Pipeline completed successfully - <code>on_failure</code> - Pipeline failed - <code>on_quarantine</code> - Rows were quarantined - <code>on_gate_block</code> - Quality gate blocked the pipeline - <code>on_threshold_breach</code> - A threshold was exceeded</p> <p>Example:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre> Field Type Required Default Description type AlertType Yes - - url str Yes - Webhook URL on_events List[AlertEvent] No <code>[&lt;AlertEvent.ON_FAILURE: 'on_failure'&gt;]</code> Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach metadata Dict[str, Any] No <code>PydanticUndefined</code> Extra metadata: throttle_minutes, max_per_hour, channel, etc."},{"location":"reference/yaml_schema/#loggingconfig","title":"<code>LoggingConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Logging configuration.</p> <p>Example:</p> <pre><code>logging:\n  level: \"INFO\"\n  structured: true\n</code></pre> Field Type Required Default Description level LogLevel No <code>LogLevel.INFO</code> - structured bool No <code>False</code> Output JSON logs metadata Dict[str, Any] No <code>PydanticUndefined</code> Extra metadata in logs"},{"location":"reference/yaml_schema/#performanceconfig","title":"<code>PerformanceConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Performance tuning configuration.</p> <p>Example:</p> <pre><code>performance:\n  use_arrow: true\n  spark_config:\n    \"spark.sql.shuffle.partitions\": \"200\"\n    \"spark.sql.adaptive.enabled\": \"true\"\n    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\"\n  delta_table_properties:\n    \"delta.columnMapping.mode\": \"name\"\n</code></pre> <p>Spark Config Notes: - Configs are applied via <code>spark.conf.set()</code> at runtime - For existing sessions (e.g., Databricks), only runtime-settable configs will take effect - Session-level configs (e.g., <code>spark.executor.memory</code>) require session restart - Common runtime-safe configs: shuffle partitions, adaptive query execution, Delta optimizations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | use_arrow | bool | No | <code>True</code> | Use Apache Arrow-backed DataFrames (Pandas only). Reduces memory and speeds up I/O. | | spark_config | Dict[str, str] | No | <code>PydanticUndefined</code> | Spark configuration settings applied at runtime via spark.conf.set(). Example: {'spark.sql.shuffle.partitions': '200', 'spark.sql.adaptive.enabled': 'true'}. Note: Some configs require session restart and cannot be set at runtime. | | delta_table_properties | Dict[str, str] | No | <code>PydanticUndefined</code> | Default table properties applied to all Delta writes. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. | | skip_null_profiling | bool | No | <code>False</code> | Skip null profiling in metadata collection phase. Reduces execution time for large DataFrames by avoiding an additional Spark job. | | skip_catalog_writes | bool | No | <code>False</code> | Skip catalog metadata writes (register_asset, track_schema, log_pattern, record_lineage) after each node write. Significantly improves performance for high-throughput pipelines like Bronze layer ingestion. Set to true when catalog tracking is not needed. | | skip_run_logging | bool | No | <code>False</code> | Skip batch catalog writes at pipeline end (log_runs_batch, register_outputs_batch). Saves 10-20s per pipeline run. Enable when you don't need run history in the catalog. Stories are still generated and contain full execution details. |</p>"},{"location":"reference/yaml_schema/#retryconfig","title":"<code>RetryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Retry configuration.</p> <p>Example:</p> <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n</code></pre> Field Type Required Default Description enabled bool No <code>True</code> - max_attempts int No <code>3</code> - backoff BackoffStrategy No <code>BackoffStrategy.EXPONENTIAL</code> -"},{"location":"reference/yaml_schema/#storyconfig","title":"<code>StoryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Story generation configuration.</p> <p>Stories are ODIBI's core value - execution reports with lineage. They must use a connection for consistent, traceable output.</p> <p>Example:</p> <pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre> <p>Failure Sample Settings: - <code>failure_sample_size</code>: Number of failed rows to capture per validation (default: 100) - <code>max_failure_samples</code>: Total failed rows across all validations (default: 500) - <code>max_sampled_validations</code>: After this many validations, show only counts (default: 5) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name for story output (uses connection's path resolution) | | path | str | Yes | - | Path for stories (relative to connection base_path) | | max_sample_rows | int | No | <code>10</code> | - | | auto_generate | bool | No | <code>True</code> | - | | retention_days | Optional[int] | No | <code>30</code> | Days to keep stories | | retention_count | Optional[int] | No | <code>100</code> | Max number of stories to keep | | failure_sample_size | int | No | <code>100</code> | Number of failed rows to capture per validation rule | | max_failure_samples | int | No | <code>500</code> | Maximum total failed rows across all validations | | max_sampled_validations | int | No | <code>5</code> | After this many validations, show only counts (no samples) | | async_generation | bool | No | <code>False</code> | Generate stories asynchronously (fire-and-forget). Pipeline returns immediately while story writes in background. Improves multi-pipeline performance by ~5-10s per pipeline. |</p>"},{"location":"reference/yaml_schema/#transformation-reference","title":"Transformation Reference","text":""},{"location":"reference/yaml_schema/#how-to-use-transformers","title":"How to Use Transformers","text":"<p>You can use any transformer in two ways:</p> <p>1. As a Top-Level Transformer (\"The App\") Use this for major operations that define the node's purpose (e.g. Merge, SCD2).</p> <pre><code>- name: \"my_node\"\n  transformer: \"&lt;transformer_name&gt;\"\n  params:\n    &lt;param_name&gt;: &lt;value&gt;\n</code></pre> <p>2. As a Step in a Chain (\"The Script\") Use this for smaller operations within a <code>transform</code> block (e.g. clean_text, filter).</p> <pre><code>- name: \"my_node\"\n  transform:\n    steps:\n      - function: \"&lt;transformer_name&gt;\"\n         params:\n           &lt;param_name&gt;: &lt;value&gt;\n</code></pre> <p>Available Transformers: The models below describe the <code>params</code> required for each transformer.</p>"},{"location":"reference/yaml_schema/#common-operations","title":"\ud83d\udcc2 Common Operations","text":""},{"location":"reference/yaml_schema/#casewhencase","title":"CaseWhenCase","text":"<p>Back to Catalog</p> Field Type Required Default Description condition str Yes - - value str Yes - -"},{"location":"reference/yaml_schema/#add_prefix-addprefixparams","title":"<code>add_prefix</code> (AddPrefixParams)","text":"<p>Adds a prefix to column names.</p> <p>Configuration for adding a prefix to column names.</p> <p>Example - All columns:</p> <pre><code>add_prefix:\n  prefix: \"src_\"\n</code></pre> <p>Example - Specific columns:</p> <pre><code>add_prefix:\n  prefix: \"raw_\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description prefix str Yes - Prefix to add to column names columns Optional[List[str]] No - Columns to prefix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from prefixing"},{"location":"reference/yaml_schema/#add_suffix-addsuffixparams","title":"<code>add_suffix</code> (AddSuffixParams)","text":"<p>Adds a suffix to column names.</p> <p>Configuration for adding a suffix to column names.</p> <p>Example - All columns:</p> <pre><code>add_suffix:\n  suffix: \"_raw\"\n</code></pre> <p>Example - Specific columns:</p> <pre><code>add_suffix:\n  suffix: \"_v2\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description suffix str Yes - Suffix to add to column names columns Optional[List[str]] No - Columns to suffix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from suffixing"},{"location":"reference/yaml_schema/#case_when-casewhenparams","title":"<code>case_when</code> (CaseWhenParams)","text":"<p>Implements structured CASE WHEN logic.</p> <p>Configuration for conditional logic.</p> <p>Example:</p> <pre><code>case_when:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description cases List[CaseWhenCase] Yes - List of conditional branches default str No <code>NULL</code> Default value if no condition met output_col str Yes - Name of the resulting column"},{"location":"reference/yaml_schema/#cast_columns-castcolumnsparams","title":"<code>cast_columns</code> (CastColumnsParams)","text":"<p>Casts specific columns to new types while keeping others intact.</p> <p>Configuration for column type casting.</p> <p>Example:</p> <pre><code>cast_columns:\n  casts:\n    age: \"int\"\n    salary: \"DOUBLE\"\n    created_at: \"TIMESTAMP\"\n    tags: \"ARRAY&lt;STRING&gt;\"  # Raw SQL types allowed\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description casts Dict[str, SimpleType | str] Yes - Map of column to target SQL type"},{"location":"reference/yaml_schema/#clean_text-cleantextparams","title":"<code>clean_text</code> (CleanTextParams)","text":"<p>Applies string cleaning operations (Trim/Case) via SQL.</p> <p>Configuration for text cleaning.</p> <p>Example:</p> <pre><code>clean_text:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to clean trim bool No <code>True</code> Apply TRIM() case Literal['lower', 'upper', 'preserve'] No <code>preserve</code> Case conversion"},{"location":"reference/yaml_schema/#coalesce_columns-coalescecolumnsparams","title":"<code>coalesce_columns</code> (CoalesceColumnsParams)","text":"<p>Returns the first non-null value from a list of columns. Useful for fallback/priority scenarios.</p> <p>Configuration for coalescing columns (first non-null value).</p> <p>Example - Phone number fallback:</p> <pre><code>coalesce_columns:\n  columns: [\"mobile_phone\", \"work_phone\", \"home_phone\"]\n  output_col: \"primary_phone\"\n</code></pre> <p>Example - Timestamp fallback:</p> <pre><code>coalesce_columns:\n  columns: [\"updated_at\", \"modified_at\", \"created_at\"]\n  output_col: \"last_change_at\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to coalesce (in priority order) output_col str Yes - Name of the output column drop_source bool No <code>False</code> Drop the source columns after coalescing"},{"location":"reference/yaml_schema/#concat_columns-concatcolumnsparams","title":"<code>concat_columns</code> (ConcatColumnsParams)","text":"<p>Concatenates multiple columns into one string. NULLs are skipped (treated as empty string) using CONCAT_WS behavior.</p> <p>Configuration for string concatenation.</p> <p>Example:</p> <pre><code>concat_columns:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to concatenate separator str No - Separator string output_col str Yes - Resulting column name"},{"location":"reference/yaml_schema/#convert_timezone-converttimezoneparams","title":"<code>convert_timezone</code> (ConvertTimezoneParams)","text":"<p>Converts a timestamp from one timezone to another. Assumes the input column is a naive timestamp representing time in source_tz, or a timestamp with timezone.</p> <p>Configuration for timezone conversion.</p> <p>Example:</p> <pre><code>convert_timezone:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - Timestamp column to convert source_tz str No <code>UTC</code> Source timezone (e.g., 'UTC', 'America/New_York') target_tz str Yes - Target timezone (e.g., 'America/Los_Angeles') output_col Optional[str] No - Name of the result column (default: {col}_{target_tz})"},{"location":"reference/yaml_schema/#date_add-dateaddparams","title":"<code>date_add</code> (DateAddParams)","text":"<p>Adds an interval to a date/timestamp column.</p> <p>Configuration for date addition.</p> <p>Example:</p> <pre><code>date_add:\n  col: \"created_at\"\n  value: 1\n  unit: \"day\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - - value int Yes - - unit Literal['day', 'month', 'year', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema/#date_diff-datediffparams","title":"<code>date_diff</code> (DateDiffParams)","text":"<p>Calculates difference between two dates/timestamps. Returns the elapsed time in the specified unit (as float for sub-day units).</p> <p>Configuration for date difference.</p> <p>Example:</p> <pre><code>date_diff:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description start_col str Yes - - end_col str Yes - - unit Literal['day', 'hour', 'minute', 'second'] No <code>day</code> -"},{"location":"reference/yaml_schema/#date_trunc-datetruncparams","title":"<code>date_trunc</code> (DateTruncParams)","text":"<p>Truncates a date/timestamp to the specified precision.</p> <p>Configuration for date truncation.</p> <p>Example:</p> <pre><code>date_trunc:\n  col: \"created_at\"\n  unit: \"month\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - - unit Literal['year', 'month', 'day', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema/#derive_columns-derivecolumnsparams","title":"<code>derive_columns</code> (DeriveColumnsParams)","text":"<p>Appends new columns based on SQL expressions.</p> <p>Design: - Uses projection to add fields. - Keeps all existing columns via <code>*</code>.</p> <p>Configuration for derived columns.</p> <p>Example:</p> <pre><code>derive_columns:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre> <p>Note: Engine will fail if expressions reference non-existent columns. Back to Catalog</p> Field Type Required Default Description derivations Dict[str, str] Yes - Map of column name to SQL expression"},{"location":"reference/yaml_schema/#distinct-distinctparams","title":"<code>distinct</code> (DistinctParams)","text":"<p>Returns unique rows (SELECT DISTINCT).</p> <p>Configuration for distinct rows.</p> <p>Example:</p> <pre><code>distinct:\n  columns: [\"category\", \"status\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to project (if None, keeps all columns unique)"},{"location":"reference/yaml_schema/#drop_columns-dropcolumnsparams","title":"<code>drop_columns</code> (DropColumnsParams)","text":"<p>Removes the specified columns from the DataFrame.</p> <p>Configuration for dropping specific columns (blacklist).</p> <p>Example:</p> <pre><code>drop_columns:\n  columns: [\"_internal_id\", \"_temp_flag\", \"_processing_date\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to drop"},{"location":"reference/yaml_schema/#extract_date_parts-extractdateparams","title":"<code>extract_date_parts</code> (ExtractDateParams)","text":"<p>Extracts date parts using ANSI SQL extract/functions.</p> <p>Configuration for extracting date parts.</p> <p>Example:</p> <pre><code>extract_date_parts:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description source_col str Yes - - prefix Optional[str] No - - parts Literal[typing.Literal['year', 'month', 'day', 'hour']] No <code>['year', 'month', 'day']</code> -"},{"location":"reference/yaml_schema/#fill_nulls-fillnullsparams","title":"<code>fill_nulls</code> (FillNullsParams)","text":"<p>Replaces null values with specified defaults using COALESCE.</p> <p>Configuration for filling null values.</p> <p>Example:</p> <pre><code>fill_nulls:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description values Dict[str, str | int | float | bool] Yes - Map of column to fill value"},{"location":"reference/yaml_schema/#filter_rows-filterrowsparams","title":"<code>filter_rows</code> (FilterRowsParams)","text":"<p>Filters rows using a standard SQL WHERE clause.</p> <p>Design: - SQL-First: Pushes filtering to the engine's optimizer. - Zero-Copy: No data movement to Python.</p> <p>Configuration for filtering rows.</p> <p>Example:</p> <pre><code>filter_rows:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre> <p>Example (Null Check):</p> <pre><code>filter_rows:\n  condition: \"email IS NOT NULL AND email != ''\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description condition str Yes - SQL WHERE clause (e.g., 'age &gt; 18 AND status = \"active\"')"},{"location":"reference/yaml_schema/#limit-limitparams","title":"<code>limit</code> (LimitParams)","text":"<p>Limits result size.</p> <p>Configuration for result limiting.</p> <p>Example:</p> <pre><code>limit:\n  n: 100\n  offset: 0\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description n int Yes - Number of rows to return offset int No <code>0</code> Number of rows to skip"},{"location":"reference/yaml_schema/#normalize_column_names-normalizecolumnnamesparams","title":"<code>normalize_column_names</code> (NormalizeColumnNamesParams)","text":"<p>Normalizes column names to a consistent style. Useful for cleaning up messy source data with spaces, mixed case, or special characters.</p> <p>Configuration for normalizing column names.</p> <p>Example:</p> <pre><code>normalize_column_names:\n  style: \"snake_case\"\n  lowercase: true\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description style Literal['snake_case', 'none'] No <code>snake_case</code> Naming style: 'snake_case' converts spaces/special chars to underscores lowercase bool No <code>True</code> Convert names to lowercase remove_special bool No <code>True</code> Remove special characters except underscores"},{"location":"reference/yaml_schema/#normalize_schema-normalizeschemaparams","title":"<code>normalize_schema</code> (NormalizeSchemaParams)","text":"<p>Structural transformation to rename, drop, and reorder columns.</p> <p>Note: This is one of the few that might behave better with native API in some cases, but SQL projection handles it perfectly and is consistent.</p> <p>Configuration for schema normalization.</p> <p>Example:</p> <pre><code>normalize_schema:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description rename Optional[Dict[str, str]] No <code>PydanticUndefined</code> old_name -&gt; new_name drop Optional[List[str]] No <code>PydanticUndefined</code> Columns to remove; ignored if not present select_order Optional[List[str]] No - Final column order; any missing columns appended after"},{"location":"reference/yaml_schema/#rename_columns-renamecolumnsparams","title":"<code>rename_columns</code> (RenameColumnsParams)","text":"<p>Renames columns according to the provided mapping. Columns not in the mapping are kept unchanged.</p> <p>Configuration for bulk column renaming.</p> <p>Example:</p> <pre><code>rename_columns:\n  mapping:\n    customer_id: cust_id\n    order_date: date\n    total_amount: amount\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description mapping Dict[str, str] Yes - Map of old column name to new column name"},{"location":"reference/yaml_schema/#replace_values-replacevaluesparams","title":"<code>replace_values</code> (ReplaceValuesParams)","text":"<p>Replaces values in specified columns according to the mapping. Supports replacing to NULL.</p> <p>Configuration for bulk value replacement.</p> <p>Example - Standardize nulls:</p> <pre><code>replace_values:\n  columns: [\"status\", \"category\"]\n  mapping:\n    \"N/A\": null\n    \"\": null\n    \"Unknown\": null\n</code></pre> <p>Example - Code replacement:</p> <pre><code>replace_values:\n  columns: [\"country_code\"]\n  mapping:\n    \"US\": \"USA\"\n    \"UK\": \"GBR\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to apply replacements to mapping Dict[str, Optional[str]] Yes - Map of old value to new value (use null for NULL)"},{"location":"reference/yaml_schema/#sample-sampleparams","title":"<code>sample</code> (SampleParams)","text":"<p>Samples data using random filtering.</p> <p>Configuration for random sampling.</p> <p>Example:</p> <pre><code>sample:\n  fraction: 0.1\n  seed: 42\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description fraction float Yes - Fraction of rows to return (0.0 to 1.0) seed Optional[int] No - -"},{"location":"reference/yaml_schema/#select_columns-selectcolumnsparams","title":"<code>select_columns</code> (SelectColumnsParams)","text":"<p>Keeps only the specified columns, dropping all others.</p> <p>Configuration for selecting specific columns (whitelist).</p> <p>Example:</p> <pre><code>select_columns:\n  columns: [\"id\", \"name\", \"created_at\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to keep"},{"location":"reference/yaml_schema/#sort-sortparams","title":"<code>sort</code> (SortParams)","text":"<p>Sorts the dataset.</p> <p>Configuration for sorting.</p> <p>Example:</p> <pre><code>sort:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description by str | List[str] Yes - Column(s) to sort by ascending bool No <code>True</code> Sort order"},{"location":"reference/yaml_schema/#split_part-splitpartparams","title":"<code>split_part</code> (SplitPartParams)","text":"<p>Extracts the Nth part of a string after splitting by a delimiter.</p> <p>Configuration for splitting strings.</p> <p>Example:</p> <pre><code>split_part:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - Column to split delimiter str Yes - Delimiter to split by index int Yes - 1-based index of the token to extract"},{"location":"reference/yaml_schema/#trim_whitespace-trimwhitespaceparams","title":"<code>trim_whitespace</code> (TrimWhitespaceParams)","text":"<p>Trims leading and trailing whitespace from string columns.</p> <p>Configuration for trimming whitespace from string columns.</p> <p>Example - All string columns:</p> <pre><code>trim_whitespace: {}\n</code></pre> <p>Example - Specific columns:</p> <pre><code>trim_whitespace:\n  columns: [\"name\", \"address\", \"city\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to trim (default: all string columns detected at runtime)"},{"location":"reference/yaml_schema/#relational-algebra","title":"\ud83d\udcc2 Relational Algebra","text":""},{"location":"reference/yaml_schema/#aggregate-aggregateparams","title":"<code>aggregate</code> (AggregateParams)","text":"<p>Performs grouping and aggregation via SQL.</p> <p>Configuration for aggregation.</p> <p>Example:</p> <pre><code>aggregate:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - Columns to group by aggregations Dict[str, AggFunc] Yes - Map of column to aggregation function (sum, avg, min, max, count)"},{"location":"reference/yaml_schema/#join-joinparams","title":"<code>join</code> (JoinParams)","text":"<p>Joins the current dataset with another dataset from the context.</p> <p>Configuration for joining datasets.</p> <p>Scenario 1: Simple Left Join</p> <pre><code>join:\n  right_dataset: \"customers\"\n  on: \"customer_id\"\n  how: \"left\"\n</code></pre> <p>Scenario 2: Join with Prefix (avoid collisions)</p> <pre><code>join:\n  right_dataset: \"orders\"\n  on: [\"user_id\"]\n  how: \"inner\"\n  prefix: \"ord\"  # Result cols: ord_date, ord_amount...\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description right_dataset str Yes - Name of the node/dataset to join with on str | List[str] Yes - Column(s) to join on how Literal['inner', 'left', 'right', 'full', 'cross', 'anti', 'semi'] No <code>left</code> Join type prefix Optional[str] No - Prefix for columns from right dataset to avoid collisions"},{"location":"reference/yaml_schema/#pivot-pivotparams","title":"<code>pivot</code> (PivotParams)","text":"<p>Pivots row values into columns.</p> <p>Configuration for pivoting data.</p> <p>Example:</p> <pre><code>pivot:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n</code></pre> <p>Example (Optimized for Spark):</p> <pre><code>pivot:\n  group_by: [\"id\"]\n  pivot_col: \"category\"\n  values: [\"A\", \"B\", \"C\"]  # Explicit values avoid extra pass\n  agg_col: \"amount\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - - pivot_col str Yes - - agg_col str Yes - - agg_func Literal['sum', 'count', 'avg', 'max', 'min', 'first'] No <code>sum</code> - values Optional[List[str]] No - Specific values to pivot (for Spark optimization)"},{"location":"reference/yaml_schema/#union-unionparams","title":"<code>union</code> (UnionParams)","text":"<p>Unions current dataset with others.</p> <p>Configuration for unioning datasets.</p> <p>Example (By Name - Default):</p> <pre><code>union:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true\n</code></pre> <p>Example (By Position):</p> <pre><code>union:\n  datasets: [\"legacy_data\"]\n  by_name: false\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description datasets List[str] Yes - List of node names to union with current by_name bool No <code>True</code> Match columns by name (UNION ALL BY NAME)"},{"location":"reference/yaml_schema/#unpivot-unpivotparams","title":"<code>unpivot</code> (UnpivotParams)","text":"<p>Unpivots columns into rows (Melt/Stack).</p> <p>Configuration for unpivoting (melting) data.</p> <p>Example:</p> <pre><code>unpivot:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description id_cols List[str] Yes - - value_vars List[str] Yes - - var_name str No <code>variable</code> - value_name str No <code>value</code> -"},{"location":"reference/yaml_schema/#data-quality","title":"\ud83d\udcc2 Data Quality","text":""},{"location":"reference/yaml_schema/#cross_check-crosscheckparams","title":"<code>cross_check</code> (CrossCheckParams)","text":"<p>Perform cross-node validation checks.</p> <p>Does not return a DataFrame (returns None). Raises ValidationError on failure.</p> <p>Configuration for cross-node validation checks.</p> <p>Example (Row Count Mismatch):</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre> <p>Example (Schema Match):</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"schema_match\"\n  inputs: [\"staging_orders\", \"prod_orders\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description type str Yes - Check type: 'row_count_diff', 'schema_match' inputs List[str] Yes - List of node names to compare threshold float No <code>0.0</code> Threshold for diff (0.0-1.0)"},{"location":"reference/yaml_schema/#warehousing-patterns","title":"\ud83d\udcc2 Warehousing Patterns","text":""},{"location":"reference/yaml_schema/#auditcolumnsconfig","title":"AuditColumnsConfig","text":"<p>Back to Catalog</p> Field Type Required Default Description created_col Optional[str] No - Column to set only on first insert updated_col Optional[str] No - Column to update on every merge"},{"location":"reference/yaml_schema/#merge-mergeparams","title":"<code>merge</code> (MergeParams)","text":"<p>Merge transformer implementation. Handles Upsert, Append-Only, and Delete-Match strategies.</p> <p>Args:     context: EngineContext (preferred) or legacy PandasContext/SparkContext     params: MergeParams object (when called via function step) or DataFrame (legacy)     current: DataFrame (legacy positional arg, deprecated)     **kwargs: Parameters when not using MergeParams</p> <p>Configuration for Merge transformer (Upsert/Append).</p>"},{"location":"reference/yaml_schema/#gdpr-compliance-guide","title":"\u2696\ufe0f \"GDPR &amp; Compliance\" Guide","text":"<p>Business Problem: \"A user exercised their 'Right to be Forgotten'. We need to remove them from our Silver tables immediately.\"</p> <p>The Solution: Use the <code>delete_match</code> strategy. The source dataframe contains the IDs to be deleted, and the transformer removes them from the target.</p> <p>Recipe 1: Right to be Forgotten (Delete)</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"delete_match\"\n</code></pre> <p>Recipe 2: Conditional Update (SCD Type 1) \"Only update if the source record is newer than the target record.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.products\"\n  keys: [\"product_id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n</code></pre> <p>Recipe 3: Safe Insert (Filter Bad Records) \"Only insert records that are not marked as deleted.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.orders\"\n  keys: [\"order_id\"]\n  strategy: \"append_only\"\n  insert_condition: \"source.is_deleted = false\"\n</code></pre> <p>Recipe 4: Audit Columns \"Track when records were created or updated.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.users\"\n  keys: [\"user_id\"]\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre> <p>Recipe 5: Full Sync (Insert + Update + Delete) \"Sync target with source: insert new, update changed, and remove soft-deleted.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  # 1. Delete if source says so\n  delete_condition: \"source.is_deleted = true\"\n  # 2. Update if changed (and not deleted)\n  update_condition: \"source.hash != target.hash\"\n  # 3. Insert new (and not deleted)\n  insert_condition: \"source.is_deleted = false\"\n</code></pre> <p>Recipe 6: Connection-based Path Resolution (ADLS) \"Use a connection to resolve paths, just like write config.\"</p> <pre><code>transform:\n  steps:\n    - function: merge\n      params:\n        connection: goat_prod\n        path: OEE/silver/customers\n        register_table: silver.customers\n        keys: [\"customer_id\"]\n        strategy: \"upsert\"\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre> <p>Strategies: *   upsert (Default): Update existing records, insert new ones. *   append_only: Ignore duplicates, only insert new keys. *   delete_match: Delete records in target that match keys in source. Back to Catalog</p> Field Type Required Default Description target Optional[str] No - Target table name or full path (use this OR connection+path) connection Optional[str] No - Connection name to resolve path (use with 'path' param) path Optional[str] No - Relative path within connection (e.g., 'OEE/silver/customers') register_table Optional[str] No - Register as Unity Catalog/metastore table after merge (e.g., 'silver.customers') keys List[str] Yes - List of join keys strategy MergeStrategy No <code>MergeStrategy.UPSERT</code> Merge behavior: 'upsert', 'append_only', 'delete_match' audit_cols Optional[AuditColumnsConfig] No - {'created_col': '...', 'updated_col': '...'} optimize_write bool No <code>False</code> Run OPTIMIZE after write (Spark) zorder_by Optional[List[str]] No - Columns to Z-Order by cluster_by Optional[List[str]] No - Columns to Liquid Cluster by (Delta) update_condition Optional[str] No - SQL condition for update clause (e.g. 'source.ver &gt; target.ver') insert_condition Optional[str] No - SQL condition for insert clause (e.g. 'source.status != \"deleted\"') delete_condition Optional[str] No - SQL condition for delete clause (e.g. 'source.status = \"deleted\"') table_properties Optional[dict] No - Delta table properties for initial table creation (e.g., column mapping)"},{"location":"reference/yaml_schema/#scd2-scd2params","title":"<code>scd2</code> (SCD2Params)","text":"<p>Implements SCD Type 2 Logic.</p> <p>Returns the FULL history dataset (to be written via Overwrite).</p> <p>Parameters for SCD Type 2 (Slowly Changing Dimensions) transformer.</p>"},{"location":"reference/yaml_schema/#the-time-machine-pattern","title":"\ud83d\udd70\ufe0f The \"Time Machine\" Pattern","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: SCD Type 2 tracks the full history of changes. Each record has an \"effective window\" (start/end dates) and a flag indicating if it is the current version.</p> <p>Recipe 1: Using table name</p> <pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"   # Registered table name\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre> <p>Recipe 2: Using connection + path (ADLS)</p> <pre><code>transformer: \"scd2\"\nparams:\n  connection: adls_prod            # Connection name\n  path: OEE/silver/dim_customers   # Relative path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre> <p>How it works: 1. Match: Finds existing records using <code>keys</code>. 2. Compare: Checks <code>track_cols</code> to see if data changed. 3. Close: If changed, updates the old record's <code>end_time_col</code> to the new <code>effective_time_col</code>. 4. Insert: Adds a new record with <code>effective_time_col</code> as start and open-ended end date.</p> <p>Note: SCD2 returns a DataFrame containing the full history. You must use a <code>write:</code> block to persist the result (typically with <code>mode: overwrite</code> to the same location as <code>target</code>). Back to Catalog</p> Field Type Required Default Description target Optional[str] No - Target table name or full path (use this OR connection+path) connection Optional[str] No - Connection name to resolve path (use with 'path' param) path Optional[str] No - Relative path within connection (e.g., 'OEE/silver/dim_customers') keys List[str] Yes - Natural keys to identify unique entities track_cols List[str] Yes - Columns to monitor for changes effective_time_col str Yes - Source column indicating when the change occurred. end_time_col str No <code>valid_to</code> Name of the end timestamp column current_flag_col str No <code>is_current</code> Name of the current record flag column delete_col Optional[str] No - Column indicating soft deletion (boolean)"},{"location":"reference/yaml_schema/#advanced-feature-engineering","title":"\ud83d\udcc2 Advanced &amp; Feature Engineering","text":""},{"location":"reference/yaml_schema/#shiftdefinition","title":"ShiftDefinition","text":"<p>Definition of a single shift. Back to Catalog</p> Field Type Required Default Description name str Yes - Name of the shift (e.g., 'Day', 'Night') start str Yes - Start time in HH:MM format (e.g., '06:00') end str Yes - End time in HH:MM format (e.g., '14:00')"},{"location":"reference/yaml_schema/#deduplicate-deduplicateparams","title":"<code>deduplicate</code> (DeduplicateParams)","text":"<p>Deduplicates data using Window functions.</p> <p>Configuration for deduplication.</p> <p>Scenario: Keep latest record</p> <pre><code>deduplicate:\n  keys: [\"id\"]\n  order_by: \"updated_at DESC\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description keys List[str] Yes - List of columns to partition by (columns that define uniqueness) order_by Optional[str] No - SQL Order by clause (e.g. 'updated_at DESC') to determine which record to keep (first one is kept)"},{"location":"reference/yaml_schema/#dict_based_mapping-dictmappingparams","title":"<code>dict_based_mapping</code> (DictMappingParams)","text":"<p>Configuration for dictionary mapping.</p> <p>Scenario: Map status codes to labels</p> <pre><code>dict_based_mapping:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column to map values from mapping Dict[str, str | int | float | bool] Yes - Dictionary of source value -&gt; target value default str | int | float | bool No - Default value if source value is not found in mapping output_column Optional[str] No - Name of output column. If not provided, overwrites source column."},{"location":"reference/yaml_schema/#explode_list_column-explodeparams","title":"<code>explode_list_column</code> (ExplodeParams)","text":"<p>Configuration for exploding lists.</p> <p>Scenario: Flatten list of items per order</p> <pre><code>explode_list_column:\n  column: \"items\"\n  outer: true  # Keep orders with empty items list\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing the list/array to explode outer bool No <code>False</code> If True, keep rows with empty lists (explode_outer behavior). If False, drops them."},{"location":"reference/yaml_schema/#generate_surrogate_key-surrogatekeyparams","title":"<code>generate_surrogate_key</code> (SurrogateKeyParams)","text":"<p>Generates a deterministic surrogate key (MD5) from a combination of columns. Handles NULLs by treating them as empty strings to ensure consistency.</p> <p>Configuration for surrogate key generation.</p> <p>Example:</p> <pre><code>generate_surrogate_key:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to combine for the key separator str No <code>-</code> Separator between values output_col str No <code>surrogate_key</code> Name of the output column"},{"location":"reference/yaml_schema/#geocode-geocodeparams","title":"<code>geocode</code> (GeocodeParams)","text":"<p>Geocoding Stub.</p> <p>Configuration for geocoding.</p> <p>Example:</p> <pre><code>geocode:\n  address_col: \"full_address\"\n  output_col: \"lat_long\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description address_col str Yes - Column containing the address to geocode output_col str No <code>lat_long</code> Name of the output column for coordinates"},{"location":"reference/yaml_schema/#hash_columns-hashparams","title":"<code>hash_columns</code> (HashParams)","text":"<p>Hashes columns for PII/Anonymization.</p> <p>Configuration for column hashing.</p> <p>Example:</p> <pre><code>hash_columns:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to hash algorithm HashAlgorithm No <code>HashAlgorithm.SHA256</code> Hashing algorithm. Options: 'sha256', 'md5'"},{"location":"reference/yaml_schema/#normalize_json-normalizejsonparams","title":"<code>normalize_json</code> (NormalizeJsonParams)","text":"<p>Flattens a nested JSON/Struct column.</p> <p>Configuration for JSON normalization.</p> <p>Example:</p> <pre><code>normalize_json:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing nested JSON/Struct sep str No <code>_</code> Separator for nested fields (e.g., 'parent_child')"},{"location":"reference/yaml_schema/#parse_json-parsejsonparams","title":"<code>parse_json</code> (ParseJsonParams)","text":"<p>Parses a JSON string column into a Struct/Map column.</p> <p>Configuration for JSON parsing.</p> <p>Example:</p> <pre><code>parse_json:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - String column containing JSON json_schema str Yes - DDL schema string (e.g. 'a INT, b STRING') or Spark StructType DDL output_col Optional[str] No - -"},{"location":"reference/yaml_schema/#regex_replace-regexreplaceparams","title":"<code>regex_replace</code> (RegexReplaceParams)","text":"<p>SQL-based Regex replacement.</p> <p>Configuration for regex replacement.</p> <p>Example:</p> <pre><code>regex_replace:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column to apply regex replacement on pattern str Yes - Regex pattern to match replacement str Yes - String to replace matches with"},{"location":"reference/yaml_schema/#sessionize-sessionizeparams","title":"<code>sessionize</code> (SessionizeParams)","text":"<p>Assigns session IDs based on inactivity threshold.</p> <p>Configuration for sessionization.</p> <p>Example:</p> <pre><code>sessionize:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description timestamp_col str Yes - Timestamp column to calculate session duration from user_col str Yes - User identifier to partition sessions by threshold_seconds int No <code>1800</code> Inactivity threshold in seconds (default: 30 minutes). If gap &gt; threshold, new session starts. session_col str No <code>session_id</code> Output column name for the generated session ID"},{"location":"reference/yaml_schema/#split_events_by_period-spliteventsbyperiodparams","title":"<code>split_events_by_period</code> (SplitEventsByPeriodParams)","text":"<p>Splits events that span multiple time periods into individual segments.</p> <p>For events spanning multiple days/hours/shifts, this creates separate rows for each period with adjusted start/end times and recalculated durations.</p> <p>Configuration for splitting events that span multiple time periods.</p> <p>Splits events that span multiple days, hours, or shifts into individual segments per period. Useful for OEE/downtime analysis, billing, and time-based aggregations.</p> <p>Example - Split by day:</p> <pre><code>split_events_by_period:\n  start_col: \"Shutdown_Start_Time\"\n  end_col: \"Shutdown_End_Time\"\n  period: \"day\"\n  duration_col: \"Shutdown_Duration_Min\"\n</code></pre> <p>Example - Split by shift:</p> <pre><code>split_events_by_period:\n  start_col: \"event_start\"\n  end_col: \"event_end\"\n  period: \"shift\"\n  duration_col: \"duration_minutes\"\n  shifts:\n    - name: \"Day\"\n      start: \"06:00\"\n      end: \"14:00\"\n    - name: \"Swing\"\n      start: \"14:00\"\n      end: \"22:00\"\n    - name: \"Night\"\n      start: \"22:00\"\n      end: \"06:00\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description start_col str Yes - Column containing the event start timestamp end_col str Yes - Column containing the event end timestamp period str No <code>day</code> Period type to split by: 'day', 'hour', or 'shift' duration_col Optional[str] No - Output column name for duration in minutes. If not set, no duration column is added. shifts Optional[List[ShiftDefinition]] No - List of shift definitions (required when period='shift') shift_col Optional[str] No <code>shift_name</code> Output column name for shift name (only used when period='shift')"},{"location":"reference/yaml_schema/#unpack_struct-unpackstructparams","title":"<code>unpack_struct</code> (UnpackStructParams)","text":"<p>Flattens a struct/dict column into top-level columns.</p> <p>Configuration for unpacking structs.</p> <p>Example:</p> <pre><code>unpack_struct:\n  column: \"user_info\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Struct/Dictionary column to unpack/flatten into individual columns"},{"location":"reference/yaml_schema/#validate_and_flag-validateandflagparams","title":"<code>validate_and_flag</code> (ValidateAndFlagParams)","text":"<p>Validates rules and appends a column with a list/string of failed rule names.</p> <p>Configuration for validation flagging.</p> <p>Example:</p> <pre><code>validate_and_flag:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description rules Dict[str, str] Yes - Map of rule name to SQL condition (must be TRUE) flag_col str No <code>_issues</code> Name of the column to store failed rules"},{"location":"reference/yaml_schema/#window_calculation-windowcalculationparams","title":"<code>window_calculation</code> (WindowCalculationParams)","text":"<p>Generic wrapper for Window functions.</p> <p>Configuration for window functions.</p> <p>Example:</p> <pre><code>window_calculation:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description target_col str Yes - - function str Yes - Window function e.g. 'sum(amount)', 'rank()' partition_by List[str] No <code>PydanticUndefined</code> - order_by Optional[str] No - -"},{"location":"reference/yaml_schema/#semantic-layer","title":"Semantic Layer","text":""},{"location":"reference/yaml_schema/#semantic-layer_1","title":"Semantic Layer","text":"<p>The semantic layer provides a unified interface for defining and querying business metrics. Define metrics once, query them by name across dimensions.</p> <p>Core Components: - MetricDefinition: Define aggregation expressions (SUM, COUNT, AVG) - DimensionDefinition: Define grouping attributes with hierarchies - MaterializationConfig: Pre-compute metrics at specific grain - SemanticQuery: Execute queries like \"revenue BY region, month\" - Project: Unified API that connects pipelines and semantic layer</p> <p>Unified Project API (Recommended):</p> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>YAML Configuration:</p> <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: gold.fact_orders    # connection.table notation\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: gold.dim_customer\n      column: region\n\nmaterializations:\n  - name: monthly_revenue\n    metrics: [revenue]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre> <p>The <code>source: gold.fact_orders</code> notation resolves paths automatically from connections.</p>"},{"location":"reference/yaml_schema/#dimensiondefinition","title":"<code>DimensionDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic dimension.</p> <p>A dimension represents an attribute for grouping and filtering metrics (e.g., date, product, region).</p> <p>Attributes:     name: Unique dimension identifier     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.dim_customer</code>         - <code>connection.path</code>: e.g., <code>gold.dim_customer</code> or <code>gold.dims/customer</code>         - <code>table_name</code>: Uses default connection     column: Column name in source (defaults to name)     hierarchy: Optional ordered list of columns for drill-down     description: Human-readable description | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique dimension identifier | | source | str | Yes | - | Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.dim_customer), connection.path (e.g., gold.dim_customer or gold.dims/customer), or bare table_name | | column | Optional[str] | No | - | Column name (defaults to name) | | hierarchy | List[str] | No | <code>PydanticUndefined</code> | Drill-down hierarchy | | description | Optional[str] | No | - | Human-readable description |</p>"},{"location":"reference/yaml_schema/#materializationconfig","title":"<code>MaterializationConfig</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Configuration for materializing metrics to a table.</p> <p>Materialization pre-computes aggregated metrics at a specific grain and persists them for faster querying.</p> <p>Attributes:     name: Unique materialization identifier     metrics: List of metric names to include     dimensions: List of dimension names (determines grain)     output: Output table path     schedule: Optional cron schedule for refresh     incremental: Configuration for incremental refresh | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique materialization identifier | | metrics | List[str] | Yes | - | Metrics to materialize | | dimensions | List[str] | Yes | - | Dimensions for grouping | | output | str | Yes | - | Output table path | | schedule | Optional[str] | No | - | Cron schedule | | incremental | Optional[Dict[str, Any]] | No | - | Incremental refresh config |</p>"},{"location":"reference/yaml_schema/#metricdefinition","title":"<code>MetricDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic metric.</p> <p>A metric represents a measurable value that can be aggregated across dimensions (e.g., revenue, order_count, avg_order_value).</p> <p>Attributes:     name: Unique metric identifier     description: Human-readable description     expr: SQL aggregation expression (e.g., \"SUM(total_amount)\")     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.fact_orders</code>         - <code>connection.path</code>: e.g., <code>gold.fact_orders</code> or <code>gold.oee/plant_a/metrics</code>         - <code>table_name</code>: Uses default connection     filters: Optional WHERE conditions to apply     type: \"simple\" (direct aggregation) or \"derived\" (references other metrics) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique metric identifier | | description | Optional[str] | No | - | Human-readable description | | expr | str | Yes | - | SQL aggregation expression | | source | Optional[str] | No | - | Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.fact_orders), connection.path (e.g., gold.fact_orders or gold.oee/plant_a/table), or bare table_name | | filters | List[str] | No | <code>PydanticUndefined</code> | WHERE conditions | | type | MetricType | No | <code>MetricType.SIMPLE</code> | Metric type |</p>"},{"location":"reference/yaml_schema/#semanticlayerconfig","title":"<code>SemanticLayerConfig</code>","text":"<p>Complete semantic layer configuration.</p> <p>Contains all metrics, dimensions, and materializations for a semantic layer deployment.</p> <p>Attributes:     metrics: List of metric definitions     dimensions: List of dimension definitions     materializations: List of materialization configurations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | metrics | List[MetricDefinition] | No | <code>PydanticUndefined</code> | Metric definitions | | dimensions | List[DimensionDefinition] | No | <code>PydanticUndefined</code> | Dimension definitions | | materializations | List[MaterializationConfig] | No | <code>PydanticUndefined</code> | Materialization configs |</p>"},{"location":"reference/yaml_schema/#fk-validation","title":"FK Validation","text":""},{"location":"reference/yaml_schema/#fk-validation_1","title":"FK Validation","text":"<p>Declare and validate referential integrity between fact and dimension tables.</p> <p>Features: - Declare relationships in YAML - Validate FK constraints on fact load - Detect orphan records - Generate lineage from relationships</p> <p>Example:</p> <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    on_violation: error\n</code></pre>"},{"location":"reference/yaml_schema/#relationshipconfig","title":"<code>RelationshipConfig</code>","text":"<p>Used in: RelationshipRegistry</p> <p>Configuration for a foreign key relationship.</p> <p>Attributes:     name: Unique relationship identifier     fact: Fact table name     dimension: Dimension table name     fact_key: Foreign key column in fact table     dimension_key: Primary/surrogate key column in dimension     nullable: Whether nulls are allowed in fact_key     on_violation: Action on violation (\"warn\", \"error\", \"quarantine\") | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique relationship identifier | | fact | str | Yes | - | Fact table name | | dimension | str | Yes | - | Dimension table name | | fact_key | str | Yes | - | FK column in fact table | | dimension_key | str | Yes | - | PK/SK column in dimension | | nullable | bool | No | <code>False</code> | Allow nulls in fact_key | | on_violation | str | No | <code>error</code> | Action on violation |</p>"},{"location":"reference/yaml_schema/#relationshipregistry","title":"<code>RelationshipRegistry</code>","text":"<p>Registry of all declared relationships.</p> <p>Attributes:     relationships: List of relationship configurations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | relationships | List[RelationshipConfig] | No | <code>PydanticUndefined</code> | Relationship definitions |</p>"},{"location":"reference/yaml_schema/#data-patterns","title":"Data Patterns","text":""},{"location":"reference/yaml_schema/#data-patterns_1","title":"Data Patterns","text":"<p>Declarative patterns for common data warehouse building blocks. Patterns encapsulate best practices for dimensional modeling, ensuring consistent implementation across your data warehouse.</p>"},{"location":"reference/yaml_schema/#dimensionpattern","title":"DimensionPattern","text":"<p>Build complete dimension tables with surrogate keys and SCD (Slowly Changing Dimension) support.</p> <p>When to Use: - Building dimension tables from source systems (customers, products, locations) - Need surrogate keys for star schema joins - Need to track historical changes (SCD Type 2)</p> <p>Beginner Note: Dimensions are the \"who, what, where, when\" of your data warehouse. A customer dimension has customer_id (natural key) and customer_sk (surrogate key). Fact tables join to dimensions via surrogate keys.</p> <p>See Also: FactPattern, DateDimensionPattern</p> <p>Features: - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> <p>Params:</p> Parameter Type Required Description <code>natural_key</code> str Yes Natural/business key column name <code>surrogate_key</code> str Yes Surrogate key column name to generate <code>scd_type</code> int No 0=static, 1=overwrite, 2=history (default: 1) <code>track_cols</code> list SCD1/2 Columns to track for change detection <code>target</code> str SCD2 Target table path to read existing history <code>unknown_member</code> bool No Insert row with SK=0 for orphan handling <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column with value <p>Supported Target Formats: - Spark: catalog.table, Delta paths, .parquet, .csv, .json, .orc - Pandas: .parquet, .csv, .json, .xlsx, .feather, .pickle</p> <p>Example:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_cols: [name, email, address, city]\n    target: warehouse.dim_customer\n    unknown_member: true\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre>"},{"location":"reference/yaml_schema/#datedimensionpattern","title":"DateDimensionPattern","text":"<p>Generate a complete date dimension table with pre-calculated attributes for BI/reporting.</p> <p>When to Use: - Every data warehouse needs a date dimension for time-based analytics - Enable date filtering, grouping by week/month/quarter, fiscal year reporting</p> <p>Beginner Note: The date dimension is foundational for any BI/reporting system. It lets you query \"sales by month\" or \"orders in fiscal Q2\" without complex date calculations.</p> <p>See Also: DimensionPattern</p> <p>Features: - Generates all dates in a range with rich attributes - Calendar and fiscal year support - ISO week numbering - Weekend/month-end flags</p> <p>Params:</p> Parameter Type Required Description <code>start_date</code> str Yes Start date (YYYY-MM-DD) <code>end_date</code> str Yes End date (YYYY-MM-DD) <code>date_key_format</code> str No Format for date_sk (default: yyyyMMdd) <code>fiscal_year_start_month</code> int No Month fiscal year starts (1-12, default: 1) <code>unknown_member</code> bool No Add unknown date row with date_sk=0 <p>Generated Columns: <code>date_sk</code>, <code>full_date</code>, <code>day_of_week</code>, <code>day_of_week_num</code>, <code>day_of_month</code>, <code>day_of_year</code>, <code>is_weekend</code>, <code>week_of_year</code>, <code>month</code>, <code>month_name</code>, <code>quarter</code>, <code>quarter_name</code>, <code>year</code>, <code>fiscal_year</code>, <code>fiscal_quarter</code>, <code>is_month_start</code>, <code>is_month_end</code>, <code>is_year_start</code>, <code>is_year_end</code></p> <p>Example:</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    fiscal_year_start_month: 7\n    unknown_member: true\n</code></pre>"},{"location":"reference/yaml_schema/#factpattern","title":"FactPattern","text":"<p>Build fact tables with automatic surrogate key lookups from dimensions.</p> <p>When to Use: - Building fact tables from transactional data (orders, events, transactions) - Need to look up surrogate keys from dimension tables - Need to handle orphan records (missing dimension matches)</p> <p>Beginner Note: Facts are the \"how much, how many\" of your data warehouse. An orders fact has measures (quantity, revenue) and dimension keys (customer_sk, product_sk). The pattern automatically looks up SKs from dimensions.</p> <p>See Also: DimensionPattern, QuarantineConfig</p> <p>Features: - Automatic SK lookups from dimension tables (with SCD2 current-record filtering) - Orphan handling: unknown (SK=0), reject (error), quarantine (route to table) - Grain validation (detect duplicates) - Calculated measures and column renaming - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list No Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No Dimension lookup configurations (see below) <code>orphan_handling</code> str No \"unknown\" | \"reject\" | \"quarantine\" (default: unknown) <code>quarantine</code> dict quarantine Quarantine config (see below) <code>measures</code> list No Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No Remove duplicates before processing <code>keys</code> list dedupe Keys for deduplication <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Dimension Lookup Config:</p> <pre><code>dimensions:\n  - source_column: customer_id      # Column in source fact\n    dimension_table: dim_customer   # Dimension in context\n    dimension_key: customer_id      # Natural key in dimension\n    surrogate_key: customer_sk      # SK to retrieve\n    scd2: true                      # Filter is_current=true\n</code></pre> <p>Quarantine Config (for orphan_handling: quarantine):</p> <pre><code>quarantine:\n  connection: silver                # Required: connection name\n  path: fact_orders_orphans         # OR table: quarantine_table\n  add_columns:\n    _rejection_reason: true         # Add rejection reason\n    _rejected_at: true              # Add rejection timestamp\n    _source_dimension: true         # Add dimension name\n</code></pre> <p>Example:</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n        scd2: true\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n    orphan_handling: unknown\n    measures:\n      - quantity\n      - revenue: \"quantity * unit_price\"\n    audit:\n      load_timestamp: true\n      source_system: \"pos\"\n</code></pre>"},{"location":"reference/yaml_schema/#aggregationpattern","title":"AggregationPattern","text":"<p>Declarative aggregation with GROUP BY and optional incremental merge.</p> <p>When to Use: - Building summary/aggregate tables (daily sales, monthly metrics) - Need incremental aggregation (update existing aggregates) - Gold layer reporting tables</p> <p>Beginner Note: Aggregations summarize facts at a higher grain. Example: daily_sales aggregates orders by date with SUM(revenue).</p> <p>See Also: FactPattern</p> <p>Features: - Declare grain (GROUP BY columns) - Define measures with SQL aggregation expressions - Optional HAVING filter - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list Yes Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes Measure definitions with name and expr <code>having</code> str No HAVING clause for filtering aggregates <code>incremental.timestamp_column</code> str No Column to identify new data <code>incremental.merge_strategy</code> str No \"replace\", \"sum\", \"min\", or \"max\" <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Example:</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk, region]\n    measures:\n      - name: total_revenue\n        expr: \"SUM(total_amount)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n      - name: avg_order_value\n        expr: \"AVG(total_amount)\"\n    having: \"COUNT(*) &gt; 0\"\n    audit:\n      load_timestamp: true\n</code></pre>"},{"location":"reference/yaml_schema/#auditconfig","title":"<code>AuditConfig</code>","text":"<p>Configuration for audit columns. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | load_timestamp | bool | No | <code>True</code> | Add load_timestamp column | | source_system | Optional[str] | No | - | Source system name for source_system column |</p>"},{"location":"reference/api/cli/","title":"CLI API","text":""},{"location":"reference/api/cli/#odibi.cli.main","title":"<code>odibi.cli.main</code>","text":"<p>Main CLI entry point.</p>"},{"location":"reference/api/cli/#odibi.cli.main.main","title":"<code>main()</code>","text":"<p>Main CLI entry point.</p> Source code in <code>odibi\\cli\\main.py</code> <pre><code>def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    # Configure telemetry early\n    setup_telemetry()\n\n    parser = argparse.ArgumentParser(\n        description=\"Odibi Data Pipeline Framework\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  odibi run config.yaml                    Run a pipeline\n  odibi validate config.yaml               Validate configuration\n  odibi graph config.yaml                  Visualize dependencies\n  odibi story generate config.yaml        Generate documentation\n  odibi story diff run1.json run2.json    Compare two runs\n  odibi story list                         List story files\n        \"\"\",\n    )\n\n    # Global arguments\n    parser.add_argument(\n        \"--log-level\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n        default=\"INFO\",\n        help=\"Set logging verbosity (default: INFO)\",\n    )\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # odibi run\n    run_parser = subparsers.add_parser(\"run\", help=\"Execute pipeline\")\n    run_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    run_parser.add_argument(\n        \"--env\", default=\"development\", help=\"Environment (development/production)\"\n    )\n    run_parser.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Simulate execution without running operations\"\n    )\n    run_parser.add_argument(\n        \"--resume\", action=\"store_true\", help=\"Resume from last failure (skip successful nodes)\"\n    )\n    run_parser.add_argument(\n        \"--parallel\", action=\"store_true\", help=\"Run independent nodes in parallel\"\n    )\n    run_parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=4,\n        help=\"Number of worker threads for parallel execution (default: 4)\",\n    )\n    run_parser.add_argument(\n        \"--on-error\",\n        choices=[\"fail_fast\", \"fail_later\", \"ignore\"],\n        help=\"Override error handling strategy\",\n    )\n    run_parser.add_argument(\n        \"--tag\",\n        help=\"Filter nodes by tag (e.g., --tag daily)\",\n    )\n    run_parser.add_argument(\n        \"--pipeline\",\n        dest=\"pipeline_name\",\n        help=\"Run specific pipeline by name\",\n    )\n    run_parser.add_argument(\n        \"--node\",\n        dest=\"node_name\",\n        help=\"Run specific node by name\",\n    )\n\n    # odibi deploy\n    deploy_parser = subparsers.add_parser(\"deploy\", help=\"Deploy definitions to System Catalog\")\n    deploy_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    deploy_parser.add_argument(\n        \"--env\", default=\"development\", help=\"Environment (development/production)\"\n    )\n\n    # odibi validate\n    validate_parser = subparsers.add_parser(\"validate\", help=\"Validate config\")\n    validate_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n\n    # odibi test\n    test_parser = subparsers.add_parser(\"test\", help=\"Run unit tests for transformations\")\n    test_parser.add_argument(\n        \"path\", nargs=\"?\", default=\"tests\", help=\"Path to tests directory or file (default: tests)\"\n    )\n    test_parser.add_argument(\"--snapshot\", action=\"store_true\", help=\"Update snapshots for tests\")\n\n    # odibi docs\n    subparsers.add_parser(\"docs\", help=\"Generate API documentation\")\n\n    # odibi graph\n    graph_parser = subparsers.add_parser(\"graph\", help=\"Visualize dependency graph\")\n    graph_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    graph_parser.add_argument(\"--pipeline\", help=\"Pipeline name (optional)\")\n    graph_parser.add_argument(\n        \"--format\",\n        choices=[\"ascii\", \"dot\", \"mermaid\"],\n        default=\"ascii\",\n        help=\"Output format (default: ascii)\",\n    )\n    graph_parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose output\")\n\n    # odibi story\n    add_story_parser(subparsers)\n\n    # odibi secrets\n    add_secrets_parser(subparsers)\n\n    # odibi init-pipeline (create/init)\n    add_init_parser(subparsers)\n\n    # odibi doctor\n    add_doctor_parser(subparsers)\n\n    # odibi ui\n    add_ui_parser(subparsers)\n\n    # odibi export\n    add_export_parser(subparsers)\n\n    # odibi catalog\n    add_catalog_parser(subparsers)\n\n    # odibi schema\n    add_schema_parser(subparsers)\n\n    # odibi lineage\n    add_lineage_parser(subparsers)\n\n    args = parser.parse_args()\n\n    # Configure logging\n    import logging\n\n    logging.basicConfig(\n        level=getattr(logging, args.log_level),\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    if args.command == \"run\":\n        return run_command(args)\n    elif args.command == \"deploy\":\n        from odibi.cli.deploy import deploy_command\n\n        return deploy_command(args)\n    elif args.command == \"docs\":\n        generate_docs()\n        return 0\n    elif args.command == \"validate\":\n        return validate_command(args)\n    elif args.command == \"test\":\n        return test_command(args)\n    elif args.command == \"graph\":\n        return graph_command(args)\n    elif args.command == \"story\":\n        return story_command(args)\n    elif args.command == \"secrets\":\n        return secrets_command(args)\n    elif args.command in [\"init-pipeline\", \"create\", \"init\", \"generate-project\"]:\n        return init_pipeline_command(args)\n    elif args.command == \"doctor\":\n        return doctor_command(args)\n    elif args.command == \"ui\":\n        return ui_command(args)\n    elif args.command == \"export\":\n        return export_command(args)\n    elif args.command == \"catalog\":\n        return catalog_command(args)\n    elif args.command == \"schema\":\n        return schema_command(args)\n    elif args.command == \"lineage\":\n        return lineage_command(args)\n    else:\n        parser.print_help()\n        return 1\n</code></pre>"},{"location":"reference/api/config/","title":"Configuration API","text":""},{"location":"reference/api/config/#odibi.config","title":"<code>odibi.config</code>","text":"<p>Configuration models for ODIBI framework.</p>"},{"location":"reference/api/config/#odibi.config.ConnectionConfig","title":"<code>ConnectionConfig = Union[LocalConnectionConfig, AzureBlobConnectionConfig, DeltaConnectionConfig, SQLServerConnectionConfig, HttpConnectionConfig, CustomConnectionConfig]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/config/#odibi.config.EngineType","title":"<code>EngineType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported execution engines.</p> Source code in <code>odibi\\config.py</code> <pre><code>class EngineType(str, Enum):\n    \"\"\"Supported execution engines.\"\"\"\n\n    SPARK = \"spark\"\n    PANDAS = \"pandas\"\n    POLARS = \"polars\"\n</code></pre>"},{"location":"reference/api/config/#odibi.config.ConnectionType","title":"<code>ConnectionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported connection types.</p> Source code in <code>odibi\\config.py</code> <pre><code>class ConnectionType(str, Enum):\n    \"\"\"Supported connection types.\"\"\"\n\n    LOCAL = \"local\"\n    AZURE_BLOB = \"azure_blob\"\n    DELTA = \"delta\"\n    SQL_SERVER = \"sql_server\"\n    HTTP = \"http\"\n</code></pre>"},{"location":"reference/api/config/#odibi.config.WriteMode","title":"<code>WriteMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Write modes for output operations.</p> Source code in <code>odibi\\config.py</code> <pre><code>class WriteMode(str, Enum):\n    \"\"\"Write modes for output operations.\"\"\"\n\n    OVERWRITE = \"overwrite\"\n    APPEND = \"append\"\n    UPSERT = \"upsert\"\n    APPEND_ONCE = \"append_once\"\n</code></pre>"},{"location":"reference/api/config/#odibi.config.AlertConfig","title":"<code>AlertConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for alerts with throttling support.</p> <p>Supports Slack, Teams, and generic webhooks with event-specific payloads.</p> <p>Available Events: - <code>on_start</code> - Pipeline started - <code>on_success</code> - Pipeline completed successfully - <code>on_failure</code> - Pipeline failed - <code>on_quarantine</code> - Rows were quarantined - <code>on_gate_block</code> - Quality gate blocked the pipeline - <code>on_threshold_breach</code> - A threshold was exceeded</p> <p>Example:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre> Source code in <code>odibi\\config.py</code> <pre><code>class AlertConfig(BaseModel):\n    \"\"\"\n    Configuration for alerts with throttling support.\n\n    Supports Slack, Teams, and generic webhooks with event-specific payloads.\n\n    **Available Events:**\n    - `on_start` - Pipeline started\n    - `on_success` - Pipeline completed successfully\n    - `on_failure` - Pipeline failed\n    - `on_quarantine` - Rows were quarantined\n    - `on_gate_block` - Quality gate blocked the pipeline\n    - `on_threshold_breach` - A threshold was exceeded\n\n    Example:\n    ```yaml\n    alerts:\n      - type: slack\n        url: \"${SLACK_WEBHOOK_URL}\"\n        on_events:\n          - on_failure\n          - on_quarantine\n          - on_gate_block\n        metadata:\n          throttle_minutes: 15\n          max_per_hour: 10\n          channel: \"#data-alerts\"\n    ```\n    \"\"\"\n\n    type: AlertType\n    url: str = Field(description=\"Webhook URL\")\n    on_events: List[AlertEvent] = Field(\n        default=[AlertEvent.ON_FAILURE],\n        description=\"Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach\",\n    )\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Extra metadata: throttle_minutes, max_per_hour, channel, etc.\",\n    )\n</code></pre>"},{"location":"reference/api/config/#odibi.config.TransformConfig","title":"<code>TransformConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for transformation steps within a node.</p> <p>When to Use: Custom business logic, data cleaning, SQL transformations.</p> <p>Key Concepts: - <code>steps</code>: Ordered list of operations (SQL, functions, or both) - Each step receives the DataFrame from the previous step - Steps execute in order: step1 \u2192 step2 \u2192 step3</p> <p>See Also: Transformer Catalog</p> <p>Transformer vs Transform: - <code>transformer</code>: Single heavy operation (scd2, merge, deduplicate) - <code>transform.steps</code>: Chain of lighter operations</p>"},{"location":"reference/api/config/#odibi.config.TransformConfig--transformation-pipeline-guide","title":"\ud83d\udd27 \"Transformation Pipeline\" Guide","text":"<p>Business Problem: \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"</p> <p>The Solution: Chain multiple steps together. Output of Step 1 becomes input of Step 2.</p> <p>Function Registry: The <code>function</code> step type looks up functions registered with <code>@transform</code> (or <code>@register</code>). This allows you to use the same registered functions as both top-level Transformers and steps in a chain.</p> <p>Recipe: The Mix-and-Match</p> <pre><code>transform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n</code></pre> Source code in <code>odibi\\config.py</code> <pre><code>class TransformConfig(BaseModel):\n    \"\"\"\n    Configuration for transformation steps within a node.\n\n    **When to Use:** Custom business logic, data cleaning, SQL transformations.\n\n    **Key Concepts:**\n    - `steps`: Ordered list of operations (SQL, functions, or both)\n    - Each step receives the DataFrame from the previous step\n    - Steps execute in order: step1 \u2192 step2 \u2192 step3\n\n    **See Also:** [Transformer Catalog](#nodeconfig)\n\n    **Transformer vs Transform:**\n    - `transformer`: Single heavy operation (scd2, merge, deduplicate)\n    - `transform.steps`: Chain of lighter operations\n\n    ### \ud83d\udd27 \"Transformation Pipeline\" Guide\n\n    **Business Problem:**\n    \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"\n\n    **The Solution:**\n    Chain multiple steps together. Output of Step 1 becomes input of Step 2.\n\n    **Function Registry:**\n    The `function` step type looks up functions registered with `@transform` (or `@register`).\n    This allows you to use the *same* registered functions as both top-level Transformers and steps in a chain.\n\n    **Recipe: The Mix-and-Match**\n    ```yaml\n    transform:\n      steps:\n        # Step 1: SQL Filter (Fast)\n        - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n        # Step 2: Custom Python Function (Complex Logic)\n        # Looks up 'calculate_lifetime_value' in the registry\n        - function: \"calculate_lifetime_value\"\n          params: { discount_rate: 0.05 }\n\n        # Step 3: Built-in Operation (Standard)\n        - operation: \"drop_duplicates\"\n          params: { subset: [\"user_id\"] }\n    ```\n    \"\"\"\n\n    steps: List[Union[str, TransformStep]] = Field(\n        description=\"List of transformation steps (SQL strings or TransformStep configs)\"\n    )\n</code></pre>"},{"location":"reference/api/config/#odibi.config.ValidationConfig","title":"<code>ValidationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for data validation (post-transform checks).</p> <p>When to Use: Output data quality checks that run after transformation but before writing.</p> <p>See Also: Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)</p>"},{"location":"reference/api/config/#odibi.config.ValidationConfig--the-indestructible-pipeline-pattern","title":"\ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern","text":"<p>Business Problem: \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it before it lands.\"</p> <p>The Solution: A Quality Gate that runs after transformation but before writing.</p> <p>Recipe: The Quality Gate</p> <pre><code>validation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges &amp; Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at &lt;= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n</code></pre> <p>Recipe: Quarantine + Gate</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n</code></pre> Source code in <code>odibi\\config.py</code> <pre><code>class ValidationConfig(BaseModel):\n    \"\"\"\n    Configuration for data validation (post-transform checks).\n\n    **When to Use:** Output data quality checks that run after transformation but before writing.\n\n    **See Also:** Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)\n\n    ### \ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern\n\n    **Business Problem:**\n    \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it *before* it lands.\"\n\n    **The Solution:**\n    A Quality Gate that runs *after* transformation but *before* writing.\n\n    **Recipe: The Quality Gate**\n    ```yaml\n    validation:\n      mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n      on_fail: \"alert\"      # alert or ignore\n\n      tests:\n        # 1. Completeness\n        - type: \"not_null\"\n          columns: [\"transaction_id\", \"customer_id\"]\n\n        # 2. Integrity\n        - type: \"unique\"\n          columns: [\"transaction_id\"]\n\n        - type: \"accepted_values\"\n          column: \"status\"\n          values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n        # 3. Ranges &amp; Patterns\n        - type: \"range\"\n          column: \"age\"\n          min: 18\n          max: 120\n\n        - type: \"regex_match\"\n          column: \"email\"\n          pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n\n        # 4. Business Logic (SQL)\n        - type: \"custom_sql\"\n          name: \"dates_ordered\"\n          condition: \"created_at &lt;= completed_at\"\n          threshold: 0.01   # Allow 1% failure\n    ```\n\n    **Recipe: Quarantine + Gate**\n    ```yaml\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n          on_fail: quarantine\n      quarantine:\n        connection: silver\n        path: customers_quarantine\n      gate:\n        require_pass_rate: 0.95\n        on_fail: abort\n    ```\n    \"\"\"\n\n    mode: ValidationAction = Field(\n        default=ValidationAction.FAIL,\n        description=\"Execution mode: 'fail' (stop pipeline) or 'warn' (log only)\",\n    )\n    on_fail: OnFailAction = Field(\n        default=OnFailAction.ALERT,\n        description=\"Action on failure: 'alert' (send notification) or 'ignore'\",\n    )\n    tests: List[TestConfig] = Field(default_factory=list, description=\"List of validation tests\")\n    quarantine: Optional[QuarantineConfig] = Field(\n        default=None,\n        description=\"Quarantine configuration for failed rows\",\n    )\n    gate: Optional[GateConfig] = Field(\n        default=None,\n        description=\"Quality gate configuration for batch-level validation\",\n    )\n    fail_fast: bool = Field(\n        default=False,\n        description=\"Stop validation on first failure. Skips remaining tests for faster feedback.\",\n    )\n    cache_df: bool = Field(\n        default=False,\n        description=\"Cache DataFrame before validation (Spark only). Improves performance with many tests.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_quarantine_config(self):\n        \"\"\"Warn if quarantine config exists but no tests use on_fail: quarantine.\"\"\"\n        import warnings\n\n        if self.quarantine and self.tests:\n            has_quarantine_tests = any(t.on_fail == ContractSeverity.QUARANTINE for t in self.tests)\n            if not has_quarantine_tests:\n                warnings.warn(\n                    \"Quarantine config is defined but no tests have 'on_fail: quarantine'. \"\n                    \"Quarantine will not be used. Add 'on_fail: quarantine' to tests that \"\n                    \"should route failed rows to quarantine.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        return self\n</code></pre>"},{"location":"reference/api/config/#odibi.config.ValidationConfig.validate_quarantine_config","title":"<code>validate_quarantine_config()</code>","text":"<p>Warn if quarantine config exists but no tests use on_fail: quarantine.</p> Source code in <code>odibi\\config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_quarantine_config(self):\n    \"\"\"Warn if quarantine config exists but no tests use on_fail: quarantine.\"\"\"\n    import warnings\n\n    if self.quarantine and self.tests:\n        has_quarantine_tests = any(t.on_fail == ContractSeverity.QUARANTINE for t in self.tests)\n        if not has_quarantine_tests:\n            warnings.warn(\n                \"Quarantine config is defined but no tests have 'on_fail: quarantine'. \"\n                \"Quarantine will not be used. Add 'on_fail: quarantine' to tests that \"\n                \"should route failed rows to quarantine.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    return self\n</code></pre>"},{"location":"reference/api/config/#odibi.config.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a pipeline.</p> <p>Example:</p> <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"node1\"\n        ...\n</code></pre> Source code in <code>odibi\\config.py</code> <pre><code>class PipelineConfig(BaseModel):\n    \"\"\"\n    Configuration for a pipeline.\n\n    Example:\n    ```yaml\n    pipelines:\n      - pipeline: \"user_onboarding\"\n        description: \"Ingest and process new users\"\n        layer: \"silver\"\n        nodes:\n          - name: \"node1\"\n            ...\n    ```\n    \"\"\"\n\n    pipeline: str = Field(description=\"Pipeline name\")\n    description: Optional[str] = Field(default=None, description=\"Pipeline description\")\n    layer: Optional[str] = Field(default=None, description=\"Logical layer (bronze/silver/gold)\")\n    nodes: List[NodeConfig] = Field(description=\"List of nodes in this pipeline\")\n\n    @field_validator(\"nodes\")\n    @classmethod\n    def check_unique_node_names(cls, nodes: List[NodeConfig]) -&gt; List[NodeConfig]:\n        \"\"\"Ensure all node names are unique within the pipeline.\"\"\"\n        names = [node.name for node in nodes]\n        if len(names) != len(set(names)):\n            duplicates = [name for name in names if names.count(name) &gt; 1]\n            raise ValueError(f\"Duplicate node names found: {set(duplicates)}\")\n        return nodes\n</code></pre>"},{"location":"reference/api/config/#odibi.config.PipelineConfig.check_unique_node_names","title":"<code>check_unique_node_names(nodes)</code>  <code>classmethod</code>","text":"<p>Ensure all node names are unique within the pipeline.</p> Source code in <code>odibi\\config.py</code> <pre><code>@field_validator(\"nodes\")\n@classmethod\ndef check_unique_node_names(cls, nodes: List[NodeConfig]) -&gt; List[NodeConfig]:\n    \"\"\"Ensure all node names are unique within the pipeline.\"\"\"\n    names = [node.name for node in nodes]\n    if len(names) != len(set(names)):\n        duplicates = [name for name in names if names.count(name) &gt; 1]\n        raise ValueError(f\"Duplicate node names found: {set(duplicates)}\")\n    return nodes\n</code></pre>"},{"location":"reference/api/config/#odibi.config.StoryConfig","title":"<code>StoryConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Story generation configuration.</p> <p>Stories are ODIBI's core value - execution reports with lineage. They must use a connection for consistent, traceable output.</p> <p>Example:</p> <pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre> <p>Failure Sample Settings: - <code>failure_sample_size</code>: Number of failed rows to capture per validation (default: 100) - <code>max_failure_samples</code>: Total failed rows across all validations (default: 500) - <code>max_sampled_validations</code>: After this many validations, show only counts (default: 5)</p> Source code in <code>odibi\\config.py</code> <pre><code>class StoryConfig(BaseModel):\n    \"\"\"\n    Story generation configuration.\n\n    Stories are ODIBI's core value - execution reports with lineage.\n    They must use a connection for consistent, traceable output.\n\n    Example:\n    ```yaml\n    story:\n      connection: \"local_data\"\n      path: \"stories/\"\n      retention_days: 30\n      failure_sample_size: 100\n      max_failure_samples: 500\n      max_sampled_validations: 5\n    ```\n\n    **Failure Sample Settings:**\n    - `failure_sample_size`: Number of failed rows to capture per validation (default: 100)\n    - `max_failure_samples`: Total failed rows across all validations (default: 500)\n    - `max_sampled_validations`: After this many validations, show only counts (default: 5)\n    \"\"\"\n\n    connection: str = Field(\n        description=\"Connection name for story output (uses connection's path resolution)\"\n    )\n    path: str = Field(description=\"Path for stories (relative to connection base_path)\")\n    max_sample_rows: int = Field(default=10, ge=0, le=100)\n    auto_generate: bool = True\n    retention_days: Optional[int] = Field(default=30, ge=1, description=\"Days to keep stories\")\n    retention_count: Optional[int] = Field(\n        default=100, ge=1, description=\"Max number of stories to keep\"\n    )\n\n    # Failure sample settings (troubleshooting)\n    failure_sample_size: int = Field(\n        default=100,\n        ge=0,\n        le=1000,\n        description=\"Number of failed rows to capture per validation rule\",\n    )\n    max_failure_samples: int = Field(\n        default=500,\n        ge=0,\n        le=5000,\n        description=\"Maximum total failed rows across all validations\",\n    )\n    max_sampled_validations: int = Field(\n        default=5,\n        ge=1,\n        le=20,\n        description=\"After this many validations, show only counts (no samples)\",\n    )\n\n    # Performance settings\n    async_generation: bool = Field(\n        default=False,\n        description=(\n            \"Generate stories asynchronously (fire-and-forget). \"\n            \"Pipeline returns immediately while story writes in background. \"\n            \"Improves multi-pipeline performance by ~5-10s per pipeline.\"\n        ),\n    )\n\n    @model_validator(mode=\"after\")\n    def check_retention_policy(self):\n        if self.retention_days is None and self.retention_count is None:\n            raise ValueError(\n                \"StoryConfig: Specify at least one of 'retention_days' or 'retention_count'.\"\n            )\n        return self\n</code></pre>"},{"location":"reference/api/connections/","title":"Connections API","text":""},{"location":"reference/api/connections/#odibi.connections.base","title":"<code>odibi.connections.base</code>","text":"<p>Base connection interface.</p>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection","title":"<code>BaseConnection</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connections.</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>class BaseConnection(ABC):\n    \"\"\"Abstract base class for connections.\"\"\"\n\n    @abstractmethod\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full path for a relative path.\n\n        Args:\n            relative_path: Relative path or table name\n\n        Returns:\n            Full path to resource\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate(self) -&gt; None:\n        \"\"\"Validate connection configuration.\n\n        Raises:\n            ConnectionError: If validation fails\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection.get_path","title":"<code>get_path(relative_path)</code>  <code>abstractmethod</code>","text":"<p>Get full path for a relative path.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>Relative path or table name</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path to resource</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>@abstractmethod\ndef get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full path for a relative path.\n\n    Args:\n        relative_path: Relative path or table name\n\n    Returns:\n        Full path to resource\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection.validate","title":"<code>validate()</code>  <code>abstractmethod</code>","text":"<p>Validate connection configuration.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If validation fails</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>@abstractmethod\ndef validate(self) -&gt; None:\n    \"\"\"Validate connection configuration.\n\n    Raises:\n        ConnectionError: If validation fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local","title":"<code>odibi.connections.local</code>","text":"<p>Local filesystem connection.</p>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection","title":"<code>LocalConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>class LocalConnection(BaseConnection):\n    \"\"\"Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).\"\"\"\n\n    def __init__(self, base_path: str = \"./data\"):\n        \"\"\"Initialize local connection.\n\n        Args:\n            base_path: Base directory for all paths (can be local path or URI)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"local\",\n            connection_name=\"LocalConnection\",\n            action=\"init\",\n            base_path=base_path,\n        )\n\n        self.base_path_str = base_path\n        self.is_uri = \"://\" in base_path or \":/\" in base_path\n\n        if not self.is_uri:\n            self.base_path = Path(base_path)\n            ctx.debug(\n                \"LocalConnection initialized with filesystem path\",\n                base_path=base_path,\n                is_uri=False,\n            )\n        else:\n            self.base_path = None  # Not used for URIs\n            ctx.debug(\n                \"LocalConnection initialized with URI path\",\n                base_path=base_path,\n                is_uri=True,\n            )\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full path for a relative path.\n\n        Args:\n            relative_path: Relative path from base\n\n        Returns:\n            Full absolute path or URI\n        \"\"\"\n        ctx = get_logging_context()\n\n        if self.is_uri:\n            # Use os.path for simple string joining, handling slashes manually for consistency\n            # Strip leading slash from relative to avoid root replacement\n            clean_rel = relative_path.lstrip(\"/\").lstrip(\"\\\\\")\n            # Handle cases where base_path might not have trailing slash\n            if self.base_path_str.endswith(\"/\") or self.base_path_str.endswith(\"\\\\\"):\n                full_path = f\"{self.base_path_str}{clean_rel}\"\n            else:\n                # Use forward slash for URIs\n                full_path = f\"{self.base_path_str}/{clean_rel}\"\n\n            ctx.debug(\n                \"Resolved URI path\",\n                relative_path=relative_path,\n                full_path=full_path,\n            )\n            return full_path\n        else:\n            # Standard local path logic\n            full_path = self.base_path / relative_path\n            resolved = str(full_path.absolute())\n\n            ctx.debug(\n                \"Resolved local path\",\n                relative_path=relative_path,\n                full_path=resolved,\n            )\n            return resolved\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate that base path exists or can be created.\n\n        Raises:\n            ConnectionError: If validation fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating LocalConnection\",\n            base_path=self.base_path_str,\n            is_uri=self.is_uri,\n        )\n\n        if self.is_uri:\n            # Cannot validate/create URIs with local os module\n            # Assume valid or handled by engine\n            ctx.debug(\n                \"Skipping URI validation (handled by engine)\",\n                base_path=self.base_path_str,\n            )\n        else:\n            # Create base directory if it doesn't exist\n            try:\n                self.base_path.mkdir(parents=True, exist_ok=True)\n                ctx.info(\n                    \"LocalConnection validated successfully\",\n                    base_path=str(self.base_path.absolute()),\n                    created=not self.base_path.exists(),\n                )\n            except Exception as e:\n                ctx.error(\n                    \"LocalConnection validation failed\",\n                    base_path=self.base_path_str,\n                    error=str(e),\n                )\n                raise\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.__init__","title":"<code>__init__(base_path='./data')</code>","text":"<p>Initialize local connection.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Base directory for all paths (can be local path or URI)</p> <code>'./data'</code> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def __init__(self, base_path: str = \"./data\"):\n    \"\"\"Initialize local connection.\n\n    Args:\n        base_path: Base directory for all paths (can be local path or URI)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"local\",\n        connection_name=\"LocalConnection\",\n        action=\"init\",\n        base_path=base_path,\n    )\n\n    self.base_path_str = base_path\n    self.is_uri = \"://\" in base_path or \":/\" in base_path\n\n    if not self.is_uri:\n        self.base_path = Path(base_path)\n        ctx.debug(\n            \"LocalConnection initialized with filesystem path\",\n            base_path=base_path,\n            is_uri=False,\n        )\n    else:\n        self.base_path = None  # Not used for URIs\n        ctx.debug(\n            \"LocalConnection initialized with URI path\",\n            base_path=base_path,\n            is_uri=True,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get full path for a relative path.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>Relative path from base</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full absolute path or URI</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full path for a relative path.\n\n    Args:\n        relative_path: Relative path from base\n\n    Returns:\n        Full absolute path or URI\n    \"\"\"\n    ctx = get_logging_context()\n\n    if self.is_uri:\n        # Use os.path for simple string joining, handling slashes manually for consistency\n        # Strip leading slash from relative to avoid root replacement\n        clean_rel = relative_path.lstrip(\"/\").lstrip(\"\\\\\")\n        # Handle cases where base_path might not have trailing slash\n        if self.base_path_str.endswith(\"/\") or self.base_path_str.endswith(\"\\\\\"):\n            full_path = f\"{self.base_path_str}{clean_rel}\"\n        else:\n            # Use forward slash for URIs\n            full_path = f\"{self.base_path_str}/{clean_rel}\"\n\n        ctx.debug(\n            \"Resolved URI path\",\n            relative_path=relative_path,\n            full_path=full_path,\n        )\n        return full_path\n    else:\n        # Standard local path logic\n        full_path = self.base_path / relative_path\n        resolved = str(full_path.absolute())\n\n        ctx.debug(\n            \"Resolved local path\",\n            relative_path=relative_path,\n            full_path=resolved,\n        )\n        return resolved\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.validate","title":"<code>validate()</code>","text":"<p>Validate that base path exists or can be created.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If validation fails</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate that base path exists or can be created.\n\n    Raises:\n        ConnectionError: If validation fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating LocalConnection\",\n        base_path=self.base_path_str,\n        is_uri=self.is_uri,\n    )\n\n    if self.is_uri:\n        # Cannot validate/create URIs with local os module\n        # Assume valid or handled by engine\n        ctx.debug(\n            \"Skipping URI validation (handled by engine)\",\n            base_path=self.base_path_str,\n        )\n    else:\n        # Create base directory if it doesn't exist\n        try:\n            self.base_path.mkdir(parents=True, exist_ok=True)\n            ctx.info(\n                \"LocalConnection validated successfully\",\n                base_path=str(self.base_path.absolute()),\n                created=not self.base_path.exists(),\n            )\n        except Exception as e:\n            ctx.error(\n                \"LocalConnection validation failed\",\n                base_path=self.base_path_str,\n                error=str(e),\n            )\n            raise\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls","title":"<code>odibi.connections.azure_adls</code>","text":"<p>Azure Data Lake Storage Gen2 connection (Phase 2A: Multi-mode authentication).</p>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS","title":"<code>AzureADLS</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Azure Data Lake Storage Gen2 connection.</p> <p>Phase 2A: Multi-mode authentication + multi-account support Supports key_vault (recommended), direct_key, service_principal, and managed_identity.</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>class AzureADLS(BaseConnection):\n    \"\"\"Azure Data Lake Storage Gen2 connection.\n\n    Phase 2A: Multi-mode authentication + multi-account support\n    Supports key_vault (recommended), direct_key, service_principal, and managed_identity.\n    \"\"\"\n\n    def __init__(\n        self,\n        account: str,\n        container: str,\n        path_prefix: str = \"\",\n        auth_mode: str = \"key_vault\",\n        key_vault_name: Optional[str] = None,\n        secret_name: Optional[str] = None,\n        account_key: Optional[str] = None,\n        sas_token: Optional[str] = None,\n        tenant_id: Optional[str] = None,\n        client_id: Optional[str] = None,\n        client_secret: Optional[str] = None,\n        validate: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Initialize ADLS connection.\n\n        Args:\n            account: Storage account name (e.g., 'mystorageaccount')\n            container: Container/filesystem name\n            path_prefix: Optional prefix for all paths\n            auth_mode: Authentication mode\n                ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')\n            key_vault_name: Azure Key Vault name (required for key_vault mode)\n            secret_name: Secret name in Key Vault (required for key_vault mode)\n            account_key: Storage account key (required for direct_key mode)\n            sas_token: Shared Access Signature token (required for sas_token mode)\n            tenant_id: Azure Tenant ID (required for service_principal)\n            client_id: Service Principal Client ID (required for service_principal)\n            client_secret: Service Principal Client Secret (required for service_principal)\n            validate: Validate configuration on init\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"azure_adls\",\n            connection_name=f\"{account}/{container}\",\n            action=\"init\",\n            account=account,\n            container=container,\n            auth_mode=auth_mode,\n            path_prefix=path_prefix or \"(none)\",\n        )\n\n        self.account = account\n        self.container = container\n        self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n        self.auth_mode = auth_mode\n        self.key_vault_name = key_vault_name\n        self.secret_name = secret_name\n        self.account_key = account_key\n        self.sas_token = sas_token\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n        self._cached_key: Optional[str] = None\n        self._cache_lock = threading.Lock()\n\n        if validate:\n            self.validate()\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate ADLS connection configuration.\n\n        Raises:\n            ValueError: If required fields are missing for the selected auth_mode\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating AzureADLS connection\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        if not self.account:\n            ctx.error(\"ADLS connection validation failed: missing 'account'\")\n            raise ValueError(\"ADLS connection requires 'account'\")\n        if not self.container:\n            ctx.error(\n                \"ADLS connection validation failed: missing 'container'\",\n                account=self.account,\n            )\n            raise ValueError(\"ADLS connection requires 'container'\")\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"ADLS key_vault mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                    key_vault_name=self.key_vault_name or \"(missing)\",\n                    secret_name=self.secret_name or \"(missing)\",\n                )\n                raise ValueError(\n                    f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"direct_key\":\n            if not self.account_key:\n                ctx.error(\n                    \"ADLS direct_key mode validation failed: missing account_key\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"direct_key mode requires 'account_key' \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n\n            # Warn in production\n            if os.getenv(\"ODIBI_ENV\") == \"production\":\n                ctx.warning(\n                    \"Using direct_key in production is not recommended\",\n                    account=self.account,\n                    container=self.container,\n                )\n                warnings.warn(\n                    f\"\u26a0\ufe0f  Using direct_key in production is not recommended. \"\n                    f\"Use auth_mode: key_vault. Connection: {self.account}/{self.container}\",\n                    UserWarning,\n                )\n        elif self.auth_mode == \"sas_token\":\n            if not self.sas_token and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"ADLS sas_token mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"sas_token mode requires 'sas_token' (or key_vault_name/secret_name) \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"service_principal\":\n            if not self.tenant_id or not self.client_id:\n                ctx.error(\n                    \"ADLS service_principal mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                    missing=\"tenant_id and/or client_id\",\n                )\n                raise ValueError(\"service_principal mode requires 'tenant_id' and 'client_id'\")\n\n            if not self.client_secret and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"ADLS service_principal mode validation failed: missing client_secret\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"service_principal mode requires 'client_secret' \"\n                    f\"(or key_vault_name/secret_name) for {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"managed_identity\":\n            # No specific config required, but we might check if environment supports it\n            ctx.debug(\n                \"Using managed_identity auth mode\",\n                account=self.account,\n                container=self.container,\n            )\n        else:\n            ctx.error(\n                \"ADLS validation failed: unsupported auth_mode\",\n                account=self.account,\n                container=self.container,\n                auth_mode=self.auth_mode,\n            )\n            raise ValueError(\n                f\"Unsupported auth_mode: '{self.auth_mode}'. \"\n                f\"Use 'key_vault', 'direct_key', 'service_principal', or 'managed_identity'.\"\n            )\n\n        ctx.info(\n            \"AzureADLS connection validated successfully\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n    def get_storage_key(self, timeout: float = 30.0) -&gt; Optional[str]:\n        \"\"\"Get storage account key (cached).\n\n        Only relevant for 'key_vault' and 'direct_key' modes.\n\n        Args:\n            timeout: Timeout for Key Vault operations in seconds (default: 30.0)\n\n        Returns:\n            Storage account key or None if not applicable for auth_mode\n\n        Raises:\n            ImportError: If azure libraries not installed (key_vault mode)\n            TimeoutError: If Key Vault fetch exceeds timeout\n            Exception: If Key Vault access fails\n        \"\"\"\n        ctx = get_logging_context()\n\n        with self._cache_lock:\n            # Return cached key if available (double-check inside lock)\n            if self._cached_key:\n                ctx.debug(\n                    \"Using cached storage key\",\n                    account=self.account,\n                    container=self.container,\n                )\n                return self._cached_key\n\n            if self.auth_mode == \"key_vault\":\n                ctx.debug(\n                    \"Fetching storage key from Key Vault\",\n                    account=self.account,\n                    key_vault_name=self.key_vault_name,\n                    secret_name=self.secret_name,\n                    timeout=timeout,\n                )\n\n                try:\n                    import concurrent.futures\n\n                    from azure.identity import DefaultAzureCredential\n                    from azure.keyvault.secrets import SecretClient\n                except ImportError as e:\n                    ctx.error(\n                        \"Key Vault authentication failed: missing azure libraries\",\n                        account=self.account,\n                        error=str(e),\n                    )\n                    raise ImportError(\n                        \"Key Vault authentication requires 'azure-identity' and \"\n                        \"'azure-keyvault-secrets'. Install with: pip install odibi[azure]\"\n                    ) from e\n\n                # Create Key Vault client\n                credential = DefaultAzureCredential()\n                kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n                client = SecretClient(vault_url=kv_uri, credential=credential)\n\n                ctx.debug(\n                    \"Connecting to Key Vault\",\n                    key_vault_uri=kv_uri,\n                    secret_name=self.secret_name,\n                )\n\n                # Fetch secret with timeout protection\n                def _fetch():\n                    secret = client.get_secret(self.secret_name)\n                    return secret.value\n\n                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                    future = executor.submit(_fetch)\n                    try:\n                        self._cached_key = future.result(timeout=timeout)\n                        logger.register_secret(self._cached_key)\n                        ctx.info(\n                            \"Successfully fetched storage key from Key Vault\",\n                            account=self.account,\n                            key_vault_name=self.key_vault_name,\n                        )\n                        return self._cached_key\n                    except concurrent.futures.TimeoutError:\n                        ctx.error(\n                            \"Key Vault fetch timed out\",\n                            account=self.account,\n                            key_vault_name=self.key_vault_name,\n                            secret_name=self.secret_name,\n                            timeout=timeout,\n                        )\n                        raise TimeoutError(\n                            f\"Key Vault fetch timed out after {timeout}s for \"\n                            f\"vault '{self.key_vault_name}', secret '{self.secret_name}'\"\n                        )\n\n            elif self.auth_mode == \"direct_key\":\n                ctx.debug(\n                    \"Using direct account key\",\n                    account=self.account,\n                )\n                return self.account_key\n\n            elif self.auth_mode == \"sas_token\":\n                # Return cached key (fetched from KV) if available, else sas_token arg\n                ctx.debug(\n                    \"Using SAS token\",\n                    account=self.account,\n                    from_cache=bool(self._cached_key),\n                )\n                return self._cached_key or self.sas_token\n\n            # For other modes (SP, MI), we don't use an account key\n            ctx.debug(\n                \"No storage key required for auth_mode\",\n                account=self.account,\n                auth_mode=self.auth_mode,\n            )\n            return None\n\n    def get_client_secret(self) -&gt; Optional[str]:\n        \"\"\"Get Service Principal client secret (cached or literal).\"\"\"\n        return self._cached_key or self.client_secret\n\n    def pandas_storage_options(self) -&gt; Dict[str, Any]:\n        \"\"\"Get storage options for pandas/fsspec.\n\n        Returns:\n            Dictionary with appropriate authentication parameters for fsspec\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Building pandas storage options\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        base_options = {\"account_name\": self.account}\n\n        if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n            return {**base_options, \"account_key\": self.get_storage_key()}\n\n        elif self.auth_mode == \"sas_token\":\n            # Use get_storage_key() which handles KV fallback for SAS\n            return {**base_options, \"sas_token\": self.get_storage_key()}\n\n        elif self.auth_mode == \"service_principal\":\n            return {\n                **base_options,\n                \"tenant_id\": self.tenant_id,\n                \"client_id\": self.client_id,\n                \"client_secret\": self.get_client_secret(),\n            }\n\n        elif self.auth_mode == \"managed_identity\":\n            # adlfs supports using DefaultAzureCredential implicitly if anon=False\n            # and no other creds provided, assuming azure.identity is installed\n            return {**base_options, \"anon\": False}\n\n        return base_options\n\n    def configure_spark(self, spark: \"Any\") -&gt; None:\n        \"\"\"Configure Spark session with storage credentials.\n\n        Args:\n            spark: SparkSession instance\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Configuring Spark for AzureADLS\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n            config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(config_key, self.get_storage_key())\n            ctx.debug(\n                \"Set Spark config for account key\",\n                config_key=config_key,\n            )\n\n        elif self.auth_mode == \"sas_token\":\n            # SAS Token Configuration\n            # fs.azure.sas.token.provider.type -&gt; FixedSASTokenProvider\n            # fs.azure.sas.fixed.token -&gt; &lt;token&gt;\n            provider_key = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(provider_key, \"SAS\")\n\n            sas_provider_key = (\n                f\"fs.azure.sas.token.provider.type.{self.account}.dfs.core.windows.net\"\n            )\n            spark.conf.set(\n                sas_provider_key, \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n            )\n\n            sas_token = self.get_storage_key()\n\n            sas_token_key = f\"fs.azure.sas.fixed.token.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(sas_token_key, sas_token)\n\n            ctx.debug(\n                \"Set Spark config for SAS token\",\n                auth_type_key=provider_key,\n                provider_key=sas_provider_key,\n            )\n\n        elif self.auth_mode == \"service_principal\":\n            # Configure OAuth for ADLS Gen2\n            # Ref: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html\n            prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"OAuth\")\n\n            prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n            prefix = f\"fs.azure.account.oauth2.client.id.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, self.client_id)\n\n            prefix = f\"fs.azure.account.oauth2.client.secret.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, self.get_client_secret())\n\n            prefix = f\"fs.azure.account.oauth2.client.endpoint.{self.account}.dfs.core.windows.net\"\n            endpoint = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n            spark.conf.set(prefix, endpoint)\n\n            ctx.debug(\n                \"Set Spark config for service principal OAuth\",\n                tenant_id=self.tenant_id,\n                client_id=self.client_id,\n            )\n\n        elif self.auth_mode == \"managed_identity\":\n            prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"OAuth\")\n\n            prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n\n            ctx.debug(\n                \"Set Spark config for managed identity\",\n                account=self.account,\n            )\n\n        ctx.info(\n            \"Spark configuration complete\",\n            account=self.account,\n            auth_mode=self.auth_mode,\n        )\n\n    def uri(self, path: str) -&gt; str:\n        \"\"\"Build abfss:// URI for given path.\n\n        Args:\n            path: Relative path within container\n\n        Returns:\n            Full abfss:// URI\n\n        Example:\n            &gt;&gt;&gt; conn = AzureADLS(\n            ...     account=\"myaccount\", container=\"data\",\n            ...     auth_mode=\"direct_key\", account_key=\"key123\"\n            ... )\n            &gt;&gt;&gt; conn.uri(\"folder/file.csv\")\n            'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'\n        \"\"\"\n        if self.path_prefix:\n            full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n        else:\n            full_path = path.lstrip(\"/\")\n\n        return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full abfss:// URI for relative path.\"\"\"\n        ctx = get_logging_context()\n        full_uri = self.uri(relative_path)\n\n        ctx.debug(\n            \"Resolved ADLS path\",\n            account=self.account,\n            container=self.container,\n            relative_path=relative_path,\n            full_uri=full_uri,\n        )\n\n        return full_uri\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.__init__","title":"<code>__init__(account, container, path_prefix='', auth_mode='key_vault', key_vault_name=None, secret_name=None, account_key=None, sas_token=None, tenant_id=None, client_id=None, client_secret=None, validate=True, **kwargs)</code>","text":"<p>Initialize ADLS connection.</p> <p>Parameters:</p> Name Type Description Default <code>account</code> <code>str</code> <p>Storage account name (e.g., 'mystorageaccount')</p> required <code>container</code> <code>str</code> <p>Container/filesystem name</p> required <code>path_prefix</code> <code>str</code> <p>Optional prefix for all paths</p> <code>''</code> <code>auth_mode</code> <code>str</code> <p>Authentication mode ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')</p> <code>'key_vault'</code> <code>key_vault_name</code> <code>Optional[str]</code> <p>Azure Key Vault name (required for key_vault mode)</p> <code>None</code> <code>secret_name</code> <code>Optional[str]</code> <p>Secret name in Key Vault (required for key_vault mode)</p> <code>None</code> <code>account_key</code> <code>Optional[str]</code> <p>Storage account key (required for direct_key mode)</p> <code>None</code> <code>sas_token</code> <code>Optional[str]</code> <p>Shared Access Signature token (required for sas_token mode)</p> <code>None</code> <code>tenant_id</code> <code>Optional[str]</code> <p>Azure Tenant ID (required for service_principal)</p> <code>None</code> <code>client_id</code> <code>Optional[str]</code> <p>Service Principal Client ID (required for service_principal)</p> <code>None</code> <code>client_secret</code> <code>Optional[str]</code> <p>Service Principal Client Secret (required for service_principal)</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Validate configuration on init</p> <code>True</code> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def __init__(\n    self,\n    account: str,\n    container: str,\n    path_prefix: str = \"\",\n    auth_mode: str = \"key_vault\",\n    key_vault_name: Optional[str] = None,\n    secret_name: Optional[str] = None,\n    account_key: Optional[str] = None,\n    sas_token: Optional[str] = None,\n    tenant_id: Optional[str] = None,\n    client_id: Optional[str] = None,\n    client_secret: Optional[str] = None,\n    validate: bool = True,\n    **kwargs,\n):\n    \"\"\"Initialize ADLS connection.\n\n    Args:\n        account: Storage account name (e.g., 'mystorageaccount')\n        container: Container/filesystem name\n        path_prefix: Optional prefix for all paths\n        auth_mode: Authentication mode\n            ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')\n        key_vault_name: Azure Key Vault name (required for key_vault mode)\n        secret_name: Secret name in Key Vault (required for key_vault mode)\n        account_key: Storage account key (required for direct_key mode)\n        sas_token: Shared Access Signature token (required for sas_token mode)\n        tenant_id: Azure Tenant ID (required for service_principal)\n        client_id: Service Principal Client ID (required for service_principal)\n        client_secret: Service Principal Client Secret (required for service_principal)\n        validate: Validate configuration on init\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"azure_adls\",\n        connection_name=f\"{account}/{container}\",\n        action=\"init\",\n        account=account,\n        container=container,\n        auth_mode=auth_mode,\n        path_prefix=path_prefix or \"(none)\",\n    )\n\n    self.account = account\n    self.container = container\n    self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n    self.auth_mode = auth_mode\n    self.key_vault_name = key_vault_name\n    self.secret_name = secret_name\n    self.account_key = account_key\n    self.sas_token = sas_token\n    self.tenant_id = tenant_id\n    self.client_id = client_id\n    self.client_secret = client_secret\n\n    self._cached_key: Optional[str] = None\n    self._cache_lock = threading.Lock()\n\n    if validate:\n        self.validate()\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.configure_spark","title":"<code>configure_spark(spark)</code>","text":"<p>Configure Spark session with storage credentials.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>Any</code> <p>SparkSession instance</p> required Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def configure_spark(self, spark: \"Any\") -&gt; None:\n    \"\"\"Configure Spark session with storage credentials.\n\n    Args:\n        spark: SparkSession instance\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Configuring Spark for AzureADLS\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n        config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(config_key, self.get_storage_key())\n        ctx.debug(\n            \"Set Spark config for account key\",\n            config_key=config_key,\n        )\n\n    elif self.auth_mode == \"sas_token\":\n        # SAS Token Configuration\n        # fs.azure.sas.token.provider.type -&gt; FixedSASTokenProvider\n        # fs.azure.sas.fixed.token -&gt; &lt;token&gt;\n        provider_key = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(provider_key, \"SAS\")\n\n        sas_provider_key = (\n            f\"fs.azure.sas.token.provider.type.{self.account}.dfs.core.windows.net\"\n        )\n        spark.conf.set(\n            sas_provider_key, \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n        )\n\n        sas_token = self.get_storage_key()\n\n        sas_token_key = f\"fs.azure.sas.fixed.token.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(sas_token_key, sas_token)\n\n        ctx.debug(\n            \"Set Spark config for SAS token\",\n            auth_type_key=provider_key,\n            provider_key=sas_provider_key,\n        )\n\n    elif self.auth_mode == \"service_principal\":\n        # Configure OAuth for ADLS Gen2\n        # Ref: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html\n        prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"OAuth\")\n\n        prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n        prefix = f\"fs.azure.account.oauth2.client.id.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, self.client_id)\n\n        prefix = f\"fs.azure.account.oauth2.client.secret.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, self.get_client_secret())\n\n        prefix = f\"fs.azure.account.oauth2.client.endpoint.{self.account}.dfs.core.windows.net\"\n        endpoint = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n        spark.conf.set(prefix, endpoint)\n\n        ctx.debug(\n            \"Set Spark config for service principal OAuth\",\n            tenant_id=self.tenant_id,\n            client_id=self.client_id,\n        )\n\n    elif self.auth_mode == \"managed_identity\":\n        prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"OAuth\")\n\n        prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n\n        ctx.debug(\n            \"Set Spark config for managed identity\",\n            account=self.account,\n        )\n\n    ctx.info(\n        \"Spark configuration complete\",\n        account=self.account,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_client_secret","title":"<code>get_client_secret()</code>","text":"<p>Get Service Principal client secret (cached or literal).</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_client_secret(self) -&gt; Optional[str]:\n    \"\"\"Get Service Principal client secret (cached or literal).\"\"\"\n    return self._cached_key or self.client_secret\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get full abfss:// URI for relative path.</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full abfss:// URI for relative path.\"\"\"\n    ctx = get_logging_context()\n    full_uri = self.uri(relative_path)\n\n    ctx.debug(\n        \"Resolved ADLS path\",\n        account=self.account,\n        container=self.container,\n        relative_path=relative_path,\n        full_uri=full_uri,\n    )\n\n    return full_uri\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_storage_key","title":"<code>get_storage_key(timeout=30.0)</code>","text":"<p>Get storage account key (cached).</p> <p>Only relevant for 'key_vault' and 'direct_key' modes.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Timeout for Key Vault operations in seconds (default: 30.0)</p> <code>30.0</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Storage account key or None if not applicable for auth_mode</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If azure libraries not installed (key_vault mode)</p> <code>TimeoutError</code> <p>If Key Vault fetch exceeds timeout</p> <code>Exception</code> <p>If Key Vault access fails</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_storage_key(self, timeout: float = 30.0) -&gt; Optional[str]:\n    \"\"\"Get storage account key (cached).\n\n    Only relevant for 'key_vault' and 'direct_key' modes.\n\n    Args:\n        timeout: Timeout for Key Vault operations in seconds (default: 30.0)\n\n    Returns:\n        Storage account key or None if not applicable for auth_mode\n\n    Raises:\n        ImportError: If azure libraries not installed (key_vault mode)\n        TimeoutError: If Key Vault fetch exceeds timeout\n        Exception: If Key Vault access fails\n    \"\"\"\n    ctx = get_logging_context()\n\n    with self._cache_lock:\n        # Return cached key if available (double-check inside lock)\n        if self._cached_key:\n            ctx.debug(\n                \"Using cached storage key\",\n                account=self.account,\n                container=self.container,\n            )\n            return self._cached_key\n\n        if self.auth_mode == \"key_vault\":\n            ctx.debug(\n                \"Fetching storage key from Key Vault\",\n                account=self.account,\n                key_vault_name=self.key_vault_name,\n                secret_name=self.secret_name,\n                timeout=timeout,\n            )\n\n            try:\n                import concurrent.futures\n\n                from azure.identity import DefaultAzureCredential\n                from azure.keyvault.secrets import SecretClient\n            except ImportError as e:\n                ctx.error(\n                    \"Key Vault authentication failed: missing azure libraries\",\n                    account=self.account,\n                    error=str(e),\n                )\n                raise ImportError(\n                    \"Key Vault authentication requires 'azure-identity' and \"\n                    \"'azure-keyvault-secrets'. Install with: pip install odibi[azure]\"\n                ) from e\n\n            # Create Key Vault client\n            credential = DefaultAzureCredential()\n            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n            client = SecretClient(vault_url=kv_uri, credential=credential)\n\n            ctx.debug(\n                \"Connecting to Key Vault\",\n                key_vault_uri=kv_uri,\n                secret_name=self.secret_name,\n            )\n\n            # Fetch secret with timeout protection\n            def _fetch():\n                secret = client.get_secret(self.secret_name)\n                return secret.value\n\n            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                future = executor.submit(_fetch)\n                try:\n                    self._cached_key = future.result(timeout=timeout)\n                    logger.register_secret(self._cached_key)\n                    ctx.info(\n                        \"Successfully fetched storage key from Key Vault\",\n                        account=self.account,\n                        key_vault_name=self.key_vault_name,\n                    )\n                    return self._cached_key\n                except concurrent.futures.TimeoutError:\n                    ctx.error(\n                        \"Key Vault fetch timed out\",\n                        account=self.account,\n                        key_vault_name=self.key_vault_name,\n                        secret_name=self.secret_name,\n                        timeout=timeout,\n                    )\n                    raise TimeoutError(\n                        f\"Key Vault fetch timed out after {timeout}s for \"\n                        f\"vault '{self.key_vault_name}', secret '{self.secret_name}'\"\n                    )\n\n        elif self.auth_mode == \"direct_key\":\n            ctx.debug(\n                \"Using direct account key\",\n                account=self.account,\n            )\n            return self.account_key\n\n        elif self.auth_mode == \"sas_token\":\n            # Return cached key (fetched from KV) if available, else sas_token arg\n            ctx.debug(\n                \"Using SAS token\",\n                account=self.account,\n                from_cache=bool(self._cached_key),\n            )\n            return self._cached_key or self.sas_token\n\n        # For other modes (SP, MI), we don't use an account key\n        ctx.debug(\n            \"No storage key required for auth_mode\",\n            account=self.account,\n            auth_mode=self.auth_mode,\n        )\n        return None\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.pandas_storage_options","title":"<code>pandas_storage_options()</code>","text":"<p>Get storage options for pandas/fsspec.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with appropriate authentication parameters for fsspec</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def pandas_storage_options(self) -&gt; Dict[str, Any]:\n    \"\"\"Get storage options for pandas/fsspec.\n\n    Returns:\n        Dictionary with appropriate authentication parameters for fsspec\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Building pandas storage options\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    base_options = {\"account_name\": self.account}\n\n    if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n        return {**base_options, \"account_key\": self.get_storage_key()}\n\n    elif self.auth_mode == \"sas_token\":\n        # Use get_storage_key() which handles KV fallback for SAS\n        return {**base_options, \"sas_token\": self.get_storage_key()}\n\n    elif self.auth_mode == \"service_principal\":\n        return {\n            **base_options,\n            \"tenant_id\": self.tenant_id,\n            \"client_id\": self.client_id,\n            \"client_secret\": self.get_client_secret(),\n        }\n\n    elif self.auth_mode == \"managed_identity\":\n        # adlfs supports using DefaultAzureCredential implicitly if anon=False\n        # and no other creds provided, assuming azure.identity is installed\n        return {**base_options, \"anon\": False}\n\n    return base_options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.uri","title":"<code>uri(path)</code>","text":"<p>Build abfss:// URI for given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path within container</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full abfss:// URI</p> Example <p>conn = AzureADLS( ...     account=\"myaccount\", container=\"data\", ...     auth_mode=\"direct_key\", account_key=\"key123\" ... ) conn.uri(\"folder/file.csv\") 'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def uri(self, path: str) -&gt; str:\n    \"\"\"Build abfss:// URI for given path.\n\n    Args:\n        path: Relative path within container\n\n    Returns:\n        Full abfss:// URI\n\n    Example:\n        &gt;&gt;&gt; conn = AzureADLS(\n        ...     account=\"myaccount\", container=\"data\",\n        ...     auth_mode=\"direct_key\", account_key=\"key123\"\n        ... )\n        &gt;&gt;&gt; conn.uri(\"folder/file.csv\")\n        'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'\n    \"\"\"\n    if self.path_prefix:\n        full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n    else:\n        full_path = path.lstrip(\"/\")\n\n    return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.validate","title":"<code>validate()</code>","text":"<p>Validate ADLS connection configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing for the selected auth_mode</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate ADLS connection configuration.\n\n    Raises:\n        ValueError: If required fields are missing for the selected auth_mode\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating AzureADLS connection\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    if not self.account:\n        ctx.error(\"ADLS connection validation failed: missing 'account'\")\n        raise ValueError(\"ADLS connection requires 'account'\")\n    if not self.container:\n        ctx.error(\n            \"ADLS connection validation failed: missing 'container'\",\n            account=self.account,\n        )\n        raise ValueError(\"ADLS connection requires 'container'\")\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"ADLS key_vault mode validation failed\",\n                account=self.account,\n                container=self.container,\n                key_vault_name=self.key_vault_name or \"(missing)\",\n                secret_name=self.secret_name or \"(missing)\",\n            )\n            raise ValueError(\n                f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"direct_key\":\n        if not self.account_key:\n            ctx.error(\n                \"ADLS direct_key mode validation failed: missing account_key\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"direct_key mode requires 'account_key' \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n\n        # Warn in production\n        if os.getenv(\"ODIBI_ENV\") == \"production\":\n            ctx.warning(\n                \"Using direct_key in production is not recommended\",\n                account=self.account,\n                container=self.container,\n            )\n            warnings.warn(\n                f\"\u26a0\ufe0f  Using direct_key in production is not recommended. \"\n                f\"Use auth_mode: key_vault. Connection: {self.account}/{self.container}\",\n                UserWarning,\n            )\n    elif self.auth_mode == \"sas_token\":\n        if not self.sas_token and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"ADLS sas_token mode validation failed\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"sas_token mode requires 'sas_token' (or key_vault_name/secret_name) \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"service_principal\":\n        if not self.tenant_id or not self.client_id:\n            ctx.error(\n                \"ADLS service_principal mode validation failed\",\n                account=self.account,\n                container=self.container,\n                missing=\"tenant_id and/or client_id\",\n            )\n            raise ValueError(\"service_principal mode requires 'tenant_id' and 'client_id'\")\n\n        if not self.client_secret and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"ADLS service_principal mode validation failed: missing client_secret\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"service_principal mode requires 'client_secret' \"\n                f\"(or key_vault_name/secret_name) for {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"managed_identity\":\n        # No specific config required, but we might check if environment supports it\n        ctx.debug(\n            \"Using managed_identity auth mode\",\n            account=self.account,\n            container=self.container,\n        )\n    else:\n        ctx.error(\n            \"ADLS validation failed: unsupported auth_mode\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n        raise ValueError(\n            f\"Unsupported auth_mode: '{self.auth_mode}'. \"\n            f\"Use 'key_vault', 'direct_key', 'service_principal', or 'managed_identity'.\"\n        )\n\n    ctx.info(\n        \"AzureADLS connection validated successfully\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql","title":"<code>odibi.connections.azure_sql</code>","text":""},{"location":"reference/api/connections/#odibi.connections.azure_sql--azure-sql-database-connection","title":"Azure SQL Database Connection","text":"<p>Provides connectivity to Azure SQL databases with authentication support.</p>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL","title":"<code>AzureSQL</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Azure SQL Database connection.</p> <p>Supports: - SQL authentication (username/password) - Azure Active Directory Managed Identity - Connection pooling - Read/write operations via SQLAlchemy</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>class AzureSQL(BaseConnection):\n    \"\"\"\n    Azure SQL Database connection.\n\n    Supports:\n    - SQL authentication (username/password)\n    - Azure Active Directory Managed Identity\n    - Connection pooling\n    - Read/write operations via SQLAlchemy\n    \"\"\"\n\n    def __init__(\n        self,\n        server: str,\n        database: str,\n        driver: str = \"ODBC Driver 18 for SQL Server\",\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        auth_mode: str = \"aad_msi\",  # \"aad_msi\", \"sql\", \"key_vault\"\n        key_vault_name: Optional[str] = None,\n        secret_name: Optional[str] = None,\n        port: int = 1433,\n        timeout: int = 30,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize Azure SQL connection.\n\n        Args:\n            server: SQL server hostname (e.g., 'myserver.database.windows.net')\n            database: Database name\n            driver: ODBC driver name (default: ODBC Driver 18 for SQL Server)\n            username: SQL auth username (required if auth_mode='sql')\n            password: SQL auth password (required if auth_mode='sql')\n            auth_mode: Authentication mode ('aad_msi', 'sql', 'key_vault')\n            key_vault_name: Key Vault name (required if auth_mode='key_vault')\n            secret_name: Secret name containing password (required if auth_mode='key_vault')\n            port: SQL Server port (default: 1433)\n            timeout: Connection timeout in seconds (default: 30)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"azure_sql\",\n            connection_name=f\"{server}/{database}\",\n            action=\"init\",\n            server=server,\n            database=database,\n            auth_mode=auth_mode,\n            port=port,\n        )\n\n        self.server = server\n        self.database = database\n        self.driver = driver\n        self.username = username\n        self.password = password\n        self.auth_mode = auth_mode\n        self.key_vault_name = key_vault_name\n        self.secret_name = secret_name\n        self.port = port\n        self.timeout = timeout\n        self._engine = None\n        self._cached_key = None  # For consistency with ADLS / parallel fetch\n\n        ctx.debug(\n            \"AzureSQL connection initialized\",\n            server=server,\n            database=database,\n            auth_mode=auth_mode,\n            driver=driver,\n        )\n\n    def get_password(self) -&gt; Optional[str]:\n        \"\"\"Get password (cached).\"\"\"\n        ctx = get_logging_context()\n\n        if self.password:\n            ctx.debug(\n                \"Using provided password\",\n                server=self.server,\n                database=self.database,\n            )\n            return self.password\n\n        if self._cached_key:\n            ctx.debug(\n                \"Using cached password\",\n                server=self.server,\n                database=self.database,\n            )\n            return self._cached_key\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"Key Vault mode requires key_vault_name and secret_name\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"key_vault mode requires key_vault_name and secret_name\")\n\n            ctx.debug(\n                \"Fetching password from Key Vault\",\n                server=self.server,\n                key_vault_name=self.key_vault_name,\n                secret_name=self.secret_name,\n            )\n\n            try:\n                from azure.identity import DefaultAzureCredential\n                from azure.keyvault.secrets import SecretClient\n\n                credential = DefaultAzureCredential()\n                kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n                client = SecretClient(vault_url=kv_uri, credential=credential)\n                secret = client.get_secret(self.secret_name)\n                self._cached_key = secret.value\n                logger.register_secret(self._cached_key)\n\n                ctx.info(\n                    \"Successfully fetched password from Key Vault\",\n                    server=self.server,\n                    key_vault_name=self.key_vault_name,\n                )\n                return self._cached_key\n            except ImportError as e:\n                ctx.error(\n                    \"Key Vault support requires azure libraries\",\n                    server=self.server,\n                    error=str(e),\n                )\n                raise ImportError(\n                    \"Key Vault support requires 'azure-identity' and 'azure-keyvault-secrets'\"\n                )\n\n        ctx.debug(\n            \"No password required for auth_mode\",\n            server=self.server,\n            auth_mode=self.auth_mode,\n        )\n        return None\n\n    def odbc_dsn(self) -&gt; str:\n        \"\"\"Build ODBC connection string.\n\n        Returns:\n            ODBC DSN string\n\n        Example:\n            &gt;&gt;&gt; conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\")\n            &gt;&gt;&gt; conn.odbc_dsn()\n            'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Building ODBC connection string\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        dsn = (\n            f\"Driver={{{self.driver}}};\"\n            f\"Server=tcp:{self.server},1433;\"\n            f\"Database={self.database};\"\n            f\"Encrypt=yes;\"\n            f\"TrustServerCertificate=yes;\"\n            f\"Connection Timeout=30;\"\n        )\n\n        pwd = self.get_password()\n        if self.username and pwd:\n            dsn += f\"UID={self.username};PWD={pwd};\"\n            ctx.debug(\n                \"Using SQL authentication\",\n                server=self.server,\n                username=self.username,\n            )\n        elif self.auth_mode == \"aad_msi\":\n            dsn += \"Authentication=ActiveDirectoryMsi;\"\n            ctx.debug(\n                \"Using AAD Managed Identity authentication\",\n                server=self.server,\n            )\n        elif self.auth_mode == \"aad_service_principal\":\n            # Not fully supported via ODBC string simply without token usually\n            ctx.debug(\n                \"Using AAD Service Principal authentication\",\n                server=self.server,\n            )\n\n        return dsn\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get table reference for relative path.\"\"\"\n        return relative_path\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate Azure SQL connection configuration.\"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating AzureSQL connection\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        if not self.server:\n            ctx.error(\"AzureSQL validation failed: missing 'server'\")\n            raise ValueError(\"Azure SQL connection requires 'server'\")\n        if not self.database:\n            ctx.error(\n                \"AzureSQL validation failed: missing 'database'\",\n                server=self.server,\n            )\n            raise ValueError(\"Azure SQL connection requires 'database'\")\n\n        if self.auth_mode == \"sql\":\n            if not self.username:\n                ctx.error(\n                    \"AzureSQL validation failed: SQL auth requires username\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"Azure SQL with auth_mode='sql' requires username\")\n            if not self.password and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"AzureSQL validation failed: SQL auth requires password\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    \"Azure SQL with auth_mode='sql' requires password \"\n                    \"(or key_vault_name/secret_name)\"\n                )\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"AzureSQL validation failed: key_vault mode missing config\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    \"Azure SQL with auth_mode='key_vault' requires key_vault_name and secret_name\"\n                )\n            if not self.username:\n                ctx.error(\n                    \"AzureSQL validation failed: key_vault mode requires username\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"Azure SQL with auth_mode='key_vault' requires username\")\n\n        ctx.info(\n            \"AzureSQL connection validated successfully\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n    def get_engine(self) -&gt; Any:\n        \"\"\"\n        Get or create SQLAlchemy engine.\n\n        Returns:\n            SQLAlchemy engine instance\n\n        Raises:\n            ConnectionError: If connection fails or drivers missing\n        \"\"\"\n        ctx = get_logging_context()\n\n        if self._engine is not None:\n            ctx.debug(\n                \"Using cached SQLAlchemy engine\",\n                server=self.server,\n                database=self.database,\n            )\n            return self._engine\n\n        ctx.debug(\n            \"Creating SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n        )\n\n        try:\n            from urllib.parse import quote_plus\n\n            from sqlalchemy import create_engine\n        except ImportError as e:\n            ctx.error(\n                \"SQLAlchemy import failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=\"Required packages 'sqlalchemy' or 'pyodbc' not found.\",\n                suggestions=[\n                    \"Install required packages: pip install sqlalchemy pyodbc\",\n                    \"Or install odibi with azure extras: pip install 'odibi[azure]'\",\n                ],\n            )\n\n        try:\n            # Build connection string\n            conn_str = self.odbc_dsn()\n            connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}\"\n\n            ctx.debug(\n                \"Creating SQLAlchemy engine with connection pooling\",\n                server=self.server,\n                database=self.database,\n            )\n\n            # Create engine with connection pooling\n            self._engine = create_engine(\n                connection_url,\n                pool_pre_ping=True,  # Verify connections before use\n                pool_recycle=3600,  # Recycle connections after 1 hour\n                echo=False,\n            )\n\n            # Test connection\n            with self._engine.connect():\n                pass\n\n            ctx.info(\n                \"SQLAlchemy engine created successfully\",\n                server=self.server,\n                database=self.database,\n            )\n\n            return self._engine\n\n        except Exception as e:\n            suggestions = self._get_error_suggestions(str(e))\n            ctx.error(\n                \"Failed to create SQLAlchemy engine\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n                suggestions=suggestions,\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Failed to create engine: {str(e)}\",\n                suggestions=suggestions,\n            )\n\n    def read_sql(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Execute SQL query and return results as DataFrame.\n\n        Args:\n            query: SQL query string\n            params: Optional query parameters for parameterized queries\n\n        Returns:\n            Query results as pandas DataFrame\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Executing SQL query\",\n            server=self.server,\n            database=self.database,\n            query_length=len(query),\n        )\n\n        try:\n            engine = self.get_engine()\n            result = pd.read_sql(query, engine, params=params)\n\n            ctx.info(\n                \"SQL query executed successfully\",\n                server=self.server,\n                database=self.database,\n                rows_returned=len(result),\n            )\n            return result\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"SQL query execution failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Query execution failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def read_table(self, table_name: str, schema: Optional[str] = \"dbo\") -&gt; pd.DataFrame:\n        \"\"\"\n        Read entire table into DataFrame.\n\n        Args:\n            table_name: Name of the table\n            schema: Schema name (default: dbo)\n\n        Returns:\n            Table contents as pandas DataFrame\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Reading table\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            schema=schema,\n        )\n\n        if schema:\n            query = f\"SELECT * FROM [{schema}].[{table_name}]\"\n        else:\n            query = f\"SELECT * FROM [{table_name}]\"\n\n        return self.read_sql(query)\n\n    def write_table(\n        self,\n        df: pd.DataFrame,\n        table_name: str,\n        schema: Optional[str] = \"dbo\",\n        if_exists: str = \"replace\",\n        index: bool = False,\n        chunksize: Optional[int] = 1000,\n    ) -&gt; int:\n        \"\"\"\n        Write DataFrame to SQL table.\n\n        Args:\n            df: DataFrame to write\n            table_name: Name of the table\n            schema: Schema name (default: dbo)\n            if_exists: How to behave if table exists ('fail', 'replace', 'append')\n            index: Whether to write DataFrame index as column\n            chunksize: Number of rows to write in each batch (default: 1000)\n\n        Returns:\n            Number of rows written\n\n        Raises:\n            ConnectionError: If write fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Writing DataFrame to table\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            schema=schema,\n            rows=len(df),\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n\n        try:\n            engine = self.get_engine()\n\n            rows_written = df.to_sql(\n                name=table_name,\n                con=engine,\n                schema=schema,\n                if_exists=if_exists,\n                index=index,\n                chunksize=chunksize,\n                method=\"multi\",  # Use multi-row INSERT for better performance\n            )\n\n            result_rows = rows_written if rows_written is not None else len(df)\n            ctx.info(\n                \"Table write completed successfully\",\n                server=self.server,\n                database=self.database,\n                table_name=table_name,\n                rows_written=result_rows,\n            )\n            return result_rows\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"Table write failed\",\n                server=self.server,\n                database=self.database,\n                table_name=table_name,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Write operation failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n        \"\"\"\n        Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n        Args:\n            sql: SQL statement\n            params: Optional parameters for parameterized query\n\n        Returns:\n            Result from execution\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Executing SQL statement\",\n            server=self.server,\n            database=self.database,\n            statement_length=len(sql),\n        )\n\n        try:\n            engine = self.get_engine()\n            from sqlalchemy import text\n\n            with engine.connect() as conn:\n                result = conn.execute(text(sql), params or {})\n                conn.commit()\n\n                ctx.info(\n                    \"SQL statement executed successfully\",\n                    server=self.server,\n                    database=self.database,\n                )\n                return result\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"SQL statement execution failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Statement execution failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def close(self):\n        \"\"\"Close database connection and dispose of engine.\"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Closing AzureSQL connection\",\n            server=self.server,\n            database=self.database,\n        )\n\n        if self._engine:\n            self._engine.dispose()\n            self._engine = None\n            ctx.info(\n                \"AzureSQL connection closed\",\n                server=self.server,\n                database=self.database,\n            )\n\n    def _get_error_suggestions(self, error_msg: str) -&gt; List[str]:\n        \"\"\"Generate suggestions based on error message.\"\"\"\n        suggestions = []\n        error_lower = error_msg.lower()\n\n        if \"login failed\" in error_lower:\n            suggestions.append(\"Check username and password\")\n            suggestions.append(f\"Verify auth_mode is correct (current: {self.auth_mode})\")\n            if \"identity\" in error_lower:\n                suggestions.append(\"Ensure Managed Identity has access to the database\")\n\n        if \"firewall\" in error_lower or \"tcp provider\" in error_lower:\n            suggestions.append(\"Check Azure SQL Server firewall rules\")\n            suggestions.append(\"Ensure client IP is allowed\")\n\n        if \"driver\" in error_lower:\n            suggestions.append(f\"Verify ODBC driver '{self.driver}' is installed\")\n            suggestions.append(\"On Linux: sudo apt-get install msodbcsql18\")\n\n        return suggestions\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        \"\"\"Get Spark JDBC options.\n\n        Returns:\n            Dictionary of Spark JDBC options (url, user, password, etc.)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Building Spark JDBC options\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        jdbc_url = (\n            f\"jdbc:sqlserver://{self.server}:{self.port};\"\n            f\"databaseName={self.database};encrypt=true;trustServerCertificate=true;\"\n        )\n\n        if self.auth_mode == \"aad_msi\":\n            jdbc_url += (\n                \"hostNameInCertificate=*.database.windows.net;\"\n                \"loginTimeout=30;authentication=ActiveDirectoryMsi;\"\n            )\n            ctx.debug(\n                \"Configured JDBC URL for AAD MSI\",\n                server=self.server,\n            )\n        elif self.auth_mode == \"aad_service_principal\":\n            # Not fully implemented in init yet, but placeholder\n            ctx.debug(\n                \"Configured JDBC URL for AAD Service Principal\",\n                server=self.server,\n            )\n\n        options = {\n            \"url\": jdbc_url,\n            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n        }\n\n        if self.auth_mode == \"sql\" or self.auth_mode == \"key_vault\":\n            if self.username:\n                options[\"user\"] = self.username\n\n            pwd = self.get_password()\n            if pwd:\n                options[\"password\"] = pwd\n\n            ctx.debug(\n                \"Added SQL authentication to Spark options\",\n                server=self.server,\n                username=self.username,\n            )\n\n        ctx.info(\n            \"Spark JDBC options built successfully\",\n            server=self.server,\n            database=self.database,\n        )\n\n        return options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.__init__","title":"<code>__init__(server, database, driver='ODBC Driver 18 for SQL Server', username=None, password=None, auth_mode='aad_msi', key_vault_name=None, secret_name=None, port=1433, timeout=30, **kwargs)</code>","text":"<p>Initialize Azure SQL connection.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>SQL server hostname (e.g., 'myserver.database.windows.net')</p> required <code>database</code> <code>str</code> <p>Database name</p> required <code>driver</code> <code>str</code> <p>ODBC driver name (default: ODBC Driver 18 for SQL Server)</p> <code>'ODBC Driver 18 for SQL Server'</code> <code>username</code> <code>Optional[str]</code> <p>SQL auth username (required if auth_mode='sql')</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>SQL auth password (required if auth_mode='sql')</p> <code>None</code> <code>auth_mode</code> <code>str</code> <p>Authentication mode ('aad_msi', 'sql', 'key_vault')</p> <code>'aad_msi'</code> <code>key_vault_name</code> <code>Optional[str]</code> <p>Key Vault name (required if auth_mode='key_vault')</p> <code>None</code> <code>secret_name</code> <code>Optional[str]</code> <p>Secret name containing password (required if auth_mode='key_vault')</p> <code>None</code> <code>port</code> <code>int</code> <p>SQL Server port (default: 1433)</p> <code>1433</code> <code>timeout</code> <code>int</code> <p>Connection timeout in seconds (default: 30)</p> <code>30</code> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def __init__(\n    self,\n    server: str,\n    database: str,\n    driver: str = \"ODBC Driver 18 for SQL Server\",\n    username: Optional[str] = None,\n    password: Optional[str] = None,\n    auth_mode: str = \"aad_msi\",  # \"aad_msi\", \"sql\", \"key_vault\"\n    key_vault_name: Optional[str] = None,\n    secret_name: Optional[str] = None,\n    port: int = 1433,\n    timeout: int = 30,\n    **kwargs,\n):\n    \"\"\"\n    Initialize Azure SQL connection.\n\n    Args:\n        server: SQL server hostname (e.g., 'myserver.database.windows.net')\n        database: Database name\n        driver: ODBC driver name (default: ODBC Driver 18 for SQL Server)\n        username: SQL auth username (required if auth_mode='sql')\n        password: SQL auth password (required if auth_mode='sql')\n        auth_mode: Authentication mode ('aad_msi', 'sql', 'key_vault')\n        key_vault_name: Key Vault name (required if auth_mode='key_vault')\n        secret_name: Secret name containing password (required if auth_mode='key_vault')\n        port: SQL Server port (default: 1433)\n        timeout: Connection timeout in seconds (default: 30)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"azure_sql\",\n        connection_name=f\"{server}/{database}\",\n        action=\"init\",\n        server=server,\n        database=database,\n        auth_mode=auth_mode,\n        port=port,\n    )\n\n    self.server = server\n    self.database = database\n    self.driver = driver\n    self.username = username\n    self.password = password\n    self.auth_mode = auth_mode\n    self.key_vault_name = key_vault_name\n    self.secret_name = secret_name\n    self.port = port\n    self.timeout = timeout\n    self._engine = None\n    self._cached_key = None  # For consistency with ADLS / parallel fetch\n\n    ctx.debug(\n        \"AzureSQL connection initialized\",\n        server=server,\n        database=database,\n        auth_mode=auth_mode,\n        driver=driver,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.close","title":"<code>close()</code>","text":"<p>Close database connection and dispose of engine.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def close(self):\n    \"\"\"Close database connection and dispose of engine.\"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Closing AzureSQL connection\",\n        server=self.server,\n        database=self.database,\n    )\n\n    if self._engine:\n        self._engine.dispose()\n        self._engine = None\n        ctx.info(\n            \"AzureSQL connection closed\",\n            server=self.server,\n            database=self.database,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.execute","title":"<code>execute(sql, params=None)</code>","text":"<p>Execute SQL statement (INSERT, UPDATE, DELETE, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL statement</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for parameterized query</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result from execution</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n    Args:\n        sql: SQL statement\n        params: Optional parameters for parameterized query\n\n    Returns:\n        Result from execution\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Executing SQL statement\",\n        server=self.server,\n        database=self.database,\n        statement_length=len(sql),\n    )\n\n    try:\n        engine = self.get_engine()\n        from sqlalchemy import text\n\n        with engine.connect() as conn:\n            result = conn.execute(text(sql), params or {})\n            conn.commit()\n\n            ctx.info(\n                \"SQL statement executed successfully\",\n                server=self.server,\n                database=self.database,\n            )\n            return result\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"SQL statement execution failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Statement execution failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_engine","title":"<code>get_engine()</code>","text":"<p>Get or create SQLAlchemy engine.</p> <p>Returns:</p> Type Description <code>Any</code> <p>SQLAlchemy engine instance</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection fails or drivers missing</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_engine(self) -&gt; Any:\n    \"\"\"\n    Get or create SQLAlchemy engine.\n\n    Returns:\n        SQLAlchemy engine instance\n\n    Raises:\n        ConnectionError: If connection fails or drivers missing\n    \"\"\"\n    ctx = get_logging_context()\n\n    if self._engine is not None:\n        ctx.debug(\n            \"Using cached SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n        )\n        return self._engine\n\n    ctx.debug(\n        \"Creating SQLAlchemy engine\",\n        server=self.server,\n        database=self.database,\n    )\n\n    try:\n        from urllib.parse import quote_plus\n\n        from sqlalchemy import create_engine\n    except ImportError as e:\n        ctx.error(\n            \"SQLAlchemy import failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=\"Required packages 'sqlalchemy' or 'pyodbc' not found.\",\n            suggestions=[\n                \"Install required packages: pip install sqlalchemy pyodbc\",\n                \"Or install odibi with azure extras: pip install 'odibi[azure]'\",\n            ],\n        )\n\n    try:\n        # Build connection string\n        conn_str = self.odbc_dsn()\n        connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}\"\n\n        ctx.debug(\n            \"Creating SQLAlchemy engine with connection pooling\",\n            server=self.server,\n            database=self.database,\n        )\n\n        # Create engine with connection pooling\n        self._engine = create_engine(\n            connection_url,\n            pool_pre_ping=True,  # Verify connections before use\n            pool_recycle=3600,  # Recycle connections after 1 hour\n            echo=False,\n        )\n\n        # Test connection\n        with self._engine.connect():\n            pass\n\n        ctx.info(\n            \"SQLAlchemy engine created successfully\",\n            server=self.server,\n            database=self.database,\n        )\n\n        return self._engine\n\n    except Exception as e:\n        suggestions = self._get_error_suggestions(str(e))\n        ctx.error(\n            \"Failed to create SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n            suggestions=suggestions,\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Failed to create engine: {str(e)}\",\n            suggestions=suggestions,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_password","title":"<code>get_password()</code>","text":"<p>Get password (cached).</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_password(self) -&gt; Optional[str]:\n    \"\"\"Get password (cached).\"\"\"\n    ctx = get_logging_context()\n\n    if self.password:\n        ctx.debug(\n            \"Using provided password\",\n            server=self.server,\n            database=self.database,\n        )\n        return self.password\n\n    if self._cached_key:\n        ctx.debug(\n            \"Using cached password\",\n            server=self.server,\n            database=self.database,\n        )\n        return self._cached_key\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"Key Vault mode requires key_vault_name and secret_name\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"key_vault mode requires key_vault_name and secret_name\")\n\n        ctx.debug(\n            \"Fetching password from Key Vault\",\n            server=self.server,\n            key_vault_name=self.key_vault_name,\n            secret_name=self.secret_name,\n        )\n\n        try:\n            from azure.identity import DefaultAzureCredential\n            from azure.keyvault.secrets import SecretClient\n\n            credential = DefaultAzureCredential()\n            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n            client = SecretClient(vault_url=kv_uri, credential=credential)\n            secret = client.get_secret(self.secret_name)\n            self._cached_key = secret.value\n            logger.register_secret(self._cached_key)\n\n            ctx.info(\n                \"Successfully fetched password from Key Vault\",\n                server=self.server,\n                key_vault_name=self.key_vault_name,\n            )\n            return self._cached_key\n        except ImportError as e:\n            ctx.error(\n                \"Key Vault support requires azure libraries\",\n                server=self.server,\n                error=str(e),\n            )\n            raise ImportError(\n                \"Key Vault support requires 'azure-identity' and 'azure-keyvault-secrets'\"\n            )\n\n    ctx.debug(\n        \"No password required for auth_mode\",\n        server=self.server,\n        auth_mode=self.auth_mode,\n    )\n    return None\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get table reference for relative path.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get table reference for relative path.\"\"\"\n    return relative_path\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_spark_options","title":"<code>get_spark_options()</code>","text":"<p>Get Spark JDBC options.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary of Spark JDBC options (url, user, password, etc.)</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_spark_options(self) -&gt; Dict[str, str]:\n    \"\"\"Get Spark JDBC options.\n\n    Returns:\n        Dictionary of Spark JDBC options (url, user, password, etc.)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Building Spark JDBC options\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    jdbc_url = (\n        f\"jdbc:sqlserver://{self.server}:{self.port};\"\n        f\"databaseName={self.database};encrypt=true;trustServerCertificate=true;\"\n    )\n\n    if self.auth_mode == \"aad_msi\":\n        jdbc_url += (\n            \"hostNameInCertificate=*.database.windows.net;\"\n            \"loginTimeout=30;authentication=ActiveDirectoryMsi;\"\n        )\n        ctx.debug(\n            \"Configured JDBC URL for AAD MSI\",\n            server=self.server,\n        )\n    elif self.auth_mode == \"aad_service_principal\":\n        # Not fully implemented in init yet, but placeholder\n        ctx.debug(\n            \"Configured JDBC URL for AAD Service Principal\",\n            server=self.server,\n        )\n\n    options = {\n        \"url\": jdbc_url,\n        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n    }\n\n    if self.auth_mode == \"sql\" or self.auth_mode == \"key_vault\":\n        if self.username:\n            options[\"user\"] = self.username\n\n        pwd = self.get_password()\n        if pwd:\n            options[\"password\"] = pwd\n\n        ctx.debug(\n            \"Added SQL authentication to Spark options\",\n            server=self.server,\n            username=self.username,\n        )\n\n    ctx.info(\n        \"Spark JDBC options built successfully\",\n        server=self.server,\n        database=self.database,\n    )\n\n    return options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.odbc_dsn","title":"<code>odbc_dsn()</code>","text":"<p>Build ODBC connection string.</p> <p>Returns:</p> Type Description <code>str</code> <p>ODBC DSN string</p> Example <p>conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\") conn.odbc_dsn() 'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def odbc_dsn(self) -&gt; str:\n    \"\"\"Build ODBC connection string.\n\n    Returns:\n        ODBC DSN string\n\n    Example:\n        &gt;&gt;&gt; conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\")\n        &gt;&gt;&gt; conn.odbc_dsn()\n        'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Building ODBC connection string\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    dsn = (\n        f\"Driver={{{self.driver}}};\"\n        f\"Server=tcp:{self.server},1433;\"\n        f\"Database={self.database};\"\n        f\"Encrypt=yes;\"\n        f\"TrustServerCertificate=yes;\"\n        f\"Connection Timeout=30;\"\n    )\n\n    pwd = self.get_password()\n    if self.username and pwd:\n        dsn += f\"UID={self.username};PWD={pwd};\"\n        ctx.debug(\n            \"Using SQL authentication\",\n            server=self.server,\n            username=self.username,\n        )\n    elif self.auth_mode == \"aad_msi\":\n        dsn += \"Authentication=ActiveDirectoryMsi;\"\n        ctx.debug(\n            \"Using AAD Managed Identity authentication\",\n            server=self.server,\n        )\n    elif self.auth_mode == \"aad_service_principal\":\n        # Not fully supported via ODBC string simply without token usually\n        ctx.debug(\n            \"Using AAD Service Principal authentication\",\n            server=self.server,\n        )\n\n    return dsn\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_sql","title":"<code>read_sql(query, params=None)</code>","text":"<p>Execute SQL query and return results as DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional query parameters for parameterized queries</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Query results as pandas DataFrame</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_sql(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute SQL query and return results as DataFrame.\n\n    Args:\n        query: SQL query string\n        params: Optional query parameters for parameterized queries\n\n    Returns:\n        Query results as pandas DataFrame\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Executing SQL query\",\n        server=self.server,\n        database=self.database,\n        query_length=len(query),\n    )\n\n    try:\n        engine = self.get_engine()\n        result = pd.read_sql(query, engine, params=params)\n\n        ctx.info(\n            \"SQL query executed successfully\",\n            server=self.server,\n            database=self.database,\n            rows_returned=len(result),\n        )\n        return result\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"SQL query execution failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Query execution failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_table","title":"<code>read_table(table_name, schema='dbo')</code>","text":"<p>Read entire table into DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>schema</code> <code>Optional[str]</code> <p>Schema name (default: dbo)</p> <code>'dbo'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table contents as pandas DataFrame</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_table(self, table_name: str, schema: Optional[str] = \"dbo\") -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire table into DataFrame.\n\n    Args:\n        table_name: Name of the table\n        schema: Schema name (default: dbo)\n\n    Returns:\n        Table contents as pandas DataFrame\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Reading table\",\n        server=self.server,\n        database=self.database,\n        table_name=table_name,\n        schema=schema,\n    )\n\n    if schema:\n        query = f\"SELECT * FROM [{schema}].[{table_name}]\"\n    else:\n        query = f\"SELECT * FROM [{table_name}]\"\n\n    return self.read_sql(query)\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.validate","title":"<code>validate()</code>","text":"<p>Validate Azure SQL connection configuration.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate Azure SQL connection configuration.\"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating AzureSQL connection\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    if not self.server:\n        ctx.error(\"AzureSQL validation failed: missing 'server'\")\n        raise ValueError(\"Azure SQL connection requires 'server'\")\n    if not self.database:\n        ctx.error(\n            \"AzureSQL validation failed: missing 'database'\",\n            server=self.server,\n        )\n        raise ValueError(\"Azure SQL connection requires 'database'\")\n\n    if self.auth_mode == \"sql\":\n        if not self.username:\n            ctx.error(\n                \"AzureSQL validation failed: SQL auth requires username\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"Azure SQL with auth_mode='sql' requires username\")\n        if not self.password and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"AzureSQL validation failed: SQL auth requires password\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                \"Azure SQL with auth_mode='sql' requires password \"\n                \"(or key_vault_name/secret_name)\"\n            )\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"AzureSQL validation failed: key_vault mode missing config\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                \"Azure SQL with auth_mode='key_vault' requires key_vault_name and secret_name\"\n            )\n        if not self.username:\n            ctx.error(\n                \"AzureSQL validation failed: key_vault mode requires username\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"Azure SQL with auth_mode='key_vault' requires username\")\n\n    ctx.info(\n        \"AzureSQL connection validated successfully\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.write_table","title":"<code>write_table(df, table_name, schema='dbo', if_exists='replace', index=False, chunksize=1000)</code>","text":"<p>Write DataFrame to SQL table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>schema</code> <code>Optional[str]</code> <p>Schema name (default: dbo)</p> <code>'dbo'</code> <code>if_exists</code> <code>str</code> <p>How to behave if table exists ('fail', 'replace', 'append')</p> <code>'replace'</code> <code>index</code> <code>bool</code> <p>Whether to write DataFrame index as column</p> <code>False</code> <code>chunksize</code> <code>Optional[int]</code> <p>Number of rows to write in each batch (default: 1000)</p> <code>1000</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows written</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If write fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def write_table(\n    self,\n    df: pd.DataFrame,\n    table_name: str,\n    schema: Optional[str] = \"dbo\",\n    if_exists: str = \"replace\",\n    index: bool = False,\n    chunksize: Optional[int] = 1000,\n) -&gt; int:\n    \"\"\"\n    Write DataFrame to SQL table.\n\n    Args:\n        df: DataFrame to write\n        table_name: Name of the table\n        schema: Schema name (default: dbo)\n        if_exists: How to behave if table exists ('fail', 'replace', 'append')\n        index: Whether to write DataFrame index as column\n        chunksize: Number of rows to write in each batch (default: 1000)\n\n    Returns:\n        Number of rows written\n\n    Raises:\n        ConnectionError: If write fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Writing DataFrame to table\",\n        server=self.server,\n        database=self.database,\n        table_name=table_name,\n        schema=schema,\n        rows=len(df),\n        if_exists=if_exists,\n        chunksize=chunksize,\n    )\n\n    try:\n        engine = self.get_engine()\n\n        rows_written = df.to_sql(\n            name=table_name,\n            con=engine,\n            schema=schema,\n            if_exists=if_exists,\n            index=index,\n            chunksize=chunksize,\n            method=\"multi\",  # Use multi-row INSERT for better performance\n        )\n\n        result_rows = rows_written if rows_written is not None else len(df)\n        ctx.info(\n            \"Table write completed successfully\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            rows_written=result_rows,\n        )\n        return result_rows\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"Table write failed\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Write operation failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/engine/","title":"Engine API","text":""},{"location":"reference/api/engine/#odibi.engine.base","title":"<code>odibi.engine.base</code>","text":"<p>Base engine interface.</p>"},{"location":"reference/api/engine/#odibi.engine.base.Engine","title":"<code>Engine</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for execution engines.</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>class Engine(ABC):\n    \"\"\"Abstract base class for execution engines.\"\"\"\n\n    # Custom format registry\n    _custom_readers: Dict[str, Any] = {}\n    _custom_writers: Dict[str, Any] = {}\n\n    @classmethod\n    def register_format(cls, fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None):\n        \"\"\"Register custom format reader/writer.\n\n        Args:\n            fmt: Format name (e.g. 'netcdf')\n            reader: Function(path, **options) -&gt; DataFrame\n            writer: Function(df, path, **options) -&gt; None\n        \"\"\"\n        if reader:\n            cls._custom_readers[fmt] = reader\n        if writer:\n            cls._custom_writers[fmt] = writer\n\n    @abstractmethod\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n    ) -&gt; Any:\n        \"\"\"Read data from source.\n\n        Args:\n            connection: Connection object\n            format: Data format (csv, parquet, delta, etc.)\n            table: Table name (for SQL/Delta)\n            path: File path (for file-based sources)\n            options: Format-specific options\n\n        Returns:\n            DataFrame (engine-specific type)\n        \"\"\"\n        pass\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Materialized DataFrame\n        \"\"\"\n        return df\n\n    @abstractmethod\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Write data to destination.\n\n        Args:\n            df: DataFrame to write\n            connection: Connection object\n            format: Output format\n            table: Table name (for SQL/Delta)\n            path: File path (for file-based outputs)\n            mode: Write mode (overwrite/append)\n            options: Format-specific options\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_sql(self, sql: str, context: Context) -&gt; Any:\n        \"\"\"Execute SQL query.\n\n        Args:\n            sql: SQL query string\n            context: Execution context with registered DataFrames\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n        \"\"\"Execute built-in operation (pivot, etc.).\n\n        Args:\n            operation: Operation name\n            params: Operation parameters\n            df: Input DataFrame\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_schema(self, df: Any) -&gt; Any:\n        \"\"\"Get DataFrame schema.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dict[str, str] mapping column names to types, or List[str] of names (deprecated)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            (rows, columns)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Row count\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\n\n        Args:\n            df: DataFrame\n            columns: Columns to check\n\n        Returns:\n            Dictionary of column -&gt; null count\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\n\n        Args:\n            df: DataFrame\n            schema_rules: Validation rules\n\n        Returns:\n            List of validation failures (empty if valid)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate data against rules.\n\n        Args:\n            df: DataFrame to validate\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages (empty if valid)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\n\n        Args:\n            df: DataFrame\n            n: Number of rows to return\n\n        Returns:\n            List of row dictionaries\n        \"\"\"\n        pass\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            List of file paths (or empty list if not applicable/supported)\n        \"\"\"\n        return []\n\n    def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dictionary of {column_name: null_percentage} (0.0 to 1.0)\n        \"\"\"\n        return {}\n\n    @abstractmethod\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Args:\n            connection: Connection object\n            table: Table name (for catalog tables)\n            path: File path (for path-based tables)\n\n        Returns:\n            True if table/location exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\n\n        Args:\n            df: Input DataFrame\n            target_schema: Target schema (column name -&gt; type)\n            policy: SchemaPolicyConfig object\n\n        Returns:\n            Harmonized DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; Any:\n        \"\"\"Anonymize specified columns.\n\n        Args:\n            df: DataFrame to anonymize\n            columns: List of columns to anonymize\n            method: Method ('hash', 'mask', 'redact')\n            salt: Optional salt for hashing\n\n        Returns:\n            Anonymized DataFrame\n        \"\"\"\n        pass\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\n\n        Args:\n            connection: Connection object\n            table: Table name\n            path: File path\n            format: Data format (optional, helps with file-based sources)\n\n        Returns:\n            Schema dict or None if table doesn't exist or schema fetch fails.\n        \"\"\"\n        return None\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\n\n        Args:\n            connection: Connection object\n            format: Table format\n            table: Table name\n            path: Table path\n            config: AutoOptimizeConfig object\n        \"\"\"\n        pass\n\n    def add_write_metadata(\n        self,\n        df: Any,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; Any:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added (or unchanged if metadata_config is None/False)\n        \"\"\"\n        return df  # Default: no-op\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with metadata columns added (or unchanged if metadata_config is None/False)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: Any,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; Any:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added (or unchanged if metadata_config is None/False)\n    \"\"\"\n    return df  # Default: no-op\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>  <code>abstractmethod</code>","text":"<p>Anonymize specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to anonymize</p> required <code>columns</code> <code>List[str]</code> <p>List of columns to anonymize</p> required <code>method</code> <code>str</code> <p>Method ('hash', 'mask', 'redact')</p> required <code>salt</code> <code>Optional[str]</code> <p>Optional salt for hashing</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Anonymized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; Any:\n    \"\"\"Anonymize specified columns.\n\n    Args:\n        df: DataFrame to anonymize\n        columns: List of columns to anonymize\n        method: Method ('hash', 'mask', 'redact')\n        salt: Optional salt for hashing\n\n    Returns:\n        Anonymized DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.count_nulls","title":"<code>count_nulls(df, columns)</code>  <code>abstractmethod</code>","text":"<p>Count nulls in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>columns</code> <code>List[str]</code> <p>Columns to check</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary of column -&gt; null count</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\n\n    Args:\n        df: DataFrame\n        columns: Columns to check\n\n    Returns:\n        Dictionary of column -&gt; null count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.count_rows","title":"<code>count_rows(df)</code>  <code>abstractmethod</code>","text":"<p>Count rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>int</code> <p>Row count</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Row count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>  <code>abstractmethod</code>","text":"<p>Execute built-in operation (pivot, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Operation name</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Operation parameters</p> required <code>df</code> <code>Any</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n    \"\"\"Execute built-in operation (pivot, etc.).\n\n    Args:\n        operation: Operation name\n        params: Operation parameters\n        df: Input DataFrame\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.execute_sql","title":"<code>execute_sql(sql, context)</code>  <code>abstractmethod</code>","text":"<p>Execute SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context with registered DataFrames</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef execute_sql(self, sql: str, context: Context) -&gt; Any:\n    \"\"\"Execute SQL query.\n\n    Args:\n        sql: SQL query string\n        context: Execution context with registered DataFrames\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_sample","title":"<code>get_sample(df, n=10)</code>  <code>abstractmethod</code>","text":"<p>Get sample rows as list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>n</code> <code>int</code> <p>Number of rows to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of row dictionaries</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\n\n    Args:\n        df: DataFrame\n        n: Number of rows to return\n\n    Returns:\n        List of row dictionaries\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_schema","title":"<code>get_schema(df)</code>  <code>abstractmethod</code>","text":"<p>Get DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dict[str, str] mapping column names to types, or List[str] of names (deprecated)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_schema(self, df: Any) -&gt; Any:\n    \"\"\"Get DataFrame schema.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dict[str, str] mapping column names to types, or List[str] of names (deprecated)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_shape","title":"<code>get_shape(df)</code>  <code>abstractmethod</code>","text":"<p>Get DataFrame shape.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(rows, columns)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        (rows, columns)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths (or empty list if not applicable/supported)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        List of file paths (or empty list if not applicable/supported)\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Data format (optional, helps with file-based sources)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, str]]</code> <p>Schema dict or None if table doesn't exist or schema fetch fails.</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\n\n    Args:\n        connection: Connection object\n        table: Table name\n        path: File path\n        format: Data format (optional, helps with file-based sources)\n\n    Returns:\n        Schema dict or None if table doesn't exist or schema fetch fails.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>  <code>abstractmethod</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Input DataFrame</p> required <code>target_schema</code> <code>Dict[str, str]</code> <p>Target schema (column name -&gt; type)</p> required <code>policy</code> <code>Any</code> <p>SchemaPolicyConfig object</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Harmonized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\n\n    Args:\n        df: Input DataFrame\n        target_schema: Target schema (column name -&gt; type)\n        policy: SchemaPolicyConfig object\n\n    Returns:\n        Harmonized DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Table format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>Table path</p> <code>None</code> <code>config</code> <code>Optional[Any]</code> <p>AutoOptimizeConfig object</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\n\n    Args:\n        connection: Connection object\n        format: Table format\n        table: Table name\n        path: Table path\n        config: AutoOptimizeConfig object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset into memory (DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Materialized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Materialized DataFrame\n    \"\"\"\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of {column_name: null_percentage} (0.0 to 1.0)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dictionary of {column_name: null_percentage} (0.0 to 1.0)\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.read","title":"<code>read(connection, format, table=None, path=None, options=None)</code>  <code>abstractmethod</code>","text":"<p>Read data from source.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Data format (csv, parquet, delta, etc.)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for SQL/Delta)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for file-based sources)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame (engine-specific type)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n) -&gt; Any:\n    \"\"\"Read data from source.\n\n    Args:\n        connection: Connection object\n        format: Data format (csv, parquet, delta, etc.)\n        table: Table name (for SQL/Delta)\n        path: File path (for file-based sources)\n        options: Format-specific options\n\n    Returns:\n        DataFrame (engine-specific type)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.register_format","title":"<code>register_format(fmt, reader=None, writer=None)</code>  <code>classmethod</code>","text":"<p>Register custom format reader/writer.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>str</code> <p>Format name (e.g. 'netcdf')</p> required <code>reader</code> <code>Optional[Any]</code> <p>Function(path, **options) -&gt; DataFrame</p> <code>None</code> <code>writer</code> <code>Optional[Any]</code> <p>Function(df, path, **options) -&gt; None</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@classmethod\ndef register_format(cls, fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None):\n    \"\"\"Register custom format reader/writer.\n\n    Args:\n        fmt: Format name (e.g. 'netcdf')\n        reader: Function(path, **options) -&gt; DataFrame\n        writer: Function(df, path, **options) -&gt; None\n    \"\"\"\n    if reader:\n        cls._custom_readers[fmt] = reader\n    if writer:\n        cls._custom_writers[fmt] = writer\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>  <code>abstractmethod</code>","text":"<p>Check if table or location exists.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for catalog tables)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for path-based tables)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if table/location exists, False otherwise</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Args:\n        connection: Connection object\n        table: Table name (for catalog tables)\n        path: File path (for path-based tables)\n\n    Returns:\n        True if table/location exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.validate_data","title":"<code>validate_data(df, validation_config)</code>  <code>abstractmethod</code>","text":"<p>Validate data against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to validate</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages (empty if valid)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate data against rules.\n\n    Args:\n        df: DataFrame to validate\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages (empty if valid)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>  <code>abstractmethod</code>","text":"<p>Validate DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>schema_rules</code> <code>Dict[str, Any]</code> <p>Validation rules</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failures (empty if valid)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\n\n    Args:\n        df: DataFrame\n        schema_rules: Validation rules\n\n    Returns:\n        List of validation failures (empty if valid)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.write","title":"<code>write(df, connection, format, table=None, path=None, mode='overwrite', options=None, streaming_config=None)</code>  <code>abstractmethod</code>","text":"<p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to write</p> required <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Output format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for SQL/Delta)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for file-based outputs)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Write mode (overwrite/append)</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Write data to destination.\n\n    Args:\n        df: DataFrame to write\n        connection: Connection object\n        format: Output format\n        table: Table name (for SQL/Delta)\n        path: File path (for file-based outputs)\n        mode: Write mode (overwrite/append)\n        options: Format-specific options\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine","title":"<code>odibi.engine.pandas_engine</code>","text":"<p>Pandas engine implementation.</p>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.LazyDataset","title":"<code>LazyDataset</code>  <code>dataclass</code>","text":"<p>Lazy representation of a dataset (file) for out-of-core processing.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>@dataclass\nclass LazyDataset:\n    \"\"\"Lazy representation of a dataset (file) for out-of-core processing.\"\"\"\n\n    path: Union[str, List[str]]\n    format: str\n    options: Dict[str, Any]\n    connection: Optional[Any] = None  # To resolve path/credentials if needed\n\n    def __repr__(self):\n        return f\"LazyDataset(path={self.path}, format={self.format})\"\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine","title":"<code>PandasEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Pandas-based execution engine.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>class PandasEngine(Engine):\n    \"\"\"Pandas-based execution engine.\"\"\"\n\n    name = \"pandas\"\n    engine_type = EngineType.PANDAS\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Pandas engine.\n\n        Args:\n            connections: Dictionary of connection objects\n            config: Engine configuration (optional)\n        \"\"\"\n        self.connections = connections or {}\n        self.config = config or {}\n\n        # Suppress noisy delta-rs transaction conflict warnings (handled by retry)\n        if \"RUST_LOG\" not in os.environ:\n            os.environ[\"RUST_LOG\"] = \"deltalake_core::kernel::transaction=error\"\n\n        # Check for performance flags\n        performance = self.config.get(\"performance\", {})\n\n        # Determine desired state\n        if hasattr(performance, \"use_arrow\"):\n            desired_use_arrow = performance.use_arrow\n        elif isinstance(performance, dict):\n            desired_use_arrow = performance.get(\"use_arrow\", True)\n        else:\n            desired_use_arrow = True\n\n        # Verify availability\n        if desired_use_arrow:\n            try:\n                import pyarrow  # noqa: F401\n\n                self.use_arrow = True\n            except ImportError:\n                import logging\n\n                logger = logging.getLogger(__name__)\n                logger.warning(\n                    \"Apache Arrow not found. Disabling Arrow optimizations. \"\n                    \"Install 'pyarrow' to enable.\"\n                )\n                self.use_arrow = False\n        else:\n            self.use_arrow = False\n\n        # Check for DuckDB\n        self.use_duckdb = False\n        # Default to False to ensure stability with existing tests (Lazy Loading is opt-in)\n        if self.config.get(\"performance\", {}).get(\"use_duckdb\", False):\n            try:\n                import duckdb  # noqa: F401\n\n                self.use_duckdb = True\n            except ImportError:\n                pass\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset.\"\"\"\n        if isinstance(df, LazyDataset):\n            # Re-invoke read but force materialization (by bypassing Lazy check)\n            # We pass the resolved path directly\n            # Note: We need to handle the case where path was resolved.\n            # LazyDataset.path should be the FULL path.\n            return self._read_file(\n                full_path=df.path, format=df.format, options=df.options, connection=df.connection\n            )\n        return df\n\n    def _process_df(\n        self, df: Union[pd.DataFrame, Iterator[pd.DataFrame]], query: Optional[str]\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Apply post-read processing (filtering).\"\"\"\n        if query and df is not None:\n            # Handle Iterator\n            from collections.abc import Iterator\n\n            if isinstance(df, Iterator):\n                # Filter each chunk\n                return (chunk.query(query) for chunk in df)\n\n            if not df.empty:\n                try:\n                    return df.query(query)\n                except Exception as e:\n                    import logging\n\n                    logger = logging.getLogger(__name__)\n                    logger.warning(f\"Failed to apply query '{query}': {e}\")\n        return df\n\n    _CLOUD_URI_PREFIXES = (\"abfss://\", \"s3://\", \"gs://\", \"az://\", \"https://\")\n\n    def _retry_delta_operation(self, func, max_retries: int = 5, base_delay: float = 0.2):\n        \"\"\"Retry Delta operations with exponential backoff for concurrent conflicts.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                return func()\n            except Exception as e:\n                error_str = str(e).lower()\n                is_conflict = \"conflict\" in error_str or \"concurrent\" in error_str\n                if attempt == max_retries - 1 or not is_conflict:\n                    raise\n                delay = base_delay * (2**attempt) + random.uniform(0, 0.1)\n                time.sleep(delay)\n\n    def _resolve_path(self, path: Optional[str], connection: Any) -&gt; str:\n        \"\"\"Resolve path to full URI, avoiding double-prefixing for cloud URIs.\n\n        Args:\n            path: Relative or absolute path\n            connection: Connection object (may have get_path method)\n\n        Returns:\n            Full resolved path\n        \"\"\"\n        if not path:\n            raise ValueError(\"Path must be provided\")\n        if path.startswith(self._CLOUD_URI_PREFIXES):\n            return path\n        if connection:\n            return connection.get_path(path)\n        return path\n\n    def _merge_storage_options(\n        self, connection: Any, options: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Merge connection storage options with user options.\n\n        Args:\n            connection: Connection object (may have pandas_storage_options method)\n            options: User-provided options\n\n        Returns:\n            Merged options dictionary\n        \"\"\"\n        options = options or {}\n\n        # If connection provides storage_options (e.g., AzureADLS), merge them\n        if hasattr(connection, \"pandas_storage_options\"):\n            conn_storage_opts = connection.pandas_storage_options()\n            user_storage_opts = options.get(\"storage_options\", {})\n\n            # User options override connection options\n            merged_storage_opts = {**conn_storage_opts, **user_storage_opts}\n\n            # Return options with merged storage_options\n            return {**options, \"storage_options\": merged_storage_opts}\n\n        return options\n\n    def _read_parallel(self, read_func: Any, paths: List[str], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Read multiple files in parallel using threads.\n\n        Args:\n            read_func: Pandas read function (e.g. pd.read_csv)\n            paths: List of file paths\n            kwargs: Arguments to pass to read_func\n\n        Returns:\n            Concatenated DataFrame\n        \"\"\"\n        # Conservative worker count to avoid OOM on large files\n        max_workers = min(8, os.cpu_count() or 4)\n\n        dfs = []\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # map preserves order\n            results = executor.map(lambda p: read_func(p, **kwargs), paths)\n            dfs = list(results)\n\n        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        as_of_version: Optional[int] = None,\n        as_of_timestamp: Optional[str] = None,\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Read data using Pandas (or LazyDataset).\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        source = path or table\n        ctx.debug(\n            \"Starting read operation\",\n            format=format,\n            path=source,\n            streaming=streaming,\n            use_arrow=self.use_arrow,\n        )\n\n        if streaming:\n            ctx.error(\n                \"Streaming not supported in Pandas engine\",\n                format=format,\n                path=source,\n            )\n            raise ValueError(\n                \"Streaming is not supported in the Pandas engine. \"\n                \"Please use 'engine: spark' for streaming pipelines.\"\n            )\n\n        options = options or {}\n\n        # Resolve full path from connection\n        try:\n            full_path = self._resolve_path(path or table, connection)\n        except ValueError:\n            if table and not connection:\n                ctx.error(\"Connection required when specifying 'table'\", table=table)\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n            ctx.error(\"Neither path nor table provided for read operation\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Merge storage options for cloud connections\n        merged_options = self._merge_storage_options(connection, options)\n\n        # Sanitize options for pandas compatibility\n        if \"header\" in merged_options:\n            if merged_options[\"header\"] is True:\n                merged_options[\"header\"] = 0\n            elif merged_options[\"header\"] is False:\n                merged_options[\"header\"] = None\n\n        # Handle Time Travel options\n        if as_of_version is not None:\n            merged_options[\"versionAsOf\"] = as_of_version\n            ctx.debug(\"Time travel enabled\", version=as_of_version)\n        if as_of_timestamp is not None:\n            merged_options[\"timestampAsOf\"] = as_of_timestamp\n            ctx.debug(\"Time travel enabled\", timestamp=as_of_timestamp)\n\n        # Check for Lazy/DuckDB optimization\n        can_lazy_load = False\n\n        if can_lazy_load:\n            ctx.debug(\"Using lazy loading via DuckDB\", path=str(full_path))\n            if isinstance(full_path, (str, Path)):\n                return LazyDataset(\n                    path=str(full_path),\n                    format=format,\n                    options=merged_options,\n                    connection=connection,\n                )\n            elif isinstance(full_path, list):\n                return LazyDataset(\n                    path=full_path, format=format, options=merged_options, connection=connection\n                )\n\n        result = self._read_file(full_path, format, merged_options, connection)\n\n        # Log metrics for materialized DataFrames\n        elapsed = (time.time() - start) * 1000\n        if isinstance(result, pd.DataFrame):\n            row_count = len(result)\n            memory_mb = result.memory_usage(deep=True).sum() / (1024 * 1024)\n\n            ctx.log_file_io(\n                path=str(full_path) if not isinstance(full_path, list) else str(full_path[0]),\n                format=format,\n                mode=\"read\",\n                rows=row_count,\n            )\n            ctx.log_pandas_metrics(\n                memory_mb=memory_mb,\n                dtypes={col: str(dtype) for col, dtype in result.dtypes.items()},\n            )\n            ctx.info(\n                \"Read completed\",\n                format=format,\n                rows=row_count,\n                elapsed_ms=round(elapsed, 2),\n                memory_mb=round(memory_mb, 2),\n            )\n\n        return result\n\n    def _read_file(\n        self,\n        full_path: Union[str, List[str], Any],\n        format: str,\n        options: Dict[str, Any],\n        connection: Any = None,\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Internal file reading logic.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        ctx.debug(\n            \"Reading file\",\n            path=str(full_path) if not isinstance(full_path, list) else f\"{len(full_path)} files\",\n            format=format,\n        )\n\n        # Custom Readers\n        if format in self._custom_readers:\n            ctx.debug(f\"Using custom reader for format: {format}\")\n            return self._custom_readers[format](full_path, **options)\n\n        # Handle glob patterns for local files\n        is_glob = False\n        if isinstance(full_path, (str, Path)) and (\n            \"*\" in str(full_path) or \"?\" in str(full_path) or \"[\" in str(full_path)\n        ):\n            parsed = urlparse(str(full_path))\n            # Only expand for local files (no scheme, file://, or drive letter)\n            is_local = (\n                not parsed.scheme\n                or parsed.scheme == \"file\"\n                or (len(parsed.scheme) == 1 and parsed.scheme.isalpha())\n            )\n\n            if is_local:\n                glob_path = str(full_path)\n                if glob_path.startswith(\"file:///\"):\n                    glob_path = glob_path[8:]\n                elif glob_path.startswith(\"file://\"):\n                    glob_path = glob_path[7:]\n\n                matched_files = glob.glob(glob_path)\n                if not matched_files:\n                    ctx.error(\n                        \"No files matched glob pattern\",\n                        pattern=glob_path,\n                    )\n                    raise FileNotFoundError(f\"No files matched pattern: {glob_path}\")\n\n                ctx.info(\n                    \"Glob pattern expanded\",\n                    pattern=glob_path,\n                    matched_files=len(matched_files),\n                )\n                full_path = matched_files\n                is_glob = True\n\n        # Prepare read options (options already includes storage_options from caller)\n        read_kwargs = options.copy()\n\n        # Extract 'query' or 'filter' option for post-read filtering\n        post_read_query = read_kwargs.pop(\"query\", None) or read_kwargs.pop(\"filter\", None)\n\n        if self.use_arrow:\n            read_kwargs[\"dtype_backend\"] = \"pyarrow\"\n\n        # Read based on format\n        if format == \"csv\":\n            try:\n                if is_glob and isinstance(full_path, list):\n                    ctx.debug(\n                        \"Parallel CSV read\",\n                        file_count=len(full_path),\n                    )\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n            except UnicodeDecodeError:\n                ctx.warning(\n                    \"UnicodeDecodeError, retrying with latin1 encoding\",\n                    path=str(full_path),\n                )\n                read_kwargs[\"encoding\"] = \"latin1\"\n                if is_glob and isinstance(full_path, list):\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n            except pd.errors.ParserError:\n                ctx.warning(\n                    \"ParserError, retrying with on_bad_lines='skip'\",\n                    path=str(full_path),\n                )\n                read_kwargs[\"on_bad_lines\"] = \"skip\"\n                if is_glob and isinstance(full_path, list):\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n        elif format == \"parquet\":\n            ctx.debug(\"Reading parquet\", path=str(full_path))\n            df = pd.read_parquet(full_path, **read_kwargs)\n            if isinstance(full_path, list):\n                df.attrs[\"odibi_source_files\"] = full_path\n            else:\n                df.attrs[\"odibi_source_files\"] = [str(full_path)]\n            return self._process_df(df, post_read_query)\n        elif format == \"json\":\n            if is_glob and isinstance(full_path, list):\n                ctx.debug(\n                    \"Parallel JSON read\",\n                    file_count=len(full_path),\n                )\n                df = self._read_parallel(pd.read_json, full_path, **read_kwargs)\n                df.attrs[\"odibi_source_files\"] = full_path\n                return self._process_df(df, post_read_query)\n\n            df = pd.read_json(full_path, **read_kwargs)\n            if hasattr(df, \"attrs\"):\n                df.attrs[\"odibi_source_files\"] = [str(full_path)]\n            return self._process_df(df, post_read_query)\n        elif format == \"excel\":\n            ctx.debug(\"Reading Excel file\", path=str(full_path))\n            read_kwargs.pop(\"dtype_backend\", None)\n            return self._process_df(pd.read_excel(full_path, **read_kwargs), post_read_query)\n        elif format == \"delta\":\n            ctx.debug(\"Reading Delta table\", path=str(full_path))\n            try:\n                from deltalake import DeltaTable\n            except ImportError:\n                ctx.error(\n                    \"Delta Lake library not installed\",\n                    path=str(full_path),\n                )\n                raise ImportError(\n                    \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                    \"or 'pip install deltalake'. See README.md for installation instructions.\"\n                )\n\n            storage_opts = options.get(\"storage_options\", {})\n            version = options.get(\"versionAsOf\")\n            timestamp = options.get(\"timestampAsOf\")\n\n            if timestamp is not None:\n                from datetime import datetime as dt_module\n\n                if isinstance(timestamp, str):\n                    ts = dt_module.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n                else:\n                    ts = timestamp\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                dt.load_with_datetime(ts)\n                ctx.debug(\"Delta table loaded with timestamp\", timestamp=str(ts))\n            elif version is not None:\n                dt = DeltaTable(full_path, storage_options=storage_opts, version=version)\n                ctx.debug(\"Delta table loaded with version\", version=version)\n            else:\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                ctx.debug(\"Delta table loaded (latest version)\")\n\n            if self.use_arrow:\n                import inspect\n\n                sig = inspect.signature(dt.to_pandas)\n\n                if \"arrow_options\" in sig.parameters:\n                    return self._process_df(\n                        dt.to_pandas(\n                            partitions=None, arrow_options={\"types_mapper\": pd.ArrowDtype}\n                        ),\n                        post_read_query,\n                    )\n                else:\n                    return self._process_df(\n                        dt.to_pyarrow_table().to_pandas(types_mapper=pd.ArrowDtype),\n                        post_read_query,\n                    )\n            else:\n                return self._process_df(dt.to_pandas(), post_read_query)\n        elif format == \"avro\":\n            ctx.debug(\"Reading Avro file\", path=str(full_path))\n            try:\n                import fastavro\n            except ImportError:\n                ctx.error(\n                    \"fastavro library not installed\",\n                    path=str(full_path),\n                )\n                raise ImportError(\n                    \"Avro support requires 'pip install odibi[pandas]' \"\n                    \"or 'pip install fastavro'. See README.md for installation instructions.\"\n                )\n\n            parsed = urlparse(full_path)\n            if parsed.scheme and parsed.scheme not in [\"file\", \"\"]:\n                import fsspec\n\n                storage_opts = options.get(\"storage_options\", {})\n                with fsspec.open(full_path, \"rb\", **storage_opts) as f:\n                    reader = fastavro.reader(f)\n                    records = [record for record in reader]\n                return pd.DataFrame(records)\n            else:\n                with open(full_path, \"rb\") as f:\n                    reader = fastavro.reader(f)\n                    records = [record for record in reader]\n                return self._process_df(pd.DataFrame(records), post_read_query)\n        elif format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            ctx.debug(\"Reading SQL table\", table=str(full_path), format=format)\n            if not hasattr(connection, \"read_table\"):\n                ctx.error(\n                    \"Connection does not support SQL operations\",\n                    connection_type=type(connection).__name__,\n                )\n                raise ValueError(\n                    f\"Connection type '{type(connection).__name__}' does not support SQL operations\"\n                )\n\n            table_name = str(full_path)\n            if \".\" in table_name:\n                schema, tbl = table_name.split(\".\", 1)\n            else:\n                schema, tbl = \"dbo\", table_name\n\n            ctx.debug(\"Executing SQL read\", schema=schema, table=tbl)\n            return connection.read_table(table_name=tbl, schema=schema)\n        else:\n            ctx.error(\"Unsupported format\", format=format)\n            raise ValueError(f\"Unsupported format for Pandas engine: {format}\")\n\n    def write(\n        self,\n        df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Pandas.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        destination = path or table\n        ctx.debug(\n            \"Starting write operation\",\n            format=format,\n            destination=destination,\n            mode=mode,\n        )\n\n        # Ensure materialization if LazyDataset\n        df = self.materialize(df)\n\n        options = options or {}\n\n        # Handle iterator/generator input\n        from collections.abc import Iterator\n\n        if isinstance(df, Iterator):\n            ctx.debug(\"Writing iterator/generator input\")\n            return self._write_iterator(df, connection, format, table, path, mode, options)\n\n        row_count = len(df)\n        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n\n        ctx.log_pandas_metrics(\n            memory_mb=memory_mb,\n            dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n        )\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            ctx.debug(\"Writing to SQL\", table=table, mode=mode)\n            return self._write_sql(df, connection, table, mode, options)\n\n        # Resolve full path from connection\n        try:\n            full_path = self._resolve_path(path or table, connection)\n        except ValueError:\n            if table and not connection:\n                ctx.error(\"Connection required when specifying 'table'\", table=table)\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n            ctx.error(\"Neither path nor table provided for write operation\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Merge storage options for cloud connections\n        merged_options = self._merge_storage_options(connection, options)\n\n        # Custom Writers\n        if format in self._custom_writers:\n            ctx.debug(f\"Using custom writer for format: {format}\")\n            writer_options = merged_options.copy()\n            writer_options.pop(\"keys\", None)\n            self._custom_writers[format](df, full_path, mode=mode, **writer_options)\n            return None\n\n        # Ensure directory exists (local only)\n        self._ensure_directory(full_path)\n\n        # Warn about partitioning\n        self._check_partitioning(merged_options)\n\n        # Delta Lake Write\n        if format == \"delta\":\n            ctx.debug(\"Writing Delta table\", path=str(full_path), mode=mode)\n            result = self._write_delta(df, full_path, mode, merged_options)\n            elapsed = (time.time() - start) * 1000\n            ctx.log_file_io(\n                path=str(full_path),\n                format=format,\n                mode=mode,\n                rows=row_count,\n            )\n            ctx.info(\n                \"Write completed\",\n                format=format,\n                rows=row_count,\n                elapsed_ms=round(elapsed, 2),\n            )\n            return result\n\n        # Handle Generic Upsert/Append-Once for non-Delta\n        if mode in [\"upsert\", \"append_once\"]:\n            ctx.debug(f\"Handling {mode} mode for non-Delta format\")\n            df, mode = self._handle_generic_upsert(df, full_path, format, mode, merged_options)\n            row_count = len(df)\n\n        # Standard File Write\n        result = self._write_file(df, full_path, format, mode, merged_options)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.log_file_io(\n            path=str(full_path),\n            format=format,\n            mode=mode,\n            rows=row_count,\n        )\n        ctx.info(\n            \"Write completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return result\n\n    def _write_iterator(\n        self,\n        df_iter: Iterator[pd.DataFrame],\n        connection: Any,\n        format: str,\n        table: Optional[str],\n        path: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle writing of iterator/generator.\"\"\"\n        first_chunk = True\n        for chunk in df_iter:\n            # Determine mode for this chunk\n            current_mode = mode if first_chunk else \"append\"\n            current_options = options.copy()\n\n            # Handle CSV header for chunks\n            if not first_chunk and format == \"csv\":\n                if current_options.get(\"header\") is not False:\n                    current_options[\"header\"] = False\n\n            self.write(\n                chunk,\n                connection,\n                format,\n                table,\n                path,\n                mode=current_mode,\n                options=current_options,\n            )\n            first_chunk = False\n        return None\n\n    def _write_sql(\n        self,\n        df: pd.DataFrame,\n        connection: Any,\n        table: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle SQL writing.\"\"\"\n        if not hasattr(connection, \"write_table\"):\n            raise ValueError(\n                f\"Connection type '{type(connection).__name__}' does not support SQL operations\"\n            )\n\n        if not table:\n            raise ValueError(\"SQL format requires 'table' config\")\n\n        # Extract schema from table name if present\n        if \".\" in table:\n            schema, table_name = table.split(\".\", 1)\n        else:\n            schema, table_name = \"dbo\", table\n\n        # Map mode to if_exists\n        if_exists = \"replace\"  # overwrite\n        if mode == \"append\":\n            if_exists = \"append\"\n        elif mode == \"fail\":\n            if_exists = \"fail\"\n\n        chunksize = options.get(\"chunksize\", 1000)\n\n        connection.write_table(\n            df=df,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n        return None\n\n    def _ensure_directory(self, full_path: str) -&gt; None:\n        \"\"\"Ensure parent directory exists for local files.\"\"\"\n        parsed = urlparse(str(full_path))\n        is_windows_drive = (\n            len(parsed.scheme) == 1 and parsed.scheme.isalpha() if parsed.scheme else False\n        )\n\n        if not parsed.scheme or parsed.scheme == \"file\" or is_windows_drive:\n            Path(full_path).parent.mkdir(parents=True, exist_ok=True)\n\n    def _check_partitioning(self, options: Dict[str, Any]) -&gt; None:\n        \"\"\"Warn about potential partitioning issues.\"\"\"\n        partition_by = options.get(\"partition_by\") or options.get(\"partitionBy\")\n        if partition_by:\n            import warnings\n\n            warnings.warn(\n                \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n                \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n                \"and ensure each partition has &gt; 1000 rows.\",\n                UserWarning,\n            )\n\n    def _write_delta(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        mode: str,\n        merged_options: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Handle Delta Lake writing.\"\"\"\n        try:\n            from deltalake import DeltaTable, write_deltalake\n        except ImportError:\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' or 'pip install deltalake'. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        storage_opts = merged_options.get(\"storage_options\", {})\n\n        # Handle null-only columns: Delta Lake doesn't support Null dtype\n        # Cast columns with all-null values to string to avoid schema errors\n        for col in df.columns:\n            if df[col].isna().all():\n                df[col] = df[col].astype(\"string\")\n\n        # Map modes\n        delta_mode = \"overwrite\"\n        if mode == \"append\":\n            delta_mode = \"append\"\n        elif mode == \"error\" or mode == \"fail\":\n            delta_mode = \"error\"\n        elif mode == \"ignore\":\n            delta_mode = \"ignore\"\n\n        # Handle upsert/append_once logic\n        if mode == \"upsert\":\n            keys = merged_options.get(\"keys\")\n            if not keys:\n                raise ValueError(\"Upsert requires 'keys' in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            def do_upsert():\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                (\n                    dt.merge(\n                        source=df,\n                        predicate=\" AND \".join([f\"s.{k} = t.{k}\" for k in keys]),\n                        source_alias=\"s\",\n                        target_alias=\"t\",\n                    )\n                    .when_matched_update_all()\n                    .when_not_matched_insert_all()\n                    .execute()\n                )\n\n            self._retry_delta_operation(do_upsert)\n        elif mode == \"append_once\":\n            keys = merged_options.get(\"keys\")\n            if not keys:\n                raise ValueError(\"Append_once requires 'keys' in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            def do_append_once():\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                (\n                    dt.merge(\n                        source=df,\n                        predicate=\" AND \".join([f\"s.{k} = t.{k}\" for k in keys]),\n                        source_alias=\"s\",\n                        target_alias=\"t\",\n                    )\n                    .when_not_matched_insert_all()\n                    .execute()\n                )\n\n            self._retry_delta_operation(do_append_once)\n        else:\n            # Filter options supported by write_deltalake\n            write_kwargs = {\n                k: v\n                for k, v in merged_options.items()\n                if k\n                in [\n                    \"partition_by\",\n                    \"mode\",\n                    \"overwrite_schema\",\n                    \"schema_mode\",\n                    \"name\",\n                    \"description\",\n                    \"configuration\",\n                ]\n            }\n\n            def do_write():\n                write_deltalake(\n                    full_path, df, mode=delta_mode, storage_options=storage_opts, **write_kwargs\n                )\n\n            self._retry_delta_operation(do_write)\n\n        # Return commit info\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=1)\n        latest = history[0]\n\n        return {\n            \"version\": dt.version(),\n            \"timestamp\": datetime.fromtimestamp(latest.get(\"timestamp\", 0) / 1000),\n            \"operation\": latest.get(\"operation\"),\n            \"operation_metrics\": latest.get(\"operationMetrics\", {}),\n            \"read_version\": latest.get(\"readVersion\"),\n        }\n\n    def _handle_generic_upsert(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        format: str,\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; tuple[pd.DataFrame, str]:\n        \"\"\"Handle upsert/append_once for standard files by merging with existing data.\"\"\"\n        if \"keys\" not in options:\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        keys = options[\"keys\"]\n        if isinstance(keys, str):\n            keys = [keys]\n\n        # Try to read existing file\n        existing_df = None\n        try:\n            read_opts = options.copy()\n            read_opts.pop(\"keys\", None)\n\n            if format == \"csv\":\n                existing_df = pd.read_csv(full_path, **read_opts)\n            elif format == \"parquet\":\n                existing_df = pd.read_parquet(full_path, **read_opts)\n            elif format == \"json\":\n                existing_df = pd.read_json(full_path, **read_opts)\n            elif format == \"excel\":\n                existing_df = pd.read_excel(full_path, **read_opts)\n        except Exception:\n            # File doesn't exist or can't be read\n            return df, \"overwrite\"  # Treat as new write\n\n        if existing_df is None:\n            return df, \"overwrite\"\n\n        if mode == \"append_once\":\n            # Check if keys exist\n            missing_keys = set(keys) - set(df.columns)\n            if missing_keys:\n                raise KeyError(f\"Keys {missing_keys} not found in input data\")\n\n            # Identify new rows\n            merged = df.merge(existing_df[keys], on=keys, how=\"left\", indicator=True)\n            new_rows = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n            if format in [\"csv\", \"json\"]:\n                return new_rows, \"append\"\n            else:\n                # Rewrite everything\n                return pd.concat([existing_df, new_rows], ignore_index=True), \"overwrite\"\n\n        elif mode == \"upsert\":\n            # Check if keys exist\n            missing_keys = set(keys) - set(df.columns)\n            if missing_keys:\n                raise KeyError(f\"Keys {missing_keys} not found in input data\")\n\n            # 1. Remove rows from existing that are in input\n            merged_indicator = existing_df.merge(df[keys], on=keys, how=\"left\", indicator=True)\n            rows_to_keep = existing_df[merged_indicator[\"_merge\"] == \"left_only\"]\n\n            # 2. Concat rows_to_keep + input df\n            # 3. Write mode becomes overwrite\n            return pd.concat([rows_to_keep, df], ignore_index=True), \"overwrite\"\n\n        return df, mode\n\n    def _write_file(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        format: str,\n        mode: str,\n        merged_options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle standard file writing (CSV, Parquet, etc.).\"\"\"\n        writer_options = merged_options.copy()\n        writer_options.pop(\"keys\", None)\n\n        # Remove storage_options for local pandas writers usually?\n        # Some pandas writers accept storage_options (parquet, csv with fsspec)\n\n        if format == \"csv\":\n            mode_param = \"w\"\n            if mode == \"append\":\n                mode_param = \"a\"\n                if not os.path.exists(full_path):\n                    # If file doesn't exist, include header\n                    writer_options[\"header\"] = True\n                else:\n                    # If appending, don't write header unless explicit\n                    if \"header\" not in writer_options:\n                        writer_options[\"header\"] = False\n\n            df.to_csv(full_path, index=False, mode=mode_param, **writer_options)\n\n        elif format == \"parquet\":\n            if mode == \"append\":\n                # Pandas read_parquet doesn't support append directly usually.\n                # We implement simple read-concat-write for local files\n                if os.path.exists(full_path):\n                    existing = pd.read_parquet(full_path, **merged_options)\n                    df = pd.concat([existing, df], ignore_index=True)\n\n            df.to_parquet(full_path, index=False, **writer_options)\n\n        elif format == \"json\":\n            if mode == \"append\":\n                writer_options[\"mode\"] = \"a\"\n\n            # Default to records if not specified\n            if \"orient\" not in writer_options:\n                writer_options[\"orient\"] = \"records\"\n\n            # Include storage_options for cloud storage (ADLS, S3, GCS)\n            if \"storage_options\" in merged_options:\n                writer_options[\"storage_options\"] = merged_options[\"storage_options\"]\n\n            df.to_json(full_path, **writer_options)\n\n        elif format == \"excel\":\n            if mode == \"append\":\n                # Simple append for excel\n                if os.path.exists(full_path):\n                    with pd.ExcelWriter(full_path, mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n                        df.to_excel(writer, index=False, **writer_options)\n                    return\n\n            df.to_excel(full_path, index=False, **writer_options)\n\n        elif format == \"avro\":\n            try:\n                import fastavro\n            except ImportError:\n                raise ImportError(\"Avro support requires 'pip install fastavro'\")\n\n            # Convert datetime columns to microseconds for Avro timestamp-micros\n            df_avro = df.copy()\n            for col in df_avro.columns:\n                if pd.api.types.is_datetime64_any_dtype(df_avro[col].dtype):\n                    df_avro[col] = df_avro[col].apply(\n                        lambda x: int(x.timestamp() * 1_000_000) if pd.notna(x) else None\n                    )\n\n            records = df_avro.to_dict(\"records\")\n            schema = self._infer_avro_schema(df)\n\n            # Use fsspec for remote URIs (abfss://, s3://, etc.)\n            parsed = urlparse(full_path)\n            if parsed.scheme and parsed.scheme not in [\"file\", \"\"]:\n                # Remote file - use fsspec\n                import fsspec\n\n                storage_opts = merged_options.get(\"storage_options\", {})\n                write_mode = \"wb\" if mode == \"overwrite\" else \"ab\"\n                with fsspec.open(full_path, write_mode, **storage_opts) as f:\n                    fastavro.writer(f, schema, records)\n            else:\n                # Local file - use standard open\n                open_mode = \"wb\"\n                if mode == \"append\" and os.path.exists(full_path):\n                    open_mode = \"a+b\"\n\n                with open(full_path, open_mode) as f:\n                    fastavro.writer(f, schema, records)\n        else:\n            raise ValueError(f\"Unsupported format for Pandas engine: {format}\")\n\n    def add_write_metadata(\n        self,\n        df: pd.DataFrame,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: Pandas DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added\n        \"\"\"\n        from odibi.config import WriteMetadataConfig\n\n        # Normalize config: True -&gt; all defaults\n        if metadata_config is True:\n            config = WriteMetadataConfig()\n        elif isinstance(metadata_config, WriteMetadataConfig):\n            config = metadata_config\n        else:\n            return df  # None or invalid -&gt; no metadata\n\n        # Work on a copy to avoid modifying original\n        df = df.copy()\n\n        # _extracted_at: always applicable\n        if config.extracted_at:\n            df[\"_extracted_at\"] = pd.Timestamp.now()\n\n        # _source_file: only for file sources\n        if config.source_file and is_file_source and source_path:\n            df[\"_source_file\"] = source_path\n\n        # _source_connection: all sources\n        if config.source_connection and source_connection:\n            df[\"_source_connection\"] = source_connection\n\n        # _source_table: SQL sources only\n        if config.source_table and source_table:\n            df[\"_source_table\"] = source_table\n\n        return df\n\n    def _register_lazy_view_unused(self, conn, name: str, df: Any) -&gt; None:\n        \"\"\"Register a LazyDataset as a DuckDB view.\"\"\"\n        duck_fmt = df.format\n        if duck_fmt == \"json\":\n            duck_fmt = \"json_auto\"\n\n        if isinstance(df.path, list):\n            paths = \", \".join([f\"'{p}'\" for p in df.path])\n            conn.execute(\n                f\"CREATE OR REPLACE VIEW {name} AS SELECT * FROM read_{duck_fmt}([{paths}])\"\n            )\n        else:\n            conn.execute(\n                f\"CREATE OR REPLACE VIEW {name} AS SELECT * FROM read_{duck_fmt}('{df.path}')\"\n            )\n\n    def execute_sql(self, sql: str, context: Context) -&gt; pd.DataFrame:\n        \"\"\"Execute SQL query using DuckDB (if available) or pandasql.\n\n        Args:\n            sql: SQL query string\n            context: Execution context\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        if not isinstance(context, PandasContext):\n            raise TypeError(\"PandasEngine requires PandasContext\")\n\n        # Try to use DuckDB for SQL\n        try:\n            import duckdb\n\n            # Create in-memory database\n            conn = duckdb.connect(\":memory:\")\n\n            # Register all DataFrames from context\n            for name in context.list_names():\n                dataset_obj = context.get(name)\n\n                # Debug check\n                # print(f\"DEBUG: Registering {name} type={type(dataset_obj)} LazyDataset={LazyDataset}\")\n\n                # Handle LazyDataset (DuckDB optimization)\n                # if isinstance(dataset_obj, LazyDataset):\n                #     self._register_lazy_view(conn, name, dataset_obj)\n                #     # Log that we used DuckDB on file\n                #     # logger.info(f\"Executing SQL via DuckDB on lazy file: {dataset_obj.path}\")\n                #     continue\n\n                # Handle chunked data (Iterator)\n                from collections.abc import Iterator\n\n                if isinstance(dataset_obj, Iterator):\n                    # Warning: Materializing iterator for SQL execution\n                    # Note: DuckDB doesn't support streaming from iterator yet\n                    dataset_obj = pd.concat(dataset_obj, ignore_index=True)\n\n                conn.register(name, dataset_obj)\n\n            # Execute query\n            result = conn.execute(sql).df()\n            conn.close()\n\n            return result\n\n        except ImportError:\n            # Fallback: try pandasql\n            try:\n                from pandasql import sqldf\n\n                # Build local namespace with DataFrames\n                locals_dict = {}\n                for name in context.list_names():\n                    df = context.get(name)\n\n                    # Handle chunked data (Iterator)\n                    from collections.abc import Iterator\n\n                    if isinstance(df, Iterator):\n                        df = pd.concat(df, ignore_index=True)\n\n                    locals_dict[name] = df\n\n                return sqldf(sql, locals_dict)\n\n            except ImportError:\n                raise TransformError(\n                    \"SQL execution requires 'duckdb' or 'pandasql'. \"\n                    \"Install with: pip install duckdb\"\n                )\n\n    def execute_operation(\n        self,\n        operation: str,\n        params: Dict[str, Any],\n        df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute built-in operation.\n\n        Args:\n            operation: Operation name\n            params: Operation parameters\n            df: Input DataFrame or Iterator\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        # Materialize LazyDataset\n        df = self.materialize(df)\n\n        # Handle chunked data (Iterator)\n        from collections.abc import Iterator\n\n        if isinstance(df, Iterator):\n            # Warning: Materializing iterator for operation execution\n            df = pd.concat(df, ignore_index=True)\n\n        if operation == \"pivot\":\n            return self._pivot(df, params)\n        elif operation == \"drop_duplicates\":\n            return df.drop_duplicates(**params)\n        elif operation == \"fillna\":\n            return df.fillna(**params)\n        elif operation == \"drop\":\n            return df.drop(**params)\n        elif operation == \"rename\":\n            return df.rename(**params)\n        elif operation == \"sort\":\n            return df.sort_values(**params)\n        elif operation == \"sample\":\n            return df.sample(**params)\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext, PandasContext\n            from odibi.registry import FunctionRegistry\n\n            if FunctionRegistry.has_function(operation):\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df\n                engine_ctx = EngineContext(\n                    context=PandasContext(),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n            raise ValueError(f\"Unsupported operation: {operation}\")\n\n    def _pivot(self, df: pd.DataFrame, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute pivot operation.\n\n        Args:\n            df: Input DataFrame\n            params: Pivot parameters\n\n        Returns:\n            Pivoted DataFrame\n        \"\"\"\n        group_by = params.get(\"group_by\", [])\n        pivot_column = params[\"pivot_column\"]\n        value_column = params[\"value_column\"]\n        agg_func = params.get(\"agg_func\", \"first\")\n\n        # Validate columns exist\n        required_columns = set()\n        if isinstance(group_by, list):\n            required_columns.update(group_by)\n        elif isinstance(group_by, str):\n            required_columns.add(group_by)\n            group_by = [group_by]\n\n        required_columns.add(pivot_column)\n        required_columns.add(value_column)\n\n        missing = required_columns - set(df.columns)\n        if missing:\n            raise KeyError(\n                f\"Columns not found in DataFrame for pivot operation: {missing}. \"\n                f\"Available: {list(df.columns)}\"\n            )\n\n        result = df.pivot_table(\n            index=group_by, columns=pivot_column, values=value_column, aggfunc=agg_func\n        ).reset_index()\n\n        # Flatten column names if multi-level\n        if isinstance(result.columns, pd.MultiIndex):\n            result.columns = [\"_\".join(col).strip(\"_\") for col in result.columns.values]\n\n        return result\n\n    def harmonize_schema(\n        self, df: pd.DataFrame, target_schema: Dict[str, str], policy: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        target_cols = list(target_schema.keys())\n        current_cols = df.columns.tolist()\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        # 1. Check Validations\n        if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # 2. Apply Transformations\n        if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n            # Evolve: Add missing columns, Keep new columns\n            for col in missing:\n                df[col] = None\n        else:\n            # Enforce / Ignore New: Project to target schema (Drops new, Adds missing)\n            # Note: reindex adds NaN for missing columns\n            df = df.reindex(columns=target_cols)\n\n        return df\n\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Anonymize specified columns.\"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        res = df.copy()\n\n        for col in columns:\n            if col not in res.columns:\n                continue\n\n            if method == \"hash\":\n                # Vectorized Hashing (via map/apply)\n                # Note: True vectorization requires C-level support (e.g. pyarrow.compute)\n                # Standard Pandas apply is the fallback but we can optimize string handling\n\n                # Convert to string, handling nulls\n                # s_col = res[col].astype(str)\n                # Nulls become 'nan'/'None' string, we want to preserve them or hash them consistently?\n                # Typically nulls should remain null.\n\n                mask_nulls = res[col].isna()\n\n                def _hash_val(val):\n                    to_hash = val\n                    if salt:\n                        to_hash += salt\n                    return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n                # Apply only to non-nulls\n                res.loc[~mask_nulls, col] = res.loc[~mask_nulls, col].astype(str).apply(_hash_val)\n\n            elif method == \"mask\":\n                # Vectorized Masking\n                # Mask all but last 4 characters\n\n                mask_nulls = res[col].isna()\n                s_valid = res.loc[~mask_nulls, col].astype(str)\n\n                # Use vectorized regex replacement\n                # Replace any character that is followed by 4 characters with '*'\n                res.loc[~mask_nulls, col] = s_valid.str.replace(r\".(?=.{4})\", \"*\", regex=True)\n\n            elif method == \"redact\":\n                res[col] = \"[REDACTED]\"\n\n        return res\n\n    def get_schema(self, df: Any) -&gt; Dict[str, str]:\n        \"\"\"Get DataFrame schema with types.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Dict[str, str]: Column name -&gt; Type string\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res = conn.execute(\"DESCRIBE SELECT * FROM df\").df()\n                    return dict(zip(res[\"column_name\"], res[\"column_type\"]))\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return {col: str(df[col].dtype) for col in df.columns}\n\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            (rows, columns)\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            cols = len(self.get_schema(df))\n            rows = self.count_rows(df)\n            return (rows, cols)\n        return df.shape\n\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Row count\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res = conn.execute(\"SELECT count(*) FROM df\").fetchone()\n                    return res[0] if res else 0\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return len(df)\n\n    def count_nulls(self, df: pd.DataFrame, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\n\n        Args:\n            df: DataFrame\n            columns: Columns to check\n\n        Returns:\n            Dictionary of column -&gt; null count\n        \"\"\"\n        null_counts = {}\n        for col in columns:\n            if col in df.columns:\n                null_counts[col] = int(df[col].isna().sum())\n            else:\n                raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        return null_counts\n\n    def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\n\n        Args:\n            df: DataFrame\n            schema_rules: Validation rules\n\n        Returns:\n            List of validation failures\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        failures = []\n\n        # Check required columns\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(df.columns)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        # Check column types\n        if \"types\" in schema_rules:\n            type_map = {\n                \"int\": [\"int64\", \"int32\", \"int16\", \"int8\"],\n                \"float\": [\"float64\", \"float32\"],\n                \"str\": [\"object\", \"string\"],\n                \"bool\": [\"bool\"],\n            }\n\n            for col, expected_type in schema_rules[\"types\"].items():\n                if col not in df.columns:\n                    failures.append(f\"Column '{col}' not found for type validation\")\n                    continue\n\n                actual_type = str(df[col].dtype)\n                # Handle pyarrow types (e.g. int64[pyarrow])\n                if \"[\" in actual_type and \"pyarrow\" in actual_type:\n                    actual_type = actual_type.split(\"[\")[0]\n\n                expected_dtypes = type_map.get(expected_type, [expected_type])\n\n                if actual_type not in expected_dtypes:\n                    failures.append(\n                        f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def _infer_avro_schema(self, df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Infer Avro schema from pandas DataFrame.\n\n        Args:\n            df: DataFrame to infer schema from\n\n        Returns:\n            Avro schema dictionary\n        \"\"\"\n        type_mapping = {\n            \"int64\": \"long\",\n            \"int32\": \"int\",\n            \"float64\": \"double\",\n            \"float32\": \"float\",\n            \"bool\": \"boolean\",\n            \"object\": \"string\",\n            \"string\": \"string\",\n        }\n\n        fields = []\n        for col in df.columns:\n            dtype = df[col].dtype\n            dtype_str = str(dtype)\n\n            # Handle datetime types with Avro logical types\n            if pd.api.types.is_datetime64_any_dtype(dtype):\n                avro_type = {\n                    \"type\": \"long\",\n                    \"logicalType\": \"timestamp-micros\",\n                }\n            elif dtype_str == \"date\" or (hasattr(dtype, \"name\") and \"date\" in dtype.name.lower()):\n                avro_type = {\n                    \"type\": \"int\",\n                    \"logicalType\": \"date\",\n                }\n            elif pd.api.types.is_timedelta64_dtype(dtype):\n                avro_type = {\n                    \"type\": \"long\",\n                    \"logicalType\": \"time-micros\",\n                }\n            else:\n                avro_type = type_mapping.get(dtype_str, \"string\")\n\n            # Handle nullable columns\n            if df[col].isnull().any():\n                avro_type = [\"null\", avro_type]\n\n            fields.append({\"name\": col, \"type\": avro_type})\n\n        return {\"type\": \"record\", \"name\": \"DataFrame\", \"fields\": fields}\n\n    def validate_data(self, df: pd.DataFrame, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate DataFrame against rules.\n\n        Args:\n            df: DataFrame\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        failures = []\n\n        # Check not empty\n        if validation_config.not_empty:\n            if len(df) == 0:\n                failures.append(\"DataFrame is empty\")\n\n        # Check for nulls in specified columns\n        if validation_config.no_nulls:\n            null_counts = self.count_nulls(df, validation_config.no_nulls)\n            for col, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col}' has {count} null values\")\n\n        # Schema validation\n        if validation_config.schema_validation:\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        # Range validation\n        if validation_config.ranges:\n            for col, bounds in validation_config.ranges.items():\n                if col in df.columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    if min_val is not None:\n                        min_violations = df[df[col] &lt; min_val]\n                        if len(min_violations) &gt; 0:\n                            failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        max_violations = df[df[col] &gt; max_val]\n                        if len(max_violations) &gt; 0:\n                            failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col}' not found for range validation\")\n\n        # Allowed values validation\n        if validation_config.allowed_values:\n            for col, allowed in validation_config.allowed_values.items():\n                if col in df.columns:\n                    # Check for values not in allowed list\n                    invalid = df[~df[col].isin(allowed)]\n                    if len(invalid) &gt; 0:\n                        failures.append(f\"Column '{col}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n        return failures\n\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\n\n        Args:\n            df: DataFrame or LazyDataset\n            n: Number of rows to return\n\n        Returns:\n            List of row dictionaries\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res_df = conn.execute(f\"SELECT * FROM df LIMIT {n}\").df()\n                    return res_df.to_dict(\"records\")\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return df.head(n).to_dict(\"records\")\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Args:\n            connection: Connection object\n            table: Table name (not used in Pandas\u2014no catalog)\n            path: File path\n\n        Returns:\n            True if file/directory exists, False otherwise\n        \"\"\"\n        if path:\n            full_path = connection.get_path(path)\n            return os.path.exists(full_path)\n        return False\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\"\"\"\n        try:\n            if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n                # SQL Server: Read empty result\n                query = f\"SELECT TOP 0 * FROM {table}\"\n                df = connection.read_sql(query)\n                return self.get_schema(df)\n\n            if path:\n                full_path = connection.get_path(path)\n                if not os.path.exists(full_path):\n                    return None\n\n                if format == \"delta\":\n                    from deltalake import DeltaTable\n\n                    dt = DeltaTable(full_path)\n                    # Use pyarrow schema to pandas schema to avoid reading data\n                    arrow_schema = dt.schema().to_pyarrow()\n                    empty_df = arrow_schema.empty_table().to_pandas()\n                    return self.get_schema(empty_df)\n\n                elif format == \"parquet\":\n                    import pyarrow.parquet as pq\n\n                    target_path = full_path\n                    if os.path.isdir(full_path):\n                        # Find first parquet file\n                        files = glob.glob(os.path.join(full_path, \"*.parquet\"))\n                        if not files:\n                            return None\n                        target_path = files[0]\n\n                    schema = pq.read_schema(target_path)\n                    empty_df = schema.empty_table().to_pandas()\n                    return self.get_schema(empty_df)\n\n                elif format == \"csv\":\n                    df = pd.read_csv(full_path, nrows=0)\n                    return self.get_schema(df)\n\n        except (FileNotFoundError, PermissionError):\n            return None\n        except ImportError as e:\n            # Log missing optional dependency\n            import logging\n\n            logging.getLogger(__name__).warning(\n                f\"Could not infer schema due to missing dependency: {e}\"\n            )\n            return None\n        except Exception as e:\n            import logging\n\n            logging.getLogger(__name__).warning(f\"Failed to infer schema for {table or path}: {e}\")\n            return None\n        return None\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n        dry_run: bool = False,\n        enforce_retention_duration: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"VACUUM a Delta table to remove old files.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            retention_hours: Retention period (default 168 = 7 days)\n            dry_run: If True, only show files to be deleted\n            enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n        Returns:\n            Dictionary with files_deleted count\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n        )\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        deleted_files = dt.vacuum(\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n            enforce_retention_duration=enforce_retention_duration,\n        )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=str(full_path),\n            files_deleted=len(deleted_files),\n            dry_run=dry_run,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return {\"files_deleted\": len(deleted_files)}\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            limit: Maximum number of versions to return\n\n        Returns:\n            List of version metadata dictionaries\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=limit)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=str(full_path),\n            versions_returned=len(history) if history else 0,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n\n    def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n        \"\"\"Restore Delta table to a specific version.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            version: Version number to restore to\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.info(\"Starting Delta table restore\", path=path, target_version=version)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        dt.restore(version)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta table restored\",\n            path=str(full_path),\n            restored_to_version=version,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        if format != \"delta\" or not config or not config.enabled:\n            return\n\n        if not path and not table:\n            return\n\n        full_path = connection.get_path(path if path else table)\n        start = time.time()\n\n        ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.warning(\n                \"Auto-optimize skipped: 'deltalake' library not installed\",\n                path=str(full_path),\n            )\n            return\n\n        try:\n            storage_opts = {}\n            if hasattr(connection, \"pandas_storage_options\"):\n                storage_opts = connection.pandas_storage_options()\n\n            dt = DeltaTable(full_path, storage_options=storage_opts)\n\n            ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n            dt.optimize.compact()\n\n            retention = config.vacuum_retention_hours\n            if retention is not None and retention &gt; 0:\n                ctx.info(\n                    \"Running Delta VACUUM\",\n                    path=str(full_path),\n                    retention_hours=retention,\n                )\n                dt.vacuum(\n                    retention_hours=retention,\n                    enforce_retention_duration=True,\n                    dry_run=False,\n                )\n\n            elapsed = (time.time() - start) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                path=str(full_path),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            ctx.warning(\n                \"Auto-optimize failed\",\n                path=str(full_path),\n                error=str(e),\n            )\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            List of file paths\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if isinstance(df.path, list):\n                return df.path\n            return [str(df.path)]\n\n        if hasattr(df, \"attrs\"):\n            return df.attrs.get(\"odibi_source_files\", [])\n        return []\n\n    def profile_nulls(self, df: pd.DataFrame) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dictionary of {column_name: null_percentage}\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        # mean() of boolean DataFrame gives the percentage of True values\n        return df.isna().mean().to_dict()\n\n    def filter_greater_than(self, df: pd.DataFrame, column: str, value: Any) -&gt; pd.DataFrame:\n        \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n        if column not in df.columns:\n            raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n        try:\n            # Handle timestamp string comparison\n            if pd.api.types.is_datetime64_any_dtype(df[column]) and isinstance(value, str):\n                value = pd.to_datetime(value)\n\n            return df[df[column] &gt; value]\n        except Exception as e:\n            raise ValueError(f\"Failed to filter {column} &gt; {value}: {e}\")\n\n    def filter_coalesce(\n        self, df: pd.DataFrame, col1: str, col2: str, op: str, value: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n        if col1 not in df.columns:\n            # If fallback only? No, usually primary must exist.\n            raise ValueError(f\"Column '{col1}' not found\")\n\n        # If col2 missing, behave like col1\n        if col2 not in df.columns:\n            s = df[col1]\n        else:\n            s = df[col1].combine_first(df[col2])\n\n        try:\n            if pd.api.types.is_datetime64_any_dtype(s) and isinstance(value, str):\n                value = pd.to_datetime(value)\n\n            if op == \"&gt;=\":\n                return df[s &gt;= value]\n            elif op == \"&gt;\":\n                return df[s &gt; value]\n            elif op == \"&lt;=\":\n                return df[s &lt;= value]\n            elif op == \"&lt;\":\n                return df[s &lt; value]\n            elif op == \"==\" or op == \"=\":\n                return df[s == value]\n            else:\n                raise ValueError(f\"Unsupported operator: {op}\")\n        except Exception as e:\n            raise ValueError(f\"Failed to filter COALESCE({col1}, {col2}) {op} {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.__init__","title":"<code>__init__(connections=None, config=None)</code>","text":"<p>Initialize Pandas engine.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Pandas engine.\n\n    Args:\n        connections: Dictionary of connection objects\n        config: Engine configuration (optional)\n    \"\"\"\n    self.connections = connections or {}\n    self.config = config or {}\n\n    # Suppress noisy delta-rs transaction conflict warnings (handled by retry)\n    if \"RUST_LOG\" not in os.environ:\n        os.environ[\"RUST_LOG\"] = \"deltalake_core::kernel::transaction=error\"\n\n    # Check for performance flags\n    performance = self.config.get(\"performance\", {})\n\n    # Determine desired state\n    if hasattr(performance, \"use_arrow\"):\n        desired_use_arrow = performance.use_arrow\n    elif isinstance(performance, dict):\n        desired_use_arrow = performance.get(\"use_arrow\", True)\n    else:\n        desired_use_arrow = True\n\n    # Verify availability\n    if desired_use_arrow:\n        try:\n            import pyarrow  # noqa: F401\n\n            self.use_arrow = True\n        except ImportError:\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"Apache Arrow not found. Disabling Arrow optimizations. \"\n                \"Install 'pyarrow' to enable.\"\n            )\n            self.use_arrow = False\n    else:\n        self.use_arrow = False\n\n    # Check for DuckDB\n    self.use_duckdb = False\n    # Default to False to ensure stability with existing tests (Lazy Loading is opt-in)\n    if self.config.get(\"performance\", {}).get(\"use_duckdb\", False):\n        try:\n            import duckdb  # noqa: F401\n\n            self.use_duckdb = True\n        except ImportError:\n            pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with metadata columns added</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: pd.DataFrame,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: Pandas DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added\n    \"\"\"\n    from odibi.config import WriteMetadataConfig\n\n    # Normalize config: True -&gt; all defaults\n    if metadata_config is True:\n        config = WriteMetadataConfig()\n    elif isinstance(metadata_config, WriteMetadataConfig):\n        config = metadata_config\n    else:\n        return df  # None or invalid -&gt; no metadata\n\n    # Work on a copy to avoid modifying original\n    df = df.copy()\n\n    # _extracted_at: always applicable\n    if config.extracted_at:\n        df[\"_extracted_at\"] = pd.Timestamp.now()\n\n    # _source_file: only for file sources\n    if config.source_file and is_file_source and source_path:\n        df[\"_source_file\"] = source_path\n\n    # _source_connection: all sources\n    if config.source_connection and source_connection:\n        df[\"_source_connection\"] = source_connection\n\n    # _source_table: SQL sources only\n    if config.source_table and source_table:\n        df[\"_source_table\"] = source_table\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize specified columns.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Anonymize specified columns.\"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    res = df.copy()\n\n    for col in columns:\n        if col not in res.columns:\n            continue\n\n        if method == \"hash\":\n            # Vectorized Hashing (via map/apply)\n            # Note: True vectorization requires C-level support (e.g. pyarrow.compute)\n            # Standard Pandas apply is the fallback but we can optimize string handling\n\n            # Convert to string, handling nulls\n            # s_col = res[col].astype(str)\n            # Nulls become 'nan'/'None' string, we want to preserve them or hash them consistently?\n            # Typically nulls should remain null.\n\n            mask_nulls = res[col].isna()\n\n            def _hash_val(val):\n                to_hash = val\n                if salt:\n                    to_hash += salt\n                return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n            # Apply only to non-nulls\n            res.loc[~mask_nulls, col] = res.loc[~mask_nulls, col].astype(str).apply(_hash_val)\n\n        elif method == \"mask\":\n            # Vectorized Masking\n            # Mask all but last 4 characters\n\n            mask_nulls = res[col].isna()\n            s_valid = res.loc[~mask_nulls, col].astype(str)\n\n            # Use vectorized regex replacement\n            # Replace any character that is followed by 4 characters with '*'\n            res.loc[~mask_nulls, col] = s_valid.str.replace(r\".(?=.{4})\", \"*\", regex=True)\n\n        elif method == \"redact\":\n            res[col] = \"[REDACTED]\"\n\n    return res\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>columns</code> <code>List[str]</code> <p>Columns to check</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary of column -&gt; null count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def count_nulls(self, df: pd.DataFrame, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\n\n    Args:\n        df: DataFrame\n        columns: Columns to check\n\n    Returns:\n        Dictionary of column -&gt; null count\n    \"\"\"\n    null_counts = {}\n    for col in columns:\n        if col in df.columns:\n            null_counts[col] = int(df[col].isna().sum())\n        else:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    return null_counts\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>int</code> <p>Row count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Row count\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res = conn.execute(\"SELECT count(*) FROM df\").fetchone()\n                return res[0] if res else 0\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return len(df)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Operation name</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Operation parameters</p> required <code>df</code> <code>Union[DataFrame, Iterator[DataFrame]]</code> <p>Input DataFrame or Iterator</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def execute_operation(\n    self,\n    operation: str,\n    params: Dict[str, Any],\n    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n) -&gt; pd.DataFrame:\n    \"\"\"Execute built-in operation.\n\n    Args:\n        operation: Operation name\n        params: Operation parameters\n        df: Input DataFrame or Iterator\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    # Materialize LazyDataset\n    df = self.materialize(df)\n\n    # Handle chunked data (Iterator)\n    from collections.abc import Iterator\n\n    if isinstance(df, Iterator):\n        # Warning: Materializing iterator for operation execution\n        df = pd.concat(df, ignore_index=True)\n\n    if operation == \"pivot\":\n        return self._pivot(df, params)\n    elif operation == \"drop_duplicates\":\n        return df.drop_duplicates(**params)\n    elif operation == \"fillna\":\n        return df.fillna(**params)\n    elif operation == \"drop\":\n        return df.drop(**params)\n    elif operation == \"rename\":\n        return df.rename(**params)\n    elif operation == \"sort\":\n        return df.sort_values(**params)\n    elif operation == \"sample\":\n        return df.sample(**params)\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext, PandasContext\n        from odibi.registry import FunctionRegistry\n\n        if FunctionRegistry.has_function(operation):\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df\n            engine_ctx = EngineContext(\n                context=PandasContext(),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n        raise ValueError(f\"Unsupported operation: {operation}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.execute_sql","title":"<code>execute_sql(sql, context)</code>","text":"<p>Execute SQL query using DuckDB (if available) or pandasql.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Context) -&gt; pd.DataFrame:\n    \"\"\"Execute SQL query using DuckDB (if available) or pandasql.\n\n    Args:\n        sql: SQL query string\n        context: Execution context\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    if not isinstance(context, PandasContext):\n        raise TypeError(\"PandasEngine requires PandasContext\")\n\n    # Try to use DuckDB for SQL\n    try:\n        import duckdb\n\n        # Create in-memory database\n        conn = duckdb.connect(\":memory:\")\n\n        # Register all DataFrames from context\n        for name in context.list_names():\n            dataset_obj = context.get(name)\n\n            # Debug check\n            # print(f\"DEBUG: Registering {name} type={type(dataset_obj)} LazyDataset={LazyDataset}\")\n\n            # Handle LazyDataset (DuckDB optimization)\n            # if isinstance(dataset_obj, LazyDataset):\n            #     self._register_lazy_view(conn, name, dataset_obj)\n            #     # Log that we used DuckDB on file\n            #     # logger.info(f\"Executing SQL via DuckDB on lazy file: {dataset_obj.path}\")\n            #     continue\n\n            # Handle chunked data (Iterator)\n            from collections.abc import Iterator\n\n            if isinstance(dataset_obj, Iterator):\n                # Warning: Materializing iterator for SQL execution\n                # Note: DuckDB doesn't support streaming from iterator yet\n                dataset_obj = pd.concat(dataset_obj, ignore_index=True)\n\n            conn.register(name, dataset_obj)\n\n        # Execute query\n        result = conn.execute(sql).df()\n        conn.close()\n\n        return result\n\n    except ImportError:\n        # Fallback: try pandasql\n        try:\n            from pandasql import sqldf\n\n            # Build local namespace with DataFrames\n            locals_dict = {}\n            for name in context.list_names():\n                df = context.get(name)\n\n                # Handle chunked data (Iterator)\n                from collections.abc import Iterator\n\n                if isinstance(df, Iterator):\n                    df = pd.concat(df, ignore_index=True)\n\n                locals_dict[name] = df\n\n            return sqldf(sql, locals_dict)\n\n        except ImportError:\n            raise TransformError(\n                \"SQL execution requires 'duckdb' or 'pandasql'. \"\n                \"Install with: pip install duckdb\"\n            )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.filter_coalesce","title":"<code>filter_coalesce(df, col1, col2, op, value)</code>","text":"<p>Filter using COALESCE(col1, col2) op value.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def filter_coalesce(\n    self, df: pd.DataFrame, col1: str, col2: str, op: str, value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n    if col1 not in df.columns:\n        # If fallback only? No, usually primary must exist.\n        raise ValueError(f\"Column '{col1}' not found\")\n\n    # If col2 missing, behave like col1\n    if col2 not in df.columns:\n        s = df[col1]\n    else:\n        s = df[col1].combine_first(df[col2])\n\n    try:\n        if pd.api.types.is_datetime64_any_dtype(s) and isinstance(value, str):\n            value = pd.to_datetime(value)\n\n        if op == \"&gt;=\":\n            return df[s &gt;= value]\n        elif op == \"&gt;\":\n            return df[s &gt; value]\n        elif op == \"&lt;=\":\n            return df[s &lt;= value]\n        elif op == \"&lt;\":\n            return df[s &lt; value]\n        elif op == \"==\" or op == \"=\":\n            return df[s == value]\n        else:\n            raise ValueError(f\"Unsupported operator: {op}\")\n    except Exception as e:\n        raise ValueError(f\"Failed to filter COALESCE({col1}, {col2}) {op} {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.filter_greater_than","title":"<code>filter_greater_than(df, column, value)</code>","text":"<p>Filter DataFrame where column &gt; value.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def filter_greater_than(self, df: pd.DataFrame, column: str, value: Any) -&gt; pd.DataFrame:\n    \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n    try:\n        # Handle timestamp string comparison\n        if pd.api.types.is_datetime64_any_dtype(df[column]) and isinstance(value, str):\n            value = pd.to_datetime(value)\n\n        return df[df[column] &gt; value]\n    except Exception as e:\n        raise ValueError(f\"Failed to filter {column} &gt; {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>limit</code> <code>Optional[int]</code> <p>Maximum number of versions to return</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of version metadata dictionaries</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        limit: Maximum number of versions to return\n\n    Returns:\n        List of version metadata dictionaries\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    history = dt.history(limit=limit)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta history retrieved\",\n        path=str(full_path),\n        versions_returned=len(history) if history else 0,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return history\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <code>n</code> <code>int</code> <p>Number of rows to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of row dictionaries</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\n\n    Args:\n        df: DataFrame or LazyDataset\n        n: Number of rows to return\n\n    Returns:\n        List of row dictionaries\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res_df = conn.execute(f\"SELECT * FROM df LIMIT {n}\").df()\n                return res_df.to_dict(\"records\")\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return df.head(n).to_dict(\"records\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema with types.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Column name -&gt; Type string</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_schema(self, df: Any) -&gt; Dict[str, str]:\n    \"\"\"Get DataFrame schema with types.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Dict[str, str]: Column name -&gt; Type string\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res = conn.execute(\"DESCRIBE SELECT * FROM df\").df()\n                return dict(zip(res[\"column_name\"], res[\"column_type\"]))\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return {col: str(df[col].dtype) for col in df.columns}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(rows, columns)</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        (rows, columns)\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        cols = len(self.get_schema(df))\n        rows = self.count_rows(df)\n        return (rows, cols)\n    return df.shape\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        List of file paths\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if isinstance(df.path, list):\n            return df.path\n        return [str(df.path)]\n\n    if hasattr(df, \"attrs\"):\n        return df.attrs.get(\"odibi_source_files\", [])\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\"\"\"\n    try:\n        if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            # SQL Server: Read empty result\n            query = f\"SELECT TOP 0 * FROM {table}\"\n            df = connection.read_sql(query)\n            return self.get_schema(df)\n\n        if path:\n            full_path = connection.get_path(path)\n            if not os.path.exists(full_path):\n                return None\n\n            if format == \"delta\":\n                from deltalake import DeltaTable\n\n                dt = DeltaTable(full_path)\n                # Use pyarrow schema to pandas schema to avoid reading data\n                arrow_schema = dt.schema().to_pyarrow()\n                empty_df = arrow_schema.empty_table().to_pandas()\n                return self.get_schema(empty_df)\n\n            elif format == \"parquet\":\n                import pyarrow.parquet as pq\n\n                target_path = full_path\n                if os.path.isdir(full_path):\n                    # Find first parquet file\n                    files = glob.glob(os.path.join(full_path, \"*.parquet\"))\n                    if not files:\n                        return None\n                    target_path = files[0]\n\n                schema = pq.read_schema(target_path)\n                empty_df = schema.empty_table().to_pandas()\n                return self.get_schema(empty_df)\n\n            elif format == \"csv\":\n                df = pd.read_csv(full_path, nrows=0)\n                return self.get_schema(df)\n\n    except (FileNotFoundError, PermissionError):\n        return None\n    except ImportError as e:\n        # Log missing optional dependency\n        import logging\n\n        logging.getLogger(__name__).warning(\n            f\"Could not infer schema due to missing dependency: {e}\"\n        )\n        return None\n    except Exception as e:\n        import logging\n\n        logging.getLogger(__name__).warning(f\"Failed to infer schema for {table or path}: {e}\")\n        return None\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def harmonize_schema(\n    self, df: pd.DataFrame, target_schema: Dict[str, str], policy: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    target_cols = list(target_schema.keys())\n    current_cols = df.columns.tolist()\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    # 1. Check Validations\n    if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # 2. Apply Transformations\n    if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n        # Evolve: Add missing columns, Keep new columns\n        for col in missing:\n            df[col] = None\n    else:\n        # Enforce / Ignore New: Project to target schema (Drops new, Adds missing)\n        # Note: reindex adds NaN for missing columns\n        df = df.reindex(columns=target_cols)\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n\n    if format != \"delta\" or not config or not config.enabled:\n        return\n\n    if not path and not table:\n        return\n\n    full_path = connection.get_path(path if path else table)\n    start = time.time()\n\n    ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.warning(\n            \"Auto-optimize skipped: 'deltalake' library not installed\",\n            path=str(full_path),\n        )\n        return\n\n    try:\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n\n        ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n        dt.optimize.compact()\n\n        retention = config.vacuum_retention_hours\n        if retention is not None and retention &gt; 0:\n            ctx.info(\n                \"Running Delta VACUUM\",\n                path=str(full_path),\n                retention_hours=retention,\n            )\n            dt.vacuum(\n                retention_hours=retention,\n                enforce_retention_duration=True,\n                dry_run=False,\n            )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            path=str(full_path),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        ctx.warning(\n            \"Auto-optimize failed\",\n            path=str(full_path),\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset.\"\"\"\n    if isinstance(df, LazyDataset):\n        # Re-invoke read but force materialization (by bypassing Lazy check)\n        # We pass the resolved path directly\n        # Note: We need to handle the case where path was resolved.\n        # LazyDataset.path should be the FULL path.\n        return self._read_file(\n            full_path=df.path, format=df.format, options=df.options, connection=df.connection\n        )\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of {column_name: null_percentage}</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def profile_nulls(self, df: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dictionary of {column_name: null_percentage}\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    # mean() of boolean DataFrame gives the percentage of True values\n    return df.isna().mean().to_dict()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, as_of_version=None, as_of_timestamp=None)</code>","text":"<p>Read data using Pandas (or LazyDataset).</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    as_of_version: Optional[int] = None,\n    as_of_timestamp: Optional[str] = None,\n) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Read data using Pandas (or LazyDataset).\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    source = path or table\n    ctx.debug(\n        \"Starting read operation\",\n        format=format,\n        path=source,\n        streaming=streaming,\n        use_arrow=self.use_arrow,\n    )\n\n    if streaming:\n        ctx.error(\n            \"Streaming not supported in Pandas engine\",\n            format=format,\n            path=source,\n        )\n        raise ValueError(\n            \"Streaming is not supported in the Pandas engine. \"\n            \"Please use 'engine: spark' for streaming pipelines.\"\n        )\n\n    options = options or {}\n\n    # Resolve full path from connection\n    try:\n        full_path = self._resolve_path(path or table, connection)\n    except ValueError:\n        if table and not connection:\n            ctx.error(\"Connection required when specifying 'table'\", table=table)\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n        ctx.error(\"Neither path nor table provided for read operation\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Merge storage options for cloud connections\n    merged_options = self._merge_storage_options(connection, options)\n\n    # Sanitize options for pandas compatibility\n    if \"header\" in merged_options:\n        if merged_options[\"header\"] is True:\n            merged_options[\"header\"] = 0\n        elif merged_options[\"header\"] is False:\n            merged_options[\"header\"] = None\n\n    # Handle Time Travel options\n    if as_of_version is not None:\n        merged_options[\"versionAsOf\"] = as_of_version\n        ctx.debug(\"Time travel enabled\", version=as_of_version)\n    if as_of_timestamp is not None:\n        merged_options[\"timestampAsOf\"] = as_of_timestamp\n        ctx.debug(\"Time travel enabled\", timestamp=as_of_timestamp)\n\n    # Check for Lazy/DuckDB optimization\n    can_lazy_load = False\n\n    if can_lazy_load:\n        ctx.debug(\"Using lazy loading via DuckDB\", path=str(full_path))\n        if isinstance(full_path, (str, Path)):\n            return LazyDataset(\n                path=str(full_path),\n                format=format,\n                options=merged_options,\n                connection=connection,\n            )\n        elif isinstance(full_path, list):\n            return LazyDataset(\n                path=full_path, format=format, options=merged_options, connection=connection\n            )\n\n    result = self._read_file(full_path, format, merged_options, connection)\n\n    # Log metrics for materialized DataFrames\n    elapsed = (time.time() - start) * 1000\n    if isinstance(result, pd.DataFrame):\n        row_count = len(result)\n        memory_mb = result.memory_usage(deep=True).sum() / (1024 * 1024)\n\n        ctx.log_file_io(\n            path=str(full_path) if not isinstance(full_path, list) else str(full_path[0]),\n            format=format,\n            mode=\"read\",\n            rows=row_count,\n        )\n        ctx.log_pandas_metrics(\n            memory_mb=memory_mb,\n            dtypes={col: str(dtype) for col, dtype in result.dtypes.items()},\n        )\n        ctx.info(\n            \"Read completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n            memory_mb=round(memory_mb, 2),\n        )\n\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.restore_delta","title":"<code>restore_delta(connection, path, version)</code>","text":"<p>Restore Delta table to a specific version.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>version</code> <code>int</code> <p>Version number to restore to</p> required Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n    \"\"\"Restore Delta table to a specific version.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        version: Version number to restore to\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.info(\"Starting Delta table restore\", path=path, target_version=version)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    dt.restore(version)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta table restored\",\n        path=str(full_path),\n        restored_to_version=version,\n        elapsed_ms=round(elapsed, 2),\n    )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (not used in Pandas\u2014no catalog)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if file/directory exists, False otherwise</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Args:\n        connection: Connection object\n        table: Table name (not used in Pandas\u2014no catalog)\n        path: File path\n\n    Returns:\n        True if file/directory exists, False otherwise\n    \"\"\"\n    if path:\n        full_path = connection.get_path(path)\n        return os.path.exists(full_path)\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168, dry_run=False, enforce_retention_duration=True)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>retention_hours</code> <code>int</code> <p>Retention period (default 168 = 7 days)</p> <code>168</code> <code>dry_run</code> <code>bool</code> <p>If True, only show files to be deleted</p> <code>False</code> <code>enforce_retention_duration</code> <code>bool</code> <p>If False, allows retention &lt; 168 hours (testing only)</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with files_deleted count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n    dry_run: bool = False,\n    enforce_retention_duration: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"VACUUM a Delta table to remove old files.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        retention_hours: Retention period (default 168 = 7 days)\n        dry_run: If True, only show files to be deleted\n        enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n    Returns:\n        Dictionary with files_deleted count\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n    )\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    deleted_files = dt.vacuum(\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n        enforce_retention_duration=enforce_retention_duration,\n    )\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta VACUUM completed\",\n        path=str(full_path),\n        files_deleted=len(deleted_files),\n        dry_run=dry_run,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return {\"files_deleted\": len(deleted_files)}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate DataFrame against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def validate_data(self, df: pd.DataFrame, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate DataFrame against rules.\n\n    Args:\n        df: DataFrame\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    failures = []\n\n    # Check not empty\n    if validation_config.not_empty:\n        if len(df) == 0:\n            failures.append(\"DataFrame is empty\")\n\n    # Check for nulls in specified columns\n    if validation_config.no_nulls:\n        null_counts = self.count_nulls(df, validation_config.no_nulls)\n        for col, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col}' has {count} null values\")\n\n    # Schema validation\n    if validation_config.schema_validation:\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    # Range validation\n    if validation_config.ranges:\n        for col, bounds in validation_config.ranges.items():\n            if col in df.columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                if min_val is not None:\n                    min_violations = df[df[col] &lt; min_val]\n                    if len(min_violations) &gt; 0:\n                        failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    max_violations = df[df[col] &gt; max_val]\n                    if len(max_violations) &gt; 0:\n                        failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col}' not found for range validation\")\n\n    # Allowed values validation\n    if validation_config.allowed_values:\n        for col, allowed in validation_config.allowed_values.items():\n            if col in df.columns:\n                # Check for values not in allowed list\n                invalid = df[~df[col].isin(allowed)]\n                if len(invalid) &gt; 0:\n                    failures.append(f\"Column '{col}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>schema_rules</code> <code>Dict[str, Any]</code> <p>Validation rules</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failures</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\n\n    Args:\n        df: DataFrame\n        schema_rules: Validation rules\n\n    Returns:\n        List of validation failures\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    failures = []\n\n    # Check required columns\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(df.columns)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    # Check column types\n    if \"types\" in schema_rules:\n        type_map = {\n            \"int\": [\"int64\", \"int32\", \"int16\", \"int8\"],\n            \"float\": [\"float64\", \"float32\"],\n            \"str\": [\"object\", \"string\"],\n            \"bool\": [\"bool\"],\n        }\n\n        for col, expected_type in schema_rules[\"types\"].items():\n            if col not in df.columns:\n                failures.append(f\"Column '{col}' not found for type validation\")\n                continue\n\n            actual_type = str(df[col].dtype)\n            # Handle pyarrow types (e.g. int64[pyarrow])\n            if \"[\" in actual_type and \"pyarrow\" in actual_type:\n                actual_type = actual_type.split(\"[\")[0]\n\n            expected_dtypes = type_map.get(expected_type, [expected_type])\n\n            if actual_type not in expected_dtypes:\n                failures.append(\n                    f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.write","title":"<code>write(df, connection, format, table=None, path=None, register_table=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Pandas.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def write(\n    self,\n    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    register_table: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Pandas.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    destination = path or table\n    ctx.debug(\n        \"Starting write operation\",\n        format=format,\n        destination=destination,\n        mode=mode,\n    )\n\n    # Ensure materialization if LazyDataset\n    df = self.materialize(df)\n\n    options = options or {}\n\n    # Handle iterator/generator input\n    from collections.abc import Iterator\n\n    if isinstance(df, Iterator):\n        ctx.debug(\"Writing iterator/generator input\")\n        return self._write_iterator(df, connection, format, table, path, mode, options)\n\n    row_count = len(df)\n    memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n\n    ctx.log_pandas_metrics(\n        memory_mb=memory_mb,\n        dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n    )\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        ctx.debug(\"Writing to SQL\", table=table, mode=mode)\n        return self._write_sql(df, connection, table, mode, options)\n\n    # Resolve full path from connection\n    try:\n        full_path = self._resolve_path(path or table, connection)\n    except ValueError:\n        if table and not connection:\n            ctx.error(\"Connection required when specifying 'table'\", table=table)\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n        ctx.error(\"Neither path nor table provided for write operation\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Merge storage options for cloud connections\n    merged_options = self._merge_storage_options(connection, options)\n\n    # Custom Writers\n    if format in self._custom_writers:\n        ctx.debug(f\"Using custom writer for format: {format}\")\n        writer_options = merged_options.copy()\n        writer_options.pop(\"keys\", None)\n        self._custom_writers[format](df, full_path, mode=mode, **writer_options)\n        return None\n\n    # Ensure directory exists (local only)\n    self._ensure_directory(full_path)\n\n    # Warn about partitioning\n    self._check_partitioning(merged_options)\n\n    # Delta Lake Write\n    if format == \"delta\":\n        ctx.debug(\"Writing Delta table\", path=str(full_path), mode=mode)\n        result = self._write_delta(df, full_path, mode, merged_options)\n        elapsed = (time.time() - start) * 1000\n        ctx.log_file_io(\n            path=str(full_path),\n            format=format,\n            mode=mode,\n            rows=row_count,\n        )\n        ctx.info(\n            \"Write completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n        )\n        return result\n\n    # Handle Generic Upsert/Append-Once for non-Delta\n    if mode in [\"upsert\", \"append_once\"]:\n        ctx.debug(f\"Handling {mode} mode for non-Delta format\")\n        df, mode = self._handle_generic_upsert(df, full_path, format, mode, merged_options)\n        row_count = len(df)\n\n    # Standard File Write\n    result = self._write_file(df, full_path, format, mode, merged_options)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.log_file_io(\n        path=str(full_path),\n        format=format,\n        mode=mode,\n        rows=row_count,\n    )\n    ctx.info(\n        \"Write completed\",\n        format=format,\n        rows=row_count,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine","title":"<code>odibi.engine.spark_engine</code>","text":"<p>Spark execution engine (Phase 2B: Delta Lake support).</p> <p>Status: Phase 2B implemented - Delta Lake read/write, VACUUM, history, restore</p>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine","title":"<code>SparkEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Spark execution engine with PySpark backend.</p> <p>Phase 2A: Basic read/write + ADLS multi-account support Phase 2B: Delta Lake support</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>class SparkEngine(Engine):\n    \"\"\"Spark execution engine with PySpark backend.\n\n    Phase 2A: Basic read/write + ADLS multi-account support\n    Phase 2B: Delta Lake support\n    \"\"\"\n\n    name = \"spark\"\n    engine_type = EngineType.SPARK\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        spark_session: Any = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Spark engine with import guard.\n\n        Args:\n            connections: Dictionary of connection objects (for multi-account config)\n            spark_session: Existing SparkSession (optional, creates new if None)\n            config: Engine configuration (optional)\n\n        Raises:\n            ImportError: If pyspark not installed\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        ctx.debug(\"Initializing SparkEngine\", connections_count=len(connections or {}))\n\n        try:\n            from pyspark.sql import SparkSession\n        except ImportError as e:\n            ctx.error(\n                \"PySpark not installed\",\n                error_type=\"ImportError\",\n                suggestion=\"pip install odibi[spark]\",\n            )\n            raise ImportError(\n                \"Spark support requires 'pip install odibi[spark]'. \"\n                \"See docs/setup_databricks.md for setup instructions.\"\n            ) from e\n\n        start_time = time.time()\n\n        # Configure Delta Lake support\n        try:\n            from delta import configure_spark_with_delta_pip\n\n            builder = SparkSession.builder.appName(\"odibi\").config(\n                \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n            )\n\n            # Performance Optimizations\n            builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n            builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n            # Reduce Verbosity\n            builder = builder.config(\n                \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n            builder = builder.config(\n                \"spark.executor.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n\n            self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n            self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n            ctx.debug(\"Delta Lake support enabled\")\n\n        except ImportError:\n            ctx.debug(\"Delta Lake not available, using standard Spark\")\n            builder = SparkSession.builder.appName(\"odibi\").config(\n                \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n            )\n\n            # Performance Optimizations\n            builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n            builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n            # Reduce Verbosity\n            builder = builder.config(\n                \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n\n            self.spark = spark_session or builder.getOrCreate()\n            self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n        self.config = config or {}\n        self.connections = connections or {}\n\n        # Configure all ADLS connections upfront\n        self._configure_all_connections()\n\n        # Apply user-defined Spark configs from performance settings\n        self._apply_spark_config()\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"SparkEngine initialized\",\n            elapsed_ms=round(elapsed, 2),\n            app_name=self.spark.sparkContext.appName,\n            spark_version=self.spark.version,\n            connections_configured=len(self.connections),\n            using_existing_session=spark_session is not None,\n        )\n\n    def _configure_all_connections(self) -&gt; None:\n        \"\"\"Configure Spark with all ADLS connection credentials.\n\n        This sets all storage account keys upfront so Spark can access\n        multiple accounts. Keys are scoped by account name, so no conflicts.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        for conn_name, connection in self.connections.items():\n            if hasattr(connection, \"configure_spark\"):\n                ctx.log_connection(\n                    connection_type=type(connection).__name__,\n                    connection_name=conn_name,\n                    action=\"configure_spark\",\n                )\n                try:\n                    connection.configure_spark(self.spark)\n                    ctx.debug(f\"Configured ADLS connection: {conn_name}\")\n                except Exception as e:\n                    ctx.error(\n                        f\"Failed to configure ADLS connection: {conn_name}\",\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                    )\n                    raise\n\n    def _apply_spark_config(self) -&gt; None:\n        \"\"\"Apply user-defined Spark configurations from performance settings.\n\n        Applies configs via spark.conf.set() for runtime-settable options.\n        For existing sessions (e.g., Databricks), only modifiable configs take effect.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        performance = self.config.get(\"performance\", {})\n        spark_config = performance.get(\"spark_config\", {})\n\n        if not spark_config:\n            return\n\n        ctx.debug(\"Applying Spark configuration\", config_count=len(spark_config))\n\n        for key, value in spark_config.items():\n            try:\n                self.spark.conf.set(key, value)\n                ctx.debug(\n                    f\"Applied Spark config: {key}={value}\", config_key=key, config_value=value\n                )\n            except Exception as e:\n                ctx.warning(\n                    f\"Failed to set Spark config '{key}'\",\n                    config_key=key,\n                    error_message=str(e),\n                    suggestion=\"This config may require session restart\",\n                )\n\n    def _apply_table_properties(\n        self, target: str, properties: Dict[str, str], is_table: bool = False\n    ) -&gt; None:\n        \"\"\"Apply table properties to a Delta table.\n\n        Performance: Batches all properties into a single ALTER TABLE statement\n        to avoid multiple round-trips to the catalog.\n        \"\"\"\n        if not properties:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            table_ref = target if is_table else f\"delta.`{target}`\"\n            ctx.debug(\n                f\"Applying table properties to {target}\",\n                properties_count=len(properties),\n                is_table=is_table,\n            )\n\n            props_list = [f\"'{k}' = '{v}'\" for k, v in properties.items()]\n            props_str = \", \".join(props_list)\n            sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n            self.spark.sql(sql)\n            ctx.debug(f\"Set {len(properties)} table properties in single statement\")\n\n        except Exception as e:\n            ctx.warning(\n                f\"Failed to set table properties on {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n            )\n\n    def _optimize_delta_write(\n        self, target: str, options: Dict[str, Any], is_table: bool = False\n    ) -&gt; None:\n        \"\"\"Run Delta Lake optimization (OPTIMIZE / ZORDER).\"\"\"\n        should_optimize = options.get(\"optimize_write\", False)\n        zorder_by = options.get(\"zorder_by\")\n\n        if not should_optimize and not zorder_by:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        try:\n            if is_table:\n                sql = f\"OPTIMIZE {target}\"\n            else:\n                sql = f\"OPTIMIZE delta.`{target}`\"\n\n            if zorder_by:\n                if isinstance(zorder_by, str):\n                    zorder_by = [zorder_by]\n                cols = \", \".join(zorder_by)\n                sql += f\" ZORDER BY ({cols})\"\n\n            ctx.debug(\"Running Delta optimization\", sql=sql, target=target)\n            self.spark.sql(sql)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta optimization completed\",\n                target=target,\n                zorder_by=zorder_by,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.warning(\n                f\"Optimization failed for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n    def _get_last_delta_commit_info(\n        self, target: str, is_table: bool = False\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for the most recent Delta commit.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            from delta.tables import DeltaTable\n\n            if is_table:\n                dt = DeltaTable.forName(self.spark, target)\n            else:\n                dt = DeltaTable.forPath(self.spark, target)\n\n            last_commit = dt.history(1).collect()[0]\n\n            def safe_get(row, field):\n                if hasattr(row, field):\n                    return getattr(row, field)\n                if hasattr(row, \"__getitem__\"):\n                    try:\n                        return row[field]\n                    except (KeyError, ValueError):\n                        return None\n                return None\n\n            commit_info = {\n                \"version\": safe_get(last_commit, \"version\"),\n                \"timestamp\": safe_get(last_commit, \"timestamp\"),\n                \"operation\": safe_get(last_commit, \"operation\"),\n                \"operation_metrics\": safe_get(last_commit, \"operationMetrics\"),\n                \"read_version\": safe_get(last_commit, \"readVersion\"),\n            }\n\n            ctx.debug(\n                \"Delta commit metadata retrieved\",\n                target=target,\n                version=commit_info.get(\"version\"),\n                operation=commit_info.get(\"operation\"),\n            )\n\n            return commit_info\n\n        except Exception as e:\n            ctx.warning(\n                f\"Failed to fetch Delta commit info for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n            )\n            return None\n\n    def harmonize_schema(self, df, target_schema: Dict[str, str], policy: Any):\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n        from pyspark.sql.functions import col, lit\n\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        target_cols = list(target_schema.keys())\n        current_cols = df.columns\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        ctx.debug(\n            \"Schema harmonization\",\n            target_columns=len(target_cols),\n            current_columns=len(current_cols),\n            missing_columns=list(missing) if missing else None,\n            new_columns=list(new_cols) if new_cols else None,\n            policy_mode=policy.mode.value if hasattr(policy.mode, \"value\") else str(policy.mode),\n        )\n\n        # Check Validations\n        if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n            ctx.error(\n                f\"Schema Policy Violation: Missing columns {missing}\",\n                missing_columns=list(missing),\n            )\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n            ctx.error(\n                f\"Schema Policy Violation: New columns {new_cols}\",\n                new_columns=list(new_cols),\n            )\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # Apply Transformations\n        if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n            res = df\n            for c in missing:\n                res = res.withColumn(c, lit(None))\n            ctx.debug(\"Schema evolved: added missing columns as null\")\n            return res\n        else:\n            select_exprs = []\n            for c in target_cols:\n                if c in current_cols:\n                    select_exprs.append(col(c))\n                else:\n                    select_exprs.append(lit(None).alias(c))\n\n            ctx.debug(\"Schema enforced: projected to target schema\")\n            return df.select(*select_exprs)\n\n    def anonymize(self, df, columns: List[str], method: str, salt: Optional[str] = None):\n        \"\"\"Anonymize columns using Spark functions.\"\"\"\n        from pyspark.sql.functions import col, concat, lit, regexp_replace, sha2\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        ctx.debug(\n            \"Anonymizing columns\",\n            columns=columns,\n            method=method,\n            has_salt=salt is not None,\n        )\n\n        res = df\n        for c in columns:\n            if c not in df.columns:\n                ctx.warning(f\"Column '{c}' not found for anonymization, skipping\", column=c)\n                continue\n\n            if method == \"hash\":\n                if salt:\n                    res = res.withColumn(c, sha2(concat(col(c), lit(salt)), 256))\n                else:\n                    res = res.withColumn(c, sha2(col(c), 256))\n\n            elif method == \"mask\":\n                res = res.withColumn(c, regexp_replace(col(c), \".(?=.{4})\", \"*\"))\n\n            elif method == \"redact\":\n                res = res.withColumn(c, lit(\"[REDACTED]\"))\n\n        ctx.debug(f\"Anonymization completed using {method}\")\n        return res\n\n    def get_schema(self, df) -&gt; Dict[str, str]:\n        \"\"\"Get DataFrame schema with types.\"\"\"\n        return {f.name: f.dataType.simpleString() for f in df.schema}\n\n    def get_shape(self, df) -&gt; Tuple[int, int]:\n        \"\"\"Get DataFrame shape as (rows, columns).\"\"\"\n        return (df.count(), len(df.columns))\n\n    def count_rows(self, df) -&gt; int:\n        \"\"\"Count rows in DataFrame.\"\"\"\n        return df.count()\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        as_of_version: Optional[int] = None,\n        as_of_timestamp: Optional[str] = None,\n    ) -&gt; Any:\n        \"\"\"Read data using Spark.\n\n        Args:\n            connection: Connection object (with get_path method)\n            format: Data format (csv, parquet, json, delta, sql_server)\n            table: Table name\n            path: File path\n            streaming: Whether to read as a stream (readStream)\n            schema: Schema string in DDL format (required for streaming file sources)\n            options: Format-specific options (including versionAsOf for Delta time travel)\n            as_of_version: Time travel version\n            as_of_timestamp: Time travel timestamp\n\n        Returns:\n            Spark DataFrame (or Streaming DataFrame)\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        source_identifier = table or path or \"unknown\"\n        ctx.debug(\n            \"Starting Spark read\",\n            format=format,\n            source=source_identifier,\n            streaming=streaming,\n            as_of_version=as_of_version,\n            as_of_timestamp=as_of_timestamp,\n        )\n\n        # Handle Time Travel options\n        if as_of_version is not None:\n            options[\"versionAsOf\"] = as_of_version\n            ctx.debug(f\"Time travel enabled: version {as_of_version}\")\n        if as_of_timestamp is not None:\n            options[\"timestampAsOf\"] = as_of_timestamp\n            ctx.debug(f\"Time travel enabled: timestamp {as_of_timestamp}\")\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if streaming:\n                ctx.error(\"Streaming not supported for SQL Server / Azure SQL\")\n                raise ValueError(\"Streaming not supported for SQL Server / Azure SQL yet.\")\n\n            if not hasattr(connection, \"get_spark_options\"):\n                conn_type = type(connection).__name__\n                msg = f\"Connection type '{conn_type}' does not support Spark SQL read\"\n                ctx.error(msg, connection_type=conn_type)\n                raise ValueError(msg)\n\n            jdbc_options = connection.get_spark_options()\n            merged_options = {**jdbc_options, **options}\n\n            # Extract filter for SQL pushdown\n            sql_filter = merged_options.pop(\"filter\", None)\n\n            if \"query\" in merged_options:\n                merged_options.pop(\"dbtable\", None)\n                # If filter provided with query, append to WHERE clause\n                if sql_filter:\n                    existing_query = merged_options[\"query\"]\n                    # Wrap existing query and add filter\n                    if \"WHERE\" in existing_query.upper():\n                        merged_options[\"query\"] = f\"({existing_query}) AND ({sql_filter})\"\n                    else:\n                        subquery = f\"SELECT * FROM ({existing_query}) AS _subq WHERE {sql_filter}\"\n                        merged_options[\"query\"] = subquery\n                    ctx.debug(f\"Applied SQL pushdown filter to query: {sql_filter}\")\n            elif table:\n                # Build query with filter pushdown instead of using dbtable\n                if sql_filter:\n                    merged_options.pop(\"dbtable\", None)\n                    merged_options[\"query\"] = f\"SELECT * FROM {table} WHERE {sql_filter}\"\n                    ctx.debug(f\"Applied SQL pushdown filter: {sql_filter}\")\n                else:\n                    merged_options[\"dbtable\"] = table\n            elif \"dbtable\" not in merged_options:\n                ctx.error(\"SQL format requires 'table' config or 'query' option\")\n                raise ValueError(\"SQL format requires 'table' config or 'query' option\")\n\n            ctx.debug(\"Executing JDBC read\", has_query=\"query\" in merged_options)\n\n            try:\n                df = self.spark.read.format(\"jdbc\").options(**merged_options).load()\n                elapsed = (time.time() - start_time) * 1000\n                partition_count = df.rdd.getNumPartitions()\n\n                ctx.log_file_io(path=source_identifier, format=format, mode=\"read\")\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.info(\n                    \"JDBC read completed\",\n                    source=source_identifier,\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                )\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"JDBC read failed\",\n                    source=source_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Read based on format\n        if table:\n            # Managed/External Table (Catalog)\n            ctx.debug(f\"Reading from catalog table: {table}\")\n\n            if streaming:\n                reader = self.spark.readStream.format(format)\n            else:\n                reader = self.spark.read.format(format)\n\n            for key, value in options.items():\n                reader = reader.option(key, value)\n\n            try:\n                df = reader.table(table)\n\n                if \"filter\" in options:\n                    df = df.filter(options[\"filter\"])\n                    ctx.debug(f\"Applied filter: {options['filter']}\")\n\n                elapsed = (time.time() - start_time) * 1000\n\n                if not streaming:\n                    partition_count = df.rdd.getNumPartitions()\n                    ctx.log_spark_metrics(partition_count=partition_count)\n                    ctx.log_file_io(path=table, format=format, mode=\"read\")\n                    ctx.info(\n                        f\"Table read completed: {table}\",\n                        elapsed_ms=round(elapsed, 2),\n                        partitions=partition_count,\n                    )\n                else:\n                    ctx.info(f\"Streaming read started: {table}\", elapsed_ms=round(elapsed, 2))\n\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"Table read failed: {table}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        elif path:\n            # File Path\n            full_path = connection.get_path(path)\n            ctx.debug(f\"Reading from path: {full_path}\")\n\n            # Auto-detect encoding for CSV (Batch only)\n            if not streaming and format == \"csv\" and options.get(\"auto_encoding\"):\n                options = options.copy()\n                options.pop(\"auto_encoding\")\n\n                if \"encoding\" not in options:\n                    try:\n                        from odibi.utils.encoding import detect_encoding\n\n                        detected = detect_encoding(connection, path)\n                        if detected:\n                            options[\"encoding\"] = detected\n                            ctx.debug(f\"Detected encoding: {detected}\", path=path)\n                    except ImportError:\n                        pass\n                    except Exception as e:\n                        ctx.warning(\n                            f\"Encoding detection failed for {path}\",\n                            error_message=str(e),\n                        )\n\n            if streaming:\n                reader = self.spark.readStream.format(format)\n                if schema:\n                    reader = reader.schema(schema)\n                    ctx.debug(f\"Applied schema for streaming read: {schema[:100]}...\")\n                else:\n                    # Determine if we should warn about missing schema\n                    # Formats that can infer schema: delta, parquet, avro (embedded schema)\n                    # cloudFiles with schemaLocation or self-describing formats (avro, parquet) are fine\n                    should_warn = True\n\n                    if format in [\"delta\", \"parquet\"]:\n                        should_warn = False\n                    elif format == \"cloudFiles\":\n                        cloud_format = options.get(\"cloudFiles.format\", \"\")\n                        has_schema_location = \"cloudFiles.schemaLocation\" in options\n                        # avro and parquet have embedded schemas\n                        if cloud_format in [\"avro\", \"parquet\"] or has_schema_location:\n                            should_warn = False\n\n                    if should_warn:\n                        ctx.warning(\n                            f\"Streaming read from '{format}' format without schema. \"\n                            \"Schema inference is not supported for streaming sources. \"\n                            \"Consider adding 'schema' to your read config.\"\n                        )\n            else:\n                reader = self.spark.read.format(format)\n                if schema:\n                    reader = reader.schema(schema)\n\n            for key, value in options.items():\n                if key == \"header\" and isinstance(value, bool):\n                    value = str(value).lower()\n                reader = reader.option(key, value)\n\n            try:\n                df = reader.load(full_path)\n\n                if \"filter\" in options:\n                    df = df.filter(options[\"filter\"])\n                    ctx.debug(f\"Applied filter: {options['filter']}\")\n\n                elapsed = (time.time() - start_time) * 1000\n\n                if not streaming:\n                    partition_count = df.rdd.getNumPartitions()\n                    ctx.log_spark_metrics(partition_count=partition_count)\n                    ctx.log_file_io(path=path, format=format, mode=\"read\")\n                    ctx.info(\n                        f\"File read completed: {path}\",\n                        elapsed_ms=round(elapsed, 2),\n                        partitions=partition_count,\n                        format=format,\n                    )\n                else:\n                    ctx.info(f\"Streaming read started: {path}\", elapsed_ms=round(elapsed, 2))\n\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"File read failed: {path}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                    format=format,\n                )\n                raise\n        else:\n            ctx.error(\"Either path or table must be provided\")\n            raise ValueError(\"Either path or table must be provided\")\n\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Spark.\n\n        Args:\n            df: Spark DataFrame to write\n            connection: Connection object\n            format: Output format (csv, parquet, json, delta)\n            table: Table name\n            path: File path\n            register_table: Name to register as external table (if path is used)\n            mode: Write mode (overwrite, append, error, ignore, upsert, append_once)\n            options: Format-specific options (including partition_by for partitioning)\n            streaming_config: StreamingWriteConfig for streaming DataFrames\n\n        Returns:\n            Optional dictionary containing Delta commit metadata (if format=delta),\n            or streaming query info (if streaming)\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        if getattr(df, \"isStreaming\", False) is True:\n            return self._write_streaming(\n                df=df,\n                connection=connection,\n                format=format,\n                table=table,\n                path=path,\n                register_table=register_table,\n                options=options,\n                streaming_config=streaming_config,\n            )\n\n        target_identifier = table or path or \"unknown\"\n        try:\n            partition_count = df.rdd.getNumPartitions()\n        except Exception:\n            partition_count = 1  # Fallback for mocks or unsupported DataFrames\n\n        # Auto-coalesce DataFrames for Delta writes to reduce file overhead\n        # Use coalesce_partitions option to explicitly set target partitions\n        # NOTE: We avoid df.count() here as it would trigger double-evaluation of lazy DataFrames\n        coalesce_partitions = options.pop(\"coalesce_partitions\", None)\n        if (\n            coalesce_partitions\n            and isinstance(partition_count, int)\n            and partition_count &gt; coalesce_partitions\n        ):\n            df = df.coalesce(coalesce_partitions)\n            ctx.debug(\n                f\"Coalesced DataFrame to {coalesce_partitions} partition(s)\",\n                original_partitions=partition_count,\n            )\n            partition_count = coalesce_partitions\n\n        ctx.debug(\n            \"Starting Spark write\",\n            format=format,\n            target=target_identifier,\n            mode=mode,\n            partitions=partition_count,\n        )\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if not hasattr(connection, \"get_spark_options\"):\n                conn_type = type(connection).__name__\n                msg = f\"Connection type '{conn_type}' does not support Spark SQL write\"\n                ctx.error(msg, connection_type=conn_type)\n                raise ValueError(msg)\n\n            jdbc_options = connection.get_spark_options()\n            merged_options = {**jdbc_options, **options}\n\n            if table:\n                merged_options[\"dbtable\"] = table\n            elif \"dbtable\" not in merged_options:\n                ctx.error(\"SQL format requires 'table' config or 'dbtable' option\")\n                raise ValueError(\"SQL format requires 'table' config or 'dbtable' option\")\n\n            if mode not in [\"overwrite\", \"append\", \"ignore\", \"error\"]:\n                if mode == \"fail\":\n                    mode = \"error\"\n                else:\n                    ctx.error(f\"Write mode '{mode}' not supported for Spark SQL write\")\n                    raise ValueError(f\"Write mode '{mode}' not supported for Spark SQL write\")\n\n            ctx.debug(\"Executing JDBC write\", target=table or merged_options.get(\"dbtable\"))\n\n            try:\n                df.write.format(\"jdbc\").options(**merged_options).mode(mode).save()\n                elapsed = (time.time() - start_time) * 1000\n                ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                ctx.info(\n                    \"JDBC write completed\",\n                    target=target_identifier,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"JDBC write failed\",\n                    target=target_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Handle Upsert/AppendOnce (Delta Only)\n        if mode in [\"upsert\", \"append_once\"]:\n            if format != \"delta\":\n                ctx.error(f\"Mode '{mode}' only supported for Delta format\")\n                raise NotImplementedError(\n                    f\"Mode '{mode}' only supported for Delta format in Spark engine.\"\n                )\n\n            keys = options.get(\"keys\")\n            if not keys:\n                ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n                raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            exists = self.table_exists(connection, table, path)\n            ctx.debug(\"Table existence check for merge\", target=target_identifier, exists=exists)\n\n            if not exists:\n                mode = \"overwrite\"\n                ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n            else:\n                from delta.tables import DeltaTable\n\n                target_dt = None\n                target_name = \"\"\n                is_table_target = False\n\n                if table:\n                    target_dt = DeltaTable.forName(self.spark, table)\n                    target_name = table\n                    is_table_target = True\n                elif path:\n                    full_path = connection.get_path(path)\n                    target_dt = DeltaTable.forPath(self.spark, full_path)\n                    target_name = full_path\n                    is_table_target = False\n\n                condition = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in keys])\n                ctx.debug(\"Executing Delta merge\", merge_mode=mode, keys=keys, condition=condition)\n\n                merge_builder = target_dt.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n                try:\n                    if mode == \"upsert\":\n                        merge_builder.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                    elif mode == \"append_once\":\n                        merge_builder.whenNotMatchedInsertAll().execute()\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Delta merge completed\",\n                        target=target_name,\n                        mode=mode,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    self._optimize_delta_write(target_name, options, is_table=is_table_target)\n                    commit_info = self._get_last_delta_commit_info(\n                        target_name, is_table=is_table_target\n                    )\n\n                    if commit_info:\n                        ctx.debug(\n                            \"Delta commit info\",\n                            version=commit_info.get(\"version\"),\n                            operation=commit_info.get(\"operation\"),\n                        )\n\n                    return commit_info\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Delta merge failed\",\n                        target=target_name,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n        # Get output location\n        if table:\n            # Managed/External Table (Catalog)\n            ctx.debug(f\"Writing to catalog table: {table}\")\n            writer = df.write.format(format).mode(mode)\n\n            partition_by = options.get(\"partition_by\")\n            if partition_by:\n                if isinstance(partition_by, str):\n                    partition_by = [partition_by]\n                writer = writer.partitionBy(*partition_by)\n                ctx.debug(f\"Partitioning by: {partition_by}\")\n\n            for key, value in options.items():\n                writer = writer.option(key, value)\n\n            try:\n                writer.saveAsTable(table)\n                elapsed = (time.time() - start_time) * 1000\n\n                ctx.log_file_io(\n                    path=table,\n                    format=format,\n                    mode=mode,\n                    partitions=partition_by,\n                )\n                ctx.info(\n                    f\"Table write completed: {table}\",\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if format == \"delta\":\n                    self._optimize_delta_write(table, options, is_table=True)\n                    return self._get_last_delta_commit_info(table, is_table=True)\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"Table write failed: {table}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        elif path:\n            full_path = connection.get_path(path)\n        else:\n            ctx.error(\"Either path or table must be provided\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Extract partition_by option\n        partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n\n        # Extract cluster_by option (Liquid Clustering)\n        cluster_by = options.pop(\"cluster_by\", None)\n\n        # Warn about partitioning anti-patterns\n        if partition_by and cluster_by:\n            import warnings\n\n            ctx.warning(\n                \"Conflict: Both 'partition_by' and 'cluster_by' are set\",\n                partition_by=partition_by,\n                cluster_by=cluster_by,\n            )\n            warnings.warn(\n                \"\u26a0\ufe0f  Conflict: Both 'partition_by' and 'cluster_by' (Liquid Clustering) are set. \"\n                \"Liquid Clustering supersedes partitioning. 'partition_by' will be ignored \"\n                \"if the table is being created now.\",\n                UserWarning,\n            )\n\n        elif partition_by:\n            import warnings\n\n            ctx.warning(\n                \"Partitioning warning: ensure low-cardinality columns\",\n                partition_by=partition_by,\n            )\n            warnings.warn(\n                \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n                \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n                \"and ensure each partition has &gt; 1000 rows.\",\n                UserWarning,\n            )\n\n        # Handle Upsert/Append-Once for Delta Lake (Path-based only for now)\n        if format == \"delta\" and mode in [\"upsert\", \"append_once\"]:\n            try:\n                from delta.tables import DeltaTable\n            except ImportError:\n                ctx.error(\"Delta Lake support requires 'delta-spark'\")\n                raise ImportError(\"Delta Lake support requires 'delta-spark'\")\n\n            if \"keys\" not in options:\n                ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n                raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n            if DeltaTable.isDeltaTable(self.spark, full_path):\n                ctx.debug(f\"Performing Delta merge at path: {full_path}\")\n                delta_table = DeltaTable.forPath(self.spark, full_path)\n                keys = options[\"keys\"]\n                if isinstance(keys, str):\n                    keys = [keys]\n\n                condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in keys])\n                merger = delta_table.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n                try:\n                    if mode == \"upsert\":\n                        merger.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                    else:\n                        merger.whenNotMatchedInsertAll().execute()\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Delta merge completed at path\",\n                        path=path,\n                        mode=mode,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    if register_table:\n                        try:\n                            table_in_catalog = self.spark.catalog.tableExists(register_table)\n                            needs_registration = not table_in_catalog\n\n                            # Handle orphan catalog entries (only for path-not-found errors)\n                            if table_in_catalog:\n                                try:\n                                    self.spark.table(register_table).limit(0).collect()\n                                    ctx.debug(\n                                        f\"Table '{register_table}' already registered and valid\"\n                                    )\n                                except Exception as verify_err:\n                                    error_str = str(verify_err)\n                                    is_orphan = (\n                                        \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                                        or \"Path does not exist\" in error_str\n                                        or \"FileNotFoundException\" in error_str\n                                    )\n                                    if is_orphan:\n                                        ctx.warning(\n                                            f\"Table '{register_table}' is orphan, re-registering\"\n                                        )\n                                        try:\n                                            self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                                        except Exception:\n                                            pass\n                                        needs_registration = True\n                                    else:\n                                        ctx.debug(\n                                            f\"Table '{register_table}' verify failed, \"\n                                            \"skipping registration\"\n                                        )\n\n                            if needs_registration:\n                                create_sql = (\n                                    f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                    f\"USING DELTA LOCATION '{full_path}'\"\n                                )\n                                self.spark.sql(create_sql)\n                                ctx.info(f\"Registered table: {register_table}\", path=full_path)\n                        except Exception as e:\n                            ctx.error(\n                                f\"Failed to register external table '{register_table}'\",\n                                error_message=str(e),\n                            )\n\n                    self._optimize_delta_write(full_path, options, is_table=False)\n                    return self._get_last_delta_commit_info(full_path, is_table=False)\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Delta merge failed at path\",\n                        path=path,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n            else:\n                mode = \"overwrite\"\n                ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n\n        # Write based on format (Path-based)\n        ctx.debug(f\"Writing to path: {full_path}\")\n\n        # Handle Liquid Clustering (New Table Creation via SQL)\n        if format == \"delta\" and cluster_by:\n            should_create = False\n            target_name = None\n\n            if table:\n                target_name = table\n                if mode == \"overwrite\":\n                    should_create = True\n                elif mode == \"append\":\n                    if not self.spark.catalog.tableExists(table):\n                        should_create = True\n            elif path:\n                full_path = connection.get_path(path)\n                target_name = f\"delta.`{full_path}`\"\n                if mode == \"overwrite\":\n                    should_create = True\n                elif mode == \"append\":\n                    try:\n                        from delta.tables import DeltaTable\n\n                        if not DeltaTable.isDeltaTable(self.spark, full_path):\n                            should_create = True\n                    except ImportError:\n                        pass\n\n            if should_create:\n                if isinstance(cluster_by, str):\n                    cluster_by = [cluster_by]\n\n                cols = \", \".join(cluster_by)\n                temp_view = f\"odibi_temp_writer_{abs(hash(str(target_name)))}\"\n                df.createOrReplaceTempView(temp_view)\n\n                create_cmd = (\n                    \"CREATE OR REPLACE TABLE\"\n                    if mode == \"overwrite\"\n                    else \"CREATE TABLE IF NOT EXISTS\"\n                )\n\n                sql = (\n                    f\"{create_cmd} {target_name} USING DELTA CLUSTER BY ({cols}) \"\n                    f\"AS SELECT * FROM {temp_view}\"\n                )\n\n                ctx.debug(\"Creating clustered Delta table\", sql=sql, cluster_by=cluster_by)\n\n                try:\n                    self.spark.sql(sql)\n                    self.spark.catalog.dropTempView(temp_view)\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Clustered Delta table created\",\n                        target=target_name,\n                        cluster_by=cluster_by,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    if register_table and path:\n                        try:\n                            reg_sql = (\n                                f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                f\"USING DELTA LOCATION '{full_path}'\"\n                            )\n                            self.spark.sql(reg_sql)\n                            ctx.info(f\"Registered table: {register_table}\")\n                        except Exception:\n                            pass\n\n                    if format == \"delta\":\n                        self._optimize_delta_write(\n                            target_name if table else full_path, options, is_table=bool(table)\n                        )\n                        return self._get_last_delta_commit_info(\n                            target_name if table else full_path, is_table=bool(table)\n                        )\n                    return None\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Failed to create clustered Delta table\",\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n        # Extract table_properties from options\n        table_properties = options.pop(\"table_properties\", None)\n\n        # For column mapping and other properties that must be set BEFORE write\n        original_configs = {}\n        if table_properties and format == \"delta\":\n            for prop_name, prop_value in table_properties.items():\n                spark_conf_key = (\n                    f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n                )\n                try:\n                    original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n                except Exception:\n                    original_configs[spark_conf_key] = None\n                self.spark.conf.set(spark_conf_key, prop_value)\n            ctx.debug(\n                \"Applied table properties as session defaults\",\n                properties=list(table_properties.keys()),\n            )\n\n        writer = df.write.format(format).mode(mode)\n\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            writer.save(full_path)\n            elapsed = (time.time() - start_time) * 1000\n\n            ctx.log_file_io(\n                path=path,\n                format=format,\n                mode=mode,\n                partitions=partition_by,\n            )\n            ctx.info(\n                f\"File write completed: {path}\",\n                format=format,\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"File write failed: {path}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n        finally:\n            for conf_key, original_value in original_configs.items():\n                if original_value is None:\n                    self.spark.conf.unset(conf_key)\n                else:\n                    self.spark.conf.set(conf_key, original_value)\n\n        if format == \"delta\":\n            self._optimize_delta_write(full_path, options, is_table=False)\n\n        if register_table and format == \"delta\":\n            try:\n                table_in_catalog = self.spark.catalog.tableExists(register_table)\n                needs_registration = not table_in_catalog\n\n                # Handle orphan catalog entries: table exists but points to deleted path\n                # Only treat as orphan if it's specifically a DELTA_PATH_DOES_NOT_EXIST error\n                if table_in_catalog:\n                    try:\n                        self.spark.table(register_table).limit(0).collect()\n                        ctx.debug(\n                            f\"Table '{register_table}' already registered and valid, \"\n                            \"skipping registration\"\n                        )\n                    except Exception as verify_err:\n                        error_str = str(verify_err)\n                        is_orphan = (\n                            \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                            or \"Path does not exist\" in error_str\n                            or \"FileNotFoundException\" in error_str\n                        )\n\n                        if is_orphan:\n                            # Orphan entry - table in catalog but path was deleted\n                            ctx.warning(\n                                f\"Table '{register_table}' is orphan (path deleted), \"\n                                \"dropping and re-registering\",\n                                error_message=error_str[:200],\n                            )\n                            try:\n                                self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                            except Exception:\n                                pass  # Best effort cleanup\n                            needs_registration = True\n                        else:\n                            # Other error (auth, network, etc.) - don't drop, just log\n                            ctx.debug(\n                                f\"Table '{register_table}' exists but verify failed \"\n                                \"(not orphan), skipping registration\",\n                                error_message=error_str[:200],\n                            )\n\n                if needs_registration:\n                    ctx.debug(f\"Registering table '{register_table}' at '{full_path}'\")\n                    reg_sql = (\n                        f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                        f\"USING DELTA LOCATION '{full_path}'\"\n                    )\n                    self.spark.sql(reg_sql)\n                    ctx.info(f\"Registered table: {register_table}\", path=full_path)\n            except Exception as e:\n                ctx.error(\n                    f\"Failed to register table '{register_table}'\",\n                    error_message=str(e),\n                )\n                raise RuntimeError(\n                    f\"Failed to register external table '{register_table}': {e}\"\n                ) from e\n\n        if format == \"delta\":\n            return self._get_last_delta_commit_info(full_path, is_table=False)\n\n        return None\n\n    def _write_streaming(\n        self,\n        df,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Write streaming DataFrame using Spark Structured Streaming.\n\n        Args:\n            df: Streaming Spark DataFrame\n            connection: Connection object\n            format: Output format (delta, kafka, etc.)\n            table: Table name\n            path: File path\n            register_table: Name to register as external table (if path is used)\n            options: Format-specific options\n            streaming_config: StreamingWriteConfig with streaming parameters\n\n        Returns:\n            Dictionary with streaming query information\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        if streaming_config is None:\n            ctx.error(\"Streaming DataFrame requires streaming_config\")\n            raise ValueError(\n                \"Streaming DataFrame detected but no streaming_config provided. \"\n                \"Add a 'streaming' section to your write config with at least \"\n                \"'checkpoint_location' specified.\"\n            )\n\n        target_identifier = table or path or \"unknown\"\n\n        checkpoint_location = streaming_config.checkpoint_location\n        if checkpoint_location and connection:\n            if not checkpoint_location.startswith(\n                (\"abfss://\", \"s3://\", \"gs://\", \"dbfs://\", \"hdfs://\", \"wasbs://\")\n            ):\n                checkpoint_location = connection.get_path(checkpoint_location)\n                ctx.debug(\n                    \"Resolved checkpoint location through connection\",\n                    original=streaming_config.checkpoint_location,\n                    resolved=checkpoint_location,\n                )\n\n        ctx.debug(\n            \"Starting streaming write\",\n            format=format,\n            target=target_identifier,\n            output_mode=streaming_config.output_mode,\n            checkpoint=checkpoint_location,\n        )\n\n        writer = df.writeStream.format(format)\n        writer = writer.outputMode(streaming_config.output_mode)\n        writer = writer.option(\"checkpointLocation\", checkpoint_location)\n\n        if streaming_config.query_name:\n            writer = writer.queryName(streaming_config.query_name)\n\n        if streaming_config.trigger:\n            trigger = streaming_config.trigger\n            if trigger.once:\n                writer = writer.trigger(once=True)\n            elif trigger.available_now:\n                writer = writer.trigger(availableNow=True)\n            elif trigger.processing_time:\n                writer = writer.trigger(processingTime=trigger.processing_time)\n            elif trigger.continuous:\n                writer = writer.trigger(continuous=trigger.continuous)\n\n        partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            if table:\n                query = writer.toTable(table)\n                ctx.info(\n                    f\"Streaming query started: writing to table {table}\",\n                    query_id=str(query.id),\n                    query_name=query.name,\n                )\n            elif path:\n                full_path = connection.get_path(path)\n                query = writer.start(full_path)\n                ctx.info(\n                    f\"Streaming query started: writing to path {path}\",\n                    query_id=str(query.id),\n                    query_name=query.name,\n                )\n            else:\n                ctx.error(\"Either path or table must be provided for streaming write\")\n                raise ValueError(\"Either path or table must be provided for streaming write\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            result = {\n                \"streaming\": True,\n                \"query_id\": str(query.id),\n                \"query_name\": query.name,\n                \"status\": \"running\",\n                \"target\": target_identifier,\n                \"output_mode\": streaming_config.output_mode,\n                \"checkpoint_location\": streaming_config.checkpoint_location,\n                \"elapsed_ms\": round(elapsed, 2),\n            }\n\n            should_wait = streaming_config.await_termination\n            if streaming_config.trigger:\n                trigger = streaming_config.trigger\n                if trigger.once or trigger.available_now:\n                    should_wait = True\n\n            if should_wait:\n                ctx.info(\n                    \"Awaiting streaming query termination\",\n                    timeout_seconds=streaming_config.timeout_seconds,\n                )\n                query.awaitTermination(streaming_config.timeout_seconds)\n                result[\"status\"] = \"terminated\"\n                elapsed = (time.time() - start_time) * 1000\n                result[\"elapsed_ms\"] = round(elapsed, 2)\n                ctx.info(\n                    \"Streaming query terminated\",\n                    query_id=str(query.id),\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table and path and format == \"delta\":\n                    full_path = connection.get_path(path)\n                    try:\n                        self.spark.sql(\n                            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                            f\"USING DELTA LOCATION '{full_path}'\"\n                        )\n                        ctx.info(\n                            f\"Registered external table: {register_table}\",\n                            path=full_path,\n                        )\n                        result[\"registered_table\"] = register_table\n                    except Exception as reg_err:\n                        ctx.warning(\n                            f\"Failed to register external table '{register_table}'\",\n                            error=str(reg_err),\n                        )\n            else:\n                result[\"streaming_query\"] = query\n                if register_table:\n                    ctx.warning(\n                        \"register_table ignored for continuous streaming. \"\n                        \"Table will be registered after query terminates or manually.\"\n                    )\n\n            return result\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Streaming write failed\",\n                target=target_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def execute_sql(self, sql: str, context: Any = None) -&gt; Any:\n        \"\"\"Execute SQL query in Spark.\n\n        Args:\n            sql: SQL query string\n            context: Execution context (optional, not used for Spark)\n\n        Returns:\n            Spark DataFrame with query results\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Executing Spark SQL\", query_preview=sql[:200] if len(sql) &gt; 200 else sql)\n\n        try:\n            result = self.spark.sql(sql)\n            elapsed = (time.time() - start_time) * 1000\n            partition_count = result.rdd.getNumPartitions()\n\n            ctx.log_spark_metrics(partition_count=partition_count)\n            ctx.info(\n                \"Spark SQL executed\",\n                elapsed_ms=round(elapsed, 2),\n                partitions=partition_count,\n            )\n\n            return result\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            error_type = type(e).__name__\n            clean_message = _extract_spark_error_message(e)\n\n            if \"AnalysisException\" in error_type:\n                ctx.error(\n                    \"Spark SQL Analysis Error\",\n                    error_type=error_type,\n                    error_message=clean_message,\n                    query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise TransformError(f\"Spark SQL Analysis Error: {clean_message}\")\n\n            if \"ParseException\" in error_type:\n                ctx.error(\n                    \"Spark SQL Parse Error\",\n                    error_type=error_type,\n                    error_message=clean_message,\n                    query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise TransformError(f\"Spark SQL Parse Error: {clean_message}\")\n\n            ctx.error(\n                \"Spark SQL execution failed\",\n                error_type=error_type,\n                error_message=clean_message,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Error: {clean_message}\")\n\n    def execute_transform(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"SparkEngine.execute_transform() will be implemented in Phase 2B. \"\n            \"See PHASES.md for implementation plan.\"\n        )\n\n    def execute_operation(self, operation: str, params: Dict[str, Any], df) -&gt; Any:\n        \"\"\"Execute built-in operation on Spark DataFrame.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        params = params or {}\n\n        ctx.debug(f\"Executing operation: {operation}\", params=list(params.keys()))\n\n        if operation == \"pivot\":\n            group_by = params.get(\"group_by\", [])\n            pivot_column = params.get(\"pivot_column\")\n            value_column = params.get(\"value_column\")\n            agg_func = params.get(\"agg_func\", \"first\")\n\n            if not pivot_column or not value_column:\n                ctx.error(\"Pivot requires 'pivot_column' and 'value_column'\")\n                raise ValueError(\"Pivot requires 'pivot_column' and 'value_column'\")\n\n            if isinstance(group_by, str):\n                group_by = [group_by]\n\n            agg_expr = {value_column: agg_func}\n            return df.groupBy(*group_by).pivot(pivot_column).agg(agg_expr)\n\n        elif operation == \"drop_duplicates\":\n            subset = params.get(\"subset\")\n            if subset:\n                if isinstance(subset, str):\n                    subset = [subset]\n                return df.dropDuplicates(subset=subset)\n            return df.dropDuplicates()\n\n        elif operation == \"fillna\":\n            value = params.get(\"value\")\n            subset = params.get(\"subset\")\n            return df.fillna(value, subset=subset)\n\n        elif operation == \"drop\":\n            columns = params.get(\"columns\")\n            if not columns:\n                return df\n            if isinstance(columns, str):\n                columns = [columns]\n            return df.drop(*columns)\n\n        elif operation == \"rename\":\n            columns = params.get(\"columns\")\n            if not columns:\n                return df\n\n            res = df\n            for old_name, new_name in columns.items():\n                res = res.withColumnRenamed(old_name, new_name)\n            return res\n\n        elif operation == \"sort\":\n            by = params.get(\"by\")\n            ascending = params.get(\"ascending\", True)\n\n            if not by:\n                return df\n\n            if isinstance(by, str):\n                by = [by]\n\n            if not ascending:\n                from pyspark.sql.functions import desc\n\n                sort_cols = [desc(c) for c in by]\n                return df.orderBy(*sort_cols)\n\n            return df.orderBy(*by)\n\n        elif operation == \"sample\":\n            fraction = params.get(\"frac\", 0.1)\n            seed = params.get(\"random_state\")\n            with_replacement = params.get(\"replace\", False)\n            return df.sample(withReplacement=with_replacement, fraction=fraction, seed=seed)\n\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext\n            from odibi.registry import FunctionRegistry\n\n            ctx.debug(\n                f\"Checking registry for operation: {operation}\",\n                registered_functions=list(FunctionRegistry._functions.keys())[:10],\n                has_function=FunctionRegistry.has_function(operation),\n            )\n\n            if FunctionRegistry.has_function(operation):\n                ctx.debug(f\"Executing registered transformer as operation: {operation}\")\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df\n                from odibi.context import SparkContext\n\n                engine_ctx = EngineContext(\n                    context=SparkContext(self.spark),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n            ctx.error(f\"Unsupported operation for Spark engine: {operation}\")\n            raise ValueError(f\"Unsupported operation for Spark engine: {operation}\")\n\n    def count_nulls(self, df, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\"\"\"\n        from pyspark.sql.functions import col, count, when\n\n        missing = set(columns) - set(df.columns)\n        if missing:\n            raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing)}\")\n\n        aggs = [count(when(col(c).isNull(), c)).alias(c) for c in columns]\n        result = df.select(*aggs).collect()[0].asDict()\n        return result\n\n    def validate_schema(self, df, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\"\"\"\n        failures = []\n\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(df.columns)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        if \"types\" in schema_rules:\n            type_map = {\n                \"int\": [\"integer\", \"long\", \"short\", \"byte\", \"bigint\"],\n                \"float\": [\"double\", \"float\"],\n                \"str\": [\"string\"],\n                \"bool\": [\"boolean\"],\n            }\n\n            for col_name, expected_type in schema_rules[\"types\"].items():\n                if col_name not in df.columns:\n                    failures.append(f\"Column '{col_name}' not found for type validation\")\n                    continue\n\n                actual_type = dict(df.dtypes)[col_name]\n                expected_dtypes = type_map.get(expected_type, [expected_type])\n\n                if actual_type not in expected_dtypes:\n                    failures.append(\n                        f\"Column '{col_name}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def validate_data(self, df, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate DataFrame against rules.\"\"\"\n        from pyspark.sql.functions import col\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        failures = []\n\n        if validation_config.not_empty:\n            if df.isEmpty():\n                failures.append(\"DataFrame is empty\")\n\n        if validation_config.no_nulls:\n            null_counts = self.count_nulls(df, validation_config.no_nulls)\n            for col_name, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col_name}' has {count} null values\")\n\n        if validation_config.schema_validation:\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        if validation_config.ranges:\n            for col_name, bounds in validation_config.ranges.items():\n                if col_name in df.columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    if min_val is not None:\n                        count = df.filter(col(col_name) &lt; min_val).count()\n                        if count &gt; 0:\n                            failures.append(f\"Column '{col_name}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        count = df.filter(col(col_name) &gt; max_val).count()\n                        if count &gt; 0:\n                            failures.append(f\"Column '{col_name}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col_name}' not found for range validation\")\n\n        if validation_config.allowed_values:\n            for col_name, allowed in validation_config.allowed_values.items():\n                if col_name in df.columns:\n                    count = df.filter(~col(col_name).isin(allowed)).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col_name}' not found for allowed values validation\")\n\n        ctx.log_validation_result(\n            passed=len(failures) == 0,\n            rule_name=\"data_validation\",\n            failures=failures if failures else None,\n        )\n\n        return failures\n\n    def get_sample(self, df, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\"\"\"\n        return [row.asDict() for row in df.limit(n).collect()]\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Handles orphan catalog entries where the table is registered but\n        the underlying Delta path no longer exists.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        if table:\n            try:\n                if not self.spark.catalog.tableExists(table):\n                    ctx.debug(f\"Table does not exist: {table}\")\n                    return False\n                # Table exists in catalog - verify it's actually readable\n                # This catches orphan entries where path was deleted\n                self.spark.table(table).limit(0).collect()\n                ctx.debug(f\"Table existence check: {table}\", exists=True)\n                return True\n            except Exception as e:\n                # Table exists in catalog but underlying data is gone (orphan entry)\n                # This is expected during first-run detection - log at debug level\n                ctx.debug(\n                    f\"Table {table} exists in catalog but is not accessible (treating as first run)\",\n                    error_message=str(e),\n                )\n                return False\n        elif path:\n            try:\n                from delta.tables import DeltaTable\n\n                full_path = connection.get_path(path)\n                exists = DeltaTable.isDeltaTable(self.spark, full_path)\n                ctx.debug(f\"Delta table existence check: {path}\", exists=exists)\n                return exists\n            except ImportError:\n                try:\n                    full_path = connection.get_path(path)\n                    exists = (\n                        self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(\n                            self.spark.sparkContext._jsc.hadoopConfiguration()\n                        ).exists(\n                            self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path(\n                                full_path\n                            )\n                        )\n                    )\n                    ctx.debug(f\"Path existence check: {path}\", exists=exists)\n                    return exists\n                except Exception as e:\n                    ctx.warning(f\"Path existence check failed: {path}\", error_message=str(e))\n                    return False\n            except Exception as e:\n                ctx.warning(f\"Table existence check failed: {path}\", error_message=str(e))\n                return False\n        return False\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            if table:\n                if self.spark.catalog.tableExists(table):\n                    schema = self.get_schema(self.spark.table(table))\n                    ctx.debug(f\"Retrieved schema for table: {table}\", columns=len(schema))\n                    return schema\n            elif path:\n                full_path = connection.get_path(path)\n                if format == \"delta\":\n                    from delta.tables import DeltaTable\n\n                    if DeltaTable.isDeltaTable(self.spark, full_path):\n                        schema = self.get_schema(DeltaTable.forPath(self.spark, full_path).toDF())\n                        ctx.debug(f\"Retrieved Delta schema: {path}\", columns=len(schema))\n                        return schema\n                elif format == \"parquet\":\n                    schema = self.get_schema(self.spark.read.parquet(full_path))\n                    ctx.debug(f\"Retrieved Parquet schema: {path}\", columns=len(schema))\n                    return schema\n                elif format:\n                    schema = self.get_schema(self.spark.read.format(format).load(full_path))\n                    ctx.debug(f\"Retrieved schema: {path}\", format=format, columns=len(schema))\n                    return schema\n        except Exception as e:\n            ctx.warning(\n                \"Failed to get schema\",\n                table=table,\n                path=path,\n                error_message=str(e),\n            )\n        return None\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n    ) -&gt; None:\n        \"\"\"VACUUM a Delta table to remove old files.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n        )\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            delta_table.vacuum(retention_hours / 24.0)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta VACUUM completed\",\n                path=path,\n                retention_hours=retention_hours,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Delta VACUUM failed\",\n                path=path,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Fetching Delta history\", path=path, limit=limit)\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            history_df = delta_table.history(limit) if limit else delta_table.history()\n            history = [row.asDict() for row in history_df.collect()]\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta history retrieved\",\n                path=path,\n                versions_returned=len(history),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n            return history\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Failed to get Delta history\",\n                path=path,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n        \"\"\"Restore Delta table to a specific version.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Restoring Delta table\", path=path, version=version)\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            delta_table.restoreToVersion(version)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta table restored\",\n                path=path,\n                version=version,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Delta restore failed\",\n                path=path,\n                version=version,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n        if format != \"delta\" or not config or not config.enabled:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        if table:\n            target = table\n        elif path:\n            full_path = connection.get_path(path)\n            target = f\"delta.`{full_path}`\"\n        else:\n            return\n\n        ctx.debug(\"Starting table maintenance\", target=target)\n\n        try:\n            ctx.debug(f\"Running OPTIMIZE on {target}\")\n            self.spark.sql(f\"OPTIMIZE {target}\")\n\n            retention = config.vacuum_retention_hours\n            if retention is not None and retention &gt; 0:\n                ctx.debug(f\"Running VACUUM on {target}\", retention_hours=retention)\n                self.spark.sql(f\"VACUUM {target} RETAIN {retention} HOURS\")\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                target=target,\n                vacuum_retention_hours=retention,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.warning(\n                f\"Auto-optimize failed for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n    def get_source_files(self, df) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\"\"\"\n        try:\n            return df.inputFiles()\n        except Exception:\n            return []\n\n    def profile_nulls(self, df) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\"\"\"\n        from pyspark.sql.functions import col, mean, when\n\n        aggs = []\n        for c in df.columns:\n            aggs.append(mean(when(col(c).isNull(), 1).otherwise(0)).alias(c))\n\n        if not aggs:\n            return {}\n\n        try:\n            result = df.select(*aggs).collect()[0].asDict()\n            return result\n        except Exception:\n            return {}\n\n    def filter_greater_than(self, df, column: str, value: Any) -&gt; Any:\n        \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n        return df.filter(f\"{column} &gt; '{value}'\")\n\n    def filter_coalesce(self, df, col1: str, col2: str, op: str, value: Any) -&gt; Any:\n        \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n        return df.filter(f\"COALESCE({col1}, {col2}) {op} '{value}'\")\n\n    def add_write_metadata(\n        self,\n        df: Any,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; Any:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: Spark DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added\n        \"\"\"\n        from pyspark.sql import functions as F\n\n        from odibi.config import WriteMetadataConfig\n\n        if metadata_config is True:\n            config = WriteMetadataConfig()\n        elif isinstance(metadata_config, WriteMetadataConfig):\n            config = metadata_config\n        else:\n            return df\n\n        if config.extracted_at:\n            df = df.withColumn(\"_extracted_at\", F.current_timestamp())\n\n        if config.source_file and is_file_source and source_path:\n            df = df.withColumn(\"_source_file\", F.lit(source_path))\n\n        if config.source_connection and source_connection:\n            df = df.withColumn(\"_source_connection\", F.lit(source_connection))\n\n        if config.source_table and source_table:\n            df = df.withColumn(\"_source_table\", F.lit(source_table))\n\n        return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.__init__","title":"<code>__init__(connections=None, spark_session=None, config=None)</code>","text":"<p>Initialize Spark engine with import guard.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects (for multi-account config)</p> <code>None</code> <code>spark_session</code> <code>Any</code> <p>Existing SparkSession (optional, creates new if None)</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pyspark not installed</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    spark_session: Any = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Spark engine with import guard.\n\n    Args:\n        connections: Dictionary of connection objects (for multi-account config)\n        spark_session: Existing SparkSession (optional, creates new if None)\n        config: Engine configuration (optional)\n\n    Raises:\n        ImportError: If pyspark not installed\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    ctx.debug(\"Initializing SparkEngine\", connections_count=len(connections or {}))\n\n    try:\n        from pyspark.sql import SparkSession\n    except ImportError as e:\n        ctx.error(\n            \"PySpark not installed\",\n            error_type=\"ImportError\",\n            suggestion=\"pip install odibi[spark]\",\n        )\n        raise ImportError(\n            \"Spark support requires 'pip install odibi[spark]'. \"\n            \"See docs/setup_databricks.md for setup instructions.\"\n        ) from e\n\n    start_time = time.time()\n\n    # Configure Delta Lake support\n    try:\n        from delta import configure_spark_with_delta_pip\n\n        builder = SparkSession.builder.appName(\"odibi\").config(\n            \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n        )\n\n        # Performance Optimizations\n        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n        # Reduce Verbosity\n        builder = builder.config(\n            \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n        builder = builder.config(\n            \"spark.executor.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n\n        self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n        ctx.debug(\"Delta Lake support enabled\")\n\n    except ImportError:\n        ctx.debug(\"Delta Lake not available, using standard Spark\")\n        builder = SparkSession.builder.appName(\"odibi\").config(\n            \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n        )\n\n        # Performance Optimizations\n        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n        # Reduce Verbosity\n        builder = builder.config(\n            \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n\n        self.spark = spark_session or builder.getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n    self.config = config or {}\n    self.connections = connections or {}\n\n    # Configure all ADLS connections upfront\n    self._configure_all_connections()\n\n    # Apply user-defined Spark configs from performance settings\n    self._apply_spark_config()\n\n    elapsed = (time.time() - start_time) * 1000\n    ctx.info(\n        \"SparkEngine initialized\",\n        elapsed_ms=round(elapsed, 2),\n        app_name=self.spark.sparkContext.appName,\n        spark_version=self.spark.version,\n        connections_configured=len(self.connections),\n        using_existing_session=spark_session is not None,\n    )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Spark DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with metadata columns added</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: Any,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; Any:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: Spark DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added\n    \"\"\"\n    from pyspark.sql import functions as F\n\n    from odibi.config import WriteMetadataConfig\n\n    if metadata_config is True:\n        config = WriteMetadataConfig()\n    elif isinstance(metadata_config, WriteMetadataConfig):\n        config = metadata_config\n    else:\n        return df\n\n    if config.extracted_at:\n        df = df.withColumn(\"_extracted_at\", F.current_timestamp())\n\n    if config.source_file and is_file_source and source_path:\n        df = df.withColumn(\"_source_file\", F.lit(source_path))\n\n    if config.source_connection and source_connection:\n        df = df.withColumn(\"_source_connection\", F.lit(source_connection))\n\n    if config.source_table and source_table:\n        df = df.withColumn(\"_source_table\", F.lit(source_table))\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize columns using Spark functions.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def anonymize(self, df, columns: List[str], method: str, salt: Optional[str] = None):\n    \"\"\"Anonymize columns using Spark functions.\"\"\"\n    from pyspark.sql.functions import col, concat, lit, regexp_replace, sha2\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    ctx.debug(\n        \"Anonymizing columns\",\n        columns=columns,\n        method=method,\n        has_salt=salt is not None,\n    )\n\n    res = df\n    for c in columns:\n        if c not in df.columns:\n            ctx.warning(f\"Column '{c}' not found for anonymization, skipping\", column=c)\n            continue\n\n        if method == \"hash\":\n            if salt:\n                res = res.withColumn(c, sha2(concat(col(c), lit(salt)), 256))\n            else:\n                res = res.withColumn(c, sha2(col(c), 256))\n\n        elif method == \"mask\":\n            res = res.withColumn(c, regexp_replace(col(c), \".(?=.{4})\", \"*\"))\n\n        elif method == \"redact\":\n            res = res.withColumn(c, lit(\"[REDACTED]\"))\n\n    ctx.debug(f\"Anonymization completed using {method}\")\n    return res\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def count_nulls(self, df, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\"\"\"\n    from pyspark.sql.functions import col, count, when\n\n    missing = set(columns) - set(df.columns)\n    if missing:\n        raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing)}\")\n\n    aggs = [count(when(col(c).isNull(), c)).alias(c) for c in columns]\n    result = df.select(*aggs).collect()[0].asDict()\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def count_rows(self, df) -&gt; int:\n    \"\"\"Count rows in DataFrame.\"\"\"\n    return df.count()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation on Spark DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def execute_operation(self, operation: str, params: Dict[str, Any], df) -&gt; Any:\n    \"\"\"Execute built-in operation on Spark DataFrame.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    params = params or {}\n\n    ctx.debug(f\"Executing operation: {operation}\", params=list(params.keys()))\n\n    if operation == \"pivot\":\n        group_by = params.get(\"group_by\", [])\n        pivot_column = params.get(\"pivot_column\")\n        value_column = params.get(\"value_column\")\n        agg_func = params.get(\"agg_func\", \"first\")\n\n        if not pivot_column or not value_column:\n            ctx.error(\"Pivot requires 'pivot_column' and 'value_column'\")\n            raise ValueError(\"Pivot requires 'pivot_column' and 'value_column'\")\n\n        if isinstance(group_by, str):\n            group_by = [group_by]\n\n        agg_expr = {value_column: agg_func}\n        return df.groupBy(*group_by).pivot(pivot_column).agg(agg_expr)\n\n    elif operation == \"drop_duplicates\":\n        subset = params.get(\"subset\")\n        if subset:\n            if isinstance(subset, str):\n                subset = [subset]\n            return df.dropDuplicates(subset=subset)\n        return df.dropDuplicates()\n\n    elif operation == \"fillna\":\n        value = params.get(\"value\")\n        subset = params.get(\"subset\")\n        return df.fillna(value, subset=subset)\n\n    elif operation == \"drop\":\n        columns = params.get(\"columns\")\n        if not columns:\n            return df\n        if isinstance(columns, str):\n            columns = [columns]\n        return df.drop(*columns)\n\n    elif operation == \"rename\":\n        columns = params.get(\"columns\")\n        if not columns:\n            return df\n\n        res = df\n        for old_name, new_name in columns.items():\n            res = res.withColumnRenamed(old_name, new_name)\n        return res\n\n    elif operation == \"sort\":\n        by = params.get(\"by\")\n        ascending = params.get(\"ascending\", True)\n\n        if not by:\n            return df\n\n        if isinstance(by, str):\n            by = [by]\n\n        if not ascending:\n            from pyspark.sql.functions import desc\n\n            sort_cols = [desc(c) for c in by]\n            return df.orderBy(*sort_cols)\n\n        return df.orderBy(*by)\n\n    elif operation == \"sample\":\n        fraction = params.get(\"frac\", 0.1)\n        seed = params.get(\"random_state\")\n        with_replacement = params.get(\"replace\", False)\n        return df.sample(withReplacement=with_replacement, fraction=fraction, seed=seed)\n\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext\n        from odibi.registry import FunctionRegistry\n\n        ctx.debug(\n            f\"Checking registry for operation: {operation}\",\n            registered_functions=list(FunctionRegistry._functions.keys())[:10],\n            has_function=FunctionRegistry.has_function(operation),\n        )\n\n        if FunctionRegistry.has_function(operation):\n            ctx.debug(f\"Executing registered transformer as operation: {operation}\")\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df\n            from odibi.context import SparkContext\n\n            engine_ctx = EngineContext(\n                context=SparkContext(self.spark),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n        ctx.error(f\"Unsupported operation for Spark engine: {operation}\")\n        raise ValueError(f\"Unsupported operation for Spark engine: {operation}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.execute_sql","title":"<code>execute_sql(sql, context=None)</code>","text":"<p>Execute SQL query in Spark.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Any</code> <p>Execution context (optional, not used for Spark)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Spark DataFrame with query results</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Any = None) -&gt; Any:\n    \"\"\"Execute SQL query in Spark.\n\n    Args:\n        sql: SQL query string\n        context: Execution context (optional, not used for Spark)\n\n    Returns:\n        Spark DataFrame with query results\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Executing Spark SQL\", query_preview=sql[:200] if len(sql) &gt; 200 else sql)\n\n    try:\n        result = self.spark.sql(sql)\n        elapsed = (time.time() - start_time) * 1000\n        partition_count = result.rdd.getNumPartitions()\n\n        ctx.log_spark_metrics(partition_count=partition_count)\n        ctx.info(\n            \"Spark SQL executed\",\n            elapsed_ms=round(elapsed, 2),\n            partitions=partition_count,\n        )\n\n        return result\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        error_type = type(e).__name__\n        clean_message = _extract_spark_error_message(e)\n\n        if \"AnalysisException\" in error_type:\n            ctx.error(\n                \"Spark SQL Analysis Error\",\n                error_type=error_type,\n                error_message=clean_message,\n                query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Analysis Error: {clean_message}\")\n\n        if \"ParseException\" in error_type:\n            ctx.error(\n                \"Spark SQL Parse Error\",\n                error_type=error_type,\n                error_message=clean_message,\n                query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Parse Error: {clean_message}\")\n\n        ctx.error(\n            \"Spark SQL execution failed\",\n            error_type=error_type,\n            error_message=clean_message,\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise TransformError(f\"Spark SQL Error: {clean_message}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.filter_coalesce","title":"<code>filter_coalesce(df, col1, col2, op, value)</code>","text":"<p>Filter using COALESCE(col1, col2) op value.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def filter_coalesce(self, df, col1: str, col2: str, op: str, value: Any) -&gt; Any:\n    \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n    return df.filter(f\"COALESCE({col1}, {col2}) {op} '{value}'\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.filter_greater_than","title":"<code>filter_greater_than(df, column, value)</code>","text":"<p>Filter DataFrame where column &gt; value.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def filter_greater_than(self, df, column: str, value: Any) -&gt; Any:\n    \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n    return df.filter(f\"{column} &gt; '{value}'\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Fetching Delta history\", path=path, limit=limit)\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        history_df = delta_table.history(limit) if limit else delta_table.history()\n        history = [row.asDict() for row in history_df.collect()]\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=path,\n            versions_returned=len(history),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Failed to get Delta history\",\n            path=path,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_sample(self, df, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\"\"\"\n    return [row.asDict() for row in df.limit(n).collect()]\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema with types.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_schema(self, df) -&gt; Dict[str, str]:\n    \"\"\"Get DataFrame schema with types.\"\"\"\n    return {f.name: f.dataType.simpleString() for f in df.schema}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape as (rows, columns).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_shape(self, df) -&gt; Tuple[int, int]:\n    \"\"\"Get DataFrame shape as (rows, columns).\"\"\"\n    return (df.count(), len(df.columns))\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_source_files(self, df) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\"\"\"\n    try:\n        return df.inputFiles()\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    try:\n        if table:\n            if self.spark.catalog.tableExists(table):\n                schema = self.get_schema(self.spark.table(table))\n                ctx.debug(f\"Retrieved schema for table: {table}\", columns=len(schema))\n                return schema\n        elif path:\n            full_path = connection.get_path(path)\n            if format == \"delta\":\n                from delta.tables import DeltaTable\n\n                if DeltaTable.isDeltaTable(self.spark, full_path):\n                    schema = self.get_schema(DeltaTable.forPath(self.spark, full_path).toDF())\n                    ctx.debug(f\"Retrieved Delta schema: {path}\", columns=len(schema))\n                    return schema\n            elif format == \"parquet\":\n                schema = self.get_schema(self.spark.read.parquet(full_path))\n                ctx.debug(f\"Retrieved Parquet schema: {path}\", columns=len(schema))\n                return schema\n            elif format:\n                schema = self.get_schema(self.spark.read.format(format).load(full_path))\n                ctx.debug(f\"Retrieved schema: {path}\", format=format, columns=len(schema))\n                return schema\n    except Exception as e:\n        ctx.warning(\n            \"Failed to get schema\",\n            table=table,\n            path=path,\n            error_message=str(e),\n        )\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def harmonize_schema(self, df, target_schema: Dict[str, str], policy: Any):\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n    from pyspark.sql.functions import col, lit\n\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    target_cols = list(target_schema.keys())\n    current_cols = df.columns\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    ctx.debug(\n        \"Schema harmonization\",\n        target_columns=len(target_cols),\n        current_columns=len(current_cols),\n        missing_columns=list(missing) if missing else None,\n        new_columns=list(new_cols) if new_cols else None,\n        policy_mode=policy.mode.value if hasattr(policy.mode, \"value\") else str(policy.mode),\n    )\n\n    # Check Validations\n    if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n        ctx.error(\n            f\"Schema Policy Violation: Missing columns {missing}\",\n            missing_columns=list(missing),\n        )\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n        ctx.error(\n            f\"Schema Policy Violation: New columns {new_cols}\",\n            new_columns=list(new_cols),\n        )\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # Apply Transformations\n    if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n        res = df\n        for c in missing:\n            res = res.withColumn(c, lit(None))\n        ctx.debug(\"Schema evolved: added missing columns as null\")\n        return res\n    else:\n        select_exprs = []\n        for c in target_cols:\n            if c in current_cols:\n                select_exprs.append(col(c))\n            else:\n                select_exprs.append(lit(None).alias(c))\n\n        ctx.debug(\"Schema enforced: projected to target schema\")\n        return df.select(*select_exprs)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n    if format != \"delta\" or not config or not config.enabled:\n        return\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    if table:\n        target = table\n    elif path:\n        full_path = connection.get_path(path)\n        target = f\"delta.`{full_path}`\"\n    else:\n        return\n\n    ctx.debug(\"Starting table maintenance\", target=target)\n\n    try:\n        ctx.debug(f\"Running OPTIMIZE on {target}\")\n        self.spark.sql(f\"OPTIMIZE {target}\")\n\n        retention = config.vacuum_retention_hours\n        if retention is not None and retention &gt; 0:\n            ctx.debug(f\"Running VACUUM on {target}\", retention_hours=retention)\n            self.spark.sql(f\"VACUUM {target} RETAIN {retention} HOURS\")\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            target=target,\n            vacuum_retention_hours=retention,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.warning(\n            f\"Auto-optimize failed for {target}\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def profile_nulls(self, df) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\"\"\"\n    from pyspark.sql.functions import col, mean, when\n\n    aggs = []\n    for c in df.columns:\n        aggs.append(mean(when(col(c).isNull(), 1).otherwise(0)).alias(c))\n\n    if not aggs:\n        return {}\n\n    try:\n        result = df.select(*aggs).collect()[0].asDict()\n        return result\n    except Exception:\n        return {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, as_of_version=None, as_of_timestamp=None)</code>","text":"<p>Read data using Spark.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object (with get_path method)</p> required <code>format</code> <code>str</code> <p>Data format (csv, parquet, json, delta, sql_server)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Whether to read as a stream (readStream)</p> <code>False</code> <code>schema</code> <code>Optional[str]</code> <p>Schema string in DDL format (required for streaming file sources)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options (including versionAsOf for Delta time travel)</p> <code>None</code> <code>as_of_version</code> <code>Optional[int]</code> <p>Time travel version</p> <code>None</code> <code>as_of_timestamp</code> <code>Optional[str]</code> <p>Time travel timestamp</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Spark DataFrame (or Streaming DataFrame)</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    as_of_version: Optional[int] = None,\n    as_of_timestamp: Optional[str] = None,\n) -&gt; Any:\n    \"\"\"Read data using Spark.\n\n    Args:\n        connection: Connection object (with get_path method)\n        format: Data format (csv, parquet, json, delta, sql_server)\n        table: Table name\n        path: File path\n        streaming: Whether to read as a stream (readStream)\n        schema: Schema string in DDL format (required for streaming file sources)\n        options: Format-specific options (including versionAsOf for Delta time travel)\n        as_of_version: Time travel version\n        as_of_timestamp: Time travel timestamp\n\n    Returns:\n        Spark DataFrame (or Streaming DataFrame)\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n    options = options or {}\n\n    source_identifier = table or path or \"unknown\"\n    ctx.debug(\n        \"Starting Spark read\",\n        format=format,\n        source=source_identifier,\n        streaming=streaming,\n        as_of_version=as_of_version,\n        as_of_timestamp=as_of_timestamp,\n    )\n\n    # Handle Time Travel options\n    if as_of_version is not None:\n        options[\"versionAsOf\"] = as_of_version\n        ctx.debug(f\"Time travel enabled: version {as_of_version}\")\n    if as_of_timestamp is not None:\n        options[\"timestampAsOf\"] = as_of_timestamp\n        ctx.debug(f\"Time travel enabled: timestamp {as_of_timestamp}\")\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if streaming:\n            ctx.error(\"Streaming not supported for SQL Server / Azure SQL\")\n            raise ValueError(\"Streaming not supported for SQL Server / Azure SQL yet.\")\n\n        if not hasattr(connection, \"get_spark_options\"):\n            conn_type = type(connection).__name__\n            msg = f\"Connection type '{conn_type}' does not support Spark SQL read\"\n            ctx.error(msg, connection_type=conn_type)\n            raise ValueError(msg)\n\n        jdbc_options = connection.get_spark_options()\n        merged_options = {**jdbc_options, **options}\n\n        # Extract filter for SQL pushdown\n        sql_filter = merged_options.pop(\"filter\", None)\n\n        if \"query\" in merged_options:\n            merged_options.pop(\"dbtable\", None)\n            # If filter provided with query, append to WHERE clause\n            if sql_filter:\n                existing_query = merged_options[\"query\"]\n                # Wrap existing query and add filter\n                if \"WHERE\" in existing_query.upper():\n                    merged_options[\"query\"] = f\"({existing_query}) AND ({sql_filter})\"\n                else:\n                    subquery = f\"SELECT * FROM ({existing_query}) AS _subq WHERE {sql_filter}\"\n                    merged_options[\"query\"] = subquery\n                ctx.debug(f\"Applied SQL pushdown filter to query: {sql_filter}\")\n        elif table:\n            # Build query with filter pushdown instead of using dbtable\n            if sql_filter:\n                merged_options.pop(\"dbtable\", None)\n                merged_options[\"query\"] = f\"SELECT * FROM {table} WHERE {sql_filter}\"\n                ctx.debug(f\"Applied SQL pushdown filter: {sql_filter}\")\n            else:\n                merged_options[\"dbtable\"] = table\n        elif \"dbtable\" not in merged_options:\n            ctx.error(\"SQL format requires 'table' config or 'query' option\")\n            raise ValueError(\"SQL format requires 'table' config or 'query' option\")\n\n        ctx.debug(\"Executing JDBC read\", has_query=\"query\" in merged_options)\n\n        try:\n            df = self.spark.read.format(\"jdbc\").options(**merged_options).load()\n            elapsed = (time.time() - start_time) * 1000\n            partition_count = df.rdd.getNumPartitions()\n\n            ctx.log_file_io(path=source_identifier, format=format, mode=\"read\")\n            ctx.log_spark_metrics(partition_count=partition_count)\n            ctx.info(\n                \"JDBC read completed\",\n                source=source_identifier,\n                elapsed_ms=round(elapsed, 2),\n                partitions=partition_count,\n            )\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"JDBC read failed\",\n                source=source_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    # Read based on format\n    if table:\n        # Managed/External Table (Catalog)\n        ctx.debug(f\"Reading from catalog table: {table}\")\n\n        if streaming:\n            reader = self.spark.readStream.format(format)\n        else:\n            reader = self.spark.read.format(format)\n\n        for key, value in options.items():\n            reader = reader.option(key, value)\n\n        try:\n            df = reader.table(table)\n\n            if \"filter\" in options:\n                df = df.filter(options[\"filter\"])\n                ctx.debug(f\"Applied filter: {options['filter']}\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            if not streaming:\n                partition_count = df.rdd.getNumPartitions()\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.log_file_io(path=table, format=format, mode=\"read\")\n                ctx.info(\n                    f\"Table read completed: {table}\",\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                )\n            else:\n                ctx.info(f\"Streaming read started: {table}\", elapsed_ms=round(elapsed, 2))\n\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Table read failed: {table}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    elif path:\n        # File Path\n        full_path = connection.get_path(path)\n        ctx.debug(f\"Reading from path: {full_path}\")\n\n        # Auto-detect encoding for CSV (Batch only)\n        if not streaming and format == \"csv\" and options.get(\"auto_encoding\"):\n            options = options.copy()\n            options.pop(\"auto_encoding\")\n\n            if \"encoding\" not in options:\n                try:\n                    from odibi.utils.encoding import detect_encoding\n\n                    detected = detect_encoding(connection, path)\n                    if detected:\n                        options[\"encoding\"] = detected\n                        ctx.debug(f\"Detected encoding: {detected}\", path=path)\n                except ImportError:\n                    pass\n                except Exception as e:\n                    ctx.warning(\n                        f\"Encoding detection failed for {path}\",\n                        error_message=str(e),\n                    )\n\n        if streaming:\n            reader = self.spark.readStream.format(format)\n            if schema:\n                reader = reader.schema(schema)\n                ctx.debug(f\"Applied schema for streaming read: {schema[:100]}...\")\n            else:\n                # Determine if we should warn about missing schema\n                # Formats that can infer schema: delta, parquet, avro (embedded schema)\n                # cloudFiles with schemaLocation or self-describing formats (avro, parquet) are fine\n                should_warn = True\n\n                if format in [\"delta\", \"parquet\"]:\n                    should_warn = False\n                elif format == \"cloudFiles\":\n                    cloud_format = options.get(\"cloudFiles.format\", \"\")\n                    has_schema_location = \"cloudFiles.schemaLocation\" in options\n                    # avro and parquet have embedded schemas\n                    if cloud_format in [\"avro\", \"parquet\"] or has_schema_location:\n                        should_warn = False\n\n                if should_warn:\n                    ctx.warning(\n                        f\"Streaming read from '{format}' format without schema. \"\n                        \"Schema inference is not supported for streaming sources. \"\n                        \"Consider adding 'schema' to your read config.\"\n                    )\n        else:\n            reader = self.spark.read.format(format)\n            if schema:\n                reader = reader.schema(schema)\n\n        for key, value in options.items():\n            if key == \"header\" and isinstance(value, bool):\n                value = str(value).lower()\n            reader = reader.option(key, value)\n\n        try:\n            df = reader.load(full_path)\n\n            if \"filter\" in options:\n                df = df.filter(options[\"filter\"])\n                ctx.debug(f\"Applied filter: {options['filter']}\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            if not streaming:\n                partition_count = df.rdd.getNumPartitions()\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.log_file_io(path=path, format=format, mode=\"read\")\n                ctx.info(\n                    f\"File read completed: {path}\",\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                    format=format,\n                )\n            else:\n                ctx.info(f\"Streaming read started: {path}\", elapsed_ms=round(elapsed, 2))\n\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"File read failed: {path}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n                format=format,\n            )\n            raise\n    else:\n        ctx.error(\"Either path or table must be provided\")\n        raise ValueError(\"Either path or table must be provided\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.restore_delta","title":"<code>restore_delta(connection, path, version)</code>","text":"<p>Restore Delta table to a specific version.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n    \"\"\"Restore Delta table to a specific version.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Restoring Delta table\", path=path, version=version)\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        delta_table.restoreToVersion(version)\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta table restored\",\n            path=path,\n            version=version,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Delta restore failed\",\n            path=path,\n            version=version,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> <p>Handles orphan catalog entries where the table is registered but the underlying Delta path no longer exists.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Handles orphan catalog entries where the table is registered but\n    the underlying Delta path no longer exists.\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    if table:\n        try:\n            if not self.spark.catalog.tableExists(table):\n                ctx.debug(f\"Table does not exist: {table}\")\n                return False\n            # Table exists in catalog - verify it's actually readable\n            # This catches orphan entries where path was deleted\n            self.spark.table(table).limit(0).collect()\n            ctx.debug(f\"Table existence check: {table}\", exists=True)\n            return True\n        except Exception as e:\n            # Table exists in catalog but underlying data is gone (orphan entry)\n            # This is expected during first-run detection - log at debug level\n            ctx.debug(\n                f\"Table {table} exists in catalog but is not accessible (treating as first run)\",\n                error_message=str(e),\n            )\n            return False\n    elif path:\n        try:\n            from delta.tables import DeltaTable\n\n            full_path = connection.get_path(path)\n            exists = DeltaTable.isDeltaTable(self.spark, full_path)\n            ctx.debug(f\"Delta table existence check: {path}\", exists=exists)\n            return exists\n        except ImportError:\n            try:\n                full_path = connection.get_path(path)\n                exists = (\n                    self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(\n                        self.spark.sparkContext._jsc.hadoopConfiguration()\n                    ).exists(\n                        self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path(\n                            full_path\n                        )\n                    )\n                )\n                ctx.debug(f\"Path existence check: {path}\", exists=exists)\n                return exists\n            except Exception as e:\n                ctx.warning(f\"Path existence check failed: {path}\", error_message=str(e))\n                return False\n        except Exception as e:\n            ctx.warning(f\"Table existence check failed: {path}\", error_message=str(e))\n            return False\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n) -&gt; None:\n    \"\"\"VACUUM a Delta table to remove old files.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n    )\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        delta_table.vacuum(retention_hours / 24.0)\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=path,\n            retention_hours=retention_hours,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Delta VACUUM failed\",\n            path=path,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate DataFrame against rules.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def validate_data(self, df, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate DataFrame against rules.\"\"\"\n    from pyspark.sql.functions import col\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    failures = []\n\n    if validation_config.not_empty:\n        if df.isEmpty():\n            failures.append(\"DataFrame is empty\")\n\n    if validation_config.no_nulls:\n        null_counts = self.count_nulls(df, validation_config.no_nulls)\n        for col_name, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col_name}' has {count} null values\")\n\n    if validation_config.schema_validation:\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    if validation_config.ranges:\n        for col_name, bounds in validation_config.ranges.items():\n            if col_name in df.columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                if min_val is not None:\n                    count = df.filter(col(col_name) &lt; min_val).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    count = df.filter(col(col_name) &gt; max_val).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col_name}' not found for range validation\")\n\n    if validation_config.allowed_values:\n        for col_name, allowed in validation_config.allowed_values.items():\n            if col_name in df.columns:\n                count = df.filter(~col(col_name).isin(allowed)).count()\n                if count &gt; 0:\n                    failures.append(f\"Column '{col_name}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col_name}' not found for allowed values validation\")\n\n    ctx.log_validation_result(\n        passed=len(failures) == 0,\n        rule_name=\"data_validation\",\n        failures=failures if failures else None,\n    )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def validate_schema(self, df, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\"\"\"\n    failures = []\n\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(df.columns)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    if \"types\" in schema_rules:\n        type_map = {\n            \"int\": [\"integer\", \"long\", \"short\", \"byte\", \"bigint\"],\n            \"float\": [\"double\", \"float\"],\n            \"str\": [\"string\"],\n            \"bool\": [\"boolean\"],\n        }\n\n        for col_name, expected_type in schema_rules[\"types\"].items():\n            if col_name not in df.columns:\n                failures.append(f\"Column '{col_name}' not found for type validation\")\n                continue\n\n            actual_type = dict(df.dtypes)[col_name]\n            expected_dtypes = type_map.get(expected_type, [expected_type])\n\n            if actual_type not in expected_dtypes:\n                failures.append(\n                    f\"Column '{col_name}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.write","title":"<code>write(df, connection, format, table=None, path=None, register_table=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Spark.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Spark DataFrame to write</p> required <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Output format (csv, parquet, json, delta)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>register_table</code> <code>Optional[str]</code> <p>Name to register as external table (if path is used)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Write mode (overwrite, append, error, ignore, upsert, append_once)</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options (including partition_by for partitioning)</p> <code>None</code> <code>streaming_config</code> <code>Optional[Any]</code> <p>StreamingWriteConfig for streaming DataFrames</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary containing Delta commit metadata (if format=delta),</p> <code>Optional[Dict[str, Any]]</code> <p>or streaming query info (if streaming)</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    register_table: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Spark.\n\n    Args:\n        df: Spark DataFrame to write\n        connection: Connection object\n        format: Output format (csv, parquet, json, delta)\n        table: Table name\n        path: File path\n        register_table: Name to register as external table (if path is used)\n        mode: Write mode (overwrite, append, error, ignore, upsert, append_once)\n        options: Format-specific options (including partition_by for partitioning)\n        streaming_config: StreamingWriteConfig for streaming DataFrames\n\n    Returns:\n        Optional dictionary containing Delta commit metadata (if format=delta),\n        or streaming query info (if streaming)\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n    options = options or {}\n\n    if getattr(df, \"isStreaming\", False) is True:\n        return self._write_streaming(\n            df=df,\n            connection=connection,\n            format=format,\n            table=table,\n            path=path,\n            register_table=register_table,\n            options=options,\n            streaming_config=streaming_config,\n        )\n\n    target_identifier = table or path or \"unknown\"\n    try:\n        partition_count = df.rdd.getNumPartitions()\n    except Exception:\n        partition_count = 1  # Fallback for mocks or unsupported DataFrames\n\n    # Auto-coalesce DataFrames for Delta writes to reduce file overhead\n    # Use coalesce_partitions option to explicitly set target partitions\n    # NOTE: We avoid df.count() here as it would trigger double-evaluation of lazy DataFrames\n    coalesce_partitions = options.pop(\"coalesce_partitions\", None)\n    if (\n        coalesce_partitions\n        and isinstance(partition_count, int)\n        and partition_count &gt; coalesce_partitions\n    ):\n        df = df.coalesce(coalesce_partitions)\n        ctx.debug(\n            f\"Coalesced DataFrame to {coalesce_partitions} partition(s)\",\n            original_partitions=partition_count,\n        )\n        partition_count = coalesce_partitions\n\n    ctx.debug(\n        \"Starting Spark write\",\n        format=format,\n        target=target_identifier,\n        mode=mode,\n        partitions=partition_count,\n    )\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if not hasattr(connection, \"get_spark_options\"):\n            conn_type = type(connection).__name__\n            msg = f\"Connection type '{conn_type}' does not support Spark SQL write\"\n            ctx.error(msg, connection_type=conn_type)\n            raise ValueError(msg)\n\n        jdbc_options = connection.get_spark_options()\n        merged_options = {**jdbc_options, **options}\n\n        if table:\n            merged_options[\"dbtable\"] = table\n        elif \"dbtable\" not in merged_options:\n            ctx.error(\"SQL format requires 'table' config or 'dbtable' option\")\n            raise ValueError(\"SQL format requires 'table' config or 'dbtable' option\")\n\n        if mode not in [\"overwrite\", \"append\", \"ignore\", \"error\"]:\n            if mode == \"fail\":\n                mode = \"error\"\n            else:\n                ctx.error(f\"Write mode '{mode}' not supported for Spark SQL write\")\n                raise ValueError(f\"Write mode '{mode}' not supported for Spark SQL write\")\n\n        ctx.debug(\"Executing JDBC write\", target=table or merged_options.get(\"dbtable\"))\n\n        try:\n            df.write.format(\"jdbc\").options(**merged_options).mode(mode).save()\n            elapsed = (time.time() - start_time) * 1000\n            ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n            ctx.info(\n                \"JDBC write completed\",\n                target=target_identifier,\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n            return None\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"JDBC write failed\",\n                target=target_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    # Handle Upsert/AppendOnce (Delta Only)\n    if mode in [\"upsert\", \"append_once\"]:\n        if format != \"delta\":\n            ctx.error(f\"Mode '{mode}' only supported for Delta format\")\n            raise NotImplementedError(\n                f\"Mode '{mode}' only supported for Delta format in Spark engine.\"\n            )\n\n        keys = options.get(\"keys\")\n        if not keys:\n            ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        exists = self.table_exists(connection, table, path)\n        ctx.debug(\"Table existence check for merge\", target=target_identifier, exists=exists)\n\n        if not exists:\n            mode = \"overwrite\"\n            ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n        else:\n            from delta.tables import DeltaTable\n\n            target_dt = None\n            target_name = \"\"\n            is_table_target = False\n\n            if table:\n                target_dt = DeltaTable.forName(self.spark, table)\n                target_name = table\n                is_table_target = True\n            elif path:\n                full_path = connection.get_path(path)\n                target_dt = DeltaTable.forPath(self.spark, full_path)\n                target_name = full_path\n                is_table_target = False\n\n            condition = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in keys])\n            ctx.debug(\"Executing Delta merge\", merge_mode=mode, keys=keys, condition=condition)\n\n            merge_builder = target_dt.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n            try:\n                if mode == \"upsert\":\n                    merge_builder.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                elif mode == \"append_once\":\n                    merge_builder.whenNotMatchedInsertAll().execute()\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Delta merge completed\",\n                    target=target_name,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                self._optimize_delta_write(target_name, options, is_table=is_table_target)\n                commit_info = self._get_last_delta_commit_info(\n                    target_name, is_table=is_table_target\n                )\n\n                if commit_info:\n                    ctx.debug(\n                        \"Delta commit info\",\n                        version=commit_info.get(\"version\"),\n                        operation=commit_info.get(\"operation\"),\n                    )\n\n                return commit_info\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Delta merge failed\",\n                    target=target_name,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n    # Get output location\n    if table:\n        # Managed/External Table (Catalog)\n        ctx.debug(f\"Writing to catalog table: {table}\")\n        writer = df.write.format(format).mode(mode)\n\n        partition_by = options.get(\"partition_by\")\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            writer.saveAsTable(table)\n            elapsed = (time.time() - start_time) * 1000\n\n            ctx.log_file_io(\n                path=table,\n                format=format,\n                mode=mode,\n                partitions=partition_by,\n            )\n            ctx.info(\n                f\"Table write completed: {table}\",\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n            if format == \"delta\":\n                self._optimize_delta_write(table, options, is_table=True)\n                return self._get_last_delta_commit_info(table, is_table=True)\n            return None\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Table write failed: {table}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    elif path:\n        full_path = connection.get_path(path)\n    else:\n        ctx.error(\"Either path or table must be provided\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Extract partition_by option\n    partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n\n    # Extract cluster_by option (Liquid Clustering)\n    cluster_by = options.pop(\"cluster_by\", None)\n\n    # Warn about partitioning anti-patterns\n    if partition_by and cluster_by:\n        import warnings\n\n        ctx.warning(\n            \"Conflict: Both 'partition_by' and 'cluster_by' are set\",\n            partition_by=partition_by,\n            cluster_by=cluster_by,\n        )\n        warnings.warn(\n            \"\u26a0\ufe0f  Conflict: Both 'partition_by' and 'cluster_by' (Liquid Clustering) are set. \"\n            \"Liquid Clustering supersedes partitioning. 'partition_by' will be ignored \"\n            \"if the table is being created now.\",\n            UserWarning,\n        )\n\n    elif partition_by:\n        import warnings\n\n        ctx.warning(\n            \"Partitioning warning: ensure low-cardinality columns\",\n            partition_by=partition_by,\n        )\n        warnings.warn(\n            \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n            \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n            \"and ensure each partition has &gt; 1000 rows.\",\n            UserWarning,\n        )\n\n    # Handle Upsert/Append-Once for Delta Lake (Path-based only for now)\n    if format == \"delta\" and mode in [\"upsert\", \"append_once\"]:\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\"Delta Lake support requires 'delta-spark'\")\n\n        if \"keys\" not in options:\n            ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        if DeltaTable.isDeltaTable(self.spark, full_path):\n            ctx.debug(f\"Performing Delta merge at path: {full_path}\")\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            keys = options[\"keys\"]\n            if isinstance(keys, str):\n                keys = [keys]\n\n            condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in keys])\n            merger = delta_table.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n            try:\n                if mode == \"upsert\":\n                    merger.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                else:\n                    merger.whenNotMatchedInsertAll().execute()\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Delta merge completed at path\",\n                    path=path,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table:\n                    try:\n                        table_in_catalog = self.spark.catalog.tableExists(register_table)\n                        needs_registration = not table_in_catalog\n\n                        # Handle orphan catalog entries (only for path-not-found errors)\n                        if table_in_catalog:\n                            try:\n                                self.spark.table(register_table).limit(0).collect()\n                                ctx.debug(\n                                    f\"Table '{register_table}' already registered and valid\"\n                                )\n                            except Exception as verify_err:\n                                error_str = str(verify_err)\n                                is_orphan = (\n                                    \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                                    or \"Path does not exist\" in error_str\n                                    or \"FileNotFoundException\" in error_str\n                                )\n                                if is_orphan:\n                                    ctx.warning(\n                                        f\"Table '{register_table}' is orphan, re-registering\"\n                                    )\n                                    try:\n                                        self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                                    except Exception:\n                                        pass\n                                    needs_registration = True\n                                else:\n                                    ctx.debug(\n                                        f\"Table '{register_table}' verify failed, \"\n                                        \"skipping registration\"\n                                    )\n\n                        if needs_registration:\n                            create_sql = (\n                                f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                f\"USING DELTA LOCATION '{full_path}'\"\n                            )\n                            self.spark.sql(create_sql)\n                            ctx.info(f\"Registered table: {register_table}\", path=full_path)\n                    except Exception as e:\n                        ctx.error(\n                            f\"Failed to register external table '{register_table}'\",\n                            error_message=str(e),\n                        )\n\n                self._optimize_delta_write(full_path, options, is_table=False)\n                return self._get_last_delta_commit_info(full_path, is_table=False)\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Delta merge failed at path\",\n                    path=path,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n        else:\n            mode = \"overwrite\"\n            ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n\n    # Write based on format (Path-based)\n    ctx.debug(f\"Writing to path: {full_path}\")\n\n    # Handle Liquid Clustering (New Table Creation via SQL)\n    if format == \"delta\" and cluster_by:\n        should_create = False\n        target_name = None\n\n        if table:\n            target_name = table\n            if mode == \"overwrite\":\n                should_create = True\n            elif mode == \"append\":\n                if not self.spark.catalog.tableExists(table):\n                    should_create = True\n        elif path:\n            full_path = connection.get_path(path)\n            target_name = f\"delta.`{full_path}`\"\n            if mode == \"overwrite\":\n                should_create = True\n            elif mode == \"append\":\n                try:\n                    from delta.tables import DeltaTable\n\n                    if not DeltaTable.isDeltaTable(self.spark, full_path):\n                        should_create = True\n                except ImportError:\n                    pass\n\n        if should_create:\n            if isinstance(cluster_by, str):\n                cluster_by = [cluster_by]\n\n            cols = \", \".join(cluster_by)\n            temp_view = f\"odibi_temp_writer_{abs(hash(str(target_name)))}\"\n            df.createOrReplaceTempView(temp_view)\n\n            create_cmd = (\n                \"CREATE OR REPLACE TABLE\"\n                if mode == \"overwrite\"\n                else \"CREATE TABLE IF NOT EXISTS\"\n            )\n\n            sql = (\n                f\"{create_cmd} {target_name} USING DELTA CLUSTER BY ({cols}) \"\n                f\"AS SELECT * FROM {temp_view}\"\n            )\n\n            ctx.debug(\"Creating clustered Delta table\", sql=sql, cluster_by=cluster_by)\n\n            try:\n                self.spark.sql(sql)\n                self.spark.catalog.dropTempView(temp_view)\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Clustered Delta table created\",\n                    target=target_name,\n                    cluster_by=cluster_by,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table and path:\n                    try:\n                        reg_sql = (\n                            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                            f\"USING DELTA LOCATION '{full_path}'\"\n                        )\n                        self.spark.sql(reg_sql)\n                        ctx.info(f\"Registered table: {register_table}\")\n                    except Exception:\n                        pass\n\n                if format == \"delta\":\n                    self._optimize_delta_write(\n                        target_name if table else full_path, options, is_table=bool(table)\n                    )\n                    return self._get_last_delta_commit_info(\n                        target_name if table else full_path, is_table=bool(table)\n                    )\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Failed to create clustered Delta table\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n    # Extract table_properties from options\n    table_properties = options.pop(\"table_properties\", None)\n\n    # For column mapping and other properties that must be set BEFORE write\n    original_configs = {}\n    if table_properties and format == \"delta\":\n        for prop_name, prop_value in table_properties.items():\n            spark_conf_key = (\n                f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n            )\n            try:\n                original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n            except Exception:\n                original_configs[spark_conf_key] = None\n            self.spark.conf.set(spark_conf_key, prop_value)\n        ctx.debug(\n            \"Applied table properties as session defaults\",\n            properties=list(table_properties.keys()),\n        )\n\n    writer = df.write.format(format).mode(mode)\n\n    if partition_by:\n        if isinstance(partition_by, str):\n            partition_by = [partition_by]\n        writer = writer.partitionBy(*partition_by)\n        ctx.debug(f\"Partitioning by: {partition_by}\")\n\n    for key, value in options.items():\n        writer = writer.option(key, value)\n\n    try:\n        writer.save(full_path)\n        elapsed = (time.time() - start_time) * 1000\n\n        ctx.log_file_io(\n            path=path,\n            format=format,\n            mode=mode,\n            partitions=partition_by,\n        )\n        ctx.info(\n            f\"File write completed: {path}\",\n            format=format,\n            mode=mode,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            f\"File write failed: {path}\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n    finally:\n        for conf_key, original_value in original_configs.items():\n            if original_value is None:\n                self.spark.conf.unset(conf_key)\n            else:\n                self.spark.conf.set(conf_key, original_value)\n\n    if format == \"delta\":\n        self._optimize_delta_write(full_path, options, is_table=False)\n\n    if register_table and format == \"delta\":\n        try:\n            table_in_catalog = self.spark.catalog.tableExists(register_table)\n            needs_registration = not table_in_catalog\n\n            # Handle orphan catalog entries: table exists but points to deleted path\n            # Only treat as orphan if it's specifically a DELTA_PATH_DOES_NOT_EXIST error\n            if table_in_catalog:\n                try:\n                    self.spark.table(register_table).limit(0).collect()\n                    ctx.debug(\n                        f\"Table '{register_table}' already registered and valid, \"\n                        \"skipping registration\"\n                    )\n                except Exception as verify_err:\n                    error_str = str(verify_err)\n                    is_orphan = (\n                        \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                        or \"Path does not exist\" in error_str\n                        or \"FileNotFoundException\" in error_str\n                    )\n\n                    if is_orphan:\n                        # Orphan entry - table in catalog but path was deleted\n                        ctx.warning(\n                            f\"Table '{register_table}' is orphan (path deleted), \"\n                            \"dropping and re-registering\",\n                            error_message=error_str[:200],\n                        )\n                        try:\n                            self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                        except Exception:\n                            pass  # Best effort cleanup\n                        needs_registration = True\n                    else:\n                        # Other error (auth, network, etc.) - don't drop, just log\n                        ctx.debug(\n                            f\"Table '{register_table}' exists but verify failed \"\n                            \"(not orphan), skipping registration\",\n                            error_message=error_str[:200],\n                        )\n\n            if needs_registration:\n                ctx.debug(f\"Registering table '{register_table}' at '{full_path}'\")\n                reg_sql = (\n                    f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                    f\"USING DELTA LOCATION '{full_path}'\"\n                )\n                self.spark.sql(reg_sql)\n                ctx.info(f\"Registered table: {register_table}\", path=full_path)\n        except Exception as e:\n            ctx.error(\n                f\"Failed to register table '{register_table}'\",\n                error_message=str(e),\n            )\n            raise RuntimeError(\n                f\"Failed to register external table '{register_table}': {e}\"\n            ) from e\n\n    if format == \"delta\":\n        return self._get_last_delta_commit_info(full_path, is_table=False)\n\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine","title":"<code>odibi.engine.polars_engine</code>","text":"<p>Polars engine implementation.</p>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine","title":"<code>PolarsEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Polars-based execution engine (High Performance).</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>class PolarsEngine(Engine):\n    \"\"\"Polars-based execution engine (High Performance).\"\"\"\n\n    name = \"polars\"\n    engine_type = EngineType.POLARS\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Polars engine.\n\n        Args:\n            connections: Dictionary of connection objects\n            config: Engine configuration (optional)\n        \"\"\"\n        if pl is None:\n            raise ImportError(\"Polars not installed. Run 'pip install polars'.\")\n\n        self.connections = connections or {}\n        self.config = config or {}\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n        Args:\n            df: LazyFrame or DataFrame\n\n        Returns:\n            Materialized DataFrame (pl.DataFrame)\n        \"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return df.collect()\n        return df\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"Read data using Polars (Lazy by default).\n\n        Returns:\n            pl.LazyFrame or pl.DataFrame\n        \"\"\"\n        options = options or {}\n\n        # Get full path\n        if path:\n            if connection:\n                full_path = connection.get_path(path)\n            else:\n                full_path = path\n        elif table:\n            if connection:\n                full_path = connection.get_path(table)\n            else:\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n        else:\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Handle glob patterns/lists\n        # Polars scan methods often support glob strings directly.\n\n        try:\n            if format == \"csv\":\n                # scan_csv supports glob patterns\n                return pl.scan_csv(full_path, **options)\n\n            elif format == \"parquet\":\n                return pl.scan_parquet(full_path, **options)\n\n            elif format == \"json\":\n                # scan_ndjson for newline delimited json, read_json for standard\n                # Assuming ndjson/jsonl for big data usually\n                if options.get(\"json_lines\", True):  # Default to ndjson scan\n                    return pl.scan_ndjson(full_path, **options)\n                else:\n                    # Standard JSON doesn't support lazy scan well in all versions, fallback to read\n                    return pl.read_json(full_path, **options).lazy()\n\n            elif format == \"delta\":\n                # scan_delta requires 'deltalake' extra usually or feature\n                storage_options = options.get(\"storage_options\", None)\n                version = options.get(\"versionAsOf\", None)\n\n                # scan_delta is available in recent polars\n                # It might accept storage_options in recent versions\n                delta_opts = {}\n                if storage_options:\n                    delta_opts[\"storage_options\"] = storage_options\n                if version is not None:\n                    delta_opts[\"version\"] = version\n\n                return pl.scan_delta(full_path, **delta_opts)\n\n            else:\n                raise ValueError(f\"Unsupported format for Polars engine: {format}\")\n\n        except Exception as e:\n            # Fallback or error handling\n            raise ValueError(f\"Failed to read {format} from {full_path}: {e}\")\n\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Write data using Polars.\"\"\"\n        options = options or {}\n\n        if path:\n            if connection:\n                full_path = connection.get_path(path)\n            else:\n                full_path = path\n        elif table:\n            if connection:\n                full_path = connection.get_path(table)\n            else:\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n        else:\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Polars sink (streaming write) is preferred for LazyFrames\n        is_lazy = isinstance(df, pl.LazyFrame)\n\n        # Create directory if needed\n        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n\n        if format == \"parquet\":\n            if is_lazy:\n                # sink_parquet is efficient\n                df.sink_parquet(full_path, **options)\n            else:\n                df.write_parquet(full_path, **options)\n\n        elif format == \"csv\":\n            if is_lazy:\n                df.sink_csv(full_path, **options)\n            else:\n                df.write_csv(full_path, **options)\n\n        elif format == \"json\":\n            if is_lazy:\n                df.sink_ndjson(full_path, **options)\n            else:\n                df.write_ndjson(full_path, **options)\n\n        elif format == \"delta\":\n            # Polars write_delta / sink_delta might be experimental or require specific setup\n            # For now, we might need to materialize and use write_delta if sink not available\n            # Or use deltalake library directly if polars doesn't support writing yet\n\n            # As of recent polars, write_delta exists for DataFrame\n            if is_lazy:\n                df = df.collect()\n\n            storage_options = options.get(\"storage_options\", None)\n            delta_write_options = options.copy()\n            if \"storage_options\" in delta_write_options:\n                del delta_write_options[\"storage_options\"]\n\n            df.write_delta(\n                full_path, mode=mode, storage_options=storage_options, **delta_write_options\n            )\n\n        else:\n            raise ValueError(f\"Unsupported write format for Polars: {format}\")\n\n    def execute_sql(self, sql: str, context: Context) -&gt; Any:\n        \"\"\"Execute SQL query using Polars SQLContext.\n\n        Args:\n            sql: SQL query string\n            context: Execution context with registered DataFrames\n\n        Returns:\n            pl.LazyFrame\n        \"\"\"\n        ctx = pl.SQLContext()\n\n        # Register datasets from context\n        # We iterate over all registered names in the context\n        try:\n            names = context.list_names()\n            for name in names:\n                df = context.get(name)\n                # Register LazyFrame or DataFrame\n                # Polars SQLContext supports registering LazyFrame, DataFrame, and some others\n                # We might need to convert if it's not a Polars object, but we assume Polars engine uses Polars objects\n                ctx.register(name, df)\n        except Exception:\n            # If context doesn't support listing or getting, we proceed with empty context\n            # (e.g. if context is not fully compatible or empty)\n            pass\n\n        return ctx.execute(sql, eager=False)\n\n    def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n        \"\"\"Execute built-in operation.\"\"\"\n        # Ensure LazyFrame for consistency if possible, but operations work on both usually.\n        # If DataFrame, some operations might need different methods.\n\n        if operation == \"pivot\":\n            # Pivot requires materialization usually in other engines, but Polars LazyFrame has 'collect' or similar constraints?\n            # Polars lazy pivot is not fully supported in older versions without collect, but check recent.\n            # Pivot changes shape drastically.\n            # params: pivot_column, value_column, group_by, agg_func\n\n            # If lazy, we might need to collect for pivot if lazy pivot isn't supported or experimental.\n            # But let's try to keep it lazy if possible.\n            # As of recent Polars, pivot is available on DataFrame, experimental on LazyFrame?\n            # Actually, 'unstack' or 'pivot' on LazyFrame is limited.\n            # Safe bet: materialize if needed, or use lazy pivot if available.\n\n            # Let's collect if input is lazy, because pivot usually implies strict schema change hard to predict.\n            if isinstance(df, pl.LazyFrame):\n                df = df.collect()\n\n            return df.pivot(\n                index=params.get(\"group_by\"),\n                on=params[\"pivot_column\"],\n                values=params[\"value_column\"],\n                aggregate_function=params.get(\"agg_func\", \"first\"),\n            )  # Returns DataFrame\n\n        elif operation == \"drop_duplicates\":\n            subset = params.get(\"subset\")\n            if isinstance(df, pl.LazyFrame):\n                return df.unique(subset=subset)\n            return df.unique(subset=subset)\n\n        elif operation == \"fillna\":\n            value = params.get(\"value\")\n            # Polars uses fill_null\n            if isinstance(value, dict):\n                # Fill specific columns\n                # value = {'col1': 0, 'col2': 'unknown'}\n                # We need to chain with_columns\n                exprs = []\n                for col, val in value.items():\n                    exprs.append(pl.col(col).fill_null(val))\n                return df.with_columns(exprs)\n            else:\n                # Fill all columns? Polars fill_null requires specifying columns or using all()\n                return df.fill_null(value)\n\n        elif operation == \"drop\":\n            columns = params.get(\"columns\") or params.get(\"labels\")\n            return df.drop(columns)\n\n        elif operation == \"rename\":\n            columns = params.get(\"columns\") or params.get(\"mapper\")\n            return df.rename(columns)\n\n        elif operation == \"sort\":\n            by = params.get(\"by\")\n            descending = not params.get(\"ascending\", True)\n            if isinstance(df, pl.LazyFrame):\n                return df.sort(by, descending=descending)\n            return df.sort(by, descending=descending)\n\n        elif operation == \"sample\":\n            # Sample n or frac\n            n = params.get(\"n\")\n            frac = params.get(\"frac\")\n            seed = params.get(\"random_state\")\n\n            # Lazy sample supported\n            if n is not None:\n                # Note: Polars Lazy sample might be approximate or require 'collect' depending on version/backend?\n                # But usually supported.\n                if isinstance(df, pl.LazyFrame):\n                    # LazyFrame.sample takes n (int) or fraction.\n                    # But polars 0.19+ changed sample signature?\n                    # It's generally `sample(n=..., fraction=..., seed=...)`\n                    return (\n                        df.collect().sample(n=n, seed=seed).lazy()\n                    )  # Collecting for exact sample n on lazy might be needed if not supported?\n                    # Actually, fetch(n) is head. Sample is random.\n                    # Let's materialize for safety with sample as it's often for checks.\n                    pass\n                return df.sample(n=n, seed=seed)\n            elif frac is not None:\n                if isinstance(df, pl.LazyFrame):\n                    # Lazy sampling by fraction is supported\n                    pass  # fall through\n                return df.sample(fraction=frac, seed=seed)\n\n        elif operation == \"filter\":\n            # Legacy or simple filter\n            pass\n\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext, PandasContext\n            from odibi.registry import FunctionRegistry\n\n            if FunctionRegistry.has_function(operation):\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df (use PandasContext as placeholder)\n                engine_ctx = EngineContext(\n                    context=PandasContext(),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n        return df\n\n    def get_schema(self, df: Any) -&gt; Any:\n        \"\"\"Get DataFrame schema.\"\"\"\n        # Polars schema is a dict {name: DataType}\n        # We can return a dict of strings for compatibility\n        schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n        return {name: str(dtype) for name, dtype in schema.items()}\n\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            # Expensive to count rows in LazyFrame without scan\n            # But usually shape implies (rows, cols)\n            # columns is cheap. rows requires partial scan or metadata.\n            # Fetching 1 row might give columns.\n            # For exact row count, we need collect(count)\n            cols = len(df.collect_schema().names())\n            rows = df.select(pl.len()).collect().item()\n            return (rows, cols)\n        return df.shape\n\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return df.select(pl.len()).collect().item()\n        return len(df)\n\n    def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            # efficient null count\n            return df.select([pl.col(c).null_count() for c in columns]).collect().to_dicts()[0]\n\n        return df.select([pl.col(c).null_count() for c in columns]).to_dicts()[0]\n\n    def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\"\"\"\n        failures = []\n\n        # Schema is dict-like in Polars\n        current_schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n        current_cols = current_schema.keys()\n\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(current_cols)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        if \"types\" in schema_rules:\n            for col, expected_type in schema_rules[\"types\"].items():\n                if col not in current_cols:\n                    failures.append(f\"Column '{col}' not found for type validation\")\n                    continue\n\n                actual_type = str(current_schema[col])\n                # Basic type check - simplistic string matching\n                if expected_type.lower() not in actual_type.lower():\n                    failures.append(\n                        f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate data against rules.\"\"\"\n        failures = []\n\n        # We'll materialize for complex validation or use lazy expressions\n        # Lazy is better.\n\n        # Not empty check\n        if getattr(validation_config, \"not_empty\", False):\n            count = self.count_rows(df)\n            if count == 0:\n                failures.append(\"DataFrame is empty\")\n\n        # No nulls\n        if getattr(validation_config, \"no_nulls\", None):\n            cols = validation_config.no_nulls\n            null_counts = self.count_nulls(df, cols)\n            for col, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col}' has {count} null values\")\n\n        return failures\n\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return df.limit(n).collect().to_dicts()\n        return df.head(n).to_dicts()\n\n    def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            # null_count() / count()\n            # We can do this in one expression\n            total_count = df.select(pl.len()).collect().item()\n            if total_count == 0:\n                return {col: 0.0 for col in df.collect_schema().names()}\n\n            cols = df.collect_schema().names()\n            null_counts = df.select([pl.col(c).null_count().alias(c) for c in cols]).collect()\n            return {col: null_counts[col][0] / total_count for col in cols}\n\n        total_count = len(df)\n        if total_count == 0:\n            return {col: 0.0 for col in df.columns}\n\n        null_counts = df.null_count()\n        return {col: null_counts[col][0] / total_count for col in df.columns}\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\"\"\"\n        if path:\n            full_path = connection.get_path(path)\n            return os.path.exists(full_path)\n        return False\n\n    def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n        \"\"\"Harmonize DataFrame schema.\"\"\"\n        # policy: SchemaPolicyConfig\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        # Helper to get current columns/schema\n        if isinstance(df, pl.LazyFrame):\n            current_schema = df.collect_schema()\n        else:\n            current_schema = df.schema\n\n        current_cols = current_schema.names()\n        target_cols = list(target_schema.keys())\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        # 1. Validation\n        if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and getattr(policy, \"on_new_columns\", None) == OnNewColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # 2. Transformations\n        exprs = []\n\n        # Handle Missing (Add nulls)\n        # Evolve means we keep new columns, Enforce means we select only target\n        mode = getattr(policy, \"mode\", SchemaMode.ENFORCE)\n\n        if (\n            mode == SchemaMode.EVOLVE\n            and getattr(policy, \"on_new_columns\", None) == OnNewColumns.ADD_NULLABLE\n        ):\n            # Add missing (if missing cols exist, we fill them with nulls)\n            # on_missing_columns controls what to do with missing target cols.\n            # If mode is EVOLVE, we typically keep everything?\n            # But harmonize_schema is about matching a TARGET schema.\n            # If target has cols that df doesn't:\n            # If on_missing_columns == FILL_NULL -&gt; Add them as null.\n            pass\n\n        # We should respect on_missing_columns regardless of mode?\n        if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FILL_NULL:\n            for col in missing:\n                exprs.append(pl.lit(None).alias(col))\n\n        if exprs:\n            df = df.with_columns(exprs)\n\n        # Now Select\n        if mode == SchemaMode.ENFORCE:\n            # Select only target columns.\n            # Missing columns were added above if configured.\n            # New columns (not in target) are dropped implicitly by selecting target_cols.\n            # But wait, we added exprs to df (lazy).\n\n            final_cols = []\n            for col in target_cols:\n                final_cols.append(pl.col(col))\n\n            df = df.select(final_cols)\n\n        elif mode == SchemaMode.EVOLVE:\n            # We keep new columns.\n            # If target has columns that were missing in df, we added them above (if FILL_NULL).\n            # If df has columns not in target (new_cols), we keep them.\n            pass\n\n        return df\n\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; Any:\n        \"\"\"Anonymize specified columns.\"\"\"\n        if method == \"mask\":\n            # Mask all but last 4 characters: '******1234'\n            # Regex look-around not supported in some envs.\n            # Manual approach:\n            # If len &gt; 4: repeat('*', len-4) + suffix(4)\n            # Else: keep original (or mask all? Pandas engine masked all but last 4, which implies keeping small strings?)\n            # Pandas: .str.replace(r\".(?=.{4})\", \"*\") -&gt; replaces chars that are followed by 4 chars.\n            # If str is \"123\", no char is followed by 4 chars -&gt; \"123\".\n            # If str is \"12345\", '1' is followed by '2345' (4 chars) -&gt; \"*2345\".\n\n            return df.with_columns(\n                [\n                    pl.when(pl.col(c).cast(pl.Utf8).str.len_chars() &gt; 4)\n                    .then(\n                        pl.concat_str(\n                            [\n                                pl.lit(\"*\").repeat_by(pl.col(c).str.len_chars() - 4).list.join(\"\"),\n                                pl.col(c).str.slice(-4),\n                            ]\n                        )\n                    )\n                    .otherwise(pl.col(c).cast(pl.Utf8))\n                    .alias(c)\n                    for c in columns\n                ]\n            )\n\n        elif method == \"hash\":\n            # Polars hash() is non-cryptographic usually (xxHash).\n            # For cryptographic hash (sha256), we might need map_elements (slow) or plugin.\n            # Requirement is just 'hash', often consistent for analytics.\n            # Gap Analysis mentions \"salt\".\n            # PandasEngine used sha256 with salt.\n            # Polars `hash` is fast 64-bit hash.\n            # If we need SHA256, we must use map_elements (python UDF) or custom.\n            # For \"High Performance\", map_elements is bad.\n            # However, without native plugin, we have no choice for SHA256.\n            # Let's implement SHA256 via map_elements for compatibility,\n            # OR use Polars internal hash if user accepts non-crypto.\n            # But \"salt\" implies security/crypto usage.\n\n            def _hash_val(val):\n                if val is None:\n                    return None\n                to_hash = str(val)\n                if salt:\n                    to_hash += salt\n                return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n            # Apply to each column. Warning: Slow path.\n            # But Polars UDFs are still faster than Pandas apply often due to no GIL? No, Python UDF has GIL.\n            return df.with_columns(\n                [pl.col(c).map_elements(_hash_val, return_dtype=pl.Utf8).alias(c) for c in columns]\n            )\n\n        elif method == \"redact\":\n            return df.with_columns([pl.lit(\"[REDACTED]\").alias(c) for c in columns])\n\n        return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.__init__","title":"<code>__init__(connections=None, config=None)</code>","text":"<p>Initialize Polars engine.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Polars engine.\n\n    Args:\n        connections: Dictionary of connection objects\n        config: Engine configuration (optional)\n    \"\"\"\n    if pl is None:\n        raise ImportError(\"Polars not installed. Run 'pip install polars'.\")\n\n    self.connections = connections or {}\n    self.config = config or {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize specified columns.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; Any:\n    \"\"\"Anonymize specified columns.\"\"\"\n    if method == \"mask\":\n        # Mask all but last 4 characters: '******1234'\n        # Regex look-around not supported in some envs.\n        # Manual approach:\n        # If len &gt; 4: repeat('*', len-4) + suffix(4)\n        # Else: keep original (or mask all? Pandas engine masked all but last 4, which implies keeping small strings?)\n        # Pandas: .str.replace(r\".(?=.{4})\", \"*\") -&gt; replaces chars that are followed by 4 chars.\n        # If str is \"123\", no char is followed by 4 chars -&gt; \"123\".\n        # If str is \"12345\", '1' is followed by '2345' (4 chars) -&gt; \"*2345\".\n\n        return df.with_columns(\n            [\n                pl.when(pl.col(c).cast(pl.Utf8).str.len_chars() &gt; 4)\n                .then(\n                    pl.concat_str(\n                        [\n                            pl.lit(\"*\").repeat_by(pl.col(c).str.len_chars() - 4).list.join(\"\"),\n                            pl.col(c).str.slice(-4),\n                        ]\n                    )\n                )\n                .otherwise(pl.col(c).cast(pl.Utf8))\n                .alias(c)\n                for c in columns\n            ]\n        )\n\n    elif method == \"hash\":\n        # Polars hash() is non-cryptographic usually (xxHash).\n        # For cryptographic hash (sha256), we might need map_elements (slow) or plugin.\n        # Requirement is just 'hash', often consistent for analytics.\n        # Gap Analysis mentions \"salt\".\n        # PandasEngine used sha256 with salt.\n        # Polars `hash` is fast 64-bit hash.\n        # If we need SHA256, we must use map_elements (python UDF) or custom.\n        # For \"High Performance\", map_elements is bad.\n        # However, without native plugin, we have no choice for SHA256.\n        # Let's implement SHA256 via map_elements for compatibility,\n        # OR use Polars internal hash if user accepts non-crypto.\n        # But \"salt\" implies security/crypto usage.\n\n        def _hash_val(val):\n            if val is None:\n                return None\n            to_hash = str(val)\n            if salt:\n                to_hash += salt\n            return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n        # Apply to each column. Warning: Slow path.\n        # But Polars UDFs are still faster than Pandas apply often due to no GIL? No, Python UDF has GIL.\n        return df.with_columns(\n            [pl.col(c).map_elements(_hash_val, return_dtype=pl.Utf8).alias(c) for c in columns]\n        )\n\n    elif method == \"redact\":\n        return df.with_columns([pl.lit(\"[REDACTED]\").alias(c) for c in columns])\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        # efficient null count\n        return df.select([pl.col(c).null_count() for c in columns]).collect().to_dicts()[0]\n\n    return df.select([pl.col(c).null_count() for c in columns]).to_dicts()[0]\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return df.select(pl.len()).collect().item()\n    return len(df)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n    \"\"\"Execute built-in operation.\"\"\"\n    # Ensure LazyFrame for consistency if possible, but operations work on both usually.\n    # If DataFrame, some operations might need different methods.\n\n    if operation == \"pivot\":\n        # Pivot requires materialization usually in other engines, but Polars LazyFrame has 'collect' or similar constraints?\n        # Polars lazy pivot is not fully supported in older versions without collect, but check recent.\n        # Pivot changes shape drastically.\n        # params: pivot_column, value_column, group_by, agg_func\n\n        # If lazy, we might need to collect for pivot if lazy pivot isn't supported or experimental.\n        # But let's try to keep it lazy if possible.\n        # As of recent Polars, pivot is available on DataFrame, experimental on LazyFrame?\n        # Actually, 'unstack' or 'pivot' on LazyFrame is limited.\n        # Safe bet: materialize if needed, or use lazy pivot if available.\n\n        # Let's collect if input is lazy, because pivot usually implies strict schema change hard to predict.\n        if isinstance(df, pl.LazyFrame):\n            df = df.collect()\n\n        return df.pivot(\n            index=params.get(\"group_by\"),\n            on=params[\"pivot_column\"],\n            values=params[\"value_column\"],\n            aggregate_function=params.get(\"agg_func\", \"first\"),\n        )  # Returns DataFrame\n\n    elif operation == \"drop_duplicates\":\n        subset = params.get(\"subset\")\n        if isinstance(df, pl.LazyFrame):\n            return df.unique(subset=subset)\n        return df.unique(subset=subset)\n\n    elif operation == \"fillna\":\n        value = params.get(\"value\")\n        # Polars uses fill_null\n        if isinstance(value, dict):\n            # Fill specific columns\n            # value = {'col1': 0, 'col2': 'unknown'}\n            # We need to chain with_columns\n            exprs = []\n            for col, val in value.items():\n                exprs.append(pl.col(col).fill_null(val))\n            return df.with_columns(exprs)\n        else:\n            # Fill all columns? Polars fill_null requires specifying columns or using all()\n            return df.fill_null(value)\n\n    elif operation == \"drop\":\n        columns = params.get(\"columns\") or params.get(\"labels\")\n        return df.drop(columns)\n\n    elif operation == \"rename\":\n        columns = params.get(\"columns\") or params.get(\"mapper\")\n        return df.rename(columns)\n\n    elif operation == \"sort\":\n        by = params.get(\"by\")\n        descending = not params.get(\"ascending\", True)\n        if isinstance(df, pl.LazyFrame):\n            return df.sort(by, descending=descending)\n        return df.sort(by, descending=descending)\n\n    elif operation == \"sample\":\n        # Sample n or frac\n        n = params.get(\"n\")\n        frac = params.get(\"frac\")\n        seed = params.get(\"random_state\")\n\n        # Lazy sample supported\n        if n is not None:\n            # Note: Polars Lazy sample might be approximate or require 'collect' depending on version/backend?\n            # But usually supported.\n            if isinstance(df, pl.LazyFrame):\n                # LazyFrame.sample takes n (int) or fraction.\n                # But polars 0.19+ changed sample signature?\n                # It's generally `sample(n=..., fraction=..., seed=...)`\n                return (\n                    df.collect().sample(n=n, seed=seed).lazy()\n                )  # Collecting for exact sample n on lazy might be needed if not supported?\n                # Actually, fetch(n) is head. Sample is random.\n                # Let's materialize for safety with sample as it's often for checks.\n                pass\n            return df.sample(n=n, seed=seed)\n        elif frac is not None:\n            if isinstance(df, pl.LazyFrame):\n                # Lazy sampling by fraction is supported\n                pass  # fall through\n            return df.sample(fraction=frac, seed=seed)\n\n    elif operation == \"filter\":\n        # Legacy or simple filter\n        pass\n\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext, PandasContext\n        from odibi.registry import FunctionRegistry\n\n        if FunctionRegistry.has_function(operation):\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df (use PandasContext as placeholder)\n            engine_ctx = EngineContext(\n                context=PandasContext(),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.execute_sql","title":"<code>execute_sql(sql, context)</code>","text":"<p>Execute SQL query using Polars SQLContext.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context with registered DataFrames</p> required <p>Returns:</p> Type Description <code>Any</code> <p>pl.LazyFrame</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Context) -&gt; Any:\n    \"\"\"Execute SQL query using Polars SQLContext.\n\n    Args:\n        sql: SQL query string\n        context: Execution context with registered DataFrames\n\n    Returns:\n        pl.LazyFrame\n    \"\"\"\n    ctx = pl.SQLContext()\n\n    # Register datasets from context\n    # We iterate over all registered names in the context\n    try:\n        names = context.list_names()\n        for name in names:\n            df = context.get(name)\n            # Register LazyFrame or DataFrame\n            # Polars SQLContext supports registering LazyFrame, DataFrame, and some others\n            # We might need to convert if it's not a Polars object, but we assume Polars engine uses Polars objects\n            ctx.register(name, df)\n    except Exception:\n        # If context doesn't support listing or getting, we proceed with empty context\n        # (e.g. if context is not fully compatible or empty)\n        pass\n\n    return ctx.execute(sql, eager=False)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return df.limit(n).collect().to_dicts()\n    return df.head(n).to_dicts()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_schema(self, df: Any) -&gt; Any:\n    \"\"\"Get DataFrame schema.\"\"\"\n    # Polars schema is a dict {name: DataType}\n    # We can return a dict of strings for compatibility\n    schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n    return {name: str(dtype) for name, dtype in schema.items()}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        # Expensive to count rows in LazyFrame without scan\n        # But usually shape implies (rows, cols)\n        # columns is cheap. rows requires partial scan or metadata.\n        # Fetching 1 row might give columns.\n        # For exact row count, we need collect(count)\n        cols = len(df.collect_schema().names())\n        rows = df.select(pl.len()).collect().item()\n        return (rows, cols)\n    return df.shape\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n    \"\"\"Harmonize DataFrame schema.\"\"\"\n    # policy: SchemaPolicyConfig\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    # Helper to get current columns/schema\n    if isinstance(df, pl.LazyFrame):\n        current_schema = df.collect_schema()\n    else:\n        current_schema = df.schema\n\n    current_cols = current_schema.names()\n    target_cols = list(target_schema.keys())\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    # 1. Validation\n    if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and getattr(policy, \"on_new_columns\", None) == OnNewColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # 2. Transformations\n    exprs = []\n\n    # Handle Missing (Add nulls)\n    # Evolve means we keep new columns, Enforce means we select only target\n    mode = getattr(policy, \"mode\", SchemaMode.ENFORCE)\n\n    if (\n        mode == SchemaMode.EVOLVE\n        and getattr(policy, \"on_new_columns\", None) == OnNewColumns.ADD_NULLABLE\n    ):\n        # Add missing (if missing cols exist, we fill them with nulls)\n        # on_missing_columns controls what to do with missing target cols.\n        # If mode is EVOLVE, we typically keep everything?\n        # But harmonize_schema is about matching a TARGET schema.\n        # If target has cols that df doesn't:\n        # If on_missing_columns == FILL_NULL -&gt; Add them as null.\n        pass\n\n    # We should respect on_missing_columns regardless of mode?\n    if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FILL_NULL:\n        for col in missing:\n            exprs.append(pl.lit(None).alias(col))\n\n    if exprs:\n        df = df.with_columns(exprs)\n\n    # Now Select\n    if mode == SchemaMode.ENFORCE:\n        # Select only target columns.\n        # Missing columns were added above if configured.\n        # New columns (not in target) are dropped implicitly by selecting target_cols.\n        # But wait, we added exprs to df (lazy).\n\n        final_cols = []\n        for col in target_cols:\n            final_cols.append(pl.col(col))\n\n        df = df.select(final_cols)\n\n    elif mode == SchemaMode.EVOLVE:\n        # We keep new columns.\n        # If target has columns that were missing in df, we added them above (if FILL_NULL).\n        # If df has columns not in target (new_cols), we keep them.\n        pass\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset into memory (DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>LazyFrame or DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Materialized DataFrame (pl.DataFrame)</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n    Args:\n        df: LazyFrame or DataFrame\n\n    Returns:\n        Materialized DataFrame (pl.DataFrame)\n    \"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return df.collect()\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        # null_count() / count()\n        # We can do this in one expression\n        total_count = df.select(pl.len()).collect().item()\n        if total_count == 0:\n            return {col: 0.0 for col in df.collect_schema().names()}\n\n        cols = df.collect_schema().names()\n        null_counts = df.select([pl.col(c).null_count().alias(c) for c in cols]).collect()\n        return {col: null_counts[col][0] / total_count for col in cols}\n\n    total_count = len(df)\n    if total_count == 0:\n        return {col: 0.0 for col in df.columns}\n\n    null_counts = df.null_count()\n    return {col: null_counts[col][0] / total_count for col in df.columns}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, **kwargs)</code>","text":"<p>Read data using Polars (Lazy by default).</p> <p>Returns:</p> Type Description <code>Any</code> <p>pl.LazyFrame or pl.DataFrame</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Read data using Polars (Lazy by default).\n\n    Returns:\n        pl.LazyFrame or pl.DataFrame\n    \"\"\"\n    options = options or {}\n\n    # Get full path\n    if path:\n        if connection:\n            full_path = connection.get_path(path)\n        else:\n            full_path = path\n    elif table:\n        if connection:\n            full_path = connection.get_path(table)\n        else:\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n    else:\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Handle glob patterns/lists\n    # Polars scan methods often support glob strings directly.\n\n    try:\n        if format == \"csv\":\n            # scan_csv supports glob patterns\n            return pl.scan_csv(full_path, **options)\n\n        elif format == \"parquet\":\n            return pl.scan_parquet(full_path, **options)\n\n        elif format == \"json\":\n            # scan_ndjson for newline delimited json, read_json for standard\n            # Assuming ndjson/jsonl for big data usually\n            if options.get(\"json_lines\", True):  # Default to ndjson scan\n                return pl.scan_ndjson(full_path, **options)\n            else:\n                # Standard JSON doesn't support lazy scan well in all versions, fallback to read\n                return pl.read_json(full_path, **options).lazy()\n\n        elif format == \"delta\":\n            # scan_delta requires 'deltalake' extra usually or feature\n            storage_options = options.get(\"storage_options\", None)\n            version = options.get(\"versionAsOf\", None)\n\n            # scan_delta is available in recent polars\n            # It might accept storage_options in recent versions\n            delta_opts = {}\n            if storage_options:\n                delta_opts[\"storage_options\"] = storage_options\n            if version is not None:\n                delta_opts[\"version\"] = version\n\n            return pl.scan_delta(full_path, **delta_opts)\n\n        else:\n            raise ValueError(f\"Unsupported format for Polars engine: {format}\")\n\n    except Exception as e:\n        # Fallback or error handling\n        raise ValueError(f\"Failed to read {format} from {full_path}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\"\"\"\n    if path:\n        full_path = connection.get_path(path)\n        return os.path.exists(full_path)\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate data against rules.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate data against rules.\"\"\"\n    failures = []\n\n    # We'll materialize for complex validation or use lazy expressions\n    # Lazy is better.\n\n    # Not empty check\n    if getattr(validation_config, \"not_empty\", False):\n        count = self.count_rows(df)\n        if count == 0:\n            failures.append(\"DataFrame is empty\")\n\n    # No nulls\n    if getattr(validation_config, \"no_nulls\", None):\n        cols = validation_config.no_nulls\n        null_counts = self.count_nulls(df, cols)\n        for col, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col}' has {count} null values\")\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\"\"\"\n    failures = []\n\n    # Schema is dict-like in Polars\n    current_schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n    current_cols = current_schema.keys()\n\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(current_cols)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    if \"types\" in schema_rules:\n        for col, expected_type in schema_rules[\"types\"].items():\n            if col not in current_cols:\n                failures.append(f\"Column '{col}' not found for type validation\")\n                continue\n\n            actual_type = str(current_schema[col])\n            # Basic type check - simplistic string matching\n            if expected_type.lower() not in actual_type.lower():\n                failures.append(\n                    f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.write","title":"<code>write(df, connection, format, table=None, path=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Polars.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Write data using Polars.\"\"\"\n    options = options or {}\n\n    if path:\n        if connection:\n            full_path = connection.get_path(path)\n        else:\n            full_path = path\n    elif table:\n        if connection:\n            full_path = connection.get_path(table)\n        else:\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n    else:\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Polars sink (streaming write) is preferred for LazyFrames\n    is_lazy = isinstance(df, pl.LazyFrame)\n\n    # Create directory if needed\n    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n\n    if format == \"parquet\":\n        if is_lazy:\n            # sink_parquet is efficient\n            df.sink_parquet(full_path, **options)\n        else:\n            df.write_parquet(full_path, **options)\n\n    elif format == \"csv\":\n        if is_lazy:\n            df.sink_csv(full_path, **options)\n        else:\n            df.write_csv(full_path, **options)\n\n    elif format == \"json\":\n        if is_lazy:\n            df.sink_ndjson(full_path, **options)\n        else:\n            df.write_ndjson(full_path, **options)\n\n    elif format == \"delta\":\n        # Polars write_delta / sink_delta might be experimental or require specific setup\n        # For now, we might need to materialize and use write_delta if sink not available\n        # Or use deltalake library directly if polars doesn't support writing yet\n\n        # As of recent polars, write_delta exists for DataFrame\n        if is_lazy:\n            df = df.collect()\n\n        storage_options = options.get(\"storage_options\", None)\n        delta_write_options = options.copy()\n        if \"storage_options\" in delta_write_options:\n            del delta_write_options[\"storage_options\"]\n\n        df.write_delta(\n            full_path, mode=mode, storage_options=storage_options, **delta_write_options\n        )\n\n    else:\n        raise ValueError(f\"Unsupported write format for Polars: {format}\")\n</code></pre>"},{"location":"reference/api/patterns/","title":"Patterns API","text":""},{"location":"reference/api/patterns/#odibi.patterns.base","title":"<code>odibi.patterns.base</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.scd2","title":"<code>odibi.patterns.scd2</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.scd2.SCD2Pattern","title":"<code>SCD2Pattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>SCD2 Pattern: Slowly Changing Dimension Type 2.</p> <p>Tracks history by creating new rows for updates.</p> <p>Configuration Options (via params dict):     - keys (list): Business keys.     - time_col (str): Timestamp column for versioning (default: current time).     - valid_from_col (str): Name of start date column (default: valid_from).     - valid_to_col (str): Name of end date column (default: valid_to).     - is_current_col (str): Name of current flag column (default: is_current).</p> Source code in <code>odibi\\patterns\\scd2.py</code> <pre><code>class SCD2Pattern(Pattern):\n    \"\"\"\n    SCD2 Pattern: Slowly Changing Dimension Type 2.\n\n    Tracks history by creating new rows for updates.\n\n    Configuration Options (via params dict):\n        - **keys** (list): Business keys.\n        - **time_col** (str): Timestamp column for versioning (default: current time).\n        - **valid_from_col** (str): Name of start date column (default: valid_from).\n        - **valid_to_col** (str): Name of end date column (default: valid_to).\n        - **is_current_col** (str): Name of current flag column (default: is_current).\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"SCD2Pattern validation starting\",\n            pattern=\"SCD2Pattern\",\n            keys=self.params.get(\"keys\"),\n            target=self.params.get(\"target\"),\n        )\n\n        if not self.params.get(\"keys\"):\n            ctx.error(\n                \"SCD2Pattern validation failed: 'keys' parameter is required\",\n                pattern=\"SCD2Pattern\",\n            )\n            raise ValueError(\"SCD2Pattern: 'keys' parameter is required.\")\n        if not self.params.get(\"target\"):\n            ctx.error(\n                \"SCD2Pattern validation failed: 'target' parameter is required\",\n                pattern=\"SCD2Pattern\",\n            )\n            raise ValueError(\"SCD2Pattern: 'target' parameter is required (table name or path).\")\n\n        ctx.debug(\n            \"SCD2Pattern validation passed\",\n            pattern=\"SCD2Pattern\",\n            keys=self.params.get(\"keys\"),\n            target=self.params.get(\"target\"),\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        keys = self.params.get(\"keys\")\n        target = self.params.get(\"target\")\n        valid_from_col = self.params.get(\"valid_from_col\", \"valid_from\")\n        valid_to_col = self.params.get(\"valid_to_col\", \"valid_to\")\n        is_current_col = self.params.get(\"is_current_col\", \"is_current\")\n        track_cols = self.params.get(\"track_cols\")\n\n        ctx.debug(\n            \"SCD2 pattern starting\",\n            pattern=\"SCD2Pattern\",\n            keys=keys,\n            target=target,\n            valid_from_col=valid_from_col,\n            valid_to_col=valid_to_col,\n            is_current_col=is_current_col,\n            track_cols=track_cols,\n        )\n\n        source_count = None\n        try:\n            if context.engine_type == \"spark\":\n                source_count = context.df.count()\n            else:\n                source_count = len(context.df)\n            ctx.debug(\"SCD2 source data loaded\", pattern=\"SCD2Pattern\", source_rows=source_count)\n        except Exception:\n            ctx.debug(\"SCD2 could not determine source row count\", pattern=\"SCD2Pattern\")\n\n        valid_keys = SCD2Params.model_fields.keys()\n        filtered_params = {k: v for k, v in self.params.items() if k in valid_keys}\n\n        try:\n            scd_params = SCD2Params(**filtered_params)\n        except Exception as e:\n            ctx.error(\n                f\"SCD2 invalid parameters: {e}\",\n                pattern=\"SCD2Pattern\",\n                error_type=type(e).__name__,\n                params=filtered_params,\n            )\n            raise ValueError(f\"Invalid SCD2 parameters: {e}\")\n\n        try:\n            result_ctx = scd2(context, scd_params)\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"SCD2 pattern execution failed: {e}\",\n                pattern=\"SCD2Pattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n        result_df = result_ctx.df\n        elapsed_ms = (time.time() - start_time) * 1000\n\n        result_count = None\n        try:\n            if context.engine_type == \"spark\":\n                result_count = result_df.count()\n            else:\n                result_count = len(result_df)\n        except Exception:\n            pass\n\n        ctx.info(\n            \"SCD2 pattern completed\",\n            pattern=\"SCD2Pattern\",\n            elapsed_ms=round(elapsed_ms, 2),\n            source_rows=source_count,\n            result_rows=result_count,\n            keys=keys,\n            target=target,\n            valid_from_col=valid_from_col,\n            valid_to_col=valid_to_col,\n        )\n\n        return result_df\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.merge","title":"<code>odibi.patterns.merge</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.merge.MergePattern","title":"<code>MergePattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Merge Pattern: Upsert/Merge logic.</p> <p>Configuration Options (via params dict):     - target (str): Target table/path.     - keys (list): Join keys.     - strategy (str): 'upsert', 'append_only', 'delete_match'.</p> Source code in <code>odibi\\patterns\\merge.py</code> <pre><code>class MergePattern(Pattern):\n    \"\"\"\n    Merge Pattern: Upsert/Merge logic.\n\n    Configuration Options (via params dict):\n        - **target** (str): Target table/path.\n        - **keys** (list): Join keys.\n        - **strategy** (str): 'upsert', 'append_only', 'delete_match'.\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"MergePattern validation starting\",\n            pattern=\"MergePattern\",\n            target=self.params.get(\"target\"),\n            keys=self.params.get(\"keys\"),\n            strategy=self.params.get(\"strategy\"),\n        )\n\n        if not self.params.get(\"target\"):\n            ctx.error(\n                \"MergePattern validation failed: 'target' is required\",\n                pattern=\"MergePattern\",\n            )\n            raise ValueError(\"MergePattern: 'target' is required.\")\n        if not self.params.get(\"keys\"):\n            ctx.error(\n                \"MergePattern validation failed: 'keys' is required\",\n                pattern=\"MergePattern\",\n            )\n            raise ValueError(\"MergePattern: 'keys' is required.\")\n\n        ctx.debug(\n            \"MergePattern validation passed\",\n            pattern=\"MergePattern\",\n            target=self.params.get(\"target\"),\n            keys=self.params.get(\"keys\"),\n            strategy=self.params.get(\"strategy\", \"upsert\"),\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        target = self.params.get(\"target\")\n        keys = self.params.get(\"keys\")\n        strategy = self.params.get(\"strategy\", \"upsert\")\n\n        ctx.debug(\n            \"Merge pattern starting\",\n            pattern=\"MergePattern\",\n            target=target,\n            keys=keys,\n            strategy=strategy,\n        )\n\n        source_count = None\n        try:\n            if context.engine_type == \"spark\":\n                source_count = context.df.count()\n            else:\n                source_count = len(context.df)\n            ctx.debug(\n                \"Merge source data loaded\",\n                pattern=\"MergePattern\",\n                source_rows=source_count,\n            )\n        except Exception:\n            ctx.debug(\"Merge could not determine source row count\", pattern=\"MergePattern\")\n\n        valid_keys = MergeParams.model_fields.keys()\n        filtered_params = {k: v for k, v in self.params.items() if k in valid_keys}\n\n        try:\n            merge(context, context.df, **filtered_params)\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Merge pattern execution failed: {e}\",\n                pattern=\"MergePattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n                target=target,\n                keys=keys,\n                strategy=strategy,\n            )\n            raise\n\n        elapsed_ms = (time.time() - start_time) * 1000\n\n        ctx.info(\n            \"Merge pattern completed\",\n            pattern=\"MergePattern\",\n            elapsed_ms=round(elapsed_ms, 2),\n            source_rows=source_count,\n            target=target,\n            keys=keys,\n            strategy=strategy,\n        )\n\n        return context.df\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.dimension","title":"<code>odibi.patterns.dimension</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.dimension.AuditConfig","title":"<code>AuditConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for audit columns.</p> Source code in <code>odibi\\patterns\\dimension.py</code> <pre><code>class AuditConfig(BaseModel):\n    \"\"\"Configuration for audit columns.\"\"\"\n\n    load_timestamp: bool = Field(default=True, description=\"Add load_timestamp column\")\n    source_system: Optional[str] = Field(\n        default=None, description=\"Source system name for source_system column\"\n    )\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.dimension.DimensionPattern","title":"<code>DimensionPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Dimension Pattern: Builds complete dimension tables with surrogate keys and SCD support.</p> <p>Features: - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> <p>Configuration Options (via params dict):     - natural_key (str): Natural/business key column name     - surrogate_key (str): Surrogate key column name to generate     - scd_type (int): 0=static, 1=overwrite, 2=history tracking (default: 1)     - track_cols (list): Columns to track for SCD1/2 changes     - target (str): Target table path (required for SCD2 to read existing history)     - unknown_member (bool): If true, insert a row with SK=0 for orphan FK handling     - audit (dict): Audit configuration with load_timestamp and source_system</p> Supported target formats <p>Spark:     - Catalog tables: catalog.schema.table, warehouse.dim_customer     - Delta paths: /path/to/delta (no extension)     - Parquet: /path/to/file.parquet     - CSV: /path/to/file.csv     - JSON: /path/to/file.json     - ORC: /path/to/file.orc Pandas:     - Parquet: path/to/file.parquet (or directory)     - CSV: path/to/file.csv     - JSON: path/to/file.json     - Excel: path/to/file.xlsx, path/to/file.xls     - Feather/Arrow: path/to/file.feather, path/to/file.arrow     - Pickle: path/to/file.pickle, path/to/file.pkl     - Connection-prefixed: warehouse.dim_customer</p> Source code in <code>odibi\\patterns\\dimension.py</code> <pre><code>class DimensionPattern(Pattern):\n    \"\"\"\n    Dimension Pattern: Builds complete dimension tables with surrogate keys and SCD support.\n\n    Features:\n    - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows)\n    - SCD Type 0 (static), 1 (overwrite), 2 (history tracking)\n    - Optional unknown member row (SK=0) for orphan FK handling\n    - Audit columns (load_timestamp, source_system)\n\n    Configuration Options (via params dict):\n        - **natural_key** (str): Natural/business key column name\n        - **surrogate_key** (str): Surrogate key column name to generate\n        - **scd_type** (int): 0=static, 1=overwrite, 2=history tracking (default: 1)\n        - **track_cols** (list): Columns to track for SCD1/2 changes\n        - **target** (str): Target table path (required for SCD2 to read existing history)\n        - **unknown_member** (bool): If true, insert a row with SK=0 for orphan FK handling\n        - **audit** (dict): Audit configuration with load_timestamp and source_system\n\n    Supported target formats:\n        Spark:\n            - Catalog tables: catalog.schema.table, warehouse.dim_customer\n            - Delta paths: /path/to/delta (no extension)\n            - Parquet: /path/to/file.parquet\n            - CSV: /path/to/file.csv\n            - JSON: /path/to/file.json\n            - ORC: /path/to/file.orc\n        Pandas:\n            - Parquet: path/to/file.parquet (or directory)\n            - CSV: path/to/file.csv\n            - JSON: path/to/file.json\n            - Excel: path/to/file.xlsx, path/to/file.xls\n            - Feather/Arrow: path/to/file.feather, path/to/file.arrow\n            - Pickle: path/to/file.pickle, path/to/file.pkl\n            - Connection-prefixed: warehouse.dim_customer\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"DimensionPattern validation starting\",\n            pattern=\"DimensionPattern\",\n            params=self.params,\n        )\n\n        if not self.params.get(\"natural_key\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'natural_key' is required\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\"DimensionPattern: 'natural_key' parameter is required.\")\n\n        if not self.params.get(\"surrogate_key\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'surrogate_key' is required\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\"DimensionPattern: 'surrogate_key' parameter is required.\")\n\n        scd_type = self.params.get(\"scd_type\", 1)\n        if scd_type not in (0, 1, 2):\n            ctx.error(\n                f\"DimensionPattern validation failed: invalid scd_type {scd_type}\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(f\"DimensionPattern: 'scd_type' must be 0, 1, or 2. Got: {scd_type}\")\n\n        if scd_type == 2 and not self.params.get(\"target\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'target' required for SCD2\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\"DimensionPattern: 'target' parameter is required for scd_type=2.\")\n\n        if scd_type in (1, 2) and not self.params.get(\"track_cols\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'track_cols' required for SCD1/2\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\n                \"DimensionPattern: 'track_cols' parameter is required for scd_type 1 or 2.\"\n            )\n\n        ctx.debug(\n            \"DimensionPattern validation passed\",\n            pattern=\"DimensionPattern\",\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        natural_key = self.params.get(\"natural_key\")\n        surrogate_key = self.params.get(\"surrogate_key\")\n        scd_type = self.params.get(\"scd_type\", 1)\n        track_cols = self.params.get(\"track_cols\", [])\n        target = self.params.get(\"target\")\n        unknown_member = self.params.get(\"unknown_member\", False)\n        audit_config = self.params.get(\"audit\", {})\n\n        ctx.debug(\n            \"DimensionPattern starting\",\n            pattern=\"DimensionPattern\",\n            natural_key=natural_key,\n            surrogate_key=surrogate_key,\n            scd_type=scd_type,\n            track_cols=track_cols,\n            target=target,\n            unknown_member=unknown_member,\n        )\n\n        source_count = self._get_row_count(context.df, context.engine_type)\n        ctx.debug(\"Dimension source loaded\", pattern=\"DimensionPattern\", source_rows=source_count)\n\n        try:\n            if scd_type == 0:\n                result_df = self._execute_scd0(context, natural_key, surrogate_key, target)\n            elif scd_type == 1:\n                result_df = self._execute_scd1(\n                    context, natural_key, surrogate_key, track_cols, target\n                )\n            else:\n                result_df = self._execute_scd2(\n                    context, natural_key, surrogate_key, track_cols, target\n                )\n\n            result_df = self._add_audit_columns(context, result_df, audit_config)\n\n            if unknown_member:\n                result_df = self._ensure_unknown_member(\n                    context, result_df, natural_key, surrogate_key, audit_config\n                )\n\n            result_count = self._get_row_count(result_df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"DimensionPattern completed\",\n                pattern=\"DimensionPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                source_rows=source_count,\n                result_rows=result_count,\n                scd_type=scd_type,\n            )\n\n            return result_df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"DimensionPattern failed: {e}\",\n                pattern=\"DimensionPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _load_existing_target(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table if it exists.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._load_existing_spark(context, target)\n        else:\n            return self._load_existing_pandas(context, target)\n\n    def _load_existing_spark(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table from Spark with multi-format support.\"\"\"\n        ctx = get_logging_context()\n        spark = context.spark\n\n        # Try catalog table first\n        try:\n            return spark.table(target)\n        except Exception:\n            pass\n\n        # Check file extension for format detection\n        target_lower = target.lower()\n\n        try:\n            if target_lower.endswith(\".parquet\"):\n                return spark.read.parquet(target)\n            elif target_lower.endswith(\".csv\"):\n                return spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(target)\n            elif target_lower.endswith(\".json\"):\n                return spark.read.json(target)\n            elif target_lower.endswith(\".orc\"):\n                return spark.read.orc(target)\n            else:\n                # Try Delta format as fallback (for paths without extension)\n                return spark.read.format(\"delta\").load(target)\n        except Exception as e:\n            ctx.warning(\n                f\"Could not load existing target '{target}': {e}. Treating as initial load.\",\n                pattern=\"DimensionPattern\",\n                target=target,\n            )\n            return None\n\n    def _load_existing_pandas(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table from Pandas with multi-format support.\"\"\"\n        import os\n\n        import pandas as pd\n\n        ctx = get_logging_context()\n        path = target\n\n        # Handle connection-prefixed paths\n        if hasattr(context, \"engine\") and context.engine:\n            if \".\" in path:\n                parts = path.split(\".\", 1)\n                conn_name = parts[0]\n                rel_path = parts[1]\n                if conn_name in context.engine.connections:\n                    try:\n                        path = context.engine.connections[conn_name].get_path(rel_path)\n                    except Exception:\n                        pass\n\n        if not os.path.exists(path):\n            return None\n\n        path_lower = str(path).lower()\n\n        try:\n            # Parquet (file or directory)\n            if path_lower.endswith(\".parquet\") or os.path.isdir(path):\n                return pd.read_parquet(path)\n            # CSV\n            elif path_lower.endswith(\".csv\"):\n                return pd.read_csv(path)\n            # JSON\n            elif path_lower.endswith(\".json\"):\n                return pd.read_json(path)\n            # Excel\n            elif path_lower.endswith(\".xlsx\") or path_lower.endswith(\".xls\"):\n                return pd.read_excel(path)\n            # Feather / Arrow IPC\n            elif path_lower.endswith(\".feather\") or path_lower.endswith(\".arrow\"):\n                return pd.read_feather(path)\n            # Pickle\n            elif path_lower.endswith(\".pickle\") or path_lower.endswith(\".pkl\"):\n                return pd.read_pickle(path)\n            else:\n                ctx.warning(\n                    f\"Unrecognized file format for target '{target}'. \"\n                    \"Supported formats: parquet, csv, json, xlsx, xls, feather, arrow, pickle. \"\n                    \"Treating as initial load.\",\n                    pattern=\"DimensionPattern\",\n                    target=target,\n                )\n                return None\n        except Exception as e:\n            ctx.warning(\n                f\"Could not load existing target '{target}': {e}. Treating as initial load.\",\n                pattern=\"DimensionPattern\",\n                target=target,\n            )\n            return None\n\n    def _get_max_sk(self, df, surrogate_key: str, engine_type) -&gt; int:\n        \"\"\"Get the maximum surrogate key value from existing data.\"\"\"\n        if df is None:\n            return 0\n        try:\n            if engine_type == EngineType.SPARK:\n                from pyspark.sql import functions as F\n\n                max_row = df.agg(F.max(surrogate_key)).collect()[0]\n                max_val = max_row[0]\n                return max_val if max_val is not None else 0\n            else:\n                if surrogate_key not in df.columns:\n                    return 0\n                max_val = df[surrogate_key].max()\n                return int(max_val) if max_val is not None and not (max_val != max_val) else 0\n        except Exception:\n            return 0\n\n    def _generate_surrogate_keys(\n        self,\n        context: EngineContext,\n        df,\n        natural_key: str,\n        surrogate_key: str,\n        start_sk: int,\n    ):\n        \"\"\"Generate surrogate keys starting from start_sk + 1.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n            from pyspark.sql.window import Window\n\n            window = Window.orderBy(natural_key)\n            df = df.withColumn(\n                surrogate_key, (F.row_number().over(window) + F.lit(start_sk)).cast(\"int\")\n            )\n            return df\n        else:\n            df = df.copy()\n            df = df.sort_values(by=natural_key).reset_index(drop=True)\n            df[surrogate_key] = range(start_sk + 1, start_sk + 1 + len(df))\n            df[surrogate_key] = df[surrogate_key].astype(\"int64\")\n            return df\n\n    def _execute_scd0(\n        self,\n        context: EngineContext,\n        natural_key: str,\n        surrogate_key: str,\n        target: Optional[str],\n    ):\n        \"\"\"\n        SCD Type 0: Static dimension - never update existing records.\n        Only insert new records that don't exist in target.\n        \"\"\"\n        existing_df = self._load_existing_target(context, target) if target else None\n        source_df = context.df\n\n        if existing_df is None:\n            return self._generate_surrogate_keys(\n                context, source_df, natural_key, surrogate_key, start_sk=0\n            )\n\n        max_sk = self._get_max_sk(existing_df, surrogate_key, context.engine_type)\n\n        if context.engine_type == EngineType.SPARK:\n            existing_keys = existing_df.select(natural_key).distinct()\n            new_records = source_df.join(existing_keys, on=natural_key, how=\"left_anti\")\n        else:\n            existing_keys = set(existing_df[natural_key].unique())\n            new_records = source_df[~source_df[natural_key].isin(existing_keys)].copy()\n\n        if self._get_row_count(new_records, context.engine_type) == 0:\n            return existing_df\n\n        new_with_sk = self._generate_surrogate_keys(\n            context, new_records, natural_key, surrogate_key, start_sk=max_sk\n        )\n\n        if context.engine_type == EngineType.SPARK:\n            return existing_df.unionByName(new_with_sk, allowMissingColumns=True)\n        else:\n            import pandas as pd\n\n            return pd.concat([existing_df, new_with_sk], ignore_index=True)\n\n    def _execute_scd1(\n        self,\n        context: EngineContext,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        target: Optional[str],\n    ):\n        \"\"\"\n        SCD Type 1: Overwrite changes - no history tracking.\n        Update existing records in place, insert new records.\n        \"\"\"\n        existing_df = self._load_existing_target(context, target) if target else None\n        source_df = context.df\n\n        if existing_df is None:\n            return self._generate_surrogate_keys(\n                context, source_df, natural_key, surrogate_key, start_sk=0\n            )\n\n        max_sk = self._get_max_sk(existing_df, surrogate_key, context.engine_type)\n\n        if context.engine_type == EngineType.SPARK:\n            return self._execute_scd1_spark(\n                context, source_df, existing_df, natural_key, surrogate_key, track_cols, max_sk\n            )\n        else:\n            return self._execute_scd1_pandas(\n                context, source_df, existing_df, natural_key, surrogate_key, track_cols, max_sk\n            )\n\n    def _execute_scd1_spark(\n        self,\n        context: EngineContext,\n        source_df,\n        existing_df,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        max_sk: int,\n    ):\n        from pyspark.sql import functions as F\n\n        t_prefix = \"__existing_\"\n        renamed_existing = existing_df\n        for c in existing_df.columns:\n            renamed_existing = renamed_existing.withColumnRenamed(c, f\"{t_prefix}{c}\")\n\n        joined = source_df.join(\n            renamed_existing,\n            source_df[natural_key] == renamed_existing[f\"{t_prefix}{natural_key}\"],\n            \"left\",\n        )\n\n        new_records = joined.filter(F.col(f\"{t_prefix}{natural_key}\").isNull()).select(\n            source_df.columns\n        )\n\n        update_records = joined.filter(F.col(f\"{t_prefix}{natural_key}\").isNotNull())\n        update_cols = [F.col(f\"{t_prefix}{surrogate_key}\").alias(surrogate_key)] + [\n            F.col(c) for c in source_df.columns\n        ]\n        updated_records = update_records.select(update_cols)\n\n        unchanged_keys = update_records.select(F.col(f\"{t_prefix}{natural_key}\").alias(natural_key))\n        unchanged = existing_df.join(unchanged_keys, on=natural_key, how=\"left_anti\")\n\n        new_with_sk = self._generate_surrogate_keys(\n            context, new_records, natural_key, surrogate_key, start_sk=max_sk\n        )\n\n        result = unchanged.unionByName(updated_records, allowMissingColumns=True).unionByName(\n            new_with_sk, allowMissingColumns=True\n        )\n        return result\n\n    def _execute_scd1_pandas(\n        self,\n        context: EngineContext,\n        source_df,\n        existing_df,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        max_sk: int,\n    ):\n        import pandas as pd\n\n        merged = pd.merge(\n            source_df,\n            existing_df[[natural_key, surrogate_key]],\n            on=natural_key,\n            how=\"left\",\n            suffixes=(\"\", \"_existing\"),\n        )\n\n        has_existing_sk = f\"{surrogate_key}_existing\" in merged.columns\n        if has_existing_sk:\n            merged[surrogate_key] = merged[f\"{surrogate_key}_existing\"]\n            merged = merged.drop(columns=[f\"{surrogate_key}_existing\"])\n\n        new_mask = merged[surrogate_key].isna()\n        new_records = merged[new_mask].drop(columns=[surrogate_key])\n        existing_records = merged[~new_mask]\n\n        if len(new_records) &gt; 0:\n            new_with_sk = self._generate_surrogate_keys(\n                context, new_records, natural_key, surrogate_key, start_sk=max_sk\n            )\n        else:\n            new_with_sk = pd.DataFrame()\n\n        unchanged = existing_df[~existing_df[natural_key].isin(source_df[natural_key])]\n\n        result = pd.concat([unchanged, existing_records, new_with_sk], ignore_index=True)\n        return result\n\n    def _execute_scd2(\n        self,\n        context: EngineContext,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        target: str,\n    ):\n        \"\"\"\n        SCD Type 2: History tracking - reuse existing scd2 transformer.\n        Surrogate keys are generated for new/changed records.\n        \"\"\"\n        existing_df = self._load_existing_target(context, target)\n\n        valid_from_col = self.params.get(\"valid_from_col\", \"valid_from\")\n        valid_to_col = self.params.get(\"valid_to_col\", \"valid_to\")\n        is_current_col = self.params.get(\"is_current_col\", \"is_current\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            source_with_time = context.df.withColumn(valid_from_col, F.current_timestamp())\n        else:\n            source_df = context.df.copy()\n            source_df[valid_from_col] = datetime.now()\n            source_with_time = source_df\n\n        temp_context = context.with_df(source_with_time)\n\n        scd_params = SCD2Params(\n            target=target,\n            keys=[natural_key],\n            track_cols=track_cols,\n            effective_time_col=valid_from_col,\n            end_time_col=valid_to_col,\n            current_flag_col=is_current_col,\n        )\n\n        result_context = scd2(temp_context, scd_params)\n        result_df = result_context.df\n\n        max_sk = self._get_max_sk(existing_df, surrogate_key, context.engine_type)\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n            from pyspark.sql.window import Window\n\n            if surrogate_key not in result_df.columns:\n                window = Window.orderBy(natural_key, valid_from_col)\n                result_df = result_df.withColumn(\n                    surrogate_key, (F.row_number().over(window) + F.lit(max_sk)).cast(\"int\")\n                )\n            else:\n                null_sk_df = result_df.filter(F.col(surrogate_key).isNull())\n                has_sk_df = result_df.filter(F.col(surrogate_key).isNotNull())\n\n                if null_sk_df.count() &gt; 0:\n                    window = Window.orderBy(natural_key, valid_from_col)\n                    null_sk_df = null_sk_df.withColumn(\n                        surrogate_key, (F.row_number().over(window) + F.lit(max_sk)).cast(\"int\")\n                    )\n                    result_df = has_sk_df.unionByName(null_sk_df)\n        else:\n            import pandas as pd\n\n            if surrogate_key not in result_df.columns:\n                result_df = result_df.sort_values([natural_key, valid_from_col]).reset_index(\n                    drop=True\n                )\n                result_df[surrogate_key] = range(max_sk + 1, max_sk + 1 + len(result_df))\n            else:\n                null_mask = result_df[surrogate_key].isna()\n                if null_mask.any():\n                    null_df = result_df[null_mask].copy()\n                    null_df = null_df.sort_values([natural_key, valid_from_col]).reset_index(\n                        drop=True\n                    )\n                    null_df[surrogate_key] = range(max_sk + 1, max_sk + 1 + len(null_df))\n                    result_df = pd.concat([result_df[~null_mask], null_df], ignore_index=True)\n\n        return result_df\n\n    def _add_audit_columns(self, context: EngineContext, df, audit_config: dict):\n        \"\"\"Add audit columns (load_timestamp, source_system) to the dataframe.\"\"\"\n        load_timestamp = audit_config.get(\"load_timestamp\", True)\n        source_system = audit_config.get(\"source_system\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            if load_timestamp:\n                df = df.withColumn(\"load_timestamp\", F.current_timestamp())\n            if source_system:\n                df = df.withColumn(\"source_system\", F.lit(source_system))\n        else:\n            df = df.copy()\n            if load_timestamp:\n                df[\"load_timestamp\"] = datetime.now()\n            if source_system:\n                df[\"source_system\"] = source_system\n\n        return df\n\n    def _ensure_unknown_member(\n        self,\n        context: EngineContext,\n        df,\n        natural_key: str,\n        surrogate_key: str,\n        audit_config: dict,\n    ):\n        \"\"\"Ensure unknown member row exists with SK=0.\"\"\"\n        valid_from_col = self.params.get(\"valid_from_col\", \"valid_from\")\n        valid_to_col = self.params.get(\"valid_to_col\", \"valid_to\")\n        is_current_col = self.params.get(\"is_current_col\", \"is_current\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            existing_unknown = df.filter(F.col(surrogate_key) == 0)\n            if existing_unknown.count() &gt; 0:\n                return df\n\n            columns = df.columns\n            unknown_values = []\n            for col in columns:\n                if col == surrogate_key:\n                    unknown_values.append(0)\n                elif col == natural_key:\n                    unknown_values.append(\"-1\")\n                elif col == valid_from_col:\n                    unknown_values.append(datetime(1900, 1, 1))\n                elif col == valid_to_col:\n                    unknown_values.append(None)\n                elif col == is_current_col:\n                    unknown_values.append(True)\n                elif col == \"load_timestamp\":\n                    unknown_values.append(datetime.now())\n                elif col == \"source_system\":\n                    unknown_values.append(audit_config.get(\"source_system\", \"Unknown\"))\n                else:\n                    unknown_values.append(\"Unknown\")\n\n            unknown_row = context.spark.createDataFrame([unknown_values], columns)\n            return unknown_row.unionByName(df)\n        else:\n            import pandas as pd\n\n            if (df[surrogate_key] == 0).any():\n                return df\n\n            unknown_row = {}\n            for col in df.columns:\n                if col == surrogate_key:\n                    unknown_row[col] = 0\n                elif col == natural_key:\n                    unknown_row[col] = \"-1\"\n                elif col == valid_from_col:\n                    unknown_row[col] = datetime(1900, 1, 1)\n                elif col == valid_to_col:\n                    unknown_row[col] = None\n                elif col == is_current_col:\n                    unknown_row[col] = True\n                elif col == \"load_timestamp\":\n                    unknown_row[col] = datetime.now()\n                elif col == \"source_system\":\n                    unknown_row[col] = audit_config.get(\"source_system\", \"Unknown\")\n                else:\n                    dtype = df[col].dtype\n                    if pd.api.types.is_numeric_dtype(dtype):\n                        unknown_row[col] = 0\n                    else:\n                        unknown_row[col] = \"Unknown\"\n\n            unknown_df = pd.DataFrame([unknown_row])\n            for col in unknown_df.columns:\n                if col in df.columns:\n                    unknown_df[col] = unknown_df[col].astype(df[col].dtype)\n            return pd.concat([unknown_df, df], ignore_index=True)\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.date_dimension","title":"<code>odibi.patterns.date_dimension</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.date_dimension.DateDimensionPattern","title":"<code>DateDimensionPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Date Dimension Pattern: Generates a complete date dimension table.</p> <p>Creates a date dimension with pre-calculated attributes useful for BI/reporting including day of week, quarter, fiscal year, etc.</p> <p>Configuration Options (via params dict):     - start_date (str): Start date in YYYY-MM-DD format     - end_date (str): End date in YYYY-MM-DD format     - date_key_format (str): Format for date_sk (default: \"yyyyMMdd\" -&gt; 20240115)     - fiscal_year_start_month (int): Month when fiscal year starts (1-12, default: 1)     - include_time (bool): If true, generate time dimension (not implemented yet)     - unknown_member (bool): If true, add unknown date row with date_sk=0</p> Generated Columns <ul> <li>date_sk: Integer surrogate key (YYYYMMDD format)</li> <li>full_date: The actual date</li> <li>day_of_week: Day name (Monday, Tuesday, etc.)</li> <li>day_of_week_num: Day number (1=Monday, 7=Sunday)</li> <li>day_of_month: Day of month (1-31)</li> <li>day_of_year: Day of year (1-366)</li> <li>is_weekend: Boolean flag</li> <li>week_of_year: ISO week number (1-53)</li> <li>month: Month number (1-12)</li> <li>month_name: Month name (January, February, etc.)</li> <li>quarter: Calendar quarter (1-4)</li> <li>quarter_name: Q1, Q2, Q3, Q4</li> <li>year: Calendar year</li> <li>fiscal_year: Fiscal year</li> <li>fiscal_quarter: Fiscal quarter (1-4)</li> <li>is_month_start: First day of month</li> <li>is_month_end: Last day of month</li> <li>is_year_start: First day of year</li> <li>is_year_end: Last day of year</li> </ul> Source code in <code>odibi\\patterns\\date_dimension.py</code> <pre><code>class DateDimensionPattern(Pattern):\n    \"\"\"\n    Date Dimension Pattern: Generates a complete date dimension table.\n\n    Creates a date dimension with pre-calculated attributes useful for\n    BI/reporting including day of week, quarter, fiscal year, etc.\n\n    Configuration Options (via params dict):\n        - **start_date** (str): Start date in YYYY-MM-DD format\n        - **end_date** (str): End date in YYYY-MM-DD format\n        - **date_key_format** (str): Format for date_sk (default: \"yyyyMMdd\" -&gt; 20240115)\n        - **fiscal_year_start_month** (int): Month when fiscal year starts (1-12, default: 1)\n        - **include_time** (bool): If true, generate time dimension (not implemented yet)\n        - **unknown_member** (bool): If true, add unknown date row with date_sk=0\n\n    Generated Columns:\n        - date_sk: Integer surrogate key (YYYYMMDD format)\n        - full_date: The actual date\n        - day_of_week: Day name (Monday, Tuesday, etc.)\n        - day_of_week_num: Day number (1=Monday, 7=Sunday)\n        - day_of_month: Day of month (1-31)\n        - day_of_year: Day of year (1-366)\n        - is_weekend: Boolean flag\n        - week_of_year: ISO week number (1-53)\n        - month: Month number (1-12)\n        - month_name: Month name (January, February, etc.)\n        - quarter: Calendar quarter (1-4)\n        - quarter_name: Q1, Q2, Q3, Q4\n        - year: Calendar year\n        - fiscal_year: Fiscal year\n        - fiscal_quarter: Fiscal quarter (1-4)\n        - is_month_start: First day of month\n        - is_month_end: Last day of month\n        - is_year_start: First day of year\n        - is_year_end: Last day of year\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"DateDimensionPattern validation starting\",\n            pattern=\"DateDimensionPattern\",\n            params=self.params,\n        )\n\n        if not self.params.get(\"start_date\"):\n            ctx.error(\n                \"DateDimensionPattern validation failed: 'start_date' is required\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\"DateDimensionPattern: 'start_date' parameter is required.\")\n\n        if not self.params.get(\"end_date\"):\n            ctx.error(\n                \"DateDimensionPattern validation failed: 'end_date' is required\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\"DateDimensionPattern: 'end_date' parameter is required.\")\n\n        try:\n            start = self._parse_date(self.params[\"start_date\"])\n            end = self._parse_date(self.params[\"end_date\"])\n            if start &gt; end:\n                raise ValueError(\"start_date must be before or equal to end_date\")\n        except Exception as e:\n            ctx.error(\n                f\"DateDimensionPattern validation failed: {e}\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(f\"DateDimensionPattern: Invalid date parameters: {e}\")\n\n        fiscal_month = self.params.get(\"fiscal_year_start_month\", 1)\n        if not isinstance(fiscal_month, int) or fiscal_month &lt; 1 or fiscal_month &gt; 12:\n            ctx.error(\n                \"DateDimensionPattern validation failed: invalid fiscal_year_start_month\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\n                \"DateDimensionPattern: 'fiscal_year_start_month' must be an integer 1-12.\"\n            )\n\n        ctx.debug(\n            \"DateDimensionPattern validation passed\",\n            pattern=\"DateDimensionPattern\",\n        )\n\n    def _parse_date(self, date_str: str) -&gt; date:\n        \"\"\"Parse a date string in YYYY-MM-DD format.\"\"\"\n        if isinstance(date_str, (date, datetime)):\n            return date_str if isinstance(date_str, date) else date_str.date()\n        return datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        start_date = self._parse_date(self.params[\"start_date\"])\n        end_date = self._parse_date(self.params[\"end_date\"])\n        fiscal_year_start_month = self.params.get(\"fiscal_year_start_month\", 1)\n        unknown_member = self.params.get(\"unknown_member\", False)\n\n        ctx.debug(\n            \"DateDimensionPattern starting\",\n            pattern=\"DateDimensionPattern\",\n            start_date=str(start_date),\n            end_date=str(end_date),\n            fiscal_year_start_month=fiscal_year_start_month,\n        )\n\n        try:\n            if context.engine_type == EngineType.SPARK:\n                result_df = self._generate_spark(\n                    context, start_date, end_date, fiscal_year_start_month\n                )\n            else:\n                result_df = self._generate_pandas(start_date, end_date, fiscal_year_start_month)\n\n            if unknown_member:\n                result_df = self._add_unknown_member(context, result_df)\n\n            row_count = self._get_row_count(result_df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"DateDimensionPattern completed\",\n                pattern=\"DateDimensionPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                rows_generated=row_count,\n                start_date=str(start_date),\n                end_date=str(end_date),\n            )\n\n            return result_df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"DateDimensionPattern failed: {e}\",\n                pattern=\"DateDimensionPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _generate_pandas(\n        self, start_date: date, end_date: date, fiscal_year_start_month: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate date dimension using Pandas.\"\"\"\n        dates = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n        df = pd.DataFrame({\"full_date\": dates})\n\n        df[\"date_sk\"] = df[\"full_date\"].dt.strftime(\"%Y%m%d\").astype(int)\n\n        df[\"day_of_week\"] = df[\"full_date\"].dt.day_name()\n        df[\"day_of_week_num\"] = df[\"full_date\"].dt.dayofweek + 1\n        df[\"day_of_month\"] = df[\"full_date\"].dt.day\n        df[\"day_of_year\"] = df[\"full_date\"].dt.dayofyear\n\n        df[\"is_weekend\"] = df[\"day_of_week_num\"].isin([6, 7])\n\n        df[\"week_of_year\"] = df[\"full_date\"].dt.isocalendar().week.astype(int)\n\n        df[\"month\"] = df[\"full_date\"].dt.month\n        df[\"month_name\"] = df[\"full_date\"].dt.month_name()\n\n        df[\"quarter\"] = df[\"full_date\"].dt.quarter\n        df[\"quarter_name\"] = \"Q\" + df[\"quarter\"].astype(str)\n\n        df[\"year\"] = df[\"full_date\"].dt.year\n\n        df[\"fiscal_year\"] = df.apply(\n            lambda row: self._calc_fiscal_year(row[\"full_date\"], fiscal_year_start_month),\n            axis=1,\n        )\n        df[\"fiscal_quarter\"] = df.apply(\n            lambda row: self._calc_fiscal_quarter(row[\"full_date\"], fiscal_year_start_month),\n            axis=1,\n        )\n\n        df[\"is_month_start\"] = df[\"full_date\"].dt.is_month_start\n        df[\"is_month_end\"] = df[\"full_date\"].dt.is_month_end\n        df[\"is_year_start\"] = (df[\"month\"] == 1) &amp; (df[\"day_of_month\"] == 1)\n        df[\"is_year_end\"] = (df[\"month\"] == 12) &amp; (df[\"day_of_month\"] == 31)\n\n        df[\"full_date\"] = df[\"full_date\"].dt.date\n\n        column_order = [\n            \"date_sk\",\n            \"full_date\",\n            \"day_of_week\",\n            \"day_of_week_num\",\n            \"day_of_month\",\n            \"day_of_year\",\n            \"is_weekend\",\n            \"week_of_year\",\n            \"month\",\n            \"month_name\",\n            \"quarter\",\n            \"quarter_name\",\n            \"year\",\n            \"fiscal_year\",\n            \"fiscal_quarter\",\n            \"is_month_start\",\n            \"is_month_end\",\n            \"is_year_start\",\n            \"is_year_end\",\n        ]\n        return df[column_order]\n\n    def _calc_fiscal_year(self, dt, fiscal_start_month: int) -&gt; int:\n        \"\"\"Calculate fiscal year based on fiscal start month.\"\"\"\n        if isinstance(dt, pd.Timestamp):\n            month = dt.month\n            year = dt.year\n        else:\n            month = dt.month\n            year = dt.year\n\n        if fiscal_start_month == 1:\n            return year\n        if month &gt;= fiscal_start_month:\n            return year + 1\n        return year\n\n    def _calc_fiscal_quarter(self, dt, fiscal_start_month: int) -&gt; int:\n        \"\"\"Calculate fiscal quarter based on fiscal start month.\"\"\"\n        if isinstance(dt, pd.Timestamp):\n            month = dt.month\n        else:\n            month = dt.month\n\n        adjusted_month = (month - fiscal_start_month) % 12\n        return (adjusted_month // 3) + 1\n\n    def _generate_spark(\n        self, context: EngineContext, start_date: date, end_date: date, fiscal_year_start_month: int\n    ):\n        \"\"\"Generate date dimension using Spark.\"\"\"\n        from pyspark.sql import functions as F\n        from pyspark.sql.types import IntegerType\n\n        spark = context.spark\n\n        num_days = (end_date - start_date).days + 1\n        start_date_str = start_date.strftime(\"%Y-%m-%d\")\n\n        df = spark.range(num_days).select(\n            F.date_add(F.lit(start_date_str), F.col(\"id\").cast(IntegerType())).alias(\"full_date\")\n        )\n\n        df = df.withColumn(\"date_sk\", F.date_format(\"full_date\", \"yyyyMMdd\").cast(IntegerType()))\n\n        df = df.withColumn(\"day_of_week\", F.date_format(\"full_date\", \"EEEE\"))\n        df = df.withColumn(\"day_of_week_num\", F.dayofweek(\"full_date\"))\n        df = df.withColumn(\n            \"day_of_week_num\",\n            F.when(F.col(\"day_of_week_num\") == 1, 7).otherwise(F.col(\"day_of_week_num\") - 1),\n        )\n        df = df.withColumn(\"day_of_month\", F.dayofmonth(\"full_date\"))\n        df = df.withColumn(\"day_of_year\", F.dayofyear(\"full_date\"))\n\n        df = df.withColumn(\"is_weekend\", F.col(\"day_of_week_num\").isin([6, 7]))\n\n        df = df.withColumn(\"week_of_year\", F.weekofyear(\"full_date\"))\n\n        df = df.withColumn(\"month\", F.month(\"full_date\"))\n        df = df.withColumn(\"month_name\", F.date_format(\"full_date\", \"MMMM\"))\n\n        df = df.withColumn(\"quarter\", F.quarter(\"full_date\"))\n        df = df.withColumn(\"quarter_name\", F.concat(F.lit(\"Q\"), F.col(\"quarter\")))\n\n        df = df.withColumn(\"year\", F.year(\"full_date\"))\n\n        if fiscal_year_start_month == 1:\n            df = df.withColumn(\"fiscal_year\", F.col(\"year\"))\n            df = df.withColumn(\"fiscal_quarter\", F.col(\"quarter\"))\n        else:\n            df = df.withColumn(\n                \"fiscal_year\",\n                F.when(F.col(\"month\") &gt;= fiscal_year_start_month, F.col(\"year\") + 1).otherwise(\n                    F.col(\"year\")\n                ),\n            )\n            adjusted_month = (F.col(\"month\") - fiscal_year_start_month + 12) % 12\n            df = df.withColumn(\"fiscal_quarter\", (adjusted_month / 3).cast(IntegerType()) + 1)\n\n        df = df.withColumn(\n            \"is_month_start\",\n            F.col(\"day_of_month\") == 1,\n        )\n        df = df.withColumn(\n            \"is_month_end\",\n            F.col(\"full_date\") == F.last_day(\"full_date\"),\n        )\n        df = df.withColumn(\n            \"is_year_start\",\n            (F.col(\"month\") == 1) &amp; (F.col(\"day_of_month\") == 1),\n        )\n        df = df.withColumn(\n            \"is_year_end\",\n            (F.col(\"month\") == 12) &amp; (F.col(\"day_of_month\") == 31),\n        )\n\n        column_order = [\n            \"date_sk\",\n            \"full_date\",\n            \"day_of_week\",\n            \"day_of_week_num\",\n            \"day_of_month\",\n            \"day_of_year\",\n            \"is_weekend\",\n            \"week_of_year\",\n            \"month\",\n            \"month_name\",\n            \"quarter\",\n            \"quarter_name\",\n            \"year\",\n            \"fiscal_year\",\n            \"fiscal_quarter\",\n            \"is_month_start\",\n            \"is_month_end\",\n            \"is_year_start\",\n            \"is_year_end\",\n        ]\n        return df.select(column_order)\n\n    def _add_unknown_member(self, context: EngineContext, df):\n        \"\"\"Add unknown member row with date_sk=0.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import Row\n\n            unknown_data = {\n                \"date_sk\": 0,\n                \"full_date\": date(1900, 1, 1),\n                \"day_of_week\": \"Unknown\",\n                \"day_of_week_num\": 0,\n                \"day_of_month\": 0,\n                \"day_of_year\": 0,\n                \"is_weekend\": False,\n                \"week_of_year\": 0,\n                \"month\": 0,\n                \"month_name\": \"Unknown\",\n                \"quarter\": 0,\n                \"quarter_name\": \"Unknown\",\n                \"year\": 0,\n                \"fiscal_year\": 0,\n                \"fiscal_quarter\": 0,\n                \"is_month_start\": False,\n                \"is_month_end\": False,\n                \"is_year_start\": False,\n                \"is_year_end\": False,\n            }\n            unknown_row = context.spark.createDataFrame([Row(**unknown_data)])\n            return unknown_row.unionByName(df)\n        else:\n            unknown_row = pd.DataFrame(\n                [\n                    {\n                        \"date_sk\": 0,\n                        \"full_date\": date(1900, 1, 1),\n                        \"day_of_week\": \"Unknown\",\n                        \"day_of_week_num\": 0,\n                        \"day_of_month\": 0,\n                        \"day_of_year\": 0,\n                        \"is_weekend\": False,\n                        \"week_of_year\": 0,\n                        \"month\": 0,\n                        \"month_name\": \"Unknown\",\n                        \"quarter\": 0,\n                        \"quarter_name\": \"Unknown\",\n                        \"year\": 0,\n                        \"fiscal_year\": 0,\n                        \"fiscal_quarter\": 0,\n                        \"is_month_start\": False,\n                        \"is_month_end\": False,\n                        \"is_year_start\": False,\n                        \"is_year_end\": False,\n                    }\n                ]\n            )\n            return pd.concat([unknown_row, df], ignore_index=True)\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.fact","title":"<code>odibi.patterns.fact</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.fact.FactPattern","title":"<code>FactPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Enhanced Fact Pattern: Builds fact tables with automatic SK lookups.</p> <p>Features: - Automatic surrogate key lookups from dimension tables - Orphan handling (unknown member, reject, or quarantine) - Grain validation (detect duplicates at PK level) - Audit columns (load_timestamp, source_system) - Deduplication support - Measure calculations and renaming</p> <p>Basic Params (backward compatible):     deduplicate (bool): If true, removes duplicates before insert.     keys (list): Keys for deduplication.</p> Enhanced Params <p>grain (list): Columns that define uniqueness (validates no duplicates) dimensions (list): Dimension lookup configurations     - source_column: Column in source data     - dimension_table: Name of dimension in context     - dimension_key: Natural key column in dimension     - surrogate_key: Surrogate key to retrieve     - scd2 (bool): If true, filter is_current=true orphan_handling (str): \"unknown\" | \"reject\" | \"quarantine\" quarantine (dict): Quarantine configuration (required if orphan_handling=quarantine)     - connection: Connection name for quarantine writes     - path: Path for quarantine data (or use 'table')     - table: Table name for quarantine (or use 'path')     - add_columns (dict): Metadata columns to add         - _rejection_reason (bool): Add rejection reason column         - _rejected_at (bool): Add rejection timestamp column         - _source_dimension (bool): Add source dimension name column measures (list): Measure definitions (passthrough, rename, or calculated) audit (dict): Audit column configuration     - load_timestamp (bool)     - source_system (str)</p> Example Config <p>pattern:   type: fact   params:     grain: [order_id]     dimensions:       - source_column: customer_id         dimension_table: dim_customer         dimension_key: customer_id         surrogate_key: customer_sk         scd2: true     orphan_handling: unknown     measures:       - quantity       - total_amount: \"quantity * price\"     audit:       load_timestamp: true       source_system: \"pos\"</p> Example with Quarantine <p>pattern:   type: fact   params:     dimensions:       - source_column: customer_id         dimension_table: dim_customer         dimension_key: customer_id         surrogate_key: customer_sk     orphan_handling: quarantine     quarantine:       connection: silver       path: fact_orders_orphans       add_columns:         _rejection_reason: true         _rejected_at: true         _source_dimension: true</p> Source code in <code>odibi\\patterns\\fact.py</code> <pre><code>class FactPattern(Pattern):\n    \"\"\"\n    Enhanced Fact Pattern: Builds fact tables with automatic SK lookups.\n\n    Features:\n    - Automatic surrogate key lookups from dimension tables\n    - Orphan handling (unknown member, reject, or quarantine)\n    - Grain validation (detect duplicates at PK level)\n    - Audit columns (load_timestamp, source_system)\n    - Deduplication support\n    - Measure calculations and renaming\n\n    Basic Params (backward compatible):\n        deduplicate (bool): If true, removes duplicates before insert.\n        keys (list): Keys for deduplication.\n\n    Enhanced Params:\n        grain (list): Columns that define uniqueness (validates no duplicates)\n        dimensions (list): Dimension lookup configurations\n            - source_column: Column in source data\n            - dimension_table: Name of dimension in context\n            - dimension_key: Natural key column in dimension\n            - surrogate_key: Surrogate key to retrieve\n            - scd2 (bool): If true, filter is_current=true\n        orphan_handling (str): \"unknown\" | \"reject\" | \"quarantine\"\n        quarantine (dict): Quarantine configuration (required if orphan_handling=quarantine)\n            - connection: Connection name for quarantine writes\n            - path: Path for quarantine data (or use 'table')\n            - table: Table name for quarantine (or use 'path')\n            - add_columns (dict): Metadata columns to add\n                - _rejection_reason (bool): Add rejection reason column\n                - _rejected_at (bool): Add rejection timestamp column\n                - _source_dimension (bool): Add source dimension name column\n        measures (list): Measure definitions (passthrough, rename, or calculated)\n        audit (dict): Audit column configuration\n            - load_timestamp (bool)\n            - source_system (str)\n\n    Example Config:\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - total_amount: \"quantity * price\"\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n\n    Example with Quarantine:\n        pattern:\n          type: fact\n          params:\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n            orphan_handling: quarantine\n            quarantine:\n              connection: silver\n              path: fact_orders_orphans\n              add_columns:\n                _rejection_reason: true\n                _rejected_at: true\n                _source_dimension: true\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        deduplicate = self.params.get(\"deduplicate\")\n        keys = self.params.get(\"keys\")\n        grain = self.params.get(\"grain\")\n        dimensions = self.params.get(\"dimensions\", [])\n        orphan_handling = self.params.get(\"orphan_handling\", \"unknown\")\n\n        ctx.debug(\n            \"FactPattern validation starting\",\n            pattern=\"FactPattern\",\n            deduplicate=deduplicate,\n            keys=keys,\n            grain=grain,\n            dimensions_count=len(dimensions),\n        )\n\n        if deduplicate and not keys:\n            ctx.error(\n                \"FactPattern validation failed: 'keys' required when 'deduplicate' is True\",\n                pattern=\"FactPattern\",\n            )\n            raise ValueError(\"FactPattern: 'keys' required when 'deduplicate' is True.\")\n\n        if orphan_handling not in (\"unknown\", \"reject\", \"quarantine\"):\n            ctx.error(\n                f\"FactPattern validation failed: invalid orphan_handling '{orphan_handling}'\",\n                pattern=\"FactPattern\",\n            )\n            raise ValueError(\n                f\"FactPattern: 'orphan_handling' must be 'unknown', 'reject', or 'quarantine'. \"\n                f\"Got: {orphan_handling}\"\n            )\n\n        if orphan_handling == \"quarantine\":\n            quarantine_config = self.params.get(\"quarantine\")\n            if not quarantine_config:\n                ctx.error(\n                    \"FactPattern validation failed: 'quarantine' config required \"\n                    \"when orphan_handling='quarantine'\",\n                    pattern=\"FactPattern\",\n                )\n                raise ValueError(\n                    \"FactPattern: 'quarantine' configuration is required when \"\n                    \"orphan_handling='quarantine'.\"\n                )\n            if not quarantine_config.get(\"connection\"):\n                ctx.error(\n                    \"FactPattern validation failed: quarantine.connection is required\",\n                    pattern=\"FactPattern\",\n                )\n                raise ValueError(\"FactPattern: 'quarantine.connection' is required.\")\n            if not quarantine_config.get(\"path\") and not quarantine_config.get(\"table\"):\n                ctx.error(\n                    \"FactPattern validation failed: quarantine requires 'path' or 'table'\",\n                    pattern=\"FactPattern\",\n                )\n                raise ValueError(\"FactPattern: 'quarantine' requires either 'path' or 'table'.\")\n\n        for i, dim in enumerate(dimensions):\n            required_keys = [\"source_column\", \"dimension_table\", \"dimension_key\", \"surrogate_key\"]\n            for key in required_keys:\n                if key not in dim:\n                    ctx.error(\n                        f\"FactPattern validation failed: dimension[{i}] missing '{key}'\",\n                        pattern=\"FactPattern\",\n                    )\n                    raise ValueError(f\"FactPattern: dimension[{i}] missing required key '{key}'.\")\n\n        ctx.debug(\n            \"FactPattern validation passed\",\n            pattern=\"FactPattern\",\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        deduplicate = self.params.get(\"deduplicate\")\n        keys = self.params.get(\"keys\")\n        grain = self.params.get(\"grain\")\n        dimensions = self.params.get(\"dimensions\", [])\n        orphan_handling = self.params.get(\"orphan_handling\", \"unknown\")\n        quarantine_config = self.params.get(\"quarantine\", {})\n        measures = self.params.get(\"measures\", [])\n        audit_config = self.params.get(\"audit\", {})\n\n        ctx.debug(\n            \"FactPattern starting\",\n            pattern=\"FactPattern\",\n            deduplicate=deduplicate,\n            keys=keys,\n            grain=grain,\n            dimensions_count=len(dimensions),\n            orphan_handling=orphan_handling,\n        )\n\n        df = context.df\n        source_count = self._get_row_count(df, context.engine_type)\n        ctx.debug(\"Fact source loaded\", pattern=\"FactPattern\", source_rows=source_count)\n\n        try:\n            if deduplicate and keys:\n                df = self._deduplicate(context, df, keys)\n                ctx.debug(\n                    \"Fact deduplication complete\",\n                    pattern=\"FactPattern\",\n                    rows_after=self._get_row_count(df, context.engine_type),\n                )\n\n            if dimensions:\n                df, orphan_count, quarantined_df = self._lookup_dimensions(\n                    context, df, dimensions, orphan_handling, quarantine_config\n                )\n                ctx.debug(\n                    \"Fact dimension lookups complete\",\n                    pattern=\"FactPattern\",\n                    orphan_count=orphan_count,\n                )\n\n                if orphan_handling == \"quarantine\" and quarantined_df is not None:\n                    self._write_quarantine(context, quarantined_df, quarantine_config)\n                    ctx.info(\n                        f\"Quarantined {orphan_count} orphan records\",\n                        pattern=\"FactPattern\",\n                        quarantine_path=quarantine_config.get(\"path\")\n                        or quarantine_config.get(\"table\"),\n                    )\n\n            if measures:\n                df = self._apply_measures(context, df, measures)\n\n            if grain:\n                self._validate_grain(context, df, grain)\n\n            df = self._add_audit_columns(context, df, audit_config)\n\n            result_count = self._get_row_count(df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"FactPattern completed\",\n                pattern=\"FactPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                source_rows=source_count,\n                result_rows=result_count,\n            )\n\n            return df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"FactPattern failed: {e}\",\n                pattern=\"FactPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _deduplicate(self, context: EngineContext, df, keys: List[str]):\n        \"\"\"Remove duplicates based on keys.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return df.dropDuplicates(keys)\n        else:\n            return df.drop_duplicates(subset=keys)\n\n    def _lookup_dimensions(\n        self,\n        context: EngineContext,\n        df,\n        dimensions: List[Dict],\n        orphan_handling: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"\n        Perform surrogate key lookups from dimension tables.\n\n        Returns:\n            Tuple of (result_df, orphan_count, quarantined_df)\n        \"\"\"\n        total_orphans = 0\n        all_quarantined = []\n\n        for dim_config in dimensions:\n            source_col = dim_config[\"source_column\"]\n            dim_table = dim_config[\"dimension_table\"]\n            dim_key = dim_config[\"dimension_key\"]\n            sk_col = dim_config[\"surrogate_key\"]\n            is_scd2 = dim_config.get(\"scd2\", False)\n\n            dim_df = self._get_dimension_df(context, dim_table, is_scd2)\n            if dim_df is None:\n                raise ValueError(\n                    f\"FactPattern: Dimension table '{dim_table}' not found in context.\"\n                )\n\n            df, orphan_count, quarantined = self._join_dimension(\n                context,\n                df,\n                dim_df,\n                source_col,\n                dim_key,\n                sk_col,\n                orphan_handling,\n                dim_table,\n                quarantine_config,\n            )\n            total_orphans += orphan_count\n            if quarantined is not None:\n                all_quarantined.append(quarantined)\n\n        quarantined_df = None\n        if all_quarantined:\n            quarantined_df = self._union_dataframes(context, all_quarantined)\n\n        return df, total_orphans, quarantined_df\n\n    def _union_dataframes(self, context: EngineContext, dfs: List):\n        \"\"\"Union multiple DataFrames together.\"\"\"\n        if not dfs:\n            return None\n        if context.engine_type == EngineType.SPARK:\n            result = dfs[0]\n            for df in dfs[1:]:\n                result = result.unionByName(df, allowMissingColumns=True)\n            return result\n        else:\n            import pandas as pd\n\n            return pd.concat(dfs, ignore_index=True)\n\n    def _get_dimension_df(self, context: EngineContext, dim_table: str, is_scd2: bool):\n        \"\"\"Get dimension DataFrame from context, optionally filtering for current records.\"\"\"\n        try:\n            dim_df = context.get(dim_table)\n        except KeyError:\n            return None\n\n        if is_scd2:\n            is_current_col = \"is_current\"\n            if context.engine_type == EngineType.SPARK:\n                from pyspark.sql import functions as F\n\n                if is_current_col in dim_df.columns:\n                    dim_df = dim_df.filter(F.col(is_current_col) == True)  # noqa: E712\n            else:\n                if is_current_col in dim_df.columns:\n                    dim_df = dim_df[dim_df[is_current_col] == True].copy()  # noqa: E712\n\n        return dim_df\n\n    def _join_dimension(\n        self,\n        context: EngineContext,\n        fact_df,\n        dim_df,\n        source_col: str,\n        dim_key: str,\n        sk_col: str,\n        orphan_handling: str,\n        dim_table: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"\n        Join fact to dimension and retrieve surrogate key.\n\n        Returns:\n            Tuple of (result_df, orphan_count, quarantined_df)\n        \"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._join_dimension_spark(\n                context,\n                fact_df,\n                dim_df,\n                source_col,\n                dim_key,\n                sk_col,\n                orphan_handling,\n                dim_table,\n                quarantine_config,\n            )\n        else:\n            return self._join_dimension_pandas(\n                fact_df,\n                dim_df,\n                source_col,\n                dim_key,\n                sk_col,\n                orphan_handling,\n                dim_table,\n                quarantine_config,\n            )\n\n    def _join_dimension_spark(\n        self,\n        context: EngineContext,\n        fact_df,\n        dim_df,\n        source_col: str,\n        dim_key: str,\n        sk_col: str,\n        orphan_handling: str,\n        dim_table: str,\n        quarantine_config: Dict,\n    ):\n        from pyspark.sql import functions as F\n\n        dim_subset = dim_df.select(\n            F.col(dim_key).alias(f\"_dim_{dim_key}\"),\n            F.col(sk_col).alias(sk_col),\n        )\n\n        joined = fact_df.join(\n            dim_subset,\n            fact_df[source_col] == dim_subset[f\"_dim_{dim_key}\"],\n            \"left\",\n        )\n\n        orphan_mask = F.col(sk_col).isNull()\n        orphan_count = joined.filter(orphan_mask).count()\n        quarantined_df = None\n\n        if orphan_handling == \"reject\" and orphan_count &gt; 0:\n            raise ValueError(\n                f\"FactPattern: {orphan_count} orphan records found for dimension \"\n                f\"lookup on '{source_col}'. Orphan handling is set to 'reject'.\"\n            )\n\n        if orphan_handling == \"unknown\":\n            joined = joined.withColumn(sk_col, F.coalesce(F.col(sk_col), F.lit(0)))\n\n        if orphan_handling == \"quarantine\" and orphan_count &gt; 0:\n            orphan_rows = joined.filter(orphan_mask).drop(f\"_dim_{dim_key}\")\n            orphan_rows = self._add_quarantine_metadata_spark(\n                orphan_rows, dim_table, source_col, quarantine_config\n            )\n            quarantined_df = orphan_rows\n            joined = joined.filter(~orphan_mask)\n\n        result = joined.drop(f\"_dim_{dim_key}\")\n\n        return result, orphan_count, quarantined_df\n\n    def _join_dimension_pandas(\n        self,\n        fact_df,\n        dim_df,\n        source_col: str,\n        dim_key: str,\n        sk_col: str,\n        orphan_handling: str,\n        dim_table: str,\n        quarantine_config: Dict,\n    ):\n        import pandas as pd\n\n        dim_subset = dim_df[[dim_key, sk_col]].copy()\n        dim_subset = dim_subset.rename(columns={dim_key: f\"_dim_{dim_key}\"})\n\n        merged = pd.merge(\n            fact_df,\n            dim_subset,\n            left_on=source_col,\n            right_on=f\"_dim_{dim_key}\",\n            how=\"left\",\n        )\n\n        orphan_mask = merged[sk_col].isna()\n        orphan_count = orphan_mask.sum()\n        quarantined_df = None\n\n        if orphan_handling == \"reject\" and orphan_count &gt; 0:\n            raise ValueError(\n                f\"FactPattern: {orphan_count} orphan records found for dimension \"\n                f\"lookup on '{source_col}'. Orphan handling is set to 'reject'.\"\n            )\n\n        if orphan_handling == \"unknown\":\n            merged[sk_col] = merged[sk_col].fillna(0).infer_objects(copy=False).astype(int)\n\n        if orphan_handling == \"quarantine\" and orphan_count &gt; 0:\n            orphan_rows = merged[orphan_mask].drop(columns=[f\"_dim_{dim_key}\"]).copy()\n            orphan_rows = self._add_quarantine_metadata_pandas(\n                orphan_rows, dim_table, source_col, quarantine_config\n            )\n            quarantined_df = orphan_rows\n            merged = merged[~orphan_mask].copy()\n\n        result = merged.drop(columns=[f\"_dim_{dim_key}\"])\n\n        return result, int(orphan_count), quarantined_df\n\n    def _apply_measures(self, context: EngineContext, df, measures: List):\n        \"\"\"\n        Apply measure transformations.\n\n        Measures can be:\n        - String: passthrough column name\n        - Dict with single key-value: rename or calculate\n          - {\"new_name\": \"old_name\"} -&gt; rename\n          - {\"new_name\": \"expr\"} -&gt; calculate (if expr contains operators)\n        \"\"\"\n        for measure in measures:\n            if isinstance(measure, str):\n                continue\n            elif isinstance(measure, dict):\n                for new_name, expr in measure.items():\n                    if self._is_expression(expr):\n                        df = self._add_calculated_measure(context, df, new_name, expr)\n                    else:\n                        df = self._rename_column(context, df, expr, new_name)\n\n        return df\n\n    def _is_expression(self, expr: str) -&gt; bool:\n        \"\"\"Check if string is a calculation expression.\"\"\"\n        operators = [\"+\", \"-\", \"*\", \"/\", \"(\", \")\"]\n        return any(op in expr for op in operators)\n\n    def _add_calculated_measure(self, context: EngineContext, df, name: str, expr: str):\n        \"\"\"Add a calculated measure column.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            return df.withColumn(name, F.expr(expr))\n        else:\n            df = df.copy()\n            df[name] = df.eval(expr)\n            return df\n\n    def _rename_column(self, context: EngineContext, df, old_name: str, new_name: str):\n        \"\"\"Rename a column.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return df.withColumnRenamed(old_name, new_name)\n        else:\n            return df.rename(columns={old_name: new_name})\n\n    def _validate_grain(self, context: EngineContext, df, grain: List[str]):\n        \"\"\"\n        Validate that no duplicate rows exist at the grain level.\n\n        Raises ValueError if duplicates are found.\n        \"\"\"\n        ctx = get_logging_context()\n\n        if context.engine_type == EngineType.SPARK:\n            total_count = df.count()\n            distinct_count = df.select(grain).distinct().count()\n        else:\n            total_count = len(df)\n            distinct_count = len(df.drop_duplicates(subset=grain))\n\n        if total_count != distinct_count:\n            duplicate_count = total_count - distinct_count\n            ctx.error(\n                f\"FactPattern grain validation failed: {duplicate_count} duplicate rows\",\n                pattern=\"FactPattern\",\n                grain=grain,\n                total_rows=total_count,\n                distinct_rows=distinct_count,\n            )\n            raise ValueError(\n                f\"FactPattern: Grain validation failed. Found {duplicate_count} duplicate \"\n                f\"rows at grain level {grain}. Total rows: {total_count}, \"\n                f\"Distinct rows: {distinct_count}.\"\n            )\n\n        ctx.debug(\n            \"FactPattern grain validation passed\",\n            pattern=\"FactPattern\",\n            grain=grain,\n            total_rows=total_count,\n        )\n\n    def _add_audit_columns(self, context: EngineContext, df, audit_config: Dict):\n        \"\"\"Add audit columns (load_timestamp, source_system).\"\"\"\n        load_timestamp = audit_config.get(\"load_timestamp\", False)\n        source_system = audit_config.get(\"source_system\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            if load_timestamp:\n                df = df.withColumn(\"load_timestamp\", F.current_timestamp())\n            if source_system:\n                df = df.withColumn(\"source_system\", F.lit(source_system))\n        else:\n            if load_timestamp or source_system:\n                df = df.copy()\n            if load_timestamp:\n                df[\"load_timestamp\"] = datetime.now()\n            if source_system:\n                df[\"source_system\"] = source_system\n\n        return df\n\n    def _add_quarantine_metadata_spark(\n        self,\n        df,\n        dim_table: str,\n        source_col: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"Add metadata columns to quarantined Spark DataFrame.\"\"\"\n        from pyspark.sql import functions as F\n\n        add_columns = quarantine_config.get(\"add_columns\", {})\n\n        if add_columns.get(\"_rejection_reason\", False):\n            reason = f\"Orphan record: no match in dimension '{dim_table}' on column '{source_col}'\"\n            df = df.withColumn(\"_rejection_reason\", F.lit(reason))\n\n        if add_columns.get(\"_rejected_at\", False):\n            df = df.withColumn(\"_rejected_at\", F.current_timestamp())\n\n        if add_columns.get(\"_source_dimension\", False):\n            df = df.withColumn(\"_source_dimension\", F.lit(dim_table))\n\n        return df\n\n    def _add_quarantine_metadata_pandas(\n        self,\n        df,\n        dim_table: str,\n        source_col: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"Add metadata columns to quarantined Pandas DataFrame.\"\"\"\n        add_columns = quarantine_config.get(\"add_columns\", {})\n\n        if add_columns.get(\"_rejection_reason\", False):\n            reason = f\"Orphan record: no match in dimension '{dim_table}' on column '{source_col}'\"\n            df[\"_rejection_reason\"] = reason\n\n        if add_columns.get(\"_rejected_at\", False):\n            df[\"_rejected_at\"] = datetime.now()\n\n        if add_columns.get(\"_source_dimension\", False):\n            df[\"_source_dimension\"] = dim_table\n\n        return df\n\n    def _write_quarantine(\n        self,\n        context: EngineContext,\n        quarantined_df,\n        quarantine_config: Dict,\n    ):\n        \"\"\"Write quarantined records to the configured destination.\"\"\"\n        ctx = get_logging_context()\n        connection = quarantine_config.get(\"connection\")\n        path = quarantine_config.get(\"path\")\n        table = quarantine_config.get(\"table\")\n\n        if context.engine_type == EngineType.SPARK:\n            self._write_quarantine_spark(context, quarantined_df, connection, path, table)\n        else:\n            self._write_quarantine_pandas(context, quarantined_df, connection, path, table)\n\n        ctx.debug(\n            \"Quarantine data written\",\n            pattern=\"FactPattern\",\n            connection=connection,\n            destination=path or table,\n        )\n\n    def _write_quarantine_spark(\n        self,\n        context: EngineContext,\n        df,\n        connection: str,\n        path: Optional[str],\n        table: Optional[str],\n    ):\n        \"\"\"Write quarantine data using Spark.\"\"\"\n        if table:\n            full_table = f\"{connection}.{table}\" if connection else table\n            df.write.format(\"delta\").mode(\"append\").saveAsTable(full_table)\n        elif path:\n            full_path = path\n            if hasattr(context, \"engine\") and context.engine:\n                if connection in getattr(context.engine, \"connections\", {}):\n                    try:\n                        full_path = context.engine.connections[connection].get_path(path)\n                    except Exception:\n                        pass\n            df.write.format(\"delta\").mode(\"append\").save(full_path)\n\n    def _write_quarantine_pandas(\n        self,\n        context: EngineContext,\n        df,\n        connection: str,\n        path: Optional[str],\n        table: Optional[str],\n    ):\n        \"\"\"Write quarantine data using Pandas.\"\"\"\n        import os\n\n        destination = path or table\n        full_path = destination\n\n        if hasattr(context, \"engine\") and context.engine:\n            if connection in getattr(context.engine, \"connections\", {}):\n                try:\n                    full_path = context.engine.connections[connection].get_path(destination)\n                except Exception:\n                    pass\n\n        path_lower = str(full_path).lower()\n\n        if path_lower.endswith(\".csv\"):\n            if os.path.exists(full_path):\n                df.to_csv(full_path, mode=\"a\", header=False, index=False)\n            else:\n                df.to_csv(full_path, index=False)\n        elif path_lower.endswith(\".json\"):\n            if os.path.exists(full_path):\n                import pandas as pd\n\n                existing = pd.read_json(full_path)\n                combined = pd.concat([existing, df], ignore_index=True)\n                combined.to_json(full_path, orient=\"records\")\n            else:\n                df.to_json(full_path, orient=\"records\")\n        else:\n            if os.path.exists(full_path):\n                import pandas as pd\n\n                existing = pd.read_parquet(full_path)\n                combined = pd.concat([existing, df], ignore_index=True)\n                combined.to_parquet(full_path, index=False)\n            else:\n                df.to_parquet(full_path, index=False)\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.aggregation","title":"<code>odibi.patterns.aggregation</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.aggregation.AggregationPattern","title":"<code>AggregationPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Aggregation Pattern: Declarative aggregation with time-grain rollups.</p> <p>Features: - Declare grain (GROUP BY columns) - Declare measures with aggregation functions - Incremental aggregation (merge new data with existing) - Time rollups (generate multiple grain levels) - Audit columns</p> <p>Configuration Options (via params dict):     - grain (list): Columns to GROUP BY (defines uniqueness)     - measures (list): Measure definitions with name and aggregation expr         - name: Output column name         - expr: SQL aggregation expression (e.g., \"SUM(amount)\")     - incremental (dict): Incremental merge configuration (optional)         - timestamp_column: Column to identify new data         - merge_strategy: \"replace\", \"sum\", \"min\", or \"max\"     - having (str): Optional HAVING clause for filtering aggregates     - audit (dict): Audit column configuration</p> Example Config <p>pattern:   type: aggregation   params:     grain: [date_sk, product_sk]     measures:       - name: total_revenue         expr: \"SUM(total_amount)\"       - name: order_count         expr: \"COUNT()\"       - name: avg_order_value         expr: \"AVG(total_amount)\"     having: \"COUNT() &gt; 0\"     audit:       load_timestamp: true</p> Source code in <code>odibi\\patterns\\aggregation.py</code> <pre><code>class AggregationPattern(Pattern):\n    \"\"\"\n    Aggregation Pattern: Declarative aggregation with time-grain rollups.\n\n    Features:\n    - Declare grain (GROUP BY columns)\n    - Declare measures with aggregation functions\n    - Incremental aggregation (merge new data with existing)\n    - Time rollups (generate multiple grain levels)\n    - Audit columns\n\n    Configuration Options (via params dict):\n        - **grain** (list): Columns to GROUP BY (defines uniqueness)\n        - **measures** (list): Measure definitions with name and aggregation expr\n            - name: Output column name\n            - expr: SQL aggregation expression (e.g., \"SUM(amount)\")\n        - **incremental** (dict): Incremental merge configuration (optional)\n            - timestamp_column: Column to identify new data\n            - merge_strategy: \"replace\", \"sum\", \"min\", or \"max\"\n        - **having** (str): Optional HAVING clause for filtering aggregates\n        - **audit** (dict): Audit column configuration\n\n    Example Config:\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(total_amount)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: avg_order_value\n                expr: \"AVG(total_amount)\"\n            having: \"COUNT(*) &gt; 0\"\n            audit:\n              load_timestamp: true\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        grain = self.params.get(\"grain\")\n        measures = self.params.get(\"measures\", [])\n\n        ctx.debug(\n            \"AggregationPattern validation starting\",\n            pattern=\"AggregationPattern\",\n            grain=grain,\n            measures_count=len(measures),\n        )\n\n        if not grain:\n            ctx.error(\n                \"AggregationPattern validation failed: 'grain' is required\",\n                pattern=\"AggregationPattern\",\n            )\n            raise ValueError(\"AggregationPattern: 'grain' parameter is required.\")\n\n        if not measures:\n            ctx.error(\n                \"AggregationPattern validation failed: 'measures' is required\",\n                pattern=\"AggregationPattern\",\n            )\n            raise ValueError(\"AggregationPattern: 'measures' parameter is required.\")\n\n        for i, measure in enumerate(measures):\n            if not isinstance(measure, dict):\n                ctx.error(\n                    f\"AggregationPattern validation failed: measure[{i}] must be a dict\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    f\"AggregationPattern: measure[{i}] must be a dict with 'name' and 'expr'.\"\n                )\n            if \"name\" not in measure:\n                ctx.error(\n                    f\"AggregationPattern validation failed: measure[{i}] missing 'name'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(f\"AggregationPattern: measure[{i}] missing 'name'.\")\n            if \"expr\" not in measure:\n                ctx.error(\n                    f\"AggregationPattern validation failed: measure[{i}] missing 'expr'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(f\"AggregationPattern: measure[{i}] missing 'expr'.\")\n\n        incremental = self.params.get(\"incremental\")\n        if incremental:\n            if \"timestamp_column\" not in incremental:\n                ctx.error(\n                    \"AggregationPattern validation failed: incremental missing 'timestamp_column'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    \"AggregationPattern: incremental config requires 'timestamp_column'.\"\n                )\n            merge_strategy = incremental.get(\"merge_strategy\", \"replace\")\n            if merge_strategy not in (\"replace\", \"sum\", \"min\", \"max\"):\n                ctx.error(\n                    f\"AggregationPattern validation failed: invalid merge_strategy '{merge_strategy}'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    f\"AggregationPattern: 'merge_strategy' must be 'replace', 'sum', 'min', or 'max'. \"\n                    f\"Got: {merge_strategy}\"\n                )\n\n        ctx.debug(\n            \"AggregationPattern validation passed\",\n            pattern=\"AggregationPattern\",\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        grain = self.params.get(\"grain\")\n        measures = self.params.get(\"measures\", [])\n        having = self.params.get(\"having\")\n        incremental = self.params.get(\"incremental\")\n        audit_config = self.params.get(\"audit\", {})\n        target = self.params.get(\"target\")\n\n        ctx.debug(\n            \"AggregationPattern starting\",\n            pattern=\"AggregationPattern\",\n            grain=grain,\n            measures_count=len(measures),\n            incremental=incremental is not None,\n        )\n\n        df = context.df\n        source_count = self._get_row_count(df, context.engine_type)\n        ctx.debug(\n            \"Aggregation source loaded\",\n            pattern=\"AggregationPattern\",\n            source_rows=source_count,\n        )\n\n        try:\n            result_df = self._aggregate(context, df, grain, measures, having)\n\n            if incremental and target:\n                result_df = self._apply_incremental(\n                    context, result_df, grain, measures, incremental, target\n                )\n\n            result_df = self._add_audit_columns(context, result_df, audit_config)\n\n            result_count = self._get_row_count(result_df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"AggregationPattern completed\",\n                pattern=\"AggregationPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                source_rows=source_count,\n                result_rows=result_count,\n                grain=grain,\n            )\n\n            return result_df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"AggregationPattern failed: {e}\",\n                pattern=\"AggregationPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _aggregate(\n        self,\n        context: EngineContext,\n        df,\n        grain: List[str],\n        measures: List[Dict],\n        having: Optional[str],\n    ):\n        \"\"\"Perform the aggregation using SQL.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._aggregate_spark(context, df, grain, measures, having)\n        else:\n            return self._aggregate_pandas(context, df, grain, measures, having)\n\n    def _aggregate_spark(\n        self,\n        context: EngineContext,\n        df,\n        grain: List[str],\n        measures: List[Dict],\n        having: Optional[str],\n    ):\n        \"\"\"Aggregate using Spark SQL.\"\"\"\n        from pyspark.sql import functions as F\n\n        grain_cols = [F.col(c) for c in grain]\n\n        agg_exprs = []\n        for measure in measures:\n            name = measure[\"name\"]\n            expr = measure[\"expr\"]\n            agg_exprs.append(F.expr(expr).alias(name))\n\n        result = df.groupBy(*grain_cols).agg(*agg_exprs)\n\n        if having:\n            result = result.filter(F.expr(having))\n\n        return result\n\n    def _aggregate_pandas(\n        self,\n        context: EngineContext,\n        df,\n        grain: List[str],\n        measures: List[Dict],\n        having: Optional[str],\n    ):\n        \"\"\"Aggregate using DuckDB SQL via context.sql().\"\"\"\n        grain_str = \", \".join(grain)\n\n        measure_exprs = []\n        for measure in measures:\n            name = measure[\"name\"]\n            expr = measure[\"expr\"]\n            measure_exprs.append(f\"{expr} AS {name}\")\n        measures_str = \", \".join(measure_exprs)\n\n        sql = f\"SELECT {grain_str}, {measures_str} FROM df GROUP BY {grain_str}\"\n\n        if having:\n            sql += f\" HAVING {having}\"\n\n        temp_context = context.with_df(df)\n        result_context = temp_context.sql(sql)\n        return result_context.df\n\n    def _apply_incremental(\n        self,\n        context: EngineContext,\n        new_agg_df,\n        grain: List[str],\n        measures: List[Dict],\n        incremental: Dict,\n        target: str,\n    ):\n        \"\"\"Apply incremental merge with existing aggregations.\"\"\"\n        merge_strategy = incremental.get(\"merge_strategy\", \"replace\")\n\n        existing_df = self._load_existing_target(context, target)\n        if existing_df is None:\n            return new_agg_df\n\n        if merge_strategy == \"replace\":\n            return self._merge_replace(context, existing_df, new_agg_df, grain)\n        elif merge_strategy == \"sum\":\n            return self._merge_sum(context, existing_df, new_agg_df, grain, measures)\n        elif merge_strategy == \"min\":\n            return self._merge_min(context, existing_df, new_agg_df, grain, measures)\n        else:  # max\n            return self._merge_max(context, existing_df, new_agg_df, grain, measures)\n\n    def _load_existing_target(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table if it exists.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._load_existing_spark(context, target)\n        else:\n            return self._load_existing_pandas(context, target)\n\n    def _load_existing_spark(self, context: EngineContext, target: str):\n        spark = context.spark\n        try:\n            return spark.table(target)\n        except Exception:\n            try:\n                return spark.read.format(\"delta\").load(target)\n            except Exception:\n                return None\n\n    def _load_existing_pandas(self, context: EngineContext, target: str):\n        import os\n\n        import pandas as pd\n\n        path = target\n        if hasattr(context, \"engine\") and context.engine:\n            if \".\" in path:\n                parts = path.split(\".\", 1)\n                conn_name = parts[0]\n                rel_path = parts[1]\n                if conn_name in context.engine.connections:\n                    try:\n                        path = context.engine.connections[conn_name].get_path(rel_path)\n                    except Exception:\n                        pass\n\n        if not os.path.exists(path):\n            return None\n\n        try:\n            if str(path).endswith(\".parquet\") or os.path.isdir(path):\n                return pd.read_parquet(path)\n            elif str(path).endswith(\".csv\"):\n                return pd.read_csv(path)\n        except Exception:\n            return None\n\n        return None\n\n    def _merge_replace(self, context: EngineContext, existing_df, new_df, grain: List[str]):\n        \"\"\"\n        Replace strategy: New aggregates overwrite existing for matching grain keys.\n        \"\"\"\n        if context.engine_type == EngineType.SPARK:\n            new_keys = new_df.select(grain).distinct()\n\n            unchanged = existing_df.join(new_keys, on=grain, how=\"left_anti\")\n\n            return unchanged.unionByName(new_df, allowMissingColumns=True)\n        else:\n            import pandas as pd\n\n            new_keys = new_df[grain].drop_duplicates()\n\n            merged = pd.merge(existing_df, new_keys, on=grain, how=\"left\", indicator=True)\n            unchanged = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n            return pd.concat([unchanged, new_df], ignore_index=True)\n\n    def _merge_sum(\n        self,\n        context: EngineContext,\n        existing_df,\n        new_df,\n        grain: List[str],\n        measures: List[Dict],\n    ):\n        \"\"\"\n        Sum strategy: Add new measure values to existing for matching grain keys.\n        \"\"\"\n        measure_names = [m[\"name\"] for m in measures]\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            joined = existing_df.alias(\"e\").join(new_df.alias(\"n\"), on=grain, how=\"full_outer\")\n\n            select_cols = []\n            for col in grain:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            for name in measure_names:\n                select_cols.append(\n                    (\n                        F.coalesce(F.col(f\"e.{name}\"), F.lit(0))\n                        + F.coalesce(F.col(f\"n.{name}\"), F.lit(0))\n                    ).alias(name)\n                )\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            return joined.select(select_cols)\n        else:\n            import pandas as pd\n\n            merged = pd.merge(existing_df, new_df, on=grain, how=\"outer\", suffixes=(\"_e\", \"_n\"))\n\n            result = merged[grain].copy()\n\n            for name in measure_names:\n                e_col = f\"{name}_e\" if f\"{name}_e\" in merged.columns else name\n                n_col = f\"{name}_n\" if f\"{name}_n\" in merged.columns else name\n\n                if e_col in merged.columns and n_col in merged.columns:\n                    result[name] = merged[e_col].fillna(0).infer_objects(copy=False) + merged[\n                        n_col\n                    ].fillna(0).infer_objects(copy=False)\n                elif e_col in merged.columns:\n                    result[name] = merged[e_col].fillna(0).infer_objects(copy=False)\n                elif n_col in merged.columns:\n                    result[name] = merged[n_col].fillna(0).infer_objects(copy=False)\n                else:\n                    result[name] = 0\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                e_col = f\"{col}_e\" if f\"{col}_e\" in merged.columns else col\n                n_col = f\"{col}_n\" if f\"{col}_n\" in merged.columns else col\n                if e_col in merged.columns:\n                    result[col] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[col] = merged[n_col]\n\n            return result\n\n    def _merge_min(\n        self,\n        context: EngineContext,\n        existing_df,\n        new_df,\n        grain: List[str],\n        measures: List[Dict],\n    ):\n        \"\"\"\n        Min strategy: Keep the minimum value for each measure across existing and new.\n        \"\"\"\n        measure_names = [m[\"name\"] for m in measures]\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            joined = existing_df.alias(\"e\").join(new_df.alias(\"n\"), on=grain, how=\"full_outer\")\n\n            select_cols = []\n            for col in grain:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            for name in measure_names:\n                select_cols.append(\n                    F.least(\n                        F.coalesce(F.col(f\"e.{name}\"), F.col(f\"n.{name}\")),\n                        F.coalesce(F.col(f\"n.{name}\"), F.col(f\"e.{name}\")),\n                    ).alias(name)\n                )\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            return joined.select(select_cols)\n        else:\n            import pandas as pd\n\n            merged = pd.merge(existing_df, new_df, on=grain, how=\"outer\", suffixes=(\"_e\", \"_n\"))\n\n            result = merged[grain].copy()\n\n            for name in measure_names:\n                e_col = f\"{name}_e\" if f\"{name}_e\" in merged.columns else name\n                n_col = f\"{name}_n\" if f\"{name}_n\" in merged.columns else name\n\n                if e_col in merged.columns and n_col in merged.columns:\n                    result[name] = merged[[e_col, n_col]].min(axis=1)\n                elif e_col in merged.columns:\n                    result[name] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[name] = merged[n_col]\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                e_col = f\"{col}_e\" if f\"{col}_e\" in merged.columns else col\n                n_col = f\"{col}_n\" if f\"{col}_n\" in merged.columns else col\n                if e_col in merged.columns:\n                    result[col] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[col] = merged[n_col]\n\n            return result\n\n    def _merge_max(\n        self,\n        context: EngineContext,\n        existing_df,\n        new_df,\n        grain: List[str],\n        measures: List[Dict],\n    ):\n        \"\"\"\n        Max strategy: Keep the maximum value for each measure across existing and new.\n        \"\"\"\n        measure_names = [m[\"name\"] for m in measures]\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            joined = existing_df.alias(\"e\").join(new_df.alias(\"n\"), on=grain, how=\"full_outer\")\n\n            select_cols = []\n            for col in grain:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            for name in measure_names:\n                select_cols.append(\n                    F.greatest(\n                        F.coalesce(F.col(f\"e.{name}\"), F.col(f\"n.{name}\")),\n                        F.coalesce(F.col(f\"n.{name}\"), F.col(f\"e.{name}\")),\n                    ).alias(name)\n                )\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            return joined.select(select_cols)\n        else:\n            import pandas as pd\n\n            merged = pd.merge(existing_df, new_df, on=grain, how=\"outer\", suffixes=(\"_e\", \"_n\"))\n\n            result = merged[grain].copy()\n\n            for name in measure_names:\n                e_col = f\"{name}_e\" if f\"{name}_e\" in merged.columns else name\n                n_col = f\"{name}_n\" if f\"{name}_n\" in merged.columns else name\n\n                if e_col in merged.columns and n_col in merged.columns:\n                    result[name] = merged[[e_col, n_col]].max(axis=1)\n                elif e_col in merged.columns:\n                    result[name] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[name] = merged[n_col]\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                e_col = f\"{col}_e\" if f\"{col}_e\" in merged.columns else col\n                n_col = f\"{col}_n\" if f\"{col}_n\" in merged.columns else col\n                if e_col in merged.columns:\n                    result[col] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[col] = merged[n_col]\n\n            return result\n\n    def _add_audit_columns(self, context: EngineContext, df, audit_config: Dict):\n        \"\"\"Add audit columns (load_timestamp, source_system).\"\"\"\n        load_timestamp = audit_config.get(\"load_timestamp\", False)\n        source_system = audit_config.get(\"source_system\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            if load_timestamp:\n                df = df.withColumn(\"load_timestamp\", F.current_timestamp())\n            if source_system:\n                df = df.withColumn(\"source_system\", F.lit(source_system))\n        else:\n            if load_timestamp or source_system:\n                df = df.copy()\n            if load_timestamp:\n                df[\"load_timestamp\"] = datetime.now()\n            if source_system:\n                df[\"source_system\"] = source_system\n\n        return df\n</code></pre>"},{"location":"reference/api/pipeline/","title":"Pipeline API","text":""},{"location":"reference/api/pipeline/#odibi.pipeline","title":"<code>odibi.pipeline</code>","text":"<p>Pipeline executor and orchestration.</p>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager","title":"<code>PipelineManager</code>","text":"<p>Manages multiple pipelines from a YAML configuration.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>class PipelineManager:\n    \"\"\"Manages multiple pipelines from a YAML configuration.\"\"\"\n\n    def __init__(\n        self,\n        project_config: ProjectConfig,\n        connections: Dict[str, Any],\n    ):\n        \"\"\"Initialize pipeline manager.\n\n        Args:\n            project_config: Validated project configuration\n            connections: Connection objects (already instantiated)\n        \"\"\"\n        self.project_config = project_config\n        self.connections = connections\n        self._pipelines: Dict[str, Pipeline] = {}\n        self.catalog_manager = None\n        self.lineage_adapter = None\n\n        # Configure logging\n        configure_logging(\n            structured=project_config.logging.structured, level=project_config.logging.level.value\n        )\n\n        # Create manager-level logging context\n        self._ctx = create_logging_context(engine=project_config.engine)\n\n        self._ctx.info(\n            \"Initializing PipelineManager\",\n            project=project_config.project,\n            engine=project_config.engine,\n            pipeline_count=len(project_config.pipelines),\n            connection_count=len(connections),\n        )\n\n        # Initialize Lineage Adapter\n        self.lineage_adapter = OpenLineageAdapter(project_config.lineage)\n\n        # Initialize CatalogManager if configured\n        if project_config.system:\n            from odibi.catalog import CatalogManager\n\n            spark = None\n            engine_instance = None\n\n            if project_config.engine == \"spark\":\n                try:\n                    from odibi.engine.spark_engine import SparkEngine\n\n                    temp_engine = SparkEngine(connections=connections, config={})\n                    spark = temp_engine.spark\n                    self._ctx.debug(\"Spark session initialized for System Catalog\")\n                except Exception as e:\n                    self._ctx.warning(\n                        f\"Failed to initialize Spark for System Catalog: {e}\",\n                        suggestion=\"Check Spark configuration\",\n                    )\n\n            sys_conn = connections.get(project_config.system.connection)\n            if sys_conn:\n                base_path = sys_conn.get_path(project_config.system.path)\n\n                if not spark:\n                    try:\n                        from odibi.engine.pandas_engine import PandasEngine\n\n                        engine_instance = PandasEngine(config={})\n                        self._ctx.debug(\"PandasEngine initialized for System Catalog\")\n                    except Exception as e:\n                        self._ctx.warning(\n                            f\"Failed to initialize PandasEngine for System Catalog: {e}\"\n                        )\n\n                if spark or engine_instance:\n                    self.catalog_manager = CatalogManager(\n                        spark=spark,\n                        config=project_config.system,\n                        base_path=base_path,\n                        engine=engine_instance,\n                        connection=sys_conn,\n                    )\n                    self.catalog_manager.bootstrap()\n                    self._ctx.info(\"System Catalog initialized\", path=base_path)\n            else:\n                self._ctx.warning(\n                    f\"System connection '{project_config.system.connection}' not found\",\n                    suggestion=\"Configure the system connection in your config\",\n                )\n\n        # Get story configuration\n        story_config = self._get_story_config()\n\n        # Create all pipeline instances\n        self._ctx.debug(\n            \"Creating pipeline instances\",\n            pipelines=[p.pipeline for p in project_config.pipelines],\n        )\n        for pipeline_config in project_config.pipelines:\n            pipeline_name = pipeline_config.pipeline\n\n            self._pipelines[pipeline_name] = Pipeline(\n                pipeline_config=pipeline_config,\n                engine=project_config.engine,\n                connections=connections,\n                generate_story=story_config.get(\"auto_generate\", True),\n                story_config=story_config,\n                retry_config=project_config.retry,\n                alerts=project_config.alerts,\n                performance_config=project_config.performance,\n                catalog_manager=self.catalog_manager,\n                lineage_adapter=self.lineage_adapter,\n            )\n            self._pipelines[pipeline_name].project_config = project_config\n\n        self._ctx.info(\n            \"PipelineManager ready\",\n            pipelines=list(self._pipelines.keys()),\n        )\n\n    def _get_story_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Build story config from project_config.story.\n\n        Resolves story output path using connection.\n\n        Returns:\n            Dictionary for StoryGenerator initialization\n        \"\"\"\n        story_cfg = self.project_config.story\n\n        # Resolve story path using connection\n        story_conn = self.connections[story_cfg.connection]\n        output_path = story_conn.get_path(story_cfg.path)\n\n        # Get storage options (e.g., credentials) from connection if available\n        storage_options = {}\n        if hasattr(story_conn, \"pandas_storage_options\"):\n            storage_options = story_conn.pandas_storage_options()\n\n        return {\n            \"auto_generate\": story_cfg.auto_generate,\n            \"max_sample_rows\": story_cfg.max_sample_rows,\n            \"output_path\": output_path,\n            \"storage_options\": storage_options,\n            \"async_generation\": story_cfg.async_generation,\n        }\n\n    @classmethod\n    def from_yaml(cls, yaml_path: str, env: str = None) -&gt; \"PipelineManager\":\n        \"\"\"Create PipelineManager from YAML file.\n\n        Args:\n            yaml_path: Path to YAML configuration file\n            env: Environment name to apply overrides (e.g. 'prod')\n\n        Returns:\n            PipelineManager instance ready to run pipelines\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n            &gt;&gt;&gt; results = manager.run()  # Run all pipelines\n        \"\"\"\n        logger.info(f\"Loading configuration from: {yaml_path}\")\n\n        register_standard_library()\n\n        yaml_path_obj = Path(yaml_path)\n        config_dir = yaml_path_obj.parent.absolute()\n\n        import importlib.util\n        import os\n        import sys\n\n        def load_transforms_module(path):\n            if os.path.exists(path):\n                try:\n                    spec = importlib.util.spec_from_file_location(\"transforms_autodiscovered\", path)\n                    if spec and spec.loader:\n                        module = importlib.util.module_from_spec(spec)\n                        sys.modules[\"transforms_autodiscovered\"] = module\n                        spec.loader.exec_module(module)\n                        logger.info(f\"Auto-loaded transforms from: {path}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to auto-load transforms from {path}: {e}\")\n\n        load_transforms_module(os.path.join(config_dir, \"transforms.py\"))\n\n        cwd = os.getcwd()\n        if os.path.abspath(cwd) != str(config_dir):\n            load_transforms_module(os.path.join(cwd, \"transforms.py\"))\n\n        try:\n            config = load_yaml_with_env(str(yaml_path_obj), env=env)\n            logger.debug(\"Configuration loaded successfully\")\n        except FileNotFoundError:\n            logger.error(f\"YAML file not found: {yaml_path}\")\n            raise FileNotFoundError(f\"YAML file not found: {yaml_path}\")\n\n        project_config = ProjectConfig(**config)\n        logger.debug(\n            \"Project config validated\",\n            project=project_config.project,\n            pipelines=len(project_config.pipelines),\n        )\n\n        connections = cls._build_connections(project_config.connections)\n\n        return cls(\n            project_config=project_config,\n            connections=connections,\n        )\n\n    @staticmethod\n    def _build_connections(conn_configs: Dict[str, Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Convert connection configs to connection objects.\n\n        Args:\n            conn_configs: Connection configurations from ProjectConfig\n\n        Returns:\n            Dictionary of connection name -&gt; connection object\n\n        Raises:\n            ValueError: If connection type is not supported\n        \"\"\"\n        from odibi.connections.factory import register_builtins\n\n        logger.debug(f\"Building {len(conn_configs)} connections\")\n\n        connections = {}\n\n        register_builtins()\n        load_plugins()\n\n        for conn_name, conn_config in conn_configs.items():\n            if hasattr(conn_config, \"model_dump\"):\n                conn_config = conn_config.model_dump()\n            elif hasattr(conn_config, \"dict\"):\n                conn_config = conn_config.model_dump()\n\n            conn_type = conn_config.get(\"type\", \"local\")\n\n            factory = get_connection_factory(conn_type)\n            if factory:\n                try:\n                    connections[conn_name] = factory(conn_name, conn_config)\n                    logger.debug(\n                        f\"Connection created: {conn_name}\",\n                        type=conn_type,\n                    )\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to create connection '{conn_name}'\",\n                        type=conn_type,\n                        error=str(e),\n                    )\n                    raise ValueError(\n                        f\"Failed to create connection '{conn_name}' (type={conn_type}): {e}\"\n                    ) from e\n            else:\n                logger.error(\n                    f\"Unsupported connection type: {conn_type}\",\n                    connection=conn_name,\n                    suggestion=\"Check supported connection types in docs\",\n                )\n                raise ValueError(\n                    f\"Unsupported connection type: {conn_type}. \"\n                    f\"Supported types: local, azure_adls, azure_sql, delta, etc. \"\n                    f\"See docs for connection setup.\"\n                )\n\n        try:\n            from odibi.utils import configure_connections_parallel\n\n            connections, errors = configure_connections_parallel(connections, verbose=False)\n            if errors:\n                for error in errors:\n                    logger.warning(error)\n        except ImportError:\n            pass\n\n        logger.info(f\"Built {len(connections)} connections successfully\")\n\n        return connections\n\n    def run(\n        self,\n        pipelines: Optional[Union[str, List[str]]] = None,\n        dry_run: bool = False,\n        resume_from_failure: bool = False,\n        parallel: bool = False,\n        max_workers: int = 4,\n        on_error: Optional[str] = None,\n        tag: Optional[str] = None,\n        node: Optional[str] = None,\n        console: bool = False,\n    ) -&gt; Union[PipelineResults, Dict[str, PipelineResults]]:\n        \"\"\"Run one, multiple, or all pipelines.\n\n        Args:\n            pipelines: Pipeline name(s) to run.\n            dry_run: Whether to simulate execution.\n            resume_from_failure: Whether to skip successfully completed nodes from last run.\n            parallel: Whether to run nodes in parallel.\n            max_workers: Maximum number of worker threads for parallel execution.\n            on_error: Override error handling strategy (fail_fast, fail_later, ignore).\n            tag: Filter nodes by tag (only nodes with this tag will run).\n            node: Run only the specific node by name.\n            console: Whether to show rich console output with progress.\n\n        Returns:\n            PipelineResults or Dict of results\n        \"\"\"\n        if pipelines is None:\n            pipeline_names = list(self._pipelines.keys())\n        elif isinstance(pipelines, str):\n            pipeline_names = [pipelines]\n        else:\n            pipeline_names = pipelines\n\n        for name in pipeline_names:\n            if name not in self._pipelines:\n                available = \", \".join(self._pipelines.keys())\n                self._ctx.error(\n                    f\"Pipeline not found: {name}\",\n                    available=list(self._pipelines.keys()),\n                )\n                raise ValueError(f\"Pipeline '{name}' not found. Available pipelines: {available}\")\n\n        # Phase 2: Auto-register pipelines and nodes before execution\n        if self.catalog_manager:\n            self._auto_register_pipelines(pipeline_names)\n\n        self._ctx.info(\n            f\"Running {len(pipeline_names)} pipeline(s)\",\n            pipelines=pipeline_names,\n            dry_run=dry_run,\n            parallel=parallel,\n        )\n\n        results = {}\n        for idx, name in enumerate(pipeline_names):\n            # Invalidate cache before each pipeline so it sees latest outputs\n            if self.catalog_manager:\n                self.catalog_manager.invalidate_cache()\n\n            self._ctx.info(\n                f\"Executing pipeline {idx + 1}/{len(pipeline_names)}: {name}\",\n                pipeline=name,\n                order=idx + 1,\n            )\n\n            results[name] = self._pipelines[name].run(\n                dry_run=dry_run,\n                resume_from_failure=resume_from_failure,\n                parallel=parallel,\n                max_workers=max_workers,\n                on_error=on_error,\n                tag=tag,\n                node=node,\n                console=console,\n            )\n\n            result = results[name]\n            status = \"SUCCESS\" if not result.failed else \"FAILED\"\n            self._ctx.info(\n                f\"Pipeline {status}: {name}\",\n                status=status,\n                duration_s=round(result.duration, 2),\n                completed=len(result.completed),\n                failed=len(result.failed),\n            )\n\n            if result.story_path:\n                self._ctx.debug(f\"Story generated: {result.story_path}\")\n\n        if len(pipeline_names) == 1:\n            return results[pipeline_names[0]]\n        else:\n            return results\n\n    def list_pipelines(self) -&gt; List[str]:\n        \"\"\"Get list of available pipeline names.\n\n        Returns:\n            List of pipeline names\n        \"\"\"\n        return list(self._pipelines.keys())\n\n    def get_pipeline(self, name: str) -&gt; Pipeline:\n        \"\"\"Get a specific pipeline instance.\n\n        Args:\n            name: Pipeline name\n\n        Returns:\n            Pipeline instance\n\n        Raises:\n            ValueError: If pipeline not found\n        \"\"\"\n        if name not in self._pipelines:\n            available = \", \".join(self._pipelines.keys())\n            raise ValueError(f\"Pipeline '{name}' not found. Available: {available}\")\n        return self._pipelines[name]\n\n    def deploy(self, pipelines: Optional[Union[str, List[str]]] = None) -&gt; bool:\n        \"\"\"Deploy pipeline definitions to the System Catalog.\n\n        This registers pipeline and node configurations in the catalog,\n        enabling drift detection and governance features.\n\n        Args:\n            pipelines: Optional pipeline name(s) to deploy. If None, deploys all.\n\n        Returns:\n            True if deployment succeeded, False otherwise.\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"odibi.yaml\")\n            &gt;&gt;&gt; manager.deploy()  # Deploy all pipelines\n            &gt;&gt;&gt; manager.deploy(\"sales_daily\")  # Deploy specific pipeline\n        \"\"\"\n        if not self.catalog_manager:\n            self._ctx.warning(\n                \"System Catalog not configured. Cannot deploy.\",\n                suggestion=\"Configure system catalog in your YAML config\",\n            )\n            return False\n\n        if pipelines is None:\n            to_deploy = self.project_config.pipelines\n        elif isinstance(pipelines, str):\n            to_deploy = [p for p in self.project_config.pipelines if p.pipeline == pipelines]\n        else:\n            to_deploy = [p for p in self.project_config.pipelines if p.pipeline in pipelines]\n\n        if not to_deploy:\n            self._ctx.warning(\"No matching pipelines found to deploy.\")\n            return False\n\n        self._ctx.info(\n            f\"Deploying {len(to_deploy)} pipeline(s) to System Catalog\",\n            pipelines=[p.pipeline for p in to_deploy],\n        )\n\n        try:\n            self.catalog_manager.bootstrap()\n\n            for pipeline_config in to_deploy:\n                self._ctx.debug(\n                    f\"Deploying pipeline: {pipeline_config.pipeline}\",\n                    node_count=len(pipeline_config.nodes),\n                )\n                self.catalog_manager.register_pipeline(pipeline_config, self.project_config)\n\n                for node in pipeline_config.nodes:\n                    self.catalog_manager.register_node(pipeline_config.pipeline, node)\n\n            self._ctx.info(\n                f\"Deployment complete: {len(to_deploy)} pipeline(s)\",\n                deployed=[p.pipeline for p in to_deploy],\n            )\n            return True\n\n        except Exception as e:\n            self._ctx.error(\n                f\"Deployment failed: {e}\",\n                error_type=type(e).__name__,\n                suggestion=\"Check catalog configuration and permissions\",\n            )\n            return False\n\n    def _auto_register_pipelines(self, pipeline_names: List[str]) -&gt; None:\n        \"\"\"Auto-register pipelines and nodes before execution.\n\n        This ensures meta_pipelines and meta_nodes are populated automatically\n        when running pipelines, without requiring explicit deploy() calls.\n\n        Uses \"check-before-write\" pattern with batch writes for performance:\n        - Reads existing hashes in one read\n        - Compares version_hash to skip unchanged records\n        - Batch writes only changed/new records\n\n        Args:\n            pipeline_names: List of pipeline names to register\n        \"\"\"\n        if not self.catalog_manager:\n            return\n\n        try:\n            import hashlib\n            import json\n\n            existing_pipelines = self.catalog_manager.get_all_registered_pipelines()\n            existing_nodes = self.catalog_manager.get_all_registered_nodes(pipeline_names)\n\n            pipeline_records = []\n            node_records = []\n\n            for name in pipeline_names:\n                pipeline = self._pipelines[name]\n                config = pipeline.config\n\n                if hasattr(config, \"model_dump\"):\n                    dump = config.model_dump(mode=\"json\")\n                else:\n                    dump = config.model_dump()\n                dump_str = json.dumps(dump, sort_keys=True)\n                pipeline_hash = hashlib.md5(dump_str.encode(\"utf-8\")).hexdigest()\n\n                if existing_pipelines.get(name) != pipeline_hash:\n                    all_tags = set()\n                    for node in config.nodes:\n                        if node.tags:\n                            all_tags.update(node.tags)\n\n                    pipeline_records.append(\n                        {\n                            \"pipeline_name\": name,\n                            \"version_hash\": pipeline_hash,\n                            \"description\": config.description or \"\",\n                            \"layer\": config.layer or \"\",\n                            \"schedule\": \"\",\n                            \"tags_json\": json.dumps(list(all_tags)),\n                        }\n                    )\n\n                pipeline_existing_nodes = existing_nodes.get(name, {})\n                for node in config.nodes:\n                    if hasattr(node, \"model_dump\"):\n                        node_dump = node.model_dump(\n                            mode=\"json\", exclude={\"description\", \"tags\", \"log_level\"}\n                        )\n                    else:\n                        node_dump = node.model_dump(exclude={\"description\", \"tags\", \"log_level\"})\n                    node_dump_str = json.dumps(node_dump, sort_keys=True)\n                    node_hash = hashlib.md5(node_dump_str.encode(\"utf-8\")).hexdigest()\n\n                    if pipeline_existing_nodes.get(node.name) != node_hash:\n                        node_type = \"transform\"\n                        if node.read:\n                            node_type = \"read\"\n                        if node.write:\n                            node_type = \"write\"\n\n                        node_records.append(\n                            {\n                                \"pipeline_name\": name,\n                                \"node_name\": node.name,\n                                \"version_hash\": node_hash,\n                                \"type\": node_type,\n                                \"config_json\": json.dumps(node_dump),\n                            }\n                        )\n\n            if pipeline_records:\n                self.catalog_manager.register_pipelines_batch(pipeline_records)\n                self._ctx.debug(\n                    f\"Batch registered {len(pipeline_records)} changed pipeline(s)\",\n                    pipelines=[r[\"pipeline_name\"] for r in pipeline_records],\n                )\n            else:\n                self._ctx.debug(\"All pipelines unchanged - skipping registration\")\n\n            if node_records:\n                self.catalog_manager.register_nodes_batch(node_records)\n                self._ctx.debug(\n                    f\"Batch registered {len(node_records)} changed node(s)\",\n                    nodes=[r[\"node_name\"] for r in node_records],\n                )\n            else:\n                self._ctx.debug(\"All nodes unchanged - skipping registration\")\n\n        except Exception as e:\n            self._ctx.warning(\n                f\"Auto-registration failed (non-fatal): {e}\",\n                error_type=type(e).__name__,\n            )\n\n    # -------------------------------------------------------------------------\n    # Phase 5: List/Query Methods\n    # -------------------------------------------------------------------------\n\n    def list_registered_pipelines(self) -&gt; \"pd.DataFrame\":\n        \"\"\"List all registered pipelines from the system catalog.\n\n        Returns:\n            DataFrame with pipeline metadata from meta_pipelines\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(\n                self.catalog_manager.tables[\"meta_pipelines\"]\n            )\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list pipelines: {e}\")\n            return pd.DataFrame()\n\n    def list_registered_nodes(self, pipeline: Optional[str] = None) -&gt; \"pd.DataFrame\":\n        \"\"\"List nodes from the system catalog.\n\n        Args:\n            pipeline: Optional pipeline name to filter by\n\n        Returns:\n            DataFrame with node metadata from meta_nodes\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_nodes\"])\n            if not df.empty and pipeline:\n                df = df[df[\"pipeline_name\"] == pipeline]\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list nodes: {e}\")\n            return pd.DataFrame()\n\n    def list_runs(\n        self,\n        pipeline: Optional[str] = None,\n        node: Optional[str] = None,\n        status: Optional[str] = None,\n        limit: int = 10,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"List recent runs with optional filters.\n\n        Args:\n            pipeline: Optional pipeline name to filter by\n            node: Optional node name to filter by\n            status: Optional status to filter by (SUCCESS, FAILURE)\n            limit: Maximum number of runs to return\n\n        Returns:\n            DataFrame with run history from meta_runs\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n            if df.empty:\n                return df\n\n            if pipeline:\n                df = df[df[\"pipeline_name\"] == pipeline]\n            if node:\n                df = df[df[\"node_name\"] == node]\n            if status:\n                df = df[df[\"status\"] == status]\n\n            if \"timestamp\" in df.columns:\n                df = df.sort_values(\"timestamp\", ascending=False)\n\n            return df.head(limit)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list runs: {e}\")\n            return pd.DataFrame()\n\n    def list_tables(self) -&gt; \"pd.DataFrame\":\n        \"\"\"List registered assets from meta_tables.\n\n        Returns:\n            DataFrame with table/asset metadata\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list tables: {e}\")\n            return pd.DataFrame()\n\n    # -------------------------------------------------------------------------\n    # Phase 5.2: State Methods\n    # -------------------------------------------------------------------------\n\n    def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get a specific state entry (HWM, content hash, etc.).\n\n        Args:\n            key: The state key to look up\n\n        Returns:\n            Dictionary with state data or None if not found\n        \"\"\"\n\n        if not self.catalog_manager:\n            return None\n\n        try:\n            df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n            if df.empty or \"key\" not in df.columns:\n                return None\n\n            row = df[df[\"key\"] == key]\n            if row.empty:\n                return None\n\n            return row.iloc[0].to_dict()\n        except Exception:\n            return None\n\n    def get_all_state(self, prefix: Optional[str] = None) -&gt; \"pd.DataFrame\":\n        \"\"\"Get all state entries, optionally filtered by key prefix.\n\n        Args:\n            prefix: Optional key prefix to filter by\n\n        Returns:\n            DataFrame with state entries\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n            if not df.empty and prefix and \"key\" in df.columns:\n                df = df[df[\"key\"].str.startswith(prefix)]\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get state: {e}\")\n            return pd.DataFrame()\n\n    def clear_state(self, key: str) -&gt; bool:\n        \"\"\"Remove a state entry.\n\n        Args:\n            key: The state key to remove\n\n        Returns:\n            True if deleted, False otherwise\n        \"\"\"\n        if not self.catalog_manager:\n            return False\n\n        try:\n            return self.catalog_manager.clear_state_key(key)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to clear state: {e}\")\n            return False\n\n    # -------------------------------------------------------------------------\n    # Phase 5.3-5.4: Schema/Lineage and Stats Methods\n    # -------------------------------------------------------------------------\n\n    def get_schema_history(\n        self,\n        table: str,\n        limit: int = 5,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Get schema version history for a table.\n\n        Args:\n            table: Table identifier (supports smart path resolution)\n            limit: Maximum number of versions to return\n\n        Returns:\n            DataFrame with schema history\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            resolved_path = self._resolve_table_path(table)\n            history = self.catalog_manager.get_schema_history(resolved_path, limit)\n            return pd.DataFrame(history)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get schema history: {e}\")\n            return pd.DataFrame()\n\n    def get_lineage(\n        self,\n        table: str,\n        direction: str = \"both\",\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Get lineage for a table.\n\n        Args:\n            table: Table identifier (supports smart path resolution)\n            direction: \"upstream\", \"downstream\", or \"both\"\n\n        Returns:\n            DataFrame with lineage relationships\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            resolved_path = self._resolve_table_path(table)\n\n            results = []\n            if direction in (\"upstream\", \"both\"):\n                upstream = self.catalog_manager.get_upstream(resolved_path)\n                for r in upstream:\n                    r[\"direction\"] = \"upstream\"\n                results.extend(upstream)\n\n            if direction in (\"downstream\", \"both\"):\n                downstream = self.catalog_manager.get_downstream(resolved_path)\n                for r in downstream:\n                    r[\"direction\"] = \"downstream\"\n                results.extend(downstream)\n\n            return pd.DataFrame(results)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get lineage: {e}\")\n            return pd.DataFrame()\n\n    def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n        \"\"\"Get last run status, duration, timestamp for a pipeline.\n\n        Args:\n            pipeline: Pipeline name\n\n        Returns:\n            Dict with status info\n        \"\"\"\n        if not self.catalog_manager:\n            return {}\n\n        try:\n            runs = self.list_runs(pipeline=pipeline, limit=1)\n            if runs.empty:\n                return {\"status\": \"never_run\", \"pipeline\": pipeline}\n\n            last_run = runs.iloc[0].to_dict()\n            return {\n                \"pipeline\": pipeline,\n                \"last_status\": last_run.get(\"status\"),\n                \"last_run_at\": last_run.get(\"timestamp\"),\n                \"last_duration_ms\": last_run.get(\"duration_ms\"),\n                \"last_node\": last_run.get(\"node_name\"),\n            }\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get pipeline status: {e}\")\n            return {}\n\n    def get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n        \"\"\"Get average duration, row counts, success rate over period.\n\n        Args:\n            node: Node name\n            days: Number of days to look back\n\n        Returns:\n            Dict with node statistics\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return {}\n\n        try:\n            avg_duration = self.catalog_manager.get_average_duration(node, days)\n\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n            if df.empty:\n                return {\"node\": node, \"runs\": 0}\n\n            if \"timestamp\" in df.columns:\n                cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n                if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n                    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n                if df[\"timestamp\"].dt.tz is None:\n                    df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"UTC\")\n                df = df[df[\"timestamp\"] &gt;= cutoff]\n\n            node_runs = df[df[\"node_name\"] == node]\n            if node_runs.empty:\n                return {\"node\": node, \"runs\": 0}\n\n            total = len(node_runs)\n            success = len(node_runs[node_runs[\"status\"] == \"SUCCESS\"])\n            avg_rows = node_runs[\"rows_processed\"].mean() if \"rows_processed\" in node_runs else None\n\n            return {\n                \"node\": node,\n                \"runs\": total,\n                \"success_rate\": success / total if total &gt; 0 else 0,\n                \"avg_duration_s\": avg_duration,\n                \"avg_rows\": avg_rows,\n                \"period_days\": days,\n            }\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get node stats: {e}\")\n            return {}\n\n    # -------------------------------------------------------------------------\n    # Phase 6: Smart Path Resolution\n    # -------------------------------------------------------------------------\n\n    def _resolve_table_path(self, identifier: str) -&gt; str:\n        \"\"\"Resolve a user-friendly identifier to a full table path.\n\n        Accepts:\n        - Relative path: \"bronze/OEE/vw_OSMPerformanceOEE\"\n        - Registered table: \"test.vw_OSMPerformanceOEE\"\n        - Node name: \"opsvisdata_vw_OSMPerformanceOEE\"\n        - Full path: \"abfss://...\" (used as-is)\n\n        Args:\n            identifier: User-friendly table identifier\n\n        Returns:\n            Full table path\n        \"\"\"\n        if self._is_full_path(identifier):\n            return identifier\n\n        if self.catalog_manager:\n            resolved = self._lookup_in_catalog(identifier)\n            if resolved:\n                return resolved\n\n        for pipeline in self._pipelines.values():\n            for node in pipeline.config.nodes:\n                if node.name == identifier and node.write:\n                    conn = self.connections.get(node.write.connection)\n                    if conn:\n                        return conn.get_path(node.write.path or node.write.table)\n\n        sys_conn_name = (\n            self.project_config.system.connection if self.project_config.system else None\n        )\n        if sys_conn_name:\n            sys_conn = self.connections.get(sys_conn_name)\n            if sys_conn:\n                return sys_conn.get_path(identifier)\n\n        return identifier\n\n    def _is_full_path(self, identifier: str) -&gt; bool:\n        \"\"\"Check if identifier is already a full path.\"\"\"\n        full_path_prefixes = (\"abfss://\", \"s3://\", \"gs://\", \"hdfs://\", \"/\", \"C:\", \"D:\")\n        return identifier.startswith(full_path_prefixes)\n\n    def _lookup_in_catalog(self, identifier: str) -&gt; Optional[str]:\n        \"\"\"Look up identifier in meta_tables catalog.\"\"\"\n        if not self.catalog_manager:\n            return None\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n            if df.empty or \"table_name\" not in df.columns:\n                return None\n\n            match = df[df[\"table_name\"] == identifier]\n            if not match.empty and \"path\" in match.columns:\n                return match.iloc[0][\"path\"]\n\n            if \".\" in identifier:\n                parts = identifier.split(\".\", 1)\n                if len(parts) == 2:\n                    match = df[df[\"table_name\"] == parts[1]]\n                    if not match.empty and \"path\" in match.columns:\n                        return match.iloc[0][\"path\"]\n\n        except Exception:\n            pass\n\n        return None\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.__init__","title":"<code>__init__(project_config, connections)</code>","text":"<p>Initialize pipeline manager.</p> <p>Parameters:</p> Name Type Description Default <code>project_config</code> <code>ProjectConfig</code> <p>Validated project configuration</p> required <code>connections</code> <code>Dict[str, Any]</code> <p>Connection objects (already instantiated)</p> required Source code in <code>odibi\\pipeline.py</code> <pre><code>def __init__(\n    self,\n    project_config: ProjectConfig,\n    connections: Dict[str, Any],\n):\n    \"\"\"Initialize pipeline manager.\n\n    Args:\n        project_config: Validated project configuration\n        connections: Connection objects (already instantiated)\n    \"\"\"\n    self.project_config = project_config\n    self.connections = connections\n    self._pipelines: Dict[str, Pipeline] = {}\n    self.catalog_manager = None\n    self.lineage_adapter = None\n\n    # Configure logging\n    configure_logging(\n        structured=project_config.logging.structured, level=project_config.logging.level.value\n    )\n\n    # Create manager-level logging context\n    self._ctx = create_logging_context(engine=project_config.engine)\n\n    self._ctx.info(\n        \"Initializing PipelineManager\",\n        project=project_config.project,\n        engine=project_config.engine,\n        pipeline_count=len(project_config.pipelines),\n        connection_count=len(connections),\n    )\n\n    # Initialize Lineage Adapter\n    self.lineage_adapter = OpenLineageAdapter(project_config.lineage)\n\n    # Initialize CatalogManager if configured\n    if project_config.system:\n        from odibi.catalog import CatalogManager\n\n        spark = None\n        engine_instance = None\n\n        if project_config.engine == \"spark\":\n            try:\n                from odibi.engine.spark_engine import SparkEngine\n\n                temp_engine = SparkEngine(connections=connections, config={})\n                spark = temp_engine.spark\n                self._ctx.debug(\"Spark session initialized for System Catalog\")\n            except Exception as e:\n                self._ctx.warning(\n                    f\"Failed to initialize Spark for System Catalog: {e}\",\n                    suggestion=\"Check Spark configuration\",\n                )\n\n        sys_conn = connections.get(project_config.system.connection)\n        if sys_conn:\n            base_path = sys_conn.get_path(project_config.system.path)\n\n            if not spark:\n                try:\n                    from odibi.engine.pandas_engine import PandasEngine\n\n                    engine_instance = PandasEngine(config={})\n                    self._ctx.debug(\"PandasEngine initialized for System Catalog\")\n                except Exception as e:\n                    self._ctx.warning(\n                        f\"Failed to initialize PandasEngine for System Catalog: {e}\"\n                    )\n\n            if spark or engine_instance:\n                self.catalog_manager = CatalogManager(\n                    spark=spark,\n                    config=project_config.system,\n                    base_path=base_path,\n                    engine=engine_instance,\n                    connection=sys_conn,\n                )\n                self.catalog_manager.bootstrap()\n                self._ctx.info(\"System Catalog initialized\", path=base_path)\n        else:\n            self._ctx.warning(\n                f\"System connection '{project_config.system.connection}' not found\",\n                suggestion=\"Configure the system connection in your config\",\n            )\n\n    # Get story configuration\n    story_config = self._get_story_config()\n\n    # Create all pipeline instances\n    self._ctx.debug(\n        \"Creating pipeline instances\",\n        pipelines=[p.pipeline for p in project_config.pipelines],\n    )\n    for pipeline_config in project_config.pipelines:\n        pipeline_name = pipeline_config.pipeline\n\n        self._pipelines[pipeline_name] = Pipeline(\n            pipeline_config=pipeline_config,\n            engine=project_config.engine,\n            connections=connections,\n            generate_story=story_config.get(\"auto_generate\", True),\n            story_config=story_config,\n            retry_config=project_config.retry,\n            alerts=project_config.alerts,\n            performance_config=project_config.performance,\n            catalog_manager=self.catalog_manager,\n            lineage_adapter=self.lineage_adapter,\n        )\n        self._pipelines[pipeline_name].project_config = project_config\n\n    self._ctx.info(\n        \"PipelineManager ready\",\n        pipelines=list(self._pipelines.keys()),\n    )\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.clear_state","title":"<code>clear_state(key)</code>","text":"<p>Remove a state entry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The state key to remove</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False otherwise</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def clear_state(self, key: str) -&gt; bool:\n    \"\"\"Remove a state entry.\n\n    Args:\n        key: The state key to remove\n\n    Returns:\n        True if deleted, False otherwise\n    \"\"\"\n    if not self.catalog_manager:\n        return False\n\n    try:\n        return self.catalog_manager.clear_state_key(key)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to clear state: {e}\")\n        return False\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.deploy","title":"<code>deploy(pipelines=None)</code>","text":"<p>Deploy pipeline definitions to the System Catalog.</p> <p>This registers pipeline and node configurations in the catalog, enabling drift detection and governance features.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional pipeline name(s) to deploy. If None, deploys all.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if deployment succeeded, False otherwise.</p> Example <p>manager = PipelineManager.from_yaml(\"odibi.yaml\") manager.deploy()  # Deploy all pipelines manager.deploy(\"sales_daily\")  # Deploy specific pipeline</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def deploy(self, pipelines: Optional[Union[str, List[str]]] = None) -&gt; bool:\n    \"\"\"Deploy pipeline definitions to the System Catalog.\n\n    This registers pipeline and node configurations in the catalog,\n    enabling drift detection and governance features.\n\n    Args:\n        pipelines: Optional pipeline name(s) to deploy. If None, deploys all.\n\n    Returns:\n        True if deployment succeeded, False otherwise.\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"odibi.yaml\")\n        &gt;&gt;&gt; manager.deploy()  # Deploy all pipelines\n        &gt;&gt;&gt; manager.deploy(\"sales_daily\")  # Deploy specific pipeline\n    \"\"\"\n    if not self.catalog_manager:\n        self._ctx.warning(\n            \"System Catalog not configured. Cannot deploy.\",\n            suggestion=\"Configure system catalog in your YAML config\",\n        )\n        return False\n\n    if pipelines is None:\n        to_deploy = self.project_config.pipelines\n    elif isinstance(pipelines, str):\n        to_deploy = [p for p in self.project_config.pipelines if p.pipeline == pipelines]\n    else:\n        to_deploy = [p for p in self.project_config.pipelines if p.pipeline in pipelines]\n\n    if not to_deploy:\n        self._ctx.warning(\"No matching pipelines found to deploy.\")\n        return False\n\n    self._ctx.info(\n        f\"Deploying {len(to_deploy)} pipeline(s) to System Catalog\",\n        pipelines=[p.pipeline for p in to_deploy],\n    )\n\n    try:\n        self.catalog_manager.bootstrap()\n\n        for pipeline_config in to_deploy:\n            self._ctx.debug(\n                f\"Deploying pipeline: {pipeline_config.pipeline}\",\n                node_count=len(pipeline_config.nodes),\n            )\n            self.catalog_manager.register_pipeline(pipeline_config, self.project_config)\n\n            for node in pipeline_config.nodes:\n                self.catalog_manager.register_node(pipeline_config.pipeline, node)\n\n        self._ctx.info(\n            f\"Deployment complete: {len(to_deploy)} pipeline(s)\",\n            deployed=[p.pipeline for p in to_deploy],\n        )\n        return True\n\n    except Exception as e:\n        self._ctx.error(\n            f\"Deployment failed: {e}\",\n            error_type=type(e).__name__,\n            suggestion=\"Check catalog configuration and permissions\",\n        )\n        return False\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.from_yaml","title":"<code>from_yaml(yaml_path, env=None)</code>  <code>classmethod</code>","text":"<p>Create PipelineManager from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to YAML configuration file</p> required <code>env</code> <code>str</code> <p>Environment name to apply overrides (e.g. 'prod')</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineManager</code> <p>PipelineManager instance ready to run pipelines</p> Example <p>manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\") results = manager.run()  # Run all pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: str, env: str = None) -&gt; \"PipelineManager\":\n    \"\"\"Create PipelineManager from YAML file.\n\n    Args:\n        yaml_path: Path to YAML configuration file\n        env: Environment name to apply overrides (e.g. 'prod')\n\n    Returns:\n        PipelineManager instance ready to run pipelines\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n        &gt;&gt;&gt; results = manager.run()  # Run all pipelines\n    \"\"\"\n    logger.info(f\"Loading configuration from: {yaml_path}\")\n\n    register_standard_library()\n\n    yaml_path_obj = Path(yaml_path)\n    config_dir = yaml_path_obj.parent.absolute()\n\n    import importlib.util\n    import os\n    import sys\n\n    def load_transforms_module(path):\n        if os.path.exists(path):\n            try:\n                spec = importlib.util.spec_from_file_location(\"transforms_autodiscovered\", path)\n                if spec and spec.loader:\n                    module = importlib.util.module_from_spec(spec)\n                    sys.modules[\"transforms_autodiscovered\"] = module\n                    spec.loader.exec_module(module)\n                    logger.info(f\"Auto-loaded transforms from: {path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to auto-load transforms from {path}: {e}\")\n\n    load_transforms_module(os.path.join(config_dir, \"transforms.py\"))\n\n    cwd = os.getcwd()\n    if os.path.abspath(cwd) != str(config_dir):\n        load_transforms_module(os.path.join(cwd, \"transforms.py\"))\n\n    try:\n        config = load_yaml_with_env(str(yaml_path_obj), env=env)\n        logger.debug(\"Configuration loaded successfully\")\n    except FileNotFoundError:\n        logger.error(f\"YAML file not found: {yaml_path}\")\n        raise FileNotFoundError(f\"YAML file not found: {yaml_path}\")\n\n    project_config = ProjectConfig(**config)\n    logger.debug(\n        \"Project config validated\",\n        project=project_config.project,\n        pipelines=len(project_config.pipelines),\n    )\n\n    connections = cls._build_connections(project_config.connections)\n\n    return cls(\n        project_config=project_config,\n        connections=connections,\n    )\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_all_state","title":"<code>get_all_state(prefix=None)</code>","text":"<p>Get all state entries, optionally filtered by key prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Optional[str]</code> <p>Optional key prefix to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with state entries</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_all_state(self, prefix: Optional[str] = None) -&gt; \"pd.DataFrame\":\n    \"\"\"Get all state entries, optionally filtered by key prefix.\n\n    Args:\n        prefix: Optional key prefix to filter by\n\n    Returns:\n        DataFrame with state entries\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n        if not df.empty and prefix and \"key\" in df.columns:\n            df = df[df[\"key\"].str.startswith(prefix)]\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get state: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_lineage","title":"<code>get_lineage(table, direction='both')</code>","text":"<p>Get lineage for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table identifier (supports smart path resolution)</p> required <code>direction</code> <code>str</code> <p>\"upstream\", \"downstream\", or \"both\"</p> <code>'both'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with lineage relationships</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_lineage(\n    self,\n    table: str,\n    direction: str = \"both\",\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Get lineage for a table.\n\n    Args:\n        table: Table identifier (supports smart path resolution)\n        direction: \"upstream\", \"downstream\", or \"both\"\n\n    Returns:\n        DataFrame with lineage relationships\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        resolved_path = self._resolve_table_path(table)\n\n        results = []\n        if direction in (\"upstream\", \"both\"):\n            upstream = self.catalog_manager.get_upstream(resolved_path)\n            for r in upstream:\n                r[\"direction\"] = \"upstream\"\n            results.extend(upstream)\n\n        if direction in (\"downstream\", \"both\"):\n            downstream = self.catalog_manager.get_downstream(resolved_path)\n            for r in downstream:\n                r[\"direction\"] = \"downstream\"\n            results.extend(downstream)\n\n        return pd.DataFrame(results)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get lineage: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_node_stats","title":"<code>get_node_stats(node, days=7)</code>","text":"<p>Get average duration, row counts, success rate over period.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Node name</p> required <code>days</code> <code>int</code> <p>Number of days to look back</p> <code>7</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with node statistics</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n    \"\"\"Get average duration, row counts, success rate over period.\n\n    Args:\n        node: Node name\n        days: Number of days to look back\n\n    Returns:\n        Dict with node statistics\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return {}\n\n    try:\n        avg_duration = self.catalog_manager.get_average_duration(node, days)\n\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n        if df.empty:\n            return {\"node\": node, \"runs\": 0}\n\n        if \"timestamp\" in df.columns:\n            cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n            if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n            if df[\"timestamp\"].dt.tz is None:\n                df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"UTC\")\n            df = df[df[\"timestamp\"] &gt;= cutoff]\n\n        node_runs = df[df[\"node_name\"] == node]\n        if node_runs.empty:\n            return {\"node\": node, \"runs\": 0}\n\n        total = len(node_runs)\n        success = len(node_runs[node_runs[\"status\"] == \"SUCCESS\"])\n        avg_rows = node_runs[\"rows_processed\"].mean() if \"rows_processed\" in node_runs else None\n\n        return {\n            \"node\": node,\n            \"runs\": total,\n            \"success_rate\": success / total if total &gt; 0 else 0,\n            \"avg_duration_s\": avg_duration,\n            \"avg_rows\": avg_rows,\n            \"period_days\": days,\n        }\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get node stats: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_pipeline","title":"<code>get_pipeline(name)</code>","text":"<p>Get a specific pipeline instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Pipeline instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pipeline not found</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_pipeline(self, name: str) -&gt; Pipeline:\n    \"\"\"Get a specific pipeline instance.\n\n    Args:\n        name: Pipeline name\n\n    Returns:\n        Pipeline instance\n\n    Raises:\n        ValueError: If pipeline not found\n    \"\"\"\n    if name not in self._pipelines:\n        available = \", \".join(self._pipelines.keys())\n        raise ValueError(f\"Pipeline '{name}' not found. Available: {available}\")\n    return self._pipelines[name]\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_pipeline_status","title":"<code>get_pipeline_status(pipeline)</code>","text":"<p>Get last run status, duration, timestamp for a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>Pipeline name</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with status info</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n    \"\"\"Get last run status, duration, timestamp for a pipeline.\n\n    Args:\n        pipeline: Pipeline name\n\n    Returns:\n        Dict with status info\n    \"\"\"\n    if not self.catalog_manager:\n        return {}\n\n    try:\n        runs = self.list_runs(pipeline=pipeline, limit=1)\n        if runs.empty:\n            return {\"status\": \"never_run\", \"pipeline\": pipeline}\n\n        last_run = runs.iloc[0].to_dict()\n        return {\n            \"pipeline\": pipeline,\n            \"last_status\": last_run.get(\"status\"),\n            \"last_run_at\": last_run.get(\"timestamp\"),\n            \"last_duration_ms\": last_run.get(\"duration_ms\"),\n            \"last_node\": last_run.get(\"node_name\"),\n        }\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get pipeline status: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_schema_history","title":"<code>get_schema_history(table, limit=5)</code>","text":"<p>Get schema version history for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table identifier (supports smart path resolution)</p> required <code>limit</code> <code>int</code> <p>Maximum number of versions to return</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with schema history</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_schema_history(\n    self,\n    table: str,\n    limit: int = 5,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Get schema version history for a table.\n\n    Args:\n        table: Table identifier (supports smart path resolution)\n        limit: Maximum number of versions to return\n\n    Returns:\n        DataFrame with schema history\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        resolved_path = self._resolve_table_path(table)\n        history = self.catalog_manager.get_schema_history(resolved_path, limit)\n        return pd.DataFrame(history)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get schema history: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_state","title":"<code>get_state(key)</code>","text":"<p>Get a specific state entry (HWM, content hash, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The state key to look up</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary with state data or None if not found</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get a specific state entry (HWM, content hash, etc.).\n\n    Args:\n        key: The state key to look up\n\n    Returns:\n        Dictionary with state data or None if not found\n    \"\"\"\n\n    if not self.catalog_manager:\n        return None\n\n    try:\n        df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n        if df.empty or \"key\" not in df.columns:\n            return None\n\n        row = df[df[\"key\"] == key]\n        if row.empty:\n            return None\n\n        return row.iloc[0].to_dict()\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_pipelines","title":"<code>list_pipelines()</code>","text":"<p>Get list of available pipeline names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of pipeline names</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_pipelines(self) -&gt; List[str]:\n    \"\"\"Get list of available pipeline names.\n\n    Returns:\n        List of pipeline names\n    \"\"\"\n    return list(self._pipelines.keys())\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_registered_nodes","title":"<code>list_registered_nodes(pipeline=None)</code>","text":"<p>List nodes from the system catalog.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Optional[str]</code> <p>Optional pipeline name to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with node metadata from meta_nodes</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_registered_nodes(self, pipeline: Optional[str] = None) -&gt; \"pd.DataFrame\":\n    \"\"\"List nodes from the system catalog.\n\n    Args:\n        pipeline: Optional pipeline name to filter by\n\n    Returns:\n        DataFrame with node metadata from meta_nodes\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_nodes\"])\n        if not df.empty and pipeline:\n            df = df[df[\"pipeline_name\"] == pipeline]\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list nodes: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_registered_pipelines","title":"<code>list_registered_pipelines()</code>","text":"<p>List all registered pipelines from the system catalog.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with pipeline metadata from meta_pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_registered_pipelines(self) -&gt; \"pd.DataFrame\":\n    \"\"\"List all registered pipelines from the system catalog.\n\n    Returns:\n        DataFrame with pipeline metadata from meta_pipelines\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(\n            self.catalog_manager.tables[\"meta_pipelines\"]\n        )\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list pipelines: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_runs","title":"<code>list_runs(pipeline=None, node=None, status=None, limit=10)</code>","text":"<p>List recent runs with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Optional[str]</code> <p>Optional pipeline name to filter by</p> <code>None</code> <code>node</code> <code>Optional[str]</code> <p>Optional node name to filter by</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Optional status to filter by (SUCCESS, FAILURE)</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with run history from meta_runs</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_runs(\n    self,\n    pipeline: Optional[str] = None,\n    node: Optional[str] = None,\n    status: Optional[str] = None,\n    limit: int = 10,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"List recent runs with optional filters.\n\n    Args:\n        pipeline: Optional pipeline name to filter by\n        node: Optional node name to filter by\n        status: Optional status to filter by (SUCCESS, FAILURE)\n        limit: Maximum number of runs to return\n\n    Returns:\n        DataFrame with run history from meta_runs\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n        if df.empty:\n            return df\n\n        if pipeline:\n            df = df[df[\"pipeline_name\"] == pipeline]\n        if node:\n            df = df[df[\"node_name\"] == node]\n        if status:\n            df = df[df[\"status\"] == status]\n\n        if \"timestamp\" in df.columns:\n            df = df.sort_values(\"timestamp\", ascending=False)\n\n        return df.head(limit)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list runs: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_tables","title":"<code>list_tables()</code>","text":"<p>List registered assets from meta_tables.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with table/asset metadata</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_tables(self) -&gt; \"pd.DataFrame\":\n    \"\"\"List registered assets from meta_tables.\n\n    Returns:\n        DataFrame with table/asset metadata\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list tables: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.run","title":"<code>run(pipelines=None, dry_run=False, resume_from_failure=False, parallel=False, max_workers=4, on_error=None, tag=None, node=None, console=False)</code>","text":"<p>Run one, multiple, or all pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Pipeline name(s) to run.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>Whether to simulate execution.</p> <code>False</code> <code>resume_from_failure</code> <code>bool</code> <p>Whether to skip successfully completed nodes from last run.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run nodes in parallel.</p> <code>False</code> <code>max_workers</code> <code>int</code> <p>Maximum number of worker threads for parallel execution.</p> <code>4</code> <code>on_error</code> <code>Optional[str]</code> <p>Override error handling strategy (fail_fast, fail_later, ignore).</p> <code>None</code> <code>tag</code> <code>Optional[str]</code> <p>Filter nodes by tag (only nodes with this tag will run).</p> <code>None</code> <code>node</code> <code>Optional[str]</code> <p>Run only the specific node by name.</p> <code>None</code> <code>console</code> <code>bool</code> <p>Whether to show rich console output with progress.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[PipelineResults, Dict[str, PipelineResults]]</code> <p>PipelineResults or Dict of results</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def run(\n    self,\n    pipelines: Optional[Union[str, List[str]]] = None,\n    dry_run: bool = False,\n    resume_from_failure: bool = False,\n    parallel: bool = False,\n    max_workers: int = 4,\n    on_error: Optional[str] = None,\n    tag: Optional[str] = None,\n    node: Optional[str] = None,\n    console: bool = False,\n) -&gt; Union[PipelineResults, Dict[str, PipelineResults]]:\n    \"\"\"Run one, multiple, or all pipelines.\n\n    Args:\n        pipelines: Pipeline name(s) to run.\n        dry_run: Whether to simulate execution.\n        resume_from_failure: Whether to skip successfully completed nodes from last run.\n        parallel: Whether to run nodes in parallel.\n        max_workers: Maximum number of worker threads for parallel execution.\n        on_error: Override error handling strategy (fail_fast, fail_later, ignore).\n        tag: Filter nodes by tag (only nodes with this tag will run).\n        node: Run only the specific node by name.\n        console: Whether to show rich console output with progress.\n\n    Returns:\n        PipelineResults or Dict of results\n    \"\"\"\n    if pipelines is None:\n        pipeline_names = list(self._pipelines.keys())\n    elif isinstance(pipelines, str):\n        pipeline_names = [pipelines]\n    else:\n        pipeline_names = pipelines\n\n    for name in pipeline_names:\n        if name not in self._pipelines:\n            available = \", \".join(self._pipelines.keys())\n            self._ctx.error(\n                f\"Pipeline not found: {name}\",\n                available=list(self._pipelines.keys()),\n            )\n            raise ValueError(f\"Pipeline '{name}' not found. Available pipelines: {available}\")\n\n    # Phase 2: Auto-register pipelines and nodes before execution\n    if self.catalog_manager:\n        self._auto_register_pipelines(pipeline_names)\n\n    self._ctx.info(\n        f\"Running {len(pipeline_names)} pipeline(s)\",\n        pipelines=pipeline_names,\n        dry_run=dry_run,\n        parallel=parallel,\n    )\n\n    results = {}\n    for idx, name in enumerate(pipeline_names):\n        # Invalidate cache before each pipeline so it sees latest outputs\n        if self.catalog_manager:\n            self.catalog_manager.invalidate_cache()\n\n        self._ctx.info(\n            f\"Executing pipeline {idx + 1}/{len(pipeline_names)}: {name}\",\n            pipeline=name,\n            order=idx + 1,\n        )\n\n        results[name] = self._pipelines[name].run(\n            dry_run=dry_run,\n            resume_from_failure=resume_from_failure,\n            parallel=parallel,\n            max_workers=max_workers,\n            on_error=on_error,\n            tag=tag,\n            node=node,\n            console=console,\n        )\n\n        result = results[name]\n        status = \"SUCCESS\" if not result.failed else \"FAILED\"\n        self._ctx.info(\n            f\"Pipeline {status}: {name}\",\n            status=status,\n            duration_s=round(result.duration, 2),\n            completed=len(result.completed),\n            failed=len(result.failed),\n        )\n\n        if result.story_path:\n            self._ctx.debug(f\"Story generated: {result.story_path}\")\n\n    if len(pipeline_names) == 1:\n        return results[pipeline_names[0]]\n    else:\n        return results\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults","title":"<code>PipelineResults</code>  <code>dataclass</code>","text":"<p>Results from pipeline execution.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>@dataclass\nclass PipelineResults:\n    \"\"\"Results from pipeline execution.\"\"\"\n\n    pipeline_name: str\n    completed: List[str] = field(default_factory=list)\n    failed: List[str] = field(default_factory=list)\n    skipped: List[str] = field(default_factory=list)\n    node_results: Dict[str, NodeResult] = field(default_factory=dict)\n    duration: float = 0.0\n    start_time: Optional[str] = None\n    end_time: Optional[str] = None\n    story_path: Optional[str] = None\n\n    def get_node_result(self, name: str) -&gt; Optional[NodeResult]:\n        \"\"\"Get result for specific node.\n\n        Args:\n            name: Node name\n\n        Returns:\n            NodeResult if available, None otherwise\n        \"\"\"\n        return self.node_results.get(name)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary.\n\n        Returns:\n            Dictionary representation\n        \"\"\"\n        return {\n            \"pipeline_name\": self.pipeline_name,\n            \"completed\": self.completed,\n            \"failed\": self.failed,\n            \"skipped\": self.skipped,\n            \"duration\": self.duration,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"node_count\": len(self.node_results),\n        }\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.get_node_result","title":"<code>get_node_result(name)</code>","text":"<p>Get result for specific node.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Node name</p> required <p>Returns:</p> Type Description <code>Optional[NodeResult]</code> <p>NodeResult if available, None otherwise</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_node_result(self, name: str) -&gt; Optional[NodeResult]:\n    \"\"\"Get result for specific node.\n\n    Args:\n        name: Node name\n\n    Returns:\n        NodeResult if available, None otherwise\n    \"\"\"\n    return self.node_results.get(name)\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"pipeline_name\": self.pipeline_name,\n        \"completed\": self.completed,\n        \"failed\": self.failed,\n        \"skipped\": self.skipped,\n        \"duration\": self.duration,\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"node_count\": len(self.node_results),\n    }\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.create_context","title":"<code>create_context(engine, spark_session=None)</code>","text":"<p>Factory function to create appropriate context.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>str</code> <p>Engine type ('pandas' or 'spark')</p> required <code>spark_session</code> <code>Optional[Any]</code> <p>SparkSession (required if engine='spark')</p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context instance for the specified engine</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If engine is invalid or SparkSession missing for Spark</p> Source code in <code>odibi\\context.py</code> <pre><code>def create_context(engine: str, spark_session: Optional[Any] = None) -&gt; Context:\n    \"\"\"Factory function to create appropriate context.\n\n    Args:\n        engine: Engine type ('pandas' or 'spark')\n        spark_session: SparkSession (required if engine='spark')\n\n    Returns:\n        Context instance for the specified engine\n\n    Raises:\n        ValueError: If engine is invalid or SparkSession missing for Spark\n    \"\"\"\n    if engine == \"pandas\":\n        return PandasContext()\n    elif engine == \"spark\":\n        if spark_session is None:\n            raise ValueError(\"SparkSession required for Spark engine\")\n        return SparkContext(spark_session)\n    elif engine == \"polars\":\n        return PolarsContext()\n    else:\n        raise ValueError(f\"Unsupported engine: {engine}. Use 'pandas' or 'spark'\")\n</code></pre>"},{"location":"reference/api/validation/","title":"Validation API","text":""},{"location":"reference/api/validation/#odibi.validation.engine","title":"<code>odibi.validation.engine</code>","text":"<p>Optimized validation engine for executing declarative data quality tests.</p> <p>Performance optimizations: - Fail-fast mode for early exit on first failure - DataFrame caching for Spark with many tests - Lazy evaluation for Polars (avoids early .collect()) - Batched null count aggregation (single scan for NOT_NULL) - Vectorized operations (no Python loops over rows) - Memory-efficient mask operations (no full DataFrame copies)</p>"},{"location":"reference/api/validation/#odibi.validation.engine.Validator","title":"<code>Validator</code>","text":"<p>Validation engine for executing declarative data quality tests. Supports Spark, Pandas, and Polars engines with performance optimizations.</p> Source code in <code>odibi\\validation\\engine.py</code> <pre><code>class Validator:\n    \"\"\"\n    Validation engine for executing declarative data quality tests.\n    Supports Spark, Pandas, and Polars engines with performance optimizations.\n    \"\"\"\n\n    def validate(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Run validation checks against a DataFrame.\n\n        Args:\n            df: Spark, Pandas, or Polars DataFrame\n            config: Validation configuration\n            context: Optional context (e.g. {'columns': ...}) for contracts\n\n        Returns:\n            List of error messages (empty if all checks pass)\n        \"\"\"\n        ctx = get_logging_context()\n        test_count = len(config.tests)\n        failures = []\n        is_spark = False\n        is_polars = False\n        engine_type = \"pandas\"\n\n        try:\n            import pyspark\n\n            if isinstance(df, pyspark.sql.DataFrame):\n                is_spark = True\n                engine_type = \"spark\"\n        except ImportError:\n            pass\n\n        if not is_spark:\n            try:\n                import polars as pl\n\n                if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n                    is_polars = True\n                    engine_type = \"polars\"\n            except ImportError:\n                pass\n\n        ctx.debug(\n            \"Starting validation\",\n            test_count=test_count,\n            engine=engine_type,\n            df_type=type(df).__name__,\n            fail_fast=getattr(config, \"fail_fast\", False),\n        )\n\n        if is_spark:\n            failures = self._validate_spark(df, config, context)\n        elif is_polars:\n            failures = self._validate_polars(df, config, context)\n        else:\n            failures = self._validate_pandas(df, config, context)\n\n        tests_passed = test_count - len(failures)\n        ctx.info(\n            \"Validation complete\",\n            total_tests=test_count,\n            tests_passed=tests_passed,\n            tests_failed=len(failures),\n            engine=engine_type,\n        )\n\n        ctx.log_validation_result(\n            passed=len(failures) == 0,\n            rule_name=\"batch_validation\",\n            failures=failures[:5] if failures else None,\n            total_tests=test_count,\n            tests_passed=tests_passed,\n            tests_failed=len(failures),\n        )\n\n        return failures\n\n    def _handle_failure(self, message: str, test: Any) -&gt; Optional[str]:\n        \"\"\"Handle failure based on severity.\"\"\"\n        ctx = get_logging_context()\n        severity = getattr(test, \"on_fail\", ContractSeverity.FAIL)\n        test_type = getattr(test, \"type\", \"unknown\")\n\n        if severity == ContractSeverity.WARN:\n            ctx.warning(\n                f\"Validation Warning: {message}\",\n                test_type=str(test_type),\n                severity=\"warn\",\n            )\n            return None\n\n        ctx.error(\n            f\"Validation Failed: {message}\",\n            test_type=str(test_type),\n            severity=\"fail\",\n            test_config=str(test),\n        )\n        return message\n\n    def _validate_polars(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Execute checks using Polars with lazy evaluation where possible.\n\n        Optimization: Avoids collecting full LazyFrame. Uses lazy aggregations\n        and only collects scalar results.\n        \"\"\"\n        import polars as pl\n\n        ctx = get_logging_context()\n        fail_fast = getattr(config, \"fail_fast\", False)\n        is_lazy = isinstance(df, pl.LazyFrame)\n\n        if is_lazy:\n            row_count = df.select(pl.len()).collect().item()\n            columns = df.collect_schema().names()\n        else:\n            row_count = len(df)\n            columns = df.columns\n\n        ctx.debug(\"Validating Polars DataFrame\", row_count=row_count, is_lazy=is_lazy)\n\n        failures = []\n\n        for test in config.tests:\n            msg = None\n            test_type = getattr(test, \"type\", \"unknown\")\n            ctx.debug(\"Executing test\", test_type=str(test_type))\n\n            if test.type == TestType.SCHEMA:\n                if context and \"columns\" in context:\n                    expected = set(context[\"columns\"].keys())\n                    actual = set(columns)\n                    if getattr(test, \"strict\", True):\n                        if actual != expected:\n                            msg = f\"Schema mismatch. Expected {expected}, got {actual}\"\n                    else:\n                        missing = expected - actual\n                        if missing:\n                            msg = f\"Schema mismatch. Missing columns: {missing}\"\n\n            elif test.type == TestType.ROW_COUNT:\n                if test.min is not None and row_count &lt; test.min:\n                    msg = f\"Row count {row_count} &lt; min {test.min}\"\n                elif test.max is not None and row_count &gt; test.max:\n                    msg = f\"Row count {row_count} &gt; max {test.max}\"\n\n            elif test.type == TestType.FRESHNESS:\n                col = getattr(test, \"column\", \"updated_at\")\n                if col in columns:\n                    if is_lazy:\n                        max_ts = df.select(pl.col(col).max()).collect().item()\n                    else:\n                        max_ts = df[col].max()\n                    if max_ts:\n                        from datetime import datetime, timedelta, timezone\n\n                        duration_str = test.max_age\n                        delta = None\n                        if duration_str.endswith(\"h\"):\n                            delta = timedelta(hours=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"d\"):\n                            delta = timedelta(days=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"m\"):\n                            delta = timedelta(minutes=int(duration_str[:-1]))\n\n                        if delta:\n                            if datetime.now(timezone.utc) - max_ts &gt; delta:\n                                msg = (\n                                    f\"Data too old. Max timestamp {max_ts} \"\n                                    f\"is older than {test.max_age}\"\n                                )\n                else:\n                    msg = f\"Freshness check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.NOT_NULL:\n                for col in test.columns:\n                    if col in columns:\n                        if is_lazy:\n                            null_count = df.select(pl.col(col).is_null().sum()).collect().item()\n                        else:\n                            null_count = df[col].null_count()\n                        if null_count &gt; 0:\n                            col_msg = f\"Column '{col}' contains {null_count} NULLs\"\n                            ctx.debug(\n                                \"NOT_NULL check failed\",\n                                column=col,\n                                null_count=null_count,\n                                row_count=row_count,\n                            )\n                            res = self._handle_failure(col_msg, test)\n                            if res:\n                                failures.append(res)\n                                if fail_fast:\n                                    return [f for f in failures if f]\n                continue\n\n            elif test.type == TestType.UNIQUE:\n                cols = [c for c in test.columns if c in columns]\n                if len(cols) != len(test.columns):\n                    msg = f\"Unique check failed: Columns {set(test.columns) - set(cols)} not found\"\n                else:\n                    if is_lazy:\n                        dup_count = (\n                            df.group_by(cols)\n                            .agg(pl.len().alias(\"cnt\"))\n                            .filter(pl.col(\"cnt\") &gt; 1)\n                            .select(pl.len())\n                            .collect()\n                            .item()\n                        )\n                    else:\n                        dup_count = (\n                            df.group_by(cols)\n                            .agg(pl.len().alias(\"cnt\"))\n                            .filter(pl.col(\"cnt\") &gt; 1)\n                            .height\n                        )\n                    if dup_count &gt; 0:\n                        msg = f\"Column '{', '.join(cols)}' is not unique\"\n                        ctx.debug(\n                            \"UNIQUE check failed\",\n                            columns=cols,\n                            duplicate_groups=dup_count,\n                        )\n\n            elif test.type == TestType.ACCEPTED_VALUES:\n                col = test.column\n                if col in columns:\n                    if is_lazy:\n                        invalid_count = (\n                            df.filter(~pl.col(col).is_in(test.values))\n                            .select(pl.len())\n                            .collect()\n                            .item()\n                        )\n                    else:\n                        invalid_count = df.filter(~pl.col(col).is_in(test.values)).height\n                    if invalid_count &gt; 0:\n                        if is_lazy:\n                            examples = (\n                                df.filter(~pl.col(col).is_in(test.values))\n                                .select(pl.col(col))\n                                .limit(3)\n                                .collect()[col]\n                                .to_list()\n                            )\n                        else:\n                            invalid_rows = df.filter(~pl.col(col).is_in(test.values))\n                            examples = invalid_rows[col].head(3).to_list()\n                        msg = f\"Column '{col}' contains invalid values. Found: {examples}\"\n                        ctx.debug(\n                            \"ACCEPTED_VALUES check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            examples=examples,\n                        )\n                else:\n                    msg = f\"Accepted values check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.RANGE:\n                col = test.column\n                if col in columns:\n                    cond = pl.lit(False)\n                    if test.min is not None:\n                        cond = cond | (pl.col(col) &lt; test.min)\n                    if test.max is not None:\n                        cond = cond | (pl.col(col) &gt; test.max)\n                    if is_lazy:\n                        invalid_count = df.filter(cond).select(pl.len()).collect().item()\n                    else:\n                        invalid_count = df.filter(cond).height\n                    if invalid_count &gt; 0:\n                        msg = f\"Column '{col}' contains {invalid_count} values out of range\"\n                        ctx.debug(\n                            \"RANGE check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            min=test.min,\n                            max=test.max,\n                        )\n                else:\n                    msg = f\"Range check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.REGEX_MATCH:\n                col = test.column\n                if col in columns:\n                    regex_cond = pl.col(col).is_not_null() &amp; ~pl.col(col).str.contains(test.pattern)\n                    if is_lazy:\n                        invalid_count = df.filter(regex_cond).select(pl.len()).collect().item()\n                    else:\n                        invalid_count = df.filter(regex_cond).height\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Column '{col}' contains {invalid_count} values \"\n                            f\"that does not match pattern '{test.pattern}'\"\n                        )\n                        ctx.debug(\n                            \"REGEX_MATCH check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            pattern=test.pattern,\n                        )\n                else:\n                    msg = f\"Regex check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.CUSTOM_SQL:\n                ctx.warning(\n                    \"CUSTOM_SQL not fully supported in Polars; skipping\",\n                    test_name=getattr(test, \"name\", \"custom_sql\"),\n                )\n                continue\n\n            if msg:\n                res = self._handle_failure(msg, test)\n                if res:\n                    failures.append(res)\n                    if fail_fast:\n                        break\n\n        return [f for f in failures if f]\n\n    def _validate_spark(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Execute checks using Spark SQL with optimizations.\n\n        Optimizations:\n        - Optional DataFrame caching when cache_df=True\n        - Batched null count aggregation (single scan for all NOT_NULL columns)\n        - Fail-fast mode to skip remaining tests\n        - Reuses row_count instead of re-counting\n        \"\"\"\n        from pyspark.sql import functions as F\n\n        ctx = get_logging_context()\n        failures = []\n        fail_fast = getattr(config, \"fail_fast\", False)\n        cache_df = getattr(config, \"cache_df\", False)\n\n        df_work = df\n        if cache_df:\n            df_work = df.cache()\n            ctx.debug(\"DataFrame cached for validation\")\n\n        row_count = df_work.count()\n        ctx.debug(\"Validating Spark DataFrame\", row_count=row_count)\n\n        for test in config.tests:\n            msg = None\n            test_type = getattr(test, \"type\", \"unknown\")\n            ctx.debug(\"Executing test\", test_type=str(test_type))\n\n            if test.type == TestType.ROW_COUNT:\n                if test.min is not None and row_count &lt; test.min:\n                    msg = f\"Row count {row_count} &lt; min {test.min}\"\n                elif test.max is not None and row_count &gt; test.max:\n                    msg = f\"Row count {row_count} &gt; max {test.max}\"\n\n            elif test.type == TestType.SCHEMA:\n                if context and \"columns\" in context:\n                    expected = set(context[\"columns\"].keys())\n                    actual = set(df_work.columns)\n                    if getattr(test, \"strict\", True):\n                        if actual != expected:\n                            msg = f\"Schema mismatch. Expected {expected}, got {actual}\"\n                    else:\n                        missing = expected - actual\n                        if missing:\n                            msg = f\"Schema mismatch. Missing columns: {missing}\"\n\n            elif test.type == TestType.FRESHNESS:\n                col = getattr(test, \"column\", \"updated_at\")\n                if col in df_work.columns:\n                    max_ts = df_work.agg(F.max(col)).collect()[0][0]\n                    if max_ts:\n                        from datetime import datetime, timedelta, timezone\n\n                        duration_str = test.max_age\n                        delta = None\n                        if duration_str.endswith(\"h\"):\n                            delta = timedelta(hours=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"d\"):\n                            delta = timedelta(days=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"m\"):\n                            delta = timedelta(minutes=int(duration_str[:-1]))\n\n                        if delta and (datetime.now(timezone.utc) - max_ts &gt; delta):\n                            msg = (\n                                f\"Data too old. Max timestamp {max_ts} is older than {test.max_age}\"\n                            )\n                else:\n                    msg = f\"Freshness check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.NOT_NULL:\n                valid_cols = [c for c in test.columns if c in df_work.columns]\n                if valid_cols:\n                    null_aggs = [\n                        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n                        for c in valid_cols\n                    ]\n                    null_counts = df_work.agg(*null_aggs).collect()[0].asDict()\n                    for col in valid_cols:\n                        null_count = null_counts.get(col, 0) or 0\n                        if null_count &gt; 0:\n                            col_msg = f\"Column '{col}' contains {null_count} NULLs\"\n                            ctx.debug(\n                                \"NOT_NULL check failed\",\n                                column=col,\n                                null_count=null_count,\n                                row_count=row_count,\n                            )\n                            res = self._handle_failure(col_msg, test)\n                            if res:\n                                failures.append(res)\n                                if fail_fast:\n                                    if cache_df:\n                                        df_work.unpersist()\n                                    return failures\n                continue\n\n            elif test.type == TestType.UNIQUE:\n                cols = [c for c in test.columns if c in df_work.columns]\n                if len(cols) != len(test.columns):\n                    msg = f\"Unique check failed: Columns {set(test.columns) - set(cols)} not found\"\n                else:\n                    dup_count = df_work.groupBy(*cols).count().filter(\"count &gt; 1\").count()\n                    if dup_count &gt; 0:\n                        msg = f\"Column '{', '.join(cols)}' is not unique\"\n                        ctx.debug(\n                            \"UNIQUE check failed\",\n                            columns=cols,\n                            duplicate_groups=dup_count,\n                        )\n\n            elif test.type == TestType.ACCEPTED_VALUES:\n                col = test.column\n                if col in df_work.columns:\n                    invalid_df = df_work.filter(~F.col(col).isin(test.values))\n                    invalid_count = invalid_df.count()\n                    if invalid_count &gt; 0:\n                        examples_rows = invalid_df.select(col).limit(3).collect()\n                        examples = [r[0] for r in examples_rows]\n                        msg = f\"Column '{col}' contains invalid values. Found: {examples}\"\n                        ctx.debug(\n                            \"ACCEPTED_VALUES check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            examples=examples,\n                        )\n                else:\n                    msg = f\"Accepted values check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.RANGE:\n                col = test.column\n                if col in df_work.columns:\n                    cond = F.lit(False)\n                    if test.min is not None:\n                        cond = cond | (F.col(col) &lt; test.min)\n                    if test.max is not None:\n                        cond = cond | (F.col(col) &gt; test.max)\n\n                    invalid_count = df_work.filter(cond).count()\n                    if invalid_count &gt; 0:\n                        msg = f\"Column '{col}' contains {invalid_count} values out of range\"\n                        ctx.debug(\n                            \"RANGE check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            min=test.min,\n                            max=test.max,\n                        )\n                else:\n                    msg = f\"Range check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.REGEX_MATCH:\n                col = test.column\n                if col in df_work.columns:\n                    invalid_count = df_work.filter(\n                        F.col(col).isNotNull() &amp; ~F.col(col).rlike(test.pattern)\n                    ).count()\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Column '{col}' contains {invalid_count} values \"\n                            f\"that does not match pattern '{test.pattern}'\"\n                        )\n                        ctx.debug(\n                            \"REGEX_MATCH check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            pattern=test.pattern,\n                        )\n                else:\n                    msg = f\"Regex check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.CUSTOM_SQL:\n                try:\n                    invalid_count = df_work.filter(f\"NOT ({test.condition})\").count()\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Custom check '{getattr(test, 'name', 'custom_sql')}' failed. \"\n                            f\"Found {invalid_count} invalid rows.\"\n                        )\n                        ctx.debug(\n                            \"CUSTOM_SQL check failed\",\n                            condition=test.condition,\n                            invalid_count=invalid_count,\n                        )\n                except Exception as e:\n                    msg = f\"Failed to execute custom SQL '{test.condition}': {e}\"\n                    ctx.error(\n                        \"CUSTOM_SQL execution error\",\n                        condition=test.condition,\n                        error=str(e),\n                    )\n\n            if msg:\n                res = self._handle_failure(msg, test)\n                if res:\n                    failures.append(res)\n                    if fail_fast:\n                        break\n\n        if cache_df:\n            df_work.unpersist()\n\n        return failures\n\n    def _validate_pandas(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Execute checks using Pandas with optimizations.\n\n        Optimizations:\n        - Single pass for UNIQUE (no double .duplicated() call)\n        - Mask-based operations (no full DataFrame copies for invalid rows)\n        - Memory-efficient example extraction\n        - Fail-fast mode support\n        \"\"\"\n        ctx = get_logging_context()\n        failures = []\n        row_count = len(df)\n        fail_fast = getattr(config, \"fail_fast\", False)\n\n        ctx.debug(\"Validating Pandas DataFrame\", row_count=row_count)\n\n        for test in config.tests:\n            msg = None\n            test_type = getattr(test, \"type\", \"unknown\")\n            ctx.debug(\"Executing test\", test_type=str(test_type))\n\n            if test.type == TestType.SCHEMA:\n                if context and \"columns\" in context:\n                    expected = set(context[\"columns\"].keys())\n                    actual = set(df.columns)\n                    if getattr(test, \"strict\", True):\n                        if actual != expected:\n                            msg = f\"Schema mismatch. Expected {expected}, got {actual}\"\n                    else:\n                        missing = expected - actual\n                        if missing:\n                            msg = f\"Schema mismatch. Missing columns: {missing}\"\n\n            elif test.type == TestType.FRESHNESS:\n                col = getattr(test, \"column\", \"updated_at\")\n                if col in df.columns:\n                    import pandas as pd\n\n                    if not pd.api.types.is_datetime64_any_dtype(df[col]):\n                        try:\n                            s = pd.to_datetime(df[col])\n                            max_ts = s.max()\n                        except Exception:\n                            max_ts = None\n                    else:\n                        max_ts = df[col].max()\n\n                    if max_ts is not pd.NaT:\n                        from datetime import datetime, timedelta, timezone\n\n                        duration_str = test.max_age\n                        delta = None\n                        if duration_str.endswith(\"h\"):\n                            delta = timedelta(hours=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"d\"):\n                            delta = timedelta(days=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"m\"):\n                            delta = timedelta(minutes=int(duration_str[:-1]))\n\n                        if delta and (datetime.now(timezone.utc) - max_ts &gt; delta):\n                            msg = (\n                                f\"Data too old. Max timestamp {max_ts} is older than {test.max_age}\"\n                            )\n                else:\n                    msg = f\"Freshness check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.ROW_COUNT:\n                if test.min is not None and row_count &lt; test.min:\n                    msg = f\"Row count {row_count} &lt; min {test.min}\"\n                elif test.max is not None and row_count &gt; test.max:\n                    msg = f\"Row count {row_count} &gt; max {test.max}\"\n\n            elif test.type == TestType.NOT_NULL:\n                for col in test.columns:\n                    if col in df.columns:\n                        null_count = int(df[col].isnull().sum())\n                        if null_count &gt; 0:\n                            col_msg = f\"Column '{col}' contains {null_count} NULLs\"\n                            ctx.debug(\n                                \"NOT_NULL check failed\",\n                                column=col,\n                                null_count=null_count,\n                                row_count=row_count,\n                            )\n                            res = self._handle_failure(col_msg, test)\n                            if res:\n                                failures.append(res)\n                                if fail_fast:\n                                    return [f for f in failures if f]\n                    else:\n                        col_msg = f\"Column '{col}' not found in DataFrame\"\n                        ctx.debug(\n                            \"NOT_NULL check failed - column missing\",\n                            column=col,\n                        )\n                        res = self._handle_failure(col_msg, test)\n                        if res:\n                            failures.append(res)\n                            if fail_fast:\n                                return [f for f in failures if f]\n                continue\n\n            elif test.type == TestType.UNIQUE:\n                cols = [c for c in test.columns if c in df.columns]\n                if len(cols) != len(test.columns):\n                    msg = f\"Unique check failed: Columns {set(test.columns) - set(cols)} not found\"\n                else:\n                    dups = df.duplicated(subset=cols)\n                    dup_count = int(dups.sum())\n                    if dup_count &gt; 0:\n                        msg = f\"Column '{', '.join(cols)}' is not unique\"\n                        ctx.debug(\n                            \"UNIQUE check failed\",\n                            columns=cols,\n                            duplicate_rows=dup_count,\n                        )\n\n            elif test.type == TestType.ACCEPTED_VALUES:\n                col = test.column\n                if col in df.columns:\n                    mask = ~df[col].isin(test.values)\n                    invalid_count = int(mask.sum())\n                    if invalid_count &gt; 0:\n                        examples = df.loc[mask, col].dropna().unique()[:3]\n                        msg = f\"Column '{col}' contains invalid values. Found: {list(examples)}\"\n                        ctx.debug(\n                            \"ACCEPTED_VALUES check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            examples=list(examples),\n                        )\n                else:\n                    msg = f\"Accepted values check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.RANGE:\n                col = test.column\n                if col in df.columns:\n                    invalid_count = 0\n                    if test.min is not None:\n                        invalid_count += int((df[col] &lt; test.min).sum())\n                    if test.max is not None:\n                        invalid_count += int((df[col] &gt; test.max).sum())\n\n                    if invalid_count &gt; 0:\n                        msg = f\"Column '{col}' contains {invalid_count} values out of range\"\n                        ctx.debug(\n                            \"RANGE check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            min=test.min,\n                            max=test.max,\n                        )\n                else:\n                    msg = f\"Range check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.REGEX_MATCH:\n                col = test.column\n                if col in df.columns:\n                    valid_series = df[col].dropna().astype(str)\n                    if not valid_series.empty:\n                        matches = valid_series.str.match(test.pattern)\n                        invalid_count = int((~matches).sum())\n                        if invalid_count &gt; 0:\n                            msg = (\n                                f\"Column '{col}' contains {invalid_count} values \"\n                                f\"that does not match pattern '{test.pattern}'\"\n                            )\n                            ctx.debug(\n                                \"REGEX_MATCH check failed\",\n                                column=col,\n                                invalid_count=invalid_count,\n                                pattern=test.pattern,\n                            )\n                else:\n                    msg = f\"Regex check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.CUSTOM_SQL:\n                try:\n                    mask = ~df.eval(test.condition)\n                    invalid_count = int(mask.sum())\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Custom check '{getattr(test, 'name', 'custom_sql')}' failed. \"\n                            f\"Found {invalid_count} invalid rows.\"\n                        )\n                        ctx.debug(\n                            \"CUSTOM_SQL check failed\",\n                            condition=test.condition,\n                            invalid_count=invalid_count,\n                        )\n                except Exception as e:\n                    msg = f\"Failed to execute custom SQL '{test.condition}': {e}\"\n                    ctx.error(\n                        \"CUSTOM_SQL execution error\",\n                        condition=test.condition,\n                        error=str(e),\n                    )\n\n            if msg:\n                res = self._handle_failure(msg, test)\n                if res:\n                    failures.append(res)\n                    if fail_fast:\n                        break\n\n        return [f for f in failures if f]\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.engine.Validator.validate","title":"<code>validate(df, config, context=None)</code>","text":"<p>Run validation checks against a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Spark, Pandas, or Polars DataFrame</p> required <code>config</code> <code>ValidationConfig</code> <p>Validation configuration</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Optional context (e.g. {'columns': ...}) for contracts</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of error messages (empty if all checks pass)</p> Source code in <code>odibi\\validation\\engine.py</code> <pre><code>def validate(\n    self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n) -&gt; List[str]:\n    \"\"\"\n    Run validation checks against a DataFrame.\n\n    Args:\n        df: Spark, Pandas, or Polars DataFrame\n        config: Validation configuration\n        context: Optional context (e.g. {'columns': ...}) for contracts\n\n    Returns:\n        List of error messages (empty if all checks pass)\n    \"\"\"\n    ctx = get_logging_context()\n    test_count = len(config.tests)\n    failures = []\n    is_spark = False\n    is_polars = False\n    engine_type = \"pandas\"\n\n    try:\n        import pyspark\n\n        if isinstance(df, pyspark.sql.DataFrame):\n            is_spark = True\n            engine_type = \"spark\"\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n                engine_type = \"polars\"\n        except ImportError:\n            pass\n\n    ctx.debug(\n        \"Starting validation\",\n        test_count=test_count,\n        engine=engine_type,\n        df_type=type(df).__name__,\n        fail_fast=getattr(config, \"fail_fast\", False),\n    )\n\n    if is_spark:\n        failures = self._validate_spark(df, config, context)\n    elif is_polars:\n        failures = self._validate_polars(df, config, context)\n    else:\n        failures = self._validate_pandas(df, config, context)\n\n    tests_passed = test_count - len(failures)\n    ctx.info(\n        \"Validation complete\",\n        total_tests=test_count,\n        tests_passed=tests_passed,\n        tests_failed=len(failures),\n        engine=engine_type,\n    )\n\n    ctx.log_validation_result(\n        passed=len(failures) == 0,\n        rule_name=\"batch_validation\",\n        failures=failures[:5] if failures else None,\n        total_tests=test_count,\n        tests_passed=tests_passed,\n        tests_failed=len(failures),\n    )\n\n    return failures\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.gate","title":"<code>odibi.validation.gate</code>","text":"<p>Quality Gate support for batch-level validation.</p> <p>Gates evaluate the entire batch before writing, ensuring data quality thresholds are met at the aggregate level.</p>"},{"location":"reference/api/validation/#odibi.validation.gate.GateResult","title":"<code>GateResult</code>  <code>dataclass</code>","text":"<p>Result of gate evaluation.</p> Source code in <code>odibi\\validation\\gate.py</code> <pre><code>@dataclass\nclass GateResult:\n    \"\"\"Result of gate evaluation.\"\"\"\n\n    passed: bool\n    pass_rate: float\n    total_rows: int\n    passed_rows: int\n    failed_rows: int\n    details: Dict[str, Any] = field(default_factory=dict)\n    action: GateOnFail = GateOnFail.ABORT\n    failure_reasons: List[str] = field(default_factory=list)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.gate.evaluate_gate","title":"<code>evaluate_gate(df, validation_results, gate_config, engine, catalog=None, node_name=None)</code>","text":"<p>Evaluate quality gate on validation results.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame being validated</p> required <code>validation_results</code> <code>Dict[str, List[bool]]</code> <p>Dict of test_name -&gt; per-row boolean results (True=passed)</p> required <code>gate_config</code> <code>GateConfig</code> <p>Gate configuration</p> required <code>engine</code> <code>Any</code> <p>Engine instance</p> required <code>catalog</code> <code>Optional[Any]</code> <p>Optional CatalogManager for historical row count checks</p> <code>None</code> <code>node_name</code> <code>Optional[str]</code> <p>Optional node name for historical lookups</p> <code>None</code> <p>Returns:</p> Type Description <code>GateResult</code> <p>GateResult with pass/fail status and action to take</p> Source code in <code>odibi\\validation\\gate.py</code> <pre><code>def evaluate_gate(\n    df: Any,\n    validation_results: Dict[str, List[bool]],\n    gate_config: GateConfig,\n    engine: Any,\n    catalog: Optional[Any] = None,\n    node_name: Optional[str] = None,\n) -&gt; GateResult:\n    \"\"\"\n    Evaluate quality gate on validation results.\n\n    Args:\n        df: DataFrame being validated\n        validation_results: Dict of test_name -&gt; per-row boolean results (True=passed)\n        gate_config: Gate configuration\n        engine: Engine instance\n        catalog: Optional CatalogManager for historical row count checks\n        node_name: Optional node name for historical lookups\n\n    Returns:\n        GateResult with pass/fail status and action to take\n    \"\"\"\n    is_spark = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if is_spark:\n        total_rows = df.count()\n    elif hasattr(engine, \"count_rows\"):\n        total_rows = engine.count_rows(df)\n    else:\n        total_rows = len(df)\n\n    if total_rows == 0:\n        return GateResult(\n            passed=True,\n            pass_rate=1.0,\n            total_rows=0,\n            passed_rows=0,\n            failed_rows=0,\n            action=gate_config.on_fail,\n            details={\"message\": \"Empty dataset - gate passed by default\"},\n        )\n\n    passed_rows = total_rows\n    if validation_results:\n        all_pass_mask = None\n        for test_name, results in validation_results.items():\n            if len(results) == total_rows:\n                if all_pass_mask is None:\n                    all_pass_mask = results.copy()\n                else:\n                    all_pass_mask = [a and b for a, b in zip(all_pass_mask, results)]\n\n        if all_pass_mask:\n            passed_rows = sum(all_pass_mask)\n\n    pass_rate = passed_rows / total_rows if total_rows &gt; 0 else 1.0\n    failed_rows = total_rows - passed_rows\n\n    details: Dict[str, Any] = {\n        \"overall_pass_rate\": pass_rate,\n        \"per_test_rates\": {},\n        \"row_count_check\": None,\n    }\n\n    gate_passed = True\n    failure_reasons: List[str] = []\n\n    if pass_rate &lt; gate_config.require_pass_rate:\n        gate_passed = False\n        failure_reasons.append(\n            f\"Overall pass rate {pass_rate:.1%} &lt; required {gate_config.require_pass_rate:.1%}\"\n        )\n\n    for threshold in gate_config.thresholds:\n        test_results = validation_results.get(threshold.test)\n        if test_results:\n            test_total = len(test_results)\n            test_passed = sum(test_results)\n            test_pass_rate = test_passed / test_total if test_total &gt; 0 else 1.0\n            details[\"per_test_rates\"][threshold.test] = test_pass_rate\n\n            if test_pass_rate &lt; threshold.min_pass_rate:\n                gate_passed = False\n                failure_reasons.append(\n                    f\"Test '{threshold.test}' pass rate {test_pass_rate:.1%} \"\n                    f\"&lt; required {threshold.min_pass_rate:.1%}\"\n                )\n\n    if gate_config.row_count:\n        row_check = _check_row_count(\n            total_rows,\n            gate_config.row_count,\n            catalog,\n            node_name,\n        )\n        details[\"row_count_check\"] = row_check\n\n        if not row_check[\"passed\"]:\n            gate_passed = False\n            failure_reasons.append(row_check[\"reason\"])\n\n    details[\"failure_reasons\"] = failure_reasons\n\n    if gate_passed:\n        logger.info(f\"Gate passed: {pass_rate:.1%} pass rate ({passed_rows}/{total_rows} rows)\")\n    else:\n        logger.warning(f\"Gate failed: {', '.join(failure_reasons)}\")\n\n    return GateResult(\n        passed=gate_passed,\n        pass_rate=pass_rate,\n        total_rows=total_rows,\n        passed_rows=passed_rows,\n        failed_rows=failed_rows,\n        details=details,\n        action=gate_config.on_fail,\n        failure_reasons=failure_reasons,\n    )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine","title":"<code>odibi.validation.quarantine</code>","text":"<p>Optimized quarantine table support for routing failed validation rows.</p> <p>Performance optimizations: - Removed per-row test_results lists (O(N*tests) memory savings) - Added sampling/limiting for large invalid sets - Single pass for combined mask evaluation - No unnecessary Python list conversions</p> <p>This module provides functionality to: 1. Split DataFrames into valid and invalid portions based on test results 2. Add metadata columns to quarantined rows 3. Write quarantined rows to a dedicated table (with optional sampling)</p>"},{"location":"reference/api/validation/#odibi.validation.quarantine.QuarantineResult","title":"<code>QuarantineResult</code>  <code>dataclass</code>","text":"<p>Result of quarantine operation.</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>@dataclass\nclass QuarantineResult:\n    \"\"\"Result of quarantine operation.\"\"\"\n\n    valid_df: Any\n    invalid_df: Any\n    rows_quarantined: int\n    rows_valid: int\n    test_results: Dict[str, Dict[str, int]] = field(default_factory=dict)\n    failed_test_details: Dict[int, List[str]] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.add_quarantine_metadata","title":"<code>add_quarantine_metadata(invalid_df, test_results, config, engine, node_name, run_id, tests)</code>","text":"<p>Add metadata columns to quarantined rows.</p> <p>Parameters:</p> Name Type Description Default <code>invalid_df</code> <code>Any</code> <p>DataFrame of invalid rows</p> required <code>test_results</code> <code>Dict[str, Any]</code> <p>Dict of test_name -&gt; aggregate results (not per-row)</p> required <code>config</code> <code>QuarantineColumnsConfig</code> <p>QuarantineColumnsConfig specifying which columns to add</p> required <code>engine</code> <code>Any</code> <p>Engine instance</p> required <code>node_name</code> <code>str</code> <p>Name of the originating node</p> required <code>run_id</code> <code>str</code> <p>Current run ID</p> required <code>tests</code> <code>List[TestConfig]</code> <p>List of test configurations (for building failure reasons)</p> required <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with added metadata columns</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def add_quarantine_metadata(\n    invalid_df: Any,\n    test_results: Dict[str, Any],\n    config: QuarantineColumnsConfig,\n    engine: Any,\n    node_name: str,\n    run_id: str,\n    tests: List[TestConfig],\n) -&gt; Any:\n    \"\"\"\n    Add metadata columns to quarantined rows.\n\n    Args:\n        invalid_df: DataFrame of invalid rows\n        test_results: Dict of test_name -&gt; aggregate results (not per-row)\n        config: QuarantineColumnsConfig specifying which columns to add\n        engine: Engine instance\n        node_name: Name of the originating node\n        run_id: Current run ID\n        tests: List of test configurations (for building failure reasons)\n\n    Returns:\n        DataFrame with added metadata columns\n    \"\"\"\n    is_spark = False\n    is_polars = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(invalid_df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(invalid_df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n        except ImportError:\n            pass\n\n    rejected_at = datetime.now(timezone.utc).isoformat()\n\n    quarantine_tests = [t for t in tests if t.on_fail == ContractSeverity.QUARANTINE]\n    test_names = [t.name or f\"{t.type.value}\" for t in quarantine_tests]\n    failed_tests_str = \",\".join(test_names)\n    rejection_reason = f\"Failed tests: {failed_tests_str}\"\n\n    if is_spark:\n        from pyspark.sql import functions as F\n\n        result_df = invalid_df\n\n        if config.rejection_reason:\n            result_df = result_df.withColumn(\"_rejection_reason\", F.lit(rejection_reason))\n\n        if config.rejected_at:\n            result_df = result_df.withColumn(\"_rejected_at\", F.lit(rejected_at))\n\n        if config.source_batch_id:\n            result_df = result_df.withColumn(\"_source_batch_id\", F.lit(run_id))\n\n        if config.failed_tests:\n            result_df = result_df.withColumn(\"_failed_tests\", F.lit(failed_tests_str))\n\n        if config.original_node:\n            result_df = result_df.withColumn(\"_original_node\", F.lit(node_name))\n\n        return result_df\n\n    elif is_polars:\n        import polars as pl\n\n        result_df = invalid_df\n\n        if config.rejection_reason:\n            result_df = result_df.with_columns(pl.lit(rejection_reason).alias(\"_rejection_reason\"))\n\n        if config.rejected_at:\n            result_df = result_df.with_columns(pl.lit(rejected_at).alias(\"_rejected_at\"))\n\n        if config.source_batch_id:\n            result_df = result_df.with_columns(pl.lit(run_id).alias(\"_source_batch_id\"))\n\n        if config.failed_tests:\n            result_df = result_df.with_columns(pl.lit(failed_tests_str).alias(\"_failed_tests\"))\n\n        if config.original_node:\n            result_df = result_df.with_columns(pl.lit(node_name).alias(\"_original_node\"))\n\n        return result_df\n\n    else:\n        result_df = invalid_df.copy()\n\n        if config.rejection_reason:\n            result_df[\"_rejection_reason\"] = rejection_reason\n\n        if config.rejected_at:\n            result_df[\"_rejected_at\"] = rejected_at\n\n        if config.source_batch_id:\n            result_df[\"_source_batch_id\"] = run_id\n\n        if config.failed_tests:\n            result_df[\"_failed_tests\"] = failed_tests_str\n\n        if config.original_node:\n            result_df[\"_original_node\"] = node_name\n\n        return result_df\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.has_quarantine_tests","title":"<code>has_quarantine_tests(tests)</code>","text":"<p>Check if any tests use quarantine severity.</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def has_quarantine_tests(tests: List[TestConfig]) -&gt; bool:\n    \"\"\"Check if any tests use quarantine severity.\"\"\"\n    return any(t.on_fail == ContractSeverity.QUARANTINE for t in tests)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.split_valid_invalid","title":"<code>split_valid_invalid(df, tests, engine)</code>","text":"<p>Split DataFrame into valid and invalid portions based on quarantine tests.</p> <p>Only tests with on_fail == QUARANTINE are evaluated for splitting. A row is invalid if it fails ANY quarantine test.</p> <p>Performance: Removed per-row test_results lists to save O(N*tests) memory. Now stores only aggregate counts per test.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to split</p> required <code>tests</code> <code>List[TestConfig]</code> <p>List of test configurations</p> required <code>engine</code> <code>Any</code> <p>Engine instance (Spark, Pandas, or Polars)</p> required <p>Returns:</p> Type Description <code>QuarantineResult</code> <p>QuarantineResult with valid_df, invalid_df, and test metadata</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def split_valid_invalid(\n    df: Any,\n    tests: List[TestConfig],\n    engine: Any,\n) -&gt; QuarantineResult:\n    \"\"\"\n    Split DataFrame into valid and invalid portions based on quarantine tests.\n\n    Only tests with on_fail == QUARANTINE are evaluated for splitting.\n    A row is invalid if it fails ANY quarantine test.\n\n    Performance: Removed per-row test_results lists to save O(N*tests) memory.\n    Now stores only aggregate counts per test.\n\n    Args:\n        df: DataFrame to split\n        tests: List of test configurations\n        engine: Engine instance (Spark, Pandas, or Polars)\n\n    Returns:\n        QuarantineResult with valid_df, invalid_df, and test metadata\n    \"\"\"\n    is_spark = False\n    is_polars = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n        except ImportError:\n            pass\n\n    quarantine_tests = [t for t in tests if t.on_fail == ContractSeverity.QUARANTINE]\n\n    if not quarantine_tests:\n        if is_spark:\n            from pyspark.sql import functions as F\n\n            empty_df = df.filter(F.lit(False))\n        elif is_polars:\n            import polars as pl\n\n            empty_df = df.filter(pl.lit(False))\n        else:\n            empty_df = df.iloc[0:0].copy()\n\n        row_count = engine.count_rows(df) if hasattr(engine, \"count_rows\") else len(df)\n        return QuarantineResult(\n            valid_df=df,\n            invalid_df=empty_df,\n            rows_quarantined=0,\n            rows_valid=row_count,\n            test_results={},\n            failed_test_details={},\n        )\n\n    test_masks = {}\n    test_names = []\n\n    for idx, test in enumerate(quarantine_tests):\n        base_name = test.name or f\"{test.type.value}\"\n        test_name = base_name if base_name not in test_masks else f\"{base_name}_{idx}\"\n        test_names.append(test_name)\n        mask = _evaluate_test_mask(df, test, is_spark, is_polars)\n        test_masks[test_name] = mask\n\n    if is_spark:\n        from pyspark.sql import functions as F\n\n        combined_valid_mask = F.lit(True)\n        for mask in test_masks.values():\n            combined_valid_mask = combined_valid_mask &amp; mask\n\n        df_cached = df.cache()\n\n        valid_df = df_cached.filter(combined_valid_mask)\n        invalid_df = df_cached.filter(~combined_valid_mask)\n\n        valid_df = valid_df.cache()\n        invalid_df = invalid_df.cache()\n\n        rows_valid = valid_df.count()\n        rows_quarantined = invalid_df.count()\n        total = rows_valid + rows_quarantined\n\n        test_results = {}\n        for name, mask in test_masks.items():\n            pass_count = df_cached.filter(mask).count()\n            fail_count = total - pass_count\n            test_results[name] = {\"pass_count\": pass_count, \"fail_count\": fail_count}\n\n        df_cached.unpersist()\n\n    elif is_polars:\n        import polars as pl\n\n        combined_valid_mask = pl.lit(True)\n        for mask in test_masks.values():\n            combined_valid_mask = combined_valid_mask &amp; mask\n\n        valid_df = df.filter(combined_valid_mask)\n        invalid_df = df.filter(~combined_valid_mask)\n\n        rows_valid = len(valid_df)\n        rows_quarantined = len(invalid_df)\n\n        test_results = {}\n\n    else:\n        import pandas as pd\n\n        combined_valid_mask = pd.Series([True] * len(df), index=df.index)\n        for mask in test_masks.values():\n            combined_valid_mask = combined_valid_mask &amp; mask\n\n        valid_df = df[combined_valid_mask].copy()\n        invalid_df = df[~combined_valid_mask].copy()\n\n        rows_valid = len(valid_df)\n        rows_quarantined = len(invalid_df)\n\n        test_results = {}\n        for name, mask in test_masks.items():\n            pass_count = int(mask.sum())\n            fail_count = len(df) - pass_count\n            test_results[name] = {\"pass_count\": pass_count, \"fail_count\": fail_count}\n\n    logger.info(f\"Quarantine split: {rows_valid} valid, {rows_quarantined} invalid\")\n\n    return QuarantineResult(\n        valid_df=valid_df,\n        invalid_df=invalid_df,\n        rows_quarantined=rows_quarantined,\n        rows_valid=rows_valid,\n        test_results=test_results,\n        failed_test_details={},\n    )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.write_quarantine","title":"<code>write_quarantine(invalid_df, config, engine, connections)</code>","text":"<p>Write quarantined rows to destination (always append mode).</p> <p>Supports optional sampling/limiting via config.max_rows and config.sample_fraction.</p> <p>Parameters:</p> Name Type Description Default <code>invalid_df</code> <code>Any</code> <p>DataFrame of invalid rows with metadata</p> required <code>config</code> <code>QuarantineConfig</code> <p>QuarantineConfig specifying destination and sampling options</p> required <code>engine</code> <code>Any</code> <p>Engine instance</p> required <code>connections</code> <code>Dict[str, Any]</code> <p>Dict of connection configurations</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with write result metadata</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def write_quarantine(\n    invalid_df: Any,\n    config: QuarantineConfig,\n    engine: Any,\n    connections: Dict[str, Any],\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Write quarantined rows to destination (always append mode).\n\n    Supports optional sampling/limiting via config.max_rows and config.sample_fraction.\n\n    Args:\n        invalid_df: DataFrame of invalid rows with metadata\n        config: QuarantineConfig specifying destination and sampling options\n        engine: Engine instance\n        connections: Dict of connection configurations\n\n    Returns:\n        Dict with write result metadata\n    \"\"\"\n    is_spark = False\n    is_polars = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(invalid_df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(invalid_df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n        except ImportError:\n            pass\n\n    invalid_df = _apply_sampling(invalid_df, config, is_spark, is_polars)\n\n    if is_spark:\n        row_count = invalid_df.count()\n    elif is_polars:\n        row_count = len(invalid_df)\n    else:\n        row_count = len(invalid_df)\n\n    if row_count == 0:\n        return {\n            \"rows_quarantined\": 0,\n            \"quarantine_path\": config.path or config.table,\n            \"write_info\": None,\n        }\n\n    connection = connections.get(config.connection)\n    if connection is None:\n        raise ValueError(\n            f\"Quarantine connection '{config.connection}' not found. \"\n            f\"Available: {', '.join(connections.keys())}\"\n        )\n\n    try:\n        write_result = engine.write(\n            invalid_df,\n            connection=connection,\n            format=\"delta\" if config.table else \"parquet\",\n            path=config.path,\n            table=config.table,\n            mode=\"append\",\n        )\n    except Exception as e:\n        logger.error(f\"Failed to write quarantine data: {e}\")\n        raise\n\n    logger.info(f\"Wrote {row_count} rows to quarantine: {config.path or config.table}\")\n\n    return {\n        \"rows_quarantined\": row_count,\n        \"quarantine_path\": config.path or config.table,\n        \"write_info\": write_result,\n    }\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk","title":"<code>odibi.validation.fk</code>","text":""},{"location":"reference/api/validation/#odibi.validation.fk--foreign-key-validation-module","title":"Foreign Key Validation Module","text":"<p>Declare and validate referential integrity between fact and dimension tables.</p> <p>Features: - Declare relationships in YAML - Validate referential integrity on fact load - Detect orphan records - Generate lineage from relationships - Integration with FactPattern</p> Example Config <p>relationships:   - name: orders_to_customers     fact: fact_orders     dimension: dim_customer     fact_key: customer_sk     dimension_key: customer_sk</p> <ul> <li>name: orders_to_products     fact: fact_orders     dimension: dim_product     fact_key: product_sk     dimension_key: product_sk</li> </ul>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidationReport","title":"<code>FKValidationReport</code>  <code>dataclass</code>","text":"<p>Complete FK validation report for a fact table.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>@dataclass\nclass FKValidationReport:\n    \"\"\"Complete FK validation report for a fact table.\"\"\"\n\n    fact_table: str\n    all_valid: bool\n    total_relationships: int\n    valid_relationships: int\n    results: List[FKValidationResult] = field(default_factory=list)\n    orphan_records: List[OrphanRecord] = field(default_factory=list)\n    elapsed_ms: float = 0.0\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidationResult","title":"<code>FKValidationResult</code>  <code>dataclass</code>","text":"<p>Result of FK validation.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>@dataclass\nclass FKValidationResult:\n    \"\"\"Result of FK validation.\"\"\"\n\n    relationship_name: str\n    valid: bool\n    total_rows: int\n    orphan_count: int\n    null_count: int\n    orphan_values: List[Any] = field(default_factory=list)\n    elapsed_ms: float = 0.0\n    error: Optional[str] = None\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator","title":"<code>FKValidator</code>","text":"<p>Validate foreign key relationships between fact and dimension tables.</p> Usage <p>registry = RelationshipRegistry(relationships=[...]) validator = FKValidator(registry) report = validator.validate_fact(fact_df, \"fact_orders\", context)</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>class FKValidator:\n    \"\"\"\n    Validate foreign key relationships between fact and dimension tables.\n\n    Usage:\n        registry = RelationshipRegistry(relationships=[...])\n        validator = FKValidator(registry)\n        report = validator.validate_fact(fact_df, \"fact_orders\", context)\n    \"\"\"\n\n    def __init__(self, registry: RelationshipRegistry):\n        \"\"\"\n        Initialize with relationship registry.\n\n        Args:\n            registry: RelationshipRegistry with relationship definitions\n        \"\"\"\n        self.registry = registry\n\n    def validate_relationship(\n        self,\n        fact_df: Any,\n        relationship: RelationshipConfig,\n        context: EngineContext,\n    ) -&gt; FKValidationResult:\n        \"\"\"\n        Validate a single FK relationship.\n\n        Args:\n            fact_df: Fact DataFrame to validate\n            relationship: Relationship configuration\n            context: EngineContext with dimension data\n\n        Returns:\n            FKValidationResult with validation details\n        \"\"\"\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        ctx.debug(\n            \"Validating FK relationship\",\n            relationship=relationship.name,\n            fact=relationship.fact,\n            dimension=relationship.dimension,\n        )\n\n        try:\n            dim_df = context.get(relationship.dimension)\n        except KeyError:\n            elapsed_ms = (time.time() - start_time) * 1000\n            return FKValidationResult(\n                relationship_name=relationship.name,\n                valid=False,\n                total_rows=0,\n                orphan_count=0,\n                null_count=0,\n                elapsed_ms=elapsed_ms,\n                error=f\"Dimension table '{relationship.dimension}' not found\",\n            )\n\n        try:\n            if context.engine_type == EngineType.SPARK:\n                result = self._validate_spark(fact_df, dim_df, relationship)\n            else:\n                result = self._validate_pandas(fact_df, dim_df, relationship)\n\n            elapsed_ms = (time.time() - start_time) * 1000\n            result.elapsed_ms = elapsed_ms\n\n            if result.valid:\n                ctx.debug(\n                    \"FK validation passed\",\n                    relationship=relationship.name,\n                    total_rows=result.total_rows,\n                )\n            else:\n                ctx.warning(\n                    \"FK validation failed\",\n                    relationship=relationship.name,\n                    orphan_count=result.orphan_count,\n                    null_count=result.null_count,\n                )\n\n            return result\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"FK validation error: {e}\",\n                relationship=relationship.name,\n            )\n            return FKValidationResult(\n                relationship_name=relationship.name,\n                valid=False,\n                total_rows=0,\n                orphan_count=0,\n                null_count=0,\n                elapsed_ms=elapsed_ms,\n                error=str(e),\n            )\n\n    def _validate_spark(\n        self,\n        fact_df: Any,\n        dim_df: Any,\n        relationship: RelationshipConfig,\n    ) -&gt; FKValidationResult:\n        \"\"\"Validate using Spark.\"\"\"\n        from pyspark.sql import functions as F\n\n        fk_col = relationship.fact_key\n        dk_col = relationship.dimension_key\n\n        total_rows = fact_df.count()\n\n        null_count = fact_df.filter(F.col(fk_col).isNull()).count()\n\n        dim_keys = dim_df.select(F.col(dk_col).alias(\"_dim_key\")).distinct()\n\n        non_null_facts = fact_df.filter(F.col(fk_col).isNotNull())\n        orphans = non_null_facts.join(\n            dim_keys,\n            non_null_facts[fk_col] == dim_keys[\"_dim_key\"],\n            \"left_anti\",\n        )\n\n        orphan_count = orphans.count()\n\n        orphan_values = []\n        if orphan_count &gt; 0 and orphan_count &lt;= 100:\n            orphan_values = [\n                row[fk_col] for row in orphans.select(fk_col).distinct().limit(100).collect()\n            ]\n\n        is_valid = orphan_count == 0 and (relationship.nullable or null_count == 0)\n\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=is_valid,\n            total_rows=total_rows,\n            orphan_count=orphan_count,\n            null_count=null_count,\n            orphan_values=orphan_values,\n        )\n\n    def _validate_pandas(\n        self,\n        fact_df: Any,\n        dim_df: Any,\n        relationship: RelationshipConfig,\n    ) -&gt; FKValidationResult:\n        \"\"\"Validate using Pandas.\"\"\"\n\n        fk_col = relationship.fact_key\n        dk_col = relationship.dimension_key\n\n        total_rows = len(fact_df)\n\n        null_count = int(fact_df[fk_col].isna().sum())\n\n        dim_keys = set(dim_df[dk_col].dropna().unique())\n\n        non_null_fks = fact_df[fk_col].dropna()\n        orphan_mask = ~non_null_fks.isin(dim_keys)\n        orphan_count = int(orphan_mask.sum())\n\n        orphan_values = []\n        if orphan_count &gt; 0:\n            orphan_values = list(non_null_fks[orphan_mask].unique()[:100])\n\n        is_valid = orphan_count == 0 and (relationship.nullable or null_count == 0)\n\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=is_valid,\n            total_rows=total_rows,\n            orphan_count=orphan_count,\n            null_count=null_count,\n            orphan_values=orphan_values,\n        )\n\n    def validate_fact(\n        self,\n        fact_df: Any,\n        fact_table: str,\n        context: EngineContext,\n    ) -&gt; FKValidationReport:\n        \"\"\"\n        Validate all FK relationships for a fact table.\n\n        Args:\n            fact_df: Fact DataFrame to validate\n            fact_table: Fact table name\n            context: EngineContext with dimension data\n\n        Returns:\n            FKValidationReport with all validation results\n        \"\"\"\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        ctx.info(\"Starting FK validation\", fact_table=fact_table)\n\n        relationships = self.registry.get_fact_relationships(fact_table)\n\n        if not relationships:\n            ctx.warning(\n                \"No FK relationships defined\",\n                fact_table=fact_table,\n            )\n            return FKValidationReport(\n                fact_table=fact_table,\n                all_valid=True,\n                total_relationships=0,\n                valid_relationships=0,\n                elapsed_ms=(time.time() - start_time) * 1000,\n            )\n\n        results = []\n        all_orphans = []\n\n        for relationship in relationships:\n            result = self.validate_relationship(fact_df, relationship, context)\n            results.append(result)\n\n            if result.orphan_count &gt; 0:\n                for orphan_val in result.orphan_values:\n                    all_orphans.append(\n                        OrphanRecord(\n                            fact_key_value=orphan_val,\n                            fact_key_column=relationship.fact_key,\n                            dimension_table=relationship.dimension,\n                        )\n                    )\n\n        all_valid = all(r.valid for r in results)\n        valid_count = sum(1 for r in results if r.valid)\n        elapsed_ms = (time.time() - start_time) * 1000\n\n        if all_valid:\n            ctx.info(\n                \"FK validation passed\",\n                fact_table=fact_table,\n                relationships=len(relationships),\n            )\n        else:\n            ctx.warning(\n                \"FK validation failed\",\n                fact_table=fact_table,\n                valid=valid_count,\n                total=len(relationships),\n            )\n\n        return FKValidationReport(\n            fact_table=fact_table,\n            all_valid=all_valid,\n            total_relationships=len(relationships),\n            valid_relationships=valid_count,\n            results=results,\n            orphan_records=all_orphans,\n            elapsed_ms=elapsed_ms,\n        )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator.__init__","title":"<code>__init__(registry)</code>","text":"<p>Initialize with relationship registry.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>RelationshipRegistry</code> <p>RelationshipRegistry with relationship definitions</p> required Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def __init__(self, registry: RelationshipRegistry):\n    \"\"\"\n    Initialize with relationship registry.\n\n    Args:\n        registry: RelationshipRegistry with relationship definitions\n    \"\"\"\n    self.registry = registry\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator.validate_fact","title":"<code>validate_fact(fact_df, fact_table, context)</code>","text":"<p>Validate all FK relationships for a fact table.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame to validate</p> required <code>fact_table</code> <code>str</code> <p>Fact table name</p> required <code>context</code> <code>EngineContext</code> <p>EngineContext with dimension data</p> required <p>Returns:</p> Type Description <code>FKValidationReport</code> <p>FKValidationReport with all validation results</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def validate_fact(\n    self,\n    fact_df: Any,\n    fact_table: str,\n    context: EngineContext,\n) -&gt; FKValidationReport:\n    \"\"\"\n    Validate all FK relationships for a fact table.\n\n    Args:\n        fact_df: Fact DataFrame to validate\n        fact_table: Fact table name\n        context: EngineContext with dimension data\n\n    Returns:\n        FKValidationReport with all validation results\n    \"\"\"\n    ctx = get_logging_context()\n    start_time = time.time()\n\n    ctx.info(\"Starting FK validation\", fact_table=fact_table)\n\n    relationships = self.registry.get_fact_relationships(fact_table)\n\n    if not relationships:\n        ctx.warning(\n            \"No FK relationships defined\",\n            fact_table=fact_table,\n        )\n        return FKValidationReport(\n            fact_table=fact_table,\n            all_valid=True,\n            total_relationships=0,\n            valid_relationships=0,\n            elapsed_ms=(time.time() - start_time) * 1000,\n        )\n\n    results = []\n    all_orphans = []\n\n    for relationship in relationships:\n        result = self.validate_relationship(fact_df, relationship, context)\n        results.append(result)\n\n        if result.orphan_count &gt; 0:\n            for orphan_val in result.orphan_values:\n                all_orphans.append(\n                    OrphanRecord(\n                        fact_key_value=orphan_val,\n                        fact_key_column=relationship.fact_key,\n                        dimension_table=relationship.dimension,\n                    )\n                )\n\n    all_valid = all(r.valid for r in results)\n    valid_count = sum(1 for r in results if r.valid)\n    elapsed_ms = (time.time() - start_time) * 1000\n\n    if all_valid:\n        ctx.info(\n            \"FK validation passed\",\n            fact_table=fact_table,\n            relationships=len(relationships),\n        )\n    else:\n        ctx.warning(\n            \"FK validation failed\",\n            fact_table=fact_table,\n            valid=valid_count,\n            total=len(relationships),\n        )\n\n    return FKValidationReport(\n        fact_table=fact_table,\n        all_valid=all_valid,\n        total_relationships=len(relationships),\n        valid_relationships=valid_count,\n        results=results,\n        orphan_records=all_orphans,\n        elapsed_ms=elapsed_ms,\n    )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator.validate_relationship","title":"<code>validate_relationship(fact_df, relationship, context)</code>","text":"<p>Validate a single FK relationship.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame to validate</p> required <code>relationship</code> <code>RelationshipConfig</code> <p>Relationship configuration</p> required <code>context</code> <code>EngineContext</code> <p>EngineContext with dimension data</p> required <p>Returns:</p> Type Description <code>FKValidationResult</code> <p>FKValidationResult with validation details</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def validate_relationship(\n    self,\n    fact_df: Any,\n    relationship: RelationshipConfig,\n    context: EngineContext,\n) -&gt; FKValidationResult:\n    \"\"\"\n    Validate a single FK relationship.\n\n    Args:\n        fact_df: Fact DataFrame to validate\n        relationship: Relationship configuration\n        context: EngineContext with dimension data\n\n    Returns:\n        FKValidationResult with validation details\n    \"\"\"\n    ctx = get_logging_context()\n    start_time = time.time()\n\n    ctx.debug(\n        \"Validating FK relationship\",\n        relationship=relationship.name,\n        fact=relationship.fact,\n        dimension=relationship.dimension,\n    )\n\n    try:\n        dim_df = context.get(relationship.dimension)\n    except KeyError:\n        elapsed_ms = (time.time() - start_time) * 1000\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=False,\n            total_rows=0,\n            orphan_count=0,\n            null_count=0,\n            elapsed_ms=elapsed_ms,\n            error=f\"Dimension table '{relationship.dimension}' not found\",\n        )\n\n    try:\n        if context.engine_type == EngineType.SPARK:\n            result = self._validate_spark(fact_df, dim_df, relationship)\n        else:\n            result = self._validate_pandas(fact_df, dim_df, relationship)\n\n        elapsed_ms = (time.time() - start_time) * 1000\n        result.elapsed_ms = elapsed_ms\n\n        if result.valid:\n            ctx.debug(\n                \"FK validation passed\",\n                relationship=relationship.name,\n                total_rows=result.total_rows,\n            )\n        else:\n            ctx.warning(\n                \"FK validation failed\",\n                relationship=relationship.name,\n                orphan_count=result.orphan_count,\n                null_count=result.null_count,\n            )\n\n        return result\n\n    except Exception as e:\n        elapsed_ms = (time.time() - start_time) * 1000\n        ctx.error(\n            f\"FK validation error: {e}\",\n            relationship=relationship.name,\n        )\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=False,\n            total_rows=0,\n            orphan_count=0,\n            null_count=0,\n            elapsed_ms=elapsed_ms,\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.OrphanRecord","title":"<code>OrphanRecord</code>  <code>dataclass</code>","text":"<p>Details of an orphan record.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>@dataclass\nclass OrphanRecord:\n    \"\"\"Details of an orphan record.\"\"\"\n\n    fact_key_value: Any\n    fact_key_column: str\n    dimension_table: str\n    row_index: Optional[int] = None\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipConfig","title":"<code>RelationshipConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a foreign key relationship.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique relationship identifier</p> <code>fact</code> <code>str</code> <p>Fact table name</p> <code>dimension</code> <code>str</code> <p>Dimension table name</p> <code>fact_key</code> <code>str</code> <p>Foreign key column in fact table</p> <code>dimension_key</code> <code>str</code> <p>Primary/surrogate key column in dimension</p> <code>nullable</code> <code>bool</code> <p>Whether nulls are allowed in fact_key</p> <code>on_violation</code> <code>str</code> <p>Action on violation (\"warn\", \"error\", \"quarantine\")</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>class RelationshipConfig(BaseModel):\n    \"\"\"\n    Configuration for a foreign key relationship.\n\n    Attributes:\n        name: Unique relationship identifier\n        fact: Fact table name\n        dimension: Dimension table name\n        fact_key: Foreign key column in fact table\n        dimension_key: Primary/surrogate key column in dimension\n        nullable: Whether nulls are allowed in fact_key\n        on_violation: Action on violation (\"warn\", \"error\", \"quarantine\")\n    \"\"\"\n\n    name: str = Field(..., description=\"Unique relationship identifier\")\n    fact: str = Field(..., description=\"Fact table name\")\n    dimension: str = Field(..., description=\"Dimension table name\")\n    fact_key: str = Field(..., description=\"FK column in fact table\")\n    dimension_key: str = Field(..., description=\"PK/SK column in dimension\")\n    nullable: bool = Field(default=False, description=\"Allow nulls in fact_key\")\n    on_violation: str = Field(default=\"error\", description=\"Action on violation\")\n\n    @field_validator(\"name\", \"fact\", \"dimension\", \"fact_key\", \"dimension_key\")\n    @classmethod\n    def validate_not_empty(cls, v: str, info) -&gt; str:\n        if not v or not v.strip():\n            raise ValueError(f\"{info.field_name} cannot be empty\")\n        return v.strip()\n\n    @field_validator(\"on_violation\")\n    @classmethod\n    def validate_on_violation(cls, v: str) -&gt; str:\n        valid = (\"warn\", \"error\", \"quarantine\")\n        if v.lower() not in valid:\n            raise ValueError(f\"on_violation must be one of {valid}\")\n        return v.lower()\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry","title":"<code>RelationshipRegistry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Registry of all declared relationships.</p> <p>Attributes:</p> Name Type Description <code>relationships</code> <code>List[RelationshipConfig]</code> <p>List of relationship configurations</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>class RelationshipRegistry(BaseModel):\n    \"\"\"\n    Registry of all declared relationships.\n\n    Attributes:\n        relationships: List of relationship configurations\n    \"\"\"\n\n    relationships: List[RelationshipConfig] = Field(\n        default_factory=list, description=\"Relationship definitions\"\n    )\n\n    def get_relationship(self, name: str) -&gt; Optional[RelationshipConfig]:\n        \"\"\"Get a relationship by name.\"\"\"\n        for rel in self.relationships:\n            if rel.name.lower() == name.lower():\n                return rel\n        return None\n\n    def get_fact_relationships(self, fact_table: str) -&gt; List[RelationshipConfig]:\n        \"\"\"Get all relationships for a fact table.\"\"\"\n        return [rel for rel in self.relationships if rel.fact.lower() == fact_table.lower()]\n\n    def get_dimension_relationships(self, dim_table: str) -&gt; List[RelationshipConfig]:\n        \"\"\"Get all relationships referencing a dimension.\"\"\"\n        return [rel for rel in self.relationships if rel.dimension.lower() == dim_table.lower()]\n\n    def generate_lineage(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        Generate lineage map from relationships.\n\n        Returns:\n            Dict mapping fact tables to their dimension dependencies\n        \"\"\"\n        lineage: Dict[str, List[str]] = {}\n        for rel in self.relationships:\n            if rel.fact not in lineage:\n                lineage[rel.fact] = []\n            if rel.dimension not in lineage[rel.fact]:\n                lineage[rel.fact].append(rel.dimension)\n        return lineage\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.generate_lineage","title":"<code>generate_lineage()</code>","text":"<p>Generate lineage map from relationships.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict mapping fact tables to their dimension dependencies</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def generate_lineage(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    Generate lineage map from relationships.\n\n    Returns:\n        Dict mapping fact tables to their dimension dependencies\n    \"\"\"\n    lineage: Dict[str, List[str]] = {}\n    for rel in self.relationships:\n        if rel.fact not in lineage:\n            lineage[rel.fact] = []\n        if rel.dimension not in lineage[rel.fact]:\n            lineage[rel.fact].append(rel.dimension)\n    return lineage\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.get_dimension_relationships","title":"<code>get_dimension_relationships(dim_table)</code>","text":"<p>Get all relationships referencing a dimension.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_dimension_relationships(self, dim_table: str) -&gt; List[RelationshipConfig]:\n    \"\"\"Get all relationships referencing a dimension.\"\"\"\n    return [rel for rel in self.relationships if rel.dimension.lower() == dim_table.lower()]\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.get_fact_relationships","title":"<code>get_fact_relationships(fact_table)</code>","text":"<p>Get all relationships for a fact table.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_fact_relationships(self, fact_table: str) -&gt; List[RelationshipConfig]:\n    \"\"\"Get all relationships for a fact table.\"\"\"\n    return [rel for rel in self.relationships if rel.fact.lower() == fact_table.lower()]\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.get_relationship","title":"<code>get_relationship(name)</code>","text":"<p>Get a relationship by name.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_relationship(self, name: str) -&gt; Optional[RelationshipConfig]:\n    \"\"\"Get a relationship by name.\"\"\"\n    for rel in self.relationships:\n        if rel.name.lower() == name.lower():\n            return rel\n    return None\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.get_orphan_records","title":"<code>get_orphan_records(fact_df, relationship, dim_df, engine_type)</code>","text":"<p>Extract orphan records from a fact table.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame</p> required <code>relationship</code> <code>RelationshipConfig</code> <p>Relationship configuration</p> required <code>dim_df</code> <code>Any</code> <p>Dimension DataFrame</p> required <code>engine_type</code> <code>EngineType</code> <p>Engine type (SPARK or PANDAS)</p> required <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame containing orphan records</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_orphan_records(\n    fact_df: Any,\n    relationship: RelationshipConfig,\n    dim_df: Any,\n    engine_type: EngineType,\n) -&gt; Any:\n    \"\"\"\n    Extract orphan records from a fact table.\n\n    Args:\n        fact_df: Fact DataFrame\n        relationship: Relationship configuration\n        dim_df: Dimension DataFrame\n        engine_type: Engine type (SPARK or PANDAS)\n\n    Returns:\n        DataFrame containing orphan records\n    \"\"\"\n    fk_col = relationship.fact_key\n    dk_col = relationship.dimension_key\n\n    if engine_type == EngineType.SPARK:\n        from pyspark.sql import functions as F\n\n        dim_keys = dim_df.select(F.col(dk_col).alias(\"_dim_key\")).distinct()\n        non_null_facts = fact_df.filter(F.col(fk_col).isNotNull())\n        orphans = non_null_facts.join(\n            dim_keys,\n            non_null_facts[fk_col] == dim_keys[\"_dim_key\"],\n            \"left_anti\",\n        )\n        return orphans\n    else:\n        dim_keys = set(dim_df[dk_col].dropna().unique())\n        non_null_mask = fact_df[fk_col].notna()\n        orphan_mask = ~fact_df[fk_col].isin(dim_keys) &amp; non_null_mask\n        return fact_df[orphan_mask].copy()\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.parse_relationships_config","title":"<code>parse_relationships_config(config_dict)</code>","text":"<p>Parse relationships from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>Dict[str, Any]</code> <p>Config dict with \"relationships\" key</p> required <p>Returns:</p> Type Description <code>RelationshipRegistry</code> <p>RelationshipRegistry instance</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def parse_relationships_config(config_dict: Dict[str, Any]) -&gt; RelationshipRegistry:\n    \"\"\"\n    Parse relationships from a configuration dictionary.\n\n    Args:\n        config_dict: Config dict with \"relationships\" key\n\n    Returns:\n        RelationshipRegistry instance\n    \"\"\"\n    relationships = []\n    for rel_dict in config_dict.get(\"relationships\", []):\n        relationships.append(RelationshipConfig(**rel_dict))\n    return RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.validate_fk_on_load","title":"<code>validate_fk_on_load(fact_df, relationships, context, on_failure='error')</code>","text":"<p>Validate FK constraints and optionally filter orphans.</p> <p>This is a convenience function for use in FactPattern.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame to validate</p> required <code>relationships</code> <code>List[RelationshipConfig]</code> <p>List of relationship configs</p> required <code>context</code> <code>EngineContext</code> <p>EngineContext with dimension data</p> required <code>on_failure</code> <code>str</code> <p>Action on failure (\"error\", \"warn\", \"filter\")</p> <code>'error'</code> <p>Returns:</p> Type Description <code>Any</code> <p>fact_df (possibly filtered if on_failure=\"filter\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If on_failure=\"error\" and validation fails</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def validate_fk_on_load(\n    fact_df: Any,\n    relationships: List[RelationshipConfig],\n    context: EngineContext,\n    on_failure: str = \"error\",\n) -&gt; Any:\n    \"\"\"\n    Validate FK constraints and optionally filter orphans.\n\n    This is a convenience function for use in FactPattern.\n\n    Args:\n        fact_df: Fact DataFrame to validate\n        relationships: List of relationship configs\n        context: EngineContext with dimension data\n        on_failure: Action on failure (\"error\", \"warn\", \"filter\")\n\n    Returns:\n        fact_df (possibly filtered if on_failure=\"filter\")\n\n    Raises:\n        ValueError: If on_failure=\"error\" and validation fails\n    \"\"\"\n    ctx = get_logging_context()\n\n    registry = RelationshipRegistry(relationships=relationships)\n    validator = FKValidator(registry)\n\n    for rel in relationships:\n        result = validator.validate_relationship(fact_df, rel, context)\n\n        if not result.valid:\n            if on_failure == \"error\":\n                raise ValueError(\n                    f\"FK validation failed for '{rel.name}': \"\n                    f\"{result.orphan_count} orphans, {result.null_count} nulls. \"\n                    f\"Sample orphan values: {result.orphan_values[:5]}\"\n                )\n            elif on_failure == \"warn\":\n                ctx.warning(\n                    f\"FK validation warning for '{rel.name}': \"\n                    f\"{result.orphan_count} orphans, {result.null_count} nulls\"\n                )\n            elif on_failure == \"filter\":\n                try:\n                    dim_df = context.get(rel.dimension)\n                except KeyError:\n                    continue\n\n                if context.engine_type == EngineType.SPARK:\n                    from pyspark.sql import functions as F\n\n                    dim_keys = dim_df.select(F.col(rel.dimension_key).alias(\"_fk_key\")).distinct()\n                    fact_df = fact_df.join(\n                        dim_keys,\n                        fact_df[rel.fact_key] == dim_keys[\"_fk_key\"],\n                        \"inner\",\n                    ).drop(\"_fk_key\")\n                else:\n                    dim_keys = set(dim_df[rel.dimension_key].dropna().unique())\n                    fact_df = fact_df[fact_df[rel.fact_key].isin(dim_keys)].copy()\n\n                ctx.info(\n                    f\"Filtered orphans for '{rel.name}'\",\n                    remaining_rows=len(fact_df) if hasattr(fact_df, \"__len__\") else \"N/A\",\n                )\n\n    return fact_df\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/","title":"Medallion Architecture Enhancements Plan","text":"<p>Status: Draft Created: 2024-01-30 Total Effort: ~6-7 days Priority: High - Production readiness for medallion pipelines</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#executive-summary","title":"Executive Summary","text":"<p>This plan addresses 5 key gaps in Odibi's medallion architecture support:</p> # Enhancement Effort Priority 1 Quarantine Tables 1.5 days \ud83d\udd34 Critical 2 Quality Gates 1 day \ud83d\udd34 Critical 3 Alerting Enhancements 1 day \ud83d\udfe1 High 4 Schema Version Tracking 0.5 days \ud83d\udfe2 Medium 5 Cross-Pipeline Lineage 2-3 days \ud83d\udfe2 Medium"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#1-quarantine-tables","title":"1. Quarantine Tables","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem","title":"Problem","text":"<p>When validation tests fail, bad rows are either: - Logged and written anyway (<code>on_fail: warn</code>) - Cause the entire pipeline to fail (<code>on_fail: fail</code>)</p> <p>Neither option preserves bad data for later analysis/reprocessing.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution","title":"Solution","text":"<p>Route failed rows to a dedicated quarantine table with rejection metadata.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#yaml-schema","title":"YAML Schema","text":"<pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id, email]\n          on_fail: quarantine  # NEW: Route to quarantine instead of fail/warn\n        - type: regex_match\n          column: email\n          pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n          on_fail: quarantine\n\n      # NEW: Quarantine configuration\n      quarantine:\n        connection: silver\n        path: customers_quarantine  # Or table: customers_quarantine\n        add_columns:\n          _rejection_reason: true    # Which test(s) failed\n          _rejected_at: true         # Timestamp\n          _source_batch_id: true     # Link back to source batch\n          _failed_tests: true        # List of failed test names\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#config-model-changes","title":"Config Model Changes","text":"<pre><code># odibi/config.py\n\nclass QuarantineConfig(BaseModel):\n    \"\"\"Configuration for quarantine table routing.\"\"\"\n\n    connection: str = Field(description=\"Connection for quarantine writes\")\n    path: Optional[str] = Field(default=None, description=\"Path for quarantine data\")\n    table: Optional[str] = Field(default=None, description=\"Table name for quarantine\")\n\n    # Metadata columns to add\n    add_columns: QuarantineColumnsConfig = Field(\n        default_factory=QuarantineColumnsConfig,\n        description=\"Metadata columns to add to quarantined rows\"\n    )\n\n    # Retention\n    retention_days: Optional[int] = Field(\n        default=90,\n        ge=1,\n        description=\"Days to retain quarantined data (auto-cleanup)\"\n    )\n\nclass QuarantineColumnsConfig(BaseModel):\n    \"\"\"Columns added to quarantined rows.\"\"\"\n    rejection_reason: bool = Field(default=True, alias=\"_rejection_reason\")\n    rejected_at: bool = Field(default=True, alias=\"_rejected_at\")\n    source_batch_id: bool = Field(default=True, alias=\"_source_batch_id\")\n    failed_tests: bool = Field(default=True, alias=\"_failed_tests\")\n    original_node: bool = Field(default=False, alias=\"_original_node\")\n\n\nclass ValidationConfig(BaseModel):\n    \"\"\"Updated validation config with quarantine support.\"\"\"\n    tests: List[TestConfig] = Field(default_factory=list)\n    quarantine: Optional[QuarantineConfig] = Field(\n        default=None,\n        description=\"Quarantine configuration for failed rows\"\n    )\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation","title":"Implementation","text":"<p>File: <code>odibi/validation/quarantine.py</code> (new)</p> <pre><code>from typing import Any, Dict, List, Tuple\nfrom datetime import datetime\nimport hashlib\n\ndef split_valid_invalid(\n    df: Any,\n    tests: List[TestConfig],\n    engine: Engine,\n    context: EngineContext\n) -&gt; Tuple[Any, Any, List[Dict]]:\n    \"\"\"\n    Split DataFrame into valid and invalid portions.\n\n    Returns:\n        Tuple of (valid_df, invalid_df, rejection_details)\n    \"\"\"\n    # Build combined validity mask\n    validity_masks = []\n    test_results = {}  # row_index -&gt; [failed_test_names]\n\n    for test in tests:\n        if test.on_fail == ContractSeverity.QUARANTINE:\n            mask = _evaluate_test(df, test, engine)\n            validity_masks.append(mask)\n            test_results[test.name or test.type] = ~mask\n\n    # Combine masks (row is valid if passes ALL quarantine tests)\n    combined_valid = engine.all_true(validity_masks)\n\n    valid_df = engine.filter(df, combined_valid)\n    invalid_df = engine.filter(df, ~combined_valid)\n\n    return valid_df, invalid_df, test_results\n\n\ndef add_quarantine_metadata(\n    invalid_df: Any,\n    test_results: Dict,\n    config: QuarantineConfig,\n    engine: Engine,\n    node_name: str,\n    run_id: str\n) -&gt; Any:\n    \"\"\"Add metadata columns to quarantined rows.\"\"\"\n\n    if config.add_columns.rejection_reason:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_rejection_reason\",\n            _get_rejection_reasons(invalid_df, test_results, engine)\n        )\n\n    if config.add_columns.rejected_at:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_rejected_at\",\n            datetime.utcnow().isoformat()\n        )\n\n    if config.add_columns.source_batch_id:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_source_batch_id\",\n            run_id\n        )\n\n    if config.add_columns.failed_tests:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_failed_tests\",\n            _get_failed_test_list(invalid_df, test_results, engine)\n        )\n\n    if config.add_columns.original_node:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_original_node\",\n            node_name\n        )\n\n    return invalid_df\n\n\ndef write_quarantine(\n    invalid_df: Any,\n    config: QuarantineConfig,\n    engine: Engine,\n    connections: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"Write quarantined rows to destination.\"\"\"\n\n    connection = connections[config.connection]\n\n    result = engine.write(\n        invalid_df,\n        connection=connection,\n        format=\"delta\",\n        path=config.path,\n        table=config.table,\n        mode=\"append\"  # Always append\n    )\n\n    return {\n        \"rows_quarantined\": engine.count(invalid_df),\n        \"quarantine_path\": config.path or config.table,\n        \"write_info\": result\n    }\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#node-execution-changes","title":"Node Execution Changes","text":"<p>File: <code>odibi/node.py</code> - Update validation flow</p> <pre><code># In _execute_validation method:\n\ndef _execute_validation(self, df, config, context):\n    \"\"\"Execute validation with quarantine support.\"\"\"\n\n    validator = Validator()\n\n    # Check if any tests use quarantine\n    has_quarantine_tests = any(\n        t.on_fail == ContractSeverity.QUARANTINE\n        for t in config.validation.tests\n    )\n\n    if has_quarantine_tests and config.validation.quarantine:\n        # Split valid/invalid\n        valid_df, invalid_df, test_results = split_valid_invalid(\n            df, config.validation.tests, self.engine, context\n        )\n\n        invalid_count = self.engine.count(invalid_df)\n\n        if invalid_count &gt; 0:\n            # Add metadata\n            invalid_df = add_quarantine_metadata(\n                invalid_df,\n                test_results,\n                config.validation.quarantine,\n                self.engine,\n                config.name,\n                context.run_id\n            )\n\n            # Write to quarantine\n            quarantine_result = write_quarantine(\n                invalid_df,\n                config.validation.quarantine,\n                self.engine,\n                context.connections\n            )\n\n            # Emit alert if configured\n            self._emit_quarantine_alert(config, quarantine_result, context)\n\n            logger.info(\n                f\"Quarantined {invalid_count} rows to \"\n                f\"{config.validation.quarantine.path or config.validation.quarantine.table}\"\n            )\n\n        # Continue with valid rows only\n        return valid_df, []\n\n    else:\n        # Original validation logic (no quarantine)\n        failures = validator.validate(df, config.validation, context)\n        return df, failures\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#tests","title":"Tests","text":"<pre><code># tests/test_quarantine.py\n\ndef test_quarantine_splits_valid_invalid():\n    \"\"\"Valid rows continue, invalid rows go to quarantine.\"\"\"\n\ndef test_quarantine_adds_metadata_columns():\n    \"\"\"Quarantined rows have _rejection_reason, _rejected_at, etc.\"\"\"\n\ndef test_quarantine_appends_not_overwrites():\n    \"\"\"Multiple runs append to same quarantine table.\"\"\"\n\ndef test_quarantine_triggers_alert():\n    \"\"\"Alert sent when rows are quarantined.\"\"\"\n\ndef test_multiple_failed_tests_captured():\n    \"\"\"Row failing 3 tests has all 3 in _failed_tests column.\"\"\"\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#2-quality-gates","title":"2. Quality Gates","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem_1","title":"Problem","text":"<p>Validation tests run per-row, but there's no batch-level check that says \"abort if too many rows fail.\"</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution_1","title":"Solution","text":"<p>Add a <code>gate</code> configuration that evaluates the batch as a whole before writing.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#yaml-schema_1","title":"YAML Schema","text":"<pre><code>nodes:\n  - name: load_silver_customers\n    read:\n      connection: bronze\n      path: customers\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n        - type: unique\n          columns: [customer_id]\n\n    # NEW: Quality Gate\n    gate:\n      # Minimum percentage of rows that must pass ALL tests\n      require_pass_rate: 0.95  # 95%\n\n      # What to do if gate fails\n      on_fail: abort  # abort | warn_and_write | write_valid_only\n\n      # Optional: Specific thresholds per test type\n      thresholds:\n        - test: not_null\n          min_pass_rate: 0.99  # 99% must have non-null customer_id\n        - test: unique\n          min_pass_rate: 1.0   # 100% must be unique (no duplicates)\n\n      # Row count anomaly detection\n      row_count:\n        min: 100              # At least 100 rows expected\n        max: 1000000          # At most 1M rows\n        change_threshold: 0.5 # Fail if row count changes &gt;50% vs last run\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#config-model","title":"Config Model","text":"<pre><code># odibi/config.py\n\nclass GateThreshold(BaseModel):\n    \"\"\"Threshold for a specific test.\"\"\"\n    test: str = Field(description=\"Test name or type\")\n    min_pass_rate: float = Field(ge=0.0, le=1.0, description=\"Minimum pass rate\")\n\n\nclass RowCountGate(BaseModel):\n    \"\"\"Row count anomaly detection.\"\"\"\n    min: Optional[int] = Field(default=None, ge=0)\n    max: Optional[int] = Field(default=None, ge=0)\n    change_threshold: Optional[float] = Field(\n        default=None,\n        ge=0.0,\n        le=1.0,\n        description=\"Max allowed change vs previous run (e.g., 0.5 = 50%)\"\n    )\n\n\nclass GateOnFail(str, Enum):\n    \"\"\"Action when gate fails.\"\"\"\n    ABORT = \"abort\"                    # Stop, write nothing\n    WARN_AND_WRITE = \"warn_and_write\"  # Log warning, write all rows\n    WRITE_VALID_ONLY = \"write_valid_only\"  # Write only passing rows\n\n\nclass GateConfig(BaseModel):\n    \"\"\"Quality gate configuration.\"\"\"\n\n    require_pass_rate: float = Field(\n        default=0.95,\n        ge=0.0,\n        le=1.0,\n        description=\"Minimum percentage of rows passing ALL tests\"\n    )\n\n    on_fail: GateOnFail = Field(\n        default=GateOnFail.ABORT,\n        description=\"Action when gate fails\"\n    )\n\n    thresholds: List[GateThreshold] = Field(\n        default_factory=list,\n        description=\"Per-test thresholds (overrides global require_pass_rate)\"\n    )\n\n    row_count: Optional[RowCountGate] = Field(\n        default=None,\n        description=\"Row count anomaly detection\"\n    )\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation_1","title":"Implementation","text":"<p>File: <code>odibi/validation/gate.py</code> (new)</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclass\nclass GateResult:\n    \"\"\"Result of gate evaluation.\"\"\"\n    passed: bool\n    pass_rate: float\n    total_rows: int\n    passed_rows: int\n    failed_rows: int\n    details: Dict[str, Any]\n    action: GateOnFail\n\n\ndef evaluate_gate(\n    df: Any,\n    validation_results: Dict[str, List[bool]],  # test_name -&gt; per-row results\n    gate_config: GateConfig,\n    engine: Engine,\n    catalog: Optional[CatalogManager] = None,\n    node_name: Optional[str] = None\n) -&gt; GateResult:\n    \"\"\"\n    Evaluate quality gate on validation results.\n\n    Returns GateResult with pass/fail status and action to take.\n    \"\"\"\n    total_rows = engine.count(df)\n\n    # Calculate overall pass rate (row passes if ALL tests pass)\n    all_pass_mask = _combine_test_results(validation_results, engine)\n    passed_rows = engine.count_true(all_pass_mask)\n    pass_rate = passed_rows / total_rows if total_rows &gt; 0 else 1.0\n\n    details = {\n        \"overall_pass_rate\": pass_rate,\n        \"per_test_rates\": {},\n        \"row_count_check\": None\n    }\n\n    gate_passed = True\n    failure_reasons = []\n\n    # Check global threshold\n    if pass_rate &lt; gate_config.require_pass_rate:\n        gate_passed = False\n        failure_reasons.append(\n            f\"Overall pass rate {pass_rate:.1%} &lt; required {gate_config.require_pass_rate:.1%}\"\n        )\n\n    # Check per-test thresholds\n    for threshold in gate_config.thresholds:\n        test_results = validation_results.get(threshold.test)\n        if test_results:\n            test_pass_rate = sum(test_results) / len(test_results)\n            details[\"per_test_rates\"][threshold.test] = test_pass_rate\n\n            if test_pass_rate &lt; threshold.min_pass_rate:\n                gate_passed = False\n                failure_reasons.append(\n                    f\"Test '{threshold.test}' pass rate {test_pass_rate:.1%} \"\n                    f\"&lt; required {threshold.min_pass_rate:.1%}\"\n                )\n\n    # Check row count anomalies\n    if gate_config.row_count:\n        row_check = _check_row_count(\n            total_rows,\n            gate_config.row_count,\n            catalog,\n            node_name\n        )\n        details[\"row_count_check\"] = row_check\n\n        if not row_check[\"passed\"]:\n            gate_passed = False\n            failure_reasons.append(row_check[\"reason\"])\n\n    details[\"failure_reasons\"] = failure_reasons\n\n    return GateResult(\n        passed=gate_passed,\n        pass_rate=pass_rate,\n        total_rows=total_rows,\n        passed_rows=passed_rows,\n        failed_rows=total_rows - passed_rows,\n        details=details,\n        action=gate_config.on_fail if not gate_passed else None\n    )\n\n\ndef _check_row_count(\n    current_count: int,\n    config: RowCountGate,\n    catalog: Optional[CatalogManager],\n    node_name: Optional[str]\n) -&gt; Dict[str, Any]:\n    \"\"\"Check row count against thresholds and history.\"\"\"\n\n    result = {\"passed\": True, \"reason\": None, \"current\": current_count}\n\n    if config.min is not None and current_count &lt; config.min:\n        result[\"passed\"] = False\n        result[\"reason\"] = f\"Row count {current_count} &lt; minimum {config.min}\"\n        return result\n\n    if config.max is not None and current_count &gt; config.max:\n        result[\"passed\"] = False\n        result[\"reason\"] = f\"Row count {current_count} &gt; maximum {config.max}\"\n        return result\n\n    # Historical comparison\n    if config.change_threshold is not None and catalog and node_name:\n        previous_count = catalog.get_last_row_count(node_name)\n        if previous_count:\n            change = abs(current_count - previous_count) / previous_count\n            result[\"previous\"] = previous_count\n            result[\"change\"] = change\n\n            if change &gt; config.change_threshold:\n                result[\"passed\"] = False\n                result[\"reason\"] = (\n                    f\"Row count changed {change:.1%} \"\n                    f\"({previous_count} \u2192 {current_count}), \"\n                    f\"exceeds threshold {config.change_threshold:.1%}\"\n                )\n\n    return result\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#node-integration","title":"Node Integration","text":"<pre><code># In node.py - after validation, before write:\n\ndef _check_gate(self, df, validation_results, config, context):\n    \"\"\"Check quality gate before proceeding to write.\"\"\"\n\n    if not config.gate:\n        return df, True  # No gate configured\n\n    gate_result = evaluate_gate(\n        df,\n        validation_results,\n        config.gate,\n        self.engine,\n        context.catalog,\n        config.name\n    )\n\n    if gate_result.passed:\n        logger.info(f\"Gate passed: {gate_result.pass_rate:.1%} pass rate\")\n        return df, True\n\n    # Gate failed - take action\n    logger.warning(\n        f\"Gate FAILED: {gate_result.pass_rate:.1%} pass rate. \"\n        f\"Reasons: {gate_result.details['failure_reasons']}\"\n    )\n\n    # Emit alert\n    self._emit_gate_alert(config, gate_result, context)\n\n    if gate_result.action == GateOnFail.ABORT:\n        raise GateFailedError(\n            f\"Quality gate blocked write for node '{config.name}'. \"\n            f\"Pass rate: {gate_result.pass_rate:.1%}\"\n        )\n\n    elif gate_result.action == GateOnFail.WARN_AND_WRITE:\n        return df, True  # Write all, warning already logged\n\n    elif gate_result.action == GateOnFail.WRITE_VALID_ONLY:\n        valid_df = self.engine.filter(df, validation_results[\"_all_passed\"])\n        return valid_df, True\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#3-alerting-enhancements","title":"3. Alerting Enhancements","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#current-gaps","title":"Current Gaps","text":"Feature Teams Slack Gap Timestamp \u2705 \u274c Slack missing Status-based styling \u2705 (Adaptive Card styles) \u26a0\ufe0f (emoji only) Slack needs color Structured facts \u2705 (FactSet) \u26a0\ufe0f (inline fields) Minor Action buttons \u274c \u274c Both missing Validation events \u274c \u274c Both missing Throttling \u274c \u274c Both missing"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#new-alert-events","title":"New Alert Events","text":"<pre><code># odibi/config.py\n\nclass AlertEvent(str, Enum):\n    \"\"\"Events that trigger alerts.\"\"\"\n\n    # Existing\n    ON_START = \"on_start\"\n    ON_SUCCESS = \"on_success\"\n    ON_FAILURE = \"on_failure\"\n\n    # NEW: Validation &amp; Quality\n    ON_VALIDATION_FAIL = \"on_validation_fail\"    # Any validation test failed\n    ON_GATE_BLOCK = \"on_gate_block\"              # Quality gate blocked write\n    ON_QUARANTINE = \"on_quarantine\"              # Rows sent to quarantine\n    ON_THRESHOLD_BREACH = \"on_threshold_breach\"  # Row count anomaly, etc.\n\n    # NEW: Data Quality\n    ON_SCHEMA_DRIFT = \"on_schema_drift\"          # Schema changed vs previous\n    ON_FRESHNESS_FAIL = \"on_freshness_fail\"      # Data too old\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#enhanced-slack-payload","title":"Enhanced Slack Payload","text":"<pre><code># odibi/utils/alerting.py\n\ndef _build_slack_payload(config: AlertConfig, message: str, context: Dict[str, Any]) -&gt; Dict:\n    \"\"\"Enhanced Slack Block Kit payload with parity to Teams.\"\"\"\n\n    pipeline = context.get(\"pipeline\", \"Unknown\")\n    status = context.get(\"status\", \"UNKNOWN\")\n    event_type = context.get(\"event_type\", \"on_failure\")\n    duration = context.get(\"duration\", 0.0)\n    timestamp = context.get(\"timestamp\", datetime.utcnow().isoformat())\n    project_config = context.get(\"project_config\")\n\n    # Status styling\n    status_config = {\n        \"SUCCESS\": {\"icon\": \"\u2705\", \"color\": \"#36a64f\"},\n        \"STARTED\": {\"icon\": \"\ud83d\ude80\", \"color\": \"#1e90ff\"},\n        \"FAILED\": {\"icon\": \"\u274c\", \"color\": \"#dc3545\"},\n        \"GATE_BLOCKED\": {\"icon\": \"\ud83d\udeab\", \"color\": \"#ff6b35\"},\n        \"QUARANTINED\": {\"icon\": \"\ud83d\udd12\", \"color\": \"#ffc107\"},\n        \"WARNING\": {\"icon\": \"\u26a0\ufe0f\", \"color\": \"#ffc107\"},\n    }.get(status, {\"icon\": \"\u2753\", \"color\": \"#6c757d\"})\n\n    # Build blocks\n    blocks = [\n        {\n            \"type\": \"header\",\n            \"text\": {\n                \"type\": \"plain_text\",\n                \"text\": f\"{status_config['icon']} ODIBI: {pipeline} - {status}\"\n            }\n        },\n        {\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"*Project:*\\n{project_config.project if project_config else 'Unknown'}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Status:*\\n{status}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Duration:*\\n{duration:.2f}s\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Timestamp:*\\n{timestamp}\"},  # NEW\n            ]\n        },\n        {\n            \"type\": \"section\",\n            \"text\": {\"type\": \"mrkdwn\", \"text\": f\"*Message:*\\n{message}\"}\n        }\n    ]\n\n    # Add owner if present\n    if project_config and project_config.owner:\n        blocks[1][\"fields\"].append(\n            {\"type\": \"mrkdwn\", \"text\": f\"*Owner:*\\n{project_config.owner}\"}\n        )\n\n    # NEW: Add event-specific details\n    if event_type == \"on_quarantine\":\n        quarantine_details = context.get(\"quarantine_details\", {})\n        blocks.append({\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"*Rows Quarantined:*\\n{quarantine_details.get('rows_quarantined', 0):,}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Quarantine Table:*\\n`{quarantine_details.get('quarantine_path', 'N/A')}`\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Failed Tests:*\\n{', '.join(quarantine_details.get('failed_tests', []))}\"},\n            ]\n        })\n\n    elif event_type == \"on_gate_block\":\n        gate_details = context.get(\"gate_details\", {})\n        blocks.append({\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"*Pass Rate:*\\n{gate_details.get('pass_rate', 0):.1%}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Required:*\\n{gate_details.get('required_rate', 0.95):.1%}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Rows Failed:*\\n{gate_details.get('failed_rows', 0):,}\"},\n            ]\n        })\n        if gate_details.get(\"failure_reasons\"):\n            blocks.append({\n                \"type\": \"section\",\n                \"text\": {\"type\": \"mrkdwn\", \"text\": f\"*Failure Reasons:*\\n\u2022 \" + \"\\n\u2022 \".join(gate_details[\"failure_reasons\"])}\n            })\n\n    # NEW: Action buttons (link to story)\n    story_url = context.get(\"story_url\")\n    if story_url:\n        blocks.append({\n            \"type\": \"actions\",\n            \"elements\": [\n                {\n                    \"type\": \"button\",\n                    \"text\": {\"type\": \"plain_text\", \"text\": \"\ud83d\udcca View Story\"},\n                    \"url\": story_url,\n                    \"style\": \"primary\"\n                }\n            ]\n        })\n\n    # Add divider and context footer\n    blocks.extend([\n        {\"type\": \"divider\"},\n        {\n            \"type\": \"context\",\n            \"elements\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"Odibi v{__version__} | {event_type}\"}\n            ]\n        }\n    ])\n\n    # Use attachment for color sidebar\n    return {\n        \"attachments\": [{\n            \"color\": status_config[\"color\"],\n            \"blocks\": blocks\n        }]\n    }\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#enhanced-teams-payload","title":"Enhanced Teams Payload","text":"<pre><code>def _build_teams_payload(config: AlertConfig, message: str, context: Dict[str, Any]) -&gt; Dict:\n    \"\"\"Enhanced Teams Adaptive Card with event-specific details.\"\"\"\n\n    # ... existing header/status logic ...\n\n    # NEW: Event-specific sections\n    event_type = context.get(\"event_type\", \"on_failure\")\n\n    body_items = [\n        # ... existing header container ...\n    ]\n\n    # Standard facts\n    facts = [\n        {\"title\": \"\u23f1 Duration\", \"value\": f\"{duration:.2f}s\"},\n        {\"title\": \"\ud83d\udcc5 Time\", \"value\": context.get(\"timestamp\", \"\")},\n        {\"title\": \"\ud83d\udcdd Message\", \"value\": message},\n    ]\n\n    # NEW: Event-specific facts\n    if event_type == \"on_quarantine\":\n        qd = context.get(\"quarantine_details\", {})\n        facts.extend([\n            {\"title\": \"\ud83d\udd12 Rows Quarantined\", \"value\": f\"{qd.get('rows_quarantined', 0):,}\"},\n            {\"title\": \"\ud83d\udccd Quarantine Table\", \"value\": qd.get(\"quarantine_path\", \"N/A\")},\n            {\"title\": \"\u274c Failed Tests\", \"value\": \", \".join(qd.get(\"failed_tests\", []))},\n        ])\n\n    elif event_type == \"on_gate_block\":\n        gd = context.get(\"gate_details\", {})\n        facts.extend([\n            {\"title\": \"\ud83d\udcca Pass Rate\", \"value\": f\"{gd.get('pass_rate', 0):.1%}\"},\n            {\"title\": \"\ud83c\udfaf Required\", \"value\": f\"{gd.get('required_rate', 0.95):.1%}\"},\n            {\"title\": \"\u274c Rows Failed\", \"value\": f\"{gd.get('failed_rows', 0):,}\"},\n        ])\n\n    body_items.append({\"type\": \"Container\", \"items\": [{\"type\": \"FactSet\", \"facts\": facts}]})\n\n    # NEW: Action button for story\n    story_url = context.get(\"story_url\")\n    if story_url:\n        body_items.append({\n            \"type\": \"ActionSet\",\n            \"actions\": [\n                {\n                    \"type\": \"Action.OpenUrl\",\n                    \"title\": \"\ud83d\udcca View Story\",\n                    \"url\": story_url,\n                    \"style\": \"positive\"\n                }\n            ]\n        })\n\n    # ... rest of card construction ...\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#alert-throttling","title":"Alert Throttling","text":"<pre><code># odibi/utils/alerting.py\n\nclass AlertThrottler:\n    \"\"\"Prevent alert spam by throttling repeated alerts.\"\"\"\n\n    def __init__(self):\n        self._last_alerts: Dict[str, datetime] = {}\n        self._alert_counts: Dict[str, int] = {}\n\n    def should_send(\n        self,\n        alert_key: str,\n        throttle_minutes: int = 15,\n        max_per_hour: int = 10\n    ) -&gt; bool:\n        \"\"\"Check if alert should be sent based on throttling rules.\"\"\"\n\n        now = datetime.utcnow()\n        last = self._last_alerts.get(alert_key)\n\n        # Check time-based throttle\n        if last and (now - last).total_seconds() &lt; throttle_minutes * 60:\n            logger.debug(f\"Alert throttled: {alert_key} (within {throttle_minutes}m)\")\n            return False\n\n        # Check rate limit\n        hour_key = f\"{alert_key}:{now.strftime('%Y%m%d%H')}\"\n        count = self._alert_counts.get(hour_key, 0)\n        if count &gt;= max_per_hour:\n            logger.debug(f\"Alert rate-limited: {alert_key} ({count}/{max_per_hour} per hour)\")\n            return False\n\n        # Update tracking\n        self._last_alerts[alert_key] = now\n        self._alert_counts[hour_key] = count + 1\n\n        return True\n\n\n# Global throttler instance\n_throttler = AlertThrottler()\n\n\ndef send_alert(config: AlertConfig, message: str, context: Dict[str, Any]) -&gt; None:\n    \"\"\"Send alert with throttling.\"\"\"\n\n    # Build throttle key\n    pipeline = context.get(\"pipeline\", \"unknown\")\n    event = context.get(\"event_type\", \"unknown\")\n    throttle_key = f\"{pipeline}:{event}\"\n\n    # Check throttling (configurable per alert)\n    throttle_minutes = config.metadata.get(\"throttle_minutes\", 15)\n    if not _throttler.should_send(throttle_key, throttle_minutes):\n        return\n\n    # ... existing send logic ...\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#yaml-config-for-enhanced-alerting","title":"YAML Config for Enhanced Alerting","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15  # Don't repeat same alert within 15 min\n      channel: \"#data-alerts\"\n\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n      - on_threshold_breach\n    metadata:\n      throttle_minutes: 30\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#4-schema-version-tracking","title":"4. Schema Version Tracking","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem_2","title":"Problem","text":"<p>Schema changes between runs are shown in stories but not tracked historically in the catalog.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution_2","title":"Solution","text":"<p>Store schema snapshots in <code>meta_schemas</code> table for audit trail.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#catalog-schema","title":"Catalog Schema","text":"<pre><code># odibi/catalog.py\n\nSCHEMA_TABLE_SCHEMA = StructType([\n    StructField(\"table_path\", StringType(), False),      # e.g., \"silver/customers\"\n    StructField(\"schema_version\", LongType(), False),    # Auto-incrementing\n    StructField(\"schema_hash\", StringType(), False),     # MD5 of column definitions\n    StructField(\"columns\", StringType(), False),         # JSON: {\"col\": \"type\", ...}\n    StructField(\"captured_at\", TimestampType(), False),\n    StructField(\"pipeline\", StringType(), True),\n    StructField(\"node\", StringType(), True),\n    StructField(\"run_id\", StringType(), True),\n    StructField(\"columns_added\", ArrayType(StringType()), True),\n    StructField(\"columns_removed\", ArrayType(StringType()), True),\n    StructField(\"columns_type_changed\", ArrayType(StringType()), True),\n])\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation_2","title":"Implementation","text":"<pre><code># odibi/catalog.py\n\ndef track_schema(\n    self,\n    table_path: str,\n    schema: Dict[str, str],\n    pipeline: str,\n    node: str,\n    run_id: str\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Track schema version for a table.\n\n    Returns:\n        Dict with version info and detected changes\n    \"\"\"\n    schema_hash = self._hash_schema(schema)\n\n    # Get previous version\n    previous = self._get_latest_schema(table_path)\n\n    if previous and previous[\"schema_hash\"] == schema_hash:\n        # No change\n        return {\"changed\": False, \"version\": previous[\"schema_version\"]}\n\n    # Detect changes\n    changes = {}\n    if previous:\n        prev_cols = json.loads(previous[\"columns\"])\n        changes = {\n            \"columns_added\": list(set(schema.keys()) - set(prev_cols.keys())),\n            \"columns_removed\": list(set(prev_cols.keys()) - set(schema.keys())),\n            \"columns_type_changed\": [\n                col for col in schema\n                if col in prev_cols and schema[col] != prev_cols[col]\n            ]\n        }\n        new_version = previous[\"schema_version\"] + 1\n    else:\n        new_version = 1\n\n    # Insert new version\n    record = {\n        \"table_path\": table_path,\n        \"schema_version\": new_version,\n        \"schema_hash\": schema_hash,\n        \"columns\": json.dumps(schema),\n        \"captured_at\": datetime.utcnow(),\n        \"pipeline\": pipeline,\n        \"node\": node,\n        \"run_id\": run_id,\n        **changes\n    }\n\n    self._append_to_table(\"meta_schemas\", record)\n\n    return {\n        \"changed\": True,\n        \"version\": new_version,\n        \"previous_version\": previous[\"schema_version\"] if previous else None,\n        **changes\n    }\n\n\ndef get_schema_history(\n    self,\n    table_path: str,\n    limit: int = 10\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get schema version history for a table.\"\"\"\n\n    return self._query_table(\n        \"meta_schemas\",\n        filter=f\"table_path = '{table_path}'\",\n        order_by=\"schema_version DESC\",\n        limit=limit\n    )\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#cli-command","title":"CLI Command","text":"<pre><code># Show schema history\n$ odibi schema history silver/customers\n\nVersion  Captured At          Changes\n-------  -------------------  ----------------------------------------\nv5       2024-01-30 10:15:00  +loyalty_tier (new column)\nv4       2024-01-15 08:30:00  email: VARCHAR\u2192STRING (type change)\nv3       2024-01-01 12:00:00  -legacy_id (removed)\nv2       2023-12-15 09:00:00  +created_at, +updated_at\nv1       2023-12-01 10:00:00  Initial schema (12 columns)\n\n# Compare two versions\n$ odibi schema diff silver/customers --from v3 --to v5\n\n  customer_id    STRING     (unchanged)\n  email          STRING     (unchanged)\n- legacy_id      STRING     (removed in v3)\n+ loyalty_tier   STRING     (added in v5)\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#5-cross-pipeline-lineage","title":"5. Cross-Pipeline Lineage","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem_3","title":"Problem","text":"<p>Lineage is tracked within a single pipeline. No visibility into how pipelines connect (e.g., Bronze pipeline feeds Silver pipeline).</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution_3","title":"Solution","text":"<p>Track table-level lineage in the catalog, linking pipelines through shared tables.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#catalog-schema_1","title":"Catalog Schema","text":"<pre><code># odibi/catalog.py\n\nLINEAGE_TABLE_SCHEMA = StructType([\n    StructField(\"source_table\", StringType(), False),    # Full path\n    StructField(\"target_table\", StringType(), False),\n    StructField(\"source_pipeline\", StringType(), True),\n    StructField(\"source_node\", StringType(), True),\n    StructField(\"target_pipeline\", StringType(), True),\n    StructField(\"target_node\", StringType(), True),\n    StructField(\"relationship\", StringType(), False),    # \"feeds\" | \"derived_from\"\n    StructField(\"last_observed\", TimestampType(), False),\n    StructField(\"run_id\", StringType(), True),\n])\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation_3","title":"Implementation","text":"<pre><code># odibi/lineage.py (extend existing)\n\nclass LineageTracker:\n    \"\"\"Track cross-pipeline lineage relationships.\"\"\"\n\n    def __init__(self, catalog: CatalogManager):\n        self.catalog = catalog\n\n    def record_lineage(\n        self,\n        read_config: Optional[ReadConfig],\n        write_config: Optional[WriteConfig],\n        pipeline: str,\n        node: str,\n        run_id: str,\n        connections: Dict[str, Any]\n    ):\n        \"\"\"Record lineage from node's read/write config.\"\"\"\n\n        if not write_config:\n            return  # No output = no lineage to record\n\n        target_table = self._resolve_table_path(write_config, connections)\n\n        # Record read source -&gt; write target\n        if read_config:\n            source_table = self._resolve_table_path(read_config, connections)\n            self._upsert_lineage(\n                source_table=source_table,\n                target_table=target_table,\n                target_pipeline=pipeline,\n                target_node=node,\n                run_id=run_id\n            )\n\n        # Record depends_on -&gt; write target (if depends_on reads from another pipeline's output)\n        # This is discovered by matching table paths\n\n    def get_upstream(self, table_path: str, depth: int = 3) -&gt; List[Dict]:\n        \"\"\"Get all upstream sources for a table.\"\"\"\n\n        upstream = []\n        visited = set()\n        queue = [(table_path, 0)]\n\n        while queue:\n            current, level = queue.pop(0)\n            if current in visited or level &gt; depth:\n                continue\n            visited.add(current)\n\n            sources = self.catalog.query(\n                \"meta_lineage\",\n                filter=f\"target_table = '{current}'\"\n            )\n\n            for source in sources:\n                upstream.append({**source, \"depth\": level})\n                queue.append((source[\"source_table\"], level + 1))\n\n        return upstream\n\n    def get_downstream(self, table_path: str, depth: int = 3) -&gt; List[Dict]:\n        \"\"\"Get all downstream consumers of a table.\"\"\"\n\n        downstream = []\n        visited = set()\n        queue = [(table_path, 0)]\n\n        while queue:\n            current, level = queue.pop(0)\n            if current in visited or level &gt; depth:\n                continue\n            visited.add(current)\n\n            targets = self.catalog.query(\n                \"meta_lineage\",\n                filter=f\"source_table = '{current}'\"\n            )\n\n            for target in targets:\n                downstream.append({**target, \"depth\": level})\n                queue.append((target[\"target_table\"], level + 1))\n\n        return downstream\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#cli-commands","title":"CLI Commands","text":"<pre><code># Trace upstream lineage\n$ odibi lineage upstream gold.customer_360\n\ngold.customer_360\n\u2514\u2500\u2500 silver.dim_customers (silver_pipeline.dedupe_customers)\n    \u2514\u2500\u2500 bronze.customers_raw (bronze_pipeline.ingest_customers)\n        \u2514\u2500\u2500 [external] azure_sql.dbo.Customers\n\n# Trace downstream lineage\n$ odibi lineage downstream bronze.customers_raw\n\nbronze.customers_raw\n\u251c\u2500\u2500 silver.dim_customers (silver_pipeline.dedupe_customers)\n\u2502   \u251c\u2500\u2500 gold.customer_360 (gold_pipeline.build_360)\n\u2502   \u2514\u2500\u2500 gold.churn_features (gold_pipeline.churn_model)\n\u2514\u2500\u2500 silver.customer_events (silver_pipeline.process_events)\n\n# Impact analysis\n$ odibi lineage impact bronze.customers_raw --if-schema-changes\n\n\u26a0\ufe0f  Schema change to bronze.customers_raw would affect:\n  - silver.dim_customers (2 pipelines depend on this)\n  - gold.customer_360 (production dashboard)\n  - gold.churn_features (ML model input)\n\n  Total: 3 downstream tables in 2 pipelines\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#story-integration","title":"Story Integration","text":"<pre><code>&lt;!-- In run_story.html, add cross-pipeline lineage section --&gt;\n\n{% if metadata.cross_pipeline_lineage %}\n&lt;div class=\"node-card\" style=\"padding: 20px;\"&gt;\n    &lt;h3&gt;\ud83d\udd17 Cross-Pipeline Lineage&lt;/h3&gt;\n    &lt;div class=\"mermaid\"&gt;\n    graph LR\n        subgraph \"This Pipeline\"\n            {% for node in metadata.nodes %}\n            {{ node.node_name }}\n            {% endfor %}\n        end\n\n        subgraph \"Upstream Pipelines\"\n            {% for source in metadata.upstream_sources %}\n            {{ source.pipeline }}.{{ source.node }}\n            {% endfor %}\n        end\n\n        subgraph \"Downstream Pipelines\"\n            {% for target in metadata.downstream_targets %}\n            {{ target.pipeline }}.{{ target.node }}\n            {% endfor %}\n        end\n\n        {% for link in metadata.cross_pipeline_links %}\n        {{ link.source }} --&gt; {{ link.target }}\n        {% endfor %}\n    &lt;/div&gt;\n&lt;/div&gt;\n{% endif %}\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation-order","title":"Implementation Order","text":"<pre><code>Week 1:\n\u251c\u2500\u2500 Day 1-2: Quarantine Tables\n\u2502   \u251c\u2500\u2500 Config models\n\u2502   \u251c\u2500\u2500 split_valid_invalid()\n\u2502   \u251c\u2500\u2500 Quarantine metadata columns\n\u2502   \u251c\u2500\u2500 Integration with node.py\n\u2502   \u2514\u2500\u2500 Tests\n\u2502\n\u251c\u2500\u2500 Day 3: Quality Gates\n\u2502   \u251c\u2500\u2500 Gate config model\n\u2502   \u251c\u2500\u2500 evaluate_gate()\n\u2502   \u251c\u2500\u2500 Row count anomaly detection\n\u2502   \u251c\u2500\u2500 Integration with node.py\n\u2502   \u2514\u2500\u2500 Tests\n\nWeek 2:\n\u251c\u2500\u2500 Day 4: Alerting Enhancements\n\u2502   \u251c\u2500\u2500 Slack parity (timestamp, colors, action buttons)\n\u2502   \u251c\u2500\u2500 New alert events (quarantine, gate_block)\n\u2502   \u251c\u2500\u2500 Event-specific payloads\n\u2502   \u251c\u2500\u2500 Throttling\n\u2502   \u2514\u2500\u2500 Tests\n\u2502\n\u251c\u2500\u2500 Day 5 (half): Schema Version Tracking\n\u2502   \u251c\u2500\u2500 meta_schemas table\n\u2502   \u251c\u2500\u2500 track_schema() method\n\u2502   \u251c\u2500\u2500 CLI: odibi schema history\n\u2502   \u2514\u2500\u2500 Tests\n\u2502\n\u251c\u2500\u2500 Day 5-7: Cross-Pipeline Lineage\n\u2502   \u251c\u2500\u2500 meta_lineage table\n\u2502   \u251c\u2500\u2500 LineageTracker class\n\u2502   \u251c\u2500\u2500 get_upstream() / get_downstream()\n\u2502   \u251c\u2500\u2500 CLI commands\n\u2502   \u251c\u2500\u2500 Story integration\n\u2502   \u2514\u2500\u2500 Tests\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#success-criteria","title":"Success Criteria","text":"Enhancement Success Criteria Quarantine Bad rows written to quarantine table with metadata; valid rows continue Gates Pipeline aborts when pass rate below threshold; alert sent Alerting Slack/Teams parity; event-specific details; throttling works Schema Tracking Schema history queryable; changes detected between runs Lineage Can trace 3 levels up/down across pipelines; visualized in stories"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#dependencies","title":"Dependencies","text":"<ul> <li>No external dependencies required</li> <li>All features build on existing infrastructure</li> <li>Backward compatible (new config fields are optional)</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/","title":"Spark Structured Streaming Support Plan","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#overview","title":"Overview","text":"<p>This document outlines the plan to fully support Spark Structured Streaming in Odibi. Currently, the <code>streaming</code> config flag creates a streaming DataFrame but the rest of the pipeline isn't designed to handle it.</p>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#current-state","title":"Current State","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#what-works","title":"What Works","text":"<ul> <li><code>SparkEngine.read()</code> uses <code>spark.readStream</code> when <code>streaming=True</code></li> <li>Merge transformer has <code>foreachBatch</code> support for streaming sources</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#whats-broken","title":"What's Broken","text":"<ol> <li>Row counting fails - <code>df.count()</code> doesn't work on streaming DataFrames</li> <li>No streaming write - <code>engine.write()</code> doesn't handle streaming DataFrames</li> <li>No checkpoint configuration - Streaming requires checkpoint locations</li> <li>No trigger configuration - No way to specify processing intervals</li> <li>No output mode configuration - append/update/complete modes not exposed</li> <li>Validation/contracts fail - Can't run validations on streaming data</li> <li>Sample collection fails - Can't get samples from streaming DataFrames</li> </ol>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#11-add-streamingconfig-to-configpy","title":"1.1 Add StreamingConfig to config.py","text":"<pre><code>class StreamingConfig(BaseModel):\n    \"\"\"Configuration for Spark Structured Streaming.\"\"\"\n\n    enabled: bool = Field(\n        default=False,\n        description=\"Enable streaming mode for this node\"\n    )\n    checkpoint_location: Optional[str] = Field(\n        default=None,\n        description=\"Path for streaming checkpoints. Required for fault tolerance.\"\n    )\n    trigger: Optional[str] = Field(\n        default=None,\n        description=\"Trigger interval. Options: 'once', '10 seconds', '1 minute', 'availableNow'\"\n    )\n    output_mode: Literal[\"append\", \"update\", \"complete\"] = Field(\n        default=\"append\",\n        description=\"Output mode for streaming writes\"\n    )\n    watermark: Optional[str] = Field(\n        default=None,\n        description=\"Watermark column and delay, e.g., 'event_time, 10 minutes'\"\n    )\n    processing_time: Optional[str] = Field(\n        default=None,\n        description=\"Processing time trigger, e.g., '10 seconds'\"\n    )\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#12-move-streaming-from-readconfig-to-nodeconfig","title":"1.2 Move streaming from ReadConfig to NodeConfig","text":"<p>The <code>streaming</code> flag should be at the node level since it affects the entire execution flow:</p> <pre><code>nodes:\n  - name: ingest_events\n    streaming:\n      enabled: true\n      checkpoint_location: \"/checkpoints/ingest_events\"\n      trigger: \"10 seconds\"\n      output_mode: append\n    read:\n      connection: kafka\n      format: kafka\n      options:\n        subscribe: events_topic\n    write:\n      connection: silver\n      format: delta\n      table: events_stream\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#13-update-nodeexecutor-to-detect-streaming-mode","title":"1.3 Update NodeExecutor to detect streaming mode","text":"<pre><code>def execute(self, config: NodeConfig, ...):\n    is_streaming = config.streaming and config.streaming.enabled\n\n    if is_streaming:\n        return self._execute_streaming(config, ...)\n    else:\n        return self._execute_batch(config, ...)\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-2-streaming-aware-execution","title":"Phase 2: Streaming-Aware Execution","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#21-skip-incompatible-operations-for-streaming","title":"2.1 Skip incompatible operations for streaming","text":"Operation Streaming Behavior Row counting Skip (return None) Sample collection Skip Contracts/validation Skip or use foreachBatch Schema capture Use schema from DataFrame HWM updates Not applicable (Kafka offsets managed by checkpoints)"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#22-add-streaming-write-to-sparkengine","title":"2.2 Add streaming write to SparkEngine","text":"<pre><code>def write(self, df, connection, format, ..., streaming_config=None):\n    if df.isStreaming:\n        if not streaming_config:\n            raise ValueError(\"Streaming DataFrame requires streaming_config\")\n\n        writer = df.writeStream \\\n            .format(format) \\\n            .outputMode(streaming_config.output_mode) \\\n            .option(\"checkpointLocation\", streaming_config.checkpoint_location)\n\n        if streaming_config.trigger == \"once\":\n            writer = writer.trigger(once=True)\n        elif streaming_config.trigger == \"availableNow\":\n            writer = writer.trigger(availableNow=True)\n        elif streaming_config.trigger:\n            writer = writer.trigger(processingTime=streaming_config.trigger)\n\n        if table:\n            query = writer.toTable(table)\n        else:\n            query = writer.start(path)\n\n        return {\"streaming_query\": query}\n    else:\n        # existing batch logic\n        ...\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#23-handle-streaming-query-lifecycle","title":"2.3 Handle streaming query lifecycle","text":"<pre><code>def _execute_streaming(self, config, ...):\n    # Read (returns streaming DataFrame)\n    df = self._execute_read_phase(config, ...)\n\n    # Transform (must be streaming-compatible)\n    df = self._execute_transform_phase(config, df, ...)\n\n    # Write (starts streaming query)\n    query_info = self._execute_write_phase(config, df, ...)\n\n    # Return query handle for management\n    return NodeResult(\n        node_name=config.name,\n        success=True,\n        metadata={\n            \"streaming\": True,\n            \"query_id\": query_info[\"streaming_query\"].id,\n            \"query_name\": query_info[\"streaming_query\"].name,\n        }\n    )\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-3-streaming-sources-sinks","title":"Phase 3: Streaming Sources &amp; Sinks","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#31-kafka-connection-type","title":"3.1 Kafka Connection Type","text":"<pre><code>class KafkaConnection(BaseConnection):\n    \"\"\"Kafka connection for streaming.\"\"\"\n\n    bootstrap_servers: str\n    security_protocol: str = \"PLAINTEXT\"\n    sasl_mechanism: Optional[str] = None\n    sasl_username: Optional[str] = None\n    sasl_password: Optional[str] = None\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        return {\n            \"kafka.bootstrap.servers\": self.bootstrap_servers,\n            \"kafka.security.protocol\": self.security_protocol,\n            ...\n        }\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#32-event-hubs-connection-type","title":"3.2 Event Hubs Connection Type","text":"<pre><code>class EventHubsConnection(BaseConnection):\n    \"\"\"Azure Event Hubs connection for streaming.\"\"\"\n\n    connection_string: str\n    consumer_group: str = \"$Default\"\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        return {\n            \"eventhubs.connectionString\": self.connection_string,\n            \"eventhubs.consumerGroup\": self.consumer_group,\n        }\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#33-supported-streaming-formats","title":"3.3 Supported Streaming Formats","text":"Source Read Write Kafka \u2713 \u2713 Event Hubs \u2713 \u2713 Delta \u2713 \u2713 Rate (testing) \u2713 - File (continuous) \u2713 \u2713"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-4-streaming-transforms","title":"Phase 4: Streaming Transforms","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#41-watermarking-support","title":"4.1 Watermarking Support","text":"<pre><code>nodes:\n  - name: windowed_aggregation\n    streaming:\n      enabled: true\n      watermark: \"event_time, 10 minutes\"\n    transform:\n      steps:\n        - type: sql\n          query: |\n            SELECT\n              window(event_time, '5 minutes') as window,\n              COUNT(*) as event_count\n            FROM {input}\n            GROUP BY window(event_time, '5 minutes')\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#42-streaming-compatible-transforms","title":"4.2 Streaming-Compatible Transforms","text":"Transform Streaming Support SQL (stateless) \u2713 SQL (windowed aggregation) \u2713 with watermark filter_rows \u2713 derive_columns \u2713 join (stream-static) \u2713 join (stream-stream) \u2713 with watermark deduplicate \u2713 with watermark aggregate \u2713 with watermark"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#43-incompatible-transforms-fail-fast","title":"4.3 Incompatible Transforms (Fail Fast)","text":"<ul> <li><code>sort</code> (requires complete data)</li> <li><code>limit</code> (requires complete data)</li> <li><code>distinct</code> without watermark</li> <li>Any transform requiring <code>.collect()</code></li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-5-monitoring-management","title":"Phase 5: Monitoring &amp; Management","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#51-query-status-tracking","title":"5.1 Query Status Tracking","text":"<pre><code>class StreamingQueryManager:\n    \"\"\"Manage active streaming queries.\"\"\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def list_queries(self) -&gt; List[Dict]:\n        return [\n            {\n                \"id\": q.id,\n                \"name\": q.name,\n                \"status\": q.status,\n                \"recent_progress\": q.recentProgress,\n            }\n            for q in self.spark.streams.active\n        ]\n\n    def stop_query(self, query_id: str):\n        for q in self.spark.streams.active:\n            if q.id == query_id:\n                q.stop()\n                return True\n        return False\n\n    def await_termination(self, query_id: str, timeout: Optional[int] = None):\n        for q in self.spark.streams.active:\n            if q.id == query_id:\n                q.awaitTermination(timeout)\n                return\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#52-cli-commands","title":"5.2 CLI Commands","text":"<pre><code># List active streaming queries\nodibi streaming list\n\n# Stop a streaming query\nodibi streaming stop &lt;query-id&gt;\n\n# Show query progress\nodibi streaming status &lt;query-id&gt;\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#53-metrics-integration","title":"5.3 Metrics Integration","text":"<pre><code># OpenTelemetry metrics for streaming\nstreaming_records_processed = meter.create_counter(\n    \"odibi.streaming.records_processed\",\n    description=\"Records processed by streaming query\"\n)\n\nstreaming_batch_duration = meter.create_histogram(\n    \"odibi.streaming.batch_duration_ms\",\n    description=\"Duration of each micro-batch\"\n)\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-6-testing","title":"Phase 6: Testing","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#61-unit-tests","title":"6.1 Unit Tests","text":"<ul> <li>Test streaming config validation</li> <li>Test checkpoint path generation</li> <li>Test trigger parsing</li> <li>Mock streaming DataFrame handling</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#62-integration-tests-spark-required","title":"6.2 Integration Tests (Spark Required)","text":"<ul> <li>Rate source \u2192 Delta sink</li> <li>File source \u2192 File sink (continuous)</li> <li>Watermark and windowed aggregation</li> <li>Stream-static join</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#63-e2e-tests-kafka-required","title":"6.3 E2E Tests (Kafka Required)","text":"<ul> <li>Kafka \u2192 Delta pipeline</li> <li>Event Hubs \u2192 Delta pipeline</li> <li>Exactly-once semantics verification</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#implementation-order","title":"Implementation Order","text":"Phase Effort Priority Dependencies 1.1 StreamingConfig 2h P0 None 1.2 Move to NodeConfig 1h P0 1.1 1.3 Detect streaming mode 2h P0 1.2 2.1 Skip incompatible ops 2h P0 1.3 2.2 Streaming write 4h P0 2.1 2.3 Query lifecycle 3h P0 2.2 3.1 Kafka connection 3h P1 2.3 3.2 Event Hubs connection 2h P1 2.3 4.1 Watermarking 2h P1 2.3 4.2 Transform compatibility 4h P1 4.1 5.1 Query manager 3h P2 2.3 5.2 CLI commands 2h P2 5.1 5.3 Metrics 2h P2 5.1 6.x Testing 8h P1 All <p>Total Estimate: ~40 hours</p>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#example-complete-streaming-pipeline","title":"Example: Complete Streaming Pipeline","text":"<pre><code># project.yaml\nconnections:\n  kafka:\n    type: kafka\n    bootstrap_servers: \"localhost:9092\"\n\n  silver:\n    type: databricks\n    catalog: main\n    schema: silver\n\n# pipeline.yaml\npipelines:\n  - pipeline: realtime_events\n    description: \"Process events in real-time from Kafka to Delta\"\n\n    nodes:\n      - name: ingest_events\n        description: \"Ingest raw events from Kafka\"\n        streaming:\n          enabled: true\n          checkpoint_location: \"abfss://checkpoints/ingest_events\"\n          trigger: \"10 seconds\"\n          output_mode: append\n          watermark: \"event_time, 5 minutes\"\n\n        read:\n          connection: kafka\n          format: kafka\n          options:\n            subscribe: raw_events\n            startingOffsets: earliest\n\n        transform:\n          steps:\n            - type: sql\n              query: |\n                SELECT\n                  CAST(key AS STRING) as event_key,\n                  CAST(value AS STRING) as event_json,\n                  timestamp as kafka_timestamp,\n                  from_json(CAST(value AS STRING), 'event_type STRING, event_time TIMESTAMP, payload STRING') as parsed\n                FROM {input}\n\n            - type: derive\n              columns:\n                event_type: \"parsed.event_type\"\n                event_time: \"parsed.event_time\"\n                payload: \"parsed.payload\"\n\n        write:\n          connection: silver\n          format: delta\n          table: events_stream\n          partition_by: [\"event_type\"]\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#open-questions","title":"Open Questions","text":"<ol> <li>Should streaming nodes run indefinitely or support batch-like \"run once\"?</li> <li>Spark supports <code>trigger(once=True)</code> for processing all available data then stopping</li> <li> <p>Useful for testing and scheduled streaming jobs</p> </li> <li> <p>How to handle streaming in the Pipeline orchestrator?</p> </li> <li>Streaming nodes don't \"complete\" in the traditional sense</li> <li> <p>May need separate <code>run_streaming()</code> method</p> </li> <li> <p>Should we support foreachBatch for custom sinks?</p> </li> <li>Enables writing to non-streaming sinks (JDBC, etc.)</li> <li> <p>More complex but more flexible</p> </li> <li> <p>How to expose streaming metrics in Stories?</p> </li> <li>Streaming runs continuously, no single \"run\" to report on</li> <li>May need real-time dashboard integration</li> </ol>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#references","title":"References","text":"<ul> <li>Spark Structured Streaming Guide</li> <li>Delta Lake Streaming</li> <li>Kafka + Spark Integration</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/","title":"System Catalog Integration Plan","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#overview","title":"Overview","text":"<p>This plan addresses gaps in the system catalog integration to achieve the goal: \"Declare YAML, run pipeline, everything works automatically.\"</p> <p>Currently only <code>meta_state</code> works correctly. All other system tables have integration gaps.</p>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-1-fix-critical-bugs-done","title":"Phase 1: Fix Critical Bugs (Done)","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#11-fix-state_manager-in-nodeexecutor","title":"1.1 Fix state_manager in NodeExecutor \u2705","text":"<ul> <li>Issue: <code>state_manager</code> wasn't passed to <code>NodeExecutor</code>, breaking <code>skip_if_unchanged</code></li> <li>Fix: Added <code>state_manager</code> parameter to <code>NodeExecutor.__init__</code> and pass it from <code>Node</code> class</li> <li>Status: Completed this session, needs commit</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-2-auto-registration-on-run","title":"Phase 2: Auto-Registration on Run","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#21-auto-register-pipelines-and-nodes","title":"2.1 Auto-register pipelines and nodes","text":"<ul> <li>Location: <code>PipelineManager.run()</code></li> <li>Action: Before executing, call <code>register_pipeline()</code> and <code>register_node()</code> for each pipeline/node being run</li> <li>Behavior: Idempotent upsert - safe to call on every run</li> <li>Result: <code>meta_pipelines</code> and <code>meta_nodes</code> populated automatically</li> </ul> <pre><code>def run(self, pipelines=None, ...):\n    # Auto-register before execution\n    if self.catalog_manager:\n        for name in pipeline_names:\n            config = self._pipelines[name].config\n            self.catalog_manager.register_pipeline(config, self.project_config)\n            for node in config.nodes:\n                self.catalog_manager.register_node(config.pipeline, node)\n\n    # ... rest of run logic\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-3-wire-catalog-logging-into-pipeline-flow","title":"Phase 3: Wire Catalog Logging into Pipeline Flow","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#31-log-runs-from-pipelinenodeexecutor","title":"3.1 Log runs from Pipeline/NodeExecutor","text":"<ul> <li>Location: <code>Pipeline._execute_node()</code> or <code>NodeExecutor</code> after node completion</li> <li>Action: Call <code>catalog_manager.log_run()</code> with execution results</li> <li>Result: <code>meta_runs</code> populated from YAML flow (not just <code>Node</code> class)</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#32-register-assets-on-write","title":"3.2 Register assets on write","text":"<ul> <li>Location: <code>NodeExecutor._execute_write_phase()</code> after successful write</li> <li>Action: Call <code>catalog_manager.register_asset()</code> with table details</li> <li>Result: <code>meta_tables</code> populated automatically</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#33-track-schema-changes","title":"3.3 Track schema changes","text":"<ul> <li>Location: <code>NodeExecutor._execute_write_phase()</code> after write</li> <li>Action:</li> <li>Get current schema from written data</li> <li>Call <code>catalog_manager.track_schema()</code> if schema differs from previous</li> <li>Result: <code>meta_schemas</code> tracks schema evolution</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#34-log-pattern-usage","title":"3.4 Log pattern usage","text":"<ul> <li>Location: <code>Pipeline._execute_node()</code> when pattern-based nodes execute</li> <li>Action: Call <code>catalog_manager.log_pattern()</code> with pattern config</li> <li>Result: <code>meta_patterns</code> populated</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#35-record-lineage","title":"3.5 Record lineage","text":"<ul> <li>Location: <code>Pipeline._execute_node()</code> after node completion</li> <li>Action: Extract source/target from node config, call <code>record_lineage()</code></li> <li>Result: <code>meta_lineage</code> populated automatically</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#36-create-and-log-metrics","title":"3.6 Create and log metrics","text":"<ul> <li>Location: <code>CatalogManager</code> + <code>Pipeline</code></li> <li>Action:</li> <li>Create <code>log_metrics()</code> method in <code>CatalogManager</code></li> <li>Call from Pipeline with execution stats</li> <li>Result: <code>meta_metrics</code> populated</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-4-cleanupremoval-methods","title":"Phase 4: Cleanup/Removal Methods","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#41-remove-pipeline","title":"4.1 Remove pipeline","text":"<pre><code>def remove_pipeline(self, pipeline_name: str) -&gt; int:\n    \"\"\"Remove pipeline and cascade to nodes, state entries.\n    Returns count of deleted entries.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#42-remove-node","title":"4.2 Remove node","text":"<pre><code>def remove_node(self, pipeline_name: str, node_name: str) -&gt; int:\n    \"\"\"Remove node and associated state entries.\n    Returns count of deleted entries.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#43-cleanup-orphans","title":"4.3 Cleanup orphans","text":"<pre><code>def cleanup_orphans(self, current_config: ProjectConfig) -&gt; Dict[str, int]:\n    \"\"\"Compare catalog against current config, remove stale entries.\n    Returns dict of {table: deleted_count}.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#44-clear-state","title":"4.4 Clear state","text":"<pre><code>def clear_state(self, key_pattern: str) -&gt; int:\n    \"\"\"Remove state entries matching pattern (supports wildcards).\n    Returns count of deleted entries.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-5-listquery-methods-on-pipelinemanager","title":"Phase 5: List/Query Methods on PipelineManager","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#51-list-methods","title":"5.1 List methods","text":"<pre><code>def list_pipelines(self) -&gt; pd.DataFrame:\n    \"\"\"List all registered pipelines.\"\"\"\n\ndef list_nodes(self, pipeline: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"List nodes, optionally filtered by pipeline.\"\"\"\n\ndef list_runs(\n    self,\n    pipeline: Optional[str] = None,\n    node: Optional[str] = None,\n    status: Optional[str] = None,\n    limit: int = 10\n) -&gt; pd.DataFrame:\n    \"\"\"List recent runs with optional filters.\"\"\"\n\ndef list_tables(self) -&gt; pd.DataFrame:\n    \"\"\"List registered assets from meta_tables.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#52-state-methods","title":"5.2 State methods","text":"<pre><code>def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get specific state entry (HWM, content hash, etc.).\"\"\"\n\ndef get_all_state(self, prefix: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"Get all state entries, optionally filtered by key prefix.\"\"\"\n\ndef clear_state(self, key: str) -&gt; bool:\n    \"\"\"Remove a state entry. Returns True if deleted.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#53-schemalineage-methods","title":"5.3 Schema/Lineage methods","text":"<pre><code>def get_schema_history(\n    self,\n    table: str,  # Accepts friendly name, resolved automatically\n    limit: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"Get schema version history for a table.\"\"\"\n\ndef get_lineage(\n    self,\n    table: str,\n    direction: str = \"both\"  # \"upstream\", \"downstream\", \"both\"\n) -&gt; pd.DataFrame:\n    \"\"\"Get lineage for a table.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#54-statshealth-methods","title":"5.4 Stats/Health methods","text":"<pre><code>def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n    \"\"\"Get last run status, duration, timestamp.\"\"\"\n\ndef get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n    \"\"\"Get avg duration, row counts, success rate over period.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-6-smart-path-resolution","title":"Phase 6: Smart Path Resolution","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#61-implement-_resolve_table_path","title":"6.1 Implement <code>_resolve_table_path()</code>","text":"<p>All query methods accept user-friendly identifiers: - Relative path: <code>\"bronze/OEE/vw_OSMPerformanceOEE\"</code> - Registered table: <code>\"test.vw_OSMPerformanceOEE\"</code> - Node name: <code>\"opsvisdata_vw_OSMPerformanceOEE\"</code> - Full path: <code>\"abfss://...\"</code> (used as-is)</p> <pre><code>def _resolve_table_path(self, identifier: str) -&gt; str:\n    # 1. Full path - use as-is\n    if self._is_full_path(identifier):\n        return identifier\n\n    # 2. Check meta_tables for registered name\n    if self.catalog_manager:\n        resolved = self.catalog_manager.resolve_table_path(identifier)\n        if resolved:\n            return resolved\n\n    # 3. Check if it's a node name\n    for pipeline in self._pipelines.values():\n        for node in pipeline.config.nodes:\n            if node.name == identifier and node.write:\n                conn = self.connections[node.write.connection]\n                return conn.get_path(node.write.path or node.write.table)\n\n    # 4. Resolve using system connection as fallback\n    sys_conn = self.connections.get(self.project_config.system.connection)\n    return sys_conn.get_path(identifier) if sys_conn else identifier\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#implementation-order","title":"Implementation Order","text":"<ol> <li>Phase 2 - Auto-registration (quick win, enables other phases)</li> <li>Phase 3.1 - Log runs (most visible improvement)</li> <li>Phase 5.1-5.2 - List and state methods (immediate usability)</li> <li>Phase 6 - Path resolution (needed for other methods)</li> <li>Phase 3.2-3.5 - Remaining catalog logging</li> <li>Phase 4 - Cleanup methods</li> <li>Phase 5.3-5.4 - Advanced query methods</li> <li>Phase 3.6 - Metrics (lowest priority)</li> </ol>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#testing-requirements","title":"Testing Requirements","text":"<ul> <li>Unit tests for each new method</li> <li>Integration test: run pipeline, verify all 9 tables populated</li> <li>Test cleanup methods don't break active pipelines</li> <li>Test path resolution with all identifier formats</li> <li>Test with both Spark and Pandas engines</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#files-to-modify","title":"Files to Modify","text":"File Changes <code>odibi/pipeline.py</code> Auto-register, log_run, list methods, path resolution <code>odibi/node.py</code> Already fixed (state_manager), add register_asset/track_schema calls <code>odibi/catalog.py</code> Add remove_*, cleanup_orphans, log_metrics methods <code>tests/unit/test_pipeline_manager.py</code> New tests for list/query methods <code>tests/integration/test_catalog_integration.py</code> End-to-end catalog tests"},{"location":"roadmap/ease_of_life_proposals/","title":"Odibi Feature Specification: The \"Orgasmic\" Upgrade","text":"<p>2: 3: This document details the specification and implementation plan for the top 3 \"Ease of Life\" features. It is the primary reference for the implementation phase. 4: 5: --- 6: 7: ## 1. Validation (The \"Quality Gate\") \ud83d\udee1\ufe0f 8: 9: ### \ud83d\udca1 Concept 10: A declarative \"Validation Block\" on the Node level. Instead of writing manual SQL queries for common checks, users declare their intent. Odibi handles the execution, reporting, and alerting. 11: 12: ### \ud83d\udcdd User Story 13: &gt; \"As a Data Engineer, I want to ensure my 'Silver' tables never contain duplicate IDs or null timestamps, so that downstream reports are accurate. I want to define this in one line of config, not 20 lines of SQL.\" 14: 15: ### \u2699\ufe0f YAML Specification 16: <code>yaml 17: - name: \"dim_customers\" 18:   depends_on: [\"clean_customers\"] 19: 20:   validation: 21:     mode: \"fail\"          # Options: fail (stop pipeline), warn (log only) 22:     on_fail: \"alert\"      # Options: alert (send slack), ignore 23:     tests: 24:       # Built-in Test Types 25:       - type: \"not_null\" 26:         columns: [\"customer_id\", \"email\"] 27:   28:       - type: \"unique\" 29:         columns: [\"customer_id\"] 30:   31:       - type: \"accepted_values\" 32:         column: \"status\" 33:         values: [\"ACTIVE\", \"INACTIVE\", \"CHURNED\"] 34:   35:       - type: \"row_count\" 36:         min: 100 37:   38:       # Custom Logic 39:       - type: \"custom_sql\" 40:         name: \"age_sanity_check\" 41:         condition: \"age &gt;= 18 AND age &lt; 120\" 42:         threshold: 0.01   # Allow 1% failure rate before breaking 43:</code> 44: 45: ### \ud83c\udfd7\ufe0f Technical Implementation Plan 46: 1.  [x] Pydantic Models (<code>odibi/config.py</code>): 47:     *   Update <code>ValidationConfig</code> to support the new structure (<code>mode</code>, <code>on_fail</code>, <code>tests</code> list). 48:     *   Create <code>TestConfig</code> polymorphic model (discriminator on <code>type</code>). 49: 2.  [x] Validation Logic (<code>odibi/validation/engine.py</code>): 50:     *   Create a new <code>Validator</code> class. 51:     *   Implement methods for each test type (<code>check_not_null</code>, <code>check_unique</code>, etc.) that generate SQL/Pandas logic. 52:     *   Spark Optimization: Use <code>count(CASE WHEN ...)</code> to batch multiple checks into a single pass if possible. 53: 3.  [x] Node Integration (<code>odibi/node.py</code>): 54:     *   Update <code>_execute_validation_phase</code> to use the new <code>Validator</code>. 55:     *   Handle <code>mode: fail</code> vs <code>warn</code> logic. 56: 4.  [x] Reporting (<code>odibi/story/</code>): 57:     *   Capture validation results (pass/fail counts) in <code>NodeResult</code>. 58:     *   Render a \"Data Quality\" section in the Story HTML. 59: 60: --- 61: 62: ## 2. Incremental Loading (The \"Auto-Pilot\") \u2728 63: 64: ### \ud83d\udca1 Concept 65: \"Stateful\" High-Water-Mark (HWM) management. Odibi tracks the last processed state (timestamp or ID) in a persistent backend and automatically filters input data. 66: 67: ### \ud83d\udcdd User Story 68: &gt; \"As a Data Engineer, I want to ingest only new orders from Postgres into Delta Lake. I don't want to manually query the target table or manage a state file. I just want to say 'keep it in sync'.\" 69: 70: ### \u2699\ufe0f YAML Specification 71: <code>yaml 72: - name: \"orders_incremental\" 73:   read: 74:     connection: \"postgres_prod\" 75:     format: \"sql\" 76:     # The base query (Odibi appends the WHERE clause) 77:     query: \"SELECT * FROM public.orders\" 78:   79:     incremental: 80:       mode: \"stateful\"                  # The new magic mode 81:       key_column: \"updated_at\" 82:       fallback_column: \"created_at\"     # If updated_at is null, check this 83:       watermark_lag: \"2h\"               # Safety buffer (overlaps window) 84:       state_key: \"orders_prod_ingest\"   # Unique ID for state tracking 85:</code> 86: 87: ### \ud83c\udfd7\ufe0f Technical Implementation Plan 88: 1.  [x] State Backend (<code>odibi/state.py</code>): 89:     *   Done: <code>DeltaStateBackend</code> is ready. 90: 2.  [x] Config (<code>odibi/config.py</code>): 91:     *   Update <code>IncrementalConfig</code> to add <code>mode</code>, <code>state_key</code>, <code>watermark_lag</code>. 92: 3.  [x] Read Logic (<code>odibi/node.py</code> - <code>_execute_read</code>): 93:     *   Inject <code>StateManager</code>. 94:     *   Retrieve last HWM from <code>state_manager.get_last_run_state(key)</code>. 95:     *   Construct <code>WHERE</code> clause: <code>column &gt; last_hwm - lag</code>. 96:     *   Critical: After successful execution, update the State with the new <code>max(key_column)</code> from the fetched data. 97: 98: --- 99: 100: ## 3. Schema Management (The \"Drift Handler\") \ud83d\udee1\ufe0f 101: 102: ### \ud83d\udca1 Concept 103: Declarative handling of schema evolution. Instead of crashing on unknown columns, users define a policy (<code>enforce</code> vs <code>evolve</code>). 104: 105: ### \ud83d\udcdd User Story 106: &gt; \"As a Data Engineer, I work with a JSON API that frequently adds new fields. I want my 'Bronze' layer to automatically add these new columns to the table without crashing the pipeline.\" 107: 108: ### \u2699\ufe0f YAML Specification 109: <code>yaml 110: - name: \"web_events\" 111:   depends_on: [\"raw_json_logs\"] 112:   113:   schema: 114:     mode: \"evolve\"                    # Options: enforce, evolve 115:   116:     # What to do when input has columns NOT in target 117:     on_new_columns: \"add_nullable\"    # ignore, fail, add_nullable 118:   119:     # What to do when input is MISSING columns expected by target 120:     on_missing_columns: \"fill_null\"   # fail, fill_null 121:   122:     # Explicit expectations (optional) 123:     expected: 124:       event_id: { type: \"string\", nullable: false } 125:</code> 126: 127: ### \ud83c\udfd7\ufe0f Technical Implementation Plan 128: 1.  [x] Config (<code>odibi/config.py</code>): 129:     *   Add <code>SchemaPolicyConfig</code> model. 130: 2.  [x] Schema Logic (<code>odibi/engine/base.py</code> &amp; subclasses): 131:     *   Add <code>harmonize_schema(df, target_schema, policy)</code> method. 132:     *   Spark: Use <code>df.withColumn</code> to add missing cols. Use <code>mergeSchema</code> option for Delta writes. 133:     *   Pandas: Use <code>reindex</code> or direct assignment. 134: 3.  [x] Node Integration (<code>odibi/node.py</code>): 135:     *   Before Write Phase: Fetch target table schema (if exists). 136:     *   Compare <code>current_df.schema</code> vs <code>target_schema</code>. 137:     *   Apply harmonization logic based on policy. 138: 139: --- 140: 141: ## \ud83d\udd2e Future Scope: Other \"Ease of Life\" Features 142: 143: These are prioritized for subsequent iterations. 144: 145: *   \"Auto-Optimize\": <code>write.auto_optimize: true</code>. Automatically runs <code>OPTIMIZE</code> and <code>VACUUM</code> on Delta tables based on a \"reporting\" or \"ingest\" profile. 146: *   Privacy Suite: <code>privacy.anonymize: true</code>. Config-driven PII hashing/masking based on <code>ColumnMetadata</code>. 147: *   New Transformers: 148:     *   <code>normalize_json</code>: Deep flattening. 149:     *   <code>sessionize</code>: Window-based session creation. 150:     *   <code>geocode</code>: IP-to-Geo enrichment. 151: 152: --- 153: 154: ## \u2705 Validation Strategy 155: 156: For every feature above, the definition of \"Done\" includes: 157: 1.  Unit Tests: Verify logic in isolation (e.g., <code>test_validation_engine.py</code>). 158: 2.  Integration Tests: End-to-end pipeline run with a local Delta/DuckDB setup. 159: 3.  Story Check: Verify that the feature's activity is correctly logged in the Odibi Story (HTML report). 160: 161: --- 162:</p>"},{"location":"semantics/","title":"Semantic Layer","text":"<p>The Odibi Semantic Layer provides a unified interface for defining and querying business metrics. Define metrics once, query them across any dimension combination.</p>"},{"location":"semantics/#how-it-fits-into-odibi","title":"How It Fits Into Odibi","text":"<p>The semantic layer is a separate module from the core pipeline YAML. It's designed for:</p> <ol> <li>Ad-hoc metric queries via Python API</li> <li>Scheduled metric materialization to pre-compute aggregates</li> <li>Self-service analytics where business users query by metric name</li> </ol> <p>It does NOT replace the pipeline YAML - instead, it works alongside it: - Pipelines build your fact and dimension tables - The semantic layer queries those tables using metric definitions</p> <pre><code>flowchart TB\n    subgraph Pipeline[\"Odibi Pipelines (YAML)\"]\n        R[Read Sources]\n        D[Build Dimensions]\n        F[Build Facts]\n        A[Build Aggregates]\n    end\n\n    subgraph Data[\"Data Layer\"]\n        DIM[(dim_customer&lt;br&gt;dim_product&lt;br&gt;dim_date)]\n        FACT[(fact_orders)]\n        AGG[(agg_daily_sales)]\n    end\n\n    subgraph Semantic[\"Semantic Layer (Python API)\"]\n        M[Metric Definitions]\n        Q[SemanticQuery]\n        MAT[Materializer]\n    end\n\n    subgraph Output[\"Output\"]\n        DF[DataFrame Results]\n        TABLE[Materialized Tables]\n    end\n\n    R --&gt; D --&gt; DIM\n    R --&gt; F --&gt; FACT\n    FACT --&gt; A --&gt; AGG\n\n    DIM --&gt; M\n    FACT --&gt; M\n    M --&gt; Q --&gt; DF\n    M --&gt; MAT --&gt; TABLE\n\n    style Pipeline fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Semantic fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"semantics/#when-to-use-what","title":"When to Use What","text":"Use Case Solution Build dimension tables Use <code>transformer: dimension</code> in pipeline YAML Build fact tables Use <code>transformer: fact</code> in pipeline YAML Build scheduled aggregates Use <code>transformer: aggregation</code> in pipeline YAML Ad-hoc metric queries Use Semantic Layer Python API Self-service BI metrics Use Semantic Layer with materialization"},{"location":"semantics/#configuration","title":"Configuration","text":"<p>The semantic layer is configured via Python, not YAML. You can load config from a YAML file if desired:</p> <pre><code>from odibi.semantics import SemanticQuery, Materializer, parse_semantic_config\nimport yaml\n\n# Load from YAML (optional - can also build programmatically)\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Query interface\nquery = SemanticQuery(config)\nresult = query.execute(\"revenue BY region\", context)\n\n# Materialization\nmaterializer = Materializer(config)\nmaterializer.execute(\"monthly_revenue\", context)\n</code></pre>"},{"location":"semantics/#example-semantic_configyaml","title":"Example semantic_config.yaml","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  - name: avg_order_value\n    expr: \"AVG(total_amount)\"\n    source: fact_orders\n\ndimensions:\n  - name: region\n    source: fact_orders\n    column: region\n\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy: [year, quarter, month_name]\n\n  - name: category\n    source: dim_product\n    column: category\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n</code></pre>"},{"location":"semantics/#core-concepts","title":"Core Concepts","text":""},{"location":"semantics/#metrics","title":"Metrics","text":"<p>Metrics are measurable values that can be aggregated across dimensions:</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"semantics/#dimensions","title":"Dimensions","text":"<p>Dimensions are attributes for grouping and filtering:</p> <pre><code>dimensions:\n  - name: order_date\n    source: dim_date\n    hierarchy: [year, quarter, month, full_date]\n</code></pre>"},{"location":"semantics/#queries","title":"Queries","text":"<p>Query the semantic layer with a simple string syntax:</p> <pre><code>result = query.execute(\"revenue, order_count BY region, month\", context)\n</code></pre>"},{"location":"semantics/#materializations","title":"Materializations","text":"<p>Pre-compute metrics at specific grain:</p> <pre><code>materializations:\n  - name: monthly_revenue\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre>"},{"location":"semantics/#quick-start","title":"Quick Start","text":""},{"location":"semantics/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is through the unified <code>Project</code> API, which automatically resolves table paths from your connections:</p> <pre><code>from odibi import Project\n\n# Load project - semantic layer inherits connections from odibi.yaml\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics - tables are auto-loaded from connections\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\n\n# With filters\nresult = project.query(\"revenue BY category WHERE region = 'North'\")\n</code></pre> <p>odibi.yaml with semantic layer:</p> <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n      - name: dim_customer\n        write:\n          connection: gold\n          table: dim_customer\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n</code></pre> <p>The <code>source: $build_warehouse.fact_orders</code> notation tells the semantic layer to: 1. Look up the <code>fact_orders</code> node in the <code>build_warehouse</code> pipeline 2. Read its <code>write.connection</code> and <code>write.table</code> config 3. Auto-load the Delta table when queried</p> <p>Alternative: connection.path notation</p> <p>For external tables not managed by pipelines, use <code>connection.path</code>:</p> <pre><code>source: gold.fact_orders              # \u2192 /mnt/data/gold/fact_orders\nsource: gold.oee/plant_a/metrics      # \u2192 /mnt/data/gold/oee/plant_a/metrics (nested paths work!)\n</code></pre> <p>The split happens on the first dot only, so subdirectories are supported.</p>"},{"location":"semantics/#option-b-manual-setup","title":"Option B: Manual Setup","text":""},{"location":"semantics/#1-build-your-data-with-pipelines","title":"1. Build Your Data with Pipelines","text":"<p>First, use standard Odibi pipelines to build your star schema:</p> <pre><code># odibi.yaml - Build the data layer\nproject: my_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_star_schema\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols: [name, region]\n        write:\n          connection: warehouse\n          path: dim_customer\n\n      - name: fact_orders\n        depends_on: [dim_customer]\n        read:\n          connection: staging\n          path: orders\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n        write:\n          connection: warehouse\n          path: fact_orders\n</code></pre>"},{"location":"semantics/#2-define-semantic-layer","title":"2. Define Semantic Layer","text":"<p>Create a semantic config (Python or YAML):</p> <pre><code>from odibi.semantics import SemanticLayerConfig, MetricDefinition, DimensionDefinition\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            expr=\"SUM(total_amount)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\"\n        )\n    ],\n    dimensions=[\n        DimensionDefinition(\n            name=\"region\",\n            source=\"dim_customer\",\n            column=\"region\"\n        )\n    ]\n)\n</code></pre>"},{"location":"semantics/#3-query-metrics","title":"3. Query Metrics","text":"<pre><code>from odibi.semantics import SemanticQuery\nfrom odibi.context import EngineContext\n\n# Setup context with your data\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", spark.table(\"warehouse.fact_orders\"))\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\n\n# Query\nquery = SemanticQuery(config)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.show())\n</code></pre>"},{"location":"semantics/#architecture","title":"Architecture","text":"<pre><code>classDiagram\n    class SemanticLayerConfig {\n        +metrics: List[MetricDefinition]\n        +dimensions: List[DimensionDefinition]\n        +materializations: List[MaterializationConfig]\n    }\n\n    class MetricDefinition {\n        +name: str\n        +expr: str\n        +source: str\n        +filters: List[str]\n    }\n\n    class DimensionDefinition {\n        +name: str\n        +source: str\n        +column: str\n        +hierarchy: List[str]\n    }\n\n    class SemanticQuery {\n        +execute(query_string, context)\n    }\n\n    class Materializer {\n        +execute(name, context)\n        +execute_all(context)\n    }\n\n    SemanticLayerConfig --&gt; MetricDefinition\n    SemanticLayerConfig --&gt; DimensionDefinition\n    SemanticQuery --&gt; SemanticLayerConfig\n    Materializer --&gt; SemanticLayerConfig\n</code></pre>"},{"location":"semantics/#next-steps","title":"Next Steps","text":"<ul> <li>Defining Metrics - Create metric and dimension definitions</li> <li>Querying - Query syntax and examples</li> <li>Materializing - Pre-compute and schedule metrics</li> <li>Pattern Docs - Build your data layer with patterns</li> </ul>"},{"location":"semantics/materialize/","title":"Materializing Metrics","text":"<p>This guide covers how to pre-compute and persist metrics using the <code>Materializer</code> class.</p> <p>Source Notation: Metrics and dimensions use <code>source</code> to reference tables. Supports <code>$pipeline.node</code> (recommended), <code>connection.path</code>, or bare names. See Source Notation for details.</p>"},{"location":"semantics/materialize/#overview","title":"Overview","text":"<p>Materialization pre-computes aggregated metrics at a specific grain and persists them to an output table. This enables:</p> <ul> <li>Faster query response: Pre-computed aggregates vs. real-time calculation</li> <li>Scheduled refresh: Cron-based updates for dashboards</li> <li>Incremental updates: Merge new data without full recalculation</li> </ul>"},{"location":"semantics/materialize/#materializationconfig","title":"MaterializationConfig","text":"<p>Define materializations in your semantic layer config:</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region     # Unique identifier\n    metrics: [revenue, order_count]     # Metrics to include\n    dimensions: [region, month]         # Grain (GROUP BY)\n    output: gold/agg_monthly_revenue    # Output table path\n    schedule: \"0 2 1 * *\"               # Optional: cron schedule\n    incremental:                        # Optional: incremental config\n      timestamp_column: order_date\n      merge_strategy: replace\n</code></pre>"},{"location":"semantics/materialize/#fields","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique materialization identifier <code>metrics</code> list Yes - Metrics to materialize <code>dimensions</code> list Yes - Dimensions for grouping <code>output</code> str Yes - Output table path <code>schedule</code> str No - Cron schedule for refresh <code>incremental</code> dict No - Incremental refresh config"},{"location":"semantics/materialize/#materializer-class","title":"Materializer Class","text":""},{"location":"semantics/materialize/#basic-usage","title":"Basic Usage","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\n\n# Load config\nconfig = parse_semantic_config(yaml.safe_load(open(\"semantic_layer.yaml\")))\n\n# Create materializer\nmaterializer = Materializer(config)\n\n# Execute single materialization\nresult = materializer.execute(\"monthly_revenue_by_region\", context)\n\nprint(result.name)        # 'monthly_revenue_by_region'\nprint(result.output)      # 'gold/agg_monthly_revenue'\nprint(result.row_count)   # Number of aggregated rows\nprint(result.elapsed_ms)  # Execution time\nprint(result.success)     # True if successful\nprint(result.error)       # Error message if failed\n</code></pre>"},{"location":"semantics/materialize/#execute-all-materializations","title":"Execute All Materializations","text":"<pre><code># Execute all configured materializations\nresults = materializer.execute_all(context)\n\nfor result in results:\n    status = \"OK\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status} ({result.row_count} rows)\")\n</code></pre>"},{"location":"semantics/materialize/#write-callback","title":"Write Callback","text":"<p>Provide a callback to write the output:</p> <pre><code>def write_delta(df, output_path):\n    \"\"\"Write DataFrame to Delta Lake.\"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n\n# Execute with write callback\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_delta\n)\n</code></pre>"},{"location":"semantics/materialize/#scheduling","title":"Scheduling","text":"<p>Materializations can have cron schedules for automated refresh:</p> <pre><code>materializations:\n  - name: daily_revenue\n    metrics: [revenue]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    schedule: \"0 2 * * *\"  # 2am daily\n\n  - name: monthly_summary\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_summary\n    schedule: \"0 3 1 * *\"  # 3am on 1st of month\n</code></pre>"},{"location":"semantics/materialize/#reading-schedules","title":"Reading Schedules","text":"<pre><code># Get schedule for a materialization\nschedule = materializer.get_schedule(\"daily_revenue\")\nprint(schedule)  # \"0 2 * * *\"\n\n# List all materializations with schedules\nfor mat in materializer.list_materializations():\n    print(f\"{mat['name']}: {mat['schedule']}\")\n</code></pre>"},{"location":"semantics/materialize/#integration-with-orchestrators","title":"Integration with Orchestrators","text":"<p>Use schedules with your orchestrator (Airflow, Dagster, etc.):</p> <pre><code># Airflow example\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndef run_materialization(name):\n    materializer.execute(name, context, write_callback=write_delta)\n\nfor mat in materializer.list_materializations():\n    if mat['schedule']:\n        PythonOperator(\n            task_id=f\"materialize_{mat['name']}\",\n            python_callable=run_materialization,\n            op_args=[mat['name']],\n            schedule_interval=mat['schedule']\n        )\n</code></pre>"},{"location":"semantics/materialize/#incremental-materialization","title":"Incremental Materialization","text":"<p>Use <code>IncrementalMaterializer</code> for efficient updates:</p> <pre><code>from odibi.semantics.materialize import IncrementalMaterializer\n\n# Create incremental materializer\ninc_materializer = IncrementalMaterializer(config)\n\n# Load existing materialized data\nexisting_df = spark.read.format(\"delta\").load(\"gold/agg_monthly_revenue\")\n\n# Get last processed timestamp\nlast_timestamp = existing_df.agg({\"load_timestamp\": \"max\"}).collect()[0][0]\n\n# Execute incremental update\nresult = inc_materializer.execute_incremental(\n    name=\"monthly_revenue_by_region\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_timestamp,\n    merge_strategy=\"replace\"\n)\n</code></pre>"},{"location":"semantics/materialize/#merge-strategies","title":"Merge Strategies","text":""},{"location":"semantics/materialize/#replace-strategy","title":"Replace Strategy","text":"<p>New aggregates overwrite existing for matching grain keys:</p> <pre><code>result = inc_materializer.execute_incremental(\n    name=\"monthly_revenue_by_region\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_processed,\n    merge_strategy=\"replace\"\n)\n</code></pre> <p>Behavior: 1. Filter source to <code>order_date &gt; since_timestamp</code> 2. Aggregate new data at grain 3. Remove matching grain keys from existing 4. Union remaining existing + new aggregates</p> <p>Use case: Late-arriving data, corrections, any non-additive metrics</p>"},{"location":"semantics/materialize/#sum-strategy","title":"Sum Strategy","text":"<p>Add new measure values to existing aggregates:</p> <pre><code>result = inc_materializer.execute_incremental(\n    name=\"daily_order_count\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"created_at\",\n    since_timestamp=last_processed,\n    merge_strategy=\"sum\"\n)\n</code></pre> <p>Behavior: 1. Filter source to <code>created_at &gt; since_timestamp</code> 2. Aggregate new data at grain 3. Full outer join with existing on grain 4. Sum measure values</p> <p>Use case: Purely additive metrics (counts, sums) where data is append-only</p> <p>Warning: Don't use for AVG, DISTINCT counts, or ratios.</p>"},{"location":"semantics/materialize/#full-example","title":"Full Example","text":"<p>Complete materialization pipeline:</p> <pre><code># semantic_layer.yaml\nsemantic_layer:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n      filters: [\"status = 'completed'\"]\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n      source: fact_orders\n\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n      source: fact_orders\n\n  dimensions:\n    - name: region\n      source: dim_customer\n      column: region\n\n    - name: month\n      source: dim_date\n      column: month_name\n\n    - name: date_sk\n      source: dim_date\n      column: date_sk\n\n  materializations:\n    - name: daily_revenue\n      metrics: [revenue, order_count]\n      dimensions: [date_sk, region]\n      output: gold/agg_daily_revenue\n      schedule: \"0 2 * * *\"\n\n    - name: monthly_summary\n      metrics: [revenue, order_count, unique_customers]\n      dimensions: [region, month]\n      output: gold/agg_monthly_summary\n      schedule: \"0 3 1 * *\"\n</code></pre> <pre><code>from odibi.semantics import Materializer, IncrementalMaterializer, parse_semantic_config\nfrom odibi.context import EngineContext\nimport yaml\n\n# Load config\nwith open(\"semantic_layer.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f)[\"semantic_layer\"])\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", spark.table(\"silver.fact_orders\"))\ncontext.register(\"dim_customer\", spark.table(\"gold.dim_customer\"))\ncontext.register(\"dim_date\", spark.table(\"gold.dim_date\"))\n\n# Write callback\ndef write_to_delta(df, output_path):\n    df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/warehouse/{output_path}\")\n\n# Full refresh all materializations\nmaterializer = Materializer(config)\nresults = materializer.execute_all(context, write_callback=write_to_delta)\n\n# Print summary\nfor r in results:\n    status = \"SUCCESS\" if r.success else f\"FAILED: {r.error}\"\n    print(f\"{r.name}: {status} - {r.row_count} rows in {r.elapsed_ms:.0f}ms\")\n\n# Incremental refresh for daily\ninc_materializer = IncrementalMaterializer(config)\nexisting_daily = spark.read.format(\"delta\").load(\"/mnt/warehouse/gold/agg_daily_revenue\")\nlast_date = existing_daily.agg({\"date_sk\": \"max\"}).collect()[0][0]\n\nresult = inc_materializer.execute_incremental(\n    name=\"daily_revenue\",\n    context=context,\n    existing_df=existing_daily,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_date,\n    merge_strategy=\"replace\"\n)\n\n# Write incremental result\nif result.success:\n    write_to_delta(result.df, \"gold/agg_daily_revenue\")\n    print(f\"Updated daily_revenue: {result.row_count} rows\")\n</code></pre>"},{"location":"semantics/materialize/#materializationresult","title":"MaterializationResult","text":"Field Type Description <code>name</code> str Materialization name <code>output</code> str Output table path <code>row_count</code> int Number of aggregated rows <code>elapsed_ms</code> float Execution time in milliseconds <code>success</code> bool Whether execution succeeded <code>error</code> str Error message if failed"},{"location":"semantics/materialize/#best-practices","title":"Best Practices","text":""},{"location":"semantics/materialize/#grain-selection","title":"Grain Selection","text":"<ul> <li>Choose grain based on query patterns</li> <li>Finer grain = more rows, but more flexibility</li> <li>Coarser grain = faster queries, less flexibility</li> </ul>"},{"location":"semantics/materialize/#scheduling_1","title":"Scheduling","text":"<ul> <li>Schedule based on source data freshness</li> <li>Daily aggregates: run after nightly ETL</li> <li>Monthly: run after month close</li> </ul>"},{"location":"semantics/materialize/#incremental-strategy","title":"Incremental Strategy","text":"<ul> <li>Use <code>replace</code> for late-arriving data tolerance</li> <li>Use <code>sum</code> only for append-only sources</li> <li>Track <code>since_timestamp</code> in state store</li> </ul>"},{"location":"semantics/materialize/#performance","title":"Performance","text":"<ul> <li>Partition output by time dimension</li> <li>Use Delta Lake for efficient updates</li> <li>Monitor execution times</li> </ul>"},{"location":"semantics/materialize/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Create metric definitions</li> <li>Querying - Interactive metric queries</li> <li>Aggregation Pattern - Pattern-based aggregation</li> </ul>"},{"location":"semantics/metrics/","title":"Defining Metrics","text":"<p>This guide covers how to define metrics and dimensions in the Odibi semantic layer.</p> <p>Source Notation: The <code>source</code> field supports three formats: <code>$pipeline.node</code> (recommended), <code>connection.path</code>, or bare table names. See Source Notation for details.</p>"},{"location":"semantics/metrics/#metricdefinition","title":"MetricDefinition","text":"<p>A metric represents a measurable value that can be aggregated across dimensions.</p>"},{"location":"semantics/metrics/#schema","title":"Schema","text":"<pre><code>metrics:\n  - name: revenue              # Required: unique identifier\n    description: \"...\"         # Optional: human-readable description\n    expr: \"SUM(total_amount)\"  # Required: SQL aggregation expression\n    source: fact_orders        # Required for simple metrics: source table\n    filters:                   # Optional: WHERE conditions\n      - \"status = 'completed'\"\n    type: simple               # Optional: \"simple\" or \"derived\"\n</code></pre>"},{"location":"semantics/metrics/#fields","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique metric identifier (lowercase, alphanumeric + underscore) <code>description</code> str No - Human-readable description <code>expr</code> str Yes - SQL aggregation expression <code>source</code> str For simple - Source table name <code>filters</code> list No [] WHERE conditions to apply <code>type</code> str No \"simple\" \"simple\" (direct) or \"derived\" (references other metrics)"},{"location":"semantics/metrics/#simple-metrics","title":"Simple Metrics","text":"<p>Simple metrics aggregate directly from source data:</p> <pre><code>metrics:\n  # Count\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  # Sum\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n\n  # Average\n  - name: avg_order_value\n    expr: \"AVG(total_amount)\"\n    source: fact_orders\n\n  # Distinct count\n  - name: unique_customers\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n\n  # Min/Max\n  - name: max_order\n    expr: \"MAX(total_amount)\"\n    source: fact_orders\n\n  # Complex expression\n  - name: total_margin\n    expr: \"SUM(revenue - cost)\"\n    source: fact_orders\n</code></pre>"},{"location":"semantics/metrics/#filtered-metrics","title":"Filtered Metrics","text":"<p>Apply filters to constrain the aggregation:</p> <pre><code>metrics:\n  # Only completed orders\n  - name: completed_revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Multiple filters (AND)\n  - name: domestic_completed_revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n      - \"country = 'USA'\"\n\n  # Time-filtered\n  - name: last_30_days_orders\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"order_date &gt;= CURRENT_DATE - INTERVAL 30 DAY\"\n</code></pre>"},{"location":"semantics/metrics/#derived-metrics","title":"Derived Metrics","text":"<p>Derived metrics reference other metrics (future enhancement):</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  # Derived: revenue / order_count\n  - name: avg_order_value_derived\n    expr: \"revenue / order_count\"\n    type: derived\n</code></pre>"},{"location":"semantics/metrics/#dimensiondefinition","title":"DimensionDefinition","text":"<p>A dimension represents an attribute for grouping and filtering metrics.</p>"},{"location":"semantics/metrics/#schema_1","title":"Schema","text":"<pre><code>dimensions:\n  - name: region               # Required: unique identifier\n    source: fact_orders        # Required: source table\n    column: region             # Optional: column name (defaults to name)\n    hierarchy:                 # Optional: drill-down hierarchy\n      - year\n      - quarter\n      - month\n    description: \"...\"         # Optional: human-readable description\n</code></pre>"},{"location":"semantics/metrics/#fields_1","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique dimension identifier <code>source</code> str Yes - Source table name <code>column</code> str No name Column name in source <code>hierarchy</code> list No [] Ordered drill-down columns <code>description</code> str No - Human-readable description"},{"location":"semantics/metrics/#dimension-examples","title":"Dimension Examples","text":"<pre><code>dimensions:\n  # Simple dimension\n  - name: region\n    source: fact_orders\n    column: region\n\n  # Dimension with different column name\n  - name: customer_region\n    source: dim_customer\n    column: billing_region\n\n  # Date dimension with hierarchy\n  - name: order_date\n    source: dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter\n      - month\n      - week_of_year\n      - full_date\n\n  # Product category hierarchy\n  - name: product\n    source: dim_product\n    column: product_name\n    hierarchy:\n      - department\n      - category\n      - subcategory\n      - product_name\n</code></pre>"},{"location":"semantics/metrics/#complete-yaml-example","title":"Complete YAML Example","text":"<p>Full semantic layer configuration:</p> <pre><code>semantic_layer:\n  metrics:\n    # Revenue metrics\n    - name: revenue\n      description: \"Total revenue from all orders\"\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n\n    - name: completed_revenue\n      description: \"Revenue from completed orders only\"\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n    # Volume metrics\n    - name: order_count\n      description: \"Number of orders\"\n      expr: \"COUNT(*)\"\n      source: fact_orders\n\n    - name: units_sold\n      description: \"Total units sold\"\n      expr: \"SUM(quantity)\"\n      source: fact_orders\n\n    # Customer metrics\n    - name: unique_customers\n      description: \"Distinct customer count\"\n      expr: \"COUNT(DISTINCT customer_sk)\"\n      source: fact_orders\n\n    # Calculated metrics\n    - name: avg_order_value\n      description: \"Average order value\"\n      expr: \"AVG(total_amount)\"\n      source: fact_orders\n\n    - name: avg_units_per_order\n      description: \"Average units per order\"\n      expr: \"AVG(quantity)\"\n      source: fact_orders\n\n  dimensions:\n    # Geographic dimensions\n    - name: region\n      source: dim_customer\n      column: region\n      description: \"Customer region\"\n\n    - name: country\n      source: dim_customer\n      column: country\n\n    - name: city\n      source: dim_customer\n      column: city\n\n    # Time dimensions\n    - name: order_date\n      source: dim_date\n      column: full_date\n      hierarchy: [year, quarter, month, full_date]\n\n    - name: year\n      source: dim_date\n      column: year\n\n    - name: month\n      source: dim_date\n      column: month_name\n\n    - name: quarter\n      source: dim_date\n      column: quarter_name\n\n    # Product dimensions\n    - name: category\n      source: dim_product\n      column: category\n\n    - name: product\n      source: dim_product\n      column: product_name\n      hierarchy: [category, subcategory, product_name]\n\n    # Order dimensions\n    - name: channel\n      source: fact_orders\n      column: sales_channel\n\n    - name: payment_method\n      source: fact_orders\n      column: payment_type\n\n  materializations:\n    - name: daily_revenue_by_region\n      metrics: [revenue, order_count, unique_customers]\n      dimensions: [region, order_date]\n      output: gold/agg_daily_revenue_region\n\n    - name: monthly_revenue_by_category\n      metrics: [revenue, units_sold]\n      dimensions: [category, month]\n      output: gold/agg_monthly_revenue_category\n      schedule: \"0 2 1 * *\"\n</code></pre>"},{"location":"semantics/metrics/#python-api","title":"Python API","text":"<pre><code>from odibi.semantics.metrics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig,\n    parse_semantic_config\n)\n\n# Create metrics programmatically\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue\",\n    expr=\"SUM(total_amount)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Create dimensions\nregion = DimensionDefinition(\n    name=\"region\",\n    source=\"dim_customer\",\n    column=\"region\"\n)\n\norder_date = DimensionDefinition(\n    name=\"order_date\",\n    source=\"dim_date\",\n    column=\"full_date\",\n    hierarchy=[\"year\", \"quarter\", \"month\", \"full_date\"]\n)\n\n# Create config\nconfig = SemanticLayerConfig(\n    metrics=[revenue],\n    dimensions=[region, order_date]\n)\n\n# Or parse from YAML\nconfig = parse_semantic_config({\n    \"metrics\": [...],\n    \"dimensions\": [...],\n    \"materializations\": [...]\n})\n\n# Validate references\nerrors = config.validate_references()\nif errors:\n    print(\"Validation errors:\", errors)\n\n# Lookup by name\nmetric = config.get_metric(\"revenue\")\ndimension = config.get_dimension(\"region\")\n</code></pre>"},{"location":"semantics/metrics/#validation","title":"Validation","text":"<p>The semantic layer validates:</p> <ol> <li>Metric names: Must be alphanumeric + underscore, lowercase</li> <li>Non-empty expressions: <code>expr</code> cannot be empty</li> <li>Materialization references: All referenced metrics/dimensions must exist</li> </ol> <pre><code># Validate the config\nerrors = config.validate_references()\n# Returns: [\"Materialization 'x' references unknown metric 'y'\"]\n</code></pre>"},{"location":"semantics/metrics/#best-practices","title":"Best Practices","text":""},{"location":"semantics/metrics/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use <code>snake_case</code> for metric and dimension names</li> <li>Be descriptive: <code>completed_order_revenue</code> over <code>rev1</code></li> <li>Prefix related metrics: <code>revenue</code>, <code>revenue_completed</code>, <code>revenue_refunded</code></li> </ul>"},{"location":"semantics/metrics/#filter-usage","title":"Filter Usage","text":"<ul> <li>Define filtered variants as separate metrics</li> <li>Makes queries cleaner and consistent</li> <li>Enables caching of common filter combinations</li> </ul>"},{"location":"semantics/metrics/#hierarchy-design","title":"Hierarchy Design","text":"<ul> <li>Order from coarsest to finest grain</li> <li>Match BI tool drill-down expectations</li> <li>Include intermediate levels for flexibility</li> </ul>"},{"location":"semantics/metrics/#see-also","title":"See Also","text":"<ul> <li>Querying - Query syntax and execution</li> <li>Materializing - Pre-compute metrics</li> <li>Semantic Layer Overview - Architecture and concepts</li> </ul>"},{"location":"semantics/query/","title":"Querying the Semantic Layer","text":"<p>This guide covers how to query the Odibi semantic layer using the <code>SemanticQuery</code> interface.</p>"},{"location":"semantics/query/#query-syntax","title":"Query Syntax","text":"<p>The semantic query syntax follows a simple pattern:</p> <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre>"},{"location":"semantics/query/#examples","title":"Examples","text":"<pre><code># Single metric, single dimension\n\"revenue BY region\"\n\n# Multiple metrics, single dimension\n\"revenue, order_count BY region\"\n\n# Multiple metrics, multiple dimensions\n\"revenue, order_count BY region, month\"\n\n# With WHERE filter\n\"revenue BY region WHERE year = 2024\"\n\n# Complex filter\n\"revenue BY category WHERE region = 'North' AND status = 'completed'\"\n</code></pre>"},{"location":"semantics/query/#semanticquery-class","title":"SemanticQuery Class","text":""},{"location":"semantics/query/#initialization","title":"Initialization","text":"<pre><code>from odibi.semantics import SemanticQuery, SemanticLayerConfig\n\n# From config object\nconfig = SemanticLayerConfig(\n    metrics=[...],\n    dimensions=[...],\n    materializations=[...]\n)\nquery = SemanticQuery(config)\n\n# From YAML\nimport yaml\nfrom odibi.semantics.metrics import parse_semantic_config\n\nwith open(\"semantic_layer.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\nquery = SemanticQuery(config)\n</code></pre>"},{"location":"semantics/query/#execute-query","title":"Execute Query","text":"<pre><code>from odibi.context import EngineContext\n\n# Create context with source data\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\n\n# Execute query\nresult = query.execute(\"revenue BY region\", context)\n\n# Access results\nprint(result.df)           # DataFrame with results\nprint(result.metrics)      # ['revenue']\nprint(result.dimensions)   # ['region']\nprint(result.row_count)    # Number of result rows\nprint(result.elapsed_ms)   # Execution time\nprint(result.sql_generated)  # Generated SQL (for debugging)\n</code></pre>"},{"location":"semantics/query/#queryresult","title":"QueryResult","text":"Field Type Description <code>df</code> DataFrame Result DataFrame (Spark or Pandas) <code>metrics</code> List[str] Metrics that were computed <code>dimensions</code> List[str] Dimensions used for grouping <code>row_count</code> int Number of result rows <code>elapsed_ms</code> float Execution time in milliseconds <code>sql_generated</code> str Generated SQL query (for debugging)"},{"location":"semantics/query/#query-examples","title":"Query Examples","text":""},{"location":"semantics/query/#basic-queries","title":"Basic Queries","text":"<pre><code># Total revenue\nresult = query.execute(\"revenue\", context)\n# Returns single row with total\n\n# Revenue by region\nresult = query.execute(\"revenue BY region\", context)\n# Returns one row per region\n\n# Multiple metrics\nresult = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\n</code></pre>"},{"location":"semantics/query/#multi-dimensional-queries","title":"Multi-Dimensional Queries","text":"<pre><code># Two dimensions\nresult = query.execute(\"revenue BY region, category\", context)\n\n# Three dimensions\nresult = query.execute(\"revenue BY region, category, month\", context)\n\n# Time series\nresult = query.execute(\"revenue, order_count BY year, month\", context)\n</code></pre>"},{"location":"semantics/query/#filtered-queries","title":"Filtered Queries","text":"<pre><code># Single filter\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North'\",\n    context\n)\n\n# Multiple filters (combined with AND)\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North' AND year = 2024\",\n    context\n)\n\n# Using metric filters + query filters\n# If metric has filters, they combine with query filters\nresult = query.execute(\"completed_revenue BY region\", context)\n# Metric filter: status = 'completed'\n# Combined: WHERE status = 'completed'\n</code></pre>"},{"location":"semantics/query/#parse-and-validate","title":"Parse and Validate","text":"<p>You can parse and validate queries before execution:</p> <pre><code># Parse query string\nparsed = query.parse(\"revenue, order_count BY region, month WHERE year = 2024\")\n\nprint(parsed.metrics)      # ['revenue', 'order_count']\nprint(parsed.dimensions)   # ['region', 'month']\nprint(parsed.filters)      # ['year = 2024']\nprint(parsed.raw_query)    # Original query string\n\n# Validate against config\nerrors = query.validate(parsed)\nif errors:\n    print(\"Validation errors:\", errors)\n    # [\"Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count']\"]\n</code></pre>"},{"location":"semantics/query/#generated-sql","title":"Generated SQL","text":"<p>View the SQL generated from a semantic query:</p> <pre><code>parsed = query.parse(\"revenue BY region\")\nsql, source = query.generate_sql(parsed)\n\nprint(sql)\n# SELECT region, SUM(total_amount) AS revenue \n# FROM fact_orders \n# GROUP BY region\n\nprint(source)\n# fact_orders\n</code></pre>"},{"location":"semantics/query/#full-python-example","title":"Full Python Example","text":"<pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load semantic layer config\nconfig_dict = {\n    \"metrics\": [\n        {\n            \"name\": \"revenue\",\n            \"expr\": \"SUM(total_amount)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"order_count\",\n            \"expr\": \"COUNT(*)\",\n            \"source\": \"fact_orders\"\n        },\n        {\n            \"name\": \"avg_order_value\",\n            \"expr\": \"AVG(total_amount)\",\n            \"source\": \"fact_orders\"\n        }\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"fact_orders\", \"column\": \"region\"},\n        {\"name\": \"month\", \"source\": \"dim_date\", \"column\": \"month_name\"}\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\nquery = SemanticQuery(config)\n\n# Create context with data\ncontext = EngineContext(\n    df=None, \n    engine_type=EngineType.PANDAS\n)\ncontext.register(\"fact_orders\", orders_df)\ncontext.register(\"dim_date\", dates_df)\n\n# Query 1: Total revenue\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\n\n# Query 2: Revenue by region\nresult = query.execute(\"revenue, order_count BY region\", context)\nprint(\"\\nRevenue by Region:\")\nprint(result.df.to_string(index=False))\n\n# Query 3: Monthly trend\nresult = query.execute(\"revenue BY month\", context)\nprint(\"\\nMonthly Revenue:\")\nfor _, row in result.df.iterrows():\n    print(f\"  {row['month']}: ${row['revenue']:,.2f}\")\n\n# Query 4: Filtered query\nresult = query.execute(\n    \"revenue, avg_order_value BY region WHERE region IN ('North', 'South')\",\n    context\n)\nprint(\"\\nNorth/South Regions:\")\nprint(result.df.to_string(index=False))\n\n# Check execution performance\nprint(f\"\\nQuery executed in {result.elapsed_ms:.2f}ms\")\nprint(f\"Generated SQL: {result.sql_generated}\")\n</code></pre>"},{"location":"semantics/query/#using-with-source-dataframe","title":"Using with Source DataFrame","text":"<p>Override the context lookup with a specific DataFrame:</p> <pre><code># Instead of using context.get(source_table)\n# Pass source_df directly\nresult = query.execute(\n    \"revenue BY region\",\n    context,\n    source_df=my_filtered_dataframe\n)\n</code></pre>"},{"location":"semantics/query/#error-handling","title":"Error Handling","text":"<pre><code>from odibi.semantics import SemanticQuery\n\ntry:\n    result = query.execute(\"invalid_metric BY region\", context)\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\n    # \"Invalid semantic query: Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count']\"\n\ntry:\n    result = query.execute(\"revenue BY invalid_dimension\", context)\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\n    # \"Invalid semantic query: Unknown dimension 'invalid_dimension'. Available: ['region', 'month']\"\n</code></pre>"},{"location":"semantics/query/#engine-support","title":"Engine Support","text":"<p>Queries work with both Spark and Pandas:</p>"},{"location":"semantics/query/#spark","title":"Spark","text":"<pre><code>context = EngineContext(\n    df=None,\n    engine_type=EngineType.SPARK,\n    spark=spark_session\n)\nresult = query.execute(\"revenue BY region\", context)\nresult.df.show()  # Spark DataFrame\n</code></pre>"},{"location":"semantics/query/#pandas","title":"Pandas","text":"<pre><code>context = EngineContext(\n    df=None,\n    engine_type=EngineType.PANDAS\n)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df)  # Pandas DataFrame\n</code></pre>"},{"location":"semantics/query/#performance-tips","title":"Performance Tips","text":"<ol> <li>Materialize frequent queries: Use <code>Materializer</code> for dashboards</li> <li>Pre-filter source data: Pass filtered <code>source_df</code> parameter</li> <li>Limit dimensions: More dimensions = larger result set</li> <li>Use indexed columns: Ensure dimension columns are indexed in source</li> </ol>"},{"location":"semantics/query/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Create metric and dimension definitions</li> <li>Materializing Metrics - Pre-compute for performance</li> <li>Semantic Layer Overview - Architecture and concepts</li> </ul>"},{"location":"tutorials/azure_connections/","title":"Azure Connections Tutorial","text":"<p>This tutorial shows how to connect Odibi to Azure data services: Blob Storage, ADLS Gen2, and Azure SQL.</p>"},{"location":"tutorials/azure_connections/#prerequisites","title":"Prerequisites","text":"<ul> <li>Odibi installed (<code>pip install odibi</code>)</li> <li>Azure subscription with appropriate permissions</li> <li>Azure CLI installed (<code>az login</code> completed)</li> </ul>"},{"location":"tutorials/azure_connections/#1-azure-blob-storage","title":"1. Azure Blob Storage","text":""},{"location":"tutorials/azure_connections/#service-principal-authentication-recommended-for-production","title":"Service Principal Authentication (Recommended for Production)","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: landing\n    auth:\n      mode: service_principal\n      tenant_id: \"${AZURE_TENANT_ID}\"\n      client_id: \"${AZURE_CLIENT_ID}\"\n      client_secret: \"${AZURE_CLIENT_SECRET}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#managed-identity-databrickssynapse","title":"Managed Identity (Databricks/Synapse)","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: landing\n    auth:\n      mode: managed_identity\n</code></pre>"},{"location":"tutorials/azure_connections/#sas-token","title":"SAS Token","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: landing\n    auth:\n      mode: sas\n      token: \"${AZURE_SAS_TOKEN}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#connection-string","title":"Connection String","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    connection_string: \"${AZURE_STORAGE_CONNECTION_STRING}\"\n    container: landing\n</code></pre>"},{"location":"tutorials/azure_connections/#2-adls-gen2-hierarchical-namespace","title":"2. ADLS Gen2 (Hierarchical Namespace)","text":"<p>For Delta Lake on Azure, use ADLS Gen2:</p> <pre><code>connections:\n  adls_bronze:\n    type: azure_blob\n    account_name: mydatalake\n    container: bronze\n    is_adls_gen2: true  # Enable hierarchical namespace\n    auth:\n      mode: service_principal\n      tenant_id: \"${AZURE_TENANT_ID}\"\n      client_id: \"${AZURE_CLIENT_ID}\"\n      client_secret: \"${AZURE_CLIENT_SECRET}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#delta-lake-on-adls","title":"Delta Lake on ADLS","text":"<pre><code>connections:\n  delta_silver:\n    type: delta\n    base_path: abfss://silver@mydatalake.dfs.core.windows.net/\n    # Auth inherited from Spark session config\n</code></pre>"},{"location":"tutorials/azure_connections/#3-azure-sql-database","title":"3. Azure SQL Database","text":""},{"location":"tutorials/azure_connections/#sql-authentication","title":"SQL Authentication","text":"<pre><code>connections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: sql_login\n      username: \"${SQL_USER}\"\n      password: \"${SQL_PASSWORD}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#azure-ad-authentication","title":"Azure AD Authentication","text":"<pre><code>connections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: aad_password\n      username: user@company.com\n      password: \"${AAD_PASSWORD}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#managed-identity","title":"Managed Identity","text":"<pre><code>connections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: msi\n</code></pre>"},{"location":"tutorials/azure_connections/#4-complete-azure-project","title":"4. Complete Azure Project","text":"<p>Here's a full project using Azure services:</p> <pre><code># project.yaml\nproject: \"azure_data_platform\"\nengine: spark\n\nconnections:\n  # Landing zone (raw files)\n  landing:\n    type: azure_blob\n    account_name: \"${STORAGE_ACCOUNT}\"\n    container: landing\n    auth:\n      mode: service_principal\n      tenant_id: \"${AZURE_TENANT_ID}\"\n      client_id: \"${AZURE_CLIENT_ID}\"\n      client_secret: \"${AZURE_CLIENT_SECRET}\"\n\n  # Bronze layer (Delta)\n  bronze:\n    type: delta\n    base_path: abfss://bronze@${STORAGE_ACCOUNT}.dfs.core.windows.net/\n\n  # Silver layer (Delta)  \n  silver:\n    type: delta\n    base_path: abfss://silver@${STORAGE_ACCOUNT}.dfs.core.windows.net/\n\n  # Source database\n  erp_sql:\n    type: sqlserver\n    server: erp-server.database.windows.net\n    database: erp_prod\n    auth:\n      mode: msi\n\nstory:\n  connection: landing\n  path: _odibi/stories/\n\nsystem:\n  connection: landing\n  path: _odibi/catalog/\n\npipelines:\n  - pipeline: bronze_ingest\n    layer: bronze\n    nodes:\n      # Ingest from SQL\n      - name: customers_raw\n        read:\n          connection: erp_sql\n          format: sql\n          table: dbo.Customers\n        write:\n          connection: bronze\n          table: raw_customers\n\n      # Ingest from files\n      - name: orders_raw\n        read:\n          connection: landing\n          format: csv\n          path: orders/*.csv\n        write:\n          connection: bronze\n          table: raw_orders\n</code></pre>"},{"location":"tutorials/azure_connections/#5-environment-variables","title":"5. Environment Variables","text":"<p>Store secrets in environment variables or Azure Key Vault:</p> <pre><code># .env (local development - git-ignored!)\nAZURE_TENANT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nAZURE_CLIENT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nAZURE_CLIENT_SECRET=your-secret-here\nSTORAGE_ACCOUNT=mydatalake\nSQL_USER=odibi_user\nSQL_PASSWORD=secure-password\n</code></pre>"},{"location":"tutorials/azure_connections/#using-azure-key-vault","title":"Using Azure Key Vault","text":"<pre><code># Reference Key Vault secrets with ${kv:secret-name}\nconnections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: sql_login\n      username: \"${kv:sql-username}\"\n      password: \"${kv:sql-password}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#6-spark-configuration-for-azure","title":"6. Spark Configuration for Azure","text":"<p>When using Spark with Azure, configure the session:</p> <pre><code># conf/spark_azure.py\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"OdibiAzure\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n    .config(\"fs.azure.account.auth.type\", \"OAuth\") \\\n    .config(\"fs.azure.account.oauth.provider.type\", \n            \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n    .config(\"fs.azure.account.oauth2.client.id\", os.environ[\"AZURE_CLIENT_ID\"]) \\\n    .config(\"fs.azure.account.oauth2.client.secret\", os.environ[\"AZURE_CLIENT_SECRET\"]) \\\n    .config(\"fs.azure.account.oauth2.client.endpoint\", \n            f\"https://login.microsoftonline.com/{os.environ['AZURE_TENANT_ID']}/oauth2/token\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"tutorials/azure_connections/#7-running-on-azure-databricks","title":"7. Running on Azure Databricks","text":""},{"location":"tutorials/azure_connections/#cluster-configuration","title":"Cluster Configuration","text":"<p>Add to cluster Spark config:</p> <pre><code>fs.azure.account.auth.type OAuth\nfs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nfs.azure.account.oauth2.client.id {{secrets/odibi/client-id}}\nfs.azure.account.oauth2.client.secret {{secrets/odibi/client-secret}}\nfs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/{{secrets/odibi/tenant-id}}/oauth2/token\n</code></pre>"},{"location":"tutorials/azure_connections/#unity-catalog","title":"Unity Catalog","text":"<p>With Unity Catalog, use catalog-based connections:</p> <pre><code>connections:\n  bronze:\n    type: delta\n    catalog: main\n    schema: bronze\n\n  silver:\n    type: delta\n    catalog: main\n    schema: silver\n</code></pre>"},{"location":"tutorials/azure_connections/#8-running-on-azure-synapse","title":"8. Running on Azure Synapse","text":"<pre><code># Synapse notebook\n%%pyspark\nfrom odibi import run_project\n\n# Synapse auto-configures ADLS access via linked services\nrun_project(\"/synapse/project.yaml\", pipelines=[\"bronze_ingest\"])\n</code></pre>"},{"location":"tutorials/azure_connections/#common-issues","title":"Common Issues","text":""},{"location":"tutorials/azure_connections/#403-forbidden-on-blob-access","title":"\"403 Forbidden\" on Blob Access","text":"<ol> <li>Check service principal has \"Storage Blob Data Contributor\" role</li> <li>Verify container name is correct</li> <li>Check firewall allows your IP</li> </ol>"},{"location":"tutorials/azure_connections/#login-failed-on-azure-sql","title":"\"Login failed\" on Azure SQL","text":"<ol> <li>Verify Azure AD admin is set on SQL server</li> <li>Check firewall allows Azure services</li> <li>For MSI, ensure the managed identity has db_datareader/db_datawriter</li> </ol>"},{"location":"tutorials/azure_connections/#slow-adls-performance","title":"Slow ADLS Performance","text":"<p>Enable Delta caching:</p> <pre><code>performance:\n  delta_table_properties:\n    delta.autoOptimize.optimizeWrite: true\n</code></pre>"},{"location":"tutorials/azure_connections/#next-steps","title":"Next Steps","text":"<ul> <li>Spark Engine Tutorial - Spark-specific features</li> <li>Getting Started - Basic Odibi concepts</li> <li>Performance Tuning - Optimize large pipelines</li> </ul>"},{"location":"tutorials/azure_connections/#see-also","title":"See Also","text":"<ul> <li>AzureBlobConnectionConfig - Full Azure Blob options</li> <li>SQLServerConnectionConfig - Azure SQL options</li> <li>DeltaConnectionConfig - Delta Lake options</li> </ul>"},{"location":"tutorials/bronze_layer/","title":"Bronze Layer Tutorial","text":"<p>The Bronze Layer is where raw data lands. No transformations, no cleaning\u2014just reliable ingestion with traceability.</p>"},{"location":"tutorials/bronze_layer/#layer-philosophy","title":"Layer Philosophy","text":"<p>\"Raw is sacred. Preserve everything, trust nothing.\"</p> <p>Bronze is your insurance policy. If downstream logic has bugs, you can always reprocess from Bronze.</p> Principle Why Append-only Never lose source data Schema as-is Don't transform on ingest Full fidelity Keep all columns, all rows Traceable Know when each row arrived"},{"location":"tutorials/bronze_layer/#quick-start-file-ingestion","title":"Quick Start: File Ingestion","text":"<p>The simplest Bronze pipeline loads files and appends them:</p> <pre><code># pipelines/bronze/ingest_orders.yaml\npipelines:\n  - pipeline: bronze_orders\n    layer: bronze\n    nodes:\n      - name: raw_orders\n        read:\n          connection: landing\n          format: csv\n          path: orders/*.csv\n          options:\n            header: true\n            inferSchema: true\n        write:\n          connection: bronze\n          table: raw_orders\n          mode: append\n</code></pre>"},{"location":"tutorials/bronze_layer/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":""},{"location":"tutorials/bronze_layer/#1-im-reprocessing-files-ive-already-loaded","title":"1. \"I'm reprocessing files I've already loaded\"","text":"<p>Problem: Each run loads all files, creating duplicates.</p> <p>Solution: Use stateful incremental tracking with a high-water mark column.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n      incremental:\n        mode: stateful               # Track HWM (high-water mark)\n        column: file_modified_date   # Column to track\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>How it works: - Odibi records the MAX value of <code>column</code> after each run - On next run, only rows with values &gt; stored HWM are processed - For time-based lookback instead, use <code>mode: rolling_window</code></p> <p>See: Incremental Loading Pattern</p>"},{"location":"tutorials/bronze_layer/#2-files-have-inconsistent-schemas","title":"2. \"Files have inconsistent schemas\"","text":"<p>Problem: New files have extra/missing columns, breaking the pipeline.</p> <p>Solution: Enable Delta schema evolution on write.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n      options:\n        header: true\n        inferSchema: true            # Infer schema from CSV\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n      options:\n        mergeSchema: true            # Delta: allow schema evolution\n</code></pre> <p>How it works: - Spark infers schema from each file - Delta's <code>mergeSchema</code> adds new columns to the target table automatically - Odibi tracks schema changes in the System Catalog</p> <p>See: Schema Tracking</p>"},{"location":"tutorials/bronze_layer/#3-malformed-records-crash-the-pipeline","title":"3. \"Malformed records crash the pipeline\"","text":"<p>Problem: One bad CSV row fails the entire load.</p> <p>Solution: Route bad records to an error path using Spark options.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n      options:\n        mode: PERMISSIVE             # Don't fail on bad rows\n        columnNameOfCorruptRecord: _corrupt_record\n        badRecordsPath: /landing/errors/orders/\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>Result: - Valid rows load normally - Corrupt rows written to <code>badRecordsPath</code> for investigation - Pipeline doesn't fail</p>"},{"location":"tutorials/bronze_layer/#4-empty-source-files-break-downstream","title":"4. \"Empty source files break downstream\"","text":"<p>Problem: Source sends empty files, causing downstream failures.</p> <p>Solution: Add a row count contract.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n    contracts:\n      - type: row_count\n        min: 1                       # Fail if empty\n        severity: error\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>Severity options: | Severity | Behavior | |----------|----------| | <code>error</code> | Fail the node | | <code>warn</code> | Log warning, continue |</p> <p>See: Contracts</p>"},{"location":"tutorials/bronze_layer/#5-source-volume-dropped-90somethings-wrong","title":"5. \"Source volume dropped 90%\u2014something's wrong\"","text":"<p>Problem: Upstream system broke, sending almost no data.</p> <p>Solution: Add volume drop detection.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n    contracts:\n      - type: volume_drop\n        threshold: 0.5               # Fail if &lt;50% of previous run\n        lookback_runs: 3             # Compare to last 3 runs\n        severity: error\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre>"},{"location":"tutorials/bronze_layer/#6-i-need-to-reprocess-a-specific-date-range","title":"6. \"I need to reprocess a specific date range\"","text":"<p>Problem: Bug in source data, need to reload specific dates.</p> <p>Solution: Use Delta's partition replacement.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: overwrite\n      options:\n        replaceWhere: \"file_date &gt;= '2025-01-01' AND file_date &lt;= '2025-01-15'\"\n        partitionBy: [file_date]\n</code></pre> <p>How it works: - <code>replaceWhere</code> only replaces matching partitions - Rest of the table remains unchanged - Useful for targeted reloads without full table rebuild</p> <p>See: Windowed Reprocess Pattern</p>"},{"location":"tutorials/bronze_layer/#7-im-loading-from-sql-server-not-files","title":"7. \"I'm loading from SQL Server, not files\"","text":"<p>Problem: Source is a database table, not files.</p> <p>Solution: Use SQL read with stateful incremental.</p> <pre><code>connections:\n  source_db:\n    type: sql_server\n    server: server.database.windows.net\n    database: sales\n    user: \"${DB_USER}\"\n    password: \"${DB_PASSWORD}\"\n\nnodes:\n  - name: raw_orders\n    read:\n      connection: source_db\n      format: jdbc\n      table: dbo.orders\n      incremental:\n        mode: stateful               # Track HWM\n        column: updated_at           # Track by this column\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>How it works: - First run: loads all data, stores MAX(updated_at) as HWM - Next runs: loads only WHERE updated_at &gt; stored_hwm - HWM is updated after each successful run</p> <p>See: Incremental Loading Pattern</p>"},{"location":"tutorials/bronze_layer/#bronze-layer-checklist","title":"Bronze Layer Checklist","text":"<p>Before moving to Silver, verify:</p> <ul> <li>[ ] Append-only? Raw data is never overwritten (except intentional reprocess)</li> <li>[ ] Incremental? Only new/changed data is loaded each run</li> <li>[ ] Traceable? Each row has arrival metadata (<code>_loaded_at</code>, source file, etc.)</li> <li>[ ] Contracts? Row count, schema, or volume checks in place</li> <li>[ ] Error handling? Bad records routed to error path, not failing pipeline</li> </ul>"},{"location":"tutorials/bronze_layer/#next-steps","title":"Next Steps","text":"<ul> <li>Silver Layer Tutorial \u2014 Clean and transform Bronze data</li> <li>Append-Only Raw Pattern \u2014 Detailed pattern docs</li> <li>Getting Started \u2014 End-to-end first pipeline</li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started with Odibi","text":"<p>This tutorial will guide you through creating your first data pipeline. By the end, you will have a running project that reads data, cleans it, and generates an audit report (\"Data Story\").</p> <p>Prerequisites: *   Python 3.9 or higher installed. *   Basic familiarity with terminal/command line.</p>"},{"location":"tutorials/getting_started/#1-installation","title":"1. Installation","text":"<p>First, install Odibi. We recommend creating a virtual environment to keep your system clean.</p> <pre><code># 1. Create a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# 2. Install Odibi\npip install odibi\n</code></pre> <p>Note: If you plan to use Spark or Azure later, you can install <code>pip install \"odibi[spark,azure]\"</code>, but for this tutorial, the base package is enough.</p>"},{"location":"tutorials/getting_started/#2-create-sample-data","title":"2. Create Sample Data","text":"<p>Odibi shines when working with messy real-world data. Let's create some \"bad\" data to clean.</p> <p>Create a folder named <code>raw_data</code> and a file inside it named <code>customers.csv</code>:</p> <p>raw_data/customers.csv</p> <pre><code>id, name,           email,              joined_at\n1,  Alice,          alice@example.com,  2023-01-01\n2,  Bob,            bob@example.com,    2023-02-15\n3,  Charlie,        NULL,               2023-03-10\n4,  Dave,           dave@example.com,   invalid-date\n</code></pre> <p>(Notice the extra spaces, the NULL value, and the invalid date string.)</p>"},{"location":"tutorials/getting_started/#3-generate-your-project","title":"3. Generate Your Project","text":"<p>Instead of writing configuration files from scratch, use the Odibi Initializer. It creates a project skeleton with best practices baked in.</p> <p>Run this command in your terminal:</p> <pre><code>odibi init-pipeline my_first_project --template local-medallion\n</code></pre> <p>This creates a new folder <code>my_first_project</code> with a standard structure: *   <code>odibi.yaml</code>: The pipeline configuration. *   <code>data/</code>: Folders for your data layers (landing, raw, silver, etc.). *   <code>README.md</code>: Instructions for your project.</p> <p>Move your sample data into the landing zone:</p> <pre><code># On Windows (PowerShell)\nmv raw_data/customers.csv my_first_project/data/landing/\n# On Mac/Linux\nmv raw_data/customers.csv my_first_project/data/landing/\n</code></pre> <p>Note: You can also generate a project from existing data using <code>odibi generate-project</code>, but <code>init-pipeline</code> is the recommended way to start fresh.</p>"},{"location":"tutorials/getting_started/#4-explore-the-project","title":"4. Explore the Project","text":"<p>Navigate into your new project:</p> <pre><code>cd my_first_project\n</code></pre> <p>You will see a file structure like this:</p> <ul> <li><code>odibi.yaml</code>: The brain of your project. It defines the pipeline.</li> <li><code>sql/</code>: Contains SQL transformation files.</li> <li><code>data/</code>: (Created automatically) Where data will be stored.</li> </ul> <p>Open <code>odibi.yaml</code> in your text editor. You will see two \"nodes\" (steps): 1.  Ingestion Node: Reads the <code>customers.csv</code> from <code>landing/</code>. 2.  Refinement Node: Merges the data into <code>silver/</code>.</p> <p>Since we used the template, the config is already set up to look for <code>landing/customers.csv</code>.</p>"},{"location":"tutorials/getting_started/#5-run-the-pipeline","title":"5. Run the Pipeline","text":"<p>Now, execute the pipeline:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Odibi will: 1.  Read <code>customers.csv</code> from <code>landing/</code>. 2.  Convert it to Parquet in <code>raw/</code>. 3.  Merge it into a Delta/Parquet table in <code>silver/</code>. 4.  Generate a \"Data Story\".</p>"},{"location":"tutorials/getting_started/#6-view-the-data-story","title":"6. View the Data Story","text":"<p>Data engineering is often invisible. Odibi makes it visible. Every run generates a report.</p> <p>List the generated stories:</p> <pre><code>odibi story list\n</code></pre> <p>You will see output like:</p> <pre><code>\ud83d\udcda Stories in .odibi/stories:\n================================================================================\n  \ud83d\udcc4 main_documentation.html\n     Modified: 2025-11-21 14:30:00\n     Size: 15.2KB\n     Path: .odibi/stories/main_documentation.html\n</code></pre> <p>Open the HTML file in your browser to view the report: - Windows: <code>start .odibi/stories/main_documentation.html</code> - Mac: <code>open .odibi/stories/main_documentation.html</code> - Linux: <code>xdg-open .odibi/stories/main_documentation.html</code></p> <p>What to look for in the report: *   Row Counts: Did we lose any rows? *   Schema: Did the column types change? *   Execution Time: How long did it take?</p>"},{"location":"tutorials/getting_started/#7-add-data-validation","title":"7. Add Data Validation","text":"<p>Data pipelines are only as good as their data quality. Let's add validation tests to catch bad data before it corrupts your warehouse.</p>"},{"location":"tutorials/getting_started/#inline-validation-in-yaml","title":"Inline Validation in YAML","text":"<p>Add validation tests directly to your node:</p> <pre><code>nodes:\n  - name: customers\n    read:\n      connection: landing\n      format: csv\n      path: customers.csv\n    validation:\n      tests:\n        - type: not_null\n          columns: [id, name]\n        - type: unique\n          columns: [id]\n        - type: row_count\n          min: 1\n      on_failure: warn  # or \"error\" to stop the pipeline\n    write:\n      connection: raw\n      format: parquet\n      path: customers\n</code></pre>"},{"location":"tutorials/getting_started/#using-contracts-for-input-validation","title":"Using Contracts for Input Validation","text":"<p>Contracts validate data before processing:</p> <pre><code>nodes:\n  - name: validate_orders\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id, amount]\n      - type: freshness\n        column: created_at\n        max_age: \"24h\"\n    read:\n      connection: landing\n      path: orders.csv\n    write:\n      connection: raw\n      path: orders\n</code></pre> <p>If contracts fail, the pipeline stops immediately with clear error messages.</p>"},{"location":"tutorials/getting_started/#running-validation","title":"Running Validation","text":"<p>Run the pipeline and watch for validation warnings:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Validation results appear in both the console output and the Data Story.</p>"},{"location":"tutorials/getting_started/#8-building-dimensions-scd2","title":"8. Building Dimensions (SCD2)","text":"<p>Once you're comfortable with basic pipelines, you can build proper dimensional models. Here's a quick example of a Slowly Changing Dimension Type 2:</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: bronze\n      table: raw_customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id        # Business key\n        surrogate_key: customer_sk      # Generated integer key\n        scd_type: 2                     # Track history\n        track_cols: [name, email, city]\n        target: silver.dim_customer     # Read existing for merge\n        unknown_member: true            # Add SK=0 for orphans\n    write:\n      connection: silver\n      table: dim_customer\n</code></pre> <p>What this does: - Generates integer surrogate keys (<code>customer_sk</code>) - Tracks changes to <code>name</code>, <code>email</code>, <code>city</code> over time - Maintains <code>is_current</code>, <code>valid_from</code>, <code>valid_to</code> columns - Creates an \"unknown\" row (SK=0) for handling orphan fact records</p> <p>For a complete dimensional modeling tutorial, see Dimensional Modeling.</p>"},{"location":"tutorials/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/getting_started/#modulenotfounderror-no-module-named-odibi","title":"\"ModuleNotFoundError: No module named 'odibi'\"","text":"<p>Cause: Odibi not installed or virtual environment not activated.</p> <p>Fix:</p> <pre><code># Activate your virtual environment first\nsource .venv/bin/activate  # Linux/Mac\n.venv\\Scripts\\activate     # Windows\n\n# Then verify installation\npip show odibi\n</code></pre>"},{"location":"tutorials/getting_started/#pipeline-runs-but-no-output-files","title":"Pipeline runs but no output files","text":"<p>Causes: - Write path doesn't exist - Permission denied on output directory - Dry-run mode enabled</p> <p>Fix:</p> <pre><code># Check if dry-run is enabled (remove --dry-run flag)\nodibi run odibi.yaml\n\n# Ensure output directory exists\nmkdir -p data/silver\n</code></pre>"},{"location":"tutorials/getting_started/#no-such-file-or-directory-for-input-data","title":"\"No such file or directory\" for input data","text":"<p>Cause: File path in config doesn't match actual location.</p> <p>Fix: Verify the path relative to where you run the command:</p> <pre><code># If config says: path: landing/customers.csv\n# File should be at: ./data/landing/customers.csv (relative to base_path)\n\nls data/landing/customers.csv\n</code></pre>"},{"location":"tutorials/getting_started/#story-not-generated","title":"Story not generated","text":"<p>Causes: - Story connection not configured - Story path doesn't exist</p> <p>Fix: Ensure your config has a story section:</p> <pre><code>story:\n  connection: raw_data  # Must match a defined connection\n  path: stories/\n</code></pre>"},{"location":"tutorials/getting_started/#9-whats-next","title":"9. What's Next?","text":"<p>You have successfully built a data pipeline with data validation!</p> <ul> <li>Incremental Loading: Learn how to efficiently process only new data using State Tracking (\"Auto-Pilot\").</li> <li>Write Custom Transformations: Learn how to add Python logic (like advanced validation) to your pipeline.</li> <li>Data Validation Guide: Deep dive into all validation options.</li> <li>Spark Engine Tutorial: Scale up with Apache Spark.</li> <li>Azure Connections: Connect to Azure Blob, ADLS, and SQL.</li> <li>Master the CLI: Learn about <code>odibi stress</code> and <code>odibi doctor</code>.</li> </ul>"},{"location":"tutorials/gold_layer/","title":"Gold Layer Tutorial","text":"<p>The Gold Layer is where business-ready datasets live. Fact tables, aggregations, and semantic metrics\u2014optimized for consumption.</p>"},{"location":"tutorials/gold_layer/#layer-philosophy","title":"Layer Philosophy","text":"<p>\"Answers, not data.\"</p> <p>Gold is consumption-optimized. BI tools, dashboards, and ML models read from Gold. Queries should be fast and intuitive.</p> Principle Why Denormalized Fewer joins = faster queries Pre-aggregated Common rollups pre-computed Business-named Column names match business terms SK-based Surrogate keys for dimension lookups"},{"location":"tutorials/gold_layer/#quick-start-fact-table","title":"Quick Start: Fact Table","text":"<p>The most common Gold pattern is a fact table with dimension lookups:</p> <pre><code># pipelines/gold/fact_orders.yaml\npipelines:\n  - pipeline: gold_fact_orders\n    layer: gold\n    nodes:\n      - name: fact_orders\n        read:\n          connection: silver\n          table: orders\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, line_item_id]\n            dimensions:\n              - name: dim_customer\n                lookup_key: customer_id\n                surrogate_key: customer_sk\n                target: silver.dim_customer\n              - name: dim_product\n                lookup_key: product_id\n                surrogate_key: product_sk\n                target: silver.dim_product\n              - name: dim_date\n                lookup_key: order_date\n                surrogate_key: date_sk\n                target: gold.dim_date\n            orphan_handling: unknown\n        write:\n          connection: gold\n          table: fact_orders\n</code></pre>"},{"location":"tutorials/gold_layer/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":""},{"location":"tutorials/gold_layer/#1-how-do-i-build-a-star-schema-fact-table","title":"1. \"How do I build a star schema fact table?\"","text":"<p>Problem: Need to replace natural keys with surrogate keys from dimensions.</p> <p>Solution: Use the fact pattern with dimension lookups.</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: silver\n      table: orders\n    pattern:\n      type: fact\n      params:\n        grain: [order_id]            # One row per order\n        dimensions:\n          - name: dim_customer\n            lookup_key: customer_id\n            surrogate_key: customer_sk\n            target: silver.dim_customer\n          - name: dim_product\n            lookup_key: product_id\n            surrogate_key: product_sk\n            target: silver.dim_product\n    write:\n      connection: gold\n      table: fact_orders\n</code></pre> <p>Result:</p> <pre><code>order_id | customer_sk | product_sk | order_total | order_date\n1        | 42          | 15         | 150.00      | 2025-01-15\n2        | 42          | 23         | 75.00       | 2025-01-16\n</code></pre> <p>See: Fact Pattern</p>"},{"location":"tutorials/gold_layer/#2-orders-reference-customers-that-dont-exist-orphans","title":"2. \"Orders reference customers that don't exist (orphans)\"","text":"<p>Problem: Some orders have customer_id values not in dim_customer.</p> <p>Solution: Configure orphan handling.</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - name: dim_customer\n        lookup_key: customer_id\n        surrogate_key: customer_sk\n        target: silver.dim_customer\n    orphan_handling: unknown         # Assign to unknown member\n</code></pre> <p>Options for <code>orphan_handling</code>: | Option | Behavior | |--------|----------| | <code>unknown</code> | Assign SK = -1 (unknown member) | | <code>quarantine</code> | Route to quarantine table | | <code>error</code> | Fail the pipeline | | <code>null</code> | Set SK = NULL |</p> <p>See: Fact Pattern - Orphan Handling</p>"},{"location":"tutorials/gold_layer/#3-i-need-a-date-dimension","title":"3. \"I need a date dimension\"","text":"<p>Problem: Need a standard date dimension for time-based analysis.</p> <p>Solution: Use the date dimension pattern.</p> <pre><code>nodes:\n  - name: dim_date\n    pattern:\n      type: date_dimension\n      params:\n        start_date: \"2020-01-01\"\n        end_date: \"2030-12-31\"\n        columns:\n          - date_sk               # Surrogate key (YYYYMMDD)\n          - full_date             # DATE type\n          - day_of_week           # Monday, Tuesday, ...\n          - day_of_month          # 1-31\n          - month_name            # January, February, ...\n          - month_number          # 1-12\n          - quarter               # Q1, Q2, Q3, Q4\n          - year                  # 2024, 2025, ...\n          - is_weekend            # true/false\n          - fiscal_year           # Custom fiscal calendar\n    write:\n      connection: gold\n      table: dim_date\n</code></pre> <p>See: Date Dimension Pattern</p>"},{"location":"tutorials/gold_layer/#4-i-need-pre-aggregated-metrics","title":"4. \"I need pre-aggregated metrics\"","text":"<p>Problem: Dashboards are slow\u2014need pre-computed rollups.</p> <p>Solution: Use the aggregation pattern.</p> <pre><code>nodes:\n  - name: daily_sales\n    read:\n      connection: gold\n      table: fact_orders\n    pattern:\n      type: aggregation\n      params:\n        dimensions: [date_sk, product_sk]\n        measures:\n          - name: total_revenue\n            expression: \"SUM(order_total)\"\n          - name: order_count\n            expression: \"COUNT(*)\"\n          - name: avg_order_value\n            expression: \"AVG(order_total)\"\n        incremental: true            # Merge new days\n    write:\n      connection: gold\n      table: agg_daily_sales\n</code></pre> <p>Result:</p> <pre><code>date_sk  | product_sk | total_revenue | order_count | avg_order_value\n20250115 | 15         | 1500.00       | 10          | 150.00\n20250115 | 23         | 750.00        | 10          | 75.00\n</code></pre> <p>See: Aggregation Pattern</p>"},{"location":"tutorials/gold_layer/#5-i-want-to-define-reusable-metrics-for-bi","title":"5. \"I want to define reusable metrics for BI\"","text":"<p>Problem: Different dashboards calculate \"revenue\" differently.</p> <p>Solution: Define semantic metrics.</p> <pre><code>semantic:\n  metrics:\n    - name: revenue\n      expression: \"SUM(order_total)\"\n      description: \"Total order revenue\"\n      format: currency\n\n    - name: order_count\n      expression: \"COUNT(DISTINCT order_id)\"\n      description: \"Number of unique orders\"\n      format: integer\n\n    - name: aov\n      expression: \"SUM(order_total) / COUNT(DISTINCT order_id)\"\n      description: \"Average order value\"\n      format: currency\n      depends_on: [revenue, order_count]\n\n  dimensions:\n    - name: customer_name\n      column: dim_customer.name\n\n    - name: product_category\n      column: dim_product.category\n\n    - name: order_month\n      column: dim_date.month_name\n</code></pre> <p>See: Semantic Layer, Defining Metrics</p>"},{"location":"tutorials/gold_layer/#6-how-do-i-materialize-semantic-metrics-to-tables","title":"6. \"How do I materialize semantic metrics to tables?\"","text":"<p>Problem: Want to query metrics from SQL, not just the API.</p> <p>Solution: Materialize metrics to Gold tables.</p> <pre><code>nodes:\n  - name: materialized_revenue\n    semantic:\n      materialize:\n        metrics: [revenue, order_count, aov]\n        dimensions: [product_category, order_month]\n        target: gold.revenue_by_category_month\n</code></pre> <p>See: Materializing Metrics</p>"},{"location":"tutorials/gold_layer/#7-reference-data-rarely-changesskip-if-unchanged","title":"7. \"Reference data rarely changes\u2014skip if unchanged\"","text":"<p>Problem: Date dimension regenerates every run unnecessarily.</p> <p>Solution: Skip if content hash is unchanged.</p> <pre><code>nodes:\n  - name: dim_date\n    transformer: date_dimension\n    params:\n      start_date: \"2020-01-01\"\n      end_date: \"2030-12-31\"\n    write:\n      connection: gold\n      table: dim_date\n      format: delta\n      skip_if_unchanged: true        # Skip if content hash matches\n</code></pre> <p>How it works: - Before writing, Odibi computes a SHA256 hash of the DataFrame - Compares to hash stored in Delta table metadata - Skips write if hashes match (saves storage and compute)</p> <p>See: Skip If Unchanged Pattern</p>"},{"location":"tutorials/gold_layer/#8-how-do-i-validate-fact-table-grain","title":"8. \"How do I validate fact table grain?\"","text":"<p>Problem: Want to ensure no duplicate rows per grain key.</p> <p>Solution: Add grain validation contract.</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: silver\n      table: orders\n    contracts:\n      - type: unique\n        columns: [order_id, line_item_id]  # Grain columns\n        severity: error\n    pattern:\n      type: fact\n      params:\n        grain: [order_id, line_item_id]\n    write:\n      connection: gold\n      table: fact_orders\n</code></pre>"},{"location":"tutorials/gold_layer/#gold-layer-checklist","title":"Gold Layer Checklist","text":"<p>Before exposing to BI:</p> <ul> <li>[ ] Star schema? Facts reference dimensions via surrogate keys</li> <li>[ ] Grain validated? No duplicate rows per grain key</li> <li>[ ] Orphans handled? Missing dimension members \u2192 unknown or quarantine</li> <li>[ ] Pre-aggregated? Common rollups materialized</li> <li>[ ] Documented? Semantic layer defines metrics and dimensions</li> </ul>"},{"location":"tutorials/gold_layer/#star-schema-example","title":"Star Schema Example","text":"<pre><code>                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502   dim_date      \u2502\n                   \u2502  date_sk (PK)   \u2502\n                   \u2502  full_date      \u2502\n                   \u2502  month_name     \u2502\n                   \u2502  year           \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  dim_customer   \u2502    \u2502   fact_orders    \u2502    \u2502  dim_product    \u2502\n\u2502 customer_sk(PK) \u2502\u25c4\u2500\u2500\u2500\u2502 customer_sk(FK)  \u2502\u2500\u2500\u2500\u25ba\u2502 product_sk (PK) \u2502\n\u2502 customer_id     \u2502    \u2502 product_sk (FK)  \u2502    \u2502 product_id      \u2502\n\u2502 name            \u2502    \u2502 date_sk (FK)     \u2502    \u2502 name            \u2502\n\u2502 city            \u2502    \u2502 order_id         \u2502    \u2502 category        \u2502\n\u2502 state           \u2502    \u2502 order_total      \u2502    \u2502 price           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 quantity         \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/gold_layer/#next-steps","title":"Next Steps","text":"<ul> <li>Fact Pattern \u2014 Detailed fact table configuration</li> <li>Aggregation Pattern \u2014 Pre-computed rollups</li> <li>Semantic Layer Overview \u2014 Reusable metrics</li> <li>Dimensional Modeling Tutorial \u2014 Full walkthrough</li> </ul>"},{"location":"tutorials/silver_layer/","title":"Silver Layer Tutorial","text":"<p>The Silver Layer is where data gets cleaned, deduplicated, and conformed. This is your trusted, query-ready data.</p>"},{"location":"tutorials/silver_layer/#layer-philosophy","title":"Layer Philosophy","text":"<p>\"Clean once, use everywhere.\"</p> <p>Silver is your single source of truth. All downstream consumers (Gold, BI, ML) should read from Silver, not Bronze.</p> Principle Why Deduplicated One row per key Validated Data quality enforced Typed Consistent data types Conformed Standard naming, formats"},{"location":"tutorials/silver_layer/#the-one-source-test","title":"The One-Source Test","text":"<p>\"Could this node run if only ONE source system existed?\"</p> <ul> <li>YES \u2192 Silver \u2713</li> <li>NO \u2192 Probably Gold</li> </ul> <p>!!! note \"Reference Tables Are Allowed\"     Reference/lookup table joins ARE allowed in Silver. The test refers to business source systems, not supporting data.</p> <pre><code>- \u2705 `orders` JOIN `product_codes` (lookup) = Silver\n- \u274c `sap_orders` JOIN `salesforce_customers` = Gold\n</code></pre>"},{"location":"tutorials/silver_layer/#quick-start-merge-from-bronze","title":"Quick Start: Merge from Bronze","text":"<p>The most common Silver pattern merges Bronze data into a deduplicated table:</p> <pre><code># pipelines/silver/orders.yaml\npipelines:\n  - pipeline: silver_orders\n    layer: silver\n    nodes:\n      - name: orders\n        read:\n          connection: bronze\n          table: raw_orders\n        transformer: merge\n        params:\n          target: silver.orders\n          keys: [order_id]\n          strategy: upsert\n        write:\n          connection: silver\n          table: orders\n</code></pre>"},{"location":"tutorials/silver_layer/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":""},{"location":"tutorials/silver_layer/#1-bronze-has-duplicates-how-do-i-get-one-row-per-key","title":"1. \"Bronze has duplicates, how do I get one row per key?\"","text":"<p>Problem: Raw data has multiple versions of the same record.</p> <p>Solution: Use the merge transformer with deduplication.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transformer: deduplicate\n    params:\n      keys: [order_id]\n      order_by: \"updated_at DESC\"    # Keep most recent\n    write:\n      connection: silver\n      table: orders\n</code></pre> <p>Or use merge for incremental upsert:</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transformer: merge\n    params:\n      target: silver.orders\n      keys: [order_id]\n      strategy: upsert\n      audit_cols:\n        created_col: _sys_created_at\n        updated_col: _sys_updated_at\n</code></pre> <p>See: Merge/Upsert Pattern</p>"},{"location":"tutorials/silver_layer/#2-i-need-to-track-dimension-history-scd-type-2","title":"2. \"I need to track dimension history (SCD Type 2)\"","text":"<p>Problem: Customer address changes\u2014need to keep historical versions.</p> <p>Solution: Use the dimension pattern with SCD Type 2.</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: bronze\n      table: raw_customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id\n        surrogate_key: customer_sk\n        scd_type: 2\n        track_cols: [name, email, address, city, state]\n        target: silver.dim_customer\n        effective_from_col: valid_from\n        effective_to_col: valid_to\n        current_flag_col: is_current\n    write:\n      connection: silver\n      table: dim_customer\n</code></pre> <p>Result:</p> <pre><code>customer_sk | customer_id | name     | city      | valid_from | valid_to   | is_current\n1           | C001        | Alice    | Chicago   | 2024-01-01 | 2024-06-01 | false\n2           | C001        | Alice    | Boston    | 2024-06-01 | 9999-12-31 | true\n</code></pre> <p>See: SCD2 Pattern</p>"},{"location":"tutorials/silver_layer/#3-just-overwrite-dimensions-i-dont-need-history","title":"3. \"Just overwrite dimensions, I don't need history\"","text":"<p>Problem: Reference data that should just reflect current state.</p> <p>Solution: Use SCD Type 1 (no history).</p> <pre><code>nodes:\n  - name: dim_product\n    read:\n      connection: bronze\n      table: raw_products\n    pattern:\n      type: dimension\n      params:\n        natural_key: product_id\n        surrogate_key: product_sk\n        scd_type: 1                  # Overwrite changes\n        target: silver.dim_product\n    write:\n      connection: silver\n      table: dim_product\n</code></pre> <p>See: Dimension Pattern</p>"},{"location":"tutorials/silver_layer/#4-how-do-i-validate-data-quality-in-silver","title":"4. \"How do I validate data quality in Silver?\"","text":"<p>Problem: Want to catch bad data before it reaches Gold.</p> <p>Solution: Add validation tests with quarantine.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    validation:\n      tests:\n        - column: order_id\n          test: not_null\n        - column: order_total\n          test: positive\n        - column: customer_id\n          test: not_null\n        - column: order_date\n          test: not_future\n      quarantine:\n        connection: silver\n        table: _quarantine_orders\n      gate:\n        require_pass_rate: 0.95      # Allow 5% failures\n    write:\n      connection: silver\n      table: orders\n</code></pre> <p>Result: - Rows passing all tests \u2192 <code>silver.orders</code> - Rows failing tests \u2192 <code>silver._quarantine_orders</code> for review - Pipeline fails if pass rate &lt; 95%</p> <p>See: Quality Gates, Quarantine</p>"},{"location":"tutorials/silver_layer/#5-i-need-to-apply-custom-sql-transformations","title":"5. \"I need to apply custom SQL transformations\"","text":"<p>Problem: Need to clean/transform data with custom logic.</p> <p>Solution: Use transform steps.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transform:\n      steps:\n        - type: sql\n          query: |\n            SELECT\n              order_id,\n              UPPER(TRIM(customer_name)) AS customer_name,\n              CAST(order_date AS DATE) AS order_date,\n              COALESCE(order_total, 0) AS order_total,\n              CASE\n                WHEN status = 'C' THEN 'Completed'\n                WHEN status = 'P' THEN 'Pending'\n                ELSE 'Unknown'\n              END AS order_status\n            FROM {input}\n            WHERE order_id IS NOT NULL\n    write:\n      connection: silver\n      table: orders\n</code></pre> <p>See: Writing Transformations</p>"},{"location":"tutorials/silver_layer/#6-records-were-deleted-in-sourcehow-do-i-detect-that","title":"6. \"Records were deleted in source\u2014how do I detect that?\"","text":"<p>Problem: Source system hard-deletes records, need to flag them.</p> <p>Solution: Use delete detection.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transformer: merge\n    params:\n      target: silver.orders\n      keys: [order_id]\n      strategy: upsert\n    delete_detection:\n      mode: sql_compare              # Compare source to target\n      soft_delete_col: is_deleted    # Flag instead of delete\n      deleted_at_col: deleted_at     # Timestamp of detection\n</code></pre> <p>Result:</p> <pre><code>order_id | ... | is_deleted | deleted_at\n1        | ... | false      | NULL\n2        | ... | true       | 2025-01-15 10:30:00  \u2190 Detected as deleted\n</code></pre> <p>See: Delete Detection Config</p>"},{"location":"tutorials/silver_layer/#7-i-need-to-check-foreign-key-relationships","title":"7. \"I need to check foreign key relationships\"","text":"<p>Problem: Orders reference customers that don't exist.</p> <p>Solution: Use the FK validation Python API (not YAML\u2014this is a programmatic feature).</p> <pre><code>from odibi.validation.fk import FKValidator, RelationshipRegistry, RelationshipConfig\n\n# Define relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_id\",\n        dimension_key=\"customer_id\",\n        on_violation=\"warn\"  # or \"error\", \"quarantine\"\n    )\n]\n\n# Validate\nregistry = RelationshipRegistry(relationships=relationships)\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(orders_df, \"orders\", context)\n\nif not report.all_valid:\n    print(f\"Found {len(report.orphan_records)} orphan records\")\n</code></pre> <p>See: FK Validation</p>"},{"location":"tutorials/silver_layer/#8-i-need-to-join-data-from-multiple-bronze-tables","title":"8. \"I need to join data from multiple Bronze tables\"","text":"<p>Problem: Order details and order headers in separate tables.</p> <p>Solution: Use multi-read with SQL join.</p> <pre><code>nodes:\n  - name: orders_enriched\n    read:\n      - alias: headers\n        connection: bronze\n        table: raw_order_headers\n      - alias: details\n        connection: bronze\n        table: raw_order_details\n    transform:\n      steps:\n        - type: sql\n          query: |\n            SELECT\n              h.order_id,\n              h.order_date,\n              h.customer_id,\n              d.product_id,\n              d.quantity,\n              d.unit_price\n            FROM headers h\n            JOIN details d ON h.order_id = d.order_id\n    write:\n      connection: silver\n      table: orders_enriched\n</code></pre>"},{"location":"tutorials/silver_layer/#silver-layer-checklist","title":"Silver Layer Checklist","text":"<p>Before moving to Gold, verify:</p> <ul> <li>[ ] Deduplicated? One row per natural key</li> <li>[ ] Validated? Quality tests passing</li> <li>[ ] Typed? Consistent data types (dates, numbers, etc.)</li> <li>[ ] Complete? FK relationships valid (or orphans quarantined)</li> <li>[ ] Conformed? Naming conventions followed</li> </ul>"},{"location":"tutorials/silver_layer/#next-steps","title":"Next Steps","text":"<ul> <li>Gold Layer Tutorial \u2014 Build facts and aggregations</li> <li>Dimension Pattern \u2014 SCD1/SCD2 details</li> <li>Merge/Upsert Pattern \u2014 Deduplication and upsert</li> </ul>"},{"location":"tutorials/spark_engine/","title":"Getting Started with Spark Engine","text":"<p>This tutorial shows how to use Odibi with Apache Spark for large-scale data processing.</p>"},{"location":"tutorials/spark_engine/#prerequisites","title":"Prerequisites","text":"<ul> <li>Odibi installed (<code>pip install odibi</code>)</li> <li>Apache Spark 3.x installed or access to Databricks/Synapse</li> <li>Basic familiarity with Getting Started</li> </ul>"},{"location":"tutorials/spark_engine/#why-spark","title":"Why Spark?","text":"Use Case Recommended Engine Small datasets (&lt;1GB) <code>pandas</code> Medium datasets (1-10GB) <code>polars</code> Large datasets (&gt;10GB) <code>spark</code> Streaming data <code>spark</code> Delta Lake tables <code>spark</code>"},{"location":"tutorials/spark_engine/#1-configure-spark-engine","title":"1. Configure Spark Engine","text":"<p>Set <code>engine: spark</code> in your project configuration:</p> <pre><code># project.yaml\nproject: \"spark_demo\"\nengine: spark  # Use Spark instead of Pandas\n\nconnections:\n  landing:\n    type: local\n    base_path: /data/landing\n\n  bronze:\n    type: delta\n    catalog: spark_catalog\n    schema: bronze\n\n  silver:\n    type: delta\n    catalog: spark_catalog\n    schema: silver\n\nstory:\n  connection: landing\n  path: stories/\n\nsystem:\n  connection: landing\n  path: catalog/\n</code></pre>"},{"location":"tutorials/spark_engine/#2-delta-lake-connections","title":"2. Delta Lake Connections","text":"<p>Spark works best with Delta Lake for ACID transactions and time travel:</p> <pre><code>connections:\n  # Unity Catalog (Databricks)\n  unity_bronze:\n    type: delta\n    catalog: main\n    schema: bronze\n\n  # Hive Metastore\n  hive_silver:\n    type: delta\n    catalog: spark_catalog\n    schema: silver\n\n  # Path-based Delta (no catalog)\n  adls_gold:\n    type: delta\n    base_path: abfss://container@account.dfs.core.windows.net/gold\n</code></pre>"},{"location":"tutorials/spark_engine/#3-basic-spark-pipeline","title":"3. Basic Spark Pipeline","text":"<pre><code># pipelines/bronze/ingest.yaml\npipelines:\n  - pipeline: bronze_ingest\n    layer: bronze\n    nodes:\n      - name: raw_orders\n        read:\n          connection: landing\n          format: csv\n          path: orders/*.csv\n          options:\n            header: true\n            inferSchema: true\n        write:\n          connection: bronze\n          table: raw_orders\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/spark_engine/#4-spark-specific-features","title":"4. Spark-Specific Features","text":""},{"location":"tutorials/spark_engine/#streaming-ingestion","title":"Streaming Ingestion","text":"<p>Process real-time data from Kafka or Event Hub:</p> <pre><code>nodes:\n  - name: stream_events\n    streaming: true  # Enable Spark Structured Streaming\n    read:\n      connection: event_hub\n      format: kafka\n      options:\n        kafka.bootstrap.servers: \"${KAFKA_BROKERS}\"\n        subscribe: events\n        startingOffsets: latest\n    write:\n      connection: bronze\n      table: events\n      mode: append\n      options:\n        checkpointLocation: /checkpoints/events\n</code></pre>"},{"location":"tutorials/spark_engine/#delta-optimizations","title":"Delta Optimizations","text":"<pre><code>nodes:\n  - name: optimize_facts\n    read:\n      connection: silver\n      table: fact_orders\n    post_sql:\n      - \"OPTIMIZE silver.fact_orders ZORDER BY (order_date, customer_sk)\"\n      - \"VACUUM silver.fact_orders RETAIN 168 HOURS\"\n    write:\n      connection: silver\n      table: fact_orders\n      mode: overwrite\n</code></pre>"},{"location":"tutorials/spark_engine/#partition-pruning","title":"Partition Pruning","text":"<pre><code>nodes:\n  - name: partitioned_orders\n    read:\n      connection: bronze\n      table: raw_orders\n    write:\n      connection: silver\n      table: orders\n      options:\n        partitionBy: [order_date]\n        replaceWhere: \"order_date &gt;= '2024-01-01'\"\n</code></pre>"},{"location":"tutorials/spark_engine/#5-performance-tuning","title":"5. Performance Tuning","text":""},{"location":"tutorials/spark_engine/#spark-specific-settings","title":"Spark-Specific Settings","text":"<pre><code>performance:\n  use_arrow: true              # PyArrow for Pandas UDFs\n  default_parallelism: 200     # Spark partitions\n  delta_table_properties:\n    delta.columnMapping.mode: name\n    delta.autoOptimize.optimizeWrite: true\n</code></pre>"},{"location":"tutorials/spark_engine/#skip-expensive-operations","title":"Skip Expensive Operations","text":"<p>For high-throughput Bronze ingestion:</p> <pre><code>performance:\n  skip_null_profiling: true    # Skip NULL count (saves 1 Spark job)\n  skip_catalog_writes: true    # Skip metadata tracking\n  skip_run_logging: true       # Skip run history\n</code></pre>"},{"location":"tutorials/spark_engine/#caching-hot-dataframes","title":"Caching Hot DataFrames","text":"<pre><code>nodes:\n  - name: dim_customer\n    cache: true  # Cache in memory for reuse\n    read:\n      connection: silver\n      table: dim_customer\n\n  - name: fact_orders\n    depends_on: [dim_customer]  # Uses cached dim_customer\n    # ...\n</code></pre>"},{"location":"tutorials/spark_engine/#6-scd2-with-spark","title":"6. SCD2 with Spark","text":"<p>Slowly Changing Dimensions work seamlessly with Spark:</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: bronze\n      table: raw_customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id\n        surrogate_key: customer_sk\n        scd_type: 2\n        track_cols: [name, email, address, city]\n        target: silver.dim_customer\n        unknown_member: true\n    write:\n      connection: silver\n      table: dim_customer\n</code></pre>"},{"location":"tutorials/spark_engine/#7-running-on-databricks","title":"7. Running on Databricks","text":""},{"location":"tutorials/spark_engine/#option-1-databricks-asset-bundles","title":"Option 1: Databricks Asset Bundles","text":"<pre><code># databricks.yml\nbundle:\n  name: odibi_pipelines\n\nresources:\n  jobs:\n    daily_pipeline:\n      name: \"[${bundle.environment}] Daily Pipeline\"\n      tasks:\n        - task_key: run_odibi\n          python_wheel_task:\n            package_name: odibi\n            entry_point: cli\n            parameters: [\"run\", \"--config\", \"project.yaml\"]\n</code></pre>"},{"location":"tutorials/spark_engine/#option-2-notebook","title":"Option 2: Notebook","text":"<pre><code># Databricks notebook cell\n%pip install odibi\n\nfrom odibi import run_project\n\nrun_project(\"project.yaml\", pipelines=[\"bronze_ingest\"])\n</code></pre>"},{"location":"tutorials/spark_engine/#8-common-issues","title":"8. Common Issues","text":""},{"location":"tutorials/spark_engine/#java-gateway-process-exited","title":"\"Java gateway process exited\"","text":"<p>Spark isn't installed or JAVA_HOME not set:</p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk\nexport SPARK_HOME=/opt/spark\n</code></pre>"},{"location":"tutorials/spark_engine/#out-of-memory","title":"Out of Memory","text":"<p>Increase driver/executor memory:</p> <pre><code>spark = SparkSession.builder \\\n    .config(\"spark.driver.memory\", \"8g\") \\\n    .config(\"spark.executor.memory\", \"16g\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"tutorials/spark_engine/#slow-small-files","title":"Slow Small Files","text":"<p>Use Delta's auto-optimize:</p> <pre><code>write:\n  options:\n    delta.autoOptimize.optimizeWrite: true\n    delta.autoOptimize.autoCompact: true\n</code></pre>"},{"location":"tutorials/spark_engine/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/spark_engine/#py4jjavaerror-java-gateway-process-exited","title":"\"Py4JJavaError: Java gateway process exited\"","text":"<p>Cause: Java not installed or JAVA_HOME not set.</p> <p>Fix:</p> <pre><code># Install Java 11 or 17\n# Ubuntu/Debian\nsudo apt install openjdk-11-jdk\n\n# Mac (Homebrew)\nbrew install openjdk@11\n\n# Set JAVA_HOME\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n</code></pre>"},{"location":"tutorials/spark_engine/#sparksession-not-found-or-no-module-named-pyspark","title":"\"SparkSession not found\" or \"No module named pyspark\"","text":"<p>Cause: Spark extras not installed.</p> <p>Fix:</p> <pre><code>pip install \"odibi[spark]\"\n</code></pre>"},{"location":"tutorials/spark_engine/#spark-job-hangs-or-is-extremely-slow","title":"Spark job hangs or is extremely slow","text":"<p>Causes: - Too many small files (small file problem) - Insufficient memory for driver/executors - Shuffle spill to disk</p> <p>Fixes:</p> <pre><code># Add performance tuning\nperformance:\n  spark:\n    conf:\n      spark.sql.shuffle.partitions: \"200\"\n      spark.sql.files.maxPartitionBytes: \"134217728\"  # 128MB\n</code></pre>"},{"location":"tutorials/spark_engine/#analysisexception-table-not-found","title":"\"AnalysisException: Table not found\"","text":"<p>Cause: Table not registered in Spark catalog.</p> <p>Fix: Use explicit path or register the table:</p> <pre><code>read:\n  connection: delta_lake\n  path: silver/customers    # Use path, not table name\n  format: delta\n</code></pre>"},{"location":"tutorials/spark_engine/#delta-lake-merge-fails-with-concurrent-modification","title":"Delta Lake MERGE fails with \"concurrent modification\"","text":"<p>Cause: Multiple jobs writing to same table simultaneously.</p> <p>Fixes: - Enable optimistic concurrency: <code>delta.isolationLevel: WriteSerializable</code> - Use Databricks workflows with job clusters (single writer) - Add retry logic in pipeline config</p>"},{"location":"tutorials/spark_engine/#next-steps","title":"Next Steps","text":"<ul> <li>Azure Connections - Connect to Azure Blob/ADLS</li> <li>Performance Tuning - Optimize large pipelines</li> <li>Dimensional Modeling - Build star schemas</li> </ul>"},{"location":"tutorials/spark_engine/#see-also","title":"See Also","text":"<ul> <li>YAML Schema Reference - Full configuration options</li> <li>Delta Connection Config - Delta settings</li> <li>Glossary - Terminology reference</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/","title":"Introduction to Dimensional Modeling","text":"<p>Welcome to the Odibi dimensional modeling tutorial series. This comprehensive guide will teach you how to build a complete data warehouse using dimensional modeling techniques, from scratch.</p> <p>Prerequisites: Basic SQL knowledge and familiarity with data concepts like tables and columns.</p> <p>What You'll Build: A complete star schema for a retail sales system, plus a semantic layer for business intelligence.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-is-dimensional-modeling","title":"What is Dimensional Modeling?","text":"<p>Dimensional modeling is a technique for organizing data to make it easy to query and understand. It was developed by Ralph Kimball and is the foundation of most data warehouses today.</p> <p>Think of it like organizing a library: - Facts are like the checkout receipts\u2014they record what happened (a book was borrowed) - Dimensions are like the card catalogs\u2014they describe the who, what, where, and when</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#the-star-schema","title":"The Star Schema","text":"<p>The most common dimensional model is the star schema, named because it looks like a star when diagrammed:</p> <pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        int order_sk PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        int quantity\n        decimal unit_price\n        decimal line_total\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string email\n        string region\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n        decimal price\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        string month_name\n        int year\n    }\n</code></pre> <p>The fact table sits in the center and contains measurements (quantities, amounts, counts). The dimension tables surround it and provide context (who bought it, what was purchased, when did it happen).</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#facts-vs-dimensions-the-grocery-receipt-analogy","title":"Facts vs Dimensions: The Grocery Receipt Analogy","text":"<p>Imagine you're at a grocery store and you get a receipt:</p> <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n   FRESH FOODS MARKET\n   Store #42 - Downtown\n   Date: Jan 15, 2024  Time: 2:34 PM\n   Cashier: Maria\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n   Organic Milk 1 gal       $4.99\n   Wheat Bread              $3.49\n   Bananas 2.5 lb           $1.87\n   Cheddar Cheese           $5.99\n\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   SUBTOTAL                $16.34\n   TAX                      $0.82\n   TOTAL                   $17.16\n\n   Paid: VISA ****1234\n\n   Thank you for shopping with us!\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre> <p>Facts (the measurements): - Quantity of each item - Price of each item - Total amount</p> <p>Dimensions (the context): - Who: The customer (you) - What: The products (milk, bread, bananas, cheese) - Where: The store (#42, Downtown) - When: The date and time - How: The payment method (VISA)</p> <p>In a data warehouse, we'd model this as:</p> Concept Fact or Dimension? Example Columns The line items Fact quantity, unit_price, line_total The customer Dimension name, email, loyalty_tier The product Dimension product_name, category, brand The store Dimension store_name, city, region The date Dimension day_of_week, month, year"},{"location":"tutorials/dimensional_modeling/01_introduction/#why-surrogate-keys","title":"Why Surrogate Keys?","text":"<p>You might wonder: \"If customers already have a customer_id, why do we need another key?\"</p> <p>Good question! Here's why we use surrogate keys (like <code>customer_sk</code>) instead of natural keys (like <code>customer_id</code>):</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-1-business-keys-change","title":"Problem 1: Business Keys Change","text":"<p>Imagine your source system uses email as the customer identifier. What happens when a customer changes their email?</p> <p>Using natural key (email):</p> <pre><code>-- Old orders are orphaned!\nSELECT * FROM orders WHERE customer_email = 'alice@oldmail.com';  -- No longer exists\nSELECT * FROM customers WHERE email = 'alice@oldmail.com';        -- Record was updated to new email\n</code></pre> <p>Using surrogate key:</p> <pre><code>-- Customer SK never changes, even if email does\nSELECT * FROM fact_orders WHERE customer_sk = 42;     -- Still works!\nSELECT * FROM dim_customer WHERE customer_sk = 42;    -- Returns current info\n</code></pre>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-2-tracking-history-scd-type-2","title":"Problem 2: Tracking History (SCD Type 2)","text":"<p>When you need to track historical changes, surrogate keys become essential:</p> <p>Customer Data Over Time:</p> customer_sk customer_id email valid_from valid_to is_current 42 C001 alice@oldmail.com 2023-01-01 2024-01-15 false 157 C001 alice@newmail.com 2024-01-15 NULL true <p>The same customer (C001) has two dimension rows with different surrogate keys. Orders placed before Jan 15 link to SK=42 (capturing the email at that time). Orders after link to SK=157.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-3-performance","title":"Problem 3: Performance","text":"<ul> <li>Surrogate keys are simple integers (4 bytes)</li> <li>Natural keys can be long strings (variable length)</li> <li>Integer joins are much faster than string joins</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-4-multi-source-integration","title":"Problem 4: Multi-Source Integration","text":"<p>When combining data from multiple systems:</p> Source System Customer ID CRM CUST-00042 E-commerce user_42 Support 42 <p>With surrogate keys, all three become customer_sk = 42, regardless of source format.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#the-unknown-member-problem","title":"The Unknown Member Problem","text":"<p>What happens when a fact record references a dimension that doesn't exist?</p> <p>Scenario: An order arrives with <code>customer_id = 'C999'</code>, but there's no customer with that ID in the dimension table.</p> <p>Without unknown member:</p> <pre><code>-- This join loses the order entirely!\nSELECT o.*, c.name\nFROM fact_orders o\nJOIN dim_customer c ON o.customer_sk = c.customer_sk\nWHERE o.order_id = 'ORD999';\n-- Returns: (no rows)\n</code></pre> <p>With unknown member (customer_sk = 0):</p> customer_sk customer_id name email 0 -1 Unknown Unknown 1 C001 Alice Johnson alice@example.com 2 C002 Bob Smith bob@example.com <p>Now orphan orders get assigned to customer_sk = 0:</p> <pre><code>SELECT o.*, c.name\nFROM fact_orders o\nJOIN dim_customer c ON o.customer_sk = c.customer_sk\nWHERE o.order_id = 'ORD999';\n-- Returns: order data with name = 'Unknown'\n</code></pre> <p>The order isn't lost\u2014it's explicitly marked as having an unknown customer, which you can investigate later.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-types-handling-changes","title":"SCD Types: Handling Changes","text":"<p>When dimension data changes, how should you handle it? There are three common strategies:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-0-never-update","title":"SCD Type 0: Never Update","text":"<p>The dimension never changes after initial load. Use for truly static data.</p> <p>Example: ISO country codes, fixed reference data</p> country_sk country_code country_name 1 USA United States 2 CAN Canada"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-1-overwrite","title":"SCD Type 1: Overwrite","text":"<p>Update the dimension in place. History is lost, but you always see current data.</p> <p>Example: Customer email (you only care about current contact info)</p> <p>Before:</p> customer_sk customer_id email 1 C001 alice@oldmail.com <p>After (email changed):</p> customer_sk customer_id email 1 C001 alice@newmail.com"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-2-track-history","title":"SCD Type 2: Track History","text":"<p>Create a new row for each version. Full audit trail preserved.</p> <p>Example: Customer address (for accurate point-in-time reporting)</p> <p>Before:</p> customer_sk customer_id city valid_from valid_to is_current 1 C001 Chicago 2020-01-01 NULL true <p>After (customer moved):</p> customer_sk customer_id city valid_from valid_to is_current 1 C001 Chicago 2020-01-01 2024-01-15 false 42 C001 Seattle 2024-01-15 NULL true"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-youll-build-in-this-tutorial-series","title":"What You'll Build in This Tutorial Series","text":"<p>By the end of this series, you'll have built:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-1-dimensional-modeling-foundations","title":"Part 1: Dimensional Modeling Foundations","text":"<ol> <li>Introduction (this tutorial) - Core concepts</li> <li>Dimension Pattern - Build customer dimension with SCD 0/1/2</li> <li>Date Dimension Pattern - Generate complete date dimension</li> <li>Fact Pattern - Build fact table with SK lookups</li> <li>Aggregation Pattern - Build pre-aggregated tables</li> <li>Full Star Schema - Complete working example</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-2-semantic-layer","title":"Part 2: Semantic Layer","text":"<ol> <li>Semantic Layer Intro - What and why</li> <li>Defining Metrics - Revenue, counts, averages</li> <li>Defining Dimensions - Regions, dates, hierarchies</li> <li>Querying Metrics - \"revenue BY region\" syntax</li> <li>Materializing Metrics - Pre-compute for dashboards</li> <li>Semantic Full Example - Complete semantic layer</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-3-data-quality","title":"Part 3: Data Quality","text":"<ol> <li>FK Validation - Ensure referential integrity</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#sample-data","title":"Sample Data","text":"<p>Throughout these tutorials, we'll use consistent sample data representing a retail business:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#customers-12-rows","title":"Customers (12 rows)","text":"customer_id name email region city C001 Alice Johnson alice@example.com North Chicago C002 Bob Smith bob@example.com South Houston C003 Carol White carol@example.com North Detroit C004 David Brown david@example.com East New York C005 Emma Davis emma@example.com West Seattle C006 Frank Miller frank@example.com South Miami C007 Grace Lee grace@example.com East Boston C008 Henry Wilson henry@example.com West Portland C009 Ivy Chen ivy@example.com North Minneapolis C010 Jack Taylor jack@example.com South Dallas C011 Karen Martinez karen@example.com East Philadelphia C012 Leo Anderson leo@example.com West Denver"},{"location":"tutorials/dimensional_modeling/01_introduction/#products-10-rows","title":"Products (10 rows)","text":"product_id name category price P001 Laptop Pro 15 Electronics $1,299.99 P002 Wireless Mouse Electronics $29.99 P003 Office Chair Furniture $249.99 P004 USB-C Hub Electronics $49.99 P005 Standing Desk Furniture $599.99 P006 Mechanical Keyboard Electronics $149.99 P007 Monitor 27\" Electronics $399.99 P008 Desk Lamp Furniture $45.99 P009 Webcam HD Electronics $79.99 P010 Filing Cabinet Furniture $189.99"},{"location":"tutorials/dimensional_modeling/01_introduction/#orders-30-rows-across-14-days","title":"Orders (30 rows across 14 days)","text":"<p>Sample orders spanning January 15-28, 2024, with various quantities and statuses.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-you-learned","title":"What You Learned","text":"<p>In this introduction, you learned:</p> <ul> <li>Dimensional modeling organizes data into facts and dimensions</li> <li>Star schemas put the fact table in the center, surrounded by dimensions</li> <li>Facts contain measurements (quantities, amounts)</li> <li>Dimensions provide context (who, what, where, when)</li> <li>Surrogate keys solve problems with changing business keys and enable history tracking</li> <li>Unknown members prevent orphan records from being lost</li> <li>SCD Types define how to handle dimension changes (0=static, 1=overwrite, 2=history)</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/#next-steps","title":"Next Steps","text":"<p>Ready to build your first dimension table?</p> <p>Next: Dimension Pattern Tutorial - Build a customer dimension with SCD support</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#navigation","title":"Navigation","text":"Previous Up Next - Tutorials Dimension Pattern"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/","title":"Dimension Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>dimension</code> pattern to build dimension tables with automatic surrogate key generation and SCD (Slowly Changing Dimension) support.</p> <p>What You'll Learn: - How surrogate keys are generated - SCD Type 0 (static) - never update - SCD Type 1 (overwrite) - update in place - SCD Type 2 (history) - track all changes - Unknown member handling</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#source-data","title":"Source Data","text":"<p>We'll start with this customer data (12 rows):</p> <p>Source Data (customers.csv) - 12 rows:</p> customer_id name email region city state C001 Alice Johnson alice@example.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david@example.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace@example.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-1-scd-type-0-static-dimensions","title":"Step 1: SCD Type 0 - Static Dimensions","text":"<p>When to use: Reference data that never changes (country codes, fixed lookups).</p> <p>SCD Type 0 creates surrogate keys but never updates existing records. New records are inserted, but changes to existing records are ignored.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd0\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 0\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#output-dim_customer-13-rows","title":"Output: dim_customer (13 rows)","text":"<p>After running with <code>scd_type: 0</code>, here's the dimension table with generated surrogate keys:</p> customer_sk customer_id name email region city state load_timestamp 0 -1 Unknown Unknown Unknown Unknown Unknown 1900-01-01 00:00:00 1 C001 Alice Johnson alice@example.com North Chicago IL 2024-01-15 10:00:00 2 C002 Bob Smith bob@example.com South Houston TX 2024-01-15 10:00:00 3 C003 Carol White carol@example.com North Detroit MI 2024-01-15 10:00:00 4 C004 David Brown david@example.com East New York NY 2024-01-15 10:00:00 5 C005 Emma Davis emma@example.com West Seattle WA 2024-01-15 10:00:00 6 C006 Frank Miller frank@example.com South Miami FL 2024-01-15 10:00:00 7 C007 Grace Lee grace@example.com East Boston MA 2024-01-15 10:00:00 8 C008 Henry Wilson henry@example.com West Portland OR 2024-01-15 10:00:00 9 C009 Ivy Chen ivy@example.com North Minneapolis MN 2024-01-15 10:00:00 10 C010 Jack Taylor jack@example.com South Dallas TX 2024-01-15 10:00:00 11 C011 Karen Martinez karen@example.com East Philadelphia PA 2024-01-15 10:00:00 12 C012 Leo Anderson leo@example.com West Denver CO 2024-01-15 10:00:00 <p>Key observations: - Row 0 is the unknown member (customer_sk = 0, customer_id = -1) - Surrogate keys are sequential integers starting at 1 - Each source row gets a unique SK</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-2-scd-type-1-overwrite-updates","title":"Step 2: SCD Type 1 - Overwrite Updates","text":"<p>When to use: Attributes where you only need the current value (email, phone, preferences).</p> <p>SCD Type 1 updates existing records in place when changes are detected. No history is preserved.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#the-update-scenario","title":"The Update Scenario","text":"<p>Three customers changed their email addresses:</p> <p>Updated Source Data (customers_updated.csv) - 3 changes highlighted:</p> customer_id name email region city state C001 Alice Johnson alice.johnson@newmail.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david.b@corporate.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace.lee@gmail.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration_1","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd1\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 1\n          track_cols:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          unknown_member: true\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#before-vs-after-comparison","title":"Before vs After Comparison","text":"<p>BEFORE (original load):</p> customer_sk customer_id email load_timestamp 0 -1 Unknown 1900-01-01 00:00:00 1 C001 alice@example.com 2024-01-15 10:00:00 4 C004 david@example.com 2024-01-15 10:00:00 7 C007 grace@example.com 2024-01-15 10:00:00 ... ... ... ... <p>AFTER (SCD1 update):</p> customer_sk customer_id email load_timestamp 0 -1 Unknown 1900-01-01 00:00:00 1 C001 alice.johnson@newmail.com 2024-01-20 14:30:00 4 C004 david.b@corporate.com 2024-01-20 14:30:00 7 C007 grace.lee@gmail.com 2024-01-20 14:30:00 ... ... ... ... <p>Key observations: - Same surrogate keys - C001 is still customer_sk = 1 - Values updated in place - old emails are gone - Timestamp updated - shows when the record was last modified - No history preserved - we can't see the old email addresses</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#complete-scd1-output-13-rows","title":"Complete SCD1 Output (13 rows)","text":"customer_sk customer_id name email region city state load_timestamp 0 -1 Unknown Unknown Unknown Unknown Unknown 1900-01-01 00:00:00 1 C001 Alice Johnson alice.johnson@newmail.com North Chicago IL 2024-01-20 14:30:00 2 C002 Bob Smith bob@example.com South Houston TX 2024-01-15 10:00:00 3 C003 Carol White carol@example.com North Detroit MI 2024-01-15 10:00:00 4 C004 David Brown david.b@corporate.com East New York NY 2024-01-20 14:30:00 5 C005 Emma Davis emma@example.com West Seattle WA 2024-01-15 10:00:00 6 C006 Frank Miller frank@example.com South Miami FL 2024-01-15 10:00:00 7 C007 Grace Lee grace.lee@gmail.com East Boston MA 2024-01-20 14:30:00 8 C008 Henry Wilson henry@example.com West Portland OR 2024-01-15 10:00:00 9 C009 Ivy Chen ivy@example.com North Minneapolis MN 2024-01-15 10:00:00 10 C010 Jack Taylor jack@example.com South Dallas TX 2024-01-15 10:00:00 11 C011 Karen Martinez karen@example.com East Philadelphia PA 2024-01-15 10:00:00 12 C012 Leo Anderson leo@example.com West Denver CO 2024-01-15 10:00:00"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-3-scd-type-2-full-history-tracking","title":"Step 3: SCD Type 2 - Full History Tracking","text":"<p>When to use: Attributes where historical accuracy matters (address for shipping analysis, tier for billing history).</p> <p>SCD Type 2 preserves full history by creating a new row for each change. Old versions are closed with a <code>valid_to</code> date.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#the-history-scenario","title":"The History Scenario","text":"<p>Same three customers changed their emails. With SCD2, we keep both versions:</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration_2","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd2\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#output-full-history-16-rows","title":"Output: Full History (16 rows)","text":"customer_sk customer_id name email region valid_from valid_to is_current 0 -1 Unknown Unknown Unknown 1900-01-01 NULL true 1 C001 Alice Johnson alice@example.com North 2024-01-15 2024-01-20 false 2 C002 Bob Smith bob@example.com South 2024-01-15 NULL true 3 C003 Carol White carol@example.com North 2024-01-15 NULL true 4 C004 David Brown david@example.com East 2024-01-15 2024-01-20 false 5 C005 Emma Davis emma@example.com West 2024-01-15 NULL true 6 C006 Frank Miller frank@example.com South 2024-01-15 NULL true 7 C007 Grace Lee grace@example.com East 2024-01-15 2024-01-20 false 8 C008 Henry Wilson henry@example.com West 2024-01-15 NULL true 9 C009 Ivy Chen ivy@example.com North 2024-01-15 NULL true 10 C010 Jack Taylor jack@example.com South 2024-01-15 NULL true 11 C011 Karen Martinez karen@example.com East 2024-01-15 NULL true 12 C012 Leo Anderson leo@example.com West 2024-01-15 NULL true 13 C001 Alice Johnson alice.johnson@newmail.com North 2024-01-20 NULL true 14 C004 David Brown david.b@corporate.com East 2024-01-20 NULL true 15 C007 Grace Lee grace.lee@gmail.com East 2024-01-20 NULL true <p>Key observations: - New surrogate keys for new versions (13, 14, 15) - Old versions marked closed (is_current = false, valid_to = 2024-01-20) - New versions marked current (is_current = true, valid_to = NULL) - Full history preserved - we can query data as of any point in time</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#how-to-query-scd2","title":"How to Query SCD2","text":"<p>Current view (most common):</p> <pre><code>SELECT * FROM dim_customer WHERE is_current = true;\n</code></pre> <p>Point-in-time query (as of January 17):</p> <pre><code>SELECT * FROM dim_customer \nWHERE '2024-01-17' &gt;= valid_from \n  AND ('2024-01-17' &lt; valid_to OR valid_to IS NULL);\n</code></pre> <p>Customer C001's email history:</p> <pre><code>SELECT customer_sk, email, valid_from, valid_to \nFROM dim_customer \nWHERE customer_id = 'C001'\nORDER BY valid_from;\n</code></pre> customer_sk email valid_from valid_to 1 alice@example.com 2024-01-15 2024-01-20 13 alice.johnson@newmail.com 2024-01-20 NULL"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#understanding-the-unknown-member","title":"Understanding the Unknown Member","text":"<p>The unknown member row (customer_sk = 0) is automatically created when <code>unknown_member: true</code>:</p> customer_sk customer_id name email all other columns 0 -1 Unknown Unknown Unknown <p>Why it matters:</p> <p>When building fact tables, orders might reference a customer_id that doesn't exist in the dimension (data quality issue, late-arriving data, etc.). Instead of: - Failing the pipeline (strict but inflexible) - Losing the order data (dangerous)</p> <p>We assign those orphan records to customer_sk = 0. This: - Preserves all fact data - Makes orphans easily identifiable - Allows later cleanup/investigation</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<p>Here's a complete YAML file you can run:</p> <pre><code># File: odibi_dimension_tutorial.yaml\nproject: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  # Initial load with SCD2\n  - pipeline: initial_load\n    description: \"First load of customer dimension\"\n    nodes:\n      - name: dim_customer\n        description: \"Customer dimension with SCD2 history tracking\"\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n\n  # Incremental update with changes\n  - pipeline: incremental_update\n    description: \"Process updates to customer dimension\"\n    nodes:\n      - name: dim_customer\n        description: \"Update customer dimension with new email addresses\"\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#python-api-alternative","title":"Python API Alternative","text":"<p>If you prefer Python over YAML:</p> <pre><code>from odibi.patterns.dimension import DimensionPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load source data\nsource_df = pd.read_csv(\"examples/tutorials/dimensional_modeling/data/customers.csv\")\n\n# Create pattern\npattern = DimensionPattern(params={\n    \"natural_key\": \"customer_id\",\n    \"surrogate_key\": \"customer_sk\",\n    \"scd_type\": 2,\n    \"track_cols\": [\"name\", \"email\", \"region\", \"city\", \"state\"],\n    \"unknown_member\": True,\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"crm\"\n    }\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern\ncontext = EngineContext(df=source_df, engine_type=EngineType.PANDAS)\nresult_df = pattern.execute(context)\n\n# View results\nprint(f\"Generated {len(result_df)} dimension rows\")\nprint(result_df.head(15))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>SCD Type 0 creates surrogate keys but never updates existing records</li> <li>SCD Type 1 updates records in place, losing history but keeping current data</li> <li>SCD Type 2 creates new rows for changes, preserving full history with valid_from/valid_to dates</li> <li>Surrogate keys are auto-generated integers, sequential starting from 1</li> <li>Unknown member (SK=0) provides a default for orphan FK handling</li> <li>track_cols defines which columns trigger a new version in SCD1/SCD2</li> <li>Audit columns (load_timestamp, source_system) track when/where data came from</li> </ul>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you can build customer dimensions, let's create a date dimension that's automatically generated.</p> <p>Next: Date Dimension Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#navigation","title":"Navigation","text":"Previous Up Next Introduction Tutorials Date Dimension"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Dimension Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/","title":"Date Dimension Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>date_dimension</code> pattern to generate a complete date dimension table with pre-calculated attributes for reporting and analytics.</p> <p>What You'll Learn: - Why you need a date dimension - How the pattern generates dates automatically - Understanding all 19 generated columns - Fiscal calendar configuration - Unknown date handling</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#why-do-you-need-a-date-dimension","title":"Why Do You Need a Date Dimension?","text":"<p>Consider this question: \"What were our sales on Tuesdays in January?\"</p> <p>Your raw order data looks like this:</p> order_id order_date amount ORD001 2024-01-15 1,299.99 ORD002 2024-01-16 249.99 ORD003 2024-01-23 599.99 <p>Problem: The date <code>2024-01-15</code> doesn't tell you it's a Tuesday. You'd need to calculate that in every query.</p> <p>Without a date dimension:</p> <pre><code>-- Complex, repeated logic in every query\nSELECT \n    DATENAME(weekday, order_date) AS day_of_week,\n    SUM(amount) AS total\nFROM orders\nWHERE MONTH(order_date) = 1\n  AND DATENAME(weekday, order_date) = 'Tuesday'\nGROUP BY DATENAME(weekday, order_date);\n</code></pre> <p>With a date dimension:</p> <pre><code>-- Simple join, pre-calculated attributes\nSELECT \n    d.day_of_week,\n    SUM(o.amount) AS total\nFROM fact_orders o\nJOIN dim_date d ON o.date_sk = d.date_sk\nWHERE d.month = 1\n  AND d.day_of_week = 'Tuesday'\nGROUP BY d.day_of_week;\n</code></pre> <p>A date dimension also provides: - Fiscal calendar attributes (fiscal year, fiscal quarter) - Holiday flags (is_holiday, holiday_name) - Business day calculations (is_weekend, is_month_start) - Consistent naming (\"January\" not \"1\", \"Q1\" not \"1\")</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#the-date-dimension-pattern","title":"The Date Dimension Pattern","text":"<p>Unlike other patterns that transform source data, the <code>date_dimension</code> pattern generates data. You don't need a <code>read:</code> block\u2014just configure the date range and options.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#basic-yaml-configuration","title":"Basic YAML Configuration","text":"<pre><code>project: date_dimension_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    nodes:\n      - name: dim_date\n        # No read block needed - pattern generates data\n        transformer: date_dimension\n        params:\n          start_date: \"2024-01-15\"\n          end_date: \"2024-01-28\"\n          fiscal_year_start_month: 7\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-1-generate-a-small-date-range","title":"Step 1: Generate a Small Date Range","text":"<p>Let's generate 14 days (January 15-28, 2024) to see exactly what columns are created.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#output-dim_date-15-rows-including-unknown","title":"Output: dim_date (15 rows including unknown)","text":"<p>Here are the first 10 rows showing all 19 columns:</p> date_sk full_date day_of_week day_of_week_num day_of_month day_of_year is_weekend week_of_year month month_name quarter quarter_name year fiscal_year fiscal_quarter is_month_start is_month_end is_year_start is_year_end 0 1900-01-01 Unknown 0 0 0 false 0 0 Unknown 0 Unknown 0 0 0 false false false false 20240115 2024-01-15 Monday 1 15 15 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240116 2024-01-16 Tuesday 2 16 16 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240117 2024-01-17 Wednesday 3 17 17 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240118 2024-01-18 Thursday 4 18 18 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240119 2024-01-19 Friday 5 19 19 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240120 2024-01-20 Saturday 6 20 20 true 3 1 January 1 Q1 2024 2024 3 false false false false 20240121 2024-01-21 Sunday 7 21 21 true 3 1 January 1 Q1 2024 2024 3 false false false false 20240122 2024-01-22 Monday 1 22 22 false 4 1 January 1 Q1 2024 2024 3 false false false false 20240123 2024-01-23 Tuesday 2 23 23 false 4 1 January 1 Q1 2024 2024 3 false false false false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#remaining-rows-24-28","title":"Remaining rows (24-28):","text":"date_sk full_date day_of_week day_of_week_num is_weekend week_of_year fiscal_year fiscal_quarter 20240124 2024-01-24 Wednesday 3 false 4 2024 3 20240125 2024-01-25 Thursday 4 false 4 2024 3 20240126 2024-01-26 Friday 5 false 4 2024 3 20240127 2024-01-27 Saturday 6 true 4 2024 3 20240128 2024-01-28 Sunday 7 true 4 2024 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-2-understanding-the-19-columns","title":"Step 2: Understanding the 19 Columns","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#surrogate-key","title":"Surrogate Key","text":"Column Type Description Example <code>date_sk</code> int Surrogate key in YYYYMMDD format 20240115 <p>The <code>date_sk</code> uses YYYYMMDD format, which: - Is human-readable (you can see the date in the key) - Sorts chronologically - Is efficient for range queries</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#date-columns","title":"Date Columns","text":"Column Type Description Example <code>full_date</code> date The actual date 2024-01-15"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#day-attributes","title":"Day Attributes","text":"Column Type Description Example <code>day_of_week</code> string Day name Monday <code>day_of_week_num</code> int Day number (1=Monday, 7=Sunday) 1 <code>day_of_month</code> int Day of month (1-31) 15 <code>day_of_year</code> int Day of year (1-366) 15 <code>is_weekend</code> bool Weekend flag false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#week-attributes","title":"Week Attributes","text":"Column Type Description Example <code>week_of_year</code> int ISO week number (1-53) 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#month-attributes","title":"Month Attributes","text":"Column Type Description Example <code>month</code> int Month number (1-12) 1 <code>month_name</code> string Month name January <code>is_month_start</code> bool First day of month false <code>is_month_end</code> bool Last day of month false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#quarter-attributes","title":"Quarter Attributes","text":"Column Type Description Example <code>quarter</code> int Calendar quarter (1-4) 1 <code>quarter_name</code> string Quarter name Q1"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#year-attributes","title":"Year Attributes","text":"Column Type Description Example <code>year</code> int Calendar year 2024 <code>is_year_start</code> bool First day of year false <code>is_year_end</code> bool Last day of year false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-calendar","title":"Fiscal Calendar","text":"Column Type Description Example <code>fiscal_year</code> int Fiscal year 2024 <code>fiscal_quarter</code> int Fiscal quarter (1-4) 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-3-understanding-fiscal-calendars","title":"Step 3: Understanding Fiscal Calendars","text":"<p>Many organizations don't use January-December as their fiscal year. Common alternatives:</p> Industry Fiscal Year Start Example US Government October 1 FY2024 = Oct 2023 - Sep 2024 Retail February 1 FY2024 = Feb 2024 - Jan 2025 Education July 1 FY2024 = Jul 2023 - Jun 2024"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#how-fiscal-year-calculation-works","title":"How Fiscal Year Calculation Works","text":"<p>With <code>fiscal_year_start_month: 7</code> (July):</p> Calendar Date Calendar Year Fiscal Year Why June 15, 2024 2024 2024 Before fiscal year start July 1, 2024 2024 2025 Fiscal year starts December 31, 2024 2024 2025 Still FY2025 <p>Formula: If current month &gt;= fiscal_year_start_month, add 1 to calendar year.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-quarters","title":"Fiscal Quarters","text":"<p>With <code>fiscal_year_start_month: 7</code>:</p> Fiscal Quarter Months Q1 July, August, September Q2 October, November, December Q3 January, February, March Q4 April, May, June"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#example-output-with-july-fiscal-year","title":"Example Output with July Fiscal Year","text":"<p>January 2024 dates would show:</p> full_date year fiscal_year quarter fiscal_quarter 2024-01-15 2024 2024 1 3 2024-01-16 2024 2024 1 3 <p>Note: January is calendar Q1 but fiscal Q3 (since the fiscal year started in July).</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-4-the-unknown-date-row","title":"Step 4: The Unknown Date Row","text":"<p>When <code>unknown_member: true</code>, a special row is added with <code>date_sk = 0</code>:</p> Column Value date_sk 0 full_date 1900-01-01 day_of_week Unknown day_of_week_num 0 day_of_month 0 day_of_year 0 is_weekend false week_of_year 0 month 0 month_name Unknown quarter 0 quarter_name Unknown year 0 fiscal_year 0 fiscal_quarter 0 is_month_start false is_month_end false is_year_start false is_year_end false <p>Use case: When fact records have NULL or invalid dates, they can be assigned to date_sk = 0 rather than being dropped.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<p>Here's a complete YAML file for a production-ready date dimension spanning 10 years:</p> <pre><code># File: odibi_date_dimension_tutorial.yaml\nproject: date_dimension_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    description: \"Generate date dimension for 10-year range\"\n    nodes:\n      - name: dim_date\n        description: \"Standard date dimension with fiscal calendar\"\n        transformer: date_dimension\n        params:\n          # 10-year range for typical warehouse\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n\n          # July fiscal year (education/government style)\n          fiscal_year_start_month: 7\n\n          # Add unknown member for orphan handling\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n</code></pre> <p>This generates 4,018 rows (4,017 days + 1 unknown member).</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#common-fiscal-year-configurations","title":"Common Fiscal Year Configurations","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#standard-calendar-year-default","title":"Standard Calendar Year (Default)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 1  # Default\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#us-governmentretail-october","title":"US Government/Retail (October)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 10\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#education-july","title":"Education (July)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 7\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#uk-tax-year-april","title":"UK Tax Year (April)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 4\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#python-api-alternative","title":"Python API Alternative","text":"<pre><code>from odibi.patterns.date_dimension import DateDimensionPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\n\n# Create pattern\npattern = DateDimensionPattern(params={\n    \"start_date\": \"2024-01-15\",\n    \"end_date\": \"2024-01-28\",\n    \"fiscal_year_start_month\": 7,\n    \"unknown_member\": True\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern (no input df needed - generates data)\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\nresult_df = pattern.execute(context)\n\n# View results\nprint(f\"Generated {len(result_df)} date rows\")\nprint(\"\\nColumns:\", result_df.columns.tolist())\nprint(\"\\nSample data:\")\nprint(result_df.head(10).to_string())\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#querying-the-date-dimension","title":"Querying the Date Dimension","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#what-day-of-the-week-had-the-most-sales","title":"\"What day of the week had the most sales?\"","text":"<pre><code>SELECT \n    d.day_of_week,\n    SUM(f.line_total) AS total_sales,\n    COUNT(*) AS order_count\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.day_of_week\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#show-monthly-sales-trend","title":"\"Show monthly sales trend\"","text":"<pre><code>SELECT \n    d.year,\n    d.month_name,\n    SUM(f.line_total) AS monthly_sales\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.year, d.month, d.month_name\nORDER BY d.year, d.month;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#compare-weekday-vs-weekend-sales","title":"\"Compare weekday vs weekend sales\"","text":"<pre><code>SELECT \n    CASE WHEN d.is_weekend THEN 'Weekend' ELSE 'Weekday' END AS period,\n    SUM(f.line_total) AS total_sales,\n    AVG(f.line_total) AS avg_order\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.is_weekend;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-year-performance","title":"\"Fiscal year performance\"","text":"<pre><code>SELECT \n    d.fiscal_year,\n    d.fiscal_quarter,\n    SUM(f.line_total) AS quarterly_sales\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.fiscal_year, d.fiscal_quarter\nORDER BY d.fiscal_year, d.fiscal_quarter;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Date dimensions pre-calculate date attributes for easier querying</li> <li>The pattern generates data\u2014no source file needed</li> <li>19 columns are created automatically covering day, week, month, quarter, year, and fiscal calendar</li> <li>Surrogate keys use YYYYMMDD format (e.g., 20240115)</li> <li>Fiscal calendars can start on any month\u2014the pattern calculates fiscal year and quarter automatically</li> <li>Unknown member (date_sk = 0) handles NULL or invalid dates in fact tables</li> </ul>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you have customer and date dimensions, let's build a fact table that links them together.</p> <p>Next: Fact Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#navigation","title":"Navigation","text":"Previous Up Next Dimension Pattern Tutorials Fact Pattern"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Date Dimension Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/","title":"Fact Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>fact</code> pattern to build fact tables with automatic surrogate key lookups, orphan handling, grain validation, and measure calculations.</p> <p>What You'll Learn: - How surrogate key lookups work - Orphan handling strategies - Grain validation and deduplication - Measure calculations - Joining facts with dimensions</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#source-data","title":"Source Data","text":"<p>We'll use orders data that references customers and products by their natural keys:</p> <p>Source Data (orders.csv) - 30 rows:</p> order_id customer_id product_id order_date quantity unit_price status ORD001 C001 P001 2024-01-15 1 1299.99 completed ORD002 C001 P002 2024-01-15 2 29.99 completed ORD003 C002 P003 2024-01-16 1 249.99 completed ORD004 C003 P004 2024-01-16 3 49.99 completed ORD005 C004 P005 2024-01-17 1 599.99 completed ORD006 C005 P006 2024-01-17 1 149.99 completed ORD007 C006 P007 2024-01-18 2 399.99 completed ORD008 C007 P008 2024-01-18 4 45.99 completed ORD009 C008 P009 2024-01-19 1 79.99 completed ORD010 C009 P010 2024-01-19 1 189.99 completed ORD011 C010 P001 2024-01-20 1 1299.99 completed ORD012 C011 P002 2024-01-20 5 29.99 completed ORD013 C012 P003 2024-01-21 2 249.99 completed ORD014 C001 P004 2024-01-21 1 49.99 completed ORD015 C002 P005 2024-01-22 1 599.99 pending ORD016 C003 P006 2024-01-22 2 149.99 completed ORD017 C004 P007 2024-01-23 1 399.99 completed ORD018 C005 P008 2024-01-23 3 45.99 completed ORD019 C006 P009 2024-01-24 2 79.99 completed ORD020 C007 P010 2024-01-24 1 189.99 completed ORD021 C008 P001 2024-01-25 1 1299.99 completed ORD022 C009 P002 2024-01-25 3 29.99 completed ORD023 C010 P003 2024-01-26 1 249.99 cancelled ORD024 C011 P004 2024-01-26 2 49.99 completed ORD025 C012 P005 2024-01-27 1 599.99 completed ORD026 C001 P006 2024-01-27 1 149.99 completed ORD027 C002 P007 2024-01-28 1 399.99 completed ORD028 C003 P008 2024-01-28 2 45.99 completed ORD029 C004 P009 2024-01-15 1 79.99 completed ORD030 C005 P010 2024-01-16 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dimension-tables-pre-built","title":"Dimension Tables (Pre-built)","text":"<p>Before building fact tables, we need dimension tables. Here are the dimensions from previous tutorials:</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_customer-13-rows-including-unknown","title":"dim_customer (13 rows including unknown)","text":"customer_sk customer_id name region is_current 0 -1 Unknown Unknown true 1 C001 Alice Johnson North true 2 C002 Bob Smith South true 3 C003 Carol White North true 4 C004 David Brown East true 5 C005 Emma Davis West true 6 C006 Frank Miller South true 7 C007 Grace Lee East true 8 C008 Henry Wilson West true 9 C009 Ivy Chen North true 10 C010 Jack Taylor South true 11 C011 Karen Martinez East true 12 C012 Leo Anderson West true"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_product-11-rows-including-unknown","title":"dim_product (11 rows including unknown)","text":"product_sk product_id name category 0 -1 Unknown Unknown 1 P001 Laptop Pro 15 Electronics 2 P002 Wireless Mouse Electronics 3 P003 Office Chair Furniture 4 P004 USB-C Hub Electronics 5 P005 Standing Desk Furniture 6 P006 Mechanical Keyboard Electronics 7 P007 Monitor 27\" Electronics 8 P008 Desk Lamp Furniture 9 P009 Webcam HD Electronics 10 P010 Filing Cabinet Furniture"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_date-15-rows-for-jan-15-28-unknown","title":"dim_date (15 rows for Jan 15-28 + unknown)","text":"date_sk full_date day_of_week month_name 0 1900-01-01 Unknown Unknown 20240115 2024-01-15 Monday January 20240116 2024-01-16 Tuesday January 20240117 2024-01-17 Wednesday January 20240118 2024-01-18 Thursday January 20240119 2024-01-19 Friday January 20240120 2024-01-20 Saturday January 20240121 2024-01-21 Sunday January 20240122 2024-01-22 Monday January 20240123 2024-01-23 Tuesday January 20240124 2024-01-24 Wednesday January 20240125 2024-01-25 Thursday January 20240126 2024-01-26 Friday January 20240127 2024-01-27 Saturday January 20240128 2024-01-28 Sunday January"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-1-understanding-sk-lookups","title":"Step 1: Understanding SK Lookups","text":"<p>The fact pattern's main job is to replace natural keys with surrogate keys:</p> <pre><code>Source:     customer_id = \"C001\"\n                    \u2193\n            Look up in dim_customer where customer_id = \"C001\"\n                    \u2193\nFact:       customer_sk = 1\n</code></pre> <p>This transformation happens for every dimension referenced.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: fact_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_fact_orders\n    nodes:\n      # First, load the dimension tables into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # Then build the fact table\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#the-transformation","title":"The Transformation","text":"<p>Here's what happens to the first 10 rows:</p> <p>Before (Source Data):</p> order_id customer_id product_id order_date quantity unit_price ORD001 C001 P001 2024-01-15 1 1299.99 ORD002 C001 P002 2024-01-15 2 29.99 ORD003 C002 P003 2024-01-16 1 249.99 ORD004 C003 P004 2024-01-16 3 49.99 ORD005 C004 P005 2024-01-17 1 599.99 ORD006 C005 P006 2024-01-17 1 149.99 ORD007 C006 P007 2024-01-18 2 399.99 ORD008 C007 P008 2024-01-18 4 45.99 ORD009 C008 P009 2024-01-19 1 79.99 ORD010 C009 P010 2024-01-19 1 189.99 <p>After (Fact Table):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status load_timestamp ORD001 1 1 20240115 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD002 1 2 20240115 2 29.99 59.98 completed 2024-01-30 10:00:00 ORD003 2 3 20240116 1 249.99 249.99 completed 2024-01-30 10:00:00 ORD004 3 4 20240116 3 49.99 149.97 completed 2024-01-30 10:00:00 ORD005 4 5 20240117 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD006 5 6 20240117 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD007 6 7 20240118 2 399.99 799.98 completed 2024-01-30 10:00:00 ORD008 7 8 20240118 4 45.99 183.96 completed 2024-01-30 10:00:00 ORD009 8 9 20240119 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD010 9 10 20240119 1 189.99 189.99 completed 2024-01-30 10:00:00 <p>Key observations: - <code>customer_id = \"C001\"</code> \u2192 <code>customer_sk = 1</code> - <code>product_id = \"P001\"</code> \u2192 <code>product_sk = 1</code> - <code>order_date = \"2024-01-15\"</code> \u2192 <code>date_sk = 20240115</code> - New column <code>line_total</code> calculated as <code>quantity * unit_price</code> - Original natural keys can be dropped or kept (configurable)</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-2-orphan-handling","title":"Step 2: Orphan Handling","text":"<p>What happens when a source record references a dimension value that doesn't exist?</p> <p>Source with Orphans (orders_with_orphans.csv) - 5 orphan rows:</p> order_id customer_id product_id order_date quantity unit_price ... ... ... ... ... ... ORD031 C999 P001 2024-01-17 1 1299.99 ORD032 C888 P002 2024-01-18 2 29.99 ORD033 C777 P003 2024-01-19 1 249.99 ORD034 C666 P004 2024-01-20 1 49.99 ORD035 C555 P005 2024-01-21 1 599.99 <p>Customer IDs C999, C888, C777, C666, C555 don't exist in dim_customer.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-1-unknown-default","title":"Strategy 1: Unknown (Default)","text":"<pre><code>params:\n  orphan_handling: unknown\n</code></pre> <p>Orphans are assigned to the unknown member (SK = 0):</p> order_id customer_sk product_sk note ORD031 0 1 C999 not found \u2192 SK = 0 ORD032 0 2 C888 not found \u2192 SK = 0 ORD033 0 3 C777 not found \u2192 SK = 0 ORD034 0 4 C666 not found \u2192 SK = 0 ORD035 0 5 C555 not found \u2192 SK = 0 <p>Pros: Data isn't lost, can investigate later Cons: Need to ensure unknown member exists in dimension</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-2-reject","title":"Strategy 2: Reject","text":"<pre><code>params:\n  orphan_handling: reject\n</code></pre> <p>Pipeline fails with an error listing orphan values:</p> <pre><code>OrphanRecordError: Found 5 orphan records for dimension 'dim_customer':\n  - customer_id='C999' (1 record)\n  - customer_id='C888' (1 record)\n  - customer_id='C777' (1 record)\n  - customer_id='C666' (1 record)\n  - customer_id='C555' (1 record)\n</code></pre> <p>Pros: Strict data quality enforcement Cons: Entire load fails, need to fix source data</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-3-quarantine","title":"Strategy 3: Quarantine","text":"<pre><code>params:\n  orphan_handling: quarantine\n</code></pre> <p>Orphan records are routed to a separate quarantine table:</p> <p>fact_orders (valid records): 30 rows</p> <p>fact_orders_quarantine (orphans): 5 rows</p> order_id customer_id orphan_reason ORD031 C999 customer_id not found in dim_customer ORD032 C888 customer_id not found in dim_customer ORD033 C777 customer_id not found in dim_customer ORD034 C666 customer_id not found in dim_customer ORD035 C555 customer_id not found in dim_customer <p>Pros: Valid data loads, orphans are preserved for review Cons: Need to manage quarantine table</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-3-grain-validation","title":"Step 3: Grain Validation","text":"<p>The grain defines what makes a fact row unique. For orders, it's typically <code>order_id</code>.</p> <pre><code>params:\n  grain: [order_id]\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#what-happens-with-duplicate-grain","title":"What Happens with Duplicate Grain?","text":"<p>If your source has duplicate order IDs:</p> order_id customer_id quantity ORD001 C001 1 ORD001 C001 2 <p>The pattern detects this and raises an error:</p> <pre><code>GrainValidationError: Duplicate grain detected in fact_orders\n  Grain columns: ['order_id']\n  Duplicate count: 1\n  Sample duplicates:\n    - order_id='ORD001' (2 occurrences)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#enabling-deduplication","title":"Enabling Deduplication","text":"<p>If duplicates are expected and you want to keep the latest:</p> <pre><code>params:\n  grain: [order_id]\n  deduplicate: true\n  keys: [order_id]\n</code></pre> <p>This keeps only the last occurrence of each order_id.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-4-measure-calculations","title":"Step 4: Measure Calculations","text":"<p>Measures are the numeric values in your fact table. You can: - Pass through existing columns - Rename columns - Calculate derived values</p> <pre><code>params:\n  measures:\n    # Pass through\n    - quantity\n    - unit_price\n\n    # Rename (maps status to order_status)\n    - order_status: status\n\n    # Calculate\n    - line_total: \"quantity * unit_price\"\n    - discount_amount: \"unit_price * 0.1\"\n    - net_total: \"quantity * unit_price * 0.9\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#output-with-calculated-measures","title":"Output with Calculated Measures","text":"order_id quantity unit_price line_total discount_amount net_total ORD001 1 1299.99 1299.99 130.00 1169.99 ORD002 2 29.99 59.98 3.00 53.98 ORD003 1 249.99 249.99 25.00 224.99"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#complete-fact-table-output","title":"Complete Fact Table Output","text":"<p>Here's the complete fact_orders table (30 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status load_timestamp ORD001 1 1 20240115 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD002 1 2 20240115 2 29.99 59.98 completed 2024-01-30 10:00:00 ORD003 2 3 20240116 1 249.99 249.99 completed 2024-01-30 10:00:00 ORD004 3 4 20240116 3 49.99 149.97 completed 2024-01-30 10:00:00 ORD005 4 5 20240117 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD006 5 6 20240117 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD007 6 7 20240118 2 399.99 799.98 completed 2024-01-30 10:00:00 ORD008 7 8 20240118 4 45.99 183.96 completed 2024-01-30 10:00:00 ORD009 8 9 20240119 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD010 9 10 20240119 1 189.99 189.99 completed 2024-01-30 10:00:00 ORD011 10 1 20240120 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD012 11 2 20240120 5 29.99 149.95 completed 2024-01-30 10:00:00 ORD013 12 3 20240121 2 249.99 499.98 completed 2024-01-30 10:00:00 ORD014 1 4 20240121 1 49.99 49.99 completed 2024-01-30 10:00:00 ORD015 2 5 20240122 1 599.99 599.99 pending 2024-01-30 10:00:00 ORD016 3 6 20240122 2 149.99 299.98 completed 2024-01-30 10:00:00 ORD017 4 7 20240123 1 399.99 399.99 completed 2024-01-30 10:00:00 ORD018 5 8 20240123 3 45.99 137.97 completed 2024-01-30 10:00:00 ORD019 6 9 20240124 2 79.99 159.98 completed 2024-01-30 10:00:00 ORD020 7 10 20240124 1 189.99 189.99 completed 2024-01-30 10:00:00 ORD021 8 1 20240125 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD022 9 2 20240125 3 29.99 89.97 completed 2024-01-30 10:00:00 ORD023 10 3 20240126 1 249.99 249.99 cancelled 2024-01-30 10:00:00 ORD024 11 4 20240126 2 49.99 99.98 completed 2024-01-30 10:00:00 ORD025 12 5 20240127 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD026 1 6 20240127 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD027 2 7 20240128 1 399.99 399.99 completed 2024-01-30 10:00:00 ORD028 3 8 20240128 2 45.99 91.98 completed 2024-01-30 10:00:00 ORD029 4 9 20240115 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD030 5 10 20240116 1 189.99 189.99 completed 2024-01-30 10:00:00"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<pre><code># File: odibi_fact_tutorial.yaml\nproject: fact_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_fact_orders\n    description: \"Build fact table with SK lookups\"\n    nodes:\n      # Load dimension tables\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # Build fact table\n      - name: fact_orders\n        description: \"Orders fact table with surrogate key lookups\"\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        transformer: fact\n        params:\n          # Define the grain (uniqueness)\n          grain: [order_id]\n\n          # Define dimension lookups\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true  # Filter to is_current = true\n\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n\n          # Handle missing dimension values\n          orphan_handling: unknown\n\n          # Define measures\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n\n          # Add audit columns\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#python-api-alternative","title":"Python API Alternative","text":"<pre><code>from odibi.patterns.fact import FactPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load source data\norders_df = pd.read_csv(\"examples/tutorials/dimensional_modeling/data/orders.csv\")\n\n# Load dimensions\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\n# Create context and register dimensions\ncontext = EngineContext(df=orders_df, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# Create pattern\npattern = FactPattern(params={\n    \"grain\": [\"order_id\"],\n    \"dimensions\": [\n        {\n            \"source_column\": \"customer_id\",\n            \"dimension_table\": \"dim_customer\",\n            \"dimension_key\": \"customer_id\",\n            \"surrogate_key\": \"customer_sk\",\n            \"scd2\": True\n        },\n        {\n            \"source_column\": \"product_id\",\n            \"dimension_table\": \"dim_product\",\n            \"dimension_key\": \"product_id\",\n            \"surrogate_key\": \"product_sk\"\n        },\n        {\n            \"source_column\": \"order_date\",\n            \"dimension_table\": \"dim_date\",\n            \"dimension_key\": \"full_date\",\n            \"surrogate_key\": \"date_sk\"\n        }\n    ],\n    \"orphan_handling\": \"unknown\",\n    \"measures\": [\n        \"quantity\",\n        \"unit_price\",\n        {\"line_total\": \"quantity * unit_price\"}\n    ],\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"pos\"\n    }\n})\n\n# Execute pattern\nresult_df = pattern.execute(context)\n\nprint(f\"Generated {len(result_df)} fact rows\")\nprint(result_df.head(10))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#querying-the-star-schema","title":"Querying the Star Schema","text":"<p>Now you can run powerful analytical queries:</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#sales-by-region","title":"\"Sales by region\"","text":"<pre><code>SELECT \n    c.region,\n    COUNT(*) AS order_count,\n    SUM(f.line_total) AS total_revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.region\nORDER BY total_revenue DESC;\n</code></pre> region order_count total_revenue North 8 2,599.87 East 8 2,023.87 South 7 2,449.92 West 7 2,629.88"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#daily-sales-trend","title":"\"Daily sales trend\"","text":"<pre><code>SELECT \n    d.full_date,\n    d.day_of_week,\n    SUM(f.line_total) AS daily_revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.full_date, d.day_of_week\nORDER BY d.full_date;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#top-products-by-category","title":"\"Top products by category\"","text":"<pre><code>SELECT \n    p.category,\n    p.name,\n    SUM(f.quantity) AS units_sold,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.category, p.name\nORDER BY revenue DESC;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Surrogate key lookups automatically replace natural keys with dimension SKs</li> <li>scd2: true filters to current dimension rows when looking up SCD2 dimensions</li> <li>Orphan handling strategies: unknown (default), reject, or quarantine</li> <li>Grain validation detects duplicate records at the grain level</li> <li>Measures can be passed through, renamed, or calculated</li> <li>depends_on ensures dimension tables are loaded before the fact pattern runs</li> <li>Audit columns track when and where data was loaded</li> </ul>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you have a complete star schema, let's build aggregated tables for faster reporting.</p> <p>Next: Aggregation Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#navigation","title":"Navigation","text":"Previous Up Next Date Dimension Tutorials Aggregation Pattern"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Fact Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/","title":"Aggregation Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>aggregation</code> pattern to build pre-aggregated tables for faster reporting and dashboards.</p> <p>What You'll Learn: - Why pre-aggregate fact tables - Defining grain and measures - Using aggregate functions (SUM, COUNT, AVG) - Incremental merge strategies (replace vs sum)</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#why-pre-aggregate","title":"Why Pre-Aggregate?","text":"<p>Consider a dashboard showing \"Daily Revenue by Product Category\":</p> <p>Without pre-aggregation:</p> <pre><code>-- Runs against 10 million fact rows every time\nSELECT \n    d.full_date,\n    p.category,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY d.full_date, p.category;\n-- Takes 30 seconds\n</code></pre> <p>With pre-aggregation:</p> <pre><code>-- Runs against 5,000 aggregated rows\nSELECT full_date, category, total_revenue\nFROM agg_daily_category_sales;\n-- Takes 0.1 seconds\n</code></pre> <p>Pre-aggregated tables trade storage for speed. For frequently-queried metrics, this is almost always worth it.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#source-data-fact_orders","title":"Source Data: fact_orders","text":"<p>We'll aggregate the fact table from the previous tutorial (30 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD011 10 1 20240120 1 1299.99 1299.99 completed ORD012 11 2 20240120 5 29.99 149.95 completed ORD013 12 3 20240121 2 249.99 499.98 completed ORD014 1 4 20240121 1 49.99 49.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending ORD016 3 6 20240122 2 149.99 299.98 completed ORD017 4 7 20240123 1 399.99 399.99 completed ORD018 5 8 20240123 3 45.99 137.97 completed ORD019 6 9 20240124 2 79.99 159.98 completed ORD020 7 10 20240124 1 189.99 189.99 completed ORD021 8 1 20240125 1 1299.99 1299.99 completed ORD022 9 2 20240125 3 29.99 89.97 completed ORD023 10 3 20240126 1 249.99 249.99 cancelled ORD024 11 4 20240126 2 49.99 99.98 completed ORD025 12 5 20240127 1 599.99 599.99 completed ORD026 1 6 20240127 1 149.99 149.99 completed ORD027 2 7 20240128 1 399.99 399.99 completed ORD028 3 8 20240128 2 45.99 91.98 completed ORD029 4 9 20240115 1 79.99 79.99 completed ORD030 5 10 20240116 1 189.99 189.99 completed"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-1-basic-aggregation-by-date-and-product","title":"Step 1: Basic Aggregation by Date and Product","text":"<p>Let's aggregate orders by date and product to see daily product sales.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: aggregation_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    nodes:\n      - name: agg_daily_product_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [date_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: total_quantity\n              expr: \"SUM(quantity)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#understanding-grain-and-measures","title":"Understanding Grain and Measures","text":"<p>Grain: The columns that define uniqueness in the output. Here, each combination of <code>date_sk</code> + <code>product_sk</code> gets one row.</p> <p>Measures: The aggregations to compute. Each measure needs: - <code>name</code>: Output column name - <code>expr</code>: SQL aggregation expression</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#output-agg_daily_product_sales-26-rows","title":"Output: agg_daily_product_sales (26 rows)","text":"<p>Here are all 26 aggregated rows:</p> date_sk product_sk total_revenue order_count total_quantity load_timestamp 20240115 1 1299.99 1 1 2024-01-30 10:00:00 20240115 2 59.98 1 2 2024-01-30 10:00:00 20240115 9 79.99 1 1 2024-01-30 10:00:00 20240116 3 249.99 1 1 2024-01-30 10:00:00 20240116 4 149.97 1 3 2024-01-30 10:00:00 20240116 10 189.99 1 1 2024-01-30 10:00:00 20240117 5 599.99 1 1 2024-01-30 10:00:00 20240117 6 149.99 1 1 2024-01-30 10:00:00 20240118 7 799.98 1 2 2024-01-30 10:00:00 20240118 8 183.96 1 4 2024-01-30 10:00:00 20240119 9 79.99 1 1 2024-01-30 10:00:00 20240119 10 189.99 1 1 2024-01-30 10:00:00 20240120 1 1299.99 1 1 2024-01-30 10:00:00 20240120 2 149.95 1 5 2024-01-30 10:00:00 20240121 3 499.98 1 2 2024-01-30 10:00:00 20240121 4 49.99 1 1 2024-01-30 10:00:00 20240122 5 599.99 1 1 2024-01-30 10:00:00 20240122 6 299.98 1 2 2024-01-30 10:00:00 20240123 7 399.99 1 1 2024-01-30 10:00:00 20240123 8 137.97 1 3 2024-01-30 10:00:00 20240124 9 159.98 1 2 2024-01-30 10:00:00 20240124 10 189.99 1 1 2024-01-30 10:00:00 20240125 1 1299.99 1 1 2024-01-30 10:00:00 20240125 2 89.97 1 3 2024-01-30 10:00:00 20240126 3 249.99 1 1 2024-01-30 10:00:00 20240126 4 99.98 1 2 2024-01-30 10:00:00 20240127 5 599.99 1 1 2024-01-30 10:00:00 20240127 6 149.99 1 1 2024-01-30 10:00:00 20240128 7 399.99 1 1 2024-01-30 10:00:00 20240128 8 91.98 1 2 2024-01-30 10:00:00 <p>Result: 30 fact rows became 26 aggregate rows (some date+product combinations had multiple orders).</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-2-adding-more-measures","title":"Step 2: Adding More Measures","text":"<p>Let's add average and distinct count measures:</p> <pre><code>params:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: total_quantity\n      expr: \"SUM(quantity)\"\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n    - name: max_order_value\n      expr: \"MAX(line_total)\"\n    - name: min_order_value\n      expr: \"MIN(line_total)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>SUM(column)</code> Total of values <code>SUM(line_total)</code> <code>COUNT(*)</code> Number of rows <code>COUNT(*)</code> <code>COUNT(column)</code> Non-null count <code>COUNT(customer_sk)</code> <code>COUNT(DISTINCT column)</code> Unique values <code>COUNT(DISTINCT customer_sk)</code> <code>AVG(column)</code> Average value <code>AVG(line_total)</code> <code>MAX(column)</code> Maximum value <code>MAX(line_total)</code> <code>MIN(column)</code> Minimum value <code>MIN(line_total)</code>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#complex-expressions","title":"Complex Expressions","text":"<p>You can use expressions within aggregations:</p> <pre><code>measures:\n  # Total after 10% discount\n  - name: discounted_revenue\n    expr: \"SUM(line_total * 0.9)\"\n\n  # Margin (if cost column existed)\n  - name: total_margin\n    expr: \"SUM(line_total - cost)\"\n\n  # Discount rate\n  - name: avg_discount_rate\n    expr: \"AVG(discount_amount / line_total)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-3-aggregating-by-different-grains","title":"Step 3: Aggregating by Different Grains","text":""},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#daily-sales-grain-date-only","title":"Daily Sales (Grain: date only)","text":"<pre><code>params:\n  grain: [date_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: unique_products\n      expr: \"COUNT(DISTINCT product_sk)\"\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n</code></pre> <p>Output: agg_daily_sales (14 rows)</p> date_sk total_revenue order_count unique_products unique_customers 20240115 1439.96 3 3 2 20240116 589.95 3 3 3 20240117 749.98 2 2 2 20240118 983.94 2 2 2 20240119 269.98 2 2 2 20240120 1449.94 2 2 2 20240121 549.97 2 2 2 20240122 899.97 2 2 2 20240123 537.96 2 2 2 20240124 349.97 2 2 2 20240125 1389.96 2 2 2 20240126 349.97 2 2 2 20240127 749.98 2 2 2 20240128 491.97 2 2 2"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#product-sales-grain-product-only","title":"Product Sales (Grain: product only)","text":"<pre><code>params:\n  grain: [product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: total_quantity\n      expr: \"SUM(quantity)\"\n</code></pre> <p>Output: agg_product_sales (10 rows)</p> product_sk total_revenue order_count total_quantity 1 3899.97 3 3 2 299.90 3 10 3 999.96 3 4 4 299.94 3 6 5 1799.97 3 3 6 599.96 3 4 7 1599.96 3 4 8 413.91 3 9 9 319.96 3 4 10 569.97 3 3"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-4-incremental-merge-strategies","title":"Step 4: Incremental Merge Strategies","text":"<p>For ongoing pipelines, you don't want to rebuild the entire aggregate table. Incremental strategies allow you to update only affected rows.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#replace-strategy","title":"Replace Strategy","text":"<p>How it works: New aggregates completely replace existing rows for matching grain keys.</p> <pre><code>params:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n  incremental:\n    timestamp_column: load_timestamp\n    merge_strategy: replace\n  target: warehouse.agg_daily_product_sales\n</code></pre> <p>Example Scenario:</p> <p>Existing agg_daily_product_sales:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 20240115 2 59.98 1 20240116 3 249.99 1 <p>New orders arrive for 2024-01-15:</p> order_id product_sk date_sk line_total ORD100 1 20240115 1299.99 <p>New aggregate computed:</p> date_sk product_sk total_revenue order_count 20240115 1 2599.98 2 <p>After Replace Merge:</p> date_sk product_sk total_revenue order_count Note 20240115 1 2599.98 2 Replaced 20240115 2 59.98 1 Unchanged 20240116 3 249.99 1 Unchanged <p>Use case: Best for most scenarios. Handles late-arriving data, corrections, and restatements correctly.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#sum-strategy","title":"Sum Strategy","text":"<p>How it works: New measure values are added to existing values.</p> <pre><code>params:\n  incremental:\n    timestamp_column: load_timestamp\n    merge_strategy: sum\n</code></pre> <p>Example Scenario:</p> <p>Existing:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 <p>New aggregate:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 <p>After Sum Merge:</p> date_sk product_sk total_revenue order_count 20240115 1 2599.98 2 <p>Use case: Only for purely additive metrics (counts, sums) on append-only data.</p> <p>Warning: Do NOT use sum strategy for: - AVG (would become average of averages, incorrect) - COUNT DISTINCT (would overcount) - MIN/MAX (would be wrong) - Data with updates or late-arriving records</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#complete-yaml-example","title":"Complete YAML Example","text":"<p>Here's a complete example with multiple aggregate tables:</p> <pre><code># File: odibi_aggregation_tutorial.yaml\nproject: aggregation_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    description: \"Build all aggregate tables from fact_orders\"\n    nodes:\n      # Daily aggregate at product level\n      - name: agg_daily_product_sales\n        description: \"Daily sales by product\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain:\n            - date_sk\n            - product_sk\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: total_quantity\n              expr: \"SUM(quantity)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          having: \"SUM(line_total) &gt; 0\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: parquet\n          mode: overwrite\n\n      # Daily summary (no product breakdown)\n      - name: agg_daily_sales\n        description: \"Daily sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [date_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: unique_products\n              expr: \"COUNT(DISTINCT product_sk)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          format: parquet\n          mode: overwrite\n\n      # Product summary (no date breakdown)\n      - name: agg_product_sales\n        description: \"Product sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: total_quantity\n              expr: \"SUM(quantity)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_product_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#using-aggregates-in-queries","title":"Using Aggregates in Queries","text":""},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#dashboard-query-daily-revenue-trend","title":"Dashboard Query: Daily Revenue Trend","text":"<pre><code>SELECT \n    a.date_sk,\n    d.full_date,\n    d.day_of_week,\n    a.total_revenue,\n    a.order_count\nFROM agg_daily_sales a\nJOIN dim_date d ON a.date_sk = d.date_sk\nORDER BY a.date_sk;\n</code></pre> full_date day_of_week total_revenue order_count 2024-01-15 Monday 1439.96 3 2024-01-16 Tuesday 589.95 3 2024-01-17 Wednesday 749.98 2 2024-01-18 Thursday 983.94 2 2024-01-19 Friday 269.98 2 2024-01-20 Saturday 1449.94 2 2024-01-21 Sunday 549.97 2 2024-01-22 Monday 899.97 2 2024-01-23 Tuesday 537.96 2 2024-01-24 Wednesday 349.97 2 2024-01-25 Thursday 1389.96 2 2024-01-26 Friday 349.97 2 2024-01-27 Saturday 749.98 2 2024-01-28 Sunday 491.97 2"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#dashboard-query-top-products","title":"Dashboard Query: Top Products","text":"<pre><code>SELECT \n    p.name AS product_name,\n    p.category,\n    a.total_revenue,\n    a.order_count,\n    a.total_quantity\nFROM agg_product_sales a\nJOIN dim_product p ON a.product_sk = p.product_sk\nORDER BY a.total_revenue DESC\nLIMIT 5;\n</code></pre> product_name category total_revenue order_count total_quantity Laptop Pro 15 Electronics 3899.97 3 3 Standing Desk Furniture 1799.97 3 3 Monitor 27\" Electronics 1599.96 3 4 Office Chair Furniture 999.96 3 4 Mechanical Keyboard Electronics 599.96 3 4"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Pre-aggregation speeds up reporting by reducing data volume</li> <li>Grain defines the uniqueness (GROUP BY columns) of aggregate rows</li> <li>Measures define what to compute (SUM, COUNT, AVG, etc.)</li> <li>Complex expressions can be used within aggregations</li> <li>Replace strategy is best for most incremental scenarios</li> <li>Sum strategy works only for purely additive metrics on append-only data</li> <li>Aggregate tables are joined back to dimensions for rich reporting</li> </ul>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#next-steps","title":"Next Steps","text":"<p>Now let's put it all together with a complete star schema example.</p> <p>Next: Full Star Schema Tutorial</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#navigation","title":"Navigation","text":"Previous Up Next Fact Pattern Tutorials Full Star Schema"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Aggregation Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/","title":"Full Star Schema Tutorial","text":"<p>In this tutorial, you'll build a complete star schema from start to finish, combining all the patterns from previous tutorials into a single, cohesive data warehouse.</p> <p>What You'll Build: - <code>dim_customer</code> - Customer dimension (SCD2) - <code>dim_product</code> - Product dimension (SCD1) - <code>dim_date</code> - Date dimension (generated) - <code>fact_orders</code> - Orders fact table - <code>agg_daily_sales</code> - Daily sales aggregate</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#the-complete-star-schema","title":"The Complete Star Schema","text":"<pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n    AGG_DAILY_SALES ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        string order_id PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        int quantity\n        decimal unit_price\n        decimal line_total\n        string status\n        timestamp load_timestamp\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string email\n        string region\n        string city\n        string state\n        date valid_from\n        date valid_to\n        bool is_current\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n        string subcategory\n        decimal price\n        decimal cost\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        int month\n        string month_name\n        int quarter\n        int year\n        int fiscal_year\n        int fiscal_quarter\n    }\n\n    AGG_DAILY_SALES {\n        int date_sk PK\n        decimal total_revenue\n        int order_count\n        int unique_customers\n        int unique_products\n    }\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#source-data-files","title":"Source Data Files","text":"<p>All source data is in <code>examples/tutorials/dimensional_modeling/data/</code>:</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#customerscsv-12-rows","title":"customers.csv (12 rows)","text":"customer_id name email region city state C001 Alice Johnson alice@example.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david@example.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace@example.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#productscsv-10-rows","title":"products.csv (10 rows)","text":"product_id name category subcategory price cost P001 Laptop Pro 15 Electronics Computers 1299.99 850.00 P002 Wireless Mouse Electronics Accessories 29.99 12.00 P003 Office Chair Furniture Seating 249.99 120.00 P004 USB-C Hub Electronics Accessories 49.99 22.00 P005 Standing Desk Furniture Desks 599.99 320.00 P006 Mechanical Keyboard Electronics Accessories 149.99 65.00 P007 Monitor 27\" Electronics Displays 399.99 210.00 P008 Desk Lamp Furniture Lighting 45.99 18.00 P009 Webcam HD Electronics Accessories 79.99 35.00 P010 Filing Cabinet Furniture Storage 189.99 95.00"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#orderscsv-30-rows","title":"orders.csv (30 rows)","text":"order_id customer_id product_id order_date quantity unit_price status ORD001 C001 P001 2024-01-15 1 1299.99 completed ORD002 C001 P002 2024-01-15 2 29.99 completed ORD003 C002 P003 2024-01-16 1 249.99 completed ORD004 C003 P004 2024-01-16 3 49.99 completed ORD005 C004 P005 2024-01-17 1 599.99 completed ORD006 C005 P006 2024-01-17 1 149.99 completed ORD007 C006 P007 2024-01-18 2 399.99 completed ORD008 C007 P008 2024-01-18 4 45.99 completed ORD009 C008 P009 2024-01-19 1 79.99 completed ORD010 C009 P010 2024-01-19 1 189.99 completed ORD011 C010 P001 2024-01-20 1 1299.99 completed ORD012 C011 P002 2024-01-20 5 29.99 completed ORD013 C012 P003 2024-01-21 2 249.99 completed ORD014 C001 P004 2024-01-21 1 49.99 completed ORD015 C002 P005 2024-01-22 1 599.99 pending ORD016 C003 P006 2024-01-22 2 149.99 completed ORD017 C004 P007 2024-01-23 1 399.99 completed ORD018 C005 P008 2024-01-23 3 45.99 completed ORD019 C006 P009 2024-01-24 2 79.99 completed ORD020 C007 P010 2024-01-24 1 189.99 completed ORD021 C008 P001 2024-01-25 1 1299.99 completed ORD022 C009 P002 2024-01-25 3 29.99 completed ORD023 C010 P003 2024-01-26 1 249.99 cancelled ORD024 C011 P004 2024-01-26 2 49.99 completed ORD025 C012 P005 2024-01-27 1 599.99 completed ORD026 C001 P006 2024-01-27 1 149.99 completed ORD027 C002 P007 2024-01-28 1 399.99 completed ORD028 C003 P008 2024-01-28 2 45.99 completed ORD029 C004 P009 2024-01-15 1 79.99 completed ORD030 C005 P010 2024-01-16 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#complete-yaml-configuration","title":"Complete YAML Configuration","text":"<p>Here's the full pipeline that builds everything:</p> <pre><code># File: examples/tutorials/dimensional_modeling/star_schema.yaml\nproject: retail_star_schema\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  # ==========================================\n  # PIPELINE 1: Build all dimensions\n  # ==========================================\n  - pipeline: build_dimensions\n    description: \"Build customer, product, and date dimensions\"\n    nodes:\n      # ------------------------------------------\n      # Customer Dimension (SCD Type 2)\n      # ------------------------------------------\n      - name: dim_customer\n        description: \"Customer dimension with full history tracking\"\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_cols:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n\n      # ------------------------------------------\n      # Product Dimension (SCD Type 1)\n      # ------------------------------------------\n      - name: dim_product\n        description: \"Product dimension with overwrite updates\"\n        read:\n          connection: source\n          path: products.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: product_id\n          surrogate_key: product_sk\n          scd_type: 1\n          track_cols:\n            - name\n            - category\n            - subcategory\n            - price\n            - cost\n          target: warehouse.dim_product\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"inventory\"\n\n        write:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n          mode: overwrite\n\n      # ------------------------------------------\n      # Date Dimension (Generated)\n      # ------------------------------------------\n      - name: dim_date\n        description: \"Date dimension covering Jan 2024\"\n        transformer: date_dimension\n        params:\n          start_date: \"2024-01-01\"\n          end_date: \"2024-01-31\"\n          fiscal_year_start_month: 7\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n\n  # ==========================================\n  # PIPELINE 2: Build fact tables\n  # ==========================================\n  - pipeline: build_facts\n    description: \"Build fact_orders with SK lookups\"\n    nodes:\n      # Load dimensions into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # ------------------------------------------\n      # Orders Fact Table\n      # ------------------------------------------\n      - name: fact_orders\n        description: \"Orders fact with all dimension lookups\"\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n\n  # ==========================================\n  # PIPELINE 3: Build aggregates\n  # ==========================================\n  - pipeline: build_aggregates\n    description: \"Build daily sales aggregate\"\n    nodes:\n      # ------------------------------------------\n      # Daily Sales Aggregate\n      # ------------------------------------------\n      - name: agg_daily_sales\n        description: \"Daily sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [date_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n            - name: unique_products\n              expr: \"COUNT(DISTINCT product_sk)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Run the pipelines in order:</p> <pre><code># Build dimensions first\nodibi run --config star_schema.yaml --pipeline build_dimensions\n\n# Then build facts (requires dimensions)\nodibi run --config star_schema.yaml --pipeline build_facts\n\n# Finally build aggregates (requires facts)\nodibi run --config star_schema.yaml --pipeline build_aggregates\n</code></pre> <p>Or run everything:</p> <pre><code>odibi run --config star_schema.yaml\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#final-table-schemas-and-sample-data","title":"Final Table Schemas and Sample Data","text":""},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_customer-13-rows","title":"dim_customer (13 rows)","text":"<p>Schema:</p> Column Type Description customer_sk int Surrogate key customer_id string Natural key name string Customer name email string Email address region string Geographic region city string City state string State code valid_from timestamp Version start date valid_to timestamp Version end date (NULL if current) is_current boolean Current version flag load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (13 rows):</p> customer_sk customer_id name email region city is_current 0 -1 Unknown Unknown Unknown Unknown true 1 C001 Alice Johnson alice@example.com North Chicago true 2 C002 Bob Smith bob@example.com South Houston true 3 C003 Carol White carol@example.com North Detroit true 4 C004 David Brown david@example.com East New York true 5 C005 Emma Davis emma@example.com West Seattle true 6 C006 Frank Miller frank@example.com South Miami true 7 C007 Grace Lee grace@example.com East Boston true 8 C008 Henry Wilson henry@example.com West Portland true 9 C009 Ivy Chen ivy@example.com North Minneapolis true 10 C010 Jack Taylor jack@example.com South Dallas true 11 C011 Karen Martinez karen@example.com East Philadelphia true 12 C012 Leo Anderson leo@example.com West Denver true"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_product-11-rows","title":"dim_product (11 rows)","text":"<p>Schema:</p> Column Type Description product_sk int Surrogate key product_id string Natural key name string Product name category string Product category subcategory string Product subcategory price decimal List price cost decimal Unit cost load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (11 rows):</p> product_sk product_id name category subcategory price cost 0 -1 Unknown Unknown Unknown 0.00 0.00 1 P001 Laptop Pro 15 Electronics Computers 1299.99 850.00 2 P002 Wireless Mouse Electronics Accessories 29.99 12.00 3 P003 Office Chair Furniture Seating 249.99 120.00 4 P004 USB-C Hub Electronics Accessories 49.99 22.00 5 P005 Standing Desk Furniture Desks 599.99 320.00 6 P006 Mechanical Keyboard Electronics Accessories 149.99 65.00 7 P007 Monitor 27\" Electronics Displays 399.99 210.00 8 P008 Desk Lamp Furniture Lighting 45.99 18.00 9 P009 Webcam HD Electronics Accessories 79.99 35.00 10 P010 Filing Cabinet Furniture Storage 189.99 95.00"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_date-32-rows-for-january-unknown","title":"dim_date (32 rows for January + unknown)","text":"<p>Sample Data (first 15 rows):</p> date_sk full_date day_of_week month_name quarter_name year fiscal_year fiscal_quarter 0 1900-01-01 Unknown Unknown Unknown 0 0 0 20240101 2024-01-01 Monday January Q1 2024 2024 3 20240102 2024-01-02 Tuesday January Q1 2024 2024 3 20240103 2024-01-03 Wednesday January Q1 2024 2024 3 20240104 2024-01-04 Thursday January Q1 2024 2024 3 20240105 2024-01-05 Friday January Q1 2024 2024 3 20240106 2024-01-06 Saturday January Q1 2024 2024 3 20240107 2024-01-07 Sunday January Q1 2024 2024 3 20240108 2024-01-08 Monday January Q1 2024 2024 3 20240109 2024-01-09 Tuesday January Q1 2024 2024 3 20240110 2024-01-10 Wednesday January Q1 2024 2024 3 20240111 2024-01-11 Thursday January Q1 2024 2024 3 20240112 2024-01-12 Friday January Q1 2024 2024 3 20240113 2024-01-13 Saturday January Q1 2024 2024 3 20240114 2024-01-14 Sunday January Q1 2024 2024 3"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#fact_orders-30-rows","title":"fact_orders (30 rows)","text":"<p>Schema:</p> Column Type Description order_id string Order identifier (grain) customer_sk int Customer surrogate key product_sk int Product surrogate key date_sk int Date surrogate key quantity int Quantity ordered unit_price decimal Price per unit line_total decimal Calculated: quantity \u00d7 unit_price status string Order status load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (first 15 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD011 10 1 20240120 1 1299.99 1299.99 completed ORD012 11 2 20240120 5 29.99 149.95 completed ORD013 12 3 20240121 2 249.99 499.98 completed ORD014 1 4 20240121 1 49.99 49.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#agg_daily_sales-14-rows","title":"agg_daily_sales (14 rows)","text":"<p>Sample Data (all 14 rows):</p> date_sk total_revenue order_count unique_customers unique_products avg_order_value 20240115 1439.96 3 2 3 479.99 20240116 589.95 3 3 3 196.65 20240117 749.98 2 2 2 374.99 20240118 983.94 2 2 2 491.97 20240119 269.98 2 2 2 134.99 20240120 1449.94 2 2 2 724.97 20240121 549.97 2 2 2 274.99 20240122 899.97 2 2 2 449.99 20240123 537.96 2 2 2 268.98 20240124 349.97 2 2 2 174.99 20240125 1389.96 2 2 2 694.98 20240126 349.97 2 2 2 174.99 20240127 749.98 2 2 2 374.99 20240128 491.97 2 2 2 245.99"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#sample-analytical-queries","title":"Sample Analytical Queries","text":""},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#revenue-by-region","title":"Revenue by Region","text":"<pre><code>SELECT \n    c.region,\n    COUNT(DISTINCT f.order_id) AS orders,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.region\nORDER BY revenue DESC;\n</code></pre> region orders revenue North 8 2,599.87 West 7 2,629.88 South 7 2,449.92 East 8 2,023.87"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#revenue-by-category","title":"Revenue by Category","text":"<pre><code>SELECT \n    p.category,\n    COUNT(*) AS orders,\n    SUM(f.line_total) AS revenue,\n    AVG(f.line_total) AS avg_order\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.category\nORDER BY revenue DESC;\n</code></pre> category orders revenue avg_order Electronics 18 6,619.63 367.76 Furniture 12 3,183.91 265.33"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#daily-trend-with-day-of-week","title":"Daily Trend with Day of Week","text":"<pre><code>SELECT \n    d.full_date,\n    d.day_of_week,\n    a.total_revenue,\n    a.order_count\nFROM agg_daily_sales a\nJOIN dim_date d ON a.date_sk = d.date_sk\nORDER BY a.date_sk;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#top-customers","title":"Top Customers","text":"<pre><code>SELECT \n    c.name,\n    c.region,\n    COUNT(*) AS orders,\n    SUM(f.line_total) AS total_spent\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.customer_sk, c.name, c.region\nORDER BY total_spent DESC\nLIMIT 5;\n</code></pre> name region orders total_spent Alice Johnson North 4 1,559.95 Bob Smith South 3 1,049.97 Carol White North 3 541.93 David Brown East 3 1,079.97 Emma Davis West 3 477.97"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#data-flow-diagram","title":"Data Flow Diagram","text":"<pre><code>flowchart TB\n    subgraph Sources[\"Source Data\"]\n        S1[customers.csv]\n        S2[products.csv]\n        S3[orders.csv]\n    end\n\n    subgraph Dimensions[\"Dimension Layer\"]\n        D1[dim_customer&lt;br/&gt;SCD Type 2]\n        D2[dim_product&lt;br/&gt;SCD Type 1]\n        D3[dim_date&lt;br/&gt;Generated]\n    end\n\n    subgraph Facts[\"Fact Layer\"]\n        F1[fact_orders&lt;br/&gt;30 rows]\n    end\n\n    subgraph Aggregates[\"Aggregate Layer\"]\n        A1[agg_daily_sales&lt;br/&gt;14 rows]\n    end\n\n    S1 --&gt; D1\n    S2 --&gt; D2\n    S3 --&gt; F1\n\n    D1 --&gt; F1\n    D2 --&gt; F1\n    D3 --&gt; F1\n\n    F1 --&gt; A1\n\n    style Sources fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Dimensions fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Facts fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Aggregates fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>How to combine all patterns into a complete star schema</li> <li>The order of operations: dimensions first, then facts, then aggregates</li> <li>How to organize pipelines for maintainability</li> <li>Pipeline dependencies using <code>depends_on</code></li> <li>How to query the completed star schema for analytics</li> </ul>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#congratulations","title":"Congratulations!","text":"<p>You've built a complete dimensional data warehouse! This foundation can scale to handle millions of rows and complex business requirements.</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#next-steps","title":"Next Steps","text":"<p>Now that you have a working star schema, let's add a semantic layer to make querying even easier.</p> <p>Next: Semantic Layer Introduction</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#navigation","title":"Navigation","text":"Previous Up Next Aggregation Pattern Tutorials Semantic Layer Intro"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/","title":"Semantic Layer Introduction","text":"<p>In this tutorial, you'll learn what a semantic layer is, why it's valuable, and how it sits on top of your star schema to provide a business-friendly query interface.</p> <p>What You'll Learn: - What is a semantic layer? - Why not just write SQL? - Metrics, dimensions, and materializations - When to use pipelines vs semantic layer</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#what-is-a-semantic-layer","title":"What is a Semantic Layer?","text":"<p>A semantic layer is a translation layer between your raw data and your business users. It defines business concepts (like \"revenue\" or \"active customers\") once, and lets everyone query them consistently.</p> <p>Think of it like a glossary for your data:</p> Business Term Technical Definition Revenue <code>SUM(line_total)</code> from <code>fact_orders</code> where <code>status = 'completed'</code> Order Count <code>COUNT(*)</code> from <code>fact_orders</code> Active Customer Customer with order in last 90 days North Region <code>region = 'North'</code> in <code>dim_customer</code> <p>Without a semantic layer, everyone writes their own SQL\u2014and everyone might calculate \"revenue\" differently.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#the-problem-inconsistent-definitions","title":"The Problem: Inconsistent Definitions","text":""},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#without-semantic-layer","title":"Without Semantic Layer","text":"<p>Marketing team:</p> <pre><code>-- \"Revenue\" = all orders\nSELECT SUM(line_total) AS revenue FROM fact_orders;\n-- Result: $9,803.54\n</code></pre> <p>Finance team:</p> <pre><code>-- \"Revenue\" = only completed orders\nSELECT SUM(line_total) AS revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Result: $8,953.56\n</code></pre> <p>Executive dashboard:</p> <pre><code>-- \"Revenue\" = completed orders, excluding discounts\nSELECT SUM(line_total * 0.95) AS revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Result: $8,505.88\n</code></pre> <p>Result: Three different \"revenue\" numbers in the same meeting. Chaos.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#with-semantic-layer","title":"With Semantic Layer","text":"<pre><code># Everyone uses the same metric definition\nresult = query.execute(\"revenue\", context)\n# Result: $8,953.56 (always)\n</code></pre> <p>The semantic layer enforces a single, governed definition of \"revenue.\"</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#why-not-just-write-sql","title":"Why Not Just Write SQL?","text":"<p>SQL is powerful, but it has limitations for business users:</p> Challenge Without Semantic Layer With Semantic Layer Complex joins Users must know table relationships Automatic join handling Consistent definitions Everyone writes their own Define once, use everywhere Filter logic Repeated in every query Embedded in metric definition Aggregation errors Easy to make mistakes Pre-validated expressions Self-service Requires SQL expertise Business-friendly syntax Governance No central control Single source of truth"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#how-the-semantic-layer-fits","title":"How the Semantic Layer Fits","text":"<p>The semantic layer sits on top of your star schema:</p> <pre><code>flowchart TB\n    subgraph Sources[\"Source Systems\"]\n        S1[CRM]\n        S2[ERP]\n        S3[POS]\n    end\n\n    subgraph Pipeline[\"Odibi Pipelines (YAML)\"]\n        P1[dimension pattern]\n        P2[fact pattern]\n        P3[aggregation pattern]\n    end\n\n    subgraph StarSchema[\"Star Schema\"]\n        D1[(dim_customer)]\n        D2[(dim_product)]\n        D3[(dim_date)]\n        F1[(fact_orders)]\n        A1[(agg_daily_sales)]\n    end\n\n    subgraph Semantic[\"Semantic Layer (Python API)\"]\n        M1[Metrics]\n        M2[Dimensions]\n        M3[Materializations]\n    end\n\n    subgraph Consumers[\"Consumers\"]\n        C1[Dashboards]\n        C2[Reports]\n        C3[Ad-hoc Queries]\n        C4[Data Apps]\n    end\n\n    S1 --&gt; P1\n    S2 --&gt; P2\n    S3 --&gt; P3\n\n    P1 --&gt; D1\n    P1 --&gt; D2\n    P2 --&gt; D3\n    P2 --&gt; F1\n    P3 --&gt; A1\n\n    D1 --&gt; M1\n    D2 --&gt; M1\n    D3 --&gt; M1\n    F1 --&gt; M1\n    A1 --&gt; M1\n\n    M1 --&gt; C1\n    M2 --&gt; C2\n    M3 --&gt; C3\n    M1 --&gt; C4\n\n    style Sources fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Pipeline fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style StarSchema fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Semantic fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Consumers fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre> <p>Odibi Pipelines build the data (dimensions, facts, aggregates). Semantic Layer defines how to query the data (metrics, dimensions, materializations).</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#core-concepts","title":"Core Concepts","text":""},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#metrics","title":"Metrics","text":"<p>A metric is a measurable value that can be aggregated. It answers \"how much?\" or \"how many?\"</p> <pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre> <p>Examples: - Revenue (<code>SUM(line_total)</code>) - Order Count (<code>COUNT(*)</code>) - Average Order Value (<code>AVG(line_total)</code>) - Unique Customers (<code>COUNT(DISTINCT customer_sk)</code>)</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#dimensions","title":"Dimensions","text":"<p>A dimension is an attribute for grouping and filtering. It answers \"by what?\" or \"for what?\"</p> <pre><code>dimensions:\n  - name: region\n    source: dim_customer\n    column: region\n</code></pre> <p>Examples: - Region (North, South, East, West) - Category (Electronics, Furniture) - Month (January, February, ...) - Day of Week (Monday, Tuesday, ...)</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#materializations","title":"Materializations","text":"<p>A materialization pre-computes metrics at a specific grain and saves them to a table. It answers \"what should be pre-calculated for dashboards?\"</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n    schedule: \"0 2 1 * *\"  # Monthly\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#when-to-use-what","title":"When to Use What","text":"Task Solution Build dimension tables from source Odibi Pipeline: <code>dimension</code> pattern Build fact tables from source Odibi Pipeline: <code>fact</code> pattern Build scheduled aggregates Odibi Pipeline: <code>aggregation</code> pattern Ad-hoc metric queries Semantic Layer: <code>SemanticQuery</code> Self-service analytics Semantic Layer with dimensions Dashboard metrics Semantic Layer: <code>Materializer</code>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#pipelines-vs-semantic-layer","title":"Pipelines vs Semantic Layer","text":"<p>Use Pipelines when: - Building the star schema from source data - Scheduled ETL/ELT processes - Transforming and cleaning data - Generating surrogate keys</p> <p>Use Semantic Layer when: - Defining business metrics consistently - Enabling self-service analytics - Pre-computing dashboard metrics - Creating a governed metric catalog</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#unified-project-api-recommended","title":"Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is through the unified <code>Project</code> API. This connects your pipelines and semantic layer seamlessly:</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#1-add-semantic-config-to-odibiyaml","title":"1. Add Semantic Config to odibi.yaml","text":"<pre><code># odibi.yaml\nproject: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#2-query-with-two-lines-of-code","title":"2. Query with Two Lines of Code","text":"<pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>That's it! The <code>Project</code> class: - Reads connections and pipelines from your YAML - Resolves <code>$build_warehouse.fact_orders</code> \u2192 node's write path - Auto-loads Delta tables when queried - No manual <code>context.register()</code> calls needed</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#manual-approach","title":"Manual Approach","text":"<p>If you prefer more control, you can use the semantic layer components directly.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#example-revenue-metric","title":"Example: Revenue Metric","text":"<p>Let's see how a simple metric works:</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#1-define-the-metric","title":"1. Define the Metric","text":"<pre><code>from odibi.semantics import MetricDefinition\n\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from completed orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#2-create-the-config","title":"2. Create the Config","text":"<pre><code>from odibi.semantics import SemanticLayerConfig, DimensionDefinition\n\nconfig = SemanticLayerConfig(\n    metrics=[revenue],\n    dimensions=[\n        DimensionDefinition(\n            name=\"region\",\n            source=\"dim_customer\",\n            column=\"region\"\n        )\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#3-query-the-metric","title":"3. Query the Metric","text":"<pre><code>from odibi.semantics import SemanticQuery\n\nquery = SemanticQuery(config)\n\n# Total revenue\nresult = query.execute(\"revenue\", context)\nprint(result.df)\n# | revenue    |\n# |------------|\n# | 8,953.56   |\n\n# Revenue by region\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df)\n# | region | revenue  |\n# |--------|----------|\n# | North  | 2,549.88 |\n# | South  | 2,349.93 |\n# | East   | 1,923.88 |\n# | West   | 2,129.87 |\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#the-query-syntax","title":"The Query Syntax","text":"<p>Semantic queries use a simple, business-friendly syntax:</p> <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre> <p>Examples:</p> Query Meaning <code>\"revenue\"</code> Total revenue <code>\"revenue BY region\"</code> Revenue grouped by region <code>\"revenue, order_count BY region\"</code> Multiple metrics by region <code>\"revenue BY region, month\"</code> Revenue by region and month <code>\"revenue BY region WHERE year = 2024\"</code> Filtered revenue by region"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#benefits-summary","title":"Benefits Summary","text":"Benefit Description Consistency One definition of \"revenue\" everywhere Governance Central control over metric logic Self-Service Business users query without SQL Performance Pre-computed materializations for dashboards Discoverability Metrics are documented and cataloged Maintainability Change definition once, updates everywhere"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#architecture-overview","title":"Architecture Overview","text":"<pre><code>classDiagram\n    class SemanticLayerConfig {\n        +metrics: List[MetricDefinition]\n        +dimensions: List[DimensionDefinition]\n        +materializations: List[MaterializationConfig]\n        +get_metric(name)\n        +get_dimension(name)\n    }\n\n    class MetricDefinition {\n        +name: str\n        +description: str\n        +expr: str\n        +source: str\n        +filters: List[str]\n    }\n\n    class DimensionDefinition {\n        +name: str\n        +source: str\n        +column: str\n        +hierarchy: List[str]\n    }\n\n    class SemanticQuery {\n        +config: SemanticLayerConfig\n        +execute(query_string, context)\n        +parse(query_string)\n        +validate(parsed_query)\n    }\n\n    class Materializer {\n        +config: SemanticLayerConfig\n        +execute(name, context)\n        +execute_all(context)\n    }\n\n    SemanticLayerConfig --&gt; MetricDefinition\n    SemanticLayerConfig --&gt; DimensionDefinition\n    SemanticQuery --&gt; SemanticLayerConfig\n    Materializer --&gt; SemanticLayerConfig\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>A semantic layer translates business concepts into technical queries</li> <li>It ensures consistent definitions across all users</li> <li>Metrics are measurable values (SUM, COUNT, AVG)</li> <li>Dimensions are grouping attributes (region, category, date)</li> <li>Materializations pre-compute metrics for performance</li> <li>Pipelines build the data; semantic layer queries it</li> <li>The query syntax is simple: <code>\"metric BY dimension WHERE filter\"</code></li> </ul>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to define metrics in detail.</p> <p>Next: Defining Metrics</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#navigation","title":"Navigation","text":"Previous Up Next Full Star Schema Tutorials Defining Metrics"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#reference","title":"Reference","text":"<p>For complete documentation, see: Semantic Layer Overview</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/","title":"Defining Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to define metrics in the Odibi semantic layer, from simple aggregations to complex filtered and derived metrics.</p> <p>What You'll Learn: - Simple metrics (SUM, COUNT, AVG) - Filtered metrics (with WHERE conditions) - Multiple metrics together - Derived metrics (calculated from other metrics)</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#source-data-fact_orders","title":"Source Data: fact_orders","text":"<p>We'll use the fact_orders table from Tutorial 06 (15 sample rows shown):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending ORD023 10 3 20240126 1 249.99 249.99 cancelled ... ... ... ... ... ... ... ... <p>Total: 30 rows - 27 completed - 1 pending - 2 cancelled</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-1-define-a-simple-metric","title":"Step 1: Define a Simple Metric","text":"<p>Let's start with the most basic metric: total revenue.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import MetricDefinition\n\n# Define a simple revenue metric\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from all orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from all orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"revenue\"</code> How you'll reference this metric in queries <code>description</code> <code>\"Total revenue...\"</code> Human-readable documentation <code>expr</code> <code>\"SUM(line_total)\"</code> SQL aggregation expression <code>source</code> <code>\"fact_orders\"</code> Table to aggregate from"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#source-notation","title":"Source Notation","text":"<p>The <code>source</code> field supports three formats:</p> Format Example When to Use $pipeline.node <code>$build_warehouse.fact_orders</code> With <code>Project</code> API (recommended) connection.path <code>gold.fact_orders</code> External tables not in pipelines bare name <code>fact_orders</code> Manual setup with <code>context.register()</code> <p>Note: This tutorial shows both the <code>Project</code> API (recommended) and manual setup approaches. The manual sections use bare names like <code>source: fact_orders</code> because they match what you register with <code>context.register(\"fact_orders\", df)</code>.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#option-1-node-reference-recommended","title":"Option 1: Node Reference (Recommended)","text":"<p>Reference the pipeline node that produces the table. The semantic layer automatically reads from wherever that node writes:</p> <pre><code># odibi.yaml\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References the node above\n</code></pre> <p>This approach: - DRY - No duplication; the node already knows its write location - Auto-synced - If you change the node's write config, the semantic layer follows - Uses the same <code>$pipeline.node</code> pattern as cross-pipeline <code>inputs</code></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#option-2-connectionpath-explicit","title":"Option 2: Connection.Path (Explicit)","text":"<p>For tables that exist outside pipelines or when you want explicit control:</p> <pre><code>semantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: gold.fact_orders    # Resolves to /mnt/data/gold/fact_orders\n</code></pre> <p>Nested paths are supported. The split happens on the first dot only, so everything after becomes the path:</p> <pre><code># Given connection:\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# These all work:\nsource: gold.fact_orders              # \u2192 /mnt/data/gold/fact_orders\nsource: gold.oee/plant_a/metrics      # \u2192 /mnt/data/gold/oee/plant_a/metrics\nsource: gold.domain/v2/fact_sales     # \u2192 /mnt/data/gold/domain/v2/fact_sales\n</code></pre> <p>For Unity Catalog connections with <code>catalog</code> + <code>schema_name</code>:</p> <pre><code>connections:\n  gold:\n    type: delta\n    catalog: main\n    schema_name: gold_db\n\nsource: gold.fact_orders    # \u2192 main.gold_db.fact_orders\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-result","title":"Query Result","text":"<pre><code>result = query.execute(\"revenue\", context)\n</code></pre> revenue 9,803.54 <p>This includes ALL orders (completed, pending, cancelled).</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-2-define-a-filtered-metric","title":"Step 2: Define a Filtered Metric","text":"<p>Usually, you only want \"revenue\" to include completed orders. Add a filter:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_1","title":"Python Code","text":"<pre><code>completed_revenue = MetricDefinition(\n    name=\"completed_revenue\",\n    description=\"Revenue from completed orders only\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_1","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: completed_revenue\n    description: \"Revenue from completed orders only\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#how-filters-work","title":"How Filters Work","text":"<p>Without filter (revenue):</p> <pre><code>SELECT SUM(line_total) AS revenue FROM fact_orders;\n-- Uses all 30 rows\n</code></pre> <p>With filter (completed_revenue):</p> <pre><code>SELECT SUM(line_total) AS completed_revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Uses only 27 completed rows\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#data-comparison","title":"Data Comparison","text":"<p>All orders (30 rows) - includes:</p> order_id line_total status ORD001 1299.99 completed ORD002 59.98 completed ... ... ... ORD015 599.99 pending (excluded) ORD023 249.99 cancelled (excluded) ... ... ... <p>Completed only (27 rows):</p> order_id line_total status ORD001 1299.99 completed ORD002 59.98 completed ... ... ..."},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-results","title":"Query Results","text":"Metric Value Rows Included revenue 9,803.54 30 (all) completed_revenue 8,953.56 27 (completed only)"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-3-define-multiple-metrics","title":"Step 3: Define Multiple Metrics","text":"<p>Let's define a complete set of metrics:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_2","title":"Python Code","text":"<pre><code>from odibi.semantics import MetricDefinition, SemanticLayerConfig\n\n# Revenue metrics\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from completed orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Count metrics\norder_count = MetricDefinition(\n    name=\"order_count\",\n    description=\"Number of orders\",\n    expr=\"COUNT(*)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\nunique_customers = MetricDefinition(\n    name=\"unique_customers\",\n    description=\"Number of unique customers\",\n    expr=\"COUNT(DISTINCT customer_sk)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Average metrics\navg_order_value = MetricDefinition(\n    name=\"avg_order_value\",\n    description=\"Average order value\",\n    expr=\"AVG(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Volume metrics\ntotal_quantity = MetricDefinition(\n    name=\"total_quantity\",\n    description=\"Total units sold\",\n    expr=\"SUM(quantity)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Combine into config\nconfig = SemanticLayerConfig(\n    metrics=[\n        revenue,\n        order_count,\n        unique_customers,\n        avg_order_value,\n        total_quantity\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_2","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: order_count\n    description: \"Number of orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-results_1","title":"Query Results","text":"<p>Single metric:</p> <pre><code>result = query.execute(\"revenue\", context)\n</code></pre> revenue 8,953.56 <p>Multiple metrics:</p> <pre><code>result = query.execute(\"revenue, order_count, avg_order_value\", context)\n</code></pre> revenue order_count avg_order_value 8,953.56 27 331.61"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-4-define-a-derived-metric","title":"Step 4: Define a Derived Metric","text":"<p>A derived metric is calculated from other metrics. It doesn't aggregate directly from the source.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#scenario-profit-margin","title":"Scenario: Profit Margin","text":"<p>We want to calculate profit margin, which requires cost data. Let's assume we've added a cost column:</p> <p>fact_orders with cost (sample):</p> order_id line_total cost_total ORD001 1299.99 850.00 ORD002 59.98 24.00 ORD003 249.99 120.00"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_3","title":"Python Code","text":"<pre><code># Base metrics\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\ntotal_cost = MetricDefinition(\n    name=\"total_cost\",\n    expr=\"SUM(cost_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\nprofit = MetricDefinition(\n    name=\"profit\",\n    expr=\"SUM(line_total) - SUM(cost_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Derived metric (calculated from other metrics)\nprofit_margin = MetricDefinition(\n    name=\"profit_margin\",\n    description=\"Profit as percentage of revenue\",\n    expr=\"(revenue - total_cost) / revenue\",\n    type=\"derived\"  # Indicates this references other metrics\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_3","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: total_cost\n    expr: \"SUM(cost_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit\n    expr: \"SUM(line_total) - SUM(cost_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit_margin\n    description: \"Profit as percentage of revenue\"\n    expr: \"(revenue - total_cost) / revenue\"\n    type: derived\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#complete-semanticlayerconfig","title":"Complete SemanticLayerConfig","text":"<p>Here's the complete configuration with all our metrics:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_4","title":"Python Code","text":"<pre><code>from odibi.semantics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig\n)\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        # Revenue metrics\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"pending_revenue\",\n            description=\"Revenue from pending orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'pending'\"]\n        ),\n\n        # Count metrics\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"unique_customers\",\n            description=\"Number of unique customers\",\n            expr=\"COUNT(DISTINCT customer_sk)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n\n        # Average metrics\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n\n        # Volume metrics\n        MetricDefinition(\n            name=\"total_quantity\",\n            description=\"Total units sold\",\n            expr=\"SUM(quantity)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[]  # We'll add these in the next tutorial\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_4","title":"YAML Alternative","text":"<pre><code># File: semantic_config.yaml\nmetrics:\n  # Revenue metrics\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: pending_revenue\n    description: \"Revenue from pending orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'pending'\"\n\n  # Count metrics\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Average metrics\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Volume metrics\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\ndimensions: []  # Added in next tutorial\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>SUM(column)</code> Sum of values <code>SUM(line_total)</code> <code>COUNT(*)</code> Row count <code>COUNT(*)</code> <code>COUNT(column)</code> Non-null count <code>COUNT(customer_sk)</code> <code>COUNT(DISTINCT column)</code> Unique count <code>COUNT(DISTINCT customer_sk)</code> <code>AVG(column)</code> Average <code>AVG(line_total)</code> <code>MIN(column)</code> Minimum <code>MIN(line_total)</code> <code>MAX(column)</code> Maximum <code>MAX(line_total)</code>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#complex-expressions","title":"Complex Expressions","text":"<pre><code># Percentage of total (within group)\n- name: revenue_share\n  expr: \"SUM(line_total) / SUM(SUM(line_total)) OVER ()\"\n\n# Conditional sum\n- name: high_value_revenue\n  expr: \"SUM(CASE WHEN line_total &gt; 500 THEN line_total ELSE 0 END)\"\n\n# Ratio\n- name: items_per_order\n  expr: \"SUM(quantity) / COUNT(*)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#metric-naming-best-practices","title":"Metric Naming Best Practices","text":"Do Don't <code>revenue</code> <code>rev</code> <code>order_count</code> <code>cnt</code> <code>completed_revenue</code> <code>rev_comp</code> <code>avg_order_value</code> <code>aov</code> (unless standard) <code>unique_customers</code> <code>cust_distinct</code> <p>Guidelines: - Use <code>snake_case</code> - Be descriptive: <code>completed_order_revenue</code> over <code>rev1</code> - Prefix related metrics: <code>revenue</code>, <code>revenue_completed</code>, <code>revenue_pending</code> - Include the filter in the name: <code>last_30_days_revenue</code></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple metrics aggregate directly from source: <code>SUM(line_total)</code></li> <li>Filters constrain which rows are included: <code>status = 'completed'</code></li> <li>Multiple metrics can be defined and queried together</li> <li>Derived metrics calculate from other metrics: <code>revenue - cost</code></li> <li>The expr field uses SQL aggregation syntax</li> <li>The source field specifies which table to query</li> <li>Naming conventions make metrics discoverable</li> </ul>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to define dimensions for grouping and filtering.</p> <p>Next: Defining Dimensions</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#navigation","title":"Navigation","text":"Previous Up Next Semantic Layer Intro Tutorials Defining Dimensions"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Defining Metrics Reference</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/","title":"Defining Dimensions Tutorial","text":"<p>In this tutorial, you'll learn how to define semantic layer dimensions for grouping and filtering metrics. Dimensions are the \"BY\" part of queries like <code>\"revenue BY region\"</code>.</p> <p>What You'll Learn: - Simple dimensions (single column) - Dimensions with different column names - Hierarchical dimensions (drill-down) - Complete config with metrics AND dimensions</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#star-schema-data","title":"Star Schema Data","text":"<p>We'll use the star schema from Tutorial 06:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_customer-sample","title":"dim_customer (sample)","text":"customer_sk customer_id name region city state 1 C001 Alice Johnson North Chicago IL 2 C002 Bob Smith South Houston TX 3 C003 Carol White North Detroit MI 4 C004 David Brown East New York NY 5 C005 Emma Davis West Seattle WA 6 C006 Frank Miller South Miami FL 7 C007 Grace Lee East Boston MA 8 C008 Henry Wilson West Portland OR 9 C009 Ivy Chen North Minneapolis MN 10 C010 Jack Taylor South Dallas TX 11 C011 Karen Martinez East Philadelphia PA 12 C012 Leo Anderson West Denver CO"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_product-sample","title":"dim_product (sample)","text":"product_sk product_id name category subcategory 1 P001 Laptop Pro 15 Electronics Computers 2 P002 Wireless Mouse Electronics Accessories 3 P003 Office Chair Furniture Seating 4 P004 USB-C Hub Electronics Accessories 5 P005 Standing Desk Furniture Desks 6 P006 Mechanical Keyboard Electronics Accessories 7 P007 Monitor 27\" Electronics Displays 8 P008 Desk Lamp Furniture Lighting 9 P009 Webcam HD Electronics Accessories 10 P010 Filing Cabinet Furniture Storage"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_date-sample","title":"dim_date (sample)","text":"date_sk full_date day_of_week month month_name quarter_name year 20240115 2024-01-15 Monday 1 January Q1 2024 20240116 2024-01-16 Tuesday 1 January Q1 2024 20240117 2024-01-17 Wednesday 1 January Q1 2024 20240118 2024-01-18 Thursday 1 January Q1 2024 20240119 2024-01-19 Friday 1 January Q1 2024 20240120 2024-01-20 Saturday 1 January Q1 2024 20240121 2024-01-21 Sunday 1 January Q1 2024 20240122 2024-01-22 Monday 1 January Q1 2024 20240123 2024-01-23 Tuesday 1 January Q1 2024 20240124 2024-01-24 Wednesday 1 January Q1 2024"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#unified-project-api-recommended","title":"Unified Project API (Recommended)","text":"<p>When using the unified <code>Project</code> API, dimensions can reference pipeline nodes directly using the <code>$pipeline.node</code> notation:</p> <pre><code># odibi.yaml\nproject: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: dim_customer\n        write:\n          connection: gold\n          table: dim_customer\n      - name: dim_product\n        write:\n          connection: gold\n          table: dim_product\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # References node's write target\n      column: region\n\n    - name: category\n      source: $build_warehouse.dim_product    # No path duplication!\n      column: category\n</code></pre> <p>The <code>source: $build_warehouse.dim_customer</code> notation: 1. Looks up the <code>dim_customer</code> node in the <code>build_warehouse</code> pipeline 2. Reads its <code>write.connection</code> and <code>write.table</code> config 3. Resolves the full path automatically</p> <p>Alternative: You can also use <code>source: gold.dim_customer</code> (connection.table) for tables not managed by pipelines.</p> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region, category\")  # Tables auto-loaded\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-1-define-a-simple-dimension","title":"Step 1: Define a Simple Dimension","text":"<p>The simplest dimension maps a column directly:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import DimensionDefinition\n\n# Simple dimension: region from dim_customer\nregion = DimensionDefinition(\n    name=\"region\",\n    source=\"dim_customer\",\n    column=\"region\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: region\n    source: dim_customer\n    column: region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"region\"</code> How you reference it in queries <code>source</code> <code>\"dim_customer\"</code> Table containing the dimension <code>column</code> <code>\"region\"</code> Column to GROUP BY"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#query-example","title":"Query Example","text":"<pre><code>result = query.execute(\"revenue BY region\", context)\n</code></pre> <p>Result (4 rows):</p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-2-dimension-with-different-column-name","title":"Step 2: Dimension with Different Column Name","text":"<p>Sometimes you want the dimension name to differ from the column name:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_1","title":"Python Code","text":"<pre><code># Dimension name differs from column name\ncustomer_city = DimensionDefinition(\n    name=\"customer_city\",      # Query uses \"customer_city\"\n    source=\"dim_customer\",\n    column=\"city\"              # Actual column is \"city\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_1","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: customer_city\n    source: dim_customer\n    column: city\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#query-example_1","title":"Query Example","text":"<pre><code>result = query.execute(\"revenue BY customer_city\", context)\n</code></pre> <p>Result (12 rows):</p> customer_city revenue Chicago 1,559.95 Houston 1,049.97 Detroit 541.93 New York 1,079.97 Seattle 477.97 Miami 959.95 Boston 573.94 Portland 1,379.98 Minneapolis 469.95 Dallas 1,549.98 Philadelphia 249.92 Denver 1,309.96"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-3-dimension-with-hierarchy","title":"Step 3: Dimension with Hierarchy","text":"<p>A hierarchy defines drill-down levels. Users can start at a high level (year) and drill into details (month, week, day).</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#the-drill-down-concept","title":"The Drill-Down Concept","text":"<pre><code>Year (2024)\n  \u2514\u2500\u2500 Quarter (Q1)\n        \u2514\u2500\u2500 Month (January)\n              \u2514\u2500\u2500 Week (Week 3)\n                    \u2514\u2500\u2500 Day (Jan 15)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_2","title":"Python Code","text":"<pre><code># Date dimension with hierarchy\norder_date = DimensionDefinition(\n    name=\"order_date\",\n    source=\"dim_date\",\n    column=\"full_date\",\n    hierarchy=[\"year\", \"quarter_name\", \"month_name\", \"full_date\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_2","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: order_date\n    source: dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n      - full_date\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#using-hierarchy-levels-in-queries","title":"Using Hierarchy Levels in Queries","text":"<p>Top level - Year:</p> <pre><code>result = query.execute(\"revenue BY year\", context)\n</code></pre> year revenue 2024 8,953.56 <p>Drill down - Quarter:</p> <pre><code>result = query.execute(\"revenue BY quarter_name\", context)\n</code></pre> quarter_name revenue Q1 8,953.56 <p>Drill down - Month:</p> <pre><code>result = query.execute(\"revenue BY month_name\", context)\n</code></pre> month_name revenue January 8,953.56 <p>Drill down - Day:</p> <pre><code>result = query.execute(\"revenue BY full_date\", context)\n</code></pre> full_date revenue 2024-01-15 1,439.96 2024-01-16 589.95 2024-01-17 749.98 ... ..."},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-4-define-all-dimensions-for-our-star-schema","title":"Step 4: Define All Dimensions for Our Star Schema","text":"<p>Let's define a complete set of dimensions:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_3","title":"Python Code","text":"<pre><code>from odibi.semantics import DimensionDefinition\n\ndimensions = [\n    # Geographic dimensions (from dim_customer)\n    DimensionDefinition(\n        name=\"region\",\n        source=\"dim_customer\",\n        column=\"region\",\n        description=\"Customer geographic region\"\n    ),\n    DimensionDefinition(\n        name=\"city\",\n        source=\"dim_customer\",\n        column=\"city\"\n    ),\n    DimensionDefinition(\n        name=\"state\",\n        source=\"dim_customer\",\n        column=\"state\"\n    ),\n\n    # Product dimensions (from dim_product)\n    DimensionDefinition(\n        name=\"category\",\n        source=\"dim_product\",\n        column=\"category\",\n        description=\"Product category\"\n    ),\n    DimensionDefinition(\n        name=\"subcategory\",\n        source=\"dim_product\",\n        column=\"subcategory\"\n    ),\n    DimensionDefinition(\n        name=\"product_name\",\n        source=\"dim_product\",\n        column=\"name\",\n        hierarchy=[\"category\", \"subcategory\", \"name\"]\n    ),\n\n    # Time dimensions (from dim_date)\n    DimensionDefinition(\n        name=\"year\",\n        source=\"dim_date\",\n        column=\"year\"\n    ),\n    DimensionDefinition(\n        name=\"quarter\",\n        source=\"dim_date\",\n        column=\"quarter_name\"\n    ),\n    DimensionDefinition(\n        name=\"month\",\n        source=\"dim_date\",\n        column=\"month_name\",\n        hierarchy=[\"year\", \"quarter_name\", \"month_name\"]\n    ),\n    DimensionDefinition(\n        name=\"day_of_week\",\n        source=\"dim_date\",\n        column=\"day_of_week\"\n    ),\n\n    # Order dimensions (from fact_orders)\n    DimensionDefinition(\n        name=\"status\",\n        source=\"fact_orders\",\n        column=\"status\"\n    )\n]\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_3","title":"YAML Alternative","text":"<pre><code>dimensions:\n  # Geographic dimensions\n  - name: region\n    source: dim_customer\n    column: region\n    description: \"Customer geographic region\"\n\n  - name: city\n    source: dim_customer\n    column: city\n\n  - name: state\n    source: dim_customer\n    column: state\n\n  # Product dimensions\n  - name: category\n    source: dim_product\n    column: category\n    description: \"Product category\"\n\n  - name: subcategory\n    source: dim_product\n    column: subcategory\n\n  - name: product_name\n    source: dim_product\n    column: name\n    hierarchy:\n      - category\n      - subcategory\n      - name\n\n  # Time dimensions\n  - name: year\n    source: dim_date\n    column: year\n\n  - name: quarter\n    source: dim_date\n    column: quarter_name\n\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n\n  - name: day_of_week\n    source: dim_date\n    column: day_of_week\n\n  # Order dimensions\n  - name: status\n    source: fact_orders\n    column: status\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#complete-config-with-metrics-and-dimensions","title":"Complete Config with Metrics AND Dimensions","text":"<p>Here's the full SemanticLayerConfig:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_4","title":"Python Code","text":"<pre><code>from odibi.semantics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig\n)\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"unique_customers\",\n            description=\"Number of unique customers\",\n            expr=\"COUNT(DISTINCT customer_sk)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"total_quantity\",\n            description=\"Total units sold\",\n            expr=\"SUM(quantity)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[\n        # Geographic\n        DimensionDefinition(name=\"region\", source=\"dim_customer\", column=\"region\"),\n        DimensionDefinition(name=\"city\", source=\"dim_customer\", column=\"city\"),\n        DimensionDefinition(name=\"state\", source=\"dim_customer\", column=\"state\"),\n\n        # Product\n        DimensionDefinition(name=\"category\", source=\"dim_product\", column=\"category\"),\n        DimensionDefinition(name=\"subcategory\", source=\"dim_product\", column=\"subcategory\"),\n        DimensionDefinition(\n            name=\"product_name\", \n            source=\"dim_product\", \n            column=\"name\",\n            hierarchy=[\"category\", \"subcategory\", \"name\"]\n        ),\n\n        # Time\n        DimensionDefinition(name=\"year\", source=\"dim_date\", column=\"year\"),\n        DimensionDefinition(name=\"quarter\", source=\"dim_date\", column=\"quarter_name\"),\n        DimensionDefinition(\n            name=\"month\", \n            source=\"dim_date\", \n            column=\"month_name\",\n            hierarchy=[\"year\", \"quarter_name\", \"month_name\"]\n        ),\n        DimensionDefinition(name=\"day_of_week\", source=\"dim_date\", column=\"day_of_week\"),\n\n        # Order\n        DimensionDefinition(name=\"status\", source=\"fact_orders\", column=\"status\")\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative-semantic_configyaml","title":"YAML Alternative (semantic_config.yaml)","text":"<pre><code># File: semantic_config.yaml\nmetrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\ndimensions:\n  # Geographic\n  - name: region\n    source: dim_customer\n    column: region\n  - name: city\n    source: dim_customer\n    column: city\n  - name: state\n    source: dim_customer\n    column: state\n\n  # Product\n  - name: category\n    source: dim_product\n    column: category\n  - name: subcategory\n    source: dim_product\n    column: subcategory\n  - name: product_name\n    source: dim_product\n    column: name\n    hierarchy: [category, subcategory, name]\n\n  # Time\n  - name: year\n    source: dim_date\n    column: year\n  - name: quarter\n    source: dim_date\n    column: quarter_name\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy: [year, quarter_name, month_name]\n  - name: day_of_week\n    source: dim_date\n    column: day_of_week\n\n  # Order\n  - name: status\n    source: fact_orders\n    column: status\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#example-queries-with-dimensions","title":"Example Queries with Dimensions","text":"<p>Now you can run rich queries:</p> <p>Revenue by region:</p> <pre><code>result = query.execute(\"revenue BY region\", context)\n</code></pre> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87 <p>Revenue by category and region:</p> <pre><code>result = query.execute(\"revenue BY category, region\", context)\n</code></pre> category region revenue Electronics North 1,549.94 Electronics South 1,449.95 Electronics East 1,323.91 Electronics West 1,079.93 Furniture North 999.94 Furniture South 899.98 Furniture East 599.97 Furniture West 1,049.94 <p>Multiple metrics by day of week:</p> <pre><code>result = query.execute(\"revenue, order_count, avg_order_value BY day_of_week\", context)\n</code></pre> day_of_week revenue order_count avg_order_value Monday 2,189.94 5 437.99 Tuesday 1,177.93 5 235.59 Wednesday 1,099.95 4 275.00 Thursday 2,373.90 4 593.48 Friday 619.96 4 155.00 Saturday 1,549.94 3 516.65 Sunday 941.94 2 470.97"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple dimensions map a column for grouping: <code>region</code>, <code>category</code></li> <li>Column renaming lets dimension names differ from columns: <code>customer_city</code> \u2192 <code>city</code></li> <li>Hierarchies define drill-down paths: <code>year &gt; quarter &gt; month</code></li> <li>Dimensions can come from dimension tables or fact tables</li> <li>Complete config includes both metrics and dimensions</li> <li>Queries use dimensions with <code>BY</code>: <code>\"revenue BY region, category\"</code></li> </ul>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to execute queries against our semantic layer.</p> <p>Next: Querying Metrics</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#navigation","title":"Navigation","text":"Previous Up Next Defining Metrics Tutorials Querying Metrics"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#reference","title":"Reference","text":"<p>For complete documentation, see: Defining Metrics Reference</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/","title":"Querying Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to execute queries against the semantic layer using both the unified <code>Project</code> API and the <code>SemanticQuery</code> interface.</p> <p>What You'll Learn: - Unified Project API (recommended) - simplest approach - Simple queries (total, no grouping) - Queries with one dimension - Queries with multiple dimensions - Filtered queries - Multiple metrics together</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to query metrics is through the <code>Project</code> class:</p> <pre><code>from odibi import Project\n\n# Load project - semantic layer inherits connections\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics - tables auto-loaded from connections\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>The Project API: - Reads connections from your <code>odibi.yaml</code> - Resolves <code>source: $pipeline.node</code> or <code>connection.path</code> to full paths - Auto-loads Delta tables when queried - No manual <code>context.register()</code> calls needed</p> <p>Example queries:</p> <pre><code># Total revenue\nresult = project.query(\"revenue\")\n\n# Revenue by region\nresult = project.query(\"revenue BY region\")\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\n\n# With filter\nresult = project.query(\"revenue BY category WHERE region = 'North'\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#option-b-manual-semanticquery-interface","title":"Option B: Manual SemanticQuery Interface","text":"<p>For more control, use the <code>SemanticQuery</code> class directly.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#source-data","title":"Source Data","text":"<p>We'll use the star schema and semantic config from previous tutorials:</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#fact_orders-20-sample-rows","title":"fact_orders (20 sample rows)","text":"order_id customer_sk product_sk date_sk quantity line_total status ORD001 1 1 20240115 1 1299.99 completed ORD002 1 2 20240115 2 59.98 completed ORD003 2 3 20240116 1 249.99 completed ORD004 3 4 20240116 3 149.97 completed ORD005 4 5 20240117 1 599.99 completed ORD006 5 6 20240117 1 149.99 completed ORD007 6 7 20240118 2 799.98 completed ORD008 7 8 20240118 4 183.96 completed ORD009 8 9 20240119 1 79.99 completed ORD010 9 10 20240119 1 189.99 completed ORD011 10 1 20240120 1 1299.99 completed ORD012 11 2 20240120 5 149.95 completed ORD013 12 3 20240121 2 499.98 completed ORD014 1 4 20240121 1 49.99 completed ORD015 2 5 20240122 1 599.99 pending ORD016 3 6 20240122 2 299.98 completed ORD017 4 7 20240123 1 399.99 completed ORD018 5 8 20240123 3 137.97 completed ORD019 6 9 20240124 2 159.98 completed ORD020 7 10 20240124 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#dim_customer-12-rows","title":"dim_customer (12 rows)","text":"customer_sk name region city 1 Alice Johnson North Chicago 2 Bob Smith South Houston 3 Carol White North Detroit 4 David Brown East New York 5 Emma Davis West Seattle 6 Frank Miller South Miami 7 Grace Lee East Boston 8 Henry Wilson West Portland 9 Ivy Chen North Minneapolis 10 Jack Taylor South Dallas 11 Karen Martinez East Philadelphia 12 Leo Anderson West Denver"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#dim_product-10-rows","title":"dim_product (10 rows)","text":"product_sk name category 1 Laptop Pro 15 Electronics 2 Wireless Mouse Electronics 3 Office Chair Furniture 4 USB-C Hub Electronics 5 Standing Desk Furniture 6 Mechanical Keyboard Electronics 7 Monitor 27\" Electronics 8 Desk Lamp Furniture 9 Webcam HD Electronics 10 Filing Cabinet Furniture"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-1-simple-query-total-no-grouping","title":"Step 1: Simple Query - Total (No Grouping)","text":"<p>The simplest query returns a single aggregated value.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query","title":"Query","text":"<pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load config\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Create query interface\nquery = SemanticQuery(config)\n\n# Setup context with data\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\ncontext.register(\"dim_product\", dim_product_df)\ncontext.register(\"dim_date\", dim_date_df)\n\n# Execute query\nresult = query.execute(\"revenue\", context)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string","title":"Query String","text":"<pre><code>\"revenue\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql","title":"Generated SQL","text":"<pre><code>SELECT SUM(line_total) AS revenue\nFROM fact_orders\nWHERE status = 'completed'\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-1-row","title":"Result (1 row)","text":"revenue 8,953.56"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#accessing-the-result","title":"Accessing the Result","text":"<pre><code>print(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\n# Output: Total Revenue: $8,953.56\n\nprint(f\"Row count: {result.row_count}\")\n# Output: Row count: 1\n\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n# Output: Execution time: 12.34ms\n\nprint(f\"Generated SQL: {result.sql_generated}\")\n# Output: SELECT SUM(line_total) AS revenue FROM fact_orders WHERE status = 'completed'\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-2-query-with-one-dimension","title":"Step 2: Query with One Dimension","text":"<p>Add a dimension to group the results.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_1","title":"Query String","text":"<pre><code>\"revenue BY region\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_1","title":"Generated SQL","text":"<pre><code>SELECT \n    c.region,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE f.status = 'completed'\nGROUP BY c.region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue BY region\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-4-rows","title":"Result (4 rows)","text":"region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-3-query-with-multiple-dimensions","title":"Step 3: Query with Multiple Dimensions","text":"<p>Add more dimensions for a cross-tabulation.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_2","title":"Query String","text":"<pre><code>\"revenue, order_count BY region, category\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_2","title":"Generated SQL","text":"<pre><code>SELECT \n    c.region,\n    p.category,\n    SUM(f.line_total) AS revenue,\n    COUNT(*) AS order_count\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nWHERE f.status = 'completed'\nGROUP BY c.region, p.category\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_1","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue, order_count BY region, category\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-8-rows-region-category","title":"Result (8 rows - region \u00d7 category)","text":"region category revenue order_count North Electronics 1,549.94 4 North Furniture 999.94 3 South Electronics 1,449.95 4 South Furniture 899.98 3 East Electronics 1,323.91 4 East Furniture 599.97 3 West Electronics 1,079.93 3 West Furniture 1,049.94 4"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-4-query-with-filter","title":"Step 4: Query with Filter","text":"<p>Add a WHERE clause to filter results.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_3","title":"Query String","text":"<pre><code>\"revenue BY category WHERE region = 'North'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_3","title":"Generated SQL","text":"<pre><code>SELECT \n    p.category,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nWHERE f.status = 'completed'\n  AND c.region = 'North'\nGROUP BY p.category\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_2","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue BY category WHERE region = 'North'\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-2-rows","title":"Result (2 rows)","text":"category revenue Electronics 1,549.94 Furniture 999.94"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#multiple-filters","title":"Multiple Filters","text":"<p>You can combine multiple filter conditions:</p> <pre><code># Multiple conditions with AND\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North' AND year = 2024\",\n    context\n)\n\n# IN clause\nresult = query.execute(\n    \"revenue BY region WHERE region IN ('North', 'South')\",\n    context\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-5-multiple-metrics","title":"Step 5: Multiple Metrics","text":"<p>Query multiple metrics in a single call.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_4","title":"Query String","text":"<pre><code>\"revenue, order_count, avg_order_value BY region\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_3","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-4-rows_1","title":"Result (4 rows)","text":"region revenue order_count avg_order_value North 2,549.88 7 364.27 South 2,349.93 7 335.70 East 1,923.88 7 274.84 West 2,129.87 7 304.27"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#complete-python-script","title":"Complete Python Script","text":"<p>Here's a complete, runnable example:</p> <pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\n\n# ===========================================\n# 1. Load the semantic config\n# ===========================================\nconfig_dict = {\n    \"metrics\": [\n        {\n            \"name\": \"revenue\",\n            \"description\": \"Total revenue from completed orders\",\n            \"expr\": \"SUM(line_total)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"order_count\",\n            \"expr\": \"COUNT(*)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"avg_order_value\",\n            \"expr\": \"AVG(line_total)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        }\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"dim_customer\", \"column\": \"region\"},\n        {\"name\": \"category\", \"source\": \"dim_product\", \"column\": \"category\"},\n        {\"name\": \"month\", \"source\": \"dim_date\", \"column\": \"month_name\"}\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\nquery = SemanticQuery(config)\n\n# ===========================================\n# 2. Load data and create context\n# ===========================================\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# ===========================================\n# 3. Execute queries\n# ===========================================\n\n# Query 1: Total revenue\nprint(\"=\" * 50)\nprint(\"Query 1: Total Revenue\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\nprint()\n\n# Query 2: Revenue by region\nprint(\"=\" * 50)\nprint(\"Query 2: Revenue by Region\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 3: Multiple metrics by region\nprint(\"=\" * 50)\nprint(\"Query 3: Multiple Metrics by Region\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 4: Revenue by region and category\nprint(\"=\" * 50)\nprint(\"Query 4: Revenue by Region and Category\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue BY region, category\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 5: Filtered query\nprint(\"=\" * 50)\nprint(\"Query 5: North Region Only\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue, order_count BY category WHERE region = 'North'\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# ===========================================\n# 4. Show execution details\n# ===========================================\nprint(\"=\" * 50)\nprint(\"Execution Details (last query)\")\nprint(\"=\" * 50)\nprint(f\"Metrics: {result.metrics}\")\nprint(f\"Dimensions: {result.dimensions}\")\nprint(f\"Row count: {result.row_count}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#output","title":"Output","text":"<pre><code>==================================================\nQuery 1: Total Revenue\n==================================================\nTotal Revenue: $8,953.56\n\n==================================================\nQuery 2: Revenue by Region\n==================================================\n region   revenue\n  North  2549.88\n  South  2349.93\n   East  1923.88\n   West  2129.87\n\n==================================================\nQuery 3: Multiple Metrics by Region\n==================================================\n region   revenue  order_count  avg_order_value\n  North  2549.88            7           364.27\n  South  2349.93            7           335.70\n   East  1923.88            7           274.84\n   West  2129.87            7           304.27\n\n==================================================\nQuery 4: Revenue by Region and Category\n==================================================\n region     category   revenue\n  North  Electronics  1549.94\n  North    Furniture   999.94\n  South  Electronics  1449.95\n  South    Furniture   899.98\n   East  Electronics  1323.91\n   East    Furniture   599.97\n   West  Electronics  1079.93\n   West    Furniture  1049.94\n\n==================================================\nQuery 5: North Region Only\n==================================================\n    category   revenue  order_count\n Electronics  1549.94            4\n   Furniture   999.94            3\n\n==================================================\nExecution Details (last query)\n==================================================\nMetrics: ['revenue', 'order_count']\nDimensions: ['category']\nRow count: 2\nExecution time: 8.45ms\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-syntax-reference","title":"Query Syntax Reference","text":"Pattern Example Description Single metric <code>\"revenue\"</code> Total, no grouping Metric + dimension <code>\"revenue BY region\"</code> Grouped by one dimension Multiple metrics <code>\"revenue, order_count\"</code> Multiple metrics together Multiple dimensions <code>\"revenue BY region, category\"</code> Cross-tabulation With filter <code>\"revenue BY region WHERE year = 2024\"</code> Filtered results Complex filter <code>\"revenue BY region WHERE region IN ('North', 'South')\"</code> IN clause"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#error-handling","title":"Error Handling","text":"<pre><code># Invalid metric\ntry:\n    result = query.execute(\"invalid_metric BY region\", context)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count', 'avg_order_value']\n\n# Invalid dimension\ntry:\n    result = query.execute(\"revenue BY invalid_dimension\", context)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Unknown dimension 'invalid_dimension'. Available: ['region', 'category', 'month']\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple queries return totals: <code>\"revenue\"</code></li> <li>One dimension groups results: <code>\"revenue BY region\"</code></li> <li>Multiple dimensions create cross-tabs: <code>\"revenue BY region, category\"</code></li> <li>Filters constrain results: <code>\"revenue BY region WHERE year = 2024\"</code></li> <li>Multiple metrics can be queried together: <code>\"revenue, order_count BY region\"</code></li> <li>QueryResult contains the DataFrame, metrics, dimensions, and execution info</li> </ul>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to pre-compute metrics for dashboard performance.</p> <p>Next: Materializing Metrics</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#navigation","title":"Navigation","text":"Previous Up Next Defining Dimensions Tutorials Materializing Metrics"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Querying Reference</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/","title":"Materializing Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to pre-compute and persist metrics using the <code>Materializer</code> class. Materialization creates pre-aggregated tables for fast dashboard performance.</p> <p>What You'll Learn: - Why materialize metrics - Defining materializations - Executing materializations - Scheduling with cron - Incremental strategies (replace vs sum)</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#unified-project-api-note","title":"Unified Project API Note","text":"<p>When using the unified <code>Project</code> API, materializations are defined in the <code>semantic</code> section of your <code>odibi.yaml</code>. Sources can reference pipeline nodes directly:</p> <pre><code># odibi.yaml\nproject: retail_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n      - name: dim_date\n        write: { connection: gold, table: dim_date }\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters: [\"status = 'completed'\"]\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n    - name: month\n      source: $build_warehouse.dim_date\n      column: month_name\n\n  materializations:\n    - name: monthly_revenue_by_region\n      metrics: [revenue]\n      dimensions: [region, month]\n      output: gold/agg_monthly_revenue\n      schedule: \"0 2 1 * *\"\n</code></pre> <p>The <code>$pipeline.node</code> notation automatically reads from wherever the node writes. For full control over materialization execution, continue with the <code>Materializer</code> class below.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#why-materialize","title":"Why Materialize?","text":"<p>Ad-hoc queries are powerful but slow for dashboards:</p> Approach Query Time Use Case Ad-hoc query 5-30 seconds Exploratory analysis Materialized table 0.1-0.5 seconds Production dashboards <p>Materialization pre-computes metrics at a specific grain and saves them to a table. Dashboards query the pre-computed table instead of raw data.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#semantic-config-with-materializations","title":"Semantic Config with Materializations","text":"<p>Let's extend our semantic config to include materializations:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># semantic_config.yaml\nmetrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: unique_customers\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: avg_order_value\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\ndimensions:\n  - name: region\n    source: dim_customer\n    column: region\n\n  - name: category\n    source: dim_product\n    column: category\n\n  - name: month\n    source: dim_date\n    column: month_name\n\n  - name: date_sk\n    source: dim_date\n    column: date_sk\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue_region\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n\n  - name: daily_revenue\n    metrics: [revenue, order_count, unique_customers]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    schedule: \"0 3 * * *\"  # 3am daily\n\n  - name: category_summary\n    metrics: [revenue, order_count, avg_order_value]\n    dimensions: [category]\n    output: gold/agg_category_summary\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-1-define-a-materialization","title":"Step 1: Define a Materialization","text":"<p>A materialization specifies which metrics and dimensions to pre-compute:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import MaterializationConfig\n\n# Define a materialization\nmonthly_revenue = MaterializationConfig(\n    name=\"monthly_revenue_by_region\",\n    metrics=[\"revenue\", \"order_count\"],\n    dimensions=[\"region\", \"month\"],\n    output=\"gold/agg_monthly_revenue_region\",\n    schedule=\"0 2 1 * *\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"monthly_revenue_by_region\"</code> Unique identifier <code>metrics</code> <code>[\"revenue\", \"order_count\"]</code> Which metrics to compute <code>dimensions</code> <code>[\"region\", \"month\"]</code> Grain (GROUP BY) <code>output</code> <code>\"gold/agg_monthly_revenue_region\"</code> Output table path <code>schedule</code> <code>\"0 2 1 * *\"</code> Cron schedule (optional)"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#what-gets-generated","title":"What Gets Generated","text":"<p>The materialization creates a table with: - One row per unique combination of <code>region</code> \u00d7 <code>month</code> - Columns for each metric (<code>revenue</code>, <code>order_count</code>)</p> <p>Output Table (12 rows - 4 regions \u00d7 3 months):</p> region month revenue order_count North January 2,549.88 7 North February 3,120.50 9 North March 2,890.25 8 South January 2,349.93 7 South February 2,780.40 8 South March 3,050.75 9 East January 1,923.88 7 East February 2,450.60 7 East March 2,180.35 6 West January 2,129.87 7 West February 2,890.45 8 West March 2,650.90 7"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-2-execute-a-materialization","title":"Step 2: Execute a Materialization","text":"<p>Use the <code>Materializer</code> class to execute:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_1","title":"Python Code","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load config\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Create context\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\ncontext.register(\"dim_product\", dim_product_df)\ncontext.register(\"dim_date\", dim_date_df)\n\n# Create materializer\nmaterializer = Materializer(config)\n\n# Execute single materialization\nresult = materializer.execute(\"monthly_revenue_by_region\", context)\n\n# Check result\nprint(f\"Name: {result.name}\")\nprint(f\"Output: {result.output}\")\nprint(f\"Row count: {result.row_count}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#materializationresult","title":"MaterializationResult","text":"Field Type Description <code>name</code> str Materialization name <code>output</code> str Output table path <code>row_count</code> int Number of rows generated <code>elapsed_ms</code> float Execution time in ms <code>success</code> bool Whether it succeeded <code>error</code> str Error message (if failed) <code>df</code> DataFrame The computed data"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-3-write-the-output","title":"Step 3: Write the Output","text":"<p>Use a callback to write the materialized data:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_2","title":"Python Code","text":"<pre><code># Define write callback\ndef write_to_parquet(df, output_path):\n    \"\"\"Write DataFrame to Parquet.\"\"\"\n    full_path = f\"warehouse/{output_path}.parquet\"\n    df.to_parquet(full_path, index=False)\n    print(f\"Wrote {len(df)} rows to {full_path}\")\n\n# Execute with write callback\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_to_parquet\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#spark-example","title":"Spark Example","text":"<pre><code>def write_to_delta(df, output_path):\n    \"\"\"Write Spark DataFrame to Delta Lake.\"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/warehouse/{output_path}\")\n\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_to_delta\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-4-understanding-schedules","title":"Step 4: Understanding Schedules","text":"<p>The <code>schedule</code> field uses cron syntax:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0-59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0-23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 day of month (1-31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500 month (1-12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500 day of week (0-6, Sun=0)\n\u2502 \u2502 \u2502 \u2502 \u2502\n* * * * *\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#common-schedules","title":"Common Schedules","text":"Schedule Cron When Daily at 2am <code>0 2 * * *</code> Every day at 2:00 AM Monthly at 2am <code>0 2 1 * *</code> 1st of month at 2:00 AM Weekly Sunday <code>0 3 * * 0</code> Sunday at 3:00 AM Hourly <code>0 * * * *</code> Every hour on the hour"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#reading-schedules","title":"Reading Schedules","text":"<pre><code># Get schedule for a materialization\nmat_config = materializer.get_materialization(\"monthly_revenue_by_region\")\nprint(f\"Schedule: {mat_config.schedule}\")\n# Output: Schedule: 0 2 1 * *\n\n# List all materializations with schedules\nfor mat in materializer.list_materializations():\n    print(f\"{mat['name']}: {mat['schedule'] or 'No schedule'}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-5-execute-all-materializations","title":"Step 5: Execute All Materializations","text":"<p>Execute all defined materializations at once:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_3","title":"Python Code","text":"<pre><code># Execute all materializations\nresults = materializer.execute_all(context, write_callback=write_to_parquet)\n\n# Print summary\nprint(\"=\" * 60)\nprint(\"Materialization Summary\")\nprint(\"=\" * 60)\nfor result in results:\n    status = \"SUCCESS\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status}\")\n    if result.success:\n        print(f\"    Rows: {result.row_count}, Time: {result.elapsed_ms:.0f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#output","title":"Output","text":"<pre><code>============================================================\nMaterialization Summary\n============================================================\n  monthly_revenue_by_region: SUCCESS\n    Rows: 12, Time: 45ms\n  daily_revenue: SUCCESS\n    Rows: 14, Time: 32ms\n  category_summary: SUCCESS\n    Rows: 2, Time: 18ms\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-6-incremental-materialization","title":"Step 6: Incremental Materialization","text":"<p>For large datasets, use incremental updates instead of full rebuilds.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#replace-strategy","title":"Replace Strategy","text":"<p>New data replaces existing rows for matching grain keys:</p> <pre><code>materializations:\n  - name: daily_revenue\n    metrics: [revenue, order_count]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    incremental:\n      timestamp_column: load_timestamp\n      merge_strategy: replace\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#how-replace-works","title":"How Replace Works","text":"<p>Existing Table:</p> date_sk revenue order_count 20240115 1,439.96 3 20240116 589.95 3 20240117 749.98 2 <p>New Data Arrives (late order for Jan 15):</p> date_sk revenue order_count 20240115 1,539.96 4 <p>After Replace Merge:</p> date_sk revenue order_count Note 20240115 1,539.96 4 Replaced 20240116 589.95 3 Unchanged 20240117 749.98 2 Unchanged"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#sum-strategy-use-with-caution","title":"Sum Strategy (Use with Caution)","text":"<p>New measure values add to existing values:</p> <pre><code>materializations:\n  - name: daily_order_count\n    metrics: [order_count]  # Only COUNT metrics!\n    dimensions: [date_sk]\n    output: gold/agg_daily_count\n    incremental:\n      timestamp_column: created_at\n      merge_strategy: sum\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#how-sum-works","title":"How Sum Works","text":"<p>Existing Table:</p> date_sk order_count 20240115 3 20240116 3 <p>New Orders (2 new orders on Jan 15):</p> date_sk order_count 20240115 2 <p>After Sum Merge:</p> date_sk order_count Note 20240115 5 3 + 2 = 5 20240116 3 Unchanged"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#when-not-to-use-sum","title":"When NOT to Use Sum","text":"<p>Never use sum for: - <code>AVG()</code> - Would become average of averages - <code>COUNT(DISTINCT)</code> - Would overcount - <code>MIN()</code> / <code>MAX()</code> - Would be wrong - Data with corrections/updates</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#complete-python-example","title":"Complete Python Example","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\n\n# ===========================================\n# 1. Load config with materializations\n# ===========================================\nconfig_dict = {\n    \"metrics\": [\n        {\"name\": \"revenue\", \"expr\": \"SUM(line_total)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]},\n        {\"name\": \"order_count\", \"expr\": \"COUNT(*)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]},\n        {\"name\": \"avg_order_value\", \"expr\": \"AVG(line_total)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]}\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"dim_customer\", \"column\": \"region\"},\n        {\"name\": \"category\", \"source\": \"dim_product\", \"column\": \"category\"},\n        {\"name\": \"date_sk\", \"source\": \"dim_date\", \"column\": \"date_sk\"}\n    ],\n    \"materializations\": [\n        {\n            \"name\": \"daily_summary\",\n            \"metrics\": [\"revenue\", \"order_count\"],\n            \"dimensions\": [\"date_sk\"],\n            \"output\": \"gold/agg_daily_summary\",\n            \"schedule\": \"0 3 * * *\"\n        },\n        {\n            \"name\": \"region_summary\",\n            \"metrics\": [\"revenue\", \"order_count\", \"avg_order_value\"],\n            \"dimensions\": [\"region\"],\n            \"output\": \"gold/agg_region_summary\"\n        },\n        {\n            \"name\": \"category_by_region\",\n            \"metrics\": [\"revenue\", \"order_count\"],\n            \"dimensions\": [\"category\", \"region\"],\n            \"output\": \"gold/agg_category_region\"\n        }\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\n\n# ===========================================\n# 2. Setup context with data\n# ===========================================\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", pd.read_parquet(\"warehouse/fact_orders\"))\ncontext.register(\"dim_customer\", pd.read_parquet(\"warehouse/dim_customer\"))\ncontext.register(\"dim_product\", pd.read_parquet(\"warehouse/dim_product\"))\ncontext.register(\"dim_date\", pd.read_parquet(\"warehouse/dim_date\"))\n\n# ===========================================\n# 3. Define write callback\n# ===========================================\ndef write_output(df, output_path):\n    full_path = f\"warehouse/{output_path}.parquet\"\n    df.to_parquet(full_path, index=False)\n    print(f\"  \u2192 Wrote {len(df)} rows to {full_path}\")\n\n# ===========================================\n# 4. Execute all materializations\n# ===========================================\nmaterializer = Materializer(config)\n\nprint(\"=\" * 60)\nprint(\"Executing Materializations\")\nprint(\"=\" * 60)\n\nresults = materializer.execute_all(context, write_callback=write_output)\n\n# ===========================================\n# 5. Show results\n# ===========================================\nprint()\nprint(\"=\" * 60)\nprint(\"Results Summary\")\nprint(\"=\" * 60)\n\nfor result in results:\n    status = \"\u2713 SUCCESS\" if result.success else f\"\u2717 FAILED: {result.error}\"\n    print(f\"\\n{result.name}:\")\n    print(f\"  Status: {status}\")\n    print(f\"  Output: {result.output}\")\n    print(f\"  Rows: {result.row_count}\")\n    print(f\"  Time: {result.elapsed_ms:.0f}ms\")\n\n    # Show sample data\n    if result.success and result.df is not None:\n        print(f\"  Sample data:\")\n        print(result.df.head(5).to_string(index=False))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#output_1","title":"Output","text":"<pre><code>============================================================\nExecuting Materializations\n============================================================\n  \u2192 Wrote 14 rows to warehouse/gold/agg_daily_summary.parquet\n  \u2192 Wrote 4 rows to warehouse/gold/agg_region_summary.parquet\n  \u2192 Wrote 8 rows to warehouse/gold/agg_category_region.parquet\n\n============================================================\nResults Summary\n============================================================\n\ndaily_summary:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_daily_summary\n  Rows: 14\n  Time: 42ms\n  Sample data:\n   date_sk   revenue  order_count\n  20240115  1439.96            3\n  20240116   589.95            3\n  20240117   749.98            2\n  20240118   983.94            2\n  20240119   269.98            2\n\nregion_summary:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_region_summary\n  Rows: 4\n  Time: 28ms\n  Sample data:\n  region   revenue  order_count  avg_order_value\n   North  2549.88            7           364.27\n   South  2349.93            7           335.70\n    East  1923.88            7           274.84\n    West  2129.87            7           304.27\n\ncategory_by_region:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_category_region\n  Rows: 8\n  Time: 35ms\n  Sample data:\n     category  region   revenue  order_count\n  Electronics   North  1549.94            4\n  Electronics   South  1449.95            4\n  Electronics    East  1323.91            4\n  Electronics    West  1079.93            3\n    Furniture   North   999.94            3\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Materialization pre-computes metrics for fast dashboard queries</li> <li>MaterializationConfig specifies metrics, dimensions, output, and schedule</li> <li>Materializer.execute() runs a single materialization</li> <li>Materializer.execute_all() runs all configured materializations</li> <li>Schedules use cron syntax: <code>\"0 2 * * *\"</code> (daily at 2am)</li> <li>Replace strategy overwrites matching grain keys (recommended)</li> <li>Sum strategy adds to existing values (use with caution)</li> </ul>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's put everything together in a complete semantic layer example.</p> <p>Next: Semantic Full Example</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#navigation","title":"Navigation","text":"Previous Up Next Querying Metrics Tutorials Semantic Full Example"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Materializing Reference</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/","title":"Semantic Layer Full Example","text":"<p>This tutorial brings together everything you've learned about the semantic layer into a complete, end-to-end example.</p> <p>What You'll See: - Complete semantic config (5 metrics, 5 dimensions, 3 materializations) - Unified Project API (simplest approach) - Full Python script that loads data, queries metrics, and materializes results - All output tables with sample data</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is with the unified <code>Project</code> API:</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#odibiyaml-with-semantic-layer","title":"odibi.yaml with Semantic Layer","text":"<pre><code># odibi.yaml\nproject: retail_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n      - name: dim_product\n        write: { connection: gold, table: dim_product }\n      - name: dim_date\n        write: { connection: gold, table: dim_date }\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      description: \"Total revenue from completed orders\"\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n      source: $build_warehouse.fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n      source: $build_warehouse.fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n\n    - name: category\n      source: $build_warehouse.dim_product\n      column: category\n\n    - name: month\n      source: $build_warehouse.dim_date\n      column: month_name\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#query-with-two-lines","title":"Query with Two Lines","text":"<pre><code>from odibi import Project\n\n# Load project - tables auto-resolved from connections\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count, avg_order_value BY region, category\")\nprint(result.df)\n</code></pre> <p>That's it! No manual table loading or context registration required.</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#option-b-manual-approach","title":"Option B: Manual Approach","text":"<p>For more control, use the semantic layer components directly.</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#the-complete-semantic-configuration","title":"The Complete Semantic Configuration","text":""},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#semantic_configyaml","title":"semantic_config.yaml","text":"<pre><code># File: semantic_config.yaml\n# Complete semantic layer configuration for retail star schema\n\nmetrics:\n  # Revenue metrics\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: cost\n    description: \"Total cost of goods sold\"\n    expr: \"SUM(quantity * 0.6 * unit_price)\"  # Assume 60% cost ratio\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit\n    description: \"Gross profit (revenue - cost)\"\n    expr: \"SUM(line_total * 0.4)\"  # 40% margin\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Volume metrics\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\ndimensions:\n  # Geographic dimensions\n  - name: region\n    description: \"Customer geographic region\"\n    source: dim_customer\n    column: region\n\n  - name: category\n    description: \"Product category\"\n    source: dim_product\n    column: category\n\n  # Time dimensions\n  - name: month\n    description: \"Month name\"\n    source: dim_date\n    column: month_name\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n\n  - name: quarter\n    description: \"Quarter name\"\n    source: dim_date\n    column: quarter_name\n\n  - name: year\n    description: \"Calendar year\"\n    source: dim_date\n    column: year\n\nmaterializations:\n  # Daily aggregate for trend analysis\n  - name: daily_metrics\n    description: \"Daily revenue and order metrics\"\n    metrics:\n      - revenue\n      - order_count\n      - avg_order_value\n    dimensions:\n      - year\n      - month\n    output: gold/agg_daily_metrics\n    schedule: \"0 3 * * *\"  # 3am daily\n\n  # Monthly by region for regional dashboards\n  - name: monthly_by_region\n    description: \"Monthly metrics by region\"\n    metrics:\n      - revenue\n      - profit\n      - order_count\n    dimensions:\n      - region\n      - month\n    output: gold/agg_monthly_region\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n\n  # Category performance summary\n  - name: category_performance\n    description: \"Category performance metrics\"\n    metrics:\n      - revenue\n      - profit\n      - order_count\n      - avg_order_value\n    dimensions:\n      - category\n    output: gold/agg_category_performance\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#complete-python-script","title":"Complete Python Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete Semantic Layer Example\n\nThis script demonstrates the full workflow:\n1. Load star schema data\n2. Configure semantic layer\n3. Run queries\n4. Materialize metrics\n\"\"\"\n\nfrom odibi.semantics import (\n    SemanticQuery,\n    Materializer,\n    SemanticLayerConfig,\n    MetricDefinition,\n    DimensionDefinition,\n    MaterializationConfig,\n    parse_semantic_config\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\nfrom pathlib import Path\n\n# =============================================================================\n# 1. LOAD THE STAR SCHEMA DATA\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 1: Loading Star Schema Data\")\nprint(\"=\" * 70)\n\n# Load dimension tables\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\n\nprint(f\"dim_customer: {len(dim_customer)} rows\")\nprint(f\"dim_product:  {len(dim_product)} rows\")\nprint(f\"dim_date:     {len(dim_date)} rows\")\nprint(f\"fact_orders:  {len(fact_orders)} rows\")\nprint()\n\n# =============================================================================\n# 2. CREATE SEMANTIC LAYER CONFIG\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 2: Creating Semantic Layer Configuration\")\nprint(\"=\" * 70)\n\n# Option A: Load from YAML file\n# with open(\"semantic_config.yaml\") as f:\n#     config = parse_semantic_config(yaml.safe_load(f))\n\n# Option B: Build programmatically\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"cost\",\n            description=\"Total cost of goods sold\",\n            expr=\"SUM(quantity * 0.6 * unit_price)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"profit\",\n            description=\"Gross profit (revenue - cost)\",\n            expr=\"SUM(line_total * 0.4)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[\n        DimensionDefinition(name=\"region\", source=\"dim_customer\", column=\"region\"),\n        DimensionDefinition(name=\"category\", source=\"dim_product\", column=\"category\"),\n        DimensionDefinition(name=\"month\", source=\"dim_date\", column=\"month_name\",\n                           hierarchy=[\"year\", \"quarter_name\", \"month_name\"]),\n        DimensionDefinition(name=\"quarter\", source=\"dim_date\", column=\"quarter_name\"),\n        DimensionDefinition(name=\"year\", source=\"dim_date\", column=\"year\")\n    ],\n    materializations=[\n        MaterializationConfig(\n            name=\"daily_metrics\",\n            metrics=[\"revenue\", \"order_count\", \"avg_order_value\"],\n            dimensions=[\"year\", \"month\"],\n            output=\"gold/agg_daily_metrics\",\n            schedule=\"0 3 * * *\"\n        ),\n        MaterializationConfig(\n            name=\"monthly_by_region\",\n            metrics=[\"revenue\", \"profit\", \"order_count\"],\n            dimensions=[\"region\", \"month\"],\n            output=\"gold/agg_monthly_region\",\n            schedule=\"0 2 1 * *\"\n        ),\n        MaterializationConfig(\n            name=\"category_performance\",\n            metrics=[\"revenue\", \"profit\", \"order_count\", \"avg_order_value\"],\n            dimensions=[\"category\"],\n            output=\"gold/agg_category_performance\"\n        )\n    ]\n)\n\nprint(f\"Metrics defined:          {len(config.metrics)}\")\nprint(f\"Dimensions defined:       {len(config.dimensions)}\")\nprint(f\"Materializations defined: {len(config.materializations)}\")\n\n# List them\nprint(\"\\nMetrics:\")\nfor m in config.metrics:\n    print(f\"  - {m.name}: {m.description}\")\n\nprint(\"\\nDimensions:\")\nfor d in config.dimensions:\n    print(f\"  - {d.name}: from {d.source}.{d.column}\")\n\nprint(\"\\nMaterializations:\")\nfor mat in config.materializations:\n    print(f\"  - {mat.name}: {mat.metrics} BY {mat.dimensions}\")\nprint()\n\n# =============================================================================\n# 3. SETUP CONTEXT\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 3: Setting Up Engine Context\")\nprint(\"=\" * 70)\n\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\nprint(\"Registered tables: fact_orders, dim_customer, dim_product, dim_date\")\nprint()\n\n# =============================================================================\n# 4. RUN QUERIES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 4: Running Semantic Queries\")\nprint(\"=\" * 70)\n\nquery = SemanticQuery(config)\n\n# Query 1: Total revenue (no grouping)\nprint(\"\\n--- Query 1: Total Revenue ---\")\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n\n# Query 2: Revenue by region\nprint(\"\\n--- Query 2: Revenue by Region ---\")\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 3: Multiple metrics by region\nprint(\"\\n--- Query 3: Revenue, Profit, Order Count by Region ---\")\nresult = query.execute(\"revenue, profit, order_count BY region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 4: Revenue by category and region\nprint(\"\\n--- Query 4: Revenue by Category and Region ---\")\nresult = query.execute(\"revenue BY category, region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 5: Filtered query - North region only\nprint(\"\\n--- Query 5: North Region Performance ---\")\nresult = query.execute(\"revenue, profit, avg_order_value BY category WHERE region = 'North'\", context)\nprint(result.df.to_string(index=False))\n\nprint()\n\n# =============================================================================\n# 5. MATERIALIZE METRICS\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 5: Materializing Metrics\")\nprint(\"=\" * 70)\n\nmaterializer = Materializer(config)\n\n# Track outputs for display\noutput_tables = {}\n\ndef write_and_store(df, output_path):\n    \"\"\"Write to disk and store for display.\"\"\"\n    output_tables[output_path] = df.copy()\n    full_path = Path(f\"warehouse/{output_path}.parquet\")\n    full_path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_parquet(full_path, index=False)\n    print(f\"  \u2192 Wrote {len(df)} rows to {full_path}\")\n\n# Execute all materializations\nresults = materializer.execute_all(context, write_callback=write_and_store)\n\nprint(\"\\nMaterialization Results:\")\nfor result in results:\n    status = \"SUCCESS\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status} ({result.row_count} rows, {result.elapsed_ms:.0f}ms)\")\n\nprint()\n\n# =============================================================================\n# 6. SHOW OUTPUT TABLES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 6: Output Tables\")\nprint(\"=\" * 70)\n\nfor output_path, df in output_tables.items():\n    print(f\"\\n--- {output_path} ({len(df)} rows) ---\")\n    print(df.to_string(index=False))\n\nprint()\nprint(\"=\" * 70)\nprint(\"COMPLETE!\")\nprint(\"=\" * 70)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#sample-output","title":"Sample Output","text":""},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-1-data-loading","title":"Step 1: Data Loading","text":"<pre><code>======================================================================\nSTEP 1: Loading Star Schema Data\n======================================================================\ndim_customer: 13 rows\ndim_product:  11 rows\ndim_date:     32 rows\nfact_orders:  30 rows\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-4-query-results","title":"Step 4: Query Results","text":"<p>Query 1: Total Revenue</p> revenue 8,953.56 <p>Query 2: Revenue by Region (4 rows)</p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87 <p>Query 3: Multiple Metrics by Region (4 rows)</p> region revenue profit order_count North 2,549.88 1,019.95 7 South 2,349.93 939.97 7 East 1,923.88 769.55 7 West 2,129.87 851.95 7 <p>Query 4: Revenue by Category and Region (8 rows)</p> category region revenue Electronics North 1,549.94 Electronics South 1,449.95 Electronics East 1,323.91 Electronics West 1,079.93 Furniture North 999.94 Furniture South 899.98 Furniture East 599.97 Furniture West 1,049.94 <p>Query 5: North Region Only (2 rows)</p> category revenue profit avg_order_value Electronics 1,549.94 619.98 387.49 Furniture 999.94 399.98 333.31"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-5-materialization-results","title":"Step 5: Materialization Results","text":"<pre><code>======================================================================\nSTEP 5: Materializing Metrics\n======================================================================\n  \u2192 Wrote 1 rows to warehouse/gold/agg_daily_metrics.parquet\n  \u2192 Wrote 4 rows to warehouse/gold/agg_monthly_region.parquet\n  \u2192 Wrote 2 rows to warehouse/gold/agg_category_performance.parquet\n\nMaterialization Results:\n  daily_metrics: SUCCESS (1 rows, 32ms)\n  monthly_by_region: SUCCESS (4 rows, 45ms)\n  category_performance: SUCCESS (2 rows, 28ms)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-6-output-tables","title":"Step 6: Output Tables","text":"<p>gold/agg_daily_metrics (1 row)</p> year month revenue order_count avg_order_value 2024 January 8,953.56 27 331.61 <p>gold/agg_monthly_region (4 rows)</p> region month revenue profit order_count North January 2,549.88 1,019.95 7 South January 2,349.93 939.97 7 East January 1,923.88 769.55 7 West January 2,129.87 851.95 7 <p>gold/agg_category_performance (2 rows)</p> category revenue profit order_count avg_order_value Electronics 5,403.73 2,161.49 15 360.25 Furniture 3,549.83 1,419.93 12 295.82"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>flowchart TB\n    subgraph StarSchema[\"Star Schema (Built by Pipelines)\"]\n        DC[(dim_customer&lt;br/&gt;13 rows)]\n        DP[(dim_product&lt;br/&gt;11 rows)]\n        DD[(dim_date&lt;br/&gt;32 rows)]\n        FO[(fact_orders&lt;br/&gt;30 rows)]\n    end\n\n    subgraph SemanticLayer[\"Semantic Layer\"]\n        subgraph Metrics[\"5 Metrics\"]\n            M1[revenue]\n            M2[cost]\n            M3[profit]\n            M4[order_count]\n            M5[avg_order_value]\n        end\n\n        subgraph Dims[\"5 Dimensions\"]\n            D1[region]\n            D2[category]\n            D3[month]\n            D4[quarter]\n            D5[year]\n        end\n\n        subgraph Mats[\"3 Materializations\"]\n            MAT1[daily_metrics]\n            MAT2[monthly_by_region]\n            MAT3[category_performance]\n        end\n    end\n\n    subgraph Output[\"Materialized Tables\"]\n        O1[(agg_daily_metrics&lt;br/&gt;1 row)]\n        O2[(agg_monthly_region&lt;br/&gt;4 rows)]\n        O3[(agg_category_performance&lt;br/&gt;2 rows)]\n    end\n\n    DC --&gt; D1\n    DP --&gt; D2\n    DD --&gt; D3\n    DD --&gt; D4\n    DD --&gt; D5\n    FO --&gt; M1\n    FO --&gt; M2\n    FO --&gt; M3\n    FO --&gt; M4\n    FO --&gt; M5\n\n    MAT1 --&gt; O1\n    MAT2 --&gt; O2\n    MAT3 --&gt; O3\n\n    style StarSchema fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style SemanticLayer fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Output fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#summary","title":"Summary","text":"<p>In this complete example, you saw:</p> <ol> <li>5 Metrics: revenue, cost, profit, order_count, avg_order_value</li> <li>5 Dimensions: region, category, month, quarter, year</li> <li>3 Materializations: daily_metrics, monthly_by_region, category_performance</li> <li>5 Queries: Total, by region, multiple metrics, cross-tab, filtered</li> </ol> <p>The semantic layer provides: - Consistent definitions across all queries - Business-friendly syntax: <code>\"revenue BY region\"</code> - Pre-computed aggregates for fast dashboards - Self-documenting config with descriptions</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial series (Part 2: Semantic Layer), you learned:</p> <ol> <li>What a semantic layer is and why it matters</li> <li>Defining metrics with expressions and filters</li> <li>Defining dimensions for grouping and drill-down</li> <li>Querying with the simple <code>\"metric BY dimension\"</code> syntax</li> <li>Materializing metrics for dashboard performance</li> <li>Putting it all together in a production-ready configuration</li> </ol>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#next-steps","title":"Next Steps","text":"<p>The final tutorial covers FK validation for data quality.</p> <p>Next: FK Validation</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#navigation","title":"Navigation","text":"Previous Up Next Materializing Metrics Tutorials FK Validation"},{"location":"tutorials/dimensional_modeling/13_fk_validation/","title":"FK Validation Tutorial","text":"<p>In this tutorial, you'll learn how to validate foreign key relationships between fact and dimension tables to ensure data quality.</p> <p>What You'll Learn: - Why validate foreign keys - Defining relationships - Detecting orphan records - Handling orphans with different strategies - Generating lineage diagrams</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#why-validate-foreign-keys","title":"Why Validate Foreign Keys?","text":"<p>In a star schema, fact tables reference dimension tables via foreign keys. But what happens when: - A customer places an order, but the customer isn't in <code>dim_customer</code>? - An order references a product that was never loaded? - A date value doesn't exist in the date dimension?</p> <p>These are called orphan records\u2014facts that reference non-existent dimensions.</p> <p>Problems with orphans: - Reports show \"Unknown\" values - Counts and sums may be incorrect - Data quality issues go undetected - Downstream analytics are unreliable</p> <p>FK validation helps you detect, report, and handle these issues.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#source-data","title":"Source Data","text":"<p>We'll use the fact_orders table with some intentional orphan records:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#fact_orders-35-rows-including-5-orphans","title":"fact_orders (35 rows including 5 orphans)","text":"order_id customer_sk product_sk date_sk line_total status ORD001 1 1 20240115 1299.99 completed ORD002 1 2 20240115 59.98 completed ... ... ... ... ... ... ORD030 5 10 20240116 189.99 completed ORD031 99 1 20240117 1299.99 completed ORD032 88 2 20240118 29.99 completed ORD033 77 3 20240119 249.99 completed ORD034 66 4 20240120 49.99 completed ORD035 55 5 20240121 599.99 completed <p>Orphan records: ORD031-ORD035 have customer_sk values (99, 88, 77, 66, 55) that don't exist in dim_customer.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#dim_customer-13-rows","title":"dim_customer (13 rows)","text":"customer_sk customer_id name 0 -1 Unknown 1 C001 Alice Johnson 2 C002 Bob Smith ... ... ... 12 C012 Leo Anderson <p>Note: customer_sk values 55, 66, 77, 88, 99 do NOT exist.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-1-define-relationships","title":"Step 1: Define Relationships","text":"<p>First, declare the FK relationships in your star schema:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code","title":"Python Code","text":"<pre><code>from odibi.validation.fk import RelationshipConfig, RelationshipRegistry\n\n# Define all FK relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_dates\",\n        fact=\"fact_orders\",\n        dimension=\"dim_date\",\n        fact_key=\"date_sk\",\n        dimension_key=\"date_sk\",\n        nullable=True,  # Pending orders may not have dates\n        on_violation=\"warn\"\n    )\n]\n\n# Create registry\nregistry = RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#yaml-alternative","title":"YAML Alternative","text":"<pre><code># relationships.yaml\nrelationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_dates\n    fact: fact_orders\n    dimension: dim_date\n    fact_key: date_sk\n    dimension_key: date_sk\n    nullable: true\n    on_violation: warn\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#relationshipconfig-fields","title":"RelationshipConfig Fields","text":"Field Type Required Description <code>name</code> str Yes Unique identifier <code>fact</code> str Yes Fact table name <code>dimension</code> str Yes Dimension table name <code>fact_key</code> str Yes FK column in fact <code>dimension_key</code> str Yes PK/SK column in dimension <code>nullable</code> bool No Whether nulls are allowed (default: false) <code>on_violation</code> str No Action: \"error\", \"warn\", \"filter\" (default: \"error\")"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-2-validate-a-clean-fact-table","title":"Step 2: Validate a Clean Fact Table","text":"<p>Let's first validate a fact table with NO orphans:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_1","title":"Python Code","text":"<pre><code>from odibi.validation.fk import FKValidator\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load data\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")  # Clean data - 30 rows\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# Create validator\nvalidator = FKValidator(registry)\n\n# Validate\nreport = validator.validate_fact(fact_orders, \"fact_orders\", context)\n\n# Check results\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Total relationships: {report.total_relationships}\")\nprint(f\"Valid relationships: {report.valid_relationships}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#validation-report-clean-data","title":"Validation Report (Clean Data)","text":"<pre><code>All valid: True\nTotal relationships: 3\nValid relationships: 3\n\nRelationship: orders_to_customers\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n\nRelationship: orders_to_products\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n\nRelationship: orders_to_dates\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-3-validate-with-orphan-records","title":"Step 3: Validate with Orphan Records","text":"<p>Now let's validate the data with 5 orphan records:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_2","title":"Python Code","text":"<pre><code># Load data with orphans\nfact_orders_dirty = pd.read_csv(\"data/orders_with_orphans.csv\")\n\n# This has 35 rows - 5 with invalid customer_sk values\n\n# Validate\nreport = validator.validate_fact(fact_orders_dirty, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Orphan records found: {len(report.orphan_records)}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#validation-report-with-orphans","title":"Validation Report (With Orphans)","text":"<pre><code>All valid: False\nTotal relationships: 3\nValid relationships: 2\nOrphan records found: 5\n\nRelationship: orders_to_customers\n  Status: INVALID\n  Total rows: 35\n  Orphan count: 5\n  Orphan values (sample):\n    - customer_sk = 99 (1 occurrence)\n    - customer_sk = 88 (1 occurrence)\n    - customer_sk = 77 (1 occurrence)\n    - customer_sk = 66 (1 occurrence)\n    - customer_sk = 55 (1 occurrence)\n\nRelationship: orders_to_products\n  Status: VALID\n  Total rows: 35\n  Orphan count: 0\n\nRelationship: orders_to_dates\n  Status: VALID\n  Total rows: 35\n  Orphan count: 0\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#detailed-orphan-records","title":"Detailed Orphan Records","text":"<pre><code># Get the orphan records\nfor orphan in report.orphan_records:\n    print(f\"Order {orphan.order_id}: customer_sk={orphan.customer_sk} not found\")\n</code></pre> order_id customer_sk reason ORD031 99 Not found in dim_customer ORD032 88 Not found in dim_customer ORD033 77 Not found in dim_customer ORD034 66 Not found in dim_customer ORD035 55 Not found in dim_customer"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-4-handle-orphans-with-different-strategies","title":"Step 4: Handle Orphans with Different Strategies","text":""},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-1-error-default","title":"Strategy 1: Error (Default)","text":"<p>Raise an exception when orphans are found:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load\n\ntry:\n    validated_df = validate_fk_on_load(\n        fact_df=fact_orders_dirty,\n        relationships=relationships,\n        context=context,\n        on_failure=\"error\"\n    )\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre> <p>Output:</p> <pre><code>Validation failed: FK validation found 5 orphan records:\n  - orders_to_customers: 5 orphans (customer_sk: 99, 88, 77, 66, 55)\n</code></pre> <p>Use case: Strict data quality\u2014fail the pipeline if any orphans exist.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-2-warn","title":"Strategy 2: Warn","text":"<p>Log a warning but continue processing:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.WARNING)\n\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=relationships,\n    context=context,\n    on_failure=\"warn\"\n)\n\nprint(f\"Returned {len(validated_df)} rows (including orphans)\")\n</code></pre> <p>Output:</p> <pre><code>WARNING:odibi.validation.fk:FK validation found 5 orphan records for orders_to_customers\n  Orphan values: customer_sk in [99, 88, 77, 66, 55]\nReturned 35 rows (including orphans)\n</code></pre> <p>Use case: Log issues for investigation but don't block processing.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-3-filter","title":"Strategy 3: Filter","text":"<p>Remove orphan records from the result:</p> <pre><code>validated_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=relationships,\n    context=context,\n    on_failure=\"filter\"\n)\n\nprint(f\"Before: {len(fact_orders_dirty)} rows\")\nprint(f\"After:  {len(validated_df)} rows\")\nprint(f\"Filtered: {len(fact_orders_dirty) - len(validated_df)} orphan rows\")\n</code></pre> <p>Output:</p> <pre><code>Before: 35 rows\nAfter:  30 rows\nFiltered: 5 orphan rows\n</code></pre> <p>Before (35 rows):</p> order_id customer_sk line_total ORD001 1 1299.99 ORD002 1 59.98 ... ... ... ORD030 5 189.99 ORD031 99 1299.99 ORD032 88 29.99 ORD033 77 249.99 ORD034 66 49.99 ORD035 55 599.99 <p>After (30 rows - orphans removed):</p> order_id customer_sk line_total ORD001 1 1299.99 ORD002 1 59.98 ... ... ... ORD030 5 189.99 <p>Use case: Silently exclude bad data (use with caution\u2014you may lose legitimate records).</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-5-generate-lineage-from-relationships","title":"Step 5: Generate Lineage from Relationships","text":"<p>The relationship registry can generate a lineage graph:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_3","title":"Python Code","text":"<pre><code>lineage = registry.generate_lineage()\nprint(lineage)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#output","title":"Output","text":"<pre><code>{\n    'fact_orders': ['dim_customer', 'dim_product', 'dim_date']\n}\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#mermaid-diagram","title":"Mermaid Diagram","text":"<pre><code># Generate mermaid diagram code\nmermaid_code = registry.to_mermaid()\nprint(mermaid_code)\n</code></pre> <pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        string order_id PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        decimal line_total\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string region\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        string month_name\n    }\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#complete-example","title":"Complete Example","text":"<p>Here's a complete script for FK validation:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nFK Validation Example\n\nThis script demonstrates:\n1. Defining FK relationships\n2. Validating a fact table\n3. Handling orphan records\n4. Generating lineage\n\"\"\"\n\nfrom odibi.validation.fk import (\n    RelationshipConfig,\n    RelationshipRegistry,\n    FKValidator,\n    validate_fk_on_load\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# =============================================================================\n# 1. LOAD DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 1: Load Data\")\nprint(\"=\" * 60)\n\n# Clean fact data\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\nprint(f\"fact_orders: {len(fact_orders)} rows\")\n\n# Fact data with orphans\nfact_orders_dirty = pd.read_csv(\n    \"examples/tutorials/dimensional_modeling/data/orders_with_orphans.csv\"\n)\nprint(f\"fact_orders_dirty: {len(fact_orders_dirty)} rows (5 orphans)\")\n\n# Dimensions\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\nprint()\n\n# =============================================================================\n# 2. DEFINE RELATIONSHIPS\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 2: Define Relationships\")\nprint(\"=\" * 60)\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_dates\",\n        fact=\"fact_orders\",\n        dimension=\"dim_date\",\n        fact_key=\"date_sk\",\n        dimension_key=\"date_sk\",\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\nprint(f\"Defined {len(relationships)} relationships\")\nprint()\n\n# =============================================================================\n# 3. SETUP CONTEXT\n# =============================================================================\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# =============================================================================\n# 4. VALIDATE CLEAN DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 3: Validate Clean Data\")\nprint(\"=\" * 60)\n\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(fact_orders, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Relationships checked: {report.total_relationships}\")\nprint(f\"Valid: {report.valid_relationships}\")\nprint()\n\n# =============================================================================\n# 5. VALIDATE DIRTY DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 4: Validate Data with Orphans\")\nprint(\"=\" * 60)\n\nreport = validator.validate_fact(fact_orders_dirty, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Orphan records: {len(report.orphan_records)}\")\n\nfor result in report.results:\n    status = \"VALID\" if result.valid else \"INVALID\"\n    print(f\"\\n  {result.relationship_name}: {status}\")\n    print(f\"    Total rows: {result.total_rows}\")\n    print(f\"    Orphans: {result.orphan_count}\")\n    if not result.valid:\n        print(f\"    Orphan values: {result.orphan_values[:5]}\")\nprint()\n\n# =============================================================================\n# 6. DEMONSTRATE HANDLING STRATEGIES\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 5: Orphan Handling Strategies\")\nprint(\"=\" * 60)\n\n# Filter strategy\nfiltered_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=[relationships[0]],  # Just customer relationship\n    context=context,\n    on_failure=\"filter\"\n)\n\nprint(f\"\\nFilter Strategy:\")\nprint(f\"  Before: {len(fact_orders_dirty)} rows\")\nprint(f\"  After:  {len(filtered_df)} rows\")\nprint(f\"  Removed: {len(fact_orders_dirty) - len(filtered_df)} orphan rows\")\nprint()\n\n# =============================================================================\n# 7. GENERATE LINEAGE\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 6: Generate Lineage\")\nprint(\"=\" * 60)\n\nlineage = registry.generate_lineage()\nprint(f\"Lineage: {lineage}\")\n\nprint(\"\\nDimensions referenced by fact_orders:\")\nfor dim in lineage.get(\"fact_orders\", []):\n    print(f\"  \u2192 {dim}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"COMPLETE!\")\nprint(\"=\" * 60)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Why FK validation matters: Orphan records cause data quality issues</li> <li>Defining relationships: Specify fact, dimension, and key columns</li> <li>Validating facts: Use FKValidator to detect orphans</li> <li>Handling strategies:</li> <li><code>error</code>: Fail the pipeline (strict)</li> <li><code>warn</code>: Log and continue (monitoring)</li> <li><code>filter</code>: Remove orphans (permissive)</li> <li>Generating lineage: Visualize table relationships</li> </ul>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#congratulations","title":"Congratulations!","text":"<p>You've completed the entire dimensional modeling tutorial series!</p> <p>What you built: - Part 1: Complete star schema (dimensions, facts, aggregates) - Part 2: Semantic layer (metrics, dimensions, materializations) - Part 3: FK validation (data quality)</p> <p>You now have all the tools to build production-ready dimensional data warehouses with Odibi.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#navigation","title":"Navigation","text":"Previous Up Next Semantic Full Example Tutorials -"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#reference","title":"Reference","text":"<p>For complete documentation, see: FK Validation Reference</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/","title":"Pipeline Run Story: schema_evolution_demo","text":"<p>Executed: 2025-11-19T16:56:10.535762 Completed: 2025-11-19T16:56:10.611748 Duration: 0.08s Status: \u2705 Success</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#summary","title":"Summary","text":"<ul> <li>\u2705 Completed: 3 nodes</li> <li>\u274c Failed: 0 nodes</li> <li>\u23ed\ufe0f Skipped: 0 nodes</li> <li>\u23f1\ufe0f Duration: 0.08s</li> </ul> <p>Completed nodes: create_source, enrich_data, cleanup_data</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-create_source","title":"Node: create_source","text":"<p>Status: \u2705 Success Duration: 0.0336s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Output schema: - Columns (3): product, region, sales - Rows: 3</p> <p>Sample output (first 3 rows):</p> product region sales A North 100 B North 150 A South 200"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-enrich_data","title":"Node: enrich_data","text":"<p>Status: \u2705 Success Duration: 0.0264s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Input schema: - Columns (3): product, region, sales</p> <p>Sample input (first 3 rows):</p> product region sales A North 100 B North 150 A South 200 <p>Output schema: - Columns (4): product, region, sales, tax</p> <p>Schema Changes: - \ud83d\udfe2 Added: tax - Rows: 3</p> <p>Sample output (first 3 rows):</p> product region sales tax A North 100 10.0 B North 150 15.0 A South 200 20.0"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-cleanup_data","title":"Node: cleanup_data","text":"<p>Status: \u2705 Success Duration: 0.0160s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Input schema: - Columns (4): product, region, sales, tax</p> <p>Sample input (first 3 rows):</p> product region sales tax A North 100 10.0 B North 150 15.0 A South 200 20.0 <p>Output schema: - Columns (3): product, sales, tax</p> <p>Schema Changes: - \ud83d\udd34 Removed: region - Rows: 3</p> <p>Sample output (first 3 rows):</p> product sales tax A 100 10.0 B 150 15.0 A 200 20.0"},{"location":"validation/","title":"Data Validation","text":"<p>Odibi provides a comprehensive validation framework to ensure data quality at every stage of your pipeline.</p>"},{"location":"validation/#validation-layers","title":"Validation Layers","text":"Layer When it Runs Purpose Contracts Before transform Fail-fast checks on input data Validation Tests After transform Row-level data quality checks Quality Gates After validation Batch-level thresholds and pass rates FK Validation Post-pipeline Referential integrity between tables"},{"location":"validation/#quick-links","title":"Quick Links","text":""},{"location":"validation/#guides","title":"Guides","text":"<ul> <li>Contracts - Pre-transform fail-fast checks (always fail on violation)</li> <li>Validation Tests - Post-transform row-level checks (fail/warn/quarantine)</li> <li>Quality Gates - Batch-level thresholds and pass rates</li> <li>Quarantine - Capture and review invalid records</li> <li>FK Validation - Referential integrity between fact and dimension tables</li> </ul>"},{"location":"validation/#reference","title":"Reference","text":"<ul> <li>Contracts Schema - All contract types</li> <li>Validation Schema - Full validation configuration</li> </ul>"},{"location":"validation/#choosing-the-right-validation","title":"Choosing the Right Validation","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INPUT DATA                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CONTRACTS (Pre-Transform)                                  \u2502\n\u2502  \u2022 not_null on required columns                             \u2502\n\u2502  \u2022 row_count min/max                                        \u2502\n\u2502  \u2022 freshness checks                                         \u2502\n\u2502  \u2192 ALWAYS FAILS on violation                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TRANSFORMATION                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VALIDATION TESTS (Post-Transform)                          \u2502\n\u2502  \u2022 Range checks, format validation                          \u2502\n\u2502  \u2022 Custom SQL conditions                                    \u2502\n\u2502  \u2192 Can WARN, QUARANTINE, or FAIL                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  QUALITY GATES (Batch-Level)                                \u2502\n\u2502  \u2022 Pass rate thresholds (e.g., 95%)                         \u2502\n\u2502  \u2022 Row count anomaly detection                              \u2502\n\u2502  \u2192 Can ABORT, WARN, or WRITE_VALID_ONLY                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    OUTPUT DATA                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"validation/#example-complete-validation-setup","title":"Example: Complete Validation Setup","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: staging\n      path: orders\n\n    # 1. Contracts - Fail fast on bad input\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id]\n      - type: row_count\n        min: 100\n\n    # 2. Transformation\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE amount &gt; 0\"\n\n    # 3. Validation Tests - Check output quality\n    validation:\n      tests:\n        - type: range\n          column: amount\n          min: 0\n          max: 1000000\n        - type: unique\n          columns: [order_id]\n      on_fail: quarantine  # Route bad rows to quarantine\n\n      # 4. Quality Gate - Batch-level threshold\n      gate:\n        require_pass_rate: 0.95\n        on_fail: abort\n\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/#see-also","title":"See Also","text":"<ul> <li>Getting Started: Validation - Tutorial walkthrough</li> <li>Fact Pattern - Orphan handling in fact tables</li> </ul>"},{"location":"validation/contracts/","title":"Contracts","text":"<p>Pre-transform data quality checks that fail fast before any transformation runs.</p>"},{"location":"validation/contracts/#overview","title":"Overview","text":"<p>Contracts validate your input data before it enters the transformation pipeline. Unlike validation tests (which run after transforms), contracts always halt execution on failure\u2014they're your first line of defense against bad data.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INPUT DATA                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CONTRACTS (You are here)                                   \u2502\n\u2502  \u2022 Runs BEFORE transformation                               \u2502\n\u2502  \u2022 Always fails on violation                                \u2502\n\u2502  \u2022 Prevents bad data from entering pipeline                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TRANSFORMATION                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"validation/contracts/#when-to-use-contracts-vs-validation","title":"When to Use Contracts vs Validation","text":"Feature Contracts Validation Tests When it runs Before transform After transform On failure Always fails pipeline Configurable (fail/warn/quarantine) Use case Input data quality Output data quality Example \"Source must have customer_id\" \"Transformed amount must be &gt; 0\" <p>Rule of thumb: Use contracts to validate what you receive, use validation to validate what you produce.</p>"},{"location":"validation/contracts/#quick-start","title":"Quick Start","text":"<pre><code>nodes:\n  - name: process_orders\n    # Contracts run first - before any transformation\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id]\n      - type: row_count\n        min: 100\n      - type: freshness\n        column: created_at\n        max_age: \"24h\"\n\n    read:\n      connection: bronze\n      path: orders_raw\n\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE amount &gt; 0\"\n\n    write:\n      connection: silver\n      path: orders\n</code></pre> <p>If any contract fails, the pipeline stops immediately with a <code>ValidationError</code>\u2014no transformation or write happens.</p>"},{"location":"validation/contracts/#available-contract-types","title":"Available Contract Types","text":""},{"location":"validation/contracts/#not_null","title":"not_null","text":"<p>Ensures columns contain no NULL values.</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n</code></pre> <p>Use for: Primary keys, required fields, foreign keys.</p>"},{"location":"validation/contracts/#unique","title":"unique","text":"<p>Ensures columns (or combination) contain unique values.</p> <pre><code># Single column\ncontracts:\n  - type: unique\n    columns: [order_id]\n\n# Composite key\ncontracts:\n  - type: unique\n    columns: [order_id, line_item_id]\n</code></pre> <p>Use for: Primary keys, natural keys, deduplication verification.</p>"},{"location":"validation/contracts/#row_count","title":"row_count","text":"<p>Validates row count falls within expected bounds.</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000      # At least 1000 rows\n    max: 100000    # At most 100K rows\n</code></pre> <p>Use for: Detect truncated loads, ensure minimum completeness, cap batch sizes.</p>"},{"location":"validation/contracts/#freshness","title":"freshness","text":"<p>Validates data is not stale by checking a timestamp column.</p> <pre><code>contracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n</code></pre> <p>Use for: SLA monitoring, detecting stale source systems.</p>"},{"location":"validation/contracts/#accepted_values","title":"accepted_values","text":"<p>Ensures a column only contains values from an allowed list.</p> <pre><code>contracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected, cancelled]\n</code></pre> <p>Use for: Enum fields, status columns, categorical data.</p>"},{"location":"validation/contracts/#range","title":"range","text":"<p>Ensures column values fall within a specified range.</p> <pre><code>contracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n\n# Date range\ncontracts:\n  - type: range\n    column: order_date\n    min: \"2020-01-01\"\n    max: \"2030-12-31\"\n</code></pre> <p>Use for: Numeric bounds (ages, prices, quantities), date ranges.</p>"},{"location":"validation/contracts/#regex_match","title":"regex_match","text":"<p>Ensures column values match a regex pattern.</p> <pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n\ncontracts:\n  - type: regex_match\n    column: phone\n    pattern: \"^\\\\+?[1-9]\\\\d{1,14}$\"  # E.164 format\n</code></pre> <p>Use for: Format validation (emails, phone numbers, IDs, codes).</p>"},{"location":"validation/contracts/#custom_sql","title":"custom_sql","text":"<p>Runs a custom SQL condition.</p> <pre><code>contracts:\n  - type: custom_sql\n    condition: \"amount &gt; 0 AND quantity &gt; 0\"\n    threshold: 0.01  # Allow up to 1% failures\n</code></pre> <p>Use for: Complex business rules, multi-column conditions.</p>"},{"location":"validation/contracts/#schema","title":"schema","text":"<p>Validates that the DataFrame schema matches expected columns.</p> <pre><code>contracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n\n# Works with column definitions\ncolumns:\n  - name: order_id\n    type: integer\n  - name: customer_id\n    type: integer\n  - name: amount\n    type: decimal\n</code></pre> <p>Use for: Schema stability, detecting upstream drift.</p>"},{"location":"validation/contracts/#distribution","title":"distribution","text":"<p>Checks if a column's statistical distribution is within expected bounds.</p> <pre><code>contracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \"&gt;100\"  # Mean must be &gt; 100\n    on_fail: warn      # Can warn instead of fail\n\ncontracts:\n  - type: distribution\n    column: customer_id\n    metric: null_percentage\n    threshold: \"&lt;0.05\"  # Less than 5% nulls\n</code></pre> <p>Metrics: <code>mean</code>, <code>min</code>, <code>max</code>, <code>null_percentage</code></p> <p>Use for: Anomaly detection, data drift monitoring.</p>"},{"location":"validation/contracts/#real-world-examples","title":"Real-World Examples","text":""},{"location":"validation/contracts/#example-1-bronze-layer-ingestion","title":"Example 1: Bronze Layer Ingestion","text":"<p>Validate raw data before any processing:</p> <pre><code>nodes:\n  - name: ingest_customers\n    contracts:\n      # Must have data\n      - type: row_count\n        min: 1\n\n      # Required fields present\n      - type: not_null\n        columns: [customer_id, email]\n\n      # Data is fresh\n      - type: freshness\n        column: _extracted_at\n        max_age: \"48h\"\n\n    read:\n      connection: source_api\n      path: customers\n      format: json\n\n    write:\n      connection: bronze\n      path: customers_raw\n      mode: append\n      add_metadata: true\n</code></pre>"},{"location":"validation/contracts/#example-2-silver-layer-processing","title":"Example 2: Silver Layer Processing","text":"<p>Validate before expensive transformations:</p> <pre><code>nodes:\n  - name: process_transactions\n    contracts:\n      # Ensure keys exist for joins\n      - type: not_null\n        columns: [transaction_id, account_id, merchant_id]\n\n      # No duplicates in source\n      - type: unique\n        columns: [transaction_id]\n\n      # Amount makes sense\n      - type: range\n        column: amount\n        min: 0.01\n        max: 1000000\n\n      # Valid transaction types\n      - type: accepted_values\n        column: type\n        values: [purchase, refund, chargeback, transfer]\n\n    read:\n      connection: bronze\n      path: transactions_raw\n\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              t.*,\n              m.merchant_name,\n              a.account_type\n            FROM df t\n            LEFT JOIN merchants m ON t.merchant_id = m.id\n            LEFT JOIN accounts a ON t.account_id = a.id\n\n    write:\n      connection: silver\n      path: transactions\n      format: delta\n</code></pre>"},{"location":"validation/contracts/#example-3-cross-system-data","title":"Example 3: Cross-System Data","text":"<p>Validate data from multiple sources before combining:</p> <pre><code>nodes:\n  - name: merge_customer_data\n    contracts:\n      # Schema must match expected structure\n      - type: schema\n        strict: true\n\n      # Prevent data explosion\n      - type: row_count\n        max: 10000000\n\n      # Statistical sanity check\n      - type: distribution\n        column: lifetime_value\n        metric: mean\n        threshold: \"&gt;0\"\n\n      # Custom business rule\n      - type: custom_sql\n        condition: \"signup_date &lt;= last_order_date OR last_order_date IS NULL\"\n        threshold: 0.0  # Zero tolerance\n\n    read:\n      - connection: crm\n        path: customers\n      - connection: ecommerce\n        path: users\n\n    transform:\n      steps:\n        - sql: |\n            SELECT * FROM df1\n            UNION ALL\n            SELECT * FROM df2\n</code></pre>"},{"location":"validation/contracts/#error-handling","title":"Error Handling","text":"<p>When a contract fails, Odibi raises a <code>ValidationError</code> with details:</p> <pre><code>from odibi.exceptions import ValidationError\n\ntry:\n    pipeline.run()\nexcept ValidationError as e:\n    print(f\"Contract failed on node: {e.node_name}\")\n    print(f\"Failures: {e.failures}\")\n    # [{'test': 'not_null', 'column': 'customer_id', 'null_count': 42}]\n</code></pre>"},{"location":"validation/contracts/#best-practices","title":"Best Practices","text":"<ol> <li>Start with not_null and unique - Most contract failures are missing keys or duplicates</li> <li>Use row_count for safety - Prevents empty loads and data explosions</li> <li>Add freshness for SLAs - Know immediately when source systems are stale</li> <li>Keep contracts fast - They run on every execution; avoid expensive checks</li> <li>Don't over-contract - Validate what matters; save detailed checks for validation tests</li> </ol>"},{"location":"validation/contracts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"validation/contracts/#validationerror-contract-failure-but-which-contract","title":"\"ValidationError: Contract Failure\" - but which contract?","text":"<p>Symptom: Contract fails but error message is unclear.</p> <p>Fix: Check the <code>failures</code> list in the error:</p> <pre><code>except ValidationError as e:\n    for failure in e.failures:\n        print(f\"Test: {failure['test']}, Details: {failure}\")\n</code></pre> <p>Or check the story report for detailed validation results.</p>"},{"location":"validation/contracts/#contract-passes-locally-but-fails-in-production","title":"Contract passes locally but fails in production","text":"<p>Common Causes: - Different data volumes (row_count thresholds) - Timezone differences (freshness checks) - Case sensitivity differences between engines</p> <p>Fixes: - Use <code>--dry-run</code> in production first to validate - Set <code>max_age</code> with buffer for timezone differences - Test with production-like data volumes locally</p>"},{"location":"validation/contracts/#freshness-contract-always-fails","title":"Freshness contract always fails","text":"<p>Symptom: <code>freshness</code> check fails even with recent data.</p> <p>Causes: - Wrong column name for timestamp - Timestamp column is string, not datetime - Timezone mismatch</p> <p>Fixes:</p> <pre><code>contracts:\n  - type: freshness\n    column: updated_at    # Must be datetime type\n    max_age: \"24h\"        # Include buffer for safety\n</code></pre>"},{"location":"validation/contracts/#contract-on-wrong-dataframe","title":"Contract on wrong DataFrame","text":"<p>Symptom: Contract checks input data when you wanted to check transformed data.</p> <p>Explanation: Contracts run on input data (before transform). For output validation, use <code>validation.tests</code> instead:</p> <pre><code># Contracts = input validation (before transform)\ncontracts:\n  - type: not_null\n    columns: [id]\n\n# Validation = output validation (after transform)\nvalidation:\n  tests:\n    - type: not_null\n      columns: [computed_field]\n</code></pre>"},{"location":"validation/contracts/#column-not-found-in-contract","title":"\"Column not found\" in contract","text":"<p>Cause: Column name doesn't exist in input DataFrame.</p> <p>Fix: Verify column names match exactly (case-sensitive):</p> <pre><code># Debug by adding a dry-run first\nodibi run config.yaml --dry-run\n</code></pre>"},{"location":"validation/contracts/#see-also","title":"See Also","text":"<ul> <li>Validation Overview - The 4-layer validation model</li> <li>Validation Tests - Post-transform row-level checks</li> <li>Quality Gates - Batch-level thresholds</li> <li>Quarantine - Route bad rows for review</li> <li>YAML Reference - Full contract schema</li> </ul>"},{"location":"validation/fk/","title":"FK Validation","text":"<p>The FK Validation module declares and validates referential integrity between fact and dimension tables.</p>"},{"location":"validation/fk/#how-it-fits-into-odibi","title":"How It Fits Into Odibi","text":"<p>FK Validation is a Python API module that complements the <code>fact</code> pattern. While the <code>fact</code> pattern handles basic orphan handling (unknown member assignment), the FK validation module provides:</p> <ul> <li>Detailed orphan reporting with sample values</li> <li>Multiple validation strategies (error, warn, filter)</li> <li>Relationship registry for documenting your data model</li> <li>Lineage generation from relationships</li> </ul> <p>It's typically used for: 1. Post-pipeline validation/auditing 2. Custom fact loading with advanced orphan handling 3. Documenting relationships for data governance</p>"},{"location":"validation/fk/#quick-start","title":"Quick Start","text":""},{"location":"validation/fk/#1-define-relationships","title":"1. Define Relationships","text":"<pre><code>from odibi.validation.fk import RelationshipConfig, RelationshipRegistry\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"validation/fk/#2-validate-a-fact-table","title":"2. Validate a Fact Table","text":"<pre><code>from odibi.validation.fk import FKValidator\nfrom odibi.context import EngineContext\n\n# Setup context with dimension tables\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\ncontext.register(\"dim_product\", spark.table(\"warehouse.dim_product\"))\n\n# Load fact table\nfact_df = spark.table(\"warehouse.fact_orders\")\n\n# Validate\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(fact_df, \"fact_orders\", context)\n\n# Check results\nif report.all_valid:\n    print(\"All FK relationships valid!\")\nelse:\n    print(f\"Found {len(report.orphan_records)} orphan records\")\n    for result in report.results:\n        if not result.valid:\n            print(f\"  {result.relationship_name}: {result.orphan_count} orphans\")\n</code></pre>"},{"location":"validation/fk/#yaml-configuration-optional","title":"YAML Configuration (Optional)","text":"<p>You can define relationships in YAML and load them:</p> <pre><code># relationships.yaml\nrelationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_dates\n    fact: fact_orders\n    dimension: dim_date\n    fact_key: order_date_sk\n    dimension_key: date_sk\n    nullable: true  # Pending orders may not have date\n    on_violation: warn\n</code></pre> <pre><code>from odibi.validation.fk import parse_relationships_config\nimport yaml\n\nwith open(\"relationships.yaml\") as f:\n    config = yaml.safe_load(f)\n\nregistry = parse_relationships_config(config)\n</code></pre>"},{"location":"validation/fk/#relationshipconfig","title":"RelationshipConfig","text":"Field Type Required Default Description <code>name</code> str Yes - Unique relationship identifier <code>fact</code> str Yes - Fact table name <code>dimension</code> str Yes - Dimension table name <code>fact_key</code> str Yes - FK column in fact table <code>dimension_key</code> str Yes - PK/SK column in dimension <code>nullable</code> bool No false Whether nulls are allowed in fact_key <code>on_violation</code> str No \"error\" Action on violation: \"error\", \"warn\", \"quarantine\""},{"location":"validation/fk/#validation-results","title":"Validation Results","text":""},{"location":"validation/fk/#fkvalidationresult-per-relationship","title":"FKValidationResult (per relationship)","text":"Field Type Description <code>relationship_name</code> str Relationship identifier <code>valid</code> bool Whether validation passed <code>total_rows</code> int Total rows in fact table <code>orphan_count</code> int Number of orphan records <code>null_count</code> int Number of null FK values <code>orphan_values</code> list Sample orphan values (up to 100) <code>elapsed_ms</code> float Validation time"},{"location":"validation/fk/#fkvalidationreport-for-entire-fact-table","title":"FKValidationReport (for entire fact table)","text":"Field Type Description <code>fact_table</code> str Fact table name <code>all_valid</code> bool True if all relationships valid <code>total_relationships</code> int Number of relationships checked <code>valid_relationships</code> int Number that passed <code>results</code> List[FKValidationResult] Individual results <code>orphan_records</code> List[OrphanRecord] All orphan records"},{"location":"validation/fk/#validate_fk_on_load","title":"validate_fk_on_load","text":"<p>Convenience function for pipeline integration:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load, RelationshipConfig\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    )\n]\n\n# Error on violation (default) - raises ValueError\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"error\"\n)\n\n# Warn on violation - logs warning, returns original\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"warn\"\n)\n\n# Filter orphans - removes orphan rows\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"filter\"\n)\n</code></pre>"},{"location":"validation/fk/#integration-with-fact-pattern","title":"Integration with Fact Pattern","text":"<p>The <code>fact</code> pattern already handles basic orphan handling. Use FK validation for additional auditing:</p> <pre><code># odibi.yaml - Build fact with orphan handling\npipelines:\n  - pipeline: build_facts\n    nodes:\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n\n      - name: fact_orders\n        depends_on: [dim_customer]\n        read:\n          connection: staging\n          path: orders\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n          orphan_handling: unknown  # Assigns SK=0 to orphans\n        write:\n          connection: warehouse\n          path: fact_orders\n</code></pre> <p>Then run FK validation as a post-pipeline check:</p> <pre><code># Post-pipeline audit\nfrom odibi.validation.fk import FKValidator, RelationshipRegistry, RelationshipConfig\n\n# Define expected relationships\nregistry = RelationshipRegistry(relationships=[\n    RelationshipConfig(\n        name=\"verify_customer_fk\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    )\n])\n\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(\n    spark.table(\"warehouse.fact_orders\"),\n    \"fact_orders\",\n    context\n)\n\n# Report findings\nif not report.all_valid:\n    print(f\"WARNING: {report.orphan_records} orphan records found\")\n    # Log to monitoring, send alert, etc.\n</code></pre>"},{"location":"validation/fk/#lineage-generation","title":"Lineage Generation","text":"<p>Generate lineage graph from relationships:</p> <pre><code>registry = RelationshipRegistry(relationships=[\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\"\n    ),\n    RelationshipConfig(\n        name=\"line_items_to_orders\",\n        fact=\"fact_line_items\",\n        dimension=\"fact_orders\",\n        fact_key=\"order_sk\",\n        dimension_key=\"order_sk\"\n    )\n])\n\nlineage = registry.generate_lineage()\n# {\n#     'fact_orders': ['dim_customer', 'dim_product'],\n#     'fact_line_items': ['fact_orders']\n# }\n</code></pre>"},{"location":"validation/fk/#full-example","title":"Full Example","text":"<p>Complete FK validation workflow:</p> <pre><code>from odibi.validation.fk import (\n    RelationshipConfig,\n    RelationshipRegistry,\n    FKValidator,\n    get_orphan_records\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\n\n# Define relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        nullable=False,\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\nvalidator = FKValidator(registry)\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\ncontext.register(\"dim_product\", spark.table(\"warehouse.dim_product\"))\n\n# Load and validate\nfact_df = spark.table(\"warehouse.fact_orders\")\nreport = validator.validate_fact(fact_df, \"fact_orders\", context)\n\n# Report\nprint(f\"Validation {'PASSED' if report.all_valid else 'FAILED'}\")\nprint(f\"Checked {report.total_relationships} relationships\")\n\nfor result in report.results:\n    status = \"PASS\" if result.valid else \"FAIL\"\n    print(f\"  {result.relationship_name}: {status}\")\n    if not result.valid:\n        print(f\"    Orphans: {result.orphan_count}\")\n        print(f\"    Sample values: {result.orphan_values[:5]}\")\n\n# Generate lineage\nlineage = registry.generate_lineage()\nprint(f\"Lineage: {lineage}\")\n</code></pre>"},{"location":"validation/fk/#pipeline-integration-patterns","title":"Pipeline Integration Patterns","text":""},{"location":"validation/fk/#pattern-1-pre-load-validation","title":"Pattern 1: Pre-Load Validation","text":"<p>Validate FK relationships before writing to the warehouse:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load, RelationshipConfig\n\n@transform\ndef validate_and_load_orders(context, current):\n    \"\"\"Validate FKs before writing to warehouse.\"\"\"\n    relationships = [\n        RelationshipConfig(\n            name=\"orders_to_customers\",\n            fact=\"orders\",\n            dimension=\"dim_customer\",\n            fact_key=\"customer_id\",\n            dimension_key=\"customer_id\"\n        )\n    ]\n\n    validated_df = validate_fk_on_load(\n        fact_df=current,\n        relationships=relationships,\n        context=context,\n        on_failure=\"filter\"  # Remove orphans\n    )\n    return validated_df\n</code></pre> <p>YAML configuration:</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: warehouse\n      path: dim_customer\n\n  - name: validated_orders\n    depends_on: [dim_customer]\n    read:\n      connection: staging\n      path: orders\n    transform:\n      steps:\n        - function: validate_and_load_orders\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/fk/#pattern-2-post-pipeline-audit-job","title":"Pattern 2: Post-Pipeline Audit Job","text":"<p>Run FK validation as a separate audit pipeline:</p> <pre><code>pipelines:\n  - pipeline: audit_referential_integrity\n    description: \"Nightly FK validation audit\"\n    nodes:\n      - name: load_dimensions\n        read:\n          connection: warehouse\n          tables:\n            - dim_customer\n            - dim_product\n            - dim_date\n\n      - name: validate_fact_orders\n        depends_on: [load_dimensions]\n        read:\n          connection: warehouse\n          path: fact_orders\n        transform:\n          steps:\n            - function: run_fk_audit\n              params:\n                relationships_file: \"config/fk_relationships.yaml\"\n</code></pre>"},{"location":"validation/fk/#pattern-3-integrated-with-data-quality-gate","title":"Pattern 3: Integrated with Data Quality Gate","text":"<p>Use FK validation as a quality gate:</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: staging\n      path: orders\n    transformer: fact\n    params:\n      grain: [order_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          dimension_key: customer_id\n          surrogate_key: customer_sk\n    gate:\n      - type: custom\n        function: fk_validation_gate\n        params:\n          max_orphan_percent: 0.1  # Fail if &gt; 0.1% orphans\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/fk/#see-also","title":"See Also","text":"<ul> <li>Fact Pattern - Build fact tables with orphan handling</li> <li>Dimension Pattern - Build dimensions with unknown member</li> <li>Patterns Overview - All available patterns</li> </ul>"},{"location":"validation/tests/","title":"Validation Tests","text":"<p>Row-level data quality checks that run after transformation.</p>"},{"location":"validation/tests/#overview","title":"Overview","text":"<p>Validation tests evaluate your output data after transformations complete. Unlike contracts (which always fail), validation tests offer flexible responses: fail the pipeline, log a warning, or quarantine bad rows.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TRANSFORMATION                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VALIDATION TESTS (You are here)                            \u2502\n\u2502  \u2022 Runs AFTER transformation                                \u2502\n\u2502  \u2022 Configurable response (fail/warn/quarantine)             \u2502\n\u2502  \u2022 Row-level evaluation                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  QUALITY GATES                                              \u2502\n\u2502  \u2022 Batch-level thresholds                                   \u2502\n\u2502  \u2022 \"Did 95% of rows pass?\"                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"validation/tests/#quick-start","title":"Quick Start","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: bronze\n      path: orders_raw\n\n    transform:\n      steps:\n        - sql: \"SELECT *, amount * quantity AS total FROM df\"\n\n    # Validation runs after transform\n    validation:\n      tests:\n        - type: not_null\n          columns: [order_id, customer_id]\n        - type: range\n          column: total\n          min: 0\n        - type: unique\n          columns: [order_id]\n\n      on_fail: quarantine  # Route bad rows instead of failing\n\n      quarantine:\n        connection: silver\n        path: quarantine/orders\n\n    write:\n      connection: silver\n      path: orders\n</code></pre>"},{"location":"validation/tests/#test-types-reference","title":"Test Types Reference","text":""},{"location":"validation/tests/#not_null","title":"not_null","text":"<p>Ensures columns contain no NULL values.</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [order_id, customer_id, email]\n      on_fail: quarantine\n</code></pre> Field Type Required Description <code>columns</code> list Yes Columns to check for nulls <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#unique","title":"unique","text":"<p>Ensures column values (or combinations) are unique.</p> <pre><code>validation:\n  tests:\n    # Single column\n    - type: unique\n      columns: [order_id]\n\n    # Composite key\n    - type: unique\n      columns: [order_id, line_item_id]\n      on_fail: fail  # Duplicates are critical\n</code></pre> Field Type Required Description <code>columns</code> list Yes Columns that form the unique key <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#range","title":"range","text":"<p>Ensures values fall within min/max bounds.</p> <pre><code>validation:\n  tests:\n    - type: range\n      column: age\n      min: 0\n      max: 150\n\n    - type: range\n      column: price\n      min: 0.01\n      # max is optional\n\n    - type: range\n      column: order_date\n      min: \"2020-01-01\"\n      max: \"2030-12-31\"\n</code></pre> Field Type Required Description <code>column</code> string Yes Column to check <code>min</code> number/string No Minimum value (inclusive) <code>max</code> number/string No Maximum value (inclusive) <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#accepted_values","title":"accepted_values","text":"<p>Ensures column only contains allowed values.</p> <pre><code>validation:\n  tests:\n    - type: accepted_values\n      column: status\n      values: [pending, approved, rejected, cancelled]\n      on_fail: warn  # Log but don't fail\n</code></pre> Field Type Required Description <code>column</code> string Yes Column to check <code>values</code> list Yes Allowed values <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#regex_match","title":"regex_match","text":"<p>Ensures values match a regex pattern.</p> <pre><code>validation:\n  tests:\n    - type: regex_match\n      column: email\n      pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n\n    - type: regex_match\n      column: phone\n      pattern: \"^\\\\+?[1-9]\\\\d{1,14}$\"\n      on_fail: quarantine\n</code></pre> Field Type Required Description <code>column</code> string Yes Column to check <code>pattern</code> string Yes Regex pattern <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#row_count","title":"row_count","text":"<p>Validates total row count.</p> <pre><code>validation:\n  tests:\n    - type: row_count\n      min: 100\n      max: 1000000\n</code></pre> Field Type Required Description <code>min</code> int No Minimum row count <code>max</code> int No Maximum row count <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#custom_sql","title":"custom_sql","text":"<p>Runs a custom SQL condition.</p> <pre><code>validation:\n  tests:\n    - type: custom_sql\n      name: positive_profit  # Named for clarity\n      condition: \"revenue &gt;= cost\"\n      threshold: 0.05  # Allow up to 5% failures\n\n    - type: custom_sql\n      condition: \"end_date &gt;= start_date OR end_date IS NULL\"\n      threshold: 0.0  # Zero tolerance\n</code></pre> Field Type Required Description <code>condition</code> string Yes SQL WHERE clause (should be true for valid rows) <code>threshold</code> float No Allowed failure rate (0.0 = no failures, 0.05 = 5%) <code>name</code> string No Name for reporting <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#volume_drop","title":"volume_drop","text":"<p>Detects unexpected drops in data volume compared to previous runs.</p> <pre><code>validation:\n  tests:\n    - type: volume_drop\n      max_drop_percentage: 50  # Fail if count drops &gt;50%\n</code></pre> Field Type Required Description <code>max_drop_percentage</code> float Yes Maximum allowed drop (0-100) <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#on-fail-actions","title":"On-Fail Actions","text":"<p>Each test can specify what happens when it fails:</p> Action Behavior <code>fail</code> Stop pipeline immediately (default) <code>warn</code> Log warning, continue with all rows <code>quarantine</code> Route failed rows to quarantine table, continue with valid rows <pre><code>validation:\n  tests:\n    # Critical - must pass\n    - type: unique\n      columns: [order_id]\n      on_fail: fail\n\n    # Important but recoverable\n    - type: not_null\n      columns: [email]\n      on_fail: quarantine\n\n    # Nice to have\n    - type: regex_match\n      column: phone\n      pattern: \"^\\\\d{10}$\"\n      on_fail: warn\n</code></pre>"},{"location":"validation/tests/#combining-with-quality-gates","title":"Combining with Quality Gates","text":"<p>Quality gates evaluate aggregate pass rates after all tests run:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n    - type: range\n      column: amount\n      min: 0\n\n  gate:\n    require_pass_rate: 0.95  # 95% must pass ALL tests\n    on_fail: abort\n\n    # Per-test thresholds\n    thresholds:\n      - test: not_null\n        min_pass_rate: 0.99  # 99% for not_null\n      - test: range\n        min_pass_rate: 0.90  # 90% for range\n</code></pre> <p>See Quality Gates for full documentation.</p>"},{"location":"validation/tests/#combining-with-quarantine","title":"Combining with Quarantine","text":"<p>Route failed rows to a quarantine table for review:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id, email]\n      on_fail: quarantine\n\n    - type: regex_match\n      column: email\n      pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n      on_fail: quarantine\n\n  quarantine:\n    connection: silver\n    path: quarantine/customers\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n      _failed_tests: true\n</code></pre> <p>See Quarantine for full documentation.</p>"},{"location":"validation/tests/#real-world-examples","title":"Real-World Examples","text":""},{"location":"validation/tests/#example-1-customer-data-validation","title":"Example 1: Customer Data Validation","text":"<pre><code>nodes:\n  - name: validate_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              customer_id,\n              LOWER(TRIM(email)) AS email,\n              phone,\n              signup_date,\n              country\n            FROM df\n\n    validation:\n      tests:\n        # Required fields\n        - type: not_null\n          columns: [customer_id, email, signup_date]\n          on_fail: quarantine\n\n        # Email format\n        - type: regex_match\n          column: email\n          pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n          on_fail: quarantine\n\n        # Valid countries\n        - type: accepted_values\n          column: country\n          values: [US, CA, UK, DE, FR, AU]\n          on_fail: warn  # Log but allow\n\n        # No duplicates\n        - type: unique\n          columns: [customer_id]\n          on_fail: fail  # Critical\n\n      quarantine:\n        connection: silver\n        path: quarantine/customers\n        add_columns:\n          _rejection_reason: true\n          _rejected_at: true\n\n      gate:\n        require_pass_rate: 0.95\n        on_fail: abort\n\n    write:\n      connection: silver\n      path: customers\n      format: delta\n</code></pre>"},{"location":"validation/tests/#example-2-financial-transactions","title":"Example 2: Financial Transactions","text":"<pre><code>nodes:\n  - name: validate_transactions\n    read:\n      connection: bronze\n      path: transactions_raw\n\n    validation:\n      tests:\n        # Business rules\n        - type: custom_sql\n          name: valid_amount\n          condition: \"amount &gt; 0\"\n          on_fail: quarantine\n\n        - type: custom_sql\n          name: balanced_transaction\n          condition: \"debit_amount = credit_amount\"\n          threshold: 0.0  # Zero tolerance\n\n        # Date sanity\n        - type: range\n          column: transaction_date\n          min: \"2020-01-01\"\n          on_fail: quarantine\n\n        # Volume monitoring\n        - type: volume_drop\n          max_drop_percentage: 30\n          on_fail: fail\n\n      quarantine:\n        connection: silver\n        path: quarantine/transactions\n\n    write:\n      connection: silver\n      path: transactions\n</code></pre>"},{"location":"validation/tests/#example-3-mixed-severity-levels","title":"Example 3: Mixed Severity Levels","text":"<pre><code>validation:\n  tests:\n    # CRITICAL - Pipeline must stop\n    - type: unique\n      columns: [transaction_id]\n      on_fail: fail\n\n    # IMPORTANT - Quarantine for review\n    - type: not_null\n      columns: [customer_id, amount]\n      on_fail: quarantine\n\n    # MODERATE - Track but continue\n    - type: accepted_values\n      column: category\n      values: [retail, wholesale, online]\n      on_fail: warn\n\n    # MONITORING - Statistical check\n    - type: custom_sql\n      name: amount_sanity\n      condition: \"amount &lt; 100000\"\n      threshold: 0.01  # Allow 1% outliers\n      on_fail: warn\n</code></pre>"},{"location":"validation/tests/#best-practices","title":"Best Practices","text":"<ol> <li>Name your tests - Use the <code>name</code> field for clarity in logs and quarantine tables</li> <li>Layer your severity - Use <code>fail</code> for critical, <code>quarantine</code> for recoverable, <code>warn</code> for monitoring</li> <li>Combine with gates - Set pass rate thresholds for batch-level control</li> <li>Use quarantine generously - Don't lose bad data; capture it for analysis</li> <li>Keep custom_sql readable - Break complex conditions into multiple named tests</li> </ol>"},{"location":"validation/tests/#see-also","title":"See Also","text":"<ul> <li>Validation Overview - The 4-layer validation model</li> <li>Contracts - Pre-transform fail-fast checks</li> <li>Quality Gates - Batch-level thresholds</li> <li>Quarantine - Route bad rows for review</li> <li>YAML Reference - Full validation schema</li> </ul>"}]}