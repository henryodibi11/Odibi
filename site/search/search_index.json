{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Odibi Documentation","text":"<p>Welcome to the Odibi documentation. Odibi is a declarative data engineering framework designed to make data pipelines transparent, self-documenting, and easy to maintain.</p>"},{"location":"#how-to-use-this-documentation","title":"\ud83d\udcda How to use this documentation","text":"<p>We follow the Di\u00e1taxis framework, organizing documentation by user needs:</p>"},{"location":"#1-tutorials-learning-oriented","title":"1. \ud83c\udfc1 Tutorials (Learning-Oriented)","text":"<p>Start here if you are new. Step-by-step lessons that take you from \"Zero\" to \"Running Pipeline\". *   Getting Started: Your first 10 minutes with Odibi. *   Databricks Setup: Running Odibi on Spark clusters.</p>"},{"location":"#2-how-to-guides-task-oriented","title":"2. \ud83d\udcd8 How-To Guides (Task-Oriented)","text":"<p>Read these when you need to solve a specific problem. Practical recipes for common tasks. *   Best Practices: Project organization, naming, performance, and more. *   Writing Custom Transformations: How to write Python logic for your pipeline. *   Using the CLI: Running, stress-testing, and debugging. *   Production Deployment: Moving from laptop to cloud. *   Performance Tuning: Optimize for speed and scale. *   WSL Setup: The definitive guide for Windows users.</p>"},{"location":"#3-reference-information-oriented","title":"3. \u2699\ufe0f Reference (Information-Oriented)","text":"<p>Look here for technical specs and syntax. *   Configuration Reference: Complete guide to <code>odibi.yaml</code>. *   Cheatsheet: Quick lookup for commands and syntax. *   Supported Formats: CSV, Parquet, Delta, JSON details.</p>"},{"location":"#4-explanation-understanding-oriented","title":"4. \ud83e\udde0 Explanation (Understanding-Oriented)","text":"<p>Read these to understand the \"Why\" and \"How\". *   Architecture: How Odibi works under the hood. *   Case Studies: Real-world patterns and examples.</p>"},{"location":"#quick-links","title":"\ud83d\ude80 Quick Links","text":"<ul> <li>Repository: GitHub</li> <li>Issues: Report a Bug</li> <li>PyPI: View Package</li> </ul> <p>This documentation is versioned with the Odibi framework. Last updated: December 2025.</p>"},{"location":"AUDIT_FINDINGS/","title":"Implementation Quality Audit - Odibi Pipeline Framework","text":"<p>Date: December 3, 2025 Audit Thread: https://ampcode.com/threads/T-e994f339-852c-4546-a642-5b02a984b896</p>"},{"location":"AUDIT_FINDINGS/#summary","title":"Summary","text":"<p>Found 17 implementation issues across the Odibi codebase. These are categorized by priority and include specific file locations, problem descriptions, impact assessments, and suggested fixes.</p>"},{"location":"AUDIT_FINDINGS/#high-priority-performance-critical","title":"\ud83d\udd34 HIGH PRIORITY (Performance Critical)","text":""},{"location":"AUDIT_FINDINGS/#issue-1-redundant-dataframe-count-operations","title":"Issue #1: Redundant DataFrame Count Operations","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 2098-2108, 2167-2172, 2228-2256 Problem Multiple <code>.count()</code> calls on same DataFrame trigger redundant Spark jobs Impact Performance - each <code>.count()</code> is an expensive distributed action <p>Current Code:</p> <pre><code>initial_count = df.count()\ndf = df.filter(...)\ndeleted_count += initial_count - df.count()  # Second count on filtered df\n</code></pre> <p>Suggested Fix: Cache DataFrame before counting, or use single aggregation to get both values:</p> <pre><code>df.cache()\ninitial_count = df.count()\n# ... filter operations\nfinal_count = df.count()\ndf.unpersist()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-2-repeated-delta-table-reads-in-catalog-methods","title":"Issue #2: Repeated Delta Table Reads in Catalog Methods","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 507-634 Problem Methods <code>get_registered_pipeline</code>, <code>get_registered_nodes</code>, <code>get_all_registered_pipelines</code>, <code>get_all_registered_nodes</code> each read the same Delta table separately Impact Performance - redundant I/O when called in sequence during auto-registration <p>Affected Methods: - <code>get_registered_pipeline()</code> - reads <code>meta_pipelines</code> - <code>get_registered_nodes()</code> - reads <code>meta_nodes</code> - <code>get_all_registered_pipelines()</code> - reads <code>meta_pipelines</code> - <code>get_all_registered_nodes()</code> - reads <code>meta_nodes</code></p> <p>Suggested Fix: Add caching layer or batch read method that fetches all needed data in one read:</p> <pre><code>def _get_cached_pipelines(self) -&gt; Dict[str, Dict]:\n    if self._pipeline_cache is None:\n        self._pipeline_cache = self._read_all_pipelines()\n    return self._pipeline_cache\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-3-expensive-count-in-validation-loop","title":"Issue #3: Expensive .count() in Validation Loop","text":"Attribute Details File <code>odibi/validation/engine.py</code> Lines 250, 269, 282, 305, 323, 337 Problem Individual <code>.count()</code> for each column validation rule Impact O(n) Spark jobs where n = number of validation columns/rules <p>Current Pattern:</p> <pre><code>for col in validation_config.no_nulls:\n    null_count = df.filter(F.col(col).isNull()).count()  # One job per column\n</code></pre> <p>Suggested Fix: Aggregate all null counts in single <code>.agg()</code> call:</p> <pre><code>null_counts = df.select([\n    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n    for c in validation_config.no_nulls\n]).collect()[0].asDict()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-4-individual-register_pipelineregister_node-methods-still-perform-single-writes","title":"Issue #4: Individual register_pipeline/register_node Methods Still Perform Single Writes","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 816-1063 Problem While <code>register_pipelines_batch</code> and <code>register_nodes_batch</code> exist, the individual methods still perform single writes Impact Callers using individual methods still suffer N individual Delta writes <p>Suggested Fix: Deprecate individual methods or have them delegate to batch internally:</p> <pre><code>def register_pipeline(self, pipeline_config, ...):\n    \"\"\"Deprecated: Use register_pipelines_batch for better performance.\"\"\"\n    warnings.warn(\"Use register_pipelines_batch instead\", DeprecationWarning)\n    return self.register_pipelines_batch([self._prepare_pipeline_record(pipeline_config)])\n</code></pre>"},{"location":"AUDIT_FINDINGS/#medium-priority-reliability-maintainability","title":"\ud83d\udfe1 MEDIUM PRIORITY (Reliability &amp; Maintainability)","text":""},{"location":"AUDIT_FINDINGS/#issue-5-silent-exception-handling-suppresses-failures","title":"Issue #5: Silent Exception Handling Suppresses Failures","text":"Attribute Details Files Multiple (see below) Problem Exception handlers log warnings but don't re-raise, hiding failures Impact Reliability - failures go unnoticed, debugging is harder <p>Locations: | File | Lines | Pattern | |------|-------|---------| | <code>odibi/story/generator.py</code> | 186-188 | <code>except Exception: pass</code> | | <code>odibi/lineage.py</code> | 66-68 | Silently disables OpenLineage | | <code>odibi/lineage.py</code> | 118-120 | Returns fake UUID on failure | | <code>odibi/lineage.py</code> | 157-158 | Silently ignores emit failure | | <code>odibi/utils/extensions.py</code> | 27-28 | Only logs warning | | <code>odibi/plugins.py</code> | 64-65, 67-68 | Only logs error |</p> <p>Suggested Fix: At minimum, log at ERROR level with stack trace. Consider raising wrapped exceptions:</p> <pre><code>except Exception as e:\n    logger.error(f\"Operation failed: {e}\", exc_info=True)\n    # Either re-raise or set a flag for caller to check\n    self._last_error = e\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-6-inconsistent-selfspark-vs-selfengine-checks","title":"Issue #6: Inconsistent self.spark vs self.engine Checks","text":"Attribute Details File <code>odibi/catalog.py</code> Lines Throughout (139, 145, 244, 282, 320, 337, 507, 516, 544, 555, etc.) Problem Code uses two patterns interchangeably without clear reasoning Impact Maintainability - inconsistent control flow makes code hard to understand <p>Current Patterns:</p> <pre><code># Pattern 1\nif self.spark:\n    # Spark logic\nelif self.engine:\n    # Engine logic\n\n# Pattern 2\nif self.spark:\n    # Spark logic\nelif self.engine and self.engine.name == \"pandas\":\n    # Pandas-specific logic\n</code></pre> <p>Suggested Fix: Standardize on single pattern using computed properties:</p> <pre><code>@property\ndef is_spark_mode(self) -&gt; bool:\n    return self.spark is not None\n\n@property\ndef is_pandas_mode(self) -&gt; bool:\n    return self.engine is not None and self.engine.name == \"pandas\"\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-7-missing-connection-closecleanup","title":"Issue #7: Missing Connection Close/Cleanup","text":"Attribute Details File <code>odibi/connections/azure_sql.py</code> Lines 573-589 (close method exists but never called) Problem <code>AzureSQL.close()</code> method exists but is never called in Pipeline/PipelineManager lifecycle Impact Resource leak - SQLAlchemy connection pools may not be properly disposed <p>Suggested Fix: Add cleanup in Pipeline/PipelineManager using context manager pattern:</p> <pre><code>class Pipeline:\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._cleanup_connections()\n\n    def _cleanup_connections(self):\n        for conn in self.connections.values():\n            if hasattr(conn, 'close'):\n                conn.close()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-8-uncached-row-count-fallback-in-schema-capture-phase","title":"Issue #8: Uncached Row Count Fallback in Schema Capture Phase","text":"Attribute Details File <code>odibi/node.py</code> Lines 227-231 Problem When read phase is skipped, row count is computed via expensive fallback Impact Performance - unnecessary Spark action <p>Current Code:</p> <pre><code>rows_in = (\n    self._read_row_count\n    if self._read_row_count is not None\n    else self._count_rows(input_df)  # Expensive fallback\n)\n</code></pre> <p>Suggested Fix: Make counting optional or ensure <code>_read_row_count</code> is always populated:</p> <pre><code>rows_in = self._read_row_count  # May be None, and that's OK\nif rows_in is None and self.performance_config.track_row_counts:\n    rows_in = self._count_rows(input_df)\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-9-dict-access-without-consistent-get-fallback","title":"Issue #9: Dict Access Without Consistent .get() Fallback","text":"Attribute Details File <code>odibi/node.py</code> Lines Various (inconsistent pattern) Problem Some connection lookups use <code>.get()</code>, others use direct access Impact Inconsistent - some missing connections raise KeyError, others return None <p>Inconsistent Patterns:</p> <pre><code># Safe pattern (used sometimes)\nconnection = self.connections.get(read_config.connection)\n\n# Unsafe pattern (used elsewhere)\nconnection = self.connections[write_config.connection]  # KeyError if missing\n</code></pre> <p>Suggested Fix: Use <code>.get()</code> consistently and check for None explicitly:</p> <pre><code>connection = self.connections.get(write_config.connection)\nif connection is None:\n    raise ValueError(f\"Connection '{write_config.connection}' not found\")\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-10-repeated-hash-calculation-pattern","title":"Issue #10: Repeated Hash Calculation Pattern","text":"Attribute Details Files <code>odibi/pipeline.py</code>, <code>odibi/catalog.py</code>, <code>odibi/node.py</code> Lines pipeline.py:400-413, catalog.py:840-848, node.py:737-745 Problem Same hash calculation pattern duplicated in multiple places Impact Code duplication, inconsistency risk <p>Duplicated Pattern:</p> <pre><code>if hasattr(config, \"model_dump\"):\n    dump = config.model_dump(mode=\"json\", exclude={\"description\", \"tags\", \"log_level\"})\nelse:\n    dump = config.dict(exclude={\"description\", \"tags\", \"log_level\"})\ndump_str = json.dumps(dump, sort_keys=True)\nversion_hash = hashlib.md5(dump_str.encode(\"utf-8\")).hexdigest()\n</code></pre> <p>Suggested Fix: Extract to shared utility function:</p> <pre><code># odibi/utils/hashing.py\ndef calculate_config_hash(config, exclude: Set[str] = None) -&gt; str:\n    exclude = exclude or {\"description\", \"tags\", \"log_level\"}\n    dump = config.model_dump(mode=\"json\", exclude=exclude) if hasattr(config, \"model_dump\") else config.dict(exclude=exclude)\n    return hashlib.md5(json.dumps(dump, sort_keys=True).encode()).hexdigest()\n</code></pre>"},{"location":"AUDIT_FINDINGS/#low-priority-minor-improvements","title":"\ud83d\udfe2 LOW PRIORITY (Minor Improvements)","text":""},{"location":"AUDIT_FINDINGS/#issue-11-import-inside-loopmethod","title":"Issue #11: Import Inside Loop/Method","text":"Attribute Details File <code>odibi/catalog.py</code> Lines 268, 508, 545, 662, 751, 871, 1000, 1085, 1160, etc. Problem <code>from pyspark.sql import functions as F</code> imported inside methods repeatedly Impact Minor performance overhead from repeated imports <p>Suggested Fix: Import once at module level with try/except guard:</p> <pre><code>try:\n    from pyspark.sql import functions as F\n    from pyspark.sql import SparkSession\n    SPARK_AVAILABLE = True\nexcept ImportError:\n    F = None\n    SparkSession = None\n    SPARK_AVAILABLE = False\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-12-duplicate-logging-context-creation","title":"Issue #12: Duplicate Logging Context Creation","text":"Attribute Details File <code>odibi/node.py</code> Lines 170-174 and 761-764 Problem <code>create_logging_context()</code> called in both <code>NodeExecutor.execute()</code> and <code>Node.execute()</code> Impact Creates redundant context objects <p>Suggested Fix: Pass context from outer to inner method:</p> <pre><code>def execute(self, ctx: Optional[LoggingContext] = None) -&gt; NodeResult:\n    ctx = ctx or create_logging_context(...)\n    result = self.executor.execute(self.config, ctx=ctx)\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-13-get_delta_history-collects-all-history","title":"Issue #13: get_delta_history Collects All History","text":"Attribute Details File <code>odibi/engine/spark_engine.py</code> Lines 806 Problem When limit is None, <code>delta_table.history()</code> returns all versions then <code>.collect()</code> brings everything to driver Impact Memory issue for tables with long history <p>Current Code:</p> <pre><code>history_df = delta_table.history(limit) if limit else delta_table.history()\nhistory = [row.asDict() for row in history_df.collect()]\n</code></pre> <p>Suggested Fix: Always require/default a reasonable limit:</p> <pre><code>DEFAULT_HISTORY_LIMIT = 100\n\ndef get_delta_history(self, connection, path, limit: int = DEFAULT_HISTORY_LIMIT):\n    history_df = delta_table.history(limit)\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-14-optional-get-without-default","title":"Issue #14: Optional .get() Without Default","text":"Attribute Details Files Various Problem Some <code>.get()</code> calls use proper defaults, others don't Impact Potential NoneType errors <p>Examples:</p> <pre><code># Good\nr.get(\"rows_processed\", 0)\nr.get(\"duration_ms\", 0)\n\n# Potentially problematic\nconfig.get(\"some_key\")  # Returns None if missing\n</code></pre> <p>Suggested Fix: Audit all <code>.get()</code> calls to ensure appropriate defaults are provided.</p>"},{"location":"AUDIT_FINDINGS/#issue-15-thread-unsafe-caching-pattern","title":"Issue #15: Thread-Unsafe Caching Pattern","text":"Attribute Details File <code>odibi/connections/azure_adls.py</code> Lines 77, 200-314 Problem <code>self._cached_key</code> used without locking in <code>get_storage_key()</code> Impact Potential race condition in parallel execution <p>Suggested Fix: Use <code>threading.Lock</code> or <code>functools.lru_cache</code>:</p> <pre><code>import threading\n\nclass AzureADLS:\n    def __init__(self, ...):\n        self._cache_lock = threading.Lock()\n        self._cached_key = None\n\n    def get_storage_key(self):\n        with self._cache_lock:\n            if self._cached_key is None:\n                self._cached_key = self._fetch_key()\n            return self._cached_key\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-16-hardcoded-magic-numbers","title":"Issue #16: Hardcoded Magic Numbers","text":"Attribute Details Files Various Problem Hardcoded values like <code>168</code> (hours), <code>1000</code> (chunk size), <code>30</code> (timeout) Impact Maintainability - hard to adjust settings <p>Examples: - <code>168</code> hours vacuum retention - <code>1000</code> chunk size for SQL writes - <code>30</code> second timeout - <code>10</code> max sample rows</p> <p>Suggested Fix: Move to configuration or named constants:</p> <pre><code># odibi/constants.py\nDEFAULT_VACUUM_RETENTION_HOURS = 168\nDEFAULT_SQL_CHUNK_SIZE = 1000\nDEFAULT_CONNECTION_TIMEOUT = 30\nDEFAULT_MAX_SAMPLE_ROWS = 10\n</code></pre>"},{"location":"AUDIT_FINDINGS/#issue-17-empty-dataframe-check-uses-isempty","title":"Issue #17: Empty DataFrame Check Uses .isEmpty()","text":"Attribute Details File <code>odibi/engine/spark_engine.py</code> Lines 597 Problem <code>df.isEmpty()</code> triggers Spark action Impact Performance cost for simple empty check <p>Suggested Fix: Use more efficient check:</p> <pre><code># Instead of\nif df.isEmpty():\n\n# Use\nif df.limit(1).count() == 0:\n# Or\nif len(df.head(1)) == 0:\n</code></pre>"},{"location":"AUDIT_FINDINGS/#recommended-priority-order","title":"Recommended Priority Order","text":"<ol> <li>Issues #1, #2, #3 (batching/performance) - Highest ROI</li> <li>Issue #7 (connection cleanup) - Reliability</li> <li>Issues #5, #6 (consistency) - Maintainability</li> <li>Issue #10 (DRY principle) - Code quality</li> <li>Other issues as time permits</li> </ol>"},{"location":"AUDIT_FINDINGS/#related-previous-fixes","title":"Related Previous Fixes","text":"<p>The audit found that some issues were already addressed: - \u2705 19 individual Delta writes \u2192 batched (fixed) - \u2705 18 individual log_run writes \u2192 <code>log_runs_batch()</code> (fixed) - \u2705 Row count caching in read phase (partially fixed)</p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/","title":"Cross-Pipeline Dependencies Plan","text":""},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#overview","title":"Overview","text":"<p>Enable pipelines to reference outputs from other pipelines using a <code>$pipeline.node</code> syntax, resolved via Odibi's internal catalog. This supports the medallion architecture pattern (bronze \u2192 silver \u2192 gold) where silver nodes depend on bronze outputs.</p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#features","title":"Features","text":""},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#1-odibi-catalog-node-outputs-registry","title":"1. Odibi Catalog: Node Outputs Registry","text":"<p>New table: <code>meta_outputs</code></p> <p>Stores output metadata for every node that has a <code>write</code> block.</p> <pre><code>Schema:\n- pipeline_name: STRING (pipeline identifier)\n- node_name: STRING (node identifier)\n- output_type: STRING (\"external_table\" | \"managed_table\")\n- connection_name: STRING (nullable, for external tables)\n- path: STRING (nullable, storage path)\n- format: STRING (delta, parquet, etc.)\n- table_name: STRING (nullable, registered table name)\n- last_run: TIMESTAMP\n- row_count: LONG (nullable)\n- updated_at: TIMESTAMP\n</code></pre> <p>Primary key: <code>(pipeline_name, node_name)</code></p> <p>Location: <code>{base_path}/meta_outputs</code></p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#2-batch-registration-performance-critical","title":"2. Batch Registration (Performance Critical)","text":"<p>Problem: Per-node catalog writes caused 40+ second overhead in Databricks (17 nodes \u00d7 ~2-3s each).</p> <p>Solution: Collect metadata in-memory during execution, single MERGE at pipeline end.</p> <pre><code># In CatalogManager\ndef register_outputs_batch(self, records: List[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Batch registers/upserts multiple node outputs to meta_outputs.\n    Uses MERGE INTO for efficient upsert.\n\n    Args:\n        records: List of dicts with keys: pipeline_name, node_name,\n                 output_type, connection_name, path, format, table_name,\n                 last_run, row_count\n    \"\"\"\n    # Same pattern as register_pipelines_batch / register_nodes_batch\n    # Single MERGE INTO delta.`{target_path}`\n</code></pre> <p>Pipeline execution flow:</p> <pre><code># In Pipeline.run()\noutput_records = []\n\nfor node in execution_order:\n    result = execute_node(node)\n\n    if node.has_write:\n        output_records.append({\n            \"pipeline_name\": self.config.pipeline,\n            \"node_name\": node.name,\n            \"output_type\": \"external_table\" if node.write.path else \"managed_table\",\n            \"connection_name\": node.write.connection,\n            \"path\": node.write.path,\n            \"format\": node.write.format,\n            \"table_name\": node.write.register_table or node.write.table,\n            \"last_run\": datetime.now(timezone.utc),\n            \"row_count\": result.row_count,\n        })\n\n# Single batch write at end\nif output_records:\n    self.catalog_manager.register_outputs_batch(output_records)\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#3-cross-pipeline-reference-syntax-pipelinenode","title":"3. Cross-Pipeline Reference Syntax (<code>$pipeline.node</code>)","text":"<p>Usage in silver pipeline:</p> <pre><code>pipelines:\n  - pipeline: transform_silver\n    layer: silver\n    nodes:\n      - name: enriched_downtime\n        inputs:\n          events: $read_bronze.opsvisdata_ShiftDowntimeEventsview\n          calendar: $read_bronze.opsvisdata_vw_calender\n          plant: $read_bronze.opsvisdata_vw_dim_plantprocess\n        transform:\n          - operation: join\n            left: events\n            right: calendar\n            on: [DateId]\n          - operation: join\n            right: plant\n            on: [P_ID]\n        write:\n          connection: goat_prod\n          format: delta\n          path: \"silver/OEE/enriched_downtime\"\n</code></pre> <p>Resolution logic:</p> <pre><code>def resolve_input_reference(ref: str, catalog: CatalogManager) -&gt; Dict:\n    \"\"\"\n    Resolves $pipeline.node to read configuration.\n\n    Args:\n        ref: Reference string like \"$read_bronze.opsvisdata_vw_calender\"\n        catalog: CatalogManager instance\n\n    Returns:\n        Dict with keys: connection, path, format, table (for engine.read())\n    \"\"\"\n    if not ref.startswith(\"$\"):\n        raise ValueError(f\"Invalid reference: {ref}\")\n\n    parts = ref[1:].split(\".\", 1)  # Remove $ and split\n    if len(parts) != 2:\n        raise ValueError(f\"Invalid reference format: {ref}. Expected $pipeline.node\")\n\n    pipeline_name, node_name = parts\n\n    # Query meta_outputs\n    output = catalog.get_node_output(pipeline_name, node_name)\n\n    if output is None:\n        raise ValueError(\n            f\"No output found for {ref}. \"\n            f\"Ensure pipeline '{pipeline_name}' has run and node '{node_name}' has a write block.\"\n        )\n\n    if output[\"output_type\"] == \"managed_table\":\n        return {\n            \"table\": output[\"table_name\"],\n            \"format\": output[\"format\"],\n        }\n    else:  # external_table\n        return {\n            \"connection\": output[\"connection_name\"],\n            \"path\": output[\"path\"],\n            \"format\": output[\"format\"],\n        }\n</code></pre> <p>CatalogManager method:</p> <pre><code>def get_node_output(self, pipeline_name: str, node_name: str) -&gt; Optional[Dict]:\n    \"\"\"\n    Retrieves output metadata for a specific node.\n\n    Returns:\n        Dict with output metadata or None if not found.\n    \"\"\"\n    # Query meta_outputs table\n    # Use caching for performance (similar to _pipelines_cache pattern)\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#4-multi-input-nodes-inputs-block","title":"4. Multi-Input Nodes (<code>inputs:</code> block)","text":"<p>Config schema:</p> <pre><code>inputs:\n  # Reference to another pipeline's node output\n  events: $read_bronze.opsvisdata_ShiftDowntimeEventsview\n\n  # Or explicit read config (for non-dependent reads)\n  calendar:\n    connection: goat_prod\n    path: \"bronze/OEE/vw_calender\"\n    format: delta\n</code></pre> <p>Execution logic:</p> <pre><code>def execute_node_with_inputs(node: NodeConfig, engine: BaseEngine, catalog: CatalogManager):\n    \"\"\"\n    Executes a node that has an inputs block.\n    \"\"\"\n    dataframes = {}\n\n    for name, ref in node.inputs.items():\n        if isinstance(ref, str) and ref.startswith(\"$\"):\n            # Cross-pipeline reference\n            read_config = resolve_input_reference(ref, catalog)\n            dataframes[name] = engine.read(**read_config)\n        elif isinstance(ref, dict):\n            # Explicit read config\n            dataframes[name] = engine.read(**ref)\n        else:\n            raise ValueError(f\"Invalid input format for '{name}': {ref}\")\n\n    # Execute transforms with named dataframes\n    result = execute_transforms(node.transforms, dataframes)\n\n    return result\n</code></pre> <p>Transform execution with named inputs:</p> <pre><code>def execute_transforms(transforms: List[TransformConfig], dataframes: Dict[str, DataFrame]):\n    \"\"\"\n    Executes transforms using named dataframes.\n\n    First transform references input names directly.\n    Subsequent transforms can reference \"_result\" for chaining.\n    \"\"\"\n    result = None\n\n    for transform in transforms:\n        if transform.operation == \"join\":\n            left_df = dataframes.get(transform.left) or result\n            right_df = dataframes.get(transform.right)\n            result = left_df.join(right_df, on=transform.on, how=transform.how or \"inner\")\n        # ... other operations\n\n    return result\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#5-config-schema-updates","title":"5. Config Schema Updates","text":"<p>NodeConfig additions:</p> <pre><code>class NodeConfig(BaseModel):\n    name: str\n    description: Optional[str] = None\n\n    # Existing\n    read: Optional[ReadConfig] = None\n    depends_on: Optional[List[str]] = None\n\n    # New: multi-input support\n    inputs: Optional[Dict[str, Union[str, ReadConfig]]] = None\n\n    transform: Optional[List[TransformConfig]] = None\n    write: Optional[WriteConfig] = None\n\n    @validator(\"inputs\", \"read\")\n    def validate_input_source(cls, v, values):\n        # Node must have either 'read' or 'inputs', not both\n        if values.get(\"read\") and values.get(\"inputs\"):\n            raise ValueError(\"Node cannot have both 'read' and 'inputs'\")\n        return v\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#engine-compatibility","title":"Engine Compatibility","text":"Feature Spark Pandas Polars <code>meta_outputs</code> writes \u2705 MERGE \u2705 upsert \u2705 upsert <code>$pipeline.node</code> (path-based) \u2705 \u2705 \u2705 <code>$pipeline.node</code> (managed table) \u2705 \u274c \u274c <code>inputs:</code> block \u2705 \u2705 \u2705 <p>Best practice: Always use <code>path:</code> in write config for cross-engine compatibility.</p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#implementation-order","title":"Implementation Order","text":""},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-1-catalog-extension-complete","title":"Phase 1: Catalog Extension \u2705 COMPLETE","text":"<ol> <li>\u2705 Add <code>meta_outputs</code> table schema to CatalogManager</li> <li>\u2705 Add <code>_get_schema_meta_outputs()</code> method</li> <li>\u2705 Add <code>register_outputs_batch()</code> method (MERGE pattern)</li> <li>\u2705 Add <code>get_node_output()</code> query method with caching</li> <li>\u2705 Update <code>bootstrap()</code> to create meta_outputs table</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-2-pipeline-integration-complete","title":"Phase 2: Pipeline Integration \u2705 COMPLETE","text":"<ol> <li>\u2705 Collect output metadata during node execution</li> <li>\u2705 Batch write to catalog at pipeline end</li> <li>\u2705 Add error handling for catalog write failures</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-3-reference-resolution-complete","title":"Phase 3: Reference Resolution \u2705 COMPLETE","text":"<ol> <li>\u2705 Add <code>resolve_input_reference()</code> function (odibi/references.py)</li> <li>\u2705 Validate references at pipeline load time (fail fast)</li> <li>\u2705 Add clear error messages for missing dependencies</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-4-multi-input-nodes-complete","title":"Phase 4: Multi-Input Nodes \u2705 COMPLETE","text":"<ol> <li>\u2705 Add <code>inputs</code> field to NodeConfig</li> <li>\u2705 Update node executor to handle inputs block (<code>_execute_inputs_phase()</code>)</li> <li>\u2705 Update transform executor for named dataframes</li> <li>\u2705 Add validation: node has either <code>read</code> or <code>inputs</code>, not both</li> </ol>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#phase-5-testing-complete","title":"Phase 5: Testing \u2705 COMPLETE","text":"<ol> <li>\u2705 Unit tests for <code>register_outputs_batch()</code></li> <li>\u2705 Unit tests for <code>resolve_input_reference()</code></li> <li>\u2705 Integration test: bronze pipeline writes, silver reads via <code>$ref</code></li> <li>\u2705 Performance test: verify single batch write (no per-node overhead)</li> </ol> <p>Implementation completed: All 26 tests passing in <code>tests/unit/test_cross_pipeline_dependencies.py</code></p>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#example-full-bronze-silver-flow","title":"Example: Full Bronze \u2192 Silver Flow","text":"<p>Bronze pipeline (<code>read_bronze.yaml</code>):</p> <pre><code>pipeline: read_bronze\nlayer: bronze\nnodes:\n  - name: opsvisdata_ShiftDowntimeEventsview\n    read:\n      connection: opsvisdata\n      format: sql\n      table: OEE.ShiftDowntimeEventsview\n    write:\n      connection: goat_prod\n      format: delta\n      path: \"bronze/OEE/shift_downtime_events\"\n      register_table: test.shift_downtime_events\n      add_metadata: true\n\n  - name: opsvisdata_vw_calender\n    read:\n      connection: opsvisdata\n      format: sql\n      table: OEE.vw_calender\n    write:\n      connection: goat_prod\n      format: delta\n      path: \"bronze/OEE/vw_calender\"\n      register_table: test.vw_calender\n</code></pre> <p>After bronze runs, <code>meta_outputs</code> contains:</p> <pre><code>| pipeline_name | node_name                           | connection_name | path                           | format | table_name                  |\n|---------------|-------------------------------------|-----------------|--------------------------------|--------|-----------------------------|\n| read_bronze   | opsvisdata_ShiftDowntimeEventsview  | goat_prod       | bronze/OEE/shift_downtime_events | delta  | test.shift_downtime_events |\n| read_bronze   | opsvisdata_vw_calender              | goat_prod       | bronze/OEE/vw_calender         | delta  | test.vw_calender            |\n</code></pre> <p>Silver pipeline (<code>transform_silver.yaml</code>):</p> <pre><code>pipeline: transform_silver\nlayer: silver\nnodes:\n  - name: enriched_events\n    inputs:\n      events: $read_bronze.opsvisdata_ShiftDowntimeEventsview\n      calendar: $read_bronze.opsvisdata_vw_calender\n    transform:\n      - operation: join\n        left: events\n        right: calendar\n        on: [DateId]\n    write:\n      connection: goat_prod\n      format: delta\n      path: \"silver/OEE/enriched_events\"\n</code></pre> <p>Resolution:</p> <pre><code>$read_bronze.opsvisdata_ShiftDowntimeEventsview\n  \u2192 catalog lookup \u2192 {connection: goat_prod, path: bronze/OEE/shift_downtime_events, format: delta}\n  \u2192 engine.read(connection=goat_prod, path=\"bronze/OEE/shift_downtime_events\", format=\"delta\")\n</code></pre>"},{"location":"CROSS_PIPELINE_DEPENDENCIES_PLAN/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Batch writes only \u2014 never write to catalog per-node</li> <li>Cache catalog queries \u2014 <code>get_node_output()</code> should cache results</li> <li>Validate early \u2014 resolve all <code>$references</code> at pipeline load, not execution</li> <li>Fail fast \u2014 clear errors if referenced pipeline/node hasn't run</li> </ol>"},{"location":"DOCUMENTATION_CAMPAIGN/","title":"Odibi Documentation Campaign","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#goal","title":"Goal","text":"<p>Make all documentation accurate, consistent, and professional - matching the quality of the codebase (958 tests passing).</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#context","title":"Context","text":"<p>Current State: 155+ markdown files across the project with significant gaps: - Broken links and missing files in mkdocs.yml - Duplicate files (yaml_schema.md in two locations) - Archived work mixed with user-facing docs - Root tracking files (GAPS.md, STABILITY_CAMPAIGN.md) outdated - GitHub issues #7-31 document specific gaps</p> <p>Success Criteria: - [x] <code>mkdocs build</code> runs without errors - [x] All nav links resolve to existing files - [x] No duplicate or broken files - [x] Root tracking files reflect current state - [x] GitHub documentation issues addressed or closed</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-1-cleanup-deletemove","title":"Phase 1: Cleanup (Delete/Move)","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#files-to-delete","title":"Files to Delete","text":"File Reason <code>docs/yaml_schema.md</code> Contains Python traceback error (3 lines)"},{"location":"DOCUMENTATION_CAMPAIGN/#files-to-move-to-_archive","title":"Files to Move to <code>_archive/</code>","text":"Source Destination <code>docs/archive/</code> (16 files) <code>_archive/docs/</code> <code>odibi/agents/docs/</code> (22 files) <code>_archive/agents/docs/</code> <code>odibi/agents/ROADMAP.md</code> <code>_archive/agents/</code> <code>odibi/agents/AUDIT_FIX_PLAN.md</code> <code>_archive/agents/</code>"},{"location":"DOCUMENTATION_CAMPAIGN/#duplicates-to-resolve","title":"Duplicates to Resolve","text":"File 1 File 2 Action <code>docs/WSL_SETUP.md</code> <code>docs/guides/wsl_setup.md</code> Delete <code>docs/WSL_SETUP.md</code>, keep guides version"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-2-fix-mkdocsyml","title":"Phase 2: Fix mkdocs.yml","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#broken-nav-links","title":"Broken Nav Links","text":"Current (broken) Fix <code>docs/patterns/hwm_pattern_guide.md</code> Change to <code>docs/patterns/incremental_stateful.md</code> <code>docs/guides/contributing.md</code> Change to <code>CONTRIBUTING.md</code> (root file)"},{"location":"DOCUMENTATION_CAMPAIGN/#missing-patterns-add-to-nav","title":"Missing Patterns (add to nav)","text":"<pre><code># Add under Patterns section:\n- SCD2 Pattern: docs/patterns/scd2.md\n- Incremental Stateful: docs/patterns/incremental_stateful.md\n- Smart Read: docs/patterns/smart_read.md\n- Skip If Unchanged: docs/patterns/skip_if_unchanged.md\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#missing-features-section-add-to-nav","title":"Missing Features Section (add to nav)","text":"<pre><code>- Features:\n    - Engines: docs/features/engines.md\n    - Pipelines: docs/features/pipelines.md\n    - Connections: docs/features/connections.md\n    - CLI: docs/features/cli.md\n    - Validation: docs/features/quality_gates.md\n    - Quarantine: docs/features/quarantine.md\n    - Alerting: docs/features/alerting.md\n    - Stories: docs/features/stories.md\n    - Catalog: docs/features/catalog.md\n    - Lineage: docs/features/lineage.md\n    - Transformers: docs/features/transformers.md\n    - Patterns: docs/features/patterns.md\n    - Schema Tracking: docs/features/schema_tracking.md\n    - Diagnostics: docs/features/diagnostics.md\n    - Orchestration: docs/features/orchestration.md\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-3-update-root-tracking-files","title":"Phase 3: Update Root Tracking Files","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#gapsmd","title":"GAPS.md","text":"<p>Verify each gap against current code:</p> Gap Status to Verify GAP-001: datetime.utcnow deprecation Check if fixed GAP-002: Pydantic .dict() Check if fixed GAP-003: Pandas FutureWarning Check if fixed GAP-004: Polars API deprecation Check if fixed GAP-005: GitHub Events dataset Check if exists GAP-006-010 Verify current state <p>Update checkboxes in \"Roadmap Recommendations\" section.</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#stability_campaignmd","title":"STABILITY_CAMPAIGN.md","text":"<p>Update success criteria checkboxes: - [x] All tests passing (958 on Windows) - [x] BUGS.md addressed (24 fixed) - [ ] Review remaining items</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#readmemd","title":"README.md","text":"<p>Fix broken documentation links:</p> Current (broken) Fix <code>docs/guides/cli_master.md</code> <code>docs/guides/cli_master_guide.md</code> <code>docs/guides/writing-transformations.md</code> <code>docs/guides/writing_transformations.md</code>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-4-audit-reference-docs","title":"Phase 4: Audit Reference Docs","text":"<p>Verify against actual code implementation:</p> File Verify Against <code>docs/reference/yaml_schema.md</code> <code>odibi/config.py</code> Pydantic models <code>docs/reference/cheatsheet.md</code> CLI commands, YAML options <code>docs/reference/configuration.md</code> <code>odibi/config.py</code> <code>docs/reference/PARITY_TABLE.md</code> Actual engine implementations"},{"location":"DOCUMENTATION_CAMPAIGN/#yaml_schemamd-regeneration","title":"yaml_schema.md Regeneration","text":"<p>Per AGENTS.md, regenerate from Pydantic models:</p> <pre><code>python odibi/introspect.py\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-5-audit-guides","title":"Phase 5: Audit Guides","text":"<p>For each guide, verify accuracy:</p> Guide Key Checks <code>cli_master_guide.md</code> All commands exist and work <code>writing_transformations.md</code> Registration process accurate <code>production_deployment.md</code> Polars mentioned? Engine options correct? <code>performance_tuning.md</code> Polars optimization section? <code>secrets.md</code> Implementation details accurate? <code>MIGRATION_GUIDE.md</code> V3 references still valid? <code>best_practices.md</code> Validation best practices included?"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-6-audit-feature-docs","title":"Phase 6: Audit Feature Docs","text":"<p>For each feature doc, verify: 1. Feature exists in code 2. Configuration examples work 3. No references to non-existent features</p> Feature Doc Priority Issue # <code>engines.md</code> HIGH #13 (Polars missing) <code>quarantine.md</code> HIGH #23 (Integration unclear) <code>quality_gates.md</code> MEDIUM #11 (Error handling) <code>schema_tracking.md</code> MEDIUM #31 (Not documented) <code>cli.md</code> MEDIUM #14 (Commands may not exist)"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-7-address-github-issues","title":"Phase 7: Address GitHub Issues","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#documentation-issues-to-closeupdate","title":"Documentation Issues to Close/Update","text":"Issue Title Action #7 Configuration.md Incomplete Audit and update #8 Getting Started Missing Validation Add example #9 README.md Commands Outdated Verify commands #10 FK Validation Integration Unclear Update docs/validation/fk.md #11 Validation Error Handling Not Documented Add to feature doc #13 Polars Engine Missing from Docs Add to engines.md #14 CLI Guide References Non-Existent Commands Audit CLI #15 Version Mismatch Between Docs Standardize versions #16 Secrets Management Missing Details Update secrets.md #17 Patterns Doc References Non-Existent File Fix reference #18 Engine Parity Table Incomplete Update PARITY_TABLE.md #19 Architecture Doc May Be Outdated Audit architecture.md #20 Date Dimension Config Not Documented Add to pattern doc #21 Custom SQL Transformation Unclear Update guide #23 Quarantine Not Integrated with Validation Guide Cross-link docs #24 Performance Guide Missing Polars Add Polars section #25 Writing Transformations Missing Registration Add registration #26 Migration Guide V3 Outdated Update or note #27 Best Practices Missing Validation Add section #28 Production Deployment References Polars Verify/add #29 CLI Commands May Not Match Audit cheatsheet #30 Cross-Env Secrets Not Documented Add to secrets.md #31 Schema Tracking Not Documented Create/update doc"},{"location":"DOCUMENTATION_CAMPAIGN/#phase-8-polish","title":"Phase 8: Polish","text":"<ol> <li>Consistent formatting across all docs</li> <li>Cross-links between related docs</li> <li>Navigation - ensure logical flow</li> <li>Build test - <code>mkdocs build</code> succeeds</li> <li>Visual review - <code>mkdocs serve</code> and check</li> </ol>"},{"location":"DOCUMENTATION_CAMPAIGN/#files-summary","title":"Files Summary","text":""},{"location":"DOCUMENTATION_CAMPAIGN/#keep-user-facing","title":"Keep (User-Facing)","text":"<pre><code>docs/\n\u251c\u2500\u2500 tutorials/          # 14 files - Getting started, dimensional modeling\n\u251c\u2500\u2500 guides/             # 15 files - How-to guides\n\u251c\u2500\u2500 reference/          # 9 files - API, schema, cheatsheet\n\u251c\u2500\u2500 patterns/           # 13 files - Pattern documentation\n\u251c\u2500\u2500 features/           # 15 files - Feature documentation\n\u251c\u2500\u2500 semantics/          # 4 files - Semantic layer\n\u251c\u2500\u2500 validation/         # 1 file - FK validation\n\u251c\u2500\u2500 explanation/        # 2 files - Architecture, case studies\n\u2514\u2500\u2500 README.md           # Docs index\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#delete","title":"Delete","text":"<pre><code>docs/yaml_schema.md     # Broken (contains traceback)\ndocs/WSL_SETUP.md       # Duplicate of guides/wsl_setup.md\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#move-to-_archive","title":"Move to _archive/","text":"<pre><code>docs/archive/           # 16 historical gap analysis files\nodibi/agents/docs/      # 22 archived agent docs\nodibi/agents/*.md       # Agent planning docs\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#keep-but-update","title":"Keep but Update","text":"<pre><code>GAPS.md                 # Update status of each gap\nSTABILITY_CAMPAIGN.md   # Check off completed items\nREADME.md               # Fix broken links\nBUGS.md                 # Keep as-is (accurate)\nCHANGELOG.md            # Keep as-is (accurate)\n</code></pre>"},{"location":"DOCUMENTATION_CAMPAIGN/#estimated-effort","title":"Estimated Effort","text":"Phase Effort Files 1. Cleanup 15 min ~40 2. mkdocs.yml 15 min 1 3. Root files 30 min 3 4. Reference audit 1 hr 5 5. Guides audit 1.5 hr 15 6. Features audit 1 hr 15 7. GitHub issues 2 hr 25 issues 8. Polish 30 min All <p>Total: ~7 hours</p>"},{"location":"DOCUMENTATION_CAMPAIGN/#success-metrics","title":"Success Metrics","text":"Metric Before After mkdocs build errors Unknown 0 Broken nav links 2 0 Duplicate files 2 0 GitHub doc issues open 25 0 Pattern docs in nav 8 13 Feature docs in nav 0 15"},{"location":"FEATURE_PARITY_AUDIT/","title":"Odibi Feature Parity Audit Report","text":"<p>Generated: 2024 Scope: Config vs Implementation, Engine Parity, CLI vs API, Docs vs Code</p>"},{"location":"FEATURE_PARITY_AUDIT/#executive-summary","title":"Executive Summary","text":"<p>This audit identifies feature parity gaps across the Odibi framework. Issues are categorized by severity:</p> Severity Count Description \ud83d\udd34 Critical 3 Documented but not implemented, or silently ignored \ud83d\udfe1 Medium 8 Missing in one engine/mode but present in another \ud83d\udfe2 Low 4 Minor inconsistencies or missing documentation"},{"location":"FEATURE_PARITY_AUDIT/#critical-issues","title":"\ud83d\udd34 Critical Issues","text":""},{"location":"FEATURE_PARITY_AUDIT/#1-cli-missing-tag-flag-documented-but-not-implemented","title":"1. CLI Missing <code>--tag</code> Flag (Documented but Not Implemented)","text":"<p>Location: <code>odibi/cli/main.py</code> (run command)</p> <p>Issue: The documentation and config reference describe tag-based execution (<code>odibi run --tag daily</code>), but the <code>--tag</code> flag is not implemented in the CLI.</p> <p>Evidence: - Config: <code>NodeConfig.tags</code> field exists (config.py#L2030) - Docs: \"Run only this with <code>odibi run --tag daily</code>\" (yaml_schema.md#L127) - CLI: No <code>--tag</code> argument in cli/main.py</p> <p>Impact: Users cannot run pipelines by tag despite documentation saying they can.</p> <p>Fix: Add <code>--tag</code> argument to the run parser and filter nodes in <code>Pipeline.run()</code>.</p>"},{"location":"FEATURE_PARITY_AUDIT/#2-cli-missing-node-and-pipeline-flags-for-run-command","title":"2. CLI Missing <code>--node</code> and <code>--pipeline</code> Flags for Run Command","text":"<p>Location: <code>odibi/cli/main.py</code> (run command)</p> <p>Issue: Orchestration templates (<code>odibi/orchestration/airflow.py</code>, <code>odibi/orchestration/dagster.py</code>) generate commands like:</p> <pre><code>odibi run --pipeline {{ pipeline_name }} --node {{ node.name }}\n</code></pre> <p>But these flags do not exist in the CLI.</p> <p>Evidence: - airflow.py#L39: <code>--pipeline</code> and <code>--node</code> usage - dagster.py#L61: <code>--pipeline</code> and <code>--node</code> usage - CLI main.py: No such flags exist</p> <p>Impact: Generated Airflow/Dagster DAGs will fail to execute.</p> <p>Fix: Add <code>--pipeline</code> and <code>--node</code> arguments to the run command.</p>"},{"location":"FEATURE_PARITY_AUDIT/#3-materialized-config-option-has-no-effect","title":"3. <code>materialized</code> Config Option Has No Effect","text":"<p>Location: <code>odibi/config.py</code> (NodeConfig), <code>odibi/node.py</code></p> <p>Issue: The <code>materialized</code> field (<code>table</code>, <code>view</code>, <code>incremental</code>) is defined in config but never read or used in node execution.</p> <p>Evidence: - Config: config.py#L2018 defines <code>materialized</code> field - Node.py: No references to <code>config.materialized</code> in execution logic - Docs: Referenced in yaml_schema.md#L338</p> <p>Impact: Users can set this option but it's silently ignored.</p> <p>Fix: Either implement materialization strategies or remove/deprecate the field.</p>"},{"location":"FEATURE_PARITY_AUDIT/#medium-issues","title":"\ud83d\udfe1 Medium Issues","text":""},{"location":"FEATURE_PARITY_AUDIT/#4-polars-engine-not-exposed-in-config-enum","title":"4. Polars Engine Not Exposed in Config Enum","text":"<p>Location: <code>odibi/config.py</code>, <code>odibi/enums.py</code></p> <p>Issue: <code>PolarsEngine</code> exists and is registered, but <code>EngineType</code> enum only has <code>SPARK</code> and <code>PANDAS</code>.</p> <p>Evidence: - polars_engine.py: Full implementation exists - engine/registry.py#L25: Registered as \"polars\" - config.py#L14-L18: Only SPARK/PANDAS</p> <p>Impact: Users cannot set <code>engine: polars</code> in YAML config.</p> <p>Fix: Add <code>POLARS = \"polars\"</code> to <code>EngineType</code> enum.</p>"},{"location":"FEATURE_PARITY_AUDIT/#5-streaming-write-not-implemented-read-only-fixed","title":"5. ~~Streaming Write Not Implemented (Read Only)~~ \u2705 FIXED","text":"<p>Location: <code>odibi/engine/spark_engine.py</code>, <code>odibi/config.py</code></p> <p>Status: \u2705 Implemented</p> <p>Implementation: - Added <code>StreamingWriteConfig</code> and <code>TriggerConfig</code> models in <code>config.py</code> - Added <code>streaming</code> field to <code>WriteConfig</code> - Implemented <code>_write_streaming()</code> method in <code>SparkEngine</code> - Supports all trigger types: <code>processing_time</code>, <code>once</code>, <code>available_now</code>, <code>continuous</code> - Supports all output modes: <code>append</code>, <code>update</code>, <code>complete</code></p> <p>Usage:</p> <pre><code>write:\n  connection: delta_lake\n  format: delta\n  table: events_stream\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events\"\n    trigger:\n      processing_time: \"10 seconds\"\n</code></pre>"},{"location":"FEATURE_PARITY_AUDIT/#6-partition_by-not-applied-for-delta-merge-operations","title":"6. <code>partition_by</code> Not Applied for Delta Merge Operations","text":"<p>Location: <code>odibi/engine/pandas_engine.py</code>, <code>odibi/engine/spark_engine.py</code></p> <p>Issue: For <code>upsert</code>/<code>append_once</code> modes with Delta tables, <code>partition_by</code> is extracted but applied after the merge logic completes. The merge itself doesn't partition-optimize.</p> <p>Evidence: - Spark: spark_engine.py#L800-L805 - partition_by applied post-merge - Pandas: Delta merge at pandas_engine.py#L773-L810 doesn't use partition_by</p> <p>Impact: Performance not optimal for partitioned upsert operations.</p>"},{"location":"FEATURE_PARITY_AUDIT/#7-pandasengine-missing-as_of_timestamp-time-travel","title":"7. PandasEngine Missing <code>as_of_timestamp</code> Time Travel","text":"<p>Location: <code>odibi/engine/pandas_engine.py</code></p> <p>Issue: <code>as_of_timestamp</code> is accepted but only <code>as_of_version</code> (versionAsOf) is implemented for Delta time travel.</p> <p>Evidence: - Method signature accepts both: pandas_engine.py#L187 - Only version is used: pandas_engine.py#L246-L247 - SparkEngine supports both: spark_engine.py#L443-L448</p> <p>Impact: Timestamp-based time travel silently ignored in Pandas engine.</p>"},{"location":"FEATURE_PARITY_AUDIT/#8-polarsengine-missing-several-abstract-methods","title":"8. PolarsEngine Missing Several Abstract Methods","text":"<p>Location: <code>odibi/engine/polars_engine.py</code></p> <p>Issue: PolarsEngine doesn't implement all abstract methods from <code>Engine</code> base class: - <code>harmonize_schema()</code> - Not implemented - <code>anonymize()</code> - Not implemented - <code>add_write_metadata()</code> - Not implemented - <code>table_exists()</code> - Partial (file only) - <code>maintain_table()</code> - Not implemented</p> <p>Impact: Polars pipelines will fail if these features are used.</p>"},{"location":"FEATURE_PARITY_AUDIT/#9-first_run_query-only-works-with-target-table-check","title":"9. <code>first_run_query</code> Only Works with Target Table Check","text":"<p>Location: <code>odibi/node.py</code> (NodeExecutor._execute_read_phase)</p> <p>Issue: <code>first_run_query</code> is only applied when <code>write</code> config exists and target table doesn't exist. If a node has <code>read</code> + <code>transform</code> but no <code>write</code>, the query is never used.</p> <p>Evidence: node.py#L344-L352</p>"},{"location":"FEATURE_PARITY_AUDIT/#10-delete_detection-config-never-read","title":"10. <code>delete_detection</code> Config Never Read","text":"<p>Location: <code>odibi/config.py</code>, <code>odibi/node.py</code></p> <p>Issue: <code>DeleteDetectionConfig</code> is fully defined with validation, but the node executor never reads or applies delete detection logic.</p> <p>Evidence: - Config: config.py#L160-L293 - full implementation - Transformer exists: transformers/delete_detection.py - Not wired: Must be used as explicit transform step, not automatic</p> <p>Clarification: This works if user explicitly adds <code>operation: detect_deletes</code> in transform steps. The config examples show this is the intended pattern.</p>"},{"location":"FEATURE_PARITY_AUDIT/#11-environments-config-not-validated-or-applied","title":"11. <code>environments</code> Config Not Validated or Applied","text":"<p>Location: <code>odibi/config.py</code> (ProjectConfig)</p> <p>Issue: The <code>environments</code> field is defined but has a validator that does nothing:</p> <pre><code>@model_validator(mode=\"after\")\ndef check_environments_not_implemented(self):\n    # Implemented in Phase 3\n    return self\n</code></pre> <p>Evidence: config.py#L980-L984</p> <p>Impact: Users can define environments but they're silently ignored.</p>"},{"location":"FEATURE_PARITY_AUDIT/#low-issues","title":"\ud83d\udfe2 Low Issues","text":""},{"location":"FEATURE_PARITY_AUDIT/#12-docs-reference-non-existent-commands","title":"12. Docs Reference Non-Existent Commands","text":"<p>Location: <code>docs/guides/cli_master_guide.md</code></p> <p>Issue: Docs reference <code>odibi init-vscode</code> but this command doesn't exist in CLI.</p> <p>Evidence: cli_master_guide.md#L56-L59</p>"},{"location":"FEATURE_PARITY_AUDIT/#13-retrybackoff-types-inconsistent","title":"13. <code>retry.backoff</code> Types Inconsistent","text":"<p>Location: <code>odibi/config.py</code>, <code>odibi/node.py</code></p> <p>Issue: Config defines <code>BackoffStrategy</code> enum with values <code>exponential</code>, <code>linear</code>, <code>constant</code>, but node retry logic only handles <code>exponential</code> and <code>linear</code>.</p> <p>Evidence: - Config: config.py#L730-L733 - includes <code>CONSTANT</code> - Node: node.py#L963-L968 - no <code>constant</code> handling</p>"},{"location":"FEATURE_PARITY_AUDIT/#14-prepost-sql-not-documented-in-cli-docs","title":"14. Pre/Post SQL Not Documented in CLI Docs","text":"<p>Location: <code>docs/features/cli.md</code></p> <p>Issue: <code>pre_sql</code> and <code>post_sql</code> node options are implemented but not mentioned in CLI documentation.</p>"},{"location":"FEATURE_PARITY_AUDIT/#15-validation_mode-on-connections-never-used","title":"15. <code>validation_mode</code> on Connections Never Used","text":"<p>Location: <code>odibi/config.py</code> (BaseConnectionConfig)</p> <p>Issue: All connection configs inherit <code>validation_mode: ValidationMode</code> but this field is never read during pipeline execution.</p>"},{"location":"FEATURE_PARITY_AUDIT/#write-mode-support-matrix","title":"Write Mode Support Matrix","text":"Mode PandasEngine SparkEngine Notes <code>overwrite</code> \u2705 \u2705 Both support all formats <code>append</code> \u2705 \u2705 Both support all formats <code>upsert</code> \u2705 Delta \u2705 Delta Both require <code>keys</code> option <code>append_once</code> \u2705 Delta \u2705 Delta Both require <code>keys</code> option <code>partition_by</code> \u2705 \u2705 Applied via options dict <code>merge_keys</code> \u274c \u274c Use <code>keys</code> in options instead"},{"location":"FEATURE_PARITY_AUDIT/#cli-vs-api-feature-matrix","title":"CLI vs API Feature Matrix","text":"Feature CLI Python API Notes Run pipeline \u2705 \u2705 Dry run \u2705 \u2705 Resume \u2705 \u2705 Parallel \u2705 \u2705 Filter by tag \u274c \u274c Not implemented Filter by node \u274c \u274c Not implemented Filter by pipeline \u274c \u2705 CLI doesn't support On-error override \u2705 \u2705"},{"location":"FEATURE_PARITY_AUDIT/#recommendations","title":"Recommendations","text":""},{"location":"FEATURE_PARITY_AUDIT/#high-priority","title":"High Priority","text":"<ol> <li>Add <code>--tag</code>, <code>--node</code>, <code>--pipeline</code> to CLI run command - Breaks orchestration exports</li> <li>Implement or remove <code>materialized</code> config - Silent no-op confuses users</li> <li>Add POLARS to EngineType enum - Blocks Polars adoption</li> </ol>"},{"location":"FEATURE_PARITY_AUDIT/#medium-priority","title":"Medium Priority","text":"<ol> <li>Implement streaming write support in SparkEngine</li> <li>Add <code>as_of_timestamp</code> support to PandasEngine</li> <li>Complete PolarsEngine abstract method implementations</li> </ol>"},{"location":"FEATURE_PARITY_AUDIT/#low-priority","title":"Low Priority","text":"<ol> <li>Add <code>constant</code> backoff handling</li> <li>Document pre_sql/post_sql in CLI docs</li> <li>Remove or implement <code>validation_mode</code> on connections</li> <li>Remove or implement <code>environments</code> config</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/","title":"Odibi Logging Audit Report","text":"<p>Date: 2025-12-01 Status: Complete Test Results: 655 passed, 0 failed, 20 skipped</p>"},{"location":"LOGGING_AUDIT_REPORT/#executive-summary","title":"Executive Summary","text":"<p>This audit enhanced the Odibi framework with comprehensive, structured logging across all major modules. The goal was to eliminate silent failures, provide better observability, and enable effective debugging in production environments.</p>"},{"location":"LOGGING_AUDIT_REPORT/#key-achievements","title":"Key Achievements","text":"<ul> <li>Created new <code>LoggingContext</code> infrastructure for context-aware logging</li> <li>Added structured logging with timing, row counts, and schema tracking</li> <li>Implemented secret redaction to prevent credential leaks</li> <li>Enhanced ~25 modules with detailed operation logging</li> <li>Fixed all silent failure patterns identified</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#new-infrastructure","title":"New Infrastructure","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibiutilslogging_contextpy","title":"<code>odibi/utils/logging_context.py</code>","text":"<p>A new module providing context-based logging with automatic correlation:</p> Component Description <code>OperationType</code> Enum categorizing operations (READ, WRITE, TRANSFORM, VALIDATE, etc.) <code>OperationMetrics</code> Dataclass tracking timing, row counts, schema changes, memory <code>StructuredLogger</code> Base logger supporting human-readable and JSON output with secret redaction <code>LoggingContext</code> Context-aware wrapper with pipeline/node/engine tracking <p>Key Features: - Automatic context injection (pipeline_id, node_id, engine) - Operation timing with millisecond precision - Row count tracking with delta calculations - Schema change detection (columns added/removed/type changes) - Secret redaction for connection strings and credentials</p> <p>Helper Methods: - <code>log_file_io()</code> - File read/write operations - <code>log_spark_metrics()</code> - Spark-specific metrics (partitions, broadcasts) - <code>log_pandas_metrics()</code> - Pandas-specific metrics (memory, dtypes) - <code>log_validation_result()</code> - Validation pass/fail with details - <code>log_graph_operation()</code> - Dependency graph operations - <code>log_connection()</code> - Connection lifecycle events - <code>log_schema_change()</code> - Schema transformation tracking - <code>log_row_count_change()</code> - Row count deltas with warnings</p>"},{"location":"LOGGING_AUDIT_REPORT/#enhanced-modules","title":"Enhanced Modules","text":""},{"location":"LOGGING_AUDIT_REPORT/#core-pipeline-components","title":"Core Pipeline Components","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibigraphpy","title":"<code>odibi/graph.py</code>","text":"<ul> <li>Added: Node count, edge count, layer count logging</li> <li>Added: Cycle detection with node names in error messages</li> <li>Added: Resolution phase timing</li> <li>Fixed: Silent failures in dependency resolution</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibinodepy","title":"<code>odibi/node.py</code>","text":"<ul> <li>Added: Per-phase logging (source_read, transform, validation, sink_write)</li> <li>Added: Row count tracking through execution phases</li> <li>Added: Schema change logging between transformations</li> <li>Added: Timing metrics for each phase</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibipipelinepy","title":"<code>odibi/pipeline.py</code>","text":"<ul> <li>Added: Pipeline start/end logging with run IDs</li> <li>Added: Layer-by-layer execution progress</li> <li>Added: Parallel node execution tracking</li> <li>Added: Summary metrics (total nodes, duration, failures)</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#configuration-utilities","title":"Configuration &amp; Utilities","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibiutilsconfig_loaderpy","title":"<code>odibi/utils/config_loader.py</code>","text":"<ul> <li>Added: YAML file loading with path logging</li> <li>Added: Environment variable substitution tracking</li> <li>Added: Missing env var warnings with variable names</li> <li>Added: Schema validation result logging</li> <li>Fixed: Silent env var substitution failures</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#validation-engine","title":"Validation Engine","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibivalidationenginepy","title":"<code>odibi/validation/engine.py</code>","text":"<ul> <li>Added: Rule-by-rule validation logging</li> <li>Added: Pass/fail counts per rule</li> <li>Added: Failure row details (configurable limit)</li> <li>Added: Timing for expensive validations</li> <li>Fixed: Silent validation failures now logged with details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#engine-layer","title":"Engine Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibienginepandas_enginepy","title":"<code>odibi/engine/pandas_engine.py</code>","text":"<ul> <li>Added: File I/O logging with format, path, row counts</li> <li>Added: Memory usage warnings (&gt;1GB threshold)</li> <li>Added: SQL execution logging with query previews</li> <li>Added: Chunk processing progress for large files</li> <li>Fixed: Silent read failures now include file path and error details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibienginespark_enginepy","title":"<code>odibi/engine/spark_engine.py</code>","text":"<ul> <li>Added: Partition count logging for reads/writes</li> <li>Added: JDBC connection logging (redacted credentials)</li> <li>Added: Delta Lake operation metrics (merge, optimize, vacuum)</li> <li>Added: Streaming batch progress logging</li> <li>Added: Broadcast join size tracking</li> <li>Fixed: Silent JDBC failures now include connection details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#connection-layer","title":"Connection Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionsfactorypy","title":"<code>odibi/connections/factory.py</code>","text":"<ul> <li>Added: Connection type resolution logging</li> <li>Added: Connection creation/reuse tracking</li> <li>Added: Failed connection attempts with error details</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionslocalpy","title":"<code>odibi/connections/local.py</code>","text":"<ul> <li>Added: File system path resolution logging</li> <li>Added: Directory creation logging</li> <li>Added: File existence checks with paths</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionsazure_adlspy","title":"<code>odibi/connections/azure_adls.py</code>","text":"<ul> <li>Added: Storage account connection logging</li> <li>Added: Container/path resolution logging</li> <li>Added: Credential type detection (SAS, OAuth, Key)</li> <li>Added: Secret redaction for connection strings</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibiconnectionsazure_sqlpy","title":"<code>odibi/connections/azure_sql.py</code>","text":"<ul> <li>Added: Server/database connection logging</li> <li>Added: Authentication method logging (SQL Auth, AAD)</li> <li>Added: JDBC/ODBC driver selection logging</li> <li>Added: Query execution timing</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#transformer-layer","title":"Transformer Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersrelationalpy","title":"<code>odibi/transformers/relational.py</code>","text":"<ul> <li>Added: Join operation logging with key columns</li> <li>Added: Row count before/after joins</li> <li>Added: Filter operation logging with predicate info</li> <li>Added: Aggregation logging with group-by columns</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersadvancedpy","title":"<code>odibi/transformers/advanced.py</code>","text":"<ul> <li>Added: Window function logging</li> <li>Added: Pivot/unpivot operation tracking</li> <li>Added: Complex expression evaluation logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersscdpy","title":"<code>odibi/transformers/scd.py</code>","text":"<ul> <li>Added: SCD Type 1/2 operation logging</li> <li>Added: Row version tracking</li> <li>Added: Effective date range logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersmerge_transformerpy","title":"<code>odibi/transformers/merge_transformer.py</code>","text":"<ul> <li>Added: Merge key logging</li> <li>Added: Insert/update/delete counts</li> <li>Added: Conflict resolution logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformersvalidationpy","title":"<code>odibi/transformers/validation.py</code>","text":"<ul> <li>Added: Validation rule application logging</li> <li>Added: Quarantine row tracking</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibitransformerssql_corepy","title":"<code>odibi/transformers/sql_core.py</code>","text":"<ul> <li>Added: SQL generation logging</li> <li>Added: Query execution timing</li> <li>Added: Result set size logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#pattern-layer","title":"Pattern Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibipatternsbasepy","title":"<code>odibi/patterns/base.py</code>","text":"<ul> <li>Added: Pattern instantiation logging</li> <li>Added: Configuration validation logging</li> <li>Added: Execution phase logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibipatternsscd2py","title":"<code>odibi/patterns/scd2.py</code>","text":"<ul> <li>Added: Slowly changing dimension processing logging</li> <li>Added: New/changed/expired row counts</li> <li>Added: Surrogate key generation logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibipatternsmergepy","title":"<code>odibi/patterns/merge.py</code>","text":"<ul> <li>Added: Merge pattern execution logging</li> <li>Added: Match condition logging</li> <li>Added: Update/insert clause logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#storydocumentation-layer","title":"Story/Documentation Layer","text":""},{"location":"LOGGING_AUDIT_REPORT/#odibistorygeneratorpy","title":"<code>odibi/story/generator.py</code>","text":"<ul> <li>Added: Story generation logging</li> <li>Added: Template rendering logging</li> <li>Added: Output file creation logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibistoryrendererspy","title":"<code>odibi/story/renderers.py</code>","text":"<ul> <li>Added: Renderer selection logging</li> <li>Added: Format conversion logging</li> <li>Added: Asset embedding logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#odibistorymetadatapy","title":"<code>odibi/story/metadata.py</code>","text":"<ul> <li>Added: Metadata extraction logging</li> <li>Added: Schema inference logging</li> <li>Added: Lineage tracking logging</li> </ul>"},{"location":"LOGGING_AUDIT_REPORT/#silent-failure-patterns-fixed","title":"Silent Failure Patterns Fixed","text":"Module Issue Fix <code>config_loader</code> Missing env vars silently returned empty string Now logs warning with variable name <code>pandas_engine</code> File read errors lost path context Error now includes full path and format <code>spark_engine</code> JDBC failures had no connection context Errors include server/database (redacted creds) <code>validation/engine</code> Failed rules returned False silently Now logs rule name, failure count, sample failures <code>graph</code> Cycle detection raised generic error Now includes node names in cycle <code>connections/factory</code> Unknown connection type silent Now logs attempted type and available types <code>transformers/*</code> Transform failures lost input context Now log input row count, schema before failure"},{"location":"LOGGING_AUDIT_REPORT/#log-output-examples","title":"Log Output Examples","text":""},{"location":"LOGGING_AUDIT_REPORT/#human-readable-mode","title":"Human-Readable Mode","text":"<pre><code>[15:42:33] Starting READ: orders.parquet (pipeline_id=etl_daily, node_id=load_orders)\n[15:42:34] Completed READ: orders.parquet (elapsed_ms=1234.56, rows=50000, partitions=8)\n[15:42:34] [DEBUG] Schema change in transform (columns_before=12, columns_after=15, columns_added=['calculated_total', 'tax_amount', 'discount'])\n[15:42:35] [WARN] Validation failed: not_null_check (rule=not_null_check, passed=False, failures=['customer_id is null: 23 rows'])\n</code></pre>"},{"location":"LOGGING_AUDIT_REPORT/#jsonstructured-mode","title":"JSON/Structured Mode","text":"<pre><code>{\"timestamp\": \"2025-12-01T15:42:33.123Z\", \"level\": \"INFO\", \"message\": \"Starting READ: orders.parquet\", \"pipeline_id\": \"etl_daily\", \"node_id\": \"load_orders\", \"operation\": \"read\"}\n{\"timestamp\": \"2025-12-01T15:42:34.456Z\", \"level\": \"INFO\", \"message\": \"Completed READ: orders.parquet\", \"pipeline_id\": \"etl_daily\", \"node_id\": \"load_orders\", \"operation\": \"read\", \"elapsed_ms\": 1234.56, \"rows\": 50000, \"partitions\": 8}\n</code></pre>"},{"location":"LOGGING_AUDIT_REPORT/#recommendations","title":"Recommendations","text":""},{"location":"LOGGING_AUDIT_REPORT/#short-term","title":"Short-term","text":"<ol> <li>Add sampling for large failure sets - Currently logs up to 10 failure examples; consider making configurable</li> <li>Add correlation IDs - Pass run_id through all operations for distributed tracing</li> <li>Memory threshold configuration - Make the 1GB warning threshold configurable</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/#medium-term","title":"Medium-term","text":"<ol> <li>Metrics export - Add Prometheus/StatsD exporters for <code>OperationMetrics</code></li> <li>Log aggregation - Consider structured logging to ELK/Splunk/Datadog</li> <li>Performance dashboards - Build Grafana dashboards from structured logs</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/#long-term","title":"Long-term","text":"<ol> <li>OpenTelemetry integration - Add spans for distributed tracing</li> <li>Automatic alerting - Trigger alerts on validation failure rates</li> <li>Log-based debugging - Replay failed operations from logs</li> </ol>"},{"location":"LOGGING_AUDIT_REPORT/#files-modified","title":"Files Modified","text":"<pre><code>odibi/utils/logging_context.py  (NEW)\nodibi/utils/logging.py\nodibi/utils/__init__.py\nodibi/utils/config_loader.py\nodibi/graph.py\nodibi/node.py\nodibi/pipeline.py\nodibi/validation/engine.py\nodibi/engine/pandas_engine.py\nodibi/engine/spark_engine.py\nodibi/connections/factory.py\nodibi/connections/local.py\nodibi/connections/azure_adls.py\nodibi/connections/azure_sql.py\nodibi/transformers/relational.py\nodibi/transformers/advanced.py\nodibi/transformers/scd.py\nodibi/transformers/merge_transformer.py\nodibi/transformers/validation.py\nodibi/transformers/sql_core.py\nodibi/patterns/base.py\nodibi/patterns/scd2.py\nodibi/patterns/merge.py\nodibi/story/generator.py\nodibi/story/renderers.py\nodibi/story/metadata.py\n</code></pre>"},{"location":"LOGGING_AUDIT_REPORT/#verification","title":"Verification","text":"<p>All tests pass after the logging enhancements:</p> <pre><code>pytest tests/ -x -q --tb=short\n# Result: 655 passed, 20 skipped\n</code></pre> <p>Lint checks pass:</p> <pre><code>ruff check .\nruff format .\n# Result: All checks passed\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/","title":"New GitHub Issues (Copy-Paste Ready)","text":""},{"location":"NEW_GITHUB_ISSUES/#issue-32","title":"Issue #32","text":"<p>Title: CLI missing <code>--tag</code> flag for running nodes by tag</p> <p>Labels: bug, cli, priority-high</p> <p>Body:</p> <pre><code>The documentation describes tag-based execution (`odibi run --tag daily`), but the `--tag` flag is not implemented in the CLI.\n\n## Evidence\n\n- Config: `NodeConfig.tags` field exists in `odibi/config.py`\n- Docs reference: `odibi run --tag daily`\n- CLI: No `--tag` argument in `odibi/cli/main.py`\n\n## Expected Behavior\n\nUsers should be able to run only nodes with specific tags:\n\n```bash\nodibi run pipeline.yaml --tag daily\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#fix","title":"Fix","text":"<p>Add <code>--tag</code> argument to the run parser and filter nodes in <code>Pipeline.run()</code>.</p> <pre><code>\n---\n\n## Issue #33\n\n**Title:** CLI missing `--node` and `--pipeline` flags for orchestration\n\n**Labels:** bug, cli, orchestration, priority-high\n\n**Body:**\n\n</code></pre> <p>Orchestration templates generate commands with flags that don't exist in the CLI.</p>"},{"location":"NEW_GITHUB_ISSUES/#evidence","title":"Evidence","text":"<p><code>odibi/orchestration/airflow.py</code> and <code>odibi/orchestration/dagster.py</code> generate:</p> <pre><code>odibi run --pipeline {{ pipeline_name }} --node {{ node.name }}\n</code></pre> <p>But these flags do not exist in <code>odibi/cli/main.py</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#impact","title":"Impact","text":"<p>Generated Airflow/Dagster DAGs will fail to execute.</p>"},{"location":"NEW_GITHUB_ISSUES/#fix_1","title":"Fix","text":"<p>Add <code>--pipeline</code> and <code>--node</code> arguments to the run command.</p> <pre><code>\n---\n\n## Issue #34\n\n**Title:** `materialized` config option has no effect\n\n**Labels:** bug, config, priority-medium\n\n**Body:**\n\n</code></pre> <p>The <code>materialized</code> field (<code>table</code>, <code>view</code>, <code>incremental</code>) is defined in <code>NodeConfig</code> but never read or used in node execution.</p>"},{"location":"NEW_GITHUB_ISSUES/#evidence_1","title":"Evidence","text":"<ul> <li>Config defines <code>materialized</code> field in <code>odibi/config.py</code></li> <li>No references to <code>config.materialized</code> in <code>odibi/node.py</code></li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#impact_1","title":"Impact","text":"<p>Users can set this option but it's silently ignored.</p>"},{"location":"NEW_GITHUB_ISSUES/#fix_2","title":"Fix","text":"<p>Either implement materialization strategies or remove/deprecate the field.</p> <pre><code>\n---\n\n## Issue #35\n\n**Title:** Add `POLARS` to `EngineType` enum\n\n**Labels:** enhancement, engine, priority-medium\n\n**Body:**\n\n</code></pre> <p><code>PolarsEngine</code> exists and is registered, but <code>EngineType</code> enum only has <code>SPARK</code> and <code>PANDAS</code>.</p>"},{"location":"NEW_GITHUB_ISSUES/#evidence_2","title":"Evidence","text":"<ul> <li><code>odibi/engine/polars_engine.py</code> - Full implementation exists</li> <li><code>odibi/engine/registry.py</code> - Registered as \"polars\"</li> <li><code>odibi/config.py</code> - Only SPARK/PANDAS in enum</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#impact_2","title":"Impact","text":"<p>Users cannot set <code>engine: polars</code> in YAML config.</p>"},{"location":"NEW_GITHUB_ISSUES/#fix_3","title":"Fix","text":"<p>Add <code>POLARS = \"polars\"</code> to <code>EngineType</code> enum in <code>odibi/config.py</code>.</p> <pre><code>\n---\n\n## Issue #36\n\n**Title:** PandasEngine missing `as_of_timestamp` time travel\n\n**Labels:** bug, engine-parity, priority-low\n\n**Body:**\n\n</code></pre> <p><code>as_of_timestamp</code> is accepted in read config but only <code>as_of_version</code> is implemented for Delta time travel in PandasEngine.</p>"},{"location":"NEW_GITHUB_ISSUES/#evidence_3","title":"Evidence","text":"<ul> <li>Method signature accepts both in <code>odibi/engine/pandas_engine.py</code></li> <li>Only version is used, timestamp is ignored</li> <li>SparkEngine supports both</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#impact_3","title":"Impact","text":"<p>Timestamp-based time travel silently ignored in Pandas engine.</p>"},{"location":"NEW_GITHUB_ISSUES/#fix_4","title":"Fix","text":"<p>Implement <code>timestampAsOf</code> option in PandasEngine's Delta read logic.</p> <pre><code>\n---\n\n## Issue #37\n\n**Title:** PolarsEngine missing abstract methods\n\n**Labels:** enhancement, engine-parity, priority-low\n\n**Body:**\n\n</code></pre> <p>PolarsEngine doesn't implement all abstract methods from <code>Engine</code> base class.</p>"},{"location":"NEW_GITHUB_ISSUES/#missing-methods","title":"Missing Methods","text":"<ul> <li><code>harmonize_schema()</code> - Not implemented</li> <li><code>anonymize()</code> - Not implemented</li> <li><code>add_write_metadata()</code> - Not implemented</li> <li><code>table_exists()</code> - Partial (file only)</li> <li><code>maintain_table()</code> - Not implemented</li> </ul>"},{"location":"NEW_GITHUB_ISSUES/#impact_4","title":"Impact","text":"<p>Polars pipelines will fail if these features are used.</p>"},{"location":"NEW_GITHUB_ISSUES/#fix_5","title":"Fix","text":"<p>Implement remaining abstract methods or raise <code>NotImplementedError</code> with clear message.</p> <pre><code>\n---\n\n## Issue #38\n\n**Title:** `environments` config not implemented\n\n**Labels:** enhancement, config, priority-low\n\n**Body:**\n\n</code></pre> <p>The <code>environments</code> field is defined in <code>ProjectConfig</code> but does nothing.</p>"},{"location":"NEW_GITHUB_ISSUES/#evidence_4","title":"Evidence","text":"<p>The validator in <code>odibi/config.py</code> explicitly notes:</p> <pre><code>@model_validator(mode=\"after\")\ndef check_environments_not_implemented(self):\n    # Implemented in Phase 3\n    return self\n</code></pre>"},{"location":"NEW_GITHUB_ISSUES/#impact_5","title":"Impact","text":"<p>Users can define environments but they're silently ignored.</p>"},{"location":"NEW_GITHUB_ISSUES/#fix_6","title":"Fix","text":"<p>Either implement environment switching or remove the field with a deprecation notice. ```</p>"},{"location":"NEW_GITHUB_ISSUES/#summary","title":"Summary","text":"Issue Title Priority #32 CLI missing <code>--tag</code> flag High #33 CLI missing <code>--node</code>/<code>--pipeline</code> flags High #34 <code>materialized</code> config ignored Medium #35 Add POLARS to EngineType enum Medium #36 PandasEngine missing timestamp time travel Low #37 PolarsEngine missing abstract methods Low #38 <code>environments</code> config not implemented Low"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/","title":"odibi Open-Source Launch Plan","text":"<p>A detailed guide for open-sourcing odibi and building community adoption.</p>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-1-pre-launch-prep-1-2-weeks","title":"Phase 1: Pre-Launch Prep (1-2 weeks)","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#11-code-cleanup","title":"1.1 Code Cleanup","text":"<ul> <li>[ ] Remove any hardcoded secrets, internal paths, company-specific references</li> <li>[ ] Audit for sensitive data in git history (consider squashing if needed)</li> <li>[ ] Ensure all credentials use environment variables</li> <li>[ ] Add <code>.env.example</code> with placeholder values</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#12-documentation-critical","title":"1.2 Documentation (Critical)","text":"<ul> <li>[ ] README.md \u2014 the first impression</li> <li>What is odibi? (one sentence)</li> <li>Key features (bullet points)</li> <li>Quick install: <code>pip install odibi</code></li> <li>5-minute getting started example</li> <li>Architecture diagram (use mermaid)</li> <li>Link to full docs</li> <li>[ ] docs/ folder or docs site (mkdocs, docusaurus, or just markdown)</li> <li>Installation guide</li> <li>Core concepts (connections, pipelines, nodes, layers)</li> <li>Bronze patterns (rolling_window, stateful, skip_if_unchanged, cloudFiles)</li> <li>Silver patterns (deduplicate, detect_deletes)</li> <li>Configuration reference (all YAML options)</li> <li>Examples for each pattern</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#13-examples","title":"1.3 Examples","text":"<ul> <li>[ ] <code>examples/</code> folder with working pipelines:</li> <li><code>examples/quickstart/</code> \u2014 minimal working example</li> <li><code>examples/bronze_sql_to_delta/</code> \u2014 SQL source to Delta</li> <li><code>examples/bronze_adls_cloudfiles/</code> \u2014 streaming file ingestion</li> <li><code>examples/silver_dedup_deletes/</code> \u2014 silver layer patterns</li> <li>[ ] Each example should be copy-paste runnable</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#14-project-hygiene","title":"1.4 Project Hygiene","text":"<ul> <li>[ ] <code>LICENSE</code> file (Apache 2.0 recommended)</li> <li>[ ] <code>CONTRIBUTING.md</code> \u2014 how to contribute</li> <li>[ ] <code>CODE_OF_CONDUCT.md</code> \u2014 standard community guidelines</li> <li>[ ] <code>CHANGELOG.md</code> \u2014 version history</li> <li>[ ] <code>.github/</code> folder:</li> <li>Issue templates (bug report, feature request)</li> <li>PR template</li> <li>GitHub Actions for CI (run tests on PR)</li> <li>[ ] Ensure <code>pytest</code> tests pass</li> <li>[ ] Add badges to README (build status, license, Python version)</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#15-packaging","title":"1.5 Packaging","text":"<ul> <li>[ ] Verify <code>pyproject.toml</code> is correct</li> <li>[ ] Test <code>pip install .</code> from fresh environment</li> <li>[ ] Publish to PyPI (or TestPyPI first): <code>pip install odibi</code></li> <li>[ ] Add install extras: <code>pip install odibi[azure]</code>, <code>pip install odibi[spark]</code></li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-2-launch-day-1-day","title":"Phase 2: Launch Day (1 day)","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#21-github","title":"2.1 GitHub","text":"<ul> <li>[ ] Make repo public</li> <li>[ ] Add topics/tags: <code>data-engineering</code>, <code>etl</code>, <code>medallion-architecture</code>, <code>delta-lake</code>, <code>spark</code>, <code>azure</code></li> <li>[ ] Pin the repo to your profile</li> <li>[ ] Star it yourself</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#22-announcement-posts","title":"2.2 Announcement Posts","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#hacker-news-show-hn","title":"Hacker News (Show HN)","text":"<pre><code>Show HN: odibi \u2013 Open-source medallion architecture framework for data engineering\n\nI built odibi to simplify bronze/silver/gold pipelines without vendor lock-in.\nIt's YAML-driven, supports Spark and Pandas, and has built-in patterns for\nincremental loads, deduplication, and CDC-like delete detection.\n\n- Runs on any Spark (Databricks, EMR, Synapse, local)\n- No vendor lock-in\n- Built-in patterns: rolling_window, skip_if_unchanged, detect_deletes\n\nGitHub: [link]\n\nI've been using it in production for OEE analytics. Looking for feedback!\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#reddit-rdataengineering-rpython-rapachespark","title":"Reddit (r/dataengineering, r/python, r/apachespark)","text":"<ul> <li>Similar post, slightly more casual</li> <li>Engage with comments, answer questions</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#twitterx","title":"Twitter/X","text":"<pre><code>Shipped odibi \u2014 an open-source framework for medallion architecture pipelines.\n\n\u2705 YAML-driven bronze \u2192 silver \u2192 gold\n\u2705 Runs on Spark or Pandas\n\u2705 No Databricks lock-in\n\u2705 Built-in CDC-like delete detection\n\nBuilt it for my own production workloads. Now it's yours.\n\nGitHub: [link]\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#linkedin","title":"LinkedIn","text":"<ul> <li>More professional angle</li> <li>\"After building data pipelines at [X], I open-sourced the framework I wish I had...\"</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#23-dev-communities","title":"2.3 Dev Communities","text":"<ul> <li>[ ] Post in Discord/Slack communities (Data Engineering, dbt, Databricks community)</li> <li>[ ] Consider a blog post on dev.to or Medium</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-3-post-launch-growth-ongoing","title":"Phase 3: Post-Launch Growth (Ongoing)","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#31-content-marketing-1-2-postsmonth","title":"3.1 Content Marketing (1-2 posts/month)","text":"<ul> <li>\"How to build CDC without CDC using odibi\"</li> <li>\"Medallion architecture on AWS EMR with odibi\"</li> <li>\"Why I stopped using Delta Live Tables\"</li> <li>\"Rolling window vs stateful incremental: when to use each\"</li> <li>Tutorial videos on YouTube</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#32-community-building","title":"3.2 Community Building","text":"<ul> <li>[ ] Respond to every GitHub issue within 24 hours</li> <li>[ ] Add a Discussions tab on GitHub for Q&amp;A</li> <li>[ ] Consider a Discord server once you have 10+ active users</li> <li>[ ] Celebrate contributors (shoutouts, contributor badges)</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#33-expand-connectors","title":"3.3 Expand Connectors","text":"<ul> <li>[ ] AWS: S3, Redshift, Glue Catalog</li> <li>[ ] GCP: GCS, BigQuery</li> <li>[ ] Snowflake</li> <li>[ ] Polars engine (once stable)</li> <li>[ ] DuckDB (for local/lightweight)</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#34-add-a-cli","title":"3.4 Add a CLI","text":"<pre><code>odibi init        # scaffold a new project\nodibi validate    # validate YAML config\nodibi run         # execute pipeline\nodibi plan        # dry-run, show what would execute\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#phase-4-monetization-via-consulting","title":"Phase 4: Monetization via Consulting","text":""},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#41-services-page","title":"4.1 Services Page","text":"<p>Add to your docs site or a simple landing page:</p> <pre><code># odibi Services\n\nodibi is free and open-source. Need help?\n\n**Implementation Support**\n- Pipeline design and setup\n- Migration from legacy ETL\n- Performance tuning\n\n**Training**\n- Team workshops on medallion architecture\n- Best practices for incremental ingestion\n\n**Support Contracts**\n- Priority issue resolution\n- SLA-backed support\n\nContact: your@email.com\n</code></pre>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#42-pricing-starting-point","title":"4.2 Pricing (Starting Point)","text":"Service Price Range Consulting $150-250/hour Implementation package $5-15k for full setup Support contract $1-3k/month"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#43-lead-generation","title":"4.3 Lead Generation","text":"<ul> <li>Add \"Need help? Contact me\" to README</li> <li>Blog posts with CTA at the end</li> <li>Engage in communities, offer free advice, build trust</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#priority-timeline","title":"Priority Timeline","text":"Week Focus 1 Code cleanup, README, LICENSE, tests passing 2 Docs site, examples, PyPI publish 3 Launch: GitHub public, HN, Reddit, Twitter 4+ Content, community, expand connectors, consulting"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ul> <li>GitHub stars</li> <li>PyPI downloads</li> <li>GitHub issues/PRs (engagement)</li> <li>Docs site traffic</li> <li>Consulting inquiries</li> </ul>"},{"location":"OPEN_SOURCE_LAUNCH_PLAN/#resources","title":"Resources","text":"<ul> <li>Choose a License \u2014 Apache 2.0 recommended</li> <li>Keep a Changelog</li> <li>Semantic Versioning</li> <li>GitHub Actions for Python</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/","title":"Reference Enrichment Campaign","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#goal","title":"Goal","text":"<p>Make the auto-generated <code>yaml_schema.md</code> reference beginner-friendly, complete, and cross-linked to relevant guides/tutorials.</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#context","title":"Context","text":"<p>Current State: - <code>docs/reference/yaml_schema.md</code> is auto-generated from Pydantic docstrings via <code>odibi/introspect.py</code> - Source of truth: docstrings in <code>odibi/config.py</code> - Currently 3400+ lines with good structure but inconsistent depth - Missing: beginner context, cross-references, \"when to use\" guidance</p> <p>Key Pain Points Identified: - Validation vs Contracts vs Quality Gates - confusing overlap - Tests/Contracts terminology inconsistent - Some configs have great examples, others are bare - No links to relevant tutorials/guides - Assumes reader knows SCD2, Delta, HWM, etc.</p> <p>Success Criteria: - [ ] Every major config has a \"When to Use\" one-liner - [ ] Every major config has a \"See Also\" linking to relevant guide/tutorial - [ ] Validation/Contracts/Quality Gates clearly distinguished - [ ] All pattern configs have realistic YAML examples - [ ] <code>mkdocs build</code> passes - [ ] <code>python odibi/introspect.py</code> regenerates successfully - [ ] <code>ruff check . &amp;&amp; ruff format .</code> passes - [ ] All tests pass</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-1-clarify-validation-architecture-high-priority","title":"Phase 1: Clarify Validation Architecture (HIGH PRIORITY)","text":"<p>The current docs are confusing about: - Contracts (pre-transform checks) - Validation (post-transform checks) - Quality Gates (pipeline-level thresholds) - Quarantine (where bad rows go)</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#11-update-section_intros-in-introspectpy","title":"1.1 Update SECTION_INTROS in introspect.py","text":"<p>File: <code>odibi/introspect.py</code> \u2192 <code>SECTION_INTROS[\"Contract\"]</code></p> <p>Add clear disambiguation:</p> <pre><code>SECTION_INTROS[\"Contract\"] = \"\"\"\n### Contracts (Pre-Transform Checks)\n\nContracts are **fail-fast data quality checks** that run on input data **before** transformation.\nThey always halt execution on failure - use them to prevent bad data from entering the pipeline.\n\n**Contracts vs Validation vs Quality Gates:**\n\n| Feature | When it Runs | On Failure | Use Case |\n|---------|--------------|------------|----------|\n| **Contracts** | Before transform | Always fails | Input data quality (not-null, unique keys) |\n| **Validation** | After transform | Configurable (fail/warn/quarantine) | Output data quality (ranges, formats) |\n| **Quality Gates** | After validation | Configurable (abort/warn) | Pipeline-level thresholds (pass rate, row counts) |\n| **Quarantine** | With validation | Routes bad rows | Capture invalid records for review |\n\n**See Also:**\n- [Validation Guide](../features/quality_gates.md) - Full validation configuration\n- [Quarantine Guide](../features/quarantine.md) - Quarantine setup and review\n- [Getting Started: Validation](../tutorials/getting_started.md#add-data-validation)\n\n**Example:**\n```yaml\n- name: \"process_orders\"\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read:\n    source: raw_orders\n</code></pre> <p>\"\"\"</p> <pre><code>\n### 1.2 Update Contract Config Docstrings\n\n**File:** `odibi/config.py`\n\nFor each test type, add:\n1. One-liner \"when to use\"\n2. See Also reference\n\n**NotNullTest:**\n```python\nclass NotNullTest(BaseModel):\n    \"\"\"Ensures specified columns contain no NULL values.\n\n    **When to Use:** Primary keys, required fields, foreign keys that must resolve.\n\n    **See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n    ```yaml\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id, created_at]\n    ```\n    \"\"\"\n</code></pre> <p>UniqueTest:</p> <pre><code>class UniqueTest(BaseModel):\n    \"\"\"Ensures specified columns (or combination) contain unique values.\n\n    **When to Use:** Primary keys, natural keys, deduplication verification.\n\n    **See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n    ```yaml\n    contracts:\n      - type: unique\n        columns: [order_id]  # Single column\n      # OR composite key:\n      - type: unique\n        columns: [customer_id, order_date]  # Composite uniqueness\n    ```\n    \"\"\"\n</code></pre> <p>FreshnessContract:</p> <pre><code>class FreshnessContract(BaseModel):\n    \"\"\"Validates that data is not stale.\n\n    **When to Use:** Source systems that should update regularly, SLA monitoring.\n\n    **See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n    ```yaml\n    contracts:\n      - type: freshness\n        column: updated_at\n        max_age: \"24h\"  # Fail if no data newer than 24 hours\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-2-enrich-core-configs-medium-priority","title":"Phase 2: Enrich Core Configs (MEDIUM PRIORITY)","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#21-readconfig","title":"2.1 ReadConfig","text":"<p>File: <code>odibi/config.py</code> \u2192 <code>ReadConfig</code> docstring</p> <p>Add:</p> <pre><code>class ReadConfig(BaseModel):\n    \"\"\"Configuration for reading data into a node.\n\n    **When to Use:** First node in a pipeline, or any node that reads from storage.\n\n    **Key Concepts:**\n    - `connection`: References a named connection from `connections:` section\n    - `format`: File format (csv, parquet, delta, json, avro)\n    - `incremental`: Enable incremental loading (only new data)\n\n    **See Also:**\n    - [Smart Read Pattern](../patterns/smart_read.md) - Automatic full/incremental switching\n    - [Incremental Loading](../patterns/incremental_stateful.md) - HWM-based loading\n    - [Supported Formats](./supported_formats.md) - All format options\n\n    **Examples:**\n\n    Basic Read:\n    ```yaml\n    read:\n      connection: bronze\n      format: parquet\n      path: raw/customers/\n    ```\n\n    Incremental Read (Stateful HWM):\n    ```yaml\n    read:\n      connection: bronze\n      format: delta\n      table: raw_orders\n      incremental:\n        type: stateful\n        column: updated_at\n    ```\n\n    First Run Override:\n    ```yaml\n    read:\n      connection: source_db\n      query: \"SELECT * FROM orders\"\n      first_run_query: \"SELECT * FROM orders WHERE order_date &gt;= '2023-01-01'\"\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#22-writeconfig","title":"2.2 WriteConfig","text":"<p>File: <code>odibi/config.py</code> \u2192 <code>WriteConfig</code> docstring</p> <p>Add:</p> <pre><code>class WriteConfig(BaseModel):\n    \"\"\"Configuration for writing data from a node.\n\n    **When to Use:** Any node that persists data to storage.\n\n    **Key Concepts:**\n    - `mode`: How to handle existing data (overwrite, append, upsert)\n    - `keys`: Required for upsert mode - columns that identify unique records\n    - `partition_by`: Columns to partition output by (improves query performance)\n\n    **See Also:**\n    - [Merge/Upsert Pattern](../patterns/merge_upsert.md) - Delta merge operations\n    - [Performance Tuning](../guides/performance_tuning.md) - Partitioning strategies\n\n    **Examples:**\n\n    Simple Overwrite:\n    ```yaml\n    write:\n      connection: silver\n      format: parquet\n      path: cleaned/customers/\n      mode: overwrite\n    ```\n\n    Delta Upsert (Merge):\n    ```yaml\n    write:\n      connection: gold\n      format: delta\n      table: dim_customer\n      mode: upsert\n      keys: [customer_id]  # Match on this column\n    ```\n\n    Partitioned Write:\n    ```yaml\n    write:\n      connection: gold\n      format: delta\n      table: fact_orders\n      mode: append\n      partition_by: [order_date]\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#23-transformconfig","title":"2.3 TransformConfig","text":"<p>File: <code>odibi/config.py</code> \u2192 <code>TransformConfig</code> docstring</p> <p>Add:</p> <pre><code>class TransformConfig(BaseModel):\n    \"\"\"Configuration for transformation steps within a node.\n\n    **When to Use:** Custom business logic, data cleaning, SQL transformations.\n\n    **Key Concepts:**\n    - `steps`: Ordered list of operations (SQL, functions, or both)\n    - Each step receives `df` (the DataFrame from previous step)\n    - Steps execute in order: step1 \u2192 step2 \u2192 step3\n\n    **See Also:**\n    - [Writing Transformations](../guides/writing_transformations.md) - Custom functions\n    - [Transformer Catalog](#transformer-catalog) - Built-in functions\n\n    **Transformer vs Transform:**\n    - `transformer`: Single heavy operation (scd2, merge, deduplicate)\n    - `transform.steps`: Chain of lighter operations\n\n    **Examples:**\n\n    SQL-Only:\n    ```yaml\n    transform:\n      steps:\n        - sql: \"SELECT *, UPPER(name) as name_upper FROM df\"\n        - sql: \"SELECT * FROM df WHERE status = 'active'\"\n    ```\n\n    Function Steps:\n    ```yaml\n    transform:\n      steps:\n        - function: clean_text\n          params:\n            columns: [email, name]\n            case: lower\n        - function: fill_nulls\n          params:\n            defaults: {status: 'unknown', region: 'US'}\n    ```\n\n    Mixed SQL + Functions:\n    ```yaml\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE order_total &gt; 0\"\n        - function: derive_columns\n          params:\n            expressions:\n              profit: \"revenue - cost\"\n              margin: \"profit / revenue * 100\"\n    ```\n    \"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-3-enrich-pattern-configs-medium-priority","title":"Phase 3: Enrich Pattern Configs (MEDIUM PRIORITY)","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#31-dimensionpattern","title":"3.1 DimensionPattern","text":"<p>Add beginner context to existing docstring:</p> <pre><code>\"\"\"Build complete dimension tables with surrogate keys and SCD support.\n\n**When to Use:**\n- Building dimension tables from source systems (customers, products, locations)\n- Need surrogate keys for star schema joins\n- Need to track historical changes (SCD Type 2)\n\n**Beginner Note:**\nDimensions are the \"who, what, where, when\" of your data warehouse.\nA customer dimension has customer_id (natural key) and customer_sk (surrogate key).\nFact tables join to dimensions via surrogate keys.\n\n**See Also:**\n- [Dimension Tutorial](../tutorials/dimensional_modeling/02_dimension_pattern.md)\n- [SCD2 Pattern](../patterns/scd2.md) - Slowly Changing Dimensions\n- [Fact Pattern](#factpattern) - Build facts that reference dimensions\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#32-factpattern","title":"3.2 FactPattern","text":"<p>Add:</p> <pre><code>\"\"\"Build fact tables with automatic surrogate key lookups from dimensions.\n\n**When to Use:**\n- Building fact tables from transactional data (orders, events, transactions)\n- Need to look up surrogate keys from dimension tables\n- Need to handle orphan records (missing dimension matches)\n\n**Beginner Note:**\nFacts are the \"how much, how many\" of your data warehouse.\nAn orders fact has measures (quantity, revenue) and dimension keys (customer_sk, product_sk).\nThe pattern automatically looks up SKs from dimensions.\n\n**See Also:**\n- [Fact Tutorial](../tutorials/dimensional_modeling/04_fact_pattern.md)\n- [Dimension Pattern](#dimensionpattern) - Build dimensions first\n- [FK Validation](../validation/fk.md) - Validate referential integrity\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#33-aggregationpattern","title":"3.3 AggregationPattern","text":"<p>Add:</p> <pre><code>\"\"\"Declarative aggregation with GROUP BY and optional incremental merge.\n\n**When to Use:**\n- Building summary/aggregate tables (daily sales, monthly metrics)\n- Need incremental aggregation (update existing aggregates)\n- Gold layer reporting tables\n\n**Beginner Note:**\nAggregations summarize facts at a higher grain.\nExample: daily_sales aggregates orders by date with SUM(revenue).\n\n**See Also:**\n- [Aggregation Tutorial](../tutorials/dimensional_modeling/05_aggregation_pattern.md)\n- [Windowed Reprocess](../patterns/windowed_reprocess.md) - Rolling aggregations\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-4-enrich-connection-configs-low-priority","title":"Phase 4: Enrich Connection Configs (LOW PRIORITY)","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#41-add-when-to-use-to-each-connection","title":"4.1 Add \"When to Use\" to Each Connection","text":"<p>LocalConnectionConfig:</p> <pre><code>\"\"\"Local filesystem connection.\n\n**When to Use:** Development, testing, small datasets, local processing.\n\n**See Also:** [Azure Setup](../guides/setup_azure.md) for cloud alternatives.\n\"\"\"\n</code></pre> <p>DeltaConnectionConfig:</p> <pre><code>\"\"\"Delta Lake connection for ACID-compliant data lakes.\n\n**When to Use:**\n- Production data lakes on Azure/AWS/GCP\n- Need time travel, ACID transactions, schema evolution\n- Upsert/merge operations\n\n**See Also:**\n- [Supported Formats](./supported_formats.md#delta) - Delta-specific options\n- [Merge Pattern](../patterns/merge_upsert.md) - Delta merge operations\n\"\"\"\n</code></pre> <p>AzureBlobConnectionConfig:</p> <pre><code>\"\"\"Azure Blob Storage / ADLS Gen2 connection.\n\n**When to Use:** Azure-based data lakes, landing zones, raw data storage.\n\n**See Also:**\n- [Azure Setup Guide](../guides/setup_azure.md) - Full Azure configuration\n- [Secrets Management](../guides/secrets.md) - Secure credential handling\n\"\"\"\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-5-fix-cross-references-in-introspectpy","title":"Phase 5: Fix Cross-References in introspect.py","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#51-add-see-also-links-generator","title":"5.1 Add See Also Links Generator","text":"<p>File: <code>odibi/introspect.py</code></p> <p>Add a mapping for auto-generating \"See Also\" sections:</p> <pre><code>SEE_ALSO_LINKS = {\n    \"ReadConfig\": [\n        (\"Smart Read Pattern\", \"../patterns/smart_read.md\"),\n        (\"Incremental Loading\", \"../patterns/incremental_stateful.md\"),\n    ],\n    \"WriteConfig\": [\n        (\"Merge/Upsert Pattern\", \"../patterns/merge_upsert.md\"),\n        (\"Performance Tuning\", \"../guides/performance_tuning.md\"),\n    ],\n    \"ValidationConfig\": [\n        (\"Validation Guide\", \"../features/quality_gates.md\"),\n        (\"Quarantine\", \"../features/quarantine.md\"),\n    ],\n    \"QuarantineConfig\": [\n        (\"Quarantine Guide\", \"../features/quarantine.md\"),\n        (\"Validation\", \"../features/quality_gates.md\"),\n    ],\n    \"GateConfig\": [\n        (\"Quality Gates\", \"../features/quality_gates.md\"),\n    ],\n    # Add more as needed\n}\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#phase-6-regenerate-and-verify","title":"Phase 6: Regenerate and Verify","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#61-regenerate-yaml_schemamd","title":"6.1 Regenerate yaml_schema.md","text":"<pre><code>python odibi/introspect.py\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#62-verify-build","title":"6.2 Verify Build","text":"<pre><code>python -m mkdocs build\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#63-lint-and-format","title":"6.3 Lint and Format","text":"<pre><code>ruff check . --fix\nruff format .\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#64-run-tests","title":"6.4 Run Tests","text":"<pre><code>pytest tests/ -v\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#65-commit-and-push","title":"6.5 Commit and Push","text":"<pre><code>git add .\ngit commit -m \"docs: Reference enrichment campaign - beginner-friendly yaml_schema\"\ngit push\n</code></pre>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#files-to-modify","title":"Files to Modify","text":"File Changes <code>odibi/config.py</code> Update docstrings for ~25 config classes <code>odibi/introspect.py</code> Update SECTION_INTROS, add SEE_ALSO_LINKS <code>docs/reference/yaml_schema.md</code> Auto-regenerated"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#config-classes-to-enrich","title":"Config Classes to Enrich","text":""},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#critical-must-have","title":"Critical (Must Have)","text":"<ul> <li>[ ] <code>NotNullTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>UniqueTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>FreshnessContract</code> - Add when-to-use, see-also</li> <li>[ ] <code>RowCountTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>RangeTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>AcceptedValuesTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>RegexMatchTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>VolumeDropTest</code> - Add when-to-use, see-also</li> <li>[ ] <code>SchemaContract</code> - Add when-to-use, see-also</li> <li>[ ] <code>DistributionContract</code> - Add when-to-use, see-also</li> <li>[ ] <code>ValidationConfig</code> - Add disambiguation, see-also</li> <li>[ ] <code>QuarantineConfig</code> - Add when-to-use, see-also</li> <li>[ ] <code>GateConfig</code> - Add when-to-use, see-also</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#important-should-have","title":"Important (Should Have)","text":"<ul> <li>[ ] <code>ReadConfig</code> - Add full examples, see-also</li> <li>[ ] <code>WriteConfig</code> - Add full examples, see-also</li> <li>[ ] <code>TransformConfig</code> - Add examples, see-also</li> <li>[ ] <code>IncrementalConfig</code> - Add when-to-use, examples</li> <li>[ ] <code>DimensionPattern</code> - Add beginner context</li> <li>[ ] <code>FactPattern</code> - Add beginner context</li> <li>[ ] <code>AggregationPattern</code> - Add beginner context</li> <li>[ ] <code>DateDimensionPattern</code> - Add when-to-use</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#nice-to-have","title":"Nice to Have","text":"<ul> <li>[ ] <code>LocalConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>DeltaConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>AzureBlobConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>SQLServerConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>HttpConnectionConfig</code> - Add when-to-use</li> <li>[ ] <code>RetryConfig</code> - Add when-to-use</li> <li>[ ] <code>AlertConfig</code> - Add when-to-use</li> <li>[ ] <code>PerformanceConfig</code> - Add see-also</li> </ul>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#estimated-effort","title":"Estimated Effort","text":"Phase Focus Time 1 Validation/Contracts/Gates disambiguation 1 hour 2 Core configs (Read/Write/Transform) 1.5 hours 3 Pattern configs 1 hour 4 Connection configs 30 min 5 Cross-references in introspect.py 30 min 6 Regenerate, test, push 30 min <p>Total: ~5 hours</p>"},{"location":"REFERENCE_ENRICHMENT_CAMPAIGN/#validation-checklist","title":"Validation Checklist","text":"<p>Before marking complete:</p> <ul> <li>[ ] <code>python odibi/introspect.py</code> runs without errors</li> <li>[ ] <code>docs/reference/yaml_schema.md</code> is regenerated</li> <li>[ ] <code>python -m mkdocs build</code> passes</li> <li>[ ] <code>ruff check .</code> passes</li> <li>[ ] <code>ruff format .</code> passes (no changes)</li> <li>[ ] <code>pytest tests/</code> passes</li> <li>[ ] Git commit and push successful</li> </ul>"},{"location":"ROADMAP/","title":"Odibi Phase 3 Plan: The \"Rich Docs\" Strategy","text":"<p>We are pivoting from a separate Cookbook to an Embedded Knowledge strategy. The goal is to make <code>docs/api.md</code> a single, high-value artifact that serves as both Reference and Guide.</p> <p>We will achieve this by embedding \"Gold Mine\" recipes directly into the Python code docstrings.</p>"},{"location":"ROADMAP/#1-the-strategy-code-as-documentation","title":"1. The Strategy: \"Code as Documentation\"","text":"<p>We will rewrite the Pydantic model docstrings in <code>odibi/config.py</code> and <code>odibi/transformers/*.py</code> to include: 1.  The Business Problem: Why do I need this? 2.  The Recipe: A copy-pasteable YAML block. 3.  The \"Why\": Brief explanation of the mechanics.</p>"},{"location":"ROADMAP/#2-targeted-upgrades","title":"2. Targeted Upgrades","text":"<p>We will enhance the following models with extensive examples:</p>"},{"location":"ROADMAP/#a-scd2params-in-odibitransformersscdpy","title":"A. <code>SCD2Params</code> (in <code>odibi/transformers/scd.py</code>)","text":"<ul> <li>Theme: \"The Time Machine\"</li> <li>Content: Explain how to track history, handle effective dates, and close open records.</li> <li>Recipe: A full YAML snippet showing keys, tracking columns, and flags.</li> </ul>"},{"location":"ROADMAP/#b-writeconfig-in-odibiconfigpy","title":"B. <code>WriteConfig</code> (in <code>odibi/config.py</code>)","text":"<ul> <li>Theme: \"Big Data Performance\"</li> <li>Content: Explain when to use <code>partition_by</code> (filtering) vs <code>zorder_by</code> (skipping).</li> <li>Recipe: A \"Lakehouse Optimized\" configuration.</li> </ul>"},{"location":"ROADMAP/#c-validationconfig-in-odibiconfigpy","title":"C. <code>ValidationConfig</code> (in <code>odibi/config.py</code>)","text":"<ul> <li>Theme: \"The Indestructible Pipeline\"</li> <li>Content: Explain blocking vs. warning severity.</li> <li>Recipe: A \"Quality Gate\" configuration preventing bad data from reaching Gold.</li> </ul>"},{"location":"ROADMAP/#d-mergeparams-in-odibitransformersmerge_transformerpy","title":"D. <code>MergeParams</code> (in <code>odibi/transformers/merge_transformer.py</code>)","text":"<ul> <li>Theme: \"GDPR &amp; Compliance\"</li> <li>Content: Explain <code>delete_match</code> strategy.</li> <li>Recipe: A \"Right to be Forgotten\" pipeline snippet.</li> </ul>"},{"location":"ROADMAP/#3-execution-steps","title":"3. Execution Steps","text":"<ol> <li>[x] Edit <code>odibi/transformers/scd.py</code>: Replace the placeholder docstring with the full \"Time Machine\" guide.</li> <li>[x] Edit <code>odibi/config.py</code>: update <code>WriteConfig</code>, <code>ValidationConfig</code>, and <code>NodeConfig</code> with rich examples.</li> <li>[x] Run <code>python odibi/introspect.py</code>: Generate the new <code>api.md</code>.</li> <li>[x] Review: Confirm <code>api.md</code> looks like a \"Gold Mine\" without needing a separate cookbook.</li> </ol>"},{"location":"ROADMAP/#4-extended-improvements-completed","title":"4. Extended Improvements (Completed)","text":"<p>Based on feedback, we went further to improve usability:</p> <ul> <li>Smart Node Scenarios: Added concrete examples for \"Standard ETL\", \"Heavy Lifter\", and \"Tagged Runner\" in <code>NodeConfig</code>.</li> <li>Universal Reader: Added \"Time Traveler\" and \"Streaming\" recipes to <code>ReadConfig</code>.</li> <li>Transformer Catalog: Added a searchable list of available transformers directly in <code>NodeConfig</code> docs.</li> <li>Concept Clarity: Explicitly explained \"Transformer (App) vs. Transform Steps (Script)\" and \"Chaining Operations\".</li> <li>Navigation: Added \"Back to Catalog\" links for better UX.</li> <li>\"Kitchen Sink\" Scenario: Demonstrated <code>read</code> -&gt; <code>transformer</code> -&gt; <code>transform</code> -&gt; <code>write</code> in a single node to prove composability.</li> </ul>"},{"location":"catalog_audit_report/","title":"Catalog Implementation Audit Report","text":"<p>Date: December 2024 Scope: Comprehensive audit of the Odibi System Catalog implementation</p>"},{"location":"catalog_audit_report/#executive-summary","title":"Executive Summary","text":"<p>A comprehensive audit of the catalog implementation revealed a critical path mismatch bug that causes silent failures in the stateful incremental mode. Additionally, 52 instances of silent exception handling were found that mask errors.</p>"},{"location":"catalog_audit_report/#critical-issues-found","title":"Critical Issues Found","text":"Severity Issue Location Impact \ud83d\udd34 Critical Path naming mismatch <code>state/__init__.py:475-476</code> HWM state silently fails \ud83d\udd34 Critical Node fallback uses wrong paths <code>node.py:1457-1458</code> State operations fail \ud83d\udfe1 Medium Silent exception handling Multiple files (52 instances) Errors hidden from users"},{"location":"catalog_audit_report/#1-path-consistency-audit","title":"1. Path Consistency Audit","text":""},{"location":"catalog_audit_report/#11-catalog-manager-paths-correct-source-of-truth","title":"1.1 Catalog Manager Paths (Correct Source of Truth)","text":"<p>File: <code>odibi/catalog.py</code> (lines 89-99)</p> <pre><code>self.tables = {\n    \"meta_tables\": f\"{self.base_path}/meta_tables\",\n    \"meta_runs\": f\"{self.base_path}/meta_runs\",\n    \"meta_patterns\": f\"{self.base_path}/meta_patterns\",\n    \"meta_metrics\": f\"{self.base_path}/meta_metrics\",\n    \"meta_state\": f\"{self.base_path}/meta_state\",      # \u2705 Correct: meta_state\n    \"meta_pipelines\": f\"{self.base_path}/meta_pipelines\",\n    \"meta_nodes\": f\"{self.base_path}/meta_nodes\",\n    \"meta_schemas\": f\"{self.base_path}/meta_schemas\",\n    \"meta_lineage\": f\"{self.base_path}/meta_lineage\",\n}\n</code></pre>"},{"location":"catalog_audit_report/#12-state-backend-paths-bug-wrong-names","title":"1.2 State Backend Paths (BUG: Wrong Names)","text":"<p>File: <code>odibi/state/__init__.py</code> (lines 475-476)</p> <pre><code>meta_state_path = f\"{base_uri}/state\"    # \u274c WRONG: should be /meta_state\nmeta_runs_path = f\"{base_uri}/runs\"      # \u274c WRONG: should be /meta_runs\n</code></pre>"},{"location":"catalog_audit_report/#13-node-fallback-paths-bug-wrong-names","title":"1.3 Node Fallback Paths (BUG: Wrong Names)","text":"<p>File: <code>odibi/node.py</code> (lines 1457-1458)</p> <pre><code>backend = CatalogStateBackend(\n    spark_session=spark_session,\n    meta_state_path=\".odibi/system/state\",   # \u274c WRONG: should be .odibi/system/meta_state\n    meta_runs_path=\".odibi/system/runs\",     # \u274c WRONG: should be .odibi/system/meta_runs\n)\n</code></pre>"},{"location":"catalog_audit_report/#14-azure-blob-structure-observed","title":"1.4 Azure Blob Structure (Observed)","text":"<pre><code>datalake/_odibi_system/\n\u251c\u2500\u2500 meta_lineage/     \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_metrics/     \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_nodes/       \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_patterns/    \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_pipelines/   \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_runs/        \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_schemas/     \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_state/       \u2705 Catalog creates this\n\u251c\u2500\u2500 meta_tables/      \u2705 Catalog creates this\n\u251c\u2500\u2500 state/            \u26a0\ufe0f May be created by state backend (orphan)\n\u2514\u2500\u2500 runs/             \u26a0\ufe0f May be created by state backend (orphan)\n</code></pre>"},{"location":"catalog_audit_report/#2-catalog-manager-implementation","title":"2. Catalog Manager Implementation","text":""},{"location":"catalog_audit_report/#21-initialization-flow","title":"2.1 Initialization Flow","text":"<pre><code>PipelineManager.from_yaml()\n    \u2514\u2500\u2500 __init__()\n        \u2514\u2500\u2500 CatalogManager(spark, config, base_path, engine)\n            \u2514\u2500\u2500 bootstrap()\n                \u2514\u2500\u2500 _ensure_table() for each meta_* table\n</code></pre>"},{"location":"catalog_audit_report/#22-table-creation-bootstrap-method","title":"2.2 Table Creation (bootstrap method)","text":"<p>The 9 meta tables are created with proper schemas:</p> Table Schema Method Partition Cols Purpose meta_tables <code>_get_schema_meta_tables()</code> None Asset inventory meta_runs <code>_get_schema_meta_runs()</code> pipeline_name, date Execution history meta_patterns <code>_get_schema_meta_patterns()</code> None Pattern compliance meta_metrics <code>_get_schema_meta_metrics()</code> None Business metrics meta_state <code>_get_schema_meta_state()</code> None HWM key-value store meta_pipelines <code>_get_schema_meta_pipelines()</code> None Pipeline configs meta_nodes <code>_get_schema_meta_nodes()</code> None Node configs meta_schemas <code>_get_schema_meta_schemas()</code> None Schema versions meta_lineage <code>_get_schema_meta_lineage()</code> None Table lineage"},{"location":"catalog_audit_report/#23-unity-catalog-vs-path-based","title":"2.3 Unity Catalog vs Path-Based","text":"<p>Tables are path-based only, not registered in Unity Catalog. The code uses: - <code>spark.read.format(\"delta\").load(path)</code> for reads - <code>df.write.format(\"delta\").save(path)</code> for writes - <code>MERGE INTO delta.\\</code>{path}`` for upserts</p>"},{"location":"catalog_audit_report/#3-state-backend-integration","title":"3. State Backend Integration","text":""},{"location":"catalog_audit_report/#31-catalogstatebackend-class","title":"3.1 CatalogStateBackend Class","text":"<p>File: <code>odibi/state/__init__.py</code> (lines 105-336)</p> <pre><code>class CatalogStateBackend(StateBackend):\n    def __init__(\n        self,\n        meta_runs_path: str,\n        meta_state_path: str,\n        spark_session: Any = None,\n        storage_options: Optional[Dict[str, str]] = None,\n    ):\n        self.meta_runs_path = meta_runs_path\n        self.meta_state_path = meta_state_path\n</code></pre>"},{"location":"catalog_audit_report/#32-hwm-operations","title":"3.2 HWM Operations","text":"Method Reads From Writes To Issue <code>get_hwm(key)</code> meta_state_path - \u274c Wrong path if created by create_state_backend <code>set_hwm(key, value)</code> - meta_state_path \u274c Creates orphan table <code>get_last_run_info()</code> meta_runs_path - \u274c Wrong path if created by create_state_backend"},{"location":"catalog_audit_report/#33-path-construction-bug-location","title":"3.3 Path Construction (Bug Location)","text":"<p><code>create_state_backend()</code> function (lines 392-483):</p> <pre><code># Lines 475-476 - THE BUG\nmeta_state_path = f\"{base_uri}/state\"    # Should be: f\"{base_uri}/meta_state\"\nmeta_runs_path = f\"{base_uri}/runs\"      # Should be: f\"{base_uri}/meta_runs\"\n</code></pre>"},{"location":"catalog_audit_report/#4-readwrite-operations-matrix","title":"4. Read/Write Operations Matrix","text":""},{"location":"catalog_audit_report/#41-meta_state-table","title":"4.1 meta_state Table","text":"Operation Module Function Path Used CREATE catalog.py bootstrap() \u2705 <code>meta_state</code> WRITE state/init.py set_hwm() \u274c Depends on backend creation READ state/init.py get_hwm() \u274c Depends on backend creation"},{"location":"catalog_audit_report/#42-meta_runs-table","title":"4.2 meta_runs Table","text":"Operation Module Function Path Used CREATE catalog.py bootstrap() \u2705 <code>meta_runs</code> WRITE catalog.py log_run() \u2705 <code>tables[\"meta_runs\"]</code> READ state/init.py get_last_run_info() \u274c Depends on backend creation READ catalog.py get_average_duration() \u2705 <code>tables[\"meta_runs\"]</code>"},{"location":"catalog_audit_report/#5-connection-handling","title":"5. Connection Handling","text":""},{"location":"catalog_audit_report/#51-base-uri-construction","title":"5.1 Base URI Construction","text":"<p>Azure Blob:</p> <pre><code>base_uri = f\"abfss://{container}@{account_name}.dfs.core.windows.net/{system.path}\"\n</code></pre> <p>Local:</p> <pre><code>base_uri = os.path.join(base_path, system.path)\n</code></pre>"},{"location":"catalog_audit_report/#52-spark-session-injection","title":"5.2 Spark Session Injection","text":"<ul> <li>PipelineManager correctly injects spark session to CatalogManager</li> <li>Node class gets spark from <code>self.engine.spark</code></li> <li>CatalogStateBackend receives spark via constructor</li> </ul>"},{"location":"catalog_audit_report/#6-error-handling-review","title":"6. Error Handling Review","text":""},{"location":"catalog_audit_report/#61-silent-exception-patterns-found","title":"6.1 Silent Exception Patterns Found","text":"<p>Total: 52 instances of <code>except Exception:</code> without proper handling</p> <p>Critical locations in state/init.py: - Line 66: LocalJSONStateBackend load - Line 164-167: _get_last_run_spark (catches all, returns None) - Line 203-209: _get_last_run_local (catches all, returns None) - Line 232-234: _get_hwm_spark (catches all, returns None) - Line 256-258: _get_hwm_local (catches all, returns None) - Line 335: _spark_table_exists</p>"},{"location":"catalog_audit_report/#62-impact","title":"6.2 Impact","text":"<p>These silent exceptions mask the path mismatch bug: 1. State backend tries to read from <code>/state</code> 2. Table doesn't exist (it's at <code>/meta_state</code>) 3. Exception caught silently 4. Returns None 5. HWM appears to be empty 6. Full data reload instead of incremental</p>"},{"location":"catalog_audit_report/#7-nodepipeline-integration","title":"7. Node/Pipeline Integration","text":""},{"location":"catalog_audit_report/#71-node-class-state-manager-initialization","title":"7.1 Node Class State Manager Initialization","text":"<p>File: <code>odibi/node.py</code> (lines 1442-1461)</p> <pre><code># Initialize State Manager\nif self.catalog_manager and self.catalog_manager.tables:\n    backend = CatalogStateBackend(\n        spark_session=spark_session,\n        meta_state_path=self.catalog_manager.tables.get(\"meta_state\"),  # \u2705 Correct\n        meta_runs_path=self.catalog_manager.tables.get(\"meta_runs\"),    # \u2705 Correct\n    )\nelse:\n    # Fallback to default local paths\n    backend = CatalogStateBackend(\n        spark_session=spark_session,\n        meta_state_path=\".odibi/system/state\",   # \u274c Wrong name\n        meta_runs_path=\".odibi/system/runs\",     # \u274c Wrong name\n    )\n</code></pre>"},{"location":"catalog_audit_report/#72-pipeline-class-state-manager-creation","title":"7.2 Pipeline Class State Manager Creation","text":"<p>File: <code>odibi/pipeline.py</code> (lines 242-256)</p> <pre><code>if resume_from_failure:\n    if self.project_config:\n        backend = create_state_backend(   # \u274c Uses buggy function\n            config=self.project_config,\n            project_root=\".\",\n            spark_session=getattr(self.engine, \"spark\", None),\n        )\n</code></pre>"},{"location":"catalog_audit_report/#8-recommendations","title":"8. Recommendations","text":""},{"location":"catalog_audit_report/#81-critical-fix-path-names","title":"8.1 Critical Fix: Path Names","text":"<p>Fix 1: Update <code>create_state_backend()</code> in <code>state/__init__.py</code>:</p> <pre><code># Change lines 475-476 from:\nmeta_state_path = f\"{base_uri}/state\"\nmeta_runs_path = f\"{base_uri}/runs\"\n\n# To:\nmeta_state_path = f\"{base_uri}/meta_state\"\nmeta_runs_path = f\"{base_uri}/meta_runs\"\n</code></pre> <p>Fix 2: Update Node fallback in <code>node.py</code>:</p> <pre><code># Change lines 1457-1458 from:\nmeta_state_path=\".odibi/system/state\",\nmeta_runs_path=\".odibi/system/runs\",\n\n# To:\nmeta_state_path=\".odibi/system/meta_state\",\nmeta_runs_path=\".odibi/system/meta_runs\",\n</code></pre>"},{"location":"catalog_audit_report/#82-add-logging-to-silent-exceptions","title":"8.2 Add Logging to Silent Exceptions","text":"<p>Replace silent catches with logged warnings:</p> <pre><code>except Exception as e:\n    logger.warning(f\"Failed to get HWM for key '{key}': {e}\")\n    return None\n</code></pre>"},{"location":"catalog_audit_report/#83-add-test-coverage","title":"8.3 Add Test Coverage","text":"<p>Create tests for: 1. Path consistency between catalog and state backend 2. HWM round-trip (set then get) 3. State backend with catalog manager integration</p>"},{"location":"catalog_audit_report/#9-architecture-diagram","title":"9. Architecture Diagram","text":"<p>See generated Mermaid diagrams above showing: 1. Path flow from config to storage 2. Read/write operations and their path sources</p>"},{"location":"catalog_audit_report/#10-files-changed-summary","title":"10. Files Changed Summary","text":"File Changes Needed <code>odibi/state/__init__.py</code> Fix path names (lines 475-476), add logging <code>odibi/node.py</code> Fix fallback path names (lines 1457-1458) <code>tests/</code> Add catalog integration tests"},{"location":"phase_9a_autonomous_learning/","title":"Phase 9.A: Guarded Autonomous Learning (Observation-Only)","text":""},{"location":"phase_9a_autonomous_learning/#implementation-summary","title":"Implementation Summary","text":"<p>Phase 9.A enables unattended, large-scale learning cycles that safely execute real pipelines against frozen datasets while maintaining strict safety guarantees.</p>"},{"location":"phase_9a_autonomous_learning/#key-components","title":"Key Components","text":"Component Location Purpose <code>autonomous_learning.py</code> <code>agents/core/</code> Core module with scheduler, guards, and config <code>LearningModeGuard</code> <code>agents/core/autonomous_learning.py</code> Runtime safety enforcement <code>GuardedCycleRunner</code> <code>agents/core/autonomous_learning.py</code> Extended CycleRunner with learning guards <code>AutonomousLearningScheduler</code> <code>agents/core/autonomous_learning.py</code> Multi-cycle session management Learning Mode Disclaimer <code>agents/core/reports.py</code> Report section for learning cycles Observation Indexing <code>agents/core/indexing.py</code> Extended to index learning observations"},{"location":"phase_9a_autonomous_learning/#files-modified","title":"Files Modified","text":"<ol> <li><code>agents/core/autonomous_learning.py</code> (NEW)</li> <li><code>LearningCycleConfig</code>: Configuration with validation for learning mode</li> <li><code>LearningModeGuard</code>: Runtime guard that detects violations</li> <li><code>GuardedCycleRunner</code>: CycleRunner that skips improvement/review steps</li> <li><code>AutonomousLearningScheduler</code>: Session manager for multi-cycle runs</li> <li> <p><code>validate_learning_profile()</code>: YAML profile validation</p> </li> <li> <p><code>agents/core/reports.py</code></p> </li> <li>Added <code>_generate_learning_mode_disclaimer()</code> method</li> <li> <p>Reports now include \"Learning Mode \u2014 No Changes Applied\" disclaimer</p> </li> <li> <p><code>agents/core/indexing.py</code></p> </li> <li>Added <code>is_learning_mode</code>, <code>observations</code>, <code>patterns_discovered</code> fields</li> <li>Extended <code>_build_document()</code> to extract observations and patterns</li> <li> <p>Added <code>_extract_observations()</code> and <code>_extract_patterns()</code> methods</p> </li> <li> <p><code>agents/core/__init__.py</code></p> </li> <li> <p>Exported all new Phase 9.A components</p> </li> <li> <p><code>agents/core/tests/test_autonomous_learning.py</code> (NEW)</p> </li> <li>38 comprehensive tests for all invariants</li> </ol>"},{"location":"phase_9a_autonomous_learning/#verification-checklist","title":"Verification Checklist","text":""},{"location":"phase_9a_autonomous_learning/#phase-9a-invariants-all-verified","title":"\u2705 Phase 9.A Invariants (ALL VERIFIED)","text":"Invariant Status Enforcement Mechanism \u274c No ImprovementAgent execution \u2705 <code>GUARDED_SKIP_STEPS</code>, <code>LearningModeGuard</code> \u274c No code edits \u2705 <code>LearningModeGuard.assert_no_code_edits()</code> \u274c No file writes outside evidence/reports/artifacts/indexes \u2705 <code>ALLOWED_WRITE_PATHS</code> whitelist \u274c No source downloads \u2705 Profile <code>require_frozen=True</code> enforced \u274c No schema inference \u2705 Learning mode uses frozen pools only \u274c No agent memory writes outside LEARNING scope \u2705 <code>memory_scope: \"learning\"</code> in context \u274c No retries or recovery logic \u2705 Fail-fast on any error"},{"location":"phase_9a_autonomous_learning/#functional-requirements","title":"\u2705 Functional Requirements","text":"Requirement Status Implementation Execute real pipelines \u2705 <code>GuardedCycleRunner.run_learning_cycle()</code> Collect execution evidence \u2705 <code>ExecutionEvidence</code> artifacts persisted Produce human-readable reports \u2705 <code>CycleReportGenerator</code> with learning disclaimer Accumulate indexed learnings \u2705 <code>CycleIndexManager</code> with observations/patterns Wall-clock limit enforcement \u2705 <code>CycleTerminationEnforcer</code> + scheduler limits Step limit enforcement \u2705 <code>max_steps</code> in <code>LearningCycleConfig</code> Clean termination on error \u2705 Try/except with <code>LearningCycleResult.status=FAILED</code>"},{"location":"phase_9a_autonomous_learning/#profile-validation","title":"\u2705 Profile Validation","text":"<p>The <code>large_scale_learning.yaml</code> profile is validated for:</p> <ul> <li><code>mode: learning</code> \u2705</li> <li><code>max_improvements: 0</code> \u2705</li> <li><code>require_frozen: true</code> \u2705</li> <li><code>deterministic: true</code> \u2705</li> <li><code>allow_messy: true</code> \u2705</li> <li><code>allow_tier_mixing: false</code> \u2705</li> <li><code>tiers: tier100gb, tier600gb</code> \u2705</li> </ul>"},{"location":"phase_9a_autonomous_learning/#usage","title":"Usage","text":""},{"location":"phase_9a_autonomous_learning/#run-a-single-learning-cycle","title":"Run a Single Learning Cycle","text":"<pre><code>from agents.core import (\n    AutonomousLearningScheduler,\n    LearningCycleConfig,\n)\n\nscheduler = AutonomousLearningScheduler(odibi_root=\"d:/odibi\")\n\nconfig = LearningCycleConfig(\n    project_root=\"d:/odibi/examples\",\n    max_runtime_hours=8.0,\n)\n\nresult = scheduler.run_single_cycle(config)\nprint(f\"Cycle {result.cycle_id}: {result.status.value}\")\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#run-a-multi-cycle-session","title":"Run a Multi-Cycle Session","text":"<pre><code>session = scheduler.run_session(\n    config=config,\n    max_cycles=100,\n    max_wall_clock_hours=168.0,  # 1 week\n)\n\nprint(f\"Session completed: {session.cycles_completed} cycles\")\nprint(f\"Failed: {session.cycles_failed}\")\nprint(f\"Total duration: {session.total_duration_seconds:.2f}s\")\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#validate-a-profile","title":"Validate a Profile","text":"<pre><code>from agents.core import validate_learning_profile\n\nerrors = validate_learning_profile(\n    \".odibi/cycle_profiles/large_scale_learning.yaml\"\n)\nif errors:\n    for e in errors:\n        print(f\"ERROR: {e}\")\nelse:\n    print(\"Profile is valid for autonomous learning\")\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#test-coverage","title":"Test Coverage","text":"<pre><code>agents/core/tests/test_autonomous_learning.py - 38 tests\n\nTestLearningCycleConfig: 6 tests\nTestLearningModeGuard: 8 tests\nTestGuardedSkipSteps: 5 tests\nTestLearningModeViolation: 2 tests\nTestGuardedCycleRunnerSkipping: 3 tests\nTestLearningCycleResult: 1 test\nTestAutonomousLearningSession: 2 tests\nTestValidateLearningProfile: 6 tests\nTestPhase9AInvariants: 5 tests\n\nAll tests passing \u2705\n</code></pre>"},{"location":"phase_9a_autonomous_learning/#safety-confirmation","title":"Safety Confirmation","text":"<p>This phase is safe to run unattended for days or weeks.</p> <p>The implementation guarantees:</p> <ol> <li>No code mutations: ImprovementAgent and ReviewerAgent are mechanically skipped</li> <li>No proposals: <code>max_improvements=0</code> is enforced at config validation</li> <li>Deterministic execution: Same inputs \u2192 same outputs (verified via hashes)</li> <li>Clean termination: All cycles terminate via max_steps, timeout, or error</li> <li>Audit trail: All cycles produce reports and indexed learnings</li> <li>Fail-fast behavior: Any violation raises <code>LearningModeViolation</code></li> </ol>"},{"location":"phase_9a_autonomous_learning/#non-goals-explicitly-not-implemented","title":"Non-Goals (Explicitly NOT Implemented)","text":"<ul> <li>\u274c New agents</li> <li>\u274c New UI</li> <li>\u274c New policies</li> <li>\u274c Behavior changes to improvement or scheduled modes</li> <li>\u274c Auto-advancement from learning to improvement mode</li> <li>\u274c Memory promotion to validated curriculum</li> </ul>"},{"location":"phase_9a_autonomous_learning/#next-phase-considerations","title":"Next Phase Considerations","text":"<p>When Phase 9.B (Guarded Autonomous Improvement) is implemented:</p> <ol> <li>Create separate <code>ImprovementCycleConfig</code> with <code>max_improvements &gt; 0</code></li> <li>Require explicit human approval to switch from learning to improvement</li> <li>Add golden project requirements for improvement cycles</li> <li>Maintain learning mode as the default for autonomous execution</li> </ol>"},{"location":"phase_9c_cycle_profiles/","title":"Phase 9.C: Cycle Profiles","text":""},{"location":"phase_9c_cycle_profiles/#overview","title":"Overview","text":"<p>Phase 9.C makes cycle profiles executable, auditable, and selectable from the UI.</p> <p>Cycle profiles are YAML configuration files stored in <code>.odibi/cycle_profiles/</code> that define deterministic learning behavior. They replace hardcoded parameters in the scheduler with versioned, hashable, frozen configurations.</p>"},{"location":"phase_9c_cycle_profiles/#goals","title":"Goals","text":"<ul> <li>\u2705 Profiles loaded from <code>.odibi/cycle_profiles/</code></li> <li>\u2705 Profile config frozen for entire session</li> <li>\u2705 Profile name + hash recorded in heartbeat, reports, session metadata</li> <li>\u2705 UI dropdown to select profiles</li> <li>\u2705 Same profile + same inputs \u2192 same execution</li> </ul>"},{"location":"phase_9c_cycle_profiles/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        cycle_profile.py                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  CycleProfile (Pydantic)    \u2500\u2500\u2500\u2500\u2500\u25ba  FrozenCycleProfile          \u2502\n\u2502      \u2191                                   \u2193                      \u2502\n\u2502  YAML File                    LearningCycleConfig               \u2502\n\u2502      \u2191                                   \u2193                      \u2502\n\u2502  CycleProfileLoader  \u25c4\u2500\u2500\u2500\u2500\u2500\u2500  AutonomousLearningScheduler       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#components","title":"Components","text":""},{"location":"phase_9c_cycle_profiles/#1-cycleprofile-pydantic-schema","title":"1. CycleProfile (Pydantic Schema)","text":"<p>Located in <code>agents/core/cycle_profile.py</code>.</p> <p>Validates profile YAML against Phase 9.A learning mode invariants:</p> <pre><code>from agents.core import CycleProfile\n\nprofile = CycleProfile(\n    profile_id=\"large_scale_learning\",\n    profile_name=\"Learning at Scale\",\n    mode=\"learning\",           # Must be \"learning\"\n    max_improvements=0,        # Must be 0\n    cycle_source_config=CycleSourceConfigSchema(\n        require_frozen=True,   # Must be True\n        deterministic=True,    # Must be True\n    ),\n    guardrails=GuardrailsSchema(\n        allow_source_mutation=False,  # Must be False\n    ),\n)\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#2-frozencycleprofile-immutable-runtime-config","title":"2. FrozenCycleProfile (Immutable Runtime Config)","text":"<p>A frozen dataclass that cannot be modified after loading:</p> <pre><code>from agents.core import CycleProfileLoader\n\nloader = CycleProfileLoader(odibi_root=\"d:/odibi\")\nfrozen = loader.load_profile(\"large_scale_learning\")\n\n# Immutable - this raises an error:\nfrozen.profile_id = \"changed\"  # \u274c FrozenInstanceError\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#3-profile-hash","title":"3. Profile Hash","text":"<p>Every profile has a SHA-256 content hash (first 16 chars) for auditability:</p> <pre><code>from agents.core import compute_profile_hash\n\ncontent = open(\"profile.yaml\").read()\nhash = compute_profile_hash(content)  # e.g., \"a1b2c3d4e5f6g7h8\"\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#4-cycleprofileloader","title":"4. CycleProfileLoader","text":"<p>Discovers and loads profiles from <code>.odibi/cycle_profiles/</code>:</p> <pre><code>from agents.core import CycleProfileLoader\n\nloader = CycleProfileLoader(\"d:/odibi\")\n\n# List available profiles\nprofiles = loader.list_profiles()  # [\"large_scale_learning\", \"quick_test\"]\n\n# Load and freeze a profile\nfrozen = loader.load_profile(\"large_scale_learning\")\n\n# Get summary for UI display\nsummary = loader.get_profile_summary(\"large_scale_learning\")\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#usage","title":"Usage","text":""},{"location":"phase_9c_cycle_profiles/#running-a-session-with-a-profile","title":"Running a Session with a Profile","text":"<pre><code>from agents.core import AutonomousLearningScheduler\n\nscheduler = AutonomousLearningScheduler(odibi_root=\"d:/odibi\")\n\n# Preferred: Use run_session_with_profile()\nsession = scheduler.run_session_with_profile(\n    profile_name=\"large_scale_learning\",\n    project_root=\"d:/odibi/examples\",\n    max_cycles=10,\n    max_wall_clock_hours=8.0,\n)\n\n# Alternative: Create config from frozen profile\nfrozen = scheduler.load_profile(\"large_scale_learning\")\nconfig = LearningCycleConfig.from_frozen_profile(\n    profile=frozen,\n    project_root=\"d:/odibi/examples\",\n)\nsession = scheduler.run_session(config=config)\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#creating-a-profile","title":"Creating a Profile","text":"<p>Create a YAML file in <code>.odibi/cycle_profiles/</code>:</p> <pre><code># .odibi/cycle_profiles/my_learning.yaml\n\nprofile_id: my_learning\nprofile_name: \"My Learning Profile\"\nprofile_version: \"1.0.0\"\ndescription: |\n  Custom learning profile for my use case.\n\nmode: learning\nmax_improvements: 0\n\ncycle_source_config:\n  selection_policy: learning_default\n  allowed_tiers:\n    - tier100gb\n  max_sources: 2\n  require_frozen: true\n  deterministic: true\n\nguardrails:\n  allow_execution: false\n  allow_downloads: false\n  allow_source_mutation: false\n  max_duration_hours: 12\n\nmetadata:\n  author: you\n  intended_use: \"Custom learning runs\"\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#auditability","title":"Auditability","text":""},{"location":"phase_9c_cycle_profiles/#heartbeat","title":"Heartbeat","text":"<p>The heartbeat file includes profile info:</p> <pre><code>{\n  \"last_cycle_id\": \"abc123\",\n  \"timestamp\": \"2024-01-01T12:00:00\",\n  \"profile_id\": \"large_scale_learning\",\n  \"profile_name\": \"Learning at Scale\",\n  \"profile_hash\": \"a1b2c3d4e5f6g7h8\"\n}\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#cycle-reports","title":"Cycle Reports","text":"<p>Reports include a profile section:</p> <pre><code>## Cycle Metadata\n\n- **Cycle ID:** `abc123`\n- **Mode:** learning\n\n### Cycle Profile\n\n- **Profile ID:** `large_scale_learning`\n- **Profile Name:** Learning at Scale\n- **Profile Hash:** `a1b2c3d4e5f6g7h8`\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#session-metadata","title":"Session Metadata","text":"<pre><code>session.profile_id     # \"large_scale_learning\"\nsession.profile_name   # \"Learning at Scale\"  \nsession.profile_hash   # \"a1b2c3d4e5f6g7h8\"\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#ui-integration","title":"UI Integration","text":"<p>The Learning Session panel includes:</p> <ol> <li>Profile Dropdown - Select from available profiles</li> <li>Profile Info Display - Shows name, description, and hash</li> <li>Refresh Button - Reload available profiles</li> <li>Status Display - Shows active profile in session monitor</li> </ol>"},{"location":"phase_9c_cycle_profiles/#safety-invariants","title":"Safety Invariants","text":"<p>Phase 9.C maintains all Phase 9.A learning mode safety guards:</p> <ul> <li>\u274c No ImprovementAgent execution</li> <li>\u274c No code edits</li> <li>\u274c No file writes outside allowed directories</li> <li>\u274c No profile modification at runtime</li> <li>\u274c No agent writes to profiles</li> <li>\u2705 Profile frozen for entire session</li> <li>\u2705 Deterministic execution</li> <li>\u2705 Full audit trail</li> </ul>"},{"location":"phase_9c_cycle_profiles/#error-handling","title":"Error Handling","text":"<p>Profile loading fails fast on:</p> <ul> <li>Invalid YAML syntax \u2192 <code>CycleProfileError</code></li> <li>Missing required fields \u2192 <code>CycleProfileError</code> with validation errors</li> <li>Learning mode invariant violations \u2192 <code>ValueError</code></li> <li>Profile not found \u2192 <code>CycleProfileError</code></li> </ul> <pre><code>try:\n    frozen = loader.load_profile(\"invalid_profile\")\nexcept CycleProfileError as e:\n    print(f\"Profile error: {e}\")\n    print(f\"Validation errors: {e.errors}\")\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#testing","title":"Testing","text":"<pre><code># Run profile tests\npytest agents/core/tests/test_cycle_profile.py -v\n\n# Run all Phase 9 tests\npytest agents/core/tests/test_autonomous_learning.py -v\npytest agents/core/tests/test_disk_guard.py -v\n</code></pre>"},{"location":"phase_9c_cycle_profiles/#files-changed","title":"Files Changed","text":"<ul> <li><code>agents/core/cycle_profile.py</code> - New module</li> <li><code>agents/core/autonomous_learning.py</code> - Profile integration</li> <li><code>agents/core/disk_guard.py</code> - HeartbeatData profile fields</li> <li><code>agents/core/reports.py</code> - Profile section in reports</li> <li><code>agents/core/__init__.py</code> - Exports</li> <li><code>agents/ui/components/cycle_panel.py</code> - UI dropdown</li> <li><code>agents/core/tests/test_cycle_profile.py</code> - New tests</li> </ul>"},{"location":"source_pools_design/","title":"Source Pools Design Document","text":"<p>Phase: 7.B.1 (Preparation Only) Status: Design Complete Last Updated: 2024-12-14</p>"},{"location":"source_pools_design/#1-overview","title":"1. Overview","text":""},{"location":"source_pools_design/#11-purpose","title":"1.1 Purpose","text":"<p>Source Pools provide deterministic, replayable test data that exercises all supported Odibi data types and ingestion paths. They enable:</p> <ul> <li>Reproducible test runs across environments</li> <li>Coverage of all file formats and source types</li> <li>Testing of both clean and messy data scenarios</li> <li>Cryptographic verification of data integrity</li> </ul>"},{"location":"source_pools_design/#12-design-principles","title":"1.2 Design Principles","text":"Principle Description Determinism All data is pre-generated and frozen. No random generation at runtime. Explicit Schemas No schema inference. All types declared upfront. Disk-Backed All sources are local files with SHA256 hashes. Immutability Frozen pools cannot be modified during test cycles. Discoverability Central index enables programmatic pool discovery."},{"location":"source_pools_design/#13-non-goals","title":"1.3 Non-Goals","text":"<ul> <li>\u274c Runtime data generation</li> <li>\u274c Live API connections</li> <li>\u274c Authentication-requiring sources</li> <li>\u274c Agent autonomy to modify pools during cycles</li> <li>\u274c Schema inference at runtime</li> </ul>"},{"location":"source_pools_design/#2-sourcepool-schema","title":"2. SourcePool Schema","text":""},{"location":"source_pools_design/#21-pydantic-model-location","title":"2.1 Pydantic Model Location","text":"<pre><code>odibi/testing/source_pool.py\n</code></pre>"},{"location":"source_pools_design/#22-core-classes","title":"2.2 Core Classes","text":"Class Purpose <code>FileFormat</code> Enum: csv, json, parquet, avro, delta <code>SourceType</code> Enum: local, adls_emulated, azure_blob_emulated, sql_jdbc_local, cloudfiles <code>DataQuality</code> Enum: clean, messy, mixed <code>PoolStatus</code> Enum: draft, frozen, deprecated <code>ColumnSchema</code> Column definition (name, dtype, nullable, etc.) <code>TableSchema</code> Table schema with columns and keys <code>DataCharacteristics</code> Metadata about data properties <code>IntegrityManifest</code> SHA256 hashes for frozen pools <code>SourcePoolConfig</code> Main pool definition <code>SourcePoolIndex</code> Registry of all pools"},{"location":"source_pools_design/#23-schema-example","title":"2.3 Schema Example","text":"<pre><code>pool_id: nyc_taxi_csv_clean\nversion: \"1.0.0\"\nname: \"NYC Taxi Trips - CSV Clean\"\nfile_format: csv\nsource_type: local\ndata_quality: clean\nschema:\n  columns:\n    - name: trip_id\n      dtype: string\n      nullable: false\n      primary_key: true\n    - name: fare_amount\n      dtype: float64\n      nullable: false\n  primary_keys: [trip_id]\ncache_path: \"nyc_taxi/csv/clean/\"\ncharacteristics:\n  row_count: 10000\n  has_nulls: false\nstatus: frozen\nintegrity:\n  algorithm: sha256\n  file_hashes:\n    \"data.csv\": \"abc123...\"\n  manifest_hash: \"def456...\"\n  frozen_at: \"2024-12-14T00:00:00Z\"\n</code></pre>"},{"location":"source_pools_design/#3-proposed-source-pools","title":"3. Proposed Source Pools","text":""},{"location":"source_pools_design/#31-coverage-matrix","title":"3.1 Coverage Matrix","text":"Pool ID Format Source Type Quality Row Count Primary Use Case <code>nyc_taxi_csv_clean</code> CSV LOCAL Clean 10,000 Bronze ingestion baseline <code>nyc_taxi_csv_messy</code> CSV LOCAL Messy 5,000 Validation &amp; quarantine <code>github_events_json_clean</code> JSON LOCAL Clean 10,000 JSON parsing, nested structures <code>tpch_lineitem_parquet</code> Parquet LOCAL Clean 60,175 Columnar reads, partitioning <code>synthetic_customers_avro</code> Avro LOCAL Clean 5,000 Schema evolution, complex types <code>cdc_orders_delta</code> Delta LOCAL Clean 1,200 CDC/MERGE, time travel <code>northwind_sqlite</code> CSV* SQL_JDBC_LOCAL Clean 830 SQL ingestion, HWM <code>edge_cases_mixed</code> CSV LOCAL Messy 500 All edge cases, regression <p>*Extracted from SQLite database</p>"},{"location":"source_pools_design/#32-format-coverage","title":"3.2 Format Coverage","text":"<ul> <li>\u2705 CSV (3 pools)</li> <li>\u2705 JSON (1 pool)</li> <li>\u2705 Parquet (1 pool)</li> <li>\u2705 Avro (1 pool)</li> <li>\u2705 Delta (1 pool)</li> </ul>"},{"location":"source_pools_design/#33-source-type-coverage","title":"3.3 Source Type Coverage","text":"<ul> <li>\u2705 LOCAL (6 pools)</li> <li>\u2705 SQL_JDBC_LOCAL (1 pool)</li> <li>\ud83d\udd32 ADLS_EMULATED (planned Phase 7.C)</li> <li>\ud83d\udd32 AZURE_BLOB_EMULATED (planned Phase 7.C)</li> <li>\ud83d\udd32 CLOUDFILES (planned Phase 7.C)</li> </ul>"},{"location":"source_pools_design/#34-data-quality-coverage","title":"3.4 Data Quality Coverage","text":"<ul> <li>\u2705 Clean (6 pools)</li> <li>\u2705 Messy (2 pools)</li> </ul>"},{"location":"source_pools_design/#4-recommended-public-datasets","title":"4. Recommended Public Datasets","text":""},{"location":"source_pools_design/#41-real-datasets-for-local-caching","title":"4.1 Real Datasets for Local Caching","text":"Dataset Source License Use For NYC TLC Taxi Data nyc.gov/tlc Public Domain CSV ingestion, large files GitHub Archive gharchive.org CC BY 4.0 JSON/NDJSON, nested structures TPC-H tpc.org TPC Fair Use Parquet, benchmarking Northwind Database GitHub Public Domain SQLite/JDBC testing Faker-based Synthetic Generated locally CC0 Avro, controlled schema"},{"location":"source_pools_design/#42-data-preparation-guidelines","title":"4.2 Data Preparation Guidelines","text":"<ol> <li>Download once \u2192 Store in <code>.odibi/source_cache/</code></li> <li>Subset if needed \u2192 Keep pools &lt; 100MB each</li> <li>Convert formats \u2192 Generate Parquet/Avro from CSV</li> <li>Freeze with hashes \u2192 Generate <code>IntegrityManifest</code></li> <li>Never re-download during cycles \u2192 All data pre-staged</li> </ol>"},{"location":"source_pools_design/#5-disk-layout","title":"5. Disk Layout","text":""},{"location":"source_pools_design/#51-directory-structure","title":"5.1 Directory Structure","text":"<pre><code>.odibi/\n\u251c\u2500\u2500 source_cache/                    # Actual data files\n\u2502   \u251c\u2500\u2500 nyc_taxi/\n\u2502   \u2502   \u251c\u2500\u2500 csv/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 clean/\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 data.csv         # Frozen data file\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 messy/\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 data.csv\n\u2502   \u2502   \u2514\u2500\u2500 parquet/                 # Future: same data in Parquet\n\u2502   \u251c\u2500\u2500 github_events/\n\u2502   \u2502   \u2514\u2500\u2500 json/\n\u2502   \u2502       \u2514\u2500\u2500 clean/\n\u2502   \u2502           \u2514\u2500\u2500 events.ndjson\n\u2502   \u251c\u2500\u2500 tpch/\n\u2502   \u2502   \u2514\u2500\u2500 parquet/\n\u2502   \u2502       \u2514\u2500\u2500 lineitem/\n\u2502   \u2502           \u251c\u2500\u2500 l_shipdate_year=1992/\n\u2502   \u2502           \u2502   \u2514\u2500\u2500 part-0000.parquet\n\u2502   \u2502           \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 synthetic/\n\u2502   \u2502   \u2514\u2500\u2500 avro/\n\u2502   \u2502       \u2514\u2500\u2500 customers/\n\u2502   \u2502           \u2514\u2500\u2500 customers.avro\n\u2502   \u251c\u2500\u2500 cdc/\n\u2502   \u2502   \u2514\u2500\u2500 delta/\n\u2502   \u2502       \u2514\u2500\u2500 orders/\n\u2502   \u2502           \u251c\u2500\u2500 _delta_log/      # Delta transaction log\n\u2502   \u2502           \u2514\u2500\u2500 part-0000.parquet\n\u2502   \u251c\u2500\u2500 northwind/\n\u2502   \u2502   \u2514\u2500\u2500 sqlite/\n\u2502   \u2502       \u2514\u2500\u2500 northwind.db         # SQLite database file\n\u2502   \u2514\u2500\u2500 edge_cases/\n\u2502       \u2514\u2500\u2500 mixed/\n\u2502           \u251c\u2500\u2500 data.csv\n\u2502           \u251c\u2500\u2500 data.json\n\u2502           \u2514\u2500\u2500 data.parquet\n\u2502\n\u2514\u2500\u2500 source_metadata/                 # Pool definitions &amp; index\n    \u251c\u2500\u2500 pool_index.yaml              # Central registry\n    \u2514\u2500\u2500 pools/                       # Individual pool metadata\n        \u251c\u2500\u2500 nyc_taxi_csv_clean.yaml\n        \u251c\u2500\u2500 nyc_taxi_csv_messy.yaml\n        \u251c\u2500\u2500 github_events_json_clean.yaml\n        \u251c\u2500\u2500 tpch_lineitem_parquet.yaml\n        \u251c\u2500\u2500 synthetic_customers_avro.yaml\n        \u251c\u2500\u2500 cdc_orders_delta.yaml\n        \u251c\u2500\u2500 northwind_sqlite.yaml\n        \u2514\u2500\u2500 edge_cases_mixed.yaml\n</code></pre>"},{"location":"source_pools_design/#52-file-naming-conventions","title":"5.2 File Naming Conventions","text":"Component Convention Example Pool ID <code>snake_case</code> <code>nyc_taxi_csv_clean</code> Cache Path <code>{dataset}/{format}/{quality}/</code> <code>nyc_taxi/csv/clean/</code> Metadata File <code>{pool_id}.yaml</code> <code>nyc_taxi_csv_clean.yaml</code> Data Files Format-specific <code>data.csv</code>, <code>events.ndjson</code>, <code>part-*.parquet</code>"},{"location":"source_pools_design/#6-registration-freezing-and-indexing","title":"6. Registration, Freezing, and Indexing","text":""},{"location":"source_pools_design/#61-pool-lifecycle","title":"6.1 Pool Lifecycle","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      POOL LIFECYCLE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   DRAFT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; FROZEN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; DEPRECATED   \u2502\n\u2502     \u2502                      \u2502                       \u2502        \u2502\n\u2502     \u2502 - Schema defined     \u2502 - Data prepared       \u2502 - EOL  \u2502\n\u2502     \u2502 - No data yet        \u2502 - Hashes computed     \u2502 - Keep \u2502\n\u2502     \u2502 - Can modify         \u2502 - IMMUTABLE           \u2502   for  \u2502\n\u2502     \u2502                      \u2502 - Used in tests       \u2502   ref  \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"source_pools_design/#62-freezing-process","title":"6.2 Freezing Process","text":"<ol> <li>Validate Schema: Ensure all columns match actual data</li> <li>Compute File Hashes: SHA256 for each file in <code>cache_path</code></li> <li>Compute Manifest Hash: SHA256 of sorted <code>{path: hash}</code> pairs</li> <li>Update Metadata: Set <code>status: frozen</code>, add <code>integrity</code> block</li> <li>Update Index: Ensure pool is registered in <code>pool_index.yaml</code></li> </ol>"},{"location":"source_pools_design/#63-verification-process","title":"6.3 Verification Process","text":"<pre><code>def verify_pool(pool: SourcePoolConfig) -&gt; bool:\n    \"\"\"Verify frozen pool integrity.\"\"\"\n    if pool.status != PoolStatus.FROZEN:\n        return True  # Only verify frozen pools\n\n    for file_path, expected_hash in pool.integrity.file_hashes.items():\n        actual_hash = compute_sha256(pool.cache_path / file_path)\n        if actual_hash != expected_hash:\n            return False\n    return True\n</code></pre>"},{"location":"source_pools_design/#64-index-operations","title":"6.4 Index Operations","text":"Operation Description When <code>register_pool()</code> Add pool to index After creating metadata <code>freeze_pool()</code> Compute hashes, update status After data is ready <code>deprecate_pool()</code> Mark as deprecated Before removal <code>list_pools()</code> Enumerate all pools Test discovery <code>get_pool()</code> Load pool by ID Test setup <code>verify_pool()</code> Check integrity Before test run"},{"location":"source_pools_design/#7-agent-rules","title":"7. Agent Rules","text":""},{"location":"source_pools_design/#71-what-agents-may-do-with-sources","title":"7.1 What Agents MAY Do with Sources","text":"Action Allowed Notes Read pool metadata \u2705 YES Discovery and planning Read cached data files \u2705 YES For test execution List available pools \u2705 YES Test coverage analysis Verify pool integrity \u2705 YES Pre-test validation Use pools in pipelines \u2705 YES Primary purpose Report on pool coverage \u2705 YES Observability"},{"location":"source_pools_design/#72-what-agents-may-not-do-with-sources","title":"7.2 What Agents MAY NOT Do with Sources","text":"Action Forbidden Reason Modify frozen data files \u274c NO Breaks determinism Delete pool metadata \u274c NO Breaks registry Create new pools during cycles \u274c NO Phase 7.B.1 constraint Download live data during cycles \u274c NO No network during tests Change pool status \u274c NO Lifecycle is controlled Add entries to <code>pool_index.yaml</code> \u274c NO Human-controlled Infer schemas at runtime \u274c NO Explicit schemas only"},{"location":"source_pools_design/#73-enforcement-invariants","title":"7.3 Enforcement Invariants","text":"<pre><code># These invariants MUST hold during any test cycle:\n\nINVARIANT_1 = \"pool_index.yaml is read-only during execution\"\nINVARIANT_2 = \"source_cache/ contents match integrity manifests\"\nINVARIANT_3 = \"No network calls during pool access\"\nINVARIANT_4 = \"Schema comes from metadata, not from data inspection\"\nINVARIANT_5 = \"Frozen pools are immutable\"\n</code></pre>"},{"location":"source_pools_design/#8-implementation-recommendations","title":"8. Implementation Recommendations","text":""},{"location":"source_pools_design/#81-phase-7b2-next-steps","title":"8.1 Phase 7.B.2 (Next Steps)","text":"<ol> <li>Data Preparation Script</li> <li>Download public datasets</li> <li>Convert to target formats</li> <li>Subset to manageable sizes</li> <li> <p>Generate hash manifests</p> </li> <li> <p>Freeze Tool <code>bash    python -m odibi.testing.freeze_pool --pool-id nyc_taxi_csv_clean</code></p> </li> <li> <p>Verification Tool <code>bash    python -m odibi.testing.verify_pools --all</code></p> </li> </ol>"},{"location":"source_pools_design/#82-integration-points","title":"8.2 Integration Points","text":"Component Integration <code>TestRunner</code> Load pools via <code>SourcePoolIndex</code> <code>ReadConfig</code> Accept <code>source_pool</code> as input type <code>ExecutionGateway</code> Verify pool integrity before cycle <code>Story</code> Record which pools were used"},{"location":"source_pools_design/#83-future-extensions","title":"8.3 Future Extensions","text":"<ul> <li>ADLS Emulator: Azurite for blob storage testing</li> <li>CloudFiles Simulation: Directory-based auto-ingest</li> <li>Schema Evolution: Multiple versions per pool</li> <li>Streaming Pools: Simulated real-time data feeds</li> </ul>"},{"location":"source_pools_design/#9-appendix","title":"9. Appendix","text":""},{"location":"source_pools_design/#91-example-integrity-manifest","title":"9.1 Example Integrity Manifest","text":"<pre><code>integrity:\n  algorithm: sha256\n  file_hashes:\n    \"data.csv\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"\n    \"schema.json\": \"d7a8fbb307d7809469ca9abcb0082e4f8d5651e46d3cdb762d02d0bf37c9e592\"\n  manifest_hash: \"a948904f2f0f479b8f8564cbf12dac6b89824ca8f989c8c44c3a\"\n  frozen_at: \"2024-12-14T10:30:00Z\"\n  frozen_by: \"prep-script-v1\"\n</code></pre>"},{"location":"source_pools_design/#92-valid-data-types","title":"9.2 Valid Data Types","text":"Type Python Spark Description <code>string</code> <code>str</code> <code>StringType</code> Text data <code>int64</code> <code>int</code> <code>LongType</code> 64-bit integer <code>float64</code> <code>float</code> <code>DoubleType</code> 64-bit float <code>bool</code> <code>bool</code> <code>BooleanType</code> Boolean <code>datetime</code> <code>datetime</code> <code>TimestampType</code> Timestamp <code>date</code> <code>date</code> <code>DateType</code> Date only <code>binary</code> <code>bytes</code> <code>BinaryType</code> Raw bytes"},{"location":"source_pools_design/#93-edge-case-categories","title":"9.3 Edge Case Categories","text":"Category Examples Unicode Emoji, RTL text, combining characters Null Variants NULL, empty string, whitespace-only Numeric Edge NaN, Inf, -Inf, MAX_INT, MIN_INT, -0 Date Edge 1970-01-01, 2000-01-01, 2038-01-19, far future Injection SQL injection, JSON breaking, CSV escaping Length Empty, single char, 10KB string Duplicates Exact duplicates, case-insensitive duplicates"},{"location":"source_tiers/","title":"Source Tiers - Offline Data Preparation","text":"<p>Phase: 7.E Status: Active Last Updated: 2025-12-14</p>"},{"location":"source_tiers/#overview","title":"Overview","text":"<p>Source Tiers provide disk-backed, deterministic datasets at various scales for Odibi testing and development. All data is:</p> <ul> <li>Deterministic: Fixed seed-based transformations, fully reproducible</li> <li>Hash-locked: SHA256 integrity manifests for every file</li> <li>Replayable: No network access needed after initial download</li> <li>Scalable: From 1GB (tier0) to 2TB (tier2tb)</li> </ul>"},{"location":"source_tiers/#tier-summary","title":"Tier Summary","text":"Tier Size Datasets Use Case <code>tier0</code> ~1GB 1 Quick testing, CI <code>tier20gb</code> ~20GB 2 Local development <code>tier100gb</code> ~100GB 2 Integration testing <code>tier600gb</code> ~600GB 3 Large-scale validation <code>tier2tb</code> ~2TB 6 Full production simulation"},{"location":"source_tiers/#tier-details","title":"Tier Details","text":""},{"location":"source_tiers/#tier0-minimal-1gb","title":"tier0 (Minimal - ~1GB)","text":"<p>Quick testing tier for CI and rapid iteration.</p> Dataset Format Size Source <code>sample_enwiki_pages</code> xml.bz2 ~300MB Wikimedia Foundation (Simple English Wikipedia) <p>Use Cases: - CI pipeline testing - Quick smoke tests - Schema validation</p>"},{"location":"source_tiers/#tier20gb-small-20gb","title":"tier20gb (Small - ~20GB)","text":"<p>Local development tier with real-world datasets.</p> Dataset Format Size Source Messy Variant <code>enwiki_all_titles</code> gz ~400MB Wikimedia Foundation No <code>osm_liechtenstein</code> osm.pbf ~3MB OpenStreetMap / Geofabrik Yes (seed=42) <p>Use Cases: - Local development testing - Feature development - Performance baselines</p>"},{"location":"source_tiers/#tier100gb-medium-100gb","title":"tier100gb (Medium - ~100GB)","text":"<p>Integration testing tier with full Wikipedia and OSM extracts.</p> Dataset Format Size Source Messy Variant <code>enwiki_pages_articles</code> xml.bz2 ~22GB Wikimedia Foundation Yes (seed=12345) <code>osm_iceland</code> osm.pbf ~60MB OpenStreetMap / Geofabrik No <p>Messy Variant Recipe (enwiki_pages_articles):</p> <pre><code>operations:\n  - inject_nulls\n  - type_drift\n  - invalid_timestamps\nnull_rate: 0.01\ntype_drift_rate: 0.005\ninvalid_timestamp_rate: 0.002\nseed: 12345\n</code></pre>"},{"location":"source_tiers/#tier600gb-large-600gb","title":"tier600gb (Large - ~600GB)","text":"<p>Large-scale validation tier with massive real-world datasets.</p> Dataset Format Size Source Messy Variant <code>fineweb_cc_2024_10_sample</code> parquet ~500GB Hugging Face / FineWeb Yes (seed=77777) <code>osm_planet</code> osm.pbf ~75GB OpenStreetMap Foundation No <code>enwiki_pages_articles_full</code> xml.bz2 ~22GB Wikimedia Foundation No <p>FineWeb Dataset: - Common Crawl web text dataset - License: ODC-BY 1.0 - Columns: <code>text</code>, <code>id</code>, <code>dump</code>, <code>url</code>, <code>date</code>, <code>file_path</code>, <code>language</code>, <code>language_score</code></p> <p>Messy Variant Recipe (fineweb_cc_2024_10_sample):</p> <pre><code>operations:\n  - inject_nulls\n  - duplicate_rows\n  - type_drift\n  - truncate_lines\nnull_rate: 0.02\nduplicate_rate: 0.01\ntype_drift_rate: 0.005\ntruncate_rate: 0.003\nseed: 77777\n</code></pre>"},{"location":"source_tiers/#tier2tb-massive-2tb","title":"tier2tb (Massive - ~2TB)","text":"<p>Full production simulation tier with multiple FineWeb snapshots.</p> <p>Includes all <code>tier600gb</code> datasets plus:</p> Dataset Format Size Source <code>fineweb_cc_2024_06_sample</code> parquet ~500GB Hugging Face / FineWeb <code>fineweb_cc_2024_02_sample</code> parquet ~500GB Hugging Face / FineWeb <code>fineweb_cc_2023_10_sample</code> parquet ~500GB Hugging Face / FineWeb"},{"location":"source_tiers/#storage-budget","title":"Storage Budget","text":"Tier Raw Data Messy Variants Metadata Total tier0 1 GB 0 GB &lt;1 MB ~1 GB tier20gb 20 GB 0.5 GB &lt;1 MB ~21 GB tier100gb 100 GB 25 GB &lt;1 MB ~125 GB tier600gb 600 GB 100 GB &lt;1 MB ~700 GB tier2tb 2,000 GB 200 GB &lt;1 MB ~2.2 TB <p>Note: Messy variants are optional and require <code>--build-messy-variants</code> flag.</p>"},{"location":"source_tiers/#directory-layout","title":"Directory Layout","text":"<pre><code>.odibi/\n\u251c\u2500\u2500 source_cache/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 integrity_manifests.json\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 tiers/\n\u2502       \u251c\u2500\u2500 tier0/\n\u2502       \u2502   \u2514\u2500\u2500 sample_enwiki_pages/\n\u2502       \u2502       \u2514\u2500\u2500 raw/\n\u2502       \u2502           \u2514\u2500\u2500 simplewiki-latest-pages-articles.xml.bz2\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 tier20gb/\n\u2502       \u2502   \u251c\u2500\u2500 enwiki_abstract/\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 raw/\n\u2502       \u2502   \u2514\u2500\u2500 osm_liechtenstein/\n\u2502       \u2502       \u251c\u2500\u2500 raw/\n\u2502       \u2502       \u2514\u2500\u2500 messy/\n\u2502       \u2502           \u251c\u2500\u2500 liechtenstein-latest_messy.osm.pbf\n\u2502       \u2502           \u2514\u2500\u2500 transform_log.json\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 tier100gb/\n\u2502       \u2502   \u251c\u2500\u2500 enwiki_pages_articles/\n\u2502       \u2502   \u2514\u2500\u2500 osm_iceland/\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 tier600gb/\n\u2502       \u2502   \u251c\u2500\u2500 fineweb_cc_2024_10_sample/\n\u2502       \u2502   \u251c\u2500\u2500 osm_planet/\n\u2502       \u2502   \u2514\u2500\u2500 enwiki_pages_articles_full/\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 tier2tb/\n\u2502           \u2514\u2500\u2500 [all tier600gb + additional fineweb snapshots]\n\u2502\n\u2514\u2500\u2500 source_metadata/\n    \u251c\u2500\u2500 pool_index.yaml\n    \u2514\u2500\u2500 pools/\n        \u251c\u2500\u2500 tier0_sample_enwiki_pages.yaml\n        \u251c\u2500\u2500 tier20gb_enwiki_abstract.yaml\n        \u251c\u2500\u2500 tier20gb_osm_liechtenstein.yaml\n        \u251c\u2500\u2500 tier20gb_osm_liechtenstein_messy.yaml\n        \u2514\u2500\u2500 [...]\n</code></pre>"},{"location":"source_tiers/#usage","title":"Usage","text":""},{"location":"source_tiers/#list-available-tiers","title":"List Available Tiers","text":"<pre><code>python scripts/prepare_source_tiers.py --list-tiers\n</code></pre>"},{"location":"source_tiers/#dry-run-show-what-would-happen","title":"Dry Run (Show What Would Happen)","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier600gb --download --verify --dry-run\n</code></pre>"},{"location":"source_tiers/#download-and-verify","title":"Download and Verify","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier0 --download --verify\n</code></pre>"},{"location":"source_tiers/#build-messy-variants","title":"Build Messy Variants","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier100gb --build-messy-variants\n</code></pre>"},{"location":"source_tiers/#full-preparation","title":"Full Preparation","text":"<pre><code>python scripts/prepare_source_tiers.py --tier tier20gb --download --verify --build-messy-variants --emit-metadata\n</code></pre>"},{"location":"source_tiers/#resume-interrupted-download","title":"Resume Interrupted Download","text":"<p>Simply rerun the same command - downloads are resume-capable:</p> <pre><code>python scripts/prepare_source_tiers.py --tier tier600gb --download\n</code></pre>"},{"location":"source_tiers/#invariants","title":"Invariants","text":"<ol> <li>No Network During Cycles: Once prepared, tiers require no network access</li> <li>Hash Verification: All files verified against SHA256 manifests</li> <li>Atomic Writes: Temp files + atomic rename prevents corruption</li> <li>Deterministic Messy Variants: Fixed seed ensures reproducible transforms</li> <li>Frozen Pools: <code>lifecycle: FROZEN</code> in pool metadata</li> <li>No Agent Modifications: Agents MAY NOT modify tier data</li> </ol>"},{"location":"source_tiers/#integrity-verification","title":"Integrity Verification","text":"<p>All tier datasets are registered in <code>integrity_manifests.json</code>:</p> <pre><code>{\n  \"tier600gb_fineweb_cc_2024_10_sample\": {\n    \"algorithm\": \"sha256\",\n    \"file_hashes\": {\n      \"000_00000.parquet\": \"abc123...\"\n    },\n    \"manifest_hash\": \"def456...\",\n    \"frozen_at\": \"2025-12-14T10:00:00Z\",\n    \"frozen_by\": \"prepare_source_tiers.py\"\n  }\n}\n</code></pre> <p>To verify manually:</p> <pre><code>import hashlib\nfrom pathlib import Path\n\ndef verify_file(path: Path, expected_hash: str) -&gt; bool:\n    actual = hashlib.sha256(path.read_bytes()).hexdigest()\n    return actual == expected_hash\n</code></pre>"},{"location":"source_tiers/#messy-variant-transforms","title":"Messy Variant Transforms","text":"<p>Available transform operations:</p> Operation Description <code>inject_nulls</code> Replace random values with empty strings/null <code>duplicate_rows</code> Insert duplicate rows at random positions <code>type_drift</code> Append units to numeric values (e.g., \"100 units\") <code>truncate_lines</code> Randomly truncate CSV rows <code>invalid_timestamps</code> Replace dates with \"9999-99-99 00:00:00\" <p>All transforms are logged in <code>messy/transform_log.json</code>:</p> <pre><code>[\n  {\n    \"input\": \".../raw/data.csv\",\n    \"output\": \".../messy/data_messy.csv\",\n    \"seed\": 12345,\n    \"recipe\": {...},\n    \"stats\": {\n      \"nulls_injected\": 1234,\n      \"duplicates_added\": 567,\n      \"total_rows_input\": 100000,\n      \"total_rows_output\": 100567\n    },\n    \"timestamp\": \"2025-12-14T10:00:00Z\"\n  }\n]\n</code></pre>"},{"location":"source_tiers/#dataset-sources","title":"Dataset Sources","text":"Source URL License Wikimedia Foundation https://dumps.wikimedia.org/ CC-BY-SA 4.0 OpenStreetMap https://planet.openstreetmap.org/ ODbL 1.0 OpenStreetMap (Geofabrik) https://download.geofabrik.de/ ODbL 1.0 Hugging Face FineWeb https://huggingface.co/datasets/HuggingFaceFW/fineweb ODC-BY 1.0"},{"location":"source_tiers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"source_tiers/#download-interrupted","title":"Download Interrupted","text":"<p>Downloads are resume-capable. Simply rerun the command.</p>"},{"location":"source_tiers/#hash-mismatch","title":"Hash Mismatch","text":"<p>If a file fails verification:</p> <ol> <li>Delete the corrupted file</li> <li>Rerun with <code>--download --verify</code></li> </ol>"},{"location":"source_tiers/#disk-space","title":"Disk Space","text":"<p>Check available space before large tier downloads:</p> <pre><code># Windows\nwmic logicaldisk get size,freespace,caption\n\n# Linux/Mac\ndf -h\n</code></pre>"},{"location":"source_tiers/#permission-errors","title":"Permission Errors","text":"<p>Ensure write permissions to <code>.odibi/source_cache/tiers/</code>.</p>"},{"location":"source_tiers/#related-documentation","title":"Related Documentation","text":"<ul> <li>Source Pools Design</li> <li>Source Cache README</li> <li>Pool Index</li> </ul>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/","title":"\ud83c\udfd7\ufe0f Odibi 2.1 Architecture: Unified Catalog &amp; Pattern Engine","text":"<p>Status: Blueprint Target Version: Odibi 2.1 Philosophy: \"Intent-Based Engineering\" backed by a Stateful Catalog.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#1-executive-summary","title":"1. Executive Summary","text":""},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#the-vision","title":"The Vision","text":"<p>Odibi 2.1 unifies the System Catalog (the \"Brain\") with a Pattern-Driven Execution Engine (the \"Intent\"). Instead of just executing tasks, Odibi now understands the semantics of the data (e.g., \"This is a Fact Table\", \"This is an SCD2 Dimension\") and enforces the appropriate behavior, lineage, and quality checks automatically.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#the-core-shift","title":"The Core Shift","text":"Feature Odibi 1.0 (Task Runner) Odibi 2.1 (Data Platform) Configuration <code>transformer: scd2</code> (Implementation Detail) <code>pattern: scd2</code> (Business Intent) State Local Files / Stateless System Catalog (Delta Tables) Validation Row-level checks (<code>email != null</code>) History-Aware &amp; Pattern-Aware checks Lineage Implicit / Text Logs Queryable Meta-Tables Semantics Buried in SQL strings Metric Registry (First-class citizen)"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#2-the-system-catalog-the-brain","title":"2. The System Catalog (The Brain)","text":"<p>A set of Delta Tables auto-bootstrapped in <code>_odibi_system/</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#1-meta_tables-inventory","title":"1. <code>meta_tables</code> (Inventory)","text":"<p>Tracks physical assets. *   <code>project_name</code>: STRING (Partition) *   <code>table_name</code>: STRING (Logical Name, e.g., \"gold.orders\") *   <code>path</code>: STRING (Physical Location) *   <code>format</code>: STRING *   <code>pattern_type</code>: STRING (e.g., \"scd2\", \"merge\") *   <code>schema_hash</code>: STRING (Drift Detection) *   <code>updated_at</code>: TIMESTAMP</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#2-meta_runs-observability","title":"2. <code>meta_runs</code> (Observability)","text":"<p>Tracks execution history. *   <code>run_id</code>: STRING *   <code>pipeline_name</code>: STRING *   <code>node_name</code>: STRING *   <code>status</code>: STRING *   <code>rows_processed</code>: LONG *   <code>duration_ms</code>: LONG *   <code>metrics_json</code>: STRING (JSON stats) *   <code>timestamp</code>: TIMESTAMP</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#3-meta_patterns-governance","title":"3. <code>meta_patterns</code> (Governance)","text":"<p>Tracks pattern compliance. *   <code>table_name</code>: STRING *   <code>pattern_type</code>: STRING *   <code>configuration</code>: STRING (JSON: params used) *   <code>compliance_score</code>: DOUBLE (0.0 - 1.0)     *   Example: An SCD2 table missing a <code>valid_to</code> column gets a score of 0.5.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#4-meta_metrics-semantics","title":"4. <code>meta_metrics</code> (Semantics)","text":"<p>Tracks business logic. *   <code>metric_name</code>: STRING *   <code>definition_sql</code>: STRING *   <code>dimensions</code>: ARRAY *   <code>source_table</code>: STRING"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#5-meta_state-checkpoints","title":"5. <code>meta_state</code> (Checkpoints)","text":"<p>Tracks incremental progress. *   <code>pipeline_name</code>: STRING *   <code>node_name</code>: STRING *   <code>hwm_value</code>: STRING</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#3-pattern-driven-execution-the-intent","title":"3. Pattern-Driven Execution (The Intent)","text":"<p>Users declare the Pattern, and Odibi configures the implementation.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#supported-patterns","title":"Supported Patterns","text":"<ol> <li><code>scd2</code>: Slowly Changing Dimension (History tracking).</li> <li><code>merge</code>: Smart Upsert (Conditional updates).</li> </ol> <p>Note: For simple append or overwrite operations, use <code>write.mode: append</code> or <code>write.mode: overwrite</code> directly\u2014no pattern needed.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#yaml-example","title":"YAML Example","text":"<pre><code>- name: \"dim_customers\"\n  pattern: \"scd2\"  # &lt;--- The Intent\n  params:\n    keys: [\"customer_id\"]\n    time_col: \"updated_at\"\n\n  # Odibi Automatically:\n  # 1. Selects the SCD2 Transformer.\n  # 2. Configures the 'Merge' writer mode.\n  # 3. Validates output has 'is_current' and 'valid_to' columns.\n  # 4. Registers 'scd2' pattern in meta_tables.\n</code></pre>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#4-phased-implementation-roadmap","title":"4. Phased Implementation Roadmap","text":""},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-1-the-foundation-catalog-schema","title":"Phase 1: The Foundation (Catalog &amp; Schema)","text":"<p>Goal: Build the \"Brain\" that holds state. *   1.1 Catalog Manager: Implement <code>odibi/catalog.py</code>. Logic to bootstrap the 5 meta-tables. *   1.2 Config Update: Add <code>SystemConfig</code> (<code>connection</code>, <code>path</code>) to <code>odibi.yaml</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-2-the-wiring-engine-integration","title":"Phase 2: The Wiring (Engine Integration)","text":"<p>Goal: Connect the \"Muscle\" to the \"Brain\". *   2.1 Auto-Registration: <code>Engine.write()</code> upserts to <code>meta_tables</code> and <code>meta_patterns</code>. *   2.2 Smart Read: <code>Engine.read()</code> resolves logical names (<code>gold.orders</code>) via <code>meta_tables</code>. *   2.3 Telemetry: <code>Node.execute()</code> flushes stats to <code>meta_runs</code>. *   2.4 State Migration: <code>StateManager</code> reads/writes to <code>meta_state</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-3-the-pattern-engine","title":"Phase 3: The Pattern Engine","text":"<p>Goal: Implement \"Intent-Based\" Logic. *   3.1 Pattern Module: Create <code>odibi/patterns/</code>. Implement classes for <code>SCD2</code>, <code>Fact</code>, <code>Snapshot</code>. *   3.2 Execution Router: Update <code>Node</code> to detect <code>pattern:</code> config. If present, delegate to Pattern Class instead of generic Transformer. *   3.3 Smart Merge: Implement conditional logic (<code>whenMatched...</code>) within the Merge pattern.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-4-intelligence-guardrails","title":"Phase 4: Intelligence (Guardrails)","text":"<p>Goal: Use Catalog data for smart checks. *   4.1 History-Aware Validation: <code>Validator</code> queries <code>meta_runs</code> for anomalies (Volume Drops). *   4.2 Pattern Validation: Pattern classes enforce schema rules (e.g., \"Fact table cannot have duplicates\"). *   4.3 Semantic Module: Parse <code>semantic_model</code>, register to <code>meta_metrics</code>, and implement <code>odibi export-views</code>.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#phase-5-observability","title":"Phase 5: Observability","text":"<p>Goal: Visualization. *   5.1 Dashboard: Standard Databricks Notebook to query <code>meta_runs</code> and show platform health.</p>"},{"location":"architecture/ODIBI_2.1_UNIFIED_ARCH/#5-migration-strategy","title":"5. Migration Strategy","text":"<ol> <li>Config: Users add <code>system</code> block to <code>odibi.yaml</code>.</li> <li>Bootstrap: First run creates <code>_odibi_system/</code> tables.</li> <li>Adoption: Users gradually switch from <code>transformer: scd2</code> to <code>pattern: scd2</code> (backward compatible).</li> </ol>"},{"location":"architecture/delete-detection-design/","title":"Delete Detection Implementation Design","text":"<p>Status: Design Complete Target: Silver layer CDC-like correctness for non-CDC sources</p>"},{"location":"architecture/delete-detection-design/#1-overview","title":"1. Overview","text":"<p>Delete detection identifies records that existed in a previous extraction but no longer exist, enabling CDC-like behavior for sources without native Change Data Capture.</p> <pre><code>Bronze (append-only) \u2192 Silver (dedupe + delete detection) \u2192 Gold (clean KPIs)\n</code></pre>"},{"location":"architecture/delete-detection-design/#2-delete-detection-modes","title":"2. Delete Detection Modes","text":"Mode How it works Use when <code>none</code> Pass-through (no detection) Immutable facts (logs, events, sensors) <code>snapshot_diff</code> Compare Delta version N vs N-1 keys Dimensions, staging tables, RPA sources <code>sql_compare</code> LEFT ANTI JOIN Silver keys against live source SQL is authoritative &amp; always reachable"},{"location":"architecture/delete-detection-design/#mode-none-default","title":"Mode: <code>none</code> (Default)","text":"<p>No delete detection. Use for append-only facts.</p> <pre><code>transform:\n  steps:\n    - operation: deduplicate\n      params:\n        keys: [event_id]\n        order_by: _extracted_at DESC\n    # detect_deletes omitted = none\n</code></pre>"},{"location":"architecture/delete-detection-design/#mode-snapshot_diff","title":"Mode: <code>snapshot_diff</code>","text":"<p>Compares current Delta version to previous version. Keys missing = deleted.</p> <p>Important: This mode only works correctly with full snapshot extracts, not HWM incremental. Use <code>sql_compare</code> if you're doing HWM ingestion.</p> <pre><code>transform:\n  steps:\n    - operation: deduplicate\n      params:\n        keys: [customer_id]\n        order_by: _extracted_at DESC\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [customer_id]\n</code></pre> <p>Logic:</p> <pre><code>prev_keys = Delta version N-1, SELECT DISTINCT keys\ncurr_keys = Delta version N, SELECT DISTINCT keys\ndeleted   = prev_keys EXCEPT curr_keys\n\u2192 Flag with _is_deleted = true (soft) or remove rows (hard)\n</code></pre> <p>Implementation: Uses Delta time travel (works in both Spark and Pandas via <code>deltalake</code> library).</p>"},{"location":"architecture/delete-detection-design/#mode-sql_compare","title":"Mode: <code>sql_compare</code>","text":"<p>Queries live source to find keys that no longer exist. Recommended for HWM ingestion when source is authoritative and reachable.</p> <pre><code>transform:\n  steps:\n    - operation: deduplicate\n      params:\n        keys: [customer_id]\n        order_by: _extracted_at DESC\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n</code></pre> <p>Logic:</p> <pre><code>silver_keys = SELECT DISTINCT keys FROM silver\nsource_keys = SELECT DISTINCT keys FROM live_source\ndeleted     = silver_keys EXCEPT source_keys\n\u2192 Flag with _is_deleted = true (soft) or remove rows (hard)\n</code></pre> <p>Warning: Never use <code>sql_compare</code> on staging tables - they may be empty between loads, causing false deletes.</p>"},{"location":"architecture/delete-detection-design/#3-config-model","title":"3. Config Model","text":"<pre><code>from enum import Enum\nfrom typing import List, Optional\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass DeleteDetectionMode(str, Enum):\n    NONE = \"none\"\n    SNAPSHOT_DIFF = \"snapshot_diff\"\n    SQL_COMPARE = \"sql_compare\"\n\n\nclass DeleteDetectionConfig(BaseModel):\n    \"\"\"\n    Configuration for delete detection in Silver layer.\n\n    Example (snapshot_diff):\n    ```yaml\n    detect_deletes:\n      mode: snapshot_diff\n      keys: [customer_id]\n      soft_delete_col: _is_deleted\n    ```\n\n    Example (sql_compare):\n    ```yaml\n    detect_deletes:\n      mode: sql_compare\n      keys: [customer_id]\n      source_connection: azure_sql\n      source_table: dbo.Customers\n    ```\n    \"\"\"\n\n    mode: DeleteDetectionMode = Field(\n        default=DeleteDetectionMode.NONE,\n        description=\"Delete detection strategy: none, snapshot_diff, sql_compare\"\n    )\n\n    keys: List[str] = Field(\n        default_factory=list,\n        description=\"Business key columns for comparison\"\n    )\n\n    # Soft vs Hard delete\n    soft_delete_col: Optional[str] = Field(\n        default=\"_is_deleted\",\n        description=\"Column to flag deletes. Set to null for hard-delete.\"\n    )\n\n    # sql_compare mode options\n    source_connection: Optional[str] = Field(\n        default=None,\n        description=\"For sql_compare: connection name to query source\"\n    )\n    source_table: Optional[str] = Field(\n        default=None,\n        description=\"For sql_compare: table to query\"\n    )\n    source_query: Optional[str] = Field(\n        default=None,\n        description=\"For sql_compare: custom SQL query (overrides source_table)\"\n    )\n\n    # Fallback for non-Delta sources (rare)\n    snapshot_column: Optional[str] = Field(\n        default=None,\n        description=\"For snapshot_diff on non-Delta: column to identify snapshots. \"\n                    \"If None, uses Delta version (default).\"\n    )\n\n    # Edge case handling\n    on_first_run: Literal[\"skip\", \"error\"] = Field(\n        default=\"skip\",\n        description=\"Behavior when no previous version exists for snapshot_diff\"\n    )\n\n    max_delete_percent: Optional[float] = Field(\n        default=50.0,\n        description=\"Safety threshold: warn/error if more than X% of rows would be deleted\"\n    )\n\n    on_threshold_breach: Literal[\"warn\", \"error\", \"skip\"] = Field(\n        default=\"warn\",\n        description=\"Behavior when delete percentage exceeds max_delete_percent\"\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_mode_requirements(self):\n        if self.mode == DeleteDetectionMode.NONE:\n            return self\n\n        if not self.keys:\n            raise ValueError(\n                f\"delete_detection: 'keys' required for mode='{self.mode}'\"\n            )\n\n        if self.mode == DeleteDetectionMode.SQL_COMPARE:\n            if not self.source_connection:\n                raise ValueError(\n                    \"delete_detection: 'source_connection' required for sql_compare\"\n                )\n            if not self.source_table and not self.source_query:\n                raise ValueError(\n                    \"delete_detection: 'source_table' or 'source_query' required\"\n                )\n\n        return self\n</code></pre>"},{"location":"architecture/delete-detection-design/#4-transformer-interface","title":"4. Transformer Interface","text":"<pre><code>@transform(\"detect_deletes\", category=\"transformer\", param_model=DeleteDetectionConfig)\ndef detect_deletes(context: EngineContext, params: DeleteDetectionConfig) -&gt; EngineContext:\n    \"\"\"\n    Detects deleted records based on configured mode.\n\n    Returns:\n    - soft_delete_col set: Adds boolean column (True = deleted)\n    - soft_delete_col = None: Removes deleted rows (hard delete)\n    \"\"\"\n    if params.mode == DeleteDetectionMode.NONE:\n        return context  # Pass-through\n\n    if params.mode == DeleteDetectionMode.SNAPSHOT_DIFF:\n        return _detect_deletes_snapshot_diff(context, params)\n\n    if params.mode == DeleteDetectionMode.SQL_COMPARE:\n        return _detect_deletes_sql_compare(context, params)\n\n    raise ValueError(f\"Unknown delete detection mode: {params.mode}\")\n</code></pre>"},{"location":"architecture/delete-detection-design/#snapshot_diff-implementation","title":"snapshot_diff Implementation","text":"<pre><code>def _detect_deletes_snapshot_diff(\n    context: EngineContext,\n    params: DeleteDetectionConfig\n) -&gt; EngineContext:\n    \"\"\"\n    Compare current Delta version to previous version.\n    Keys in previous but not in current = deleted.\n    \"\"\"\n    keys = params.keys\n\n    if context.engine_type == EngineType.SPARK:\n        # Spark: Delta time travel\n        current_version = context.delta_version\n        prev_version = current_version - 1\n\n        curr_keys = context.df.select(keys).distinct()\n        prev_keys = (\n            context.spark.read\n            .format(\"delta\")\n            .option(\"versionAsOf\", prev_version)\n            .load(context.table_path)\n            .select(keys)\n            .distinct()\n        )\n\n        deleted_keys = prev_keys.exceptAll(curr_keys)\n\n    else:\n        # Pandas: deltalake time travel\n        from deltalake import DeltaTable\n\n        dt = DeltaTable(context.table_path)\n        current_version = dt.version()\n        prev_version = current_version - 1\n\n        curr_keys = context.df[keys].drop_duplicates()\n        prev_df = DeltaTable(context.table_path, version=prev_version).to_pandas()\n        prev_keys = prev_df[keys].drop_duplicates()\n\n        # Use merge with indicator to find deleted\n        merged = prev_keys.merge(curr_keys, on=keys, how=\"left\", indicator=True)\n        deleted_keys = merged[merged[\"_merge\"] == \"left_only\"][keys]\n\n    return _apply_deletes(context, deleted_keys, params)\n\n\ndef _apply_deletes(\n    context: EngineContext,\n    deleted_keys,\n    params: DeleteDetectionConfig\n) -&gt; EngineContext:\n    \"\"\"Apply soft or hard delete based on config.\"\"\"\n    keys = params.keys\n\n    if params.soft_delete_col:\n        # Soft delete: add flag column\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql.functions import lit, col, when\n\n            deleted_keys_flagged = deleted_keys.withColumn(\n                params.soft_delete_col, lit(True)\n            )\n            result = context.df.join(\n                deleted_keys_flagged,\n                on=keys,\n                how=\"left\"\n            ).withColumn(\n                params.soft_delete_col,\n                when(col(params.soft_delete_col).isNull(), False)\n                .otherwise(True)\n            )\n        else:\n            # Pandas\n            df = context.df.copy()\n            deleted_keys[params.soft_delete_col] = True\n            df = df.merge(deleted_keys, on=keys, how=\"left\")\n            df[params.soft_delete_col] = df[params.soft_delete_col].fillna(False)\n            result = df\n\n        return context.with_df(result)\n\n    else:\n        # Hard delete: remove rows\n        if context.engine_type == EngineType.SPARK:\n            result = context.df.join(deleted_keys, on=keys, how=\"left_anti\")\n        else:\n            df = context.df.copy()\n            merged = df.merge(deleted_keys, on=keys, how=\"left\", indicator=True)\n            result = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n        return context.with_df(result)\n</code></pre>"},{"location":"architecture/delete-detection-design/#sql_compare-implementation","title":"sql_compare Implementation","text":"<pre><code>def _detect_deletes_sql_compare(\n    context: EngineContext,\n    params: DeleteDetectionConfig\n) -&gt; EngineContext:\n    \"\"\"\n    Compare Silver keys against live source.\n    Keys in Silver but not in source = deleted.\n    \"\"\"\n    from odibi.connections import get_connection\n\n    keys = params.keys\n    conn = get_connection(params.source_connection)\n\n    # Build source keys query\n    if params.source_query:\n        source_keys_query = params.source_query\n    else:\n        key_cols = \", \".join(keys)\n        source_keys_query = f\"SELECT DISTINCT {key_cols} FROM {params.source_table}\"\n\n    if context.engine_type == EngineType.SPARK:\n        source_keys = (\n            context.spark.read\n            .format(\"jdbc\")\n            .option(\"url\", conn.jdbc_url)\n            .option(\"query\", source_keys_query)\n            .load()\n        )\n        silver_keys = context.df.select(keys).distinct()\n        deleted_keys = silver_keys.exceptAll(source_keys)\n\n    else:\n        # Pandas\n        import pandas as pd\n\n        source_keys = pd.read_sql(source_keys_query, conn.engine)\n        silver_keys = context.df[keys].drop_duplicates()\n\n        merged = silver_keys.merge(source_keys, on=keys, how=\"left\", indicator=True)\n        deleted_keys = merged[merged[\"_merge\"] == \"left_only\"][keys]\n\n    return _apply_deletes(context, deleted_keys, params)\n</code></pre>"},{"location":"architecture/delete-detection-design/#5-engine-parity","title":"5. Engine Parity","text":"<p>Both Pandas and Spark support all operations:</p> Operation Spark Pandas Delta time travel <code>versionAsOf</code> option <code>DeltaTable(..., version=N)</code> EXCEPT ALL Native DataFrame <code>merge</code> + indicator LEFT ANTI JOIN Native DataFrame <code>merge</code> + filter JDBC read Native SQLAlchemy <code>pd.read_sql</code>"},{"location":"architecture/delete-detection-design/#6-bronze-metadata-columns","title":"6. Bronze Metadata Columns","text":"<p>Bronze writes should include metadata for lineage and debugging. Use <code>add_metadata: true</code> for all applicable columns, or specify individual ones.</p>"},{"location":"architecture/delete-detection-design/#available-metadata-columns","title":"Available Metadata Columns","text":"Column Description Applies to <code>_extracted_at</code> Pipeline execution timestamp All sources <code>_source_file</code> Source filename/path File sources (CSV, Parquet, JSON) <code>_source_connection</code> Connection name used All sources <code>_source_table</code> Table or query name SQL sources"},{"location":"architecture/delete-detection-design/#metadata-config-model","title":"Metadata Config Model","text":"<pre><code>class WriteMetadataConfig(BaseModel):\n    \"\"\"Metadata columns to add during Bronze writes.\"\"\"\n\n    extracted_at: bool = Field(default=True)\n    source_file: bool = Field(default=True)\n    source_connection: bool = Field(default=False)\n    source_table: bool = Field(default=False)\n</code></pre>"},{"location":"architecture/delete-detection-design/#yaml-examples","title":"YAML Examples","text":"<p>Add all applicable metadata (recommended):</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n</code></pre> <p>Specify individual columns:</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: true\n</code></pre>"},{"location":"architecture/delete-detection-design/#implementation-notes","title":"Implementation Notes","text":"<p>Spark:</p> <pre><code>from pyspark.sql.functions import input_file_name, current_timestamp, lit\n\ndf = df.withColumn(\"_extracted_at\", current_timestamp())\ndf = df.withColumn(\"_source_file\", input_file_name())  # file sources only\ndf = df.withColumn(\"_source_connection\", lit(connection_name))\n</code></pre> <p>Pandas:</p> <pre><code>df[\"_extracted_at\"] = pd.Timestamp.now()\ndf[\"_source_file\"] = source_path  # tracked during file read\ndf[\"_source_connection\"] = connection_name\n</code></pre>"},{"location":"architecture/delete-detection-design/#7-hash-optimization-for-merge","title":"7. Hash Optimization for Merge","text":"<p>For large tables, use hashes to optimize change detection. Use the existing <code>generate_surrogate_key</code> transformer twice:</p> <pre><code>transform:\n  steps:\n    # REQUIRED: Deduplication\n    - operation: deduplicate\n      params:\n        keys: [customer_id]\n        order_by: _extracted_at DESC\n\n    # Hash for join key (optional - useful for composite keys)\n    - operation: generate_surrogate_key\n      params:\n        columns: [customer_id]\n        output_col: _hash_key\n\n    # Hash for change detection\n    - operation: generate_surrogate_key\n      params:\n        columns: [name, email, address, phone, status]  # non-key columns\n        output_col: _hash_diff\n\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n\nwrite:\n  connection: silver\n  table: customers\n  mode: upsert\n  keys: [customer_id]\n  update_condition: \"source._hash_diff != target._hash_diff\"  # skip unchanged rows\n</code></pre> <p>Benefits: - Skip unchanged rows during merge (performance) - Single-column comparison vs N columns - Deterministic across runs</p>"},{"location":"architecture/delete-detection-design/#8-full-yaml-examples","title":"8. Full YAML Examples","text":""},{"location":"architecture/delete-detection-design/#example-1-dimension-with-snapshot_diff-full-snapshot-only","title":"Example 1: Dimension with snapshot_diff (Full Snapshot Only)","text":"<p>Note: Only use <code>snapshot_diff</code> with full snapshot ingestion, not HWM.</p> <pre><code>nodes:\n  - name: bronze_customers\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.Customers\n      # No incremental = full snapshot\n    write:\n      connection: bronze\n      table: customers\n      mode: append\n      add_metadata: true\n\n  - name: silver_customers\n    read:\n      connection: bronze\n      table: customers\n    transform:\n      steps:\n        - operation: deduplicate\n          params:\n            keys: [customer_id]\n            order_by: _extracted_at DESC\n        - operation: detect_deletes\n          params:\n            mode: snapshot_diff\n            keys: [customer_id]\n            soft_delete_col: _is_deleted\n    write:\n      connection: silver\n      table: customers\n      mode: upsert\n      keys: [customer_id]\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-2-dimension-with-sql_compare-recommended-for-hwm","title":"Example 2: Dimension with sql_compare (Recommended for HWM)","text":"<p>Recommended: Use <code>sql_compare</code> when source is authoritative and reachable.</p> <pre><code>nodes:\n  - name: bronze_products\n    read:\n      connection: erp_sql\n      format: sql\n      table: dbo.Products\n      incremental:\n        mode: stateful\n        column: updated_at\n        watermark_lag: 2h\n    write:\n      connection: bronze\n      table: products\n      mode: append\n      add_metadata: true\n\n  - name: silver_products\n    read:\n      connection: bronze\n      table: products\n    transform:\n      steps:\n        - operation: deduplicate\n          params:\n            keys: [product_id]\n            order_by: _extracted_at DESC\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [product_id]\n            source_connection: erp_sql\n            source_table: dbo.Products\n            soft_delete_col: _is_deleted\n    write:\n      connection: silver\n      table: products\n      mode: upsert\n      keys: [product_id]\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-3-fact-table-no-delete-detection","title":"Example 3: Fact table (no delete detection)","text":"<pre><code>nodes:\n  - name: silver_orders\n    read:\n      connection: bronze\n      table: orders\n    transform:\n      steps:\n        - operation: deduplicate\n          params:\n            keys: [order_id]\n            order_by: _extracted_at DESC\n        # No detect_deletes - facts are immutable\n    write:\n      connection: silver\n      table: orders\n      mode: upsert\n      keys: [order_id]\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-4-hard-delete-instead-of-soft-delete","title":"Example 4: Hard delete instead of soft delete","text":"<pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n        soft_delete_col: null  # removes rows instead of flagging\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-5-conservative-threshold-for-critical-dimension","title":"Example 5: Conservative threshold for critical dimension","text":"<pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: erp\n        source_table: dbo.Customers\n        max_delete_percent: 20.0    # fail if &gt;20% would be deleted\n        on_threshold_breach: error  # stop pipeline\n</code></pre>"},{"location":"architecture/delete-detection-design/#example-6-first-run-handling","title":"Example 6: First run handling","text":"<pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [product_id]\n        on_first_run: skip  # default: no deletes on initial load\n</code></pre>"},{"location":"architecture/delete-detection-design/#9-edge-case-handling","title":"9. Edge Case Handling","text":"Scenario Default Behavior Config Override First run (no previous version) Skip detection <code>on_first_run: error</code> &gt;50% rows deleted Warn and continue <code>max_delete_percent</code>, <code>on_threshold_breach</code> Source returns 0 rows Triggers threshold warning Set <code>max_delete_percent: 0</code> to disable Connection failure Pipeline fails Handle via standard error handling"},{"location":"architecture/delete-detection-design/#threshold-calculation","title":"Threshold Calculation","text":"<pre><code>delete_percent = (deleted_count / total_silver_rows) * 100\n\nif delete_percent &gt; params.max_delete_percent:\n    if params.on_threshold_breach == \"error\":\n        raise DeleteThresholdExceeded(f\"{delete_percent:.1f}% exceeds {params.max_delete_percent}%\")\n    elif params.on_threshold_breach == \"warn\":\n        logger.warning(f\"Delete detection: {delete_percent:.1f}% of rows flagged for deletion\")\n    elif params.on_threshold_breach == \"skip\":\n        logger.info(\"Delete threshold exceeded, skipping delete detection\")\n        return context  # no changes\n</code></pre>"},{"location":"architecture/delete-detection-design/#10-design-decisions-summary","title":"10. Design Decisions Summary","text":"Decision Choice Rationale Default delete behavior Soft delete (<code>_is_deleted</code>) Audit trail, undo capability State for snapshot_diff Delta time travel No extra columns needed, works in Pandas + Spark batch_id column Not needed Delta versioning handles this _snapshot_date column Not needed Over-engineering for edge cases Version picker for snapshot_diff Not supported Always compare N to N-1, correct semantics Bundled SilverTransformConfig Not implemented Keep transformers separate for flexibility source_flagged mode Not implemented Use existing filter/rename transforms Hash for merge Use <code>generate_surrogate_key</code> twice No new transformer needed First run behavior Skip by default No deletes to detect on initial load Delete threshold 50% default, warn Safety net for staging table accidents Bronze metadata Flexible config <code>true</code> for all, or specify individual columns"},{"location":"architecture/delete-detection-design/#11-implementation-checklist","title":"11. Implementation Checklist","text":""},{"location":"architecture/delete-detection-design/#phase-1-config-models","title":"Phase 1: Config Models","text":"<ul> <li>[ ] Add <code>DeleteDetectionMode</code> enum to <code>odibi/config.py</code></li> <li>[ ] Add <code>DeleteDetectionConfig</code> model to <code>odibi/config.py</code></li> <li>[ ] Add <code>WriteMetadataConfig</code> model to <code>odibi/config.py</code></li> <li>[ ] Update <code>WriteConfig</code> to accept <code>add_metadata: bool | WriteMetadataConfig</code></li> </ul>"},{"location":"architecture/delete-detection-design/#phase-2-delete-detection-transformer","title":"Phase 2: Delete Detection Transformer","text":"<ul> <li>[ ] Create <code>odibi/transformers/delete_detection.py</code></li> <li>[ ] Implement <code>detect_deletes</code> transformer function</li> <li>[ ] Implement <code>_detect_deletes_snapshot_diff</code> for Spark</li> <li>[ ] Implement <code>_detect_deletes_snapshot_diff</code> for Pandas</li> <li>[ ] Implement <code>_detect_deletes_sql_compare</code> for Spark</li> <li>[ ] Implement <code>_detect_deletes_sql_compare</code> for Pandas</li> <li>[ ] Implement <code>_apply_deletes</code> helper (soft/hard delete logic)</li> <li>[ ] Implement threshold check logic</li> <li>[ ] Implement first-run detection (no previous version)</li> <li>[ ] Register transformer in <code>odibi/transformers/__init__.py</code></li> </ul>"},{"location":"architecture/delete-detection-design/#phase-3-bronze-metadata","title":"Phase 3: Bronze Metadata","text":"<ul> <li>[ ] Add <code>add_metadata</code> support to write path (Spark engine)</li> <li>[ ] Add <code>add_metadata</code> support to write path (Pandas engine)</li> <li>[ ] Implement <code>_extracted_at</code> column addition</li> <li>[ ] Implement <code>_source_file</code> column addition (file sources)</li> <li>[ ] Implement <code>_source_connection</code> column addition</li> <li>[ ] Implement <code>_source_table</code> column addition (SQL sources)</li> </ul>"},{"location":"architecture/delete-detection-design/#phase-4-testing","title":"Phase 4: Testing","text":"<ul> <li>[ ] Write unit tests for <code>DeleteDetectionConfig</code> validation</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: none</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: snapshot_diff (Spark)</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: snapshot_diff (Pandas)</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: sql_compare (Spark)</li> <li>[ ] Write unit tests for <code>detect_deletes</code> - mode: sql_compare (Pandas)</li> <li>[ ] Write unit tests for edge cases (first run, threshold breach)</li> <li>[ ] Write unit tests for soft delete vs hard delete</li> <li>[ ] Write unit tests for metadata columns</li> </ul>"},{"location":"architecture/delete-detection-design/#phase-5-introspect-documentation","title":"Phase 5: Introspect &amp; Documentation","text":"<ul> <li>[ ] Update <code>odibi/introspect.py</code> GROUP_MAPPING with new configs</li> <li>[ ] Update <code>odibi/introspect.py</code> TRANSFORM_CATEGORY_MAP for delete_detection</li> <li>[ ] Update <code>odibi/introspect.py</code> CUSTOM_ORDER for new configs</li> <li>[ ] Ensure <code>DeleteDetectionConfig</code> has top-tier docstrings with examples (like MergeParams)</li> <li>[ ] Ensure <code>WriteMetadataConfig</code> has comprehensive docstrings</li> <li>[ ] Add YAML examples in docstrings following existing patterns</li> <li>[ ] Update <code>docs/reference/yaml_schema.md</code></li> <li>[ ] Update <code>docs/architecture/ingestion-correctness.md</code> (done)</li> <li>[ ] Add cookbook examples</li> </ul>"},{"location":"architecture/delete-detection-design/#12-docstring-standards-for-introspect","title":"12. Docstring Standards (for Introspect)","text":"<p>All new config models must follow the existing docstring pattern for auto-generated docs:</p> <pre><code>class DeleteDetectionConfig(BaseModel):\n    \"\"\"\n    Configuration for delete detection in Silver layer.\n\n    ### \ud83d\udd0d \"CDC Without CDC\" Guide\n\n    **Business Problem:**\n    \"Records are deleted in our Azure SQL source, but our Silver tables still show them.\"\n\n    **The Solution:**\n    Use delete detection to identify and flag records that no longer exist in the source.\n\n    **Recipe 1: SQL Compare (Recommended for HWM)**\n    ```yaml\n    transform:\n      steps:\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [customer_id]\n            source_connection: azure_sql\n            source_table: dbo.Customers\n    ```\n\n    **Recipe 2: Snapshot Diff (For Staging Tables)**\n    ```yaml\n    transform:\n      steps:\n        - operation: detect_deletes\n          params:\n            mode: snapshot_diff\n            keys: [customer_id]\n    ```\n\n    **Recipe 3: Conservative Threshold**\n    ```yaml\n    transform:\n      steps:\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [customer_id]\n            source_connection: erp\n            source_table: dbo.Customers\n            max_delete_percent: 20.0\n            on_threshold_breach: error\n    ```\n    \"\"\"\n</code></pre> <p>This ensures the introspect tool generates high-quality, example-rich documentation.</p>"},{"location":"architecture/ingestion-correctness/","title":"Ingestion Correctness for Non-CDC Sources","text":"<p>Target Audience: Data engineers implementing pipelines against Azure SQL Database or other sources without native Change Data Capture (CDC).</p> <p>This document defines the architecture for achieving CDC-like correctness without real CDC, covering Bronze/Silver/Gold layer invariants, deduplication, and delete detection patterns.</p>"},{"location":"architecture/ingestion-correctness/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Invariants</li> <li>Bronze Layer: Append-Only Raw Truth</li> <li>Silver Layer: Dedupe + Delete Detection</li> <li>Delete Detection Patterns</li> <li>HWM vs Snapshot: Decision Guide</li> <li>YAML Configuration Reference</li> <li>Quick Reference Table</li> <li>Common Pitfalls</li> </ol>"},{"location":"architecture/ingestion-correctness/#core-invariants","title":"Core Invariants","text":"<pre><code>Bronze = append-only raw truth\nSilver = dedupe + clean + optional delete-detection\nGold   = KPIs (computed from clean Silver)\n</code></pre> <pre><code>flowchart TB\n    subgraph Source[\"Source (Azure SQL)\"]\n        SQL[(Azure SQL DB)]\n    end\n\n    subgraph Bronze[\"Bronze Layer\"]\n        B1[Append-Only Raw Truth]\n        B2[Every extraction is a snapshot]\n        B3[Never mutate, never delete]\n    end\n\n    subgraph Silver[\"Silver Layer\"]\n        S1[Dedupe: Latest per Business Key]\n        S2[Delete Detection: Optional]\n        S3[Clean + Typed + Validated]\n    end\n\n    subgraph Gold[\"Gold Layer\"]\n        G1[KPIs / Aggregates]\n    end\n\n    SQL --&gt;|HWM or Snapshot| B1\n    B1 --&gt; S1\n    S1 --&gt; G1\n</code></pre>"},{"location":"architecture/ingestion-correctness/#bronze-layer-append-only-raw-truth","title":"Bronze Layer: Append-Only Raw Truth","text":""},{"location":"architecture/ingestion-correctness/#the-rule","title":"The Rule","text":"<pre><code>Bronze = append(raw_row + _extracted_at + metadata)\n</code></pre> <p>Never dedupe in Bronze. Never delete in Bronze.</p>"},{"location":"architecture/ingestion-correctness/#what-goes-wrong-without-append-only-bronze","title":"What Goes Wrong Without Append-Only Bronze?","text":"Problem Consequence Overwrites destroy lineage You can't answer \"what did the source say on Tuesday?\" Reprocessing is impossible If Silver logic is wrong, you can't replay from Bronze Late-arriving data is lost If you overwrite with \"current\", late rows vanish Audit trail breaks Compliance (GDPR, SOX) requires provenance"},{"location":"architecture/ingestion-correctness/#bronze-yaml-example","title":"Bronze YAML Example","text":"<pre><code>nodes:\n  - name: bronze_customers\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.Customers\n      incremental:\n        mode: stateful\n        column: updated_at\n        watermark_lag: 2h\n    write:\n      connection: bronze\n      table: customers\n      mode: append              # ALWAYS append for Bronze\n      add_metadata: true        # adds _extracted_at, _source_file, etc.\n</code></pre>"},{"location":"architecture/ingestion-correctness/#silver-layer-dedupe-delete-detection","title":"Silver Layer: Dedupe + Delete Detection","text":""},{"location":"architecture/ingestion-correctness/#dedupe-is-required","title":"Dedupe is REQUIRED","text":"<p>Every Silver table must declare: - <code>keys</code>: business key columns - <code>order_by</code>: how to pick the \"latest\" (typically <code>_extracted_at DESC</code>)</p>"},{"location":"architecture/ingestion-correctness/#what-goes-wrong-without-dedupe","title":"What Goes Wrong Without Dedupe?","text":"Scenario Problem HWM overlap Watermark lag pulls same row twice \u2192 duplicates in Silver Snapshot re-extraction Same row appears in batch N and batch N+1 \u2192 duplicates Source system backfills Row updated retroactively \u2192 old and new version both exist Retry after partial failure Same batch re-ingested \u2192 duplicates"},{"location":"architecture/ingestion-correctness/#dedupe-logic","title":"Dedupe Logic","text":"<pre><code>SELECT * EXCEPT(_rn) FROM (\n    SELECT *,\n           ROW_NUMBER() OVER (\n               PARTITION BY {business_key}\n               ORDER BY _extracted_at DESC, updated_at DESC\n           ) as _rn\n    FROM bronze\n) WHERE _rn = 1\n</code></pre>"},{"location":"architecture/ingestion-correctness/#delete-detection-patterns","title":"Delete Detection Patterns","text":"<p>Delete detection is optional and has three modes:</p> <pre><code>flowchart LR\n    subgraph Modes[\"Delete Detection Modes\"]\n        direction TB\n        NONE[\"none&lt;br/&gt;(ignore deletes)\"]\n        SNAP[\"snapshot_diff&lt;br/&gt;(compare Bronze snapshots)\"]\n        SQL[\"sql_compare&lt;br/&gt;(query source directly)\"]\n    end\n\n    subgraph When[\"When to Use\"]\n        direction TB\n        W1[\"Append-only facts&lt;br/&gt;(events, logs)\"]\n        W2[\"Dimensions with soft-delete&lt;br/&gt;or staging/RPA systems\"]\n        W3[\"SQL is authoritative&lt;br/&gt;and always reachable\"]\n    end\n\n    NONE --&gt; W1\n    SNAP --&gt; W2\n    SQL --&gt; W3\n</code></pre>"},{"location":"architecture/ingestion-correctness/#mode-none-default-for-facts","title":"Mode: <code>none</code> (Default for Facts)","text":"<p>Use when: Immutable events (logs, transactions, sensor readings)</p> <pre><code>transform:\n  - dedupe:\n      keys: [event_id]\n      order_by: _extracted_at DESC\n  # delete_detection omitted = none\n</code></pre> <p>Why no delete detection for facts? - Facts don't delete by nature - Wastes compute comparing millions of events - Risk of accidentally soft-deleting valid events</p>"},{"location":"architecture/ingestion-correctness/#mode-snapshot_diff-compare-bronze-snapshots","title":"Mode: <code>snapshot_diff</code> (Compare Bronze Snapshots)","text":"<p>Use when: - Dimensions (customers, products) - Staging/RPA tables where SQL is NOT authoritative - Source system doesn't support point-in-time queries</p> <p>Logic:</p> <pre><code>deleted_keys = (keys in previous_snapshot) - (keys in current_snapshot)\n</code></pre> <p>Implementation Concept:</p> <pre><code># Uses Delta time travel (no batch_id column needed)\nprev = delta_table.version(N-1).select(keys).distinct()\ncurr = delta_table.version(N).select(keys).distinct()\ndeleted = prev.exceptAll(curr)\n</code></pre> <p>What goes wrong if we compare to SQL instead? - If SQL is a staging table, it may be empty between loads \u2192 false deletes - If SQL has different retention than Bronze \u2192 false deletes - If SQL is behind a firewall/VPN during processing \u2192 failures</p> <p>What goes wrong if we push this to Bronze? - Bronze becomes stateful (needs to know previous batch) - Bronze loses append-only guarantee - Can't replay Bronze independently</p>"},{"location":"architecture/ingestion-correctness/#mode-sql_compare-query-source-directly","title":"Mode: <code>sql_compare</code> (Query Source Directly)","text":"<p>Use when: - SQL is the authoritative source of truth - You need real-time delete detection - SQL is always reachable during Silver processing</p> <p>Logic:</p> <pre><code>-- Keys in Silver but NOT in SQL = deleted\nSELECT s.business_key\nFROM silver s\nLEFT ANTI JOIN sql_source ON s.business_key = sql_source.business_key\n</code></pre> <p>What goes wrong if we compare too early (in Bronze)? - Bronze becomes coupled to SQL availability - Can't process Bronze offline - Latency spikes if SQL is slow</p> <p>What goes wrong if we compare too late (after Gold)? - KPIs already computed with ghost records - Downstream dashboards show incorrect data - Audit logs contaminated</p> <p>What goes wrong if SQL is NOT authoritative? - Staging tables empty between loads \u2192 everything looks deleted - ERP snapshots \u2192 previous day's data appears deleted - RPA extracts \u2192 intermittent failures look like deletes</p>"},{"location":"architecture/ingestion-correctness/#hwm-vs-snapshot-decision-guide","title":"HWM vs Snapshot: Decision Guide","text":"<pre><code>flowchart TD\n    START[\"Does the source table&lt;br/&gt;support UPDATE timestamps?\"]\n\n    START --&gt;|Yes| HAS_UPDATE[\"Is updated_at reliable&lt;br/&gt;(always updated on change)?\"]\n    START --&gt;|No| SNAPSHOT[\"Use SNAPSHOT&lt;br/&gt;(full extract + snapshot_diff)\"]\n\n    HAS_UPDATE --&gt;|Yes| DELETES[\"Do you care about&lt;br/&gt;hard deletes in source?\"]\n    HAS_UPDATE --&gt;|No| SNAPSHOT\n\n    DELETES --&gt;|No| HWM[\"Use HWM&lt;br/&gt;(incremental is enough)\"]\n    DELETES --&gt;|Yes| AUTH[\"Is SQL authoritative&lt;br/&gt;(not a staging table)?\"]\n\n    AUTH --&gt;|Yes| SQL_COMPARE[\"Use HWM + sql_compare\"]\n    AUTH --&gt;|No| SNAP_DIFF[\"Use SNAPSHOT + snapshot_diff\"]\n</code></pre>"},{"location":"architecture/ingestion-correctness/#decision-matrix","title":"Decision Matrix","text":"Source Characteristics Ingestion Mode Delete Mode Has <code>updated_at</code>, no hard deletes HWM <code>none</code> Has <code>updated_at</code>, hard deletes, SQL = authoritative HWM <code>sql_compare</code> Has <code>updated_at</code>, hard deletes, SQL = staging HWM <code>snapshot_diff</code> (periodic full extract) No <code>updated_at</code>, full snapshot daily SNAPSHOT <code>snapshot_diff</code> Append-only facts (immutable) HWM on <code>event_id</code> or <code>created_at</code> <code>none</code>"},{"location":"architecture/ingestion-correctness/#hybrid-pattern-for-updatable-facts","title":"Hybrid Pattern for Updatable Facts","text":"<p>For tables like downtime, production, energy where: - Records can be updated (corrections) - Records can be hard-deleted (reversals) - But volume is high</p> <p>Recommended approaches:</p> <ol> <li> <p>Use <code>sql_compare</code> (simpler) - If source is authoritative and reachable, just use HWM with <code>sql_compare</code>. No full snapshots needed.</p> </li> <li> <p>Hybrid HWM + Snapshot (complex) - If source is unreliable/staging:</p> </li> <li>Daily: HWM ingestion</li> <li>Weekly: Full snapshot (enables <code>snapshot_diff</code>)</li> <li>Requires filtering Bronze to latest snapshot in Silver</li> </ol> <pre><code># Option 1: sql_compare (recommended)\nincremental:\n  mode: stateful\n  column: updated_at\n  watermark_lag: 2h\n# Then use detect_deletes with mode: sql_compare\n\n# Option 2: Hybrid (only if sql_compare won't work)\n# Use two separate pipeline runs or orchestrator-controlled mode\n</code></pre>"},{"location":"architecture/ingestion-correctness/#yaml-configuration-reference","title":"YAML Configuration Reference","text":""},{"location":"architecture/ingestion-correctness/#full-silver-node-example","title":"Full Silver Node Example","text":"<pre><code>nodes:\n  - name: silver_customers\n    read:\n      connection: bronze\n      table: customers\n    transform:\n      steps:\n        # REQUIRED: Deduplication\n        - operation: deduplicate\n          params:\n            keys: [customer_id]\n            order_by: _extracted_at DESC\n\n        # OPTIONAL: Delete Detection (sql_compare recommended for HWM)\n        - operation: detect_deletes\n          params:\n            mode: sql_compare\n            keys: [customer_id]\n            source_connection: azure_sql\n            source_table: dbo.Customers\n            soft_delete_col: _is_deleted\n    write:\n      connection: silver\n      table: customers\n      mode: upsert\n      keys: [customer_id]\n</code></pre>"},{"location":"architecture/ingestion-correctness/#delete-detection-config-options","title":"Delete Detection Config Options","text":"Option Type Description <code>mode</code> enum <code>none</code>, <code>snapshot_diff</code>, <code>sql_compare</code> <code>keys</code> list Business key columns for comparison <code>soft_delete_col</code> string Column to flag deletes (default: <code>_is_deleted</code>). If null, hard-delete. <code>source_connection</code> string For <code>sql_compare</code>: connection to query <code>source_table</code> string For <code>sql_compare</code>: table to query <code>source_query</code> string For <code>sql_compare</code>: custom query (overrides source_table) <code>max_delete_percent</code> float Safety threshold: warn/error if more than X% would be deleted (default: 50) <code>on_threshold_breach</code> enum Behavior when threshold exceeded: <code>warn</code>, <code>error</code>, <code>skip</code> <code>on_first_run</code> enum Behavior when no previous version exists: <code>skip</code>, <code>error</code>"},{"location":"architecture/ingestion-correctness/#quick-reference-table","title":"Quick Reference Table","text":"Table Type Example Ingestion Delete Mode Why Immutable facts Events, logs, sensor data HWM on <code>created_at</code> <code>none</code> Facts don't delete Mutable facts Downtime, production records HWM on <code>updated_at</code> <code>sql_compare</code> Source is authoritative Dimensions (SQL auth) Customers from ERP HWM on <code>updated_at</code> <code>sql_compare</code> ERP is truth Dimensions (staging) Staging from RPA Full snapshot <code>snapshot_diff</code> Staging is ephemeral Reference data Country codes, units Full snapshot <code>snapshot_diff</code> Rarely changes, small tables"},{"location":"architecture/ingestion-correctness/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"architecture/ingestion-correctness/#1-never-use-sql_compare-on-staging-tables","title":"1. Never use <code>sql_compare</code> on staging tables","text":"<p>Staging tables may be truncated between loads. If Silver compares to an empty staging table, everything appears deleted.</p>"},{"location":"architecture/ingestion-correctness/#2-always-add-watermark_lag","title":"2. Always add <code>watermark_lag</code>","text":"<p>Source systems have clock skew and replication lag. A 1-2 hour lag buffer prevents missed records.</p> <pre><code>incremental:\n  mode: stateful\n  column: updated_at\n  watermark_lag: 2h  # Safety buffer\n</code></pre>"},{"location":"architecture/ingestion-correctness/#3-prefer-soft-deletes-over-hard-deletes","title":"3. Prefer soft-deletes over hard-deletes","text":"<p>Use <code>soft_delete_col: _is_deleted</code> instead of actually removing rows. This: - Preserves audit trail - Allows undo if delete detection was wrong - Supports downstream \"as-of\" queries</p>"},{"location":"architecture/ingestion-correctness/#4-run-periodic-full-snapshots-for-hwm-tables","title":"4. Run periodic full snapshots for HWM tables","text":"<p>Even with reliable HWM, run a weekly full snapshot to: - Catch missed updates (source bugs, clock issues) - Detect hard deletes - Validate HWM accuracy</p>"},{"location":"architecture/ingestion-correctness/#summary","title":"Summary","text":"<p>The architecture for non-CDC sources follows these principles:</p> <ol> <li>Bronze is stateless and append-only \u2014 capture everything, decide nothing</li> <li>Silver owns deduplication \u2014 mandatory, not optional</li> <li>Silver owns delete detection \u2014 optional, with three explicit modes</li> <li>Gold assumes clean data \u2014 no deduplication logic needed</li> </ol> <p>This design achieves CDC-like correctness while keeping the logic simple, deterministic, and replayable.</p>"},{"location":"architecture/performance-analysis-findings/","title":"Odibi Delta Write Performance Analysis","text":""},{"location":"architecture/performance-analysis-findings/#executive-summary","title":"Executive Summary","text":"<p>The write phase accounts for ~96% of pipeline runtime (~50s for slowest nodes). Analysis identified 8 major bottlenecks with cumulative potential savings of 30-45 seconds per node.</p>"},{"location":"architecture/performance-analysis-findings/#write-path-overview","title":"Write Path Overview","text":"<pre><code>_execute_write_phase() [node.py:1585]\n    \u2502\n    \u251c\u2500 skip_if_unchanged check (if enabled)\n    \u2502   \u251c\u2500 compute_spark_dataframe_hash() [content_hash.py:57]\n    \u2502   \u2502   \u2514\u2500 _compute_spark_hash_distributed() - xxhash64 + agg\n    \u2502   \u2514\u2500 get_content_hash_from_state() - Delta read for lookup\n    \u2502\n    \u251c\u2500 schema_policy check (if enabled)\n    \u2502   \u2514\u2500 get_table_schema() - Delta table read\n    \u2502\n    \u251c\u2500 add_metadata columns (if enabled)\n    \u2502   \u2514\u2500 withColumn(\"_extracted_at\", current_timestamp())\n    \u2502\n    \u2514\u2500 engine.write() [spark_engine.py:717]\n        \u251c\u2500 df.rdd.getNumPartitions() - Spark action!\n        \u251c\u2500 [Delta path-based write]\n        \u2502   \u251c\u2500 writer.save(full_path)\n        \u2502   \u251c\u2500 register_table SQL (CREATE TABLE IF NOT EXISTS)\n        \u2502   \u251c\u2500 _apply_table_properties() - ALTER TABLE per property\n        \u2502   \u251c\u2500 _optimize_delta_write() - OPTIMIZE/ZORDER\n        \u2502   \u2514\u2500 _get_last_delta_commit_info() - Delta history read\n        \u2502\n        \u2514\u2500 _register_catalog_entries() [node.py:1806]\n            \u251c\u2500 register_asset() - Delta append\n            \u251c\u2500 track_schema() - Delta merge\n            \u251c\u2500 log_pattern() - Delta merge\n            \u2514\u2500 record_lineage() - Delta append\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#identified-bottlenecks","title":"Identified Bottlenecks","text":""},{"location":"architecture/performance-analysis-findings/#1-repeated-table-existence-checks-high-impact","title":"1. Repeated Table Existence Checks \u26a0\ufe0f HIGH IMPACT","text":"<p>Location: Multiple locations throughout write path - <code>_execute_write_phase</code> \u2192 <code>get_table_schema()</code> [line 1635] - <code>spark_engine.write()</code> \u2192 <code>table_exists()</code> [line 857] - <code>_generate_incremental_sql_filter()</code> \u2192 <code>table_exists()</code> [line 662]</p> <p>Cost: ~3-5s per redundant check (Delta table open + limit(0).collect())</p> <p>Evidence:</p> <pre><code># spark_engine.py:643-648\ndef table_exists(self, connection, table=None, path=None):\n    if table:\n        if not self.spark.catalog.tableExists(table):\n            return False\n        self.spark.table(table).limit(0).collect()  # Expensive verify\n</code></pre> <p>Fix: Cache existence check result at node level</p> <pre><code># Add to NodeExecutor.__init__\nself._table_exists_cache: Dict[str, bool] = {}\n\n# In table_exists calls\ncache_key = f\"{connection}:{table or path}\"\nif cache_key in self._table_exists_cache:\n    return self._table_exists_cache[cache_key]\n</code></pre> <p>Expected Savings: 5-10s per node</p>"},{"location":"architecture/performance-analysis-findings/#2-schema-capture-on-every-write-medium-high-impact","title":"2. Schema Capture on Every Write \u26a0\ufe0f MEDIUM-HIGH IMPACT","text":"<p>Location: <code>_execute_write_phase</code> lines 1634-1642</p> <pre><code>if config.schema_policy and df is not None:\n    target_schema = self.engine.get_table_schema(  # Delta table read!\n        connection=connection,\n        table=write_config.table,\n        path=write_config.path,\n        format=write_config.format,\n    )\n</code></pre> <p>Cost: ~2-4s (full Delta table schema read, even for small tables)</p> <p>Fix: Skip schema check if table doesn't exist (first run) or use cached schema</p> <pre><code># Only check if target exists and schema_policy requires it\nif config.schema_policy and config.schema_policy.mode != SchemaMode.EVOLVE:\n    if self._table_exists_cache.get(cache_key):\n        target_schema = self.engine.get_table_schema(...)\n</code></pre> <p>Expected Savings: 2-4s per node</p>"},{"location":"architecture/performance-analysis-findings/#3-delta-table-registration-on-every-append-high-impact","title":"3. Delta Table Registration on EVERY Append \u26a0\ufe0f HIGH IMPACT","text":"<p>Location: <code>spark_engine.py</code> lines 1050-1061</p> <pre><code>if register_table:\n    create_sql = (\n        f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n        f\"USING DELTA LOCATION '{full_path}'\"\n    )\n    self.spark.sql(create_sql)  # Runs EVERY write, even for appends\n</code></pre> <p>Cost: ~5-15s (catalog operation + metadata update)</p> <p>This is the primary bottleneck for incremental appends. Running <code>CREATE TABLE IF NOT EXISTS</code> on every write is extremely expensive.</p> <p>Fix: Check if table already registered before running SQL</p> <pre><code>if register_table:\n    if not self.spark.catalog.tableExists(register_table):\n        create_sql = ...\n        self.spark.sql(create_sql)\n</code></pre> <p>Expected Savings: 10-20s per incremental write</p>"},{"location":"architecture/performance-analysis-findings/#4-table-properties-applied-per-property-medium-impact","title":"4. Table Properties Applied Per-Property \u26a0\ufe0f MEDIUM IMPACT","text":"<p>Location: <code>spark_engine.py:225-252</code></p> <pre><code>def _apply_table_properties(self, target, properties, is_table=False):\n    for prop_name, prop_value in properties.items():\n        sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ('{prop_name}' = '{prop_value}')\"\n        self.spark.sql(sql)  # One SQL per property!\n</code></pre> <p>Cost: ~1-2s per property (3-4 properties = 5-8s total)</p> <p>Fix: Batch properties into single ALTER TABLE statement</p> <pre><code>if properties:\n    props_str = \", \".join([f\"'{k}' = '{v}'\" for k, v in properties.items()])\n    sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n    self.spark.sql(sql)\n</code></pre> <p>Expected Savings: 3-6s per node with table properties</p>"},{"location":"architecture/performance-analysis-findings/#5-catalog-entries-written-synchronously-medium-impact","title":"5. Catalog Entries Written Synchronously \u26a0\ufe0f MEDIUM IMPACT","text":"<p>Location: <code>_register_catalog_entries()</code> [node.py:1806-1910]</p> <p>Each write triggers 4 separate Delta operations: 1. <code>register_asset()</code> - Delta append 2. <code>track_schema()</code> - Delta merge (upsert) 3. <code>log_pattern()</code> - Delta merge (upsert) 4. <code>record_lineage()</code> - Delta append</p> <p>Cost: ~3-8s total (each Delta operation has transaction overhead)</p> <p>Fix: Batch catalog writes or make them async</p> <pre><code># Option 1: Batch all catalog writes into single transaction\n# Option 2: Fire-and-forget async writes for non-critical metadata\n# Option 3: Add config to disable catalog writes for performance-critical pipelines\n</code></pre> <p>Expected Savings: 3-6s per node</p>"},{"location":"architecture/performance-analysis-findings/#6-skip_if_unchanged-state-lookup-overhead-low-medium-impact","title":"6. skip_if_unchanged State Lookup Overhead \u26a0\ufe0f LOW-MEDIUM IMPACT","text":"<p>Location: <code>_check_skip_if_unchanged()</code> [node.py:1978-2041]</p> <p>State Lookup Path:</p> <pre><code>state_backend = getattr(self.state_manager, \"backend\", None)\nprevious_hash = get_content_hash_from_state(state_backend, config.name, table_name)\n</code></pre> <p>This triggers <code>_get_hwm_spark()</code> which:</p> <pre><code>df = self.spark.read.format(\"delta\").load(self.meta_state_path)  # Full table read\nrow = df.filter(F.col(\"key\") == key).select(\"value\").first()\n</code></pre> <p>Cost: ~2-3s (Delta table open + filter + collect)</p> <p>The hash computation itself is efficient (distributed xxhash64), but the state lookup reads the entire meta_state table.</p> <p>Fix: Add predicate pushdown or use data skipping</p> <pre><code># Use partition pruning or data skipping\ndf = self.spark.read.format(\"delta\").load(self.meta_state_path)\nrow = df.filter(F.col(\"key\") == key).select(\"value\").limit(1).first()\n</code></pre> <p>Or cache recent HWM values in memory during pipeline execution.</p> <p>Expected Savings: 1-2s per node with skip_if_unchanged</p>"},{"location":"architecture/performance-analysis-findings/#7-dfrddgetnumpartitions-triggers-job-low-impact","title":"7. df.rdd.getNumPartitions() Triggers Job \u26a0\ufe0f LOW IMPACT","text":"<p>Location: <code>spark_engine.write()</code> line 764</p> <pre><code>partition_count = df.rdd.getNumPartitions()  # Spark action!\n</code></pre> <p>For lazy DataFrames, this can trigger computation of the lineage to determine partitions.</p> <p>Cost: ~0.5-2s (variable based on DAG complexity)</p> <p>Fix: Make partition count optional or use lazy evaluation</p> <pre><code>try:\n    # Quick check if already materialized\n    if df.isStreaming:\n        partition_count = None\n    elif hasattr(df, \"_jdf\") and df._jdf.queryExecution().isInstanceOf(...):\n        partition_count = df.rdd.getNumPartitions()\n    else:\n        partition_count = None\nexcept:\n    partition_count = None\n</code></pre> <p>Expected Savings: 0.5-2s per node</p>"},{"location":"architecture/performance-analysis-findings/#8-delta-commit-metadata-fetched-after-write-low-impact","title":"8. Delta Commit Metadata Fetched After Write \u26a0\ufe0f LOW IMPACT","text":"<p>Location: <code>_get_last_delta_commit_info()</code> [spark_engine.py:299-348]</p> <pre><code>dt.history(1).collect()  # Opens Delta table again!\n</code></pre> <p>Cost: ~1-2s (Delta table open + history scan)</p> <p>Fix: Extract commit info from write operation directly if possible, or cache DeltaTable reference.</p> <p>Expected Savings: 1-2s per node</p>"},{"location":"architecture/performance-analysis-findings/#why-small-tables-31-84-rows-take-30-50s","title":"Why Small Tables (31-84 rows) Take 30-50s","text":"<p>The overhead is constant per write, not proportional to data volume:</p> Operation Estimated Time Table existence check 3-5s Schema capture 2-4s Delta write itself 2-5s CREATE TABLE IF NOT EXISTS 5-15s Table properties (3x) 3-6s Catalog entries (4x) 3-8s Delta commit metadata 1-2s Total 19-45s <p>The actual DataFrame write is only ~2-5s. Everything else is metadata overhead.</p>"},{"location":"architecture/performance-analysis-findings/#specific-node-analysis","title":"Specific Node Analysis","text":""},{"location":"architecture/performance-analysis-findings/#tblgrinddailyproduction-59-rows-30-45s","title":"tblGrindDailyProduction (59 rows, 30-45s)","text":"<ul> <li>Likely has: table registration, schema policy, table properties</li> <li>All constant overhead applies regardless of 59 rows</li> </ul>"},{"location":"architecture/performance-analysis-findings/#vwdryershiftlineproductrunwithdryeronhours-684-rows-40s","title":"vwDryerShiftLineProductRunWithDryerOnHours (684 rows, 40s)","text":"<ul> <li>Possibly running OPTIMIZE/ZORDER after write</li> <li>Check if <code>auto_optimize</code> or <code>zorder_by</code> is configured</li> </ul>"},{"location":"architecture/performance-analysis-findings/#recommended-priority-fixes","title":"Recommended Priority Fixes","text":""},{"location":"architecture/performance-analysis-findings/#phase-1-quick-wins-expected-15-25s-savings","title":"Phase 1: Quick Wins (Expected: 15-25s savings)","text":"<ol> <li>Skip <code>CREATE TABLE IF NOT EXISTS</code> for existing tables (10-20s)</li> <li>Batch table properties into single ALTER TABLE (3-6s)</li> <li>Cache table existence checks (5-10s)</li> </ol>"},{"location":"architecture/performance-analysis-findings/#phase-2-medium-effort-expected-5-10s-savings","title":"Phase 2: Medium Effort (Expected: 5-10s savings)","text":"<ol> <li>Skip schema capture when not needed (2-4s)</li> <li>Batch catalog entries (3-6s)</li> </ol>"},{"location":"architecture/performance-analysis-findings/#phase-3-architecture-changes-expected-5-10s-savings","title":"Phase 3: Architecture Changes (Expected: 5-10s savings)","text":"<ol> <li>Async catalog writes</li> <li>State lookup optimization</li> <li>Lazy partition count</li> </ol>"},{"location":"architecture/performance-analysis-findings/#implementation-sketch","title":"Implementation Sketch","text":""},{"location":"architecture/performance-analysis-findings/#fix-1-skip-redundant-table-registration","title":"Fix #1: Skip Redundant Table Registration","text":"<pre><code># spark_engine.py, around line 1050\nif register_table:\n    # Only register if table doesn't exist in catalog\n    if not self.spark.catalog.tableExists(register_table):\n        create_sql = (\n            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n            f\"USING DELTA LOCATION '{full_path}'\"\n        )\n        self.spark.sql(create_sql)\n        ctx.info(f\"Registered table: {register_table}\", path=full_path)\n    else:\n        ctx.debug(f\"Table {register_table} already registered, skipping\")\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#fix-2-batch-table-properties","title":"Fix #2: Batch Table Properties","text":"<pre><code># spark_engine.py, replace _apply_table_properties\ndef _apply_table_properties(self, target: str, properties: Dict[str, str], is_table: bool = False):\n    if not properties:\n        return\n\n    table_ref = target if is_table else f\"delta.`{target}`\"\n    props_list = [f\"'{k}' = '{v}'\" for k, v in properties.items()]\n    props_str = \", \".join(props_list)\n\n    sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n    self.spark.sql(sql)\n    ctx.debug(f\"Set {len(properties)} table properties in single statement\")\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#fix-3-add-table-existence-cache","title":"Fix #3: Add Table Existence Cache","text":"<pre><code># node.py, NodeExecutor class\ndef __init__(self, ...):\n    ...\n    self._table_exists_cache: Dict[str, bool] = {}\n\ndef _cached_table_exists(self, connection, table=None, path=None) -&gt; bool:\n    cache_key = f\"{id(connection)}:{table}:{path}\"\n    if cache_key not in self._table_exists_cache:\n        self._table_exists_cache[cache_key] = self.engine.table_exists(connection, table, path)\n    return self._table_exists_cache[cache_key]\n</code></pre>"},{"location":"architecture/performance-analysis-findings/#next-steps","title":"Next Steps","text":"<ol> <li>Implement Fix #1 (register_table skip) - highest ROI</li> <li>Add performance timing logs to validate bottleneck locations</li> <li>A/B test with timing before/after each fix</li> <li>Consider adding a <code>write.performance_mode: fast</code> option that skips non-essential metadata operations</li> </ol>"},{"location":"architecture/performance-optimization-plan/","title":"Pipeline Performance Optimization Plan","text":""},{"location":"architecture/performance-optimization-plan/#current-state-analysis","title":"Current State Analysis","text":"<p>Bronze Pipeline: 55.10s for 35 nodes (as of Dec 2024) - Write phase: 53.01s (96.2%) - Read phase: 1.91s (3.5%) - Transform phase: 1.33s (2.4%)</p> <p>Previous State (before optimization): 75.95s for 35 nodes</p> <p>Identified Bottlenecks:</p>"},{"location":"architecture/performance-optimization-plan/#1-slow-sql-source-reads","title":"1. Slow SQL Source Reads","text":"<p>Problem: Several nodes show extremely slow row rates: | Node | Duration | Rows | Rate | |------|----------|------|------| | <code>nkcmfgproduction_vwDryerShiftLineProductRunWithDryerOnHours</code> | 54.98s | 684 | 12 rows/s | | <code>nkcmfgproduction_tblGrindDailyProduction</code> | 40.71s | 59 | 1.4 rows/s | | <code>indyProduction_tblDryerDowntime</code> | 43.68s | 1.6K | 37 rows/s | | <code>opsvisdata_vw_ref_annualgoal</code> | 23.34s | 47 | 2 rows/s |</p> <p>Root Cause: These are SQL Server views with complex underlying joins. The pipeline reads the entire view without incremental filtering, causing full table scans at the source.</p> <p>Solution:</p> <pre><code># Add incremental config to slow nodes\nread:\n  connection: sql_source\n  format: sql_server\n  table: vwDryerShiftLineProductRunWithDryerOnHours\n  incremental:\n    mode: rolling_window\n    column: ModifiedDate  # or appropriate timestamp column\n    lookback: 7\n    unit: day\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#2-write-phase-dominates-pipeline-time","title":"2. Write Phase Dominates Pipeline Time","text":"<p>Problem: 80.8% of pipeline time spent in write phase.</p> <p>Root Causes: 1. Many small files creating metadata overhead 2. Delta transaction log operations for each table 3. No coalescing of small DataFrames</p> <p>Solutions:</p>"},{"location":"architecture/performance-optimization-plan/#a-coalesce-dataframes-opt-in","title":"a) Coalesce DataFrames (Opt-in)","text":"<p>Status: \u2705 AVAILABLE (Opt-in via config)</p> <p>Coalesce DataFrames to fewer partitions before writing to reduce file overhead.</p> <p>Why opt-in instead of automatic: Automatic coalescing required <code>df.count()</code> which triggers double-evaluation of lazy DataFrames, causing severe performance regression.</p> <p>How to use:</p> <pre><code>write:\n  format: delta\n  path: bronze/table\n  options:\n    coalesce_partitions: 1  # Coalesce to 1 partition before write\n</code></pre> <p>When to use: - Small incremental loads (&lt; 1000 rows) - Tables that accumulate many small files</p>"},{"location":"architecture/performance-optimization-plan/#b-enable-optimized-write","title":"b) Enable Optimized Write","text":"<pre><code>write:\n  format: delta\n  path: bronze/table\n  options:\n    optimize_write: true  # Let Delta optimize file sizes\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#c-parallel-execution","title":"c) Parallel Execution","text":"<p>Status: \u2705 IN USE</p> <pre><code>manager.run(pipeline=\"bronze\", parallel=True, max_workers=16)\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#3-skip_if_unchanged-hash-computation","title":"3. skip_if_unchanged Hash Computation","text":"<p>Status: \u2705 IMPLEMENTED</p> <p>Problem: Legacy implementation collected entire DataFrame to driver for hashing.</p> <p>Solution: Distributed hash computation using <code>xxhash64</code> (now the default).</p> <p>Code Location: <code>odibi/utils/content_hash.py:100-122</code></p> <pre><code>def _compute_spark_hash_distributed(df) -&gt; str:\n    \"\"\"Compute hash distributedly using Spark's xxhash64.\"\"\"\n    from pyspark.sql import functions as F\n\n    hash_cols = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"__NULL__\")) for c in df.columns]\n    work_df = df.withColumn(\"_row_hash\", F.xxhash64(*hash_cols))\n\n    result = work_df.agg(\n        F.count(\"*\").alias(\"row_count\"),\n        F.sum(\"_row_hash\").alias(\"hash_sum\"),\n    ).collect()[0]\n\n    row_count = result[\"row_count\"] or 0\n    hash_sum = result[\"hash_sum\"] or 0\n    combined = f\"v2:{row_count}:{hash_sum}:{','.join(sorted(df.columns))}\"\n    return hashlib.sha256(combined.encode()).hexdigest()\n</code></pre> <p>Benefits: - No data collection to driver (except 2 scalar values) - No full sort required (uses commutative sum) - O(1) memory on driver - Safe for arbitrarily large DataFrames</p>"},{"location":"architecture/performance-optimization-plan/#4-detect_deletes-schema-warnings","title":"4. detect_deletes Schema Warnings","text":"<p>Problem: Warnings about missing keys in previous version.</p> <pre><code>detect_deletes: Keys ['OEE_EVENT_START', ...] not found in previous version (v3).\nSchema may have changed. Skipping delete detection.\n</code></pre> <p>Root Cause: Schema evolution between runs. The previous Delta version has different column names.</p> <p>Solution: Add schema migration handling:</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      mode: snapshot_diff\n      keys: [OEE_EVENT_START, OEE_EVENT_END, Plant, Channel, Asset]\n      on_first_run: skip  # Don't error on first run/schema change\n      on_schema_change: skip  # Skip if schema incompatible\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#5-high-delete-percentage-warnings","title":"5. High Delete Percentage Warnings","text":"<p>Problem: 1279% and 3961% deletion thresholds exceeded.</p> <p>Root Cause: Either: - Wrong keys configured (causing all rows to appear deleted) - Source data genuinely changed significantly - First-time comparison against wrong version</p> <p>Solution: 1. Verify key columns are correct primary keys 2. Set reasonable threshold with warning:</p> <pre><code>- operation: detect_deletes\n  mode: snapshot_diff\n  keys: [correct_key_column]\n  max_delete_percent: 50\n  on_threshold_breach: warn  # Log warning instead of failing\n</code></pre>"},{"location":"architecture/performance-optimization-plan/#implementation-priority","title":"Implementation Priority","text":"Priority Fix Impact Effort Status \ud83d\udd34 High Distributed hash for skip_if_unchanged Avoid driver OOM, faster Medium \u2705 Done \ud83d\udd34 High Coalesce option for writes Reduce write overhead Low \u2705 Available (opt-in) \ud83d\udd34 High Add incremental to slow SQL sources 40-50s savings Low \u2705 Done \ud83d\udfe1 Medium Fix detect_deletes key configuration Clean logs, correct behavior Low \ud83d\udd04 Ongoing \ud83d\udfe2 Low Enable parallel execution Overlap I/O Low \u2705 Done"},{"location":"architecture/performance-optimization-plan/#remaining-bottleneck-delta-append-overhead","title":"Remaining Bottleneck: Delta Append Overhead","text":"<p>The remaining ~30-50s overhead per Delta append to Azure Blob Storage is inherent to the Delta Lake protocol on cloud storage. Each append requires: 1. Read transaction log from Azure 2. Write new Parquet file(s) to Azure 3. Write new transaction log entry 4. Table registration SQL (if applicable)</p> <p>This cannot be significantly reduced further without architectural changes like: - Batching multiple tables into fewer writes - Using a different storage format - Reducing write frequency</p>"},{"location":"architecture/performance-optimization-plan/#quick-wins-applied","title":"Quick Wins Applied","text":"<ol> <li>\u2705 Parallel execution: <code>manager.run(pipeline=\"bronze\", parallel=True, max_workers=16)</code></li> <li>\u2705 Rolling window incremental added to slow SQL views</li> <li>\u2705 Distributed hash now default for skip_if_unchanged</li> <li>\u2705 Coalesce option available via <code>coalesce_partitions</code> write option</li> <li>\ud83d\udd04 Set on_threshold_breach: warn for detect_deletes</li> </ol>"},{"location":"architecture/performance-optimization-plan/#lessons-learned","title":"Lessons Learned","text":"<p>\u26a0\ufe0f Never call <code>df.count()</code> before write - This triggers double-evaluation of lazy DataFrames, causing SQL sources to be read twice. An attempted auto-coalesce feature caused 2.5x performance regression (55s \u2192 138s) due to this issue.</p>"},{"location":"explanation/architecture/","title":"Architecture Guide - Odibi System Design","text":"<p>Visual guide to how Odibi works. See the big picture!</p>"},{"location":"explanation/architecture/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        USER                                  \u2502\n\u2502                          \u2502                                   \u2502\n\u2502                          \u25bc                                   \u2502\n\u2502                   config.yaml                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CONFIG LAYER                               \u2502\n\u2502                                                               \u2502\n\u2502  config.yaml \u2192 Pydantic Models \u2192 ProjectConfig               \u2502\n\u2502                     \u2193                                         \u2502\n\u2502              Validation happens here                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  PIPELINE LAYER                              \u2502\n\u2502                                                               \u2502\n\u2502  Pipeline \u2192 DependencyGraph \u2192 Execution Order                \u2502\n\u2502                     \u2193                                         \u2502\n\u2502              [Node A, Node B, Node C]                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   NODE LAYER                                 \u2502\n\u2502                                                               \u2502\n\u2502  Node \u2192 Read/Transform/Write \u2192 Engine                        \u2502\n\u2502           \u2193                      \u2193                            \u2502\n\u2502    Transformation           PandasEngine                      \u2502\n\u2502      Registry               PolarsEngine                      \u2502\n\u2502                             SparkEngine                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STATE &amp; METADATA LAYER                     \u2502\n\u2502                                                               \u2502\n\u2502  System Catalog (Delta Tables) \u2190\u2192 OpenLineage Emitter        \u2502\n\u2502           \u2193                           \u2193                       \u2502\n\u2502    _odibi_system/state          DataHub / Marquez             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STORAGE LAYER                              \u2502\n\u2502                                                               \u2502\n\u2502  Connections \u2192 Local / Azure / SQL                           \u2502\n\u2502                     \u2193                                         \u2502\n\u2502               Actual Data                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STORY LAYER                                \u2502\n\u2502                                                               \u2502\n\u2502  Metadata \u2192 Renderers \u2192 HTML/MD/JSON                         \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Automatic audit trail                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"explanation/architecture/#pipeline-execution-flow","title":"Pipeline Execution Flow","text":""},{"location":"explanation/architecture/#step-by-step-what-happens-when-you-run-odibi-run-configyaml","title":"Step-by-Step: What Happens When You Run <code>odibi run config.yaml</code>","text":"<pre><code>1. CLI Entry Point (cli/main.py)\n   \u2502\n   \u251c\u2500\u2192 Parse arguments\n   \u2514\u2500\u2192 Call run_command(args)\n\n2. Load Configuration (cli/run.py)\n   \u2502\n   \u251c\u2500\u2192 Read YAML file\n   \u251c\u2500\u2192 Parse to ProjectConfig (Pydantic validation)\n   \u2514\u2500\u2192 Create PipelineManager\n\n3. Build Dependency Graph (graph.py)\n   \u2502\n   \u251c\u2500\u2192 Extract all nodes\n   \u251c\u2500\u2192 Build dependency edges\n   \u251c\u2500\u2192 Check for cycles\n   \u2514\u2500\u2192 Topological sort \u2192 execution order\n\n4. Execute Nodes (pipeline.py)\n   \u2502\n   \u251c\u2500\u2192 For each node in order:\n   \u2502   \u2502\n   \u2502   \u251c\u2500\u2192 Create Node instance (node.py)\n   \u2502   \u251c\u2500\u2192 Execute read/transform/write\n   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u251c\u2500\u2192 Read: Engine.read() \u2192 DataFrame\n   \u2502   \u2502   \u251c\u2500\u2192 Transform: Registry.get(operation) \u2192 transformed DataFrame  \n   \u2502   \u2502   \u2514\u2500\u2192 Write: Engine.write(DataFrame)\n   \u2502   \u2502\n   \u2502   \u251c\u2500\u2192 Store result in Context\n   \u2502   \u2514\u2500\u2192 Track metadata (timing, rows, schema)\n   \u2502\n   \u2514\u2500\u2192 All nodes complete\n\n5. Generate Story (story/generator.py)\n   \u2502\n   \u251c\u2500\u2192 Collect all node metadata\n   \u251c\u2500\u2192 Calculate aggregates (success rate, total rows)\n   \u251c\u2500\u2192 Render to HTML/MD/JSON\n   \u2514\u2500\u2192 Save to stories/runs/\n\n6. Return to User\n   \u2502\n   \u2514\u2500\u2192 \"Pipeline completed successfully\" \u2705\n</code></pre>"},{"location":"explanation/architecture/#module-dependencies","title":"Module Dependencies","text":""},{"location":"explanation/architecture/#core-dependencies","title":"Core Dependencies","text":"<pre><code>config.py (no dependencies - pure Pydantic models)\n    \u2193\ncontext.py (stores DataFrames)\n    \u2193\ntransformations/ (registry + decorators)\n    \u2193\noperations/ (uses transformations)\n    \u2193\nengine/ (executes operations)\n    \u2193\nnode.py (uses engine + context)\n    \u2193\ngraph.py (orders nodes)\n    \u2193\npipeline.py (orchestrates everything)\n    \u2193\nstory/ (documents execution)\n    \u2193\ncli/ (user interface)\n</code></pre>"},{"location":"explanation/architecture/#module-relationships","title":"Module Relationships","text":"<pre><code>transformations/\n    \u251c\u2500\u2192 Used by: node.py, story/doc_story.py\n    \u2514\u2500\u2192 Uses: registry.py (core)\n\nregistry.py\n    \u251c\u2500\u2192 Used by: transformations/, engine/\n    \u2514\u2500\u2192 Uses: Nothing (singleton)\n\nstate/\n    \u251c\u2500\u2192 Used by: node.py, pipeline.py\n    \u2514\u2500\u2192 Uses: deltalake (local), spark (distributed)\n\nlineage/\n    \u251c\u2500\u2192 Used by: node.py, pipeline.py\n    \u2514\u2500\u2192 Uses: openlineage-python (optional)\n\nconnections/\n    \u251c\u2500\u2192 Used by: engine/\n    \u2514\u2500\u2192 Uses: Nothing (independent connectors)\n\nengine/\n    \u251c\u2500\u2192 Used by: node.py\n    \u2514\u2500\u2192 Uses: connections/, transformations/\n\ncli/\n    \u251c\u2500\u2192 Used by: Users!\n    \u2514\u2500\u2192 Uses: Everything\n</code></pre> <p>Key insight: <code>transformations/</code> provides the logic, <code>engine/</code> provides the horsepower, and <code>state/</code> provides the memory.</p>"},{"location":"explanation/architecture/#data-flow","title":"Data Flow","text":""},{"location":"explanation/architecture/#how-data-moves-through-a-pipeline","title":"How Data Moves Through a Pipeline","text":"<pre><code>1. User YAML Config\n   \u2193\n2. Parsed to ProjectConfig (in-memory objects)\n   \u2193\n3. Pipeline.run() starts execution\n   \u2193\n4. For each node:\n\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Node Execution                           \u2502\n   \u2502                                          \u2502\n   \u2502  1. Read Phase (if configured)           \u2502\n   \u2502     \u2514\u2500\u2192 Engine.read() \u2192 DataFrame        \u2502\n   \u2502           \u2514\u2500\u2192 Connection.get_path()      \u2502\n   \u2502                 \u2514\u2500\u2192 Actual file/DB       \u2502\n   \u2502                                          \u2502\n   \u2502  2. Transform Phase (if configured)      \u2502\n   \u2502     \u251c\u2500\u2192 Get DataFrame from context       \u2502\n   \u2502     \u251c\u2500\u2192 Registry.get(operation)          \u2502\n   \u2502     \u2514\u2500\u2192 func(df, **params) \u2192 DataFrame   \u2502\n   \u2502                                          \u2502\n   \u2502  3. Write Phase (if configured)          \u2502\n   \u2502     \u2514\u2500\u2192 Engine.write(DataFrame)          \u2502\n   \u2502           \u2514\u2500\u2192 Connection + format        \u2502\n   \u2502                                          \u2502\n   \u2502  4. Store Result                         \u2502\n   \u2502     \u2514\u2500\u2192 Context.set(node_name, df)       \u2502\n   \u2502                                          \u2502\n   \u2502  5. Track Metadata                       \u2502\n   \u2502     \u2514\u2500\u2192 NodeExecutionMetadata            \u2502\n   \u2502           \u251c\u2500\u2192 Row counts                 \u2502\n   \u2502           \u251c\u2500\u2192 Schema                     \u2502\n   \u2502           \u2514\u2500\u2192 Timing                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   \u2193\n5. All nodes complete\n   \u2193\n6. Generate Story\n   \u2514\u2500\u2192 PipelineStoryMetadata\n       \u251c\u2500\u2192 All node metadata\n       \u2514\u2500\u2192 Rendered to HTML/MD/JSON\n</code></pre>"},{"location":"explanation/architecture/#transformation-lifecycle","title":"Transformation Lifecycle","text":""},{"location":"explanation/architecture/#registration-import-time","title":"Registration (Import Time)","text":"<pre><code># When Python imports odibi/operations/unpivot.py:\n\n@transformation(\"unpivot\", category=\"reshaping\")  # \u2190 This runs immediately!\ndef unpivot(df, ...):\n    ...\n\n# What happens:\n# 1. transformation(\"unpivot\", ...) returns a decorator\n# 2. Decorator wraps unpivot function\n# 3. Decorator calls registry.register(\"unpivot\", wrapped_unpivot)\n# 4. Registry stores it globally\n# 5. Function is now available to all pipelines\n</code></pre>"},{"location":"explanation/architecture/#lookup-runtime","title":"Lookup (Runtime)","text":"<pre><code># During pipeline execution:\n\n# 1. Node config says: operation=\"unpivot\"\n# 2. Node calls: registry.get(\"unpivot\")\n# 3. Registry returns the function\n# 4. Node calls: func(df, id_vars=\"ID\", ...)\n# 5. Result returned\n</code></pre>"},{"location":"explanation/architecture/#explanation-story-generation","title":"Explanation (Story Generation)","text":"<pre><code># During story generation:\n\n# 1. Story generator calls: func.get_explanation(**params, **context)\n# 2. ExplainableFunction looks for attached explain_func\n# 3. If found: calls explain_func(**params, **context)\n# 4. Returns formatted markdown\n# 5. Included in HTML story\n</code></pre>"},{"location":"explanation/architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"explanation/architecture/#connection-abstraction","title":"Connection Abstraction","text":"<pre><code>BaseConnection (interface)\n    \u2193\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        \u2502          \u2502             \u2502\nLocal   ADLS    AzureSQL      (more...)\n\u2502        \u2502          \u2502\n\u2193        \u2193          \u2193\n./data  Azure Blob  SQL Database\n</code></pre> <p>All connections implement: - <code>get_path(relative_path)</code> - Resolve full path - <code>validate()</code> - Check configuration</p> <p>Storage-specific methods: - ADLS: <code>pandas_storage_options()</code>, <code>configure_spark()</code> - AzureSQL: <code>read_sql()</code>, <code>write_table()</code>, <code>get_engine()</code> - Local: (just path manipulation)</p>"},{"location":"explanation/architecture/#engine-abstraction","title":"Engine Abstraction","text":"<pre><code>Engine (interface)\n    \u2193\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                \u2502\nPandasEngine  SparkEngine\n\u2502                \u2502\n\u2193                \u2193\nDataFrame    pyspark.DataFrame\n</code></pre> <p>All engines implement: - <code>read(connection, path, format, options)</code> - <code>write(df, connection, path, format, mode, options)</code> - <code>execute_sql(df, query)</code></p> <p>Why? Swap Pandas \u2194 Spark without changing config!</p>"},{"location":"explanation/architecture/#story-generation-architecture","title":"Story Generation Architecture","text":""},{"location":"explanation/architecture/#three-types-of-stories","title":"Three Types of Stories","text":"<pre><code>1. RUN STORIES (automatic)\n   \u2502\n   \u2514\u2500\u2192 Generated during pipeline.run()\n       \u251c\u2500\u2192 Captures actual execution\n       \u251c\u2500\u2192 Saved to stories/runs/\n       \u2514\u2500\u2192 For audit/debugging\n\n2. DOC STORIES (on-demand)\n   \u2502\n   \u2514\u2500\u2192 Generated via CLI: odibi story generate\n       \u251c\u2500\u2192 Pulls operation explanations\n       \u251c\u2500\u2192 For stakeholder communication\n       \u2514\u2500\u2192 Saved to docs/\n\n3. DIFF STORIES (comparison)\n   \u2502\n   \u2514\u2500\u2192 Generated via CLI: odibi story diff\n       \u251c\u2500\u2192 Compares two run stories\n       \u251c\u2500\u2192 Shows what changed\n       \u2514\u2500\u2192 For troubleshooting\n</code></pre>"},{"location":"explanation/architecture/#story-generation-pipeline","title":"Story Generation Pipeline","text":"<pre><code>Execution \u2192 Metadata Collection \u2192 Rendering \u2192 Output\n   \u2193              \u2193                   \u2193          \u2193\nNodes run    NodeExecution      Renderer    HTML file\n             Metadata          (HTML/MD/JSON)\n             tracked\n</code></pre>"},{"location":"explanation/architecture/#the-registry-pattern-deep-dive","title":"The Registry Pattern (Deep Dive)","text":""},{"location":"explanation/architecture/#why-this-pattern","title":"Why This Pattern?","text":"<p>Problem: How do we make operations available globally?</p> <p>Bad Solution 1: Import everything</p> <pre><code>from odibi.operations import pivot, unpivot, join, sql, ...\n# Breaks as we add more operations\n</code></pre> <p>Bad Solution 2: String-based imports</p> <pre><code>op_module = __import__(f\"odibi.operations.{operation_name}\")\n# Fragile, hard to debug\n</code></pre> <p>Good Solution: Registry</p> <pre><code># Operations register themselves:\n@transformation(\"pivot\")\ndef pivot(...): ...\n\n# Look up by name:\nfunc = registry.get(\"pivot\")\n\n# Easy! Scalable! Type-safe!\n</code></pre>"},{"location":"explanation/architecture/#registry-singleton-pattern","title":"Registry Singleton Pattern","text":"<p>One registry for entire process:</p> <pre><code># odibi/transformations/registry.py\n\n# Create once at module level\n_global_registry = TransformationRegistry()\n\ndef get_registry():\n    return _global_registry  # Always same instance\n</code></pre> <p>Benefits: - \u2705 Single source of truth - \u2705 Operations registered once - \u2705 Available everywhere - \u2705 Easy to test (registry.clear() in tests)</p>"},{"location":"explanation/architecture/#error-handling-strategy","title":"Error Handling Strategy","text":""},{"location":"explanation/architecture/#validation-layers","title":"Validation Layers","text":"<pre><code>Layer 1: Pydantic (config validation)\n   \u2193\nLayer 2: Connection.validate() (connection validation)\n   \u2193\nLayer 3: Graph.validate() (dependency validation)\n   \u2193\nLayer 4: Runtime (execution errors)\n</code></pre> <p>Fail fast: Catch errors before execution starts!</p>"},{"location":"explanation/architecture/#error-propagation","title":"Error Propagation","text":"<pre><code>try:\n    # Node execution\n    result = node.execute()\nexcept Exception as e:\n    # Caught by node.py\n    node_result = NodeResult(\n        success=False,\n        error=e\n    )\n    # Stored in metadata\n    # Shown in story\n    # Pipeline continues (or stops, depending on config)\n</code></pre> <p>Stories capture all errors - makes debugging easy!</p>"},{"location":"explanation/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"explanation/architecture/#time-complexity","title":"Time Complexity","text":"<ul> <li>Config loading: O(1) - just YAML parse</li> <li>Dependency graph: O(n + e) - n nodes, e edges</li> <li>Node execution: O(n) - linear in number of nodes</li> <li>Story generation: O(n) - linear in number of nodes</li> </ul>"},{"location":"explanation/architecture/#space-complexity","title":"Space Complexity","text":"<ul> <li>Context storage: O(n \u00d7 m) - n nodes, m average DataFrame size</li> <li>Metadata: O(n) - one metadata object per node</li> <li>Stories: O(n) - proportional to nodes</li> </ul>"},{"location":"explanation/architecture/#optimization-points","title":"Optimization Points","text":"<p>1. Parallel Execution (future)</p> <pre><code>Current: A \u2192 B \u2192 C \u2192 D (sequential)\nFuture:  A \u2192 B \u2510\n         A \u2192 C \u2534\u2192 D (parallel)\n</code></pre> <p>2. Lazy Evaluation (future)</p> <pre><code>Current: Execute all nodes\nFuture:  Only execute nodes needed for requested output\n</code></pre> <p>3. Incremental Processing (future)</p> <pre><code>Current: Reprocess all data every time\nFuture:  Only process new/changed data\n</code></pre>"},{"location":"explanation/architecture/#testing-architecture","title":"Testing Architecture","text":""},{"location":"explanation/architecture/#test-pyramid","title":"Test Pyramid","text":"<pre><code>        /\\\n       /E2E\\          \u2190 10 tests (slow, comprehensive)\n      /\u2500\u2500\u2500\u2500\u2500\u2500\\\n     /Integration\\    \u2190 30 tests (medium speed)\n    /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n   /   Unit Tests  \\  \u2190 380+ tests (fast, focused)\n  /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n</code></pre> <p>Unit Tests (380+) - Test individual functions/classes - Use mocks for external dependencies - Run in &lt;5 seconds - Cover edge cases</p> <p>Integration Tests (30) - Test components working together - Use real files (temp directories) - Run in &lt;10 seconds - Cover common scenarios</p> <p>E2E Tests (10) - Test complete pipelines - Real configs, real data - Run in &lt;30 seconds - Cover critical paths</p>"},{"location":"explanation/architecture/#mocking-strategy","title":"Mocking Strategy","text":"<p>Mock external dependencies, not internal ones:</p> <pre><code># \u2705 Good: Mock external SQLAlchemy\n@patch('sqlalchemy.create_engine')\ndef test_azure_sql(mock_engine):\n    conn = AzureSQL(...)\n    # Test connection logic without real DB\n\n# \u274c Bad: Mock internal functions\n@patch('odibi.operations.pivot')\ndef test_something(mock_pivot):\n    # Doesn't test real code!\n</code></pre>"},{"location":"explanation/architecture/#design-patterns-used","title":"Design Patterns Used","text":""},{"location":"explanation/architecture/#1-registry-pattern","title":"1. Registry Pattern","text":"<p>Where: <code>odibi/registry.py</code></p> <p>Purpose: Centralized operation lookup</p> <p>Example: All operations register themselves globally</p> <pre><code># In odibi/transformers/math.py\n@transform(\"calculate_sum\")\ndef calculate_sum(df, ...): ...\n</code></pre>"},{"location":"explanation/architecture/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>Where: <code>odibi/connections/factory.py</code></p> <p>Purpose: Create connections by type name</p> <pre><code>conn = create_connection(config)  # Returns AzureBlobConnection, LocalConnection, etc.\n</code></pre>"},{"location":"explanation/architecture/#3-adapter-pattern-state","title":"3. Adapter Pattern (State)","text":"<p>Where: <code>odibi/state/__init__.py</code></p> <p>Purpose: Uniform interface for state management</p> <pre><code># Unified CatalogStateBackend handles both local and distributed modes:\nbackend = CatalogStateBackend(...)\n# Local: uses delta-rs (writes to local delta tables)\n# Spark: uses Spark SQL (writes to delta tables on ADLS/S3)\n</code></pre>"},{"location":"explanation/architecture/#4-observer-pattern-lineage","title":"4. Observer Pattern (Lineage)","text":"<p>Where: <code>odibi/lineage/</code></p> <p>Purpose: Emit events without coupling execution logic</p> <pre><code># Node execution emits events:\nlineage.emit_start(node)\n# ... execution ...\nlineage.emit_complete(node)\n</code></pre>"},{"location":"explanation/architecture/#5-strategy-pattern","title":"5. Strategy Pattern","text":"<p>Where: <code>engine/</code> (PandasEngine vs SparkEngine vs PolarsEngine)</p> <p>Purpose: Swap execution strategies</p> <pre><code># Same interface, different implementation:\nengine = PandasEngine()  # or PolarsEngine()\ndf = engine.read(...)    # Works with either!\n</code></pre>"},{"location":"explanation/architecture/#5-builder-pattern","title":"5. Builder Pattern","text":"<p>Where: <code>story/doc_story.py</code></p> <p>Purpose: Construct complex documentation</p> <pre><code>generator = DocStoryGenerator(config)\ngenerator.generate(\n    output_path=\"doc.html\",\n    format=\"html\",\n    theme=CORPORATE_THEME\n)\n</code></pre>"},{"location":"explanation/architecture/#6-template-method-pattern","title":"6. Template Method Pattern","text":"<p>Where: <code>story/renderers.py</code></p> <p>Purpose: Define rendering algorithm skeleton</p> <pre><code>class BaseRenderer:\n    def render_to_file(self, metadata, path):\n        content = self.render(metadata)  # Subclass implements\n        self._save(content, path)        # Common logic\n</code></pre>"},{"location":"explanation/architecture/#key-abstractions","title":"Key Abstractions","text":""},{"location":"explanation/architecture/#1-engine-abstraction","title":"1. Engine Abstraction","text":"<p>Why? Support multiple execution backends</p> <pre><code># User doesn't care if Pandas or Spark:\ndf = engine.read(connection, \"data.parquet\", \"parquet\")\n\n# PandasEngine: uses pd.read_parquet()\n# SparkEngine: uses spark.read.parquet()\n# Same interface!\n</code></pre>"},{"location":"explanation/architecture/#2-connection-abstraction","title":"2. Connection Abstraction","text":"<p>Why? Support multiple storage systems</p> <pre><code># User writes: path: \"data.csv\"\n# Connection resolves to:\n# - Local: ./data/data.csv\n# - ADLS: abfss://container@account.dfs.core.windows.net/data.csv\n# - SQL: Table reference\n\n# Same code, different storage!\n</code></pre>"},{"location":"explanation/architecture/#3-transformation-abstraction","title":"3. Transformation Abstraction","text":"<p>Why? User-defined operations work same as built-in</p> <pre><code># Built-in:\n@transformation(\"pivot\")\ndef pivot(...): ...\n\n# User-defined:\n@transformation(\"my_custom_op\")\ndef my_custom_op(...): ...\n\n# Both registered the same way!\n# Both available in YAML!\n</code></pre>"},{"location":"explanation/architecture/#extensibility-points","title":"Extensibility Points","text":""},{"location":"explanation/architecture/#where-you-can-extend-odibi","title":"Where You Can Extend Odibi","text":"<p>1. Add New Operations</p> <pre><code>Location: odibi/operations/\nPattern: Use @transformation decorator\nImpact: Available in all pipelines\n</code></pre> <p>2. Add New Connections</p> <pre><code>Location: odibi/connections/\nPattern: Extend BaseConnection\nImpact: New storage backends\n</code></pre> <p>3. Add New Engines</p> <pre><code>Location: odibi/engine/\nPattern: Implement Engine interface\nImpact: New execution backends\n</code></pre> <p>4. Add New Renderers</p> <pre><code>Location: odibi/story/renderers.py\nPattern: Implement .render() method\nImpact: New story output formats\n</code></pre> <p>5. Add New Themes</p> <pre><code>Location: odibi/story/themes.py\nPattern: Create StoryTheme instance\nImpact: Custom branding\n</code></pre> <p>6. Add New Validators</p> <pre><code>Location: odibi/validation/\nPattern: Create validator class\nImpact: Quality enforcement\n</code></pre>"},{"location":"explanation/architecture/#configuration-model","title":"Configuration Model","text":""},{"location":"explanation/architecture/#pydantic-model-hierarchy","title":"Pydantic Model Hierarchy","text":"<pre><code>ProjectConfig (root)\n    \u251c\u2500\u2500 connections: Dict[str, ConnectionConfig]\n    \u251c\u2500\u2500 story: StoryConfig\n    \u2514\u2500\u2500 pipelines: List[PipelineConfig]\n            \u2514\u2500\u2500 nodes: List[NodeConfig]\n                    \u251c\u2500\u2500 read: ReadConfig (optional)\n                    \u251c\u2500\u2500 transform: TransformConfig (optional)\n                    \u2514\u2500\u2500 write: WriteConfig (optional)\n</code></pre>"},{"location":"explanation/architecture/#validation-flow","title":"Validation Flow","text":"<pre><code>YAML file\n    \u2193\nyaml.safe_load() \u2192 dict\n    \u2193\nProjectConfig(**dict)  \u2190 Pydantic validation happens here!\n    \u2193\nIf valid: ProjectConfig instance\nIf invalid: ValidationError with helpful message\n</code></pre> <p>Example error:</p> <pre><code>ValidationError: 1 validation error for NodeConfig\nname\n  Field required [type=missing]\n</code></pre>"},{"location":"explanation/architecture/#thread-safety","title":"Thread Safety","text":""},{"location":"explanation/architecture/#current-state-single-threaded","title":"Current State: Single-Threaded","text":"<p>Registry: Thread-safe (read-only after startup) Context: NOT thread-safe (single pipeline execution) Pipeline: NOT thread-safe (sequential execution)</p>"},{"location":"explanation/architecture/#future-parallel-execution","title":"Future: Parallel Execution","text":"<p>Possible:</p> <pre><code># Execute independent nodes in parallel\nLayer 0: [A]\nLayer 1: [B, C]  \u2190 Can run in parallel!\nLayer 2: [D]\n</code></pre> <p>Required changes: - Thread-safe Context - Parallel node execution - Coordinated metadata collection</p>"},{"location":"explanation/architecture/#memory-management","title":"Memory Management","text":""},{"location":"explanation/architecture/#dataframe-lifecycle","title":"DataFrame Lifecycle","text":"<pre><code>1. Read \u2192 DataFrame created (stored in memory)\n   \u2193\n2. Transform \u2192 New DataFrame (old one can be GC'd if not reused)\n   \u2193\n3. Write \u2192 DataFrame written to disk\n   \u2193\n4. Context stores DataFrame for downstream nodes\n   \u2193\n5. Pipeline completes \u2192 Context cleared \u2192 memory freed\n</code></pre>"},{"location":"explanation/architecture/#large-dataset-strategies","title":"Large Dataset Strategies","text":"<p>Option 1: Don't store in context</p> <pre><code># Future: Streaming mode\n# Don't keep DataFrames in memory\n# Process and write immediately\n</code></pre> <p>Option 2: Use Spark</p> <pre><code>engine: spark\n# Spark handles large data with partitioning\n</code></pre> <p>Option 3: Chunk processing</p> <pre><code># Write in chunks\nchunksize: 10000  # Process 10K rows at a time\n</code></pre>"},{"location":"explanation/architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"explanation/architecture/#credential-handling","title":"Credential Handling","text":"<p>\u2705 Good:</p> <pre><code>connections:\n  azure:\n    auth_mode: key_vault  # Credentials in Key Vault\n    key_vault_name: myvault\n    secret_name: storage-key\n</code></pre> <p>\u274c Bad:</p> <pre><code>connections:\n  azure:\n    account_key: \"hardcoded_key_here\"  # DON'T!\n</code></pre>"},{"location":"explanation/architecture/#sql-injection-protection","title":"SQL Injection Protection","text":"<p>Odibi uses DuckDB which executes on DataFrames (not databases): - No SQL injection risk - DataFrames are local - Safe execution</p> <p>For Azure SQL, use parameterized queries:</p> <pre><code># \u2705 Safe\nconn.read_sql(\n    \"SELECT * FROM users WHERE id = :user_id\",\n    params={\"user_id\": 123}\n)\n\n# \u274c Unsafe\nconn.read_sql(f\"SELECT * FROM users WHERE id = {user_id}\")\n</code></pre>"},{"location":"explanation/architecture/#next-steps","title":"Next Steps","text":"<p>You now understand the architecture!</p> <p>Learn how to build on it: - Transformation Guide - Create custom operations - Troubleshooting - Debug issues - Read the code! Start with <code>operations/</code> directory</p> <p>Remember: The tests are comprehensive examples. Use them! \ud83e\uddea</p>"},{"location":"explanation/case_studies/","title":"\ud83c\udfdb\ufe0f Case Studies (Reference Projects)","text":"<p>Odibi is battle-tested. To prove it, we built \"The Gauntlet\"\u2014a series of reference implementations covering diverse industries and data challenges.</p> <p>These projects live in the <code>examples/</code> directory and serve as blueprints for your own architectures.</p>"},{"location":"explanation/case_studies/#1-odibiflix-high-volume-clickstream","title":"1. OdibiFlix (High-Volume Clickstream)","text":"<p>Scenario: A streaming service processing millions of user events (play, pause, buffer). Challenge: *   Sessionization: Grouping raw events into distinct user sessions (30-minute timeout). *   State Management: Calculating \"buffer ratio\" per session.</p> <p>Architecture: *   Engine: Spark (required for window functions). *   Pattern: Medallion (Bronze JSON -&gt; Silver Delta -&gt; Gold Aggregates). *   Key Feature: Uses <code>odibi.yaml</code> to orchestrate complex SQL window functions without writing Python code.</p>"},{"location":"explanation/case_studies/#2-odibieats-real-time-delivery","title":"2. OdibiEats (Real-Time Delivery)","text":"<p>Scenario: A food delivery app with geospatial data. Challenge: *   Geospatial Joins: Mapping driver lat/long pings to neighborhood polygons. *   SCD Type 2: Tracking menu price changes over time (historical accuracy).</p> <p>Architecture: *   Engine: Pandas (Geopandas extension). *   Pattern: Star Schema (Fact Orders, Dim Restaurants, Dim Drivers). *   Key Feature: Demonstrates how to handle \"slowly changing dimensions\" using Odibi's snapshotting capabilities.</p>"},{"location":"explanation/case_studies/#3-odibihealth-sensitive-pii","title":"3. OdibiHealth (Sensitive PII)","text":"<p>Scenario: A healthcare provider managing patient records. Challenge: *   Compliance: HIPAA requirements to mask PII (Patient Identity). *   Audit: Strict lineage tracking of who accessed what data.</p> <p>Architecture: *   Security: Uses Odibi's <code>sensitive: true</code> flag to redact columns in logs/stories. *   Observability: High-fidelity logging enabled. *   Key Feature: Shows how to build secure pipelines that generate audit trails automatically via <code>odibi story</code>.</p>"},{"location":"explanation/case_studies/#4-odibiquant-financial-time-series","title":"4. OdibiQuant (Financial Time-Series)","text":"<p>Scenario: High-frequency trading data analysis. Challenge: *   Data Quality: Ensuring no gaps in timestamps. *   Precision: Handling 64-bit floats without rounding errors.</p> <p>Architecture: *   Validation: Heavy use of <code>expectations</code> (e.g., <code>row_count &gt; 0</code>, <code>price &gt; 0</code>). *   Engine: DuckDB (via Pandas engine) for fast analytical queries.</p>"},{"location":"explanation/case_studies/#why-use-these","title":"Why use these?","text":"<p>Don't start from scratch. If you are building a fintech app, copy <code>OdibiQuant</code>. If you are building an e-commerce site, look at <code>OdibiEats</code>.</p> <p>Run them locally:</p> <pre><code>cd examples/odibi_flix\nodibi run odibi.yaml\n</code></pre>"},{"location":"features/alerting/","title":"Enhanced Alerting","text":"<p>Real-time notifications for pipeline events with throttling, event-specific payloads, and support for Slack, Teams, and generic webhooks.</p>"},{"location":"features/alerting/#overview","title":"Overview","text":"<p>Odibi's alerting system provides: - Multiple channels: Slack, Teams, generic webhooks - Event-specific payloads: Contextual information for each event type - Throttling: Prevent alert spam with rate limiting - Rich formatting: Adaptive cards for Teams, Block Kit for Slack</p>"},{"location":"features/alerting/#configuration","title":"Configuration","text":""},{"location":"features/alerting/#basic-alert-setup","title":"Basic Alert Setup","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n</code></pre>"},{"location":"features/alerting/#alert-config-options","title":"Alert Config Options","text":"Field Type Required Description <code>type</code> string Yes Alert type: <code>slack</code>, <code>teams</code>, <code>webhook</code> <code>url</code> string Yes Webhook URL <code>on_events</code> list No Events to trigger on (default: <code>on_failure</code>) <code>metadata</code> object No Extra settings (throttling, channel, etc.)"},{"location":"features/alerting/#event-types","title":"Event Types","text":"Event Description <code>on_start</code> Pipeline started <code>on_success</code> Pipeline completed successfully <code>on_failure</code> Pipeline failed <code>on_quarantine</code> Rows were quarantined <code>on_gate_block</code> Quality gate blocked the pipeline <code>on_threshold_breach</code> A threshold was exceeded"},{"location":"features/alerting/#throttling","title":"Throttling","text":"<p>Prevent alert spam with time-based and rate-based throttling:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15   # Min 15 minutes between same alerts\n      max_per_hour: 10       # Max 10 alerts per hour\n</code></pre>"},{"location":"features/alerting/#throttle-key","title":"Throttle Key","text":"<p>Throttling is applied per unique combination of: - Pipeline name - Event type</p> <p>So a <code>process_orders</code> pipeline failing twice in 5 minutes sends one alert, but a different pipeline can still alert.</p>"},{"location":"features/alerting/#event-specific-payloads","title":"Event-Specific Payloads","text":""},{"location":"features/alerting/#quarantine-events","title":"Quarantine Events","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n</code></pre> <p>Payload includes: - Rows quarantined count - Quarantine table path - Failed test names - Node name</p>"},{"location":"features/alerting/#gate-block-events","title":"Gate Block Events","text":"<pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_gate_block\n</code></pre> <p>Payload includes: - Pass rate achieved - Required pass rate - Number of failed rows - Failure reasons</p>"},{"location":"features/alerting/#threshold-breach-events","title":"Threshold Breach Events","text":"<pre><code>alerts:\n  - type: webhook\n    url: \"https://api.example.com/alerts\"\n    on_events:\n      - on_threshold_breach\n</code></pre> <p>Payload includes: - Metric name - Threshold value - Actual value - Node name</p>"},{"location":"features/alerting/#complete-example","title":"Complete Example","text":"<pre><code>project: DataPipeline\nengine: spark\n\nalerts:\n  # Critical failures - immediate notification\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 5\n      channel: \"#data-critical\"\n\n  # Data quality issues - less urgent\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n      - on_threshold_breach\n    metadata:\n      throttle_minutes: 30\n      max_per_hour: 5\n      channel: \"#data-quality\"\n\n  # Teams for management visibility\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n    metadata:\n      throttle_minutes: 60\n\nconnections:\n  # ...\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: validate_orders\n        validation:\n          tests:\n            - type: not_null\n              columns: [order_id]\n              on_fail: quarantine\n          quarantine:\n            connection: silver\n            path: quarantine/orders\n          gate:\n            require_pass_rate: 0.95\n        # ...\n</code></pre>"},{"location":"features/alerting/#slack-configuration","title":"Slack Configuration","text":""},{"location":"features/alerting/#block-kit-format","title":"Block Kit Format","text":"<p>Slack alerts use Block Kit for rich formatting:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udeab ODIBI: process_orders - GATE_BLOCKED \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Project:    DataPipeline                 \u2502\n\u2502 Status:     GATE_BLOCKED                 \u2502\n\u2502 Duration:   45.23s                       \u2502\n\u2502 Pass Rate:  92.3%                        \u2502\n\u2502 Required:   95.0%                        \u2502\n\u2502 Rows Failed: 1,542                       \u2502\n\u2502                                          \u2502\n\u2502 [\ud83d\udcca View Story]                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/alerting/#custom-channel","title":"Custom Channel","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n    metadata:\n      channel: \"#data-alerts\"  # Override default channel\n</code></pre>"},{"location":"features/alerting/#teams-configuration","title":"Teams Configuration","text":""},{"location":"features/alerting/#adaptive-card-format","title":"Adaptive Card Format","text":"<p>Teams alerts use Adaptive Cards:</p> <pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 30\n</code></pre> <p>Cards include: - Color-coded header (red for failures, orange for warnings) - Fact set with key metrics - Action button to view story</p>"},{"location":"features/alerting/#generic-webhooks","title":"Generic Webhooks","text":"<p>For custom integrations:</p> <pre><code>alerts:\n  - type: webhook\n    url: \"https://api.example.com/webhooks/odibi\"\n    on_events:\n      - on_failure\n      - on_quarantine\n</code></pre>"},{"location":"features/alerting/#webhook-payload","title":"Webhook Payload","text":"<pre><code>{\n  \"pipeline\": \"process_orders\",\n  \"status\": \"QUARANTINE\",\n  \"duration\": 45.23,\n  \"message\": \"150 rows quarantined in validate_orders\",\n  \"timestamp\": \"2024-01-30T10:15:00Z\",\n  \"event_type\": \"on_quarantine\",\n  \"quarantine_details\": {\n    \"rows_quarantined\": 150,\n    \"quarantine_path\": \"silver/quarantine/orders\",\n    \"failed_tests\": [\"not_null\", \"email_format\"],\n    \"node_name\": \"validate_orders\"\n  }\n}\n</code></pre>"},{"location":"features/alerting/#programmatic-alerts","title":"Programmatic Alerts","text":"<p>Send alerts programmatically:</p> <pre><code>from odibi.utils.alerting import (\n    send_quarantine_alert,\n    send_gate_block_alert,\n    send_threshold_breach_alert,\n)\nfrom odibi.config import AlertConfig, AlertType\n\nconfig = AlertConfig(\n    type=AlertType.SLACK,\n    url=\"https://hooks.slack.com/...\",\n)\n\n# Quarantine alert\nsend_quarantine_alert(\n    config=config,\n    pipeline=\"process_orders\",\n    node_name=\"validate_orders\",\n    rows_quarantined=150,\n    quarantine_path=\"silver/quarantine/orders\",\n    failed_tests=[\"not_null\", \"email_format\"],\n)\n\n# Gate block alert\nsend_gate_block_alert(\n    config=config,\n    pipeline=\"process_orders\",\n    node_name=\"validate_orders\",\n    pass_rate=0.92,\n    required_rate=0.95,\n    failed_rows=1542,\n    total_rows=20000,\n    failure_reasons=[\"Pass rate 92.0% &lt; required 95.0%\"],\n)\n</code></pre>"},{"location":"features/alerting/#best-practices","title":"Best Practices","text":"<ol> <li>Use throttling - Prevent alert fatigue during incidents</li> <li>Separate channels - Critical vs informational alerts</li> <li>Include context - Story URLs help with debugging</li> <li>Test webhooks - Verify connectivity before production</li> <li>Monitor alert volume - High volumes indicate systemic issues</li> </ol>"},{"location":"features/alerting/#related","title":"Related","text":"<ul> <li>Quarantine Tables - Quarantine event source</li> <li>Quality Gates - Gate block event source</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/catalog/","title":"System Catalog","text":"<p>Centralized governance and metadata management for pipelines, execution history, schema evolution, and lineage tracking.</p>"},{"location":"features/catalog/#overview","title":"Overview","text":"<p>Odibi's System Catalog (\"The Brain\") provides: - Pipeline Registry: Track pipeline and node definitions with version hashing - Execution History: Complete run history with metrics and duration - State Management: High-water marks (HWM) for incremental processing - Schema Evolution: Automatic tracking of schema changes over time - Lineage Tracking: Table-level upstream/downstream relationships - Pattern Compliance: Track medallion architecture adherence</p>"},{"location":"features/catalog/#configuration","title":"Configuration","text":""},{"location":"features/catalog/#basic-catalog-setup","title":"Basic Catalog Setup","text":"<pre><code>system:\n  connection: system_storage\n  path: _odibi_system\n\nconnections:\n  system_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n</code></pre>"},{"location":"features/catalog/#system-config-options","title":"System Config Options","text":"Field Type Required Description <code>connection</code> string Yes Connection name for catalog storage <code>path</code> string No Subdirectory for catalog tables (default: <code>_odibi_system</code>)"},{"location":"features/catalog/#catalog-tables","title":"Catalog Tables","text":"<p>The System Catalog consists of Delta tables that automatically bootstrap on first run:</p>"},{"location":"features/catalog/#meta_pipelines","title":"meta_pipelines","text":"<p>Tracks pipeline definitions and deployment versions.</p> Column Type Description <code>pipeline_name</code> string Unique pipeline identifier <code>version_hash</code> string MD5 hash of pipeline configuration <code>description</code> string Pipeline description <code>layer</code> string Medallion layer (bronze/silver/gold) <code>schedule</code> string Cron schedule (if defined) <code>tags_json</code> string JSON array of aggregated tags <code>updated_at</code> timestamp Last deployment timestamp"},{"location":"features/catalog/#meta_nodes","title":"meta_nodes","text":"<p>Tracks node configurations within pipelines.</p> Column Type Description <code>pipeline_name</code> string Parent pipeline name <code>node_name</code> string Unique node identifier <code>version_hash</code> string MD5 hash of node configuration <code>type</code> string Node type: read/transform/write <code>config_json</code> string Full node configuration as JSON <code>updated_at</code> timestamp Last deployment timestamp"},{"location":"features/catalog/#meta_runs","title":"meta_runs","text":"<p>Execution history with metrics. Partitioned by <code>pipeline_name</code> and <code>date</code>.</p> Column Type Description <code>run_id</code> string Unique execution identifier <code>pipeline_name</code> string Pipeline name <code>node_name</code> string Node name <code>status</code> string SUCCESS, FAILED, RUNNING <code>rows_processed</code> long Number of rows processed <code>duration_ms</code> long Execution time in milliseconds <code>metrics_json</code> string Additional metrics as JSON <code>timestamp</code> timestamp Execution timestamp <code>date</code> date Partition date"},{"location":"features/catalog/#meta_state","title":"meta_state","text":"<p>High-water mark (HWM) storage for incremental processing. Partitioned by <code>pipeline_name</code>.</p> Column Type Description <code>pipeline_name</code> string Pipeline name <code>node_name</code> string Node name <code>hwm_value</code> string Serialized high-water mark value"},{"location":"features/catalog/#meta_patterns","title":"meta_patterns","text":"<p>Tracks pattern compliance for governance.</p> Column Type Description <code>table_name</code> string Table identifier <code>pattern_type</code> string Pattern type (SCD2, append, etc.) <code>configuration</code> string Pattern configuration as JSON <code>compliance_score</code> double Compliance score (0.0 - 1.0)"},{"location":"features/catalog/#meta_schemas","title":"meta_schemas","text":"<p>Schema version history for drift detection.</p> Column Type Description <code>table_path</code> string Full table path <code>schema_version</code> long Incrementing version number <code>schema_hash</code> string MD5 hash of column definitions <code>columns</code> string JSON: {\"column\": \"type\", ...} <code>captured_at</code> timestamp When schema was captured <code>pipeline</code> string Pipeline that wrote the schema <code>node</code> string Node that wrote the schema <code>run_id</code> string Execution run ID <code>columns_added</code> array New columns in this version <code>columns_removed</code> array Removed columns <code>columns_type_changed</code> array Columns with type changes"},{"location":"features/catalog/#meta_lineage","title":"meta_lineage","text":"<p>Cross-pipeline table lineage relationships.</p> Column Type Description <code>source_table</code> string Source table path <code>target_table</code> string Target table path <code>source_pipeline</code> string Source pipeline (if known) <code>source_node</code> string Source node (if known) <code>target_pipeline</code> string Target pipeline <code>target_node</code> string Target node <code>relationship</code> string \"feeds\" or \"derived_from\" <code>last_observed</code> timestamp Last time relationship was seen <code>run_id</code> string Execution run ID"},{"location":"features/catalog/#meta_tables","title":"meta_tables","text":"<p>Registry of all written tables/assets for discovery.</p> Column Type Description <code>table_path</code> string Full path to the table <code>table_name</code> string Table name <code>pipeline</code> string Pipeline that owns the table <code>node</code> string Node that writes the table <code>format</code> string Storage format (delta, parquet, etc.) <code>connection</code> string Connection name <code>last_updated</code> timestamp Last write timestamp"},{"location":"features/catalog/#meta_metrics","title":"meta_metrics","text":"<p>Business metric definitions for governance and documentation.</p> Column Type Description <code>metric_name</code> string Unique metric identifier <code>definition_sql</code> string SQL definition of the metric <code>dimensions</code> array List of dimension columns <code>source_table</code> string Source table for the metric"},{"location":"features/catalog/#features","title":"Features","text":""},{"location":"features/catalog/#auto-registration","title":"Auto-Registration","text":"<p>Pipelines and nodes are automatically registered when you run them\u2014no explicit <code>deploy()</code> calls required:</p> <pre><code>from odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# Auto-registers pipeline and nodes before execution\nmanager.run(\"my_pipeline\")\n</code></pre> <p>This ensures <code>meta_pipelines</code> and <code>meta_nodes</code> are always populated. Version hashes detect configuration drift automatically.</p>"},{"location":"features/catalog/#pipeline-registration","title":"Pipeline Registration","text":"<p>For explicit registration (e.g., CI/CD pipelines), use:</p> <pre><code>from odibi.catalog import CatalogManager\n\n# Explicit registration\ncatalog.register_pipeline(pipeline_config)\n</code></pre> <p>When a pipeline's configuration changes, the <code>version_hash</code> updates, providing: - Configuration drift detection - Deployment history tracking - Audit trail for changes</p>"},{"location":"features/catalog/#schema-tracking","title":"Schema Tracking","text":"<p>Schema evolution is tracked automatically after every successful write. No manual calls required:</p> <ul> <li><code>meta_schemas</code> is updated with column changes (added, removed, type changes)</li> <li>Version numbers increment on each schema change</li> <li>Change detection compares against the previous version</li> </ul> <p>Querying schema history:</p> <pre><code># Get schema history for a table\nhistory = manager.get_schema_history(\"silver/customers\", limit=10)\n\n# Returns DataFrame with columns_added, columns_removed, columns_type_changed\n</code></pre>"},{"location":"features/catalog/#lineage-tracking","title":"Lineage Tracking","text":"<p>Lineage is tracked automatically based on node dependencies and read/write operations:</p> <ul> <li>Source tables (from <code>read</code> config) are recorded as upstream</li> <li>Target tables (from <code>write</code> config) are recorded as downstream  </li> <li>Cross-pipeline relationships are captured via <code>meta_lineage</code></li> </ul> <p>Querying lineage:</p> <pre><code># Get upstream and downstream lineage\nlineage_df = manager.get_lineage(\"silver/orders\", direction=\"both\")\n\n# Or use CatalogManager directly\nupstream = catalog.get_upstream(\"gold/order_summary\", depth=3)\ndownstream = catalog.get_downstream(\"bronze/raw_orders\", depth=3)\n</code></pre>"},{"location":"features/catalog/#run-history-and-metrics","title":"Run History and Metrics","text":"<p>Execution runs are logged automatically after each node completes:</p> <ul> <li>Status (SUCCESS/FAILURE), duration, rows processed</li> <li>Metrics stored in <code>meta_runs</code>, partitioned by pipeline and date</li> </ul> <p>Querying run history:</p> <pre><code># Get recent runs\nruns_df = manager.list_runs(pipeline=\"orders_pipeline\", limit=20)\n\n# Get average duration for a node\navg_seconds = catalog.get_average_duration(\"transform_orders\", days=7)\n</code></pre>"},{"location":"features/catalog/#asset-registration","title":"Asset Registration","text":"<p>Tables are registered automatically in <code>meta_tables</code> after writes, enabling discovery across the catalog.</p>"},{"location":"features/catalog/#catalog-optimization","title":"Catalog Optimization","text":"<p>Maintenance operations for Spark deployments:</p> <pre><code># Run VACUUM and OPTIMIZE on meta_runs\ncatalog.optimize()\n</code></pre>"},{"location":"features/catalog/#cleanup-and-removal","title":"Cleanup and Removal","text":"<p>Remove stale pipelines, nodes, or orphaned entries:</p> <pre><code># Remove a pipeline and cascade to associated nodes\ndeleted = catalog.remove_pipeline(\"old_pipeline\")\n\n# Remove a specific node\ndeleted = catalog.remove_node(\"my_pipeline\", \"deprecated_node\")\n\n# Cleanup orphans: remove entries not in current config\nresults = catalog.cleanup_orphans(project_config)\n# Returns: {\"meta_pipelines\": 2, \"meta_nodes\": 5}\n\n# Clear state entries\ncatalog.clear_state_key(\"my_pipeline::my_node::hwm\")\ncatalog.clear_state_pattern(\"my_pipeline::*\")  # Wildcards supported\n</code></pre>"},{"location":"features/catalog/#catalogmanager-api","title":"CatalogManager API","text":""},{"location":"features/catalog/#initialization","title":"Initialization","text":"<pre><code>from odibi.catalog import CatalogManager\nfrom odibi.config import SystemConfig\n\ncatalog = CatalogManager(\n    spark=spark_session,           # SparkSession (or None for Pandas)\n    config=system_config,          # SystemConfig object\n    base_path=\"abfss://...\",       # Resolved catalog path\n    engine=pandas_engine           # Optional: for Pandas mode\n)\n</code></pre>"},{"location":"features/catalog/#key-methods","title":"Key Methods","text":"Method Description <code>bootstrap()</code> Create all system tables if missing <code>register_pipeline(config)</code> Register/update pipeline definition <code>register_nodes(config)</code> Register/update node definitions <code>log_run(...)</code> Record execution run <code>track_schema(...)</code> Track schema version <code>get_schema_history(table, limit)</code> Get schema version history <code>record_lineage(...)</code> Record table lineage relationship <code>get_upstream(table, depth)</code> Get upstream dependencies <code>get_downstream(table, depth)</code> Get downstream consumers <code>get_average_duration(node, days)</code> Get average node duration <code>log_metrics(...)</code> Log business metric definitions <code>remove_pipeline(name)</code> Remove pipeline and cascade to nodes <code>remove_node(pipeline, node)</code> Remove a specific node <code>cleanup_orphans(config)</code> Remove entries not in current config <code>clear_state_key(key)</code> Remove a state entry by key <code>clear_state_pattern(pattern)</code> Remove state entries matching pattern <code>optimize()</code> Run VACUUM and OPTIMIZE (Spark only)"},{"location":"features/catalog/#pipelinemanager-query-api","title":"PipelineManager Query API","text":"<p>The <code>PipelineManager</code> provides convenient query methods that wrap catalog operations with smart path resolution:</p>"},{"location":"features/catalog/#smart-path-resolution","title":"Smart Path Resolution","text":"<p>Query methods accept user-friendly identifiers that are automatically resolved:</p> <pre><code># All these work:\nmanager.get_schema_history(\"silver/orders\")           # Relative path\nmanager.get_lineage(\"test.vw_customers\")              # Registered table\nmanager.get_lineage(\"transform_orders\")               # Node name\nmanager.get_schema_history(\"abfss://container/...\")   # Full path (as-is)\n</code></pre>"},{"location":"features/catalog/#query-methods","title":"Query Methods","text":"Method Description <code>list_registered_pipelines()</code> DataFrame of all pipelines from <code>meta_pipelines</code> <code>list_registered_nodes(pipeline=None)</code> DataFrame of nodes, optionally filtered by pipeline <code>list_runs(pipeline, node, status, limit)</code> DataFrame of recent runs with filters <code>list_tables()</code> DataFrame of registered assets from <code>meta_tables</code> <code>get_state(key)</code> Get specific state entry (HWM, etc.) as dict <code>get_all_state(prefix=None)</code> DataFrame of state entries, optionally filtered <code>clear_state(key)</code> Remove a state entry <code>get_schema_history(table, limit)</code> DataFrame of schema versions <code>get_lineage(table, direction)</code> DataFrame of upstream/downstream lineage <code>get_pipeline_status(pipeline)</code> Dict with last run status, duration, timestamp <code>get_node_stats(node, days)</code> Dict with success rate, avg duration, avg rows"},{"location":"features/catalog/#usage-examples","title":"Usage Examples","text":"<pre><code>from odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# List all registered pipelines\npipelines_df = manager.list_registered_pipelines()\n\n# List nodes in a specific pipeline\nnodes_df = manager.list_registered_nodes(pipeline=\"orders_pipeline\")\n\n# Get recent failed runs\nfailed_runs = manager.list_runs(status=\"FAILURE\", limit=20)\n\n# Get HWM state for a node\nhwm = manager.get_state(\"orders_pipeline::load_orders::hwm\")\n\n# Get lineage for a table (both directions)\nlineage_df = manager.get_lineage(\"silver/orders\", direction=\"both\")\n\n# Get node statistics\nstats = manager.get_node_stats(\"transform_orders\", days=7)\n# Returns: {\"node\": \"...\", \"runs\": 42, \"success_rate\": 0.95, \"avg_duration_s\": 12.5, ...}\n\n# Get pipeline status\nstatus = manager.get_pipeline_status(\"orders_pipeline\")\n# Returns: {\"pipeline\": \"...\", \"last_status\": \"SUCCESS\", \"last_run_at\": \"...\", ...}\n</code></pre>"},{"location":"features/catalog/#cli-integration","title":"CLI Integration","text":"<p>Query the catalog from the command line:</p>"},{"location":"features/catalog/#list-execution-runs","title":"List Execution Runs","text":"<pre><code># Recent runs\nodibi catalog runs config.yaml\n\n# Filter by pipeline, status, and time range\nodibi catalog runs config.yaml --pipeline orders_pipeline --status FAILED --days 3\n\n# JSON output\nodibi catalog runs config.yaml --format json --limit 50\n</code></pre>"},{"location":"features/catalog/#list-registered-pipelines","title":"List Registered Pipelines","text":"<pre><code>odibi catalog pipelines config.yaml\nodibi catalog pipelines config.yaml --format json\n</code></pre>"},{"location":"features/catalog/#list-registered-nodes","title":"List Registered Nodes","text":"<pre><code>odibi catalog nodes config.yaml\nodibi catalog nodes config.yaml --pipeline orders_pipeline\n</code></pre>"},{"location":"features/catalog/#view-hwm-state","title":"View HWM State","text":"<pre><code>odibi catalog state config.yaml\nodibi catalog state config.yaml --pipeline orders_pipeline\n</code></pre>"},{"location":"features/catalog/#list-registered-assets","title":"List Registered Assets","text":"<pre><code>odibi catalog tables config.yaml\nodibi catalog tables config.yaml --project MyProject\n</code></pre>"},{"location":"features/catalog/#view-execution-statistics","title":"View Execution Statistics","text":"<pre><code># Statistics for last 7 days\nodibi catalog stats config.yaml\n\n# Filter by pipeline and time range\nodibi catalog stats config.yaml --pipeline orders_pipeline --days 30\n</code></pre> <p>Output includes: - Total runs, success/failure counts - Success rate percentage - Total and average rows processed - Average and total runtime - Runs by pipeline - Most failed nodes</p>"},{"location":"features/catalog/#cli-options","title":"CLI Options","text":"Command Options <code>runs</code> <code>--pipeline</code>, <code>--node</code>, <code>--status</code>, <code>--days</code>, <code>--limit</code>, <code>--format</code> <code>pipelines</code> <code>--format</code> <code>nodes</code> <code>--pipeline</code>, <code>--format</code> <code>state</code> <code>--pipeline</code>, <code>--format</code> <code>tables</code> <code>--project</code>, <code>--format</code> <code>metrics</code> <code>--format</code> <code>patterns</code> <code>--format</code> <code>stats</code> <code>--pipeline</code>, <code>--days</code>"},{"location":"features/catalog/#complete-example","title":"Complete Example","text":""},{"location":"features/catalog/#project-configuration","title":"Project Configuration","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nsystem:\n  connection: catalog_storage\n  path: _odibi_catalog\n\nconnections:\n  catalog_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n\n  bronze:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\n  silver:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: silver\n\npipelines:\n  - pipeline: orders_bronze_to_silver\n    description: \"Transform raw orders to silver layer\"\n    layer: silver\n    nodes:\n      - name: read_raw_orders\n        type: read\n        connection: bronze\n        path: raw/orders\n        format: delta\n\n      - name: transform_orders\n        type: transform\n        input: read_raw_orders\n        transform: |\n          SELECT\n            order_id,\n            customer_id,\n            order_date,\n            total_amount\n          FROM {input}\n          WHERE order_date &gt;= '2024-01-01'\n\n      - name: write_orders\n        type: write\n        input: transform_orders\n        connection: silver\n        path: orders\n        format: delta\n        mode: merge\n        merge_keys: [order_id]\n</code></pre>"},{"location":"features/catalog/#querying-the-catalog","title":"Querying the Catalog","text":"<pre><code># Check registered pipelines\nodibi catalog pipelines config.yaml\n\n# Output:\n# pipeline_name            | layer  | description                          | version_hash | updated_at\n# -------------------------+--------+--------------------------------------+--------------+--------------------\n# orders_bronze_to_silver  | silver | Transform raw orders to silver layer | a1b2c3d4...  | 2024-01-30 10:15:00\n\n# View execution history\nodibi catalog runs config.yaml --pipeline orders_bronze_to_silver --days 7\n\n# Get statistics\nodibi catalog stats config.yaml --pipeline orders_bronze_to_silver\n\n# Output:\n# === Execution Statistics (Last 7 Days) ===\n#\n# Total Runs:     42\n# Successful:     40\n# Failed:         2\n# Success Rate:   95.2%\n#\n# Total Rows:     1,250,000\n# Avg Rows/Run:   29,762\n#\n# Avg Duration:   12.45s\n# Total Runtime:  522.90s\n</code></pre>"},{"location":"features/catalog/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from odibi.pipeline import PipelineManager\n\n# Load configuration\nmanager = PipelineManager.from_yaml(\"config.yaml\")\ncatalog = manager.catalog_manager\n\n# Query schema history\nhistory = catalog.get_schema_history(\"silver/orders\")\nfor version in history:\n    print(f\"v{version['schema_version']}: {version['columns_added']} added\")\n\n# Trace lineage\nupstream = catalog.get_upstream(\"gold/order_summary\")\nfor source in upstream:\n    print(f\"  {'  ' * source['depth']}{source['source_table']}\")\n</code></pre>"},{"location":"features/catalog/#best-practices","title":"Best Practices","text":"<ol> <li>Enable catalog early - Configure the system catalog from project start</li> <li>Use descriptive names - Pipeline and node names become permanent identifiers</li> <li>Monitor statistics - Regular <code>odibi catalog stats</code> reveals performance trends</li> <li>Review schema changes - Track breaking changes before they impact downstream</li> <li>Query lineage - Understand impact before modifying source tables</li> <li>Run optimization - Periodically run <code>catalog.optimize()</code> for Spark deployments</li> </ol>"},{"location":"features/catalog/#related","title":"Related","text":"<ul> <li>Pipeline Configuration - YAML schema reference</li> <li>Incremental Processing - HWM-based incremental loads</li> <li>Alerting - Notifications for pipeline events</li> </ul>"},{"location":"features/cli/","title":"Command-Line Interface","text":"<p>The Odibi CLI provides a comprehensive set of commands for running pipelines, managing configurations, exploring lineage, and querying the System Catalog.</p>"},{"location":"features/cli/#overview","title":"Overview","text":"<p>The Odibi CLI is your primary tool for: - Pipeline execution: Run, validate, and monitor data pipelines - Configuration management: Validate and scaffold YAML configs - Catalog queries: Explore runs, nodes, and execution metadata - Lineage exploration: Trace upstream/downstream dependencies - Schema tracking: View schema history and compare versions</p>"},{"location":"features/cli/#commands","title":"Commands","text":""},{"location":"features/cli/#odibi-run","title":"odibi run","text":"<p>Execute a pipeline from a YAML configuration file.</p> <pre><code>odibi run config.yaml\n</code></pre>"},{"location":"features/cli/#options","title":"Options","text":"Flag Description <code>--env</code> Environment to use (default: <code>development</code>) <code>--dry-run</code> Simulate execution without writing data <code>--resume</code> Resume from last failure (skip successful nodes) <code>--parallel</code> Run independent nodes in parallel <code>--workers</code> Number of worker threads for parallel execution (default: 4) <code>--on-error</code> Override error handling: <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code>"},{"location":"features/cli/#examples","title":"Examples","text":"<pre><code># Basic execution\nodibi run my_pipeline.yaml\n\n# Production run with parallel execution\nodibi run my_pipeline.yaml --env production --parallel --workers 8\n\n# Test without writing data\nodibi run my_pipeline.yaml --dry-run\n\n# Resume a failed run\nodibi run my_pipeline.yaml --resume\n</code></pre>"},{"location":"features/cli/#odibi-validate","title":"odibi validate","text":"<p>Validate a YAML configuration file for syntax and logical errors.</p> <pre><code>odibi validate config.yaml\n</code></pre> <p>Validation checks include: - YAML syntax - Required fields - Connection references - Transform function existence - Node dependency cycles</p>"},{"location":"features/cli/#example-output","title":"Example Output","text":"<pre><code>[OK] Config is valid\n</code></pre> <p>Or with errors:</p> <pre><code>[!] Pipeline 'process_orders' Errors:\n  - Node 'transform_orders' references unknown connection: missing_db\n  - Circular dependency detected: nodeA -&gt; nodeB -&gt; nodeA\n\n[X] Validation failed\n</code></pre>"},{"location":"features/cli/#odibi-catalog","title":"odibi catalog","text":"<p>Query the System Catalog for execution metadata, registered pipelines, and statistics.</p> <pre><code>odibi catalog &lt;command&gt; config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands","title":"Subcommands","text":"Subcommand Description <code>runs</code> List execution runs from <code>meta_runs</code> <code>pipelines</code> List registered pipelines from <code>meta_pipelines</code> <code>nodes</code> List registered nodes from <code>meta_nodes</code> <code>state</code> List HWM state checkpoints from <code>meta_state</code> <code>tables</code> List registered assets from <code>meta_tables</code> <code>metrics</code> List metrics definitions from <code>meta_metrics</code> <code>patterns</code> List pattern compliance from <code>meta_patterns</code> <code>stats</code> Show execution statistics"},{"location":"features/cli/#common-options","title":"Common Options","text":"Flag Description <code>--format</code>, <code>-f</code> Output format: <code>table</code> (default) or <code>json</code> <code>--pipeline</code>, <code>-p</code> Filter by pipeline name <code>--days</code>, <code>-d</code> Show data from last N days (default: 7) <code>--limit</code>, <code>-l</code> Maximum number of results (default: 20) <code>--status</code>, <code>-s</code> Filter by status: <code>SUCCESS</code>, <code>FAILED</code>, <code>RUNNING</code>"},{"location":"features/cli/#examples_1","title":"Examples","text":"<pre><code># List recent runs\nodibi catalog runs config.yaml\n\n# Filter runs by pipeline and status\nodibi catalog runs config.yaml --pipeline my_etl --status FAILED --days 14\n\n# List all registered pipelines\nodibi catalog pipelines config.yaml\n\n# View nodes for a specific pipeline\nodibi catalog nodes config.yaml --pipeline silver_pipeline\n\n# Check HWM state checkpoints\nodibi catalog state config.yaml\n\n# Get execution statistics\nodibi catalog stats config.yaml --days 30\n\n# Output as JSON\nodibi catalog runs config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output_1","title":"Example Output","text":"<pre><code>run_id     | pipeline_name | node_name      | status  | rows  | duration_ms | timestamp\n-----------+---------------+----------------+---------+-------+-------------+---------------------\nabc123     | bronze_etl    | ingest_orders  | SUCCESS | 15420 | 3250        | 2024-01-30 10:15:00\ndef456     | bronze_etl    | ingest_custo...| SUCCESS | 8932  | 2100        | 2024-01-30 10:14:00\n\nShowing 2 runs from the last 7 days.\n</code></pre>"},{"location":"features/cli/#odibi-lineage","title":"odibi lineage","text":"<p>Explore cross-pipeline data lineage and perform impact analysis.</p> <pre><code>odibi lineage &lt;command&gt; &lt;table&gt; --config config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands_1","title":"Subcommands","text":"Subcommand Description <code>upstream</code> Trace upstream sources of a table <code>downstream</code> Trace downstream consumers of a table <code>impact</code> Impact analysis for schema changes"},{"location":"features/cli/#options_1","title":"Options","text":"Flag Description <code>--config</code> Path to YAML config file (required) <code>--depth</code> Maximum depth to traverse (default: 3) <code>--format</code> Output format: <code>tree</code> (default) or <code>json</code>"},{"location":"features/cli/#examples_2","title":"Examples","text":"<pre><code># Trace upstream sources\nodibi lineage upstream gold/customer_360 --config config.yaml\n\n# Trace downstream consumers\nodibi lineage downstream bronze/customers_raw --config config.yaml\n\n# Impact analysis\nodibi lineage impact bronze/customers_raw --config config.yaml --depth 5\n\n# Output as JSON\nodibi lineage upstream gold/customer_360 --config config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output-upstream","title":"Example Output (upstream)","text":"<pre><code>Upstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre>"},{"location":"features/cli/#example-output-impact","title":"Example Output (impact)","text":"<pre><code>\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 3 downstream table(s) in 2 pipeline(s)\n</code></pre>"},{"location":"features/cli/#odibi-schema","title":"odibi schema","text":"<p>Track schema version history and compare schema changes over time.</p> <pre><code>odibi schema &lt;command&gt; &lt;table&gt; --config config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands_2","title":"Subcommands","text":"Subcommand Description <code>history</code> Show schema version history for a table <code>diff</code> Compare two schema versions"},{"location":"features/cli/#options_2","title":"Options","text":"Flag Description <code>--config</code> Path to YAML config file (required) <code>--limit</code> Maximum versions to show (default: 10) <code>--format</code> Output format: <code>table</code> (default) or <code>json</code> <code>--from-version</code> Source version number (for diff) <code>--to-version</code> Target version number (for diff)"},{"location":"features/cli/#examples_3","title":"Examples","text":"<pre><code># View schema history\nodibi schema history silver/customers --config config.yaml\n\n# Compare specific versions\nodibi schema diff silver/customers --config config.yaml --from-version 3 --to-version 5\n\n# Output history as JSON\nodibi schema history silver/customers --config config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output-history","title":"Example Output (history)","text":"<pre><code>Schema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre>"},{"location":"features/cli/#example-output-diff","title":"Example Output (diff)","text":"<pre><code>Schema Diff: silver/customers\nFrom v3 \u2192 v5\n============================================================\n+ loyalty_tier                   STRING               (added in v5)\n~ email                          VARCHAR \u2192 STRING\n- legacy_id                      INTEGER              (removed in v5)\n  customer_id                    INTEGER              (unchanged)\n  name                           STRING               (unchanged)\n</code></pre>"},{"location":"features/cli/#global-options","title":"Global Options","text":"<p>These options are available for all commands:</p> Flag Description <code>--log-level</code> Set logging verbosity: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> (default: <code>INFO</code>) <pre><code># Enable debug logging\nodibi run config.yaml --log-level DEBUG\n</code></pre>"},{"location":"features/cli/#examples_4","title":"Examples","text":""},{"location":"features/cli/#complete-workflow","title":"Complete Workflow","text":"<pre><code># 1. Validate configuration\nodibi validate my_pipeline.yaml\n\n# 2. Dry run to test logic\nodibi run my_pipeline.yaml --dry-run\n\n# 3. Execute pipeline\nodibi run my_pipeline.yaml --env production --parallel\n\n# 4. Check execution results\nodibi catalog runs my_pipeline.yaml --days 1\n\n# 5. View statistics\nodibi catalog stats my_pipeline.yaml --pipeline bronze_etl\n</code></pre>"},{"location":"features/cli/#debugging-a-failed-pipeline","title":"Debugging a Failed Pipeline","text":"<pre><code># Check recent failures\nodibi catalog runs config.yaml --status FAILED --limit 10\n\n# Resume from failure\nodibi run config.yaml --resume\n\n# Enable verbose logging\nodibi run config.yaml --log-level DEBUG\n</code></pre>"},{"location":"features/cli/#schema-change-impact-assessment","title":"Schema Change Impact Assessment","text":"<pre><code># Check schema history before making changes\nodibi schema history bronze/customers_raw --config config.yaml\n\n# Assess downstream impact\nodibi lineage impact bronze/customers_raw --config config.yaml\n\n# After changes, verify schema was captured\nodibi schema history bronze/customers_raw --config config.yaml --limit 1\n</code></pre>"},{"location":"features/cli/#monitoring-pipeline-health","title":"Monitoring Pipeline Health","text":"<pre><code># Daily stats check\nodibi catalog stats config.yaml --days 7\n\n# Find problematic nodes\nodibi catalog runs config.yaml --status FAILED --days 30\n\n# Check state for incremental loads\nodibi catalog state config.yaml --pipeline my_incremental_etl\n</code></pre>"},{"location":"features/cli/#related","title":"Related","text":"<ul> <li>Quick Start Guide - Getting started with Odibi</li> <li>CLI Master Guide - Comprehensive CLI reference</li> <li>System Catalog - Catalog metadata details</li> <li>YAML Schema Reference - Configuration reference</li> </ul>"},{"location":"features/configuration/","title":"Configuration System","text":"<p>YAML-based configuration for defining projects, pipelines, and nodes with built-in validation, environment variable support, and environment-specific overrides.</p>"},{"location":"features/configuration/#overview","title":"Overview","text":"<p>Odibi's configuration system provides: - YAML-based: Human-readable, version-controllable configuration files - Pydantic validation: Type-safe configuration with helpful error messages - Environment variables: Secure secret injection with <code>${VAR}</code> syntax - Environment overrides: Dev/staging/prod configurations in a single file - Hierarchical structure: Project \u2192 Pipelines \u2192 Nodes</p>"},{"location":"features/configuration/#project-configuration","title":"Project Configuration","text":"<p><code>ProjectConfig</code> is the root configuration defining the entire Odibi project.</p>"},{"location":"features/configuration/#required-fields","title":"Required Fields","text":"Field Type Description <code>project</code> string Project name <code>connections</code> object Named connections (at least one required) <code>pipelines</code> list Pipeline definitions (at least one required) <code>story</code> object Story generation configuration <code>system</code> object System Catalog configuration"},{"location":"features/configuration/#optional-fields","title":"Optional Fields","text":"Field Type Default Description <code>engine</code> string <code>pandas</code> Execution engine: <code>spark</code>, <code>pandas</code> <code>version</code> string <code>1.0.0</code> Project version <code>description</code> string - Project description <code>owner</code> string - Project owner/contact <code>vars</code> object <code>{}</code> Global variables for substitution <code>retry</code> object See below Retry configuration <code>logging</code> object See below Logging configuration <code>alerts</code> list <code>[]</code> Alert configurations <code>performance</code> object See below Performance tuning <code>lineage</code> object - OpenLineage configuration <code>environments</code> object - Environment-specific overrides"},{"location":"features/configuration/#basic-example","title":"Basic Example","text":"<pre><code>project: \"Customer360\"\nengine: \"spark\"\nversion: \"1.0.0\"\n\nconnections:\n  bronze:\n    type: \"local\"\n    base_path: \"./data/bronze\"\n  silver:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"silver_db\"\n\nstory:\n  connection: \"bronze\"\n  path: \"stories/\"\n  retention_days: 30\n\nsystem:\n  connection: \"bronze\"\n  path: \"_odibi_system\"\n\npipelines:\n  - pipeline: \"customer_ingestion\"\n    nodes:\n      - name: \"load_customers\"\n        read: { connection: \"bronze\", format: \"csv\", path: \"customers.csv\" }\n        write: { connection: \"silver\", table: \"customers\" }\n</code></pre>"},{"location":"features/configuration/#retry-configuration","title":"Retry Configuration","text":"<pre><code>retry:\n  enabled: true\n  max_attempts: 3        # 1-10\n  backoff: \"exponential\" # exponential, linear, constant\n</code></pre>"},{"location":"features/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>logging:\n  level: \"INFO\"          # DEBUG, INFO, WARNING, ERROR\n  structured: true       # JSON logs for Splunk/Datadog\n  metadata:\n    team: \"data-platform\"\n</code></pre>"},{"location":"features/configuration/#performance-configuration","title":"Performance Configuration","text":"<pre><code>performance:\n  use_arrow: true  # Use Apache Arrow-backed DataFrames (Pandas only)\n</code></pre>"},{"location":"features/configuration/#story-configuration","title":"Story Configuration","text":"<pre><code>story:\n  connection: \"bronze\"        # Must exist in connections\n  path: \"stories/\"            # Path relative to connection\n  max_sample_rows: 10         # 0-100\n  auto_generate: true\n  retention_days: 30          # Days to keep stories\n  retention_count: 100        # Max stories to keep\n</code></pre>"},{"location":"features/configuration/#system-configuration","title":"System Configuration","text":"<pre><code>system:\n  connection: \"bronze\"        # Must exist in connections\n  path: \"_odibi_system\"       # Path relative to connection root\n</code></pre>"},{"location":"features/configuration/#lineage-configuration","title":"Lineage Configuration","text":"<pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n  api_key: \"${LINEAGE_API_KEY}\"\n</code></pre>"},{"location":"features/configuration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p><code>PipelineConfig</code> groups related nodes into a logical unit.</p> Field Type Required Description <code>pipeline</code> string Yes Pipeline name <code>description</code> string No Pipeline description <code>layer</code> string No Logical layer: <code>bronze</code>, <code>silver</code>, <code>gold</code> <code>nodes</code> list Yes List of nodes (unique names required) <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"load_users\"\n        # ...\n      - name: \"clean_users\"\n        depends_on: [\"load_users\"]\n        # ...\n</code></pre>"},{"location":"features/configuration/#node-configuration","title":"Node Configuration","text":"<p><code>NodeConfig</code> defines individual data processing steps.</p>"},{"location":"features/configuration/#core-fields","title":"Core Fields","text":"Field Type Required Description <code>name</code> string Yes Unique node name <code>description</code> string No Human-readable description <code>enabled</code> bool No If <code>false</code>, node is skipped (default: <code>true</code>) <code>tags</code> list No Tags for selective execution (<code>odibi run --tag daily</code>) <code>depends_on</code> list No Parent nodes that must complete first"},{"location":"features/configuration/#operations-at-least-one-required","title":"Operations (at least one required)","text":"Field Type Description <code>read</code> object Input operation (load data) <code>transformer</code> string Built-in transformation app (e.g., <code>deduplicate</code>, <code>scd2</code>) <code>params</code> object Parameters for transformer <code>transform</code> object Chain of transformation steps <code>write</code> object Output operation (save data)"},{"location":"features/configuration/#execution-order","title":"Execution Order","text":"<ol> <li>Read (or dependency injection if no read block)</li> <li>Transformer (the \"App\" logic)</li> <li>Transform Steps (the \"Script\" logic)</li> <li>Validation</li> <li>Write</li> </ol>"},{"location":"features/configuration/#read-configuration","title":"Read Configuration","text":"<pre><code>read:\n  connection: \"bronze\"\n  format: \"parquet\"           # csv, parquet, delta, json, sql\n  path: \"customers/\"\n  # OR for SQL\n  query: \"SELECT * FROM customers WHERE active = 1\"\n\n  # Incremental loading\n  incremental:\n    mode: \"rolling_window\"    # or \"stateful\"\n    column: \"updated_at\"\n    lookback: 3\n    unit: \"day\"\n\n  # Time travel (Delta)\n  time_travel:\n    as_of_version: 10\n    # OR as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre>"},{"location":"features/configuration/#transform-configuration","title":"Transform Configuration","text":"<pre><code>transform:\n  steps:\n    # SQL step\n    - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n    # Function step\n    - function: \"clean_text\"\n      params:\n        columns: [\"email\"]\n        case: \"lower\"\n\n    # Operation step\n    - operation: \"detect_deletes\"\n      params:\n        mode: \"sql_compare\"\n        keys: [\"customer_id\"]\n</code></pre>"},{"location":"features/configuration/#write-configuration","title":"Write Configuration","text":"<pre><code>write:\n  connection: \"silver\"\n  format: \"delta\"\n  table: \"customers\"\n  mode: \"upsert\"              # overwrite, append, upsert, append_once\n\n  # Metadata columns\n  add_metadata: true          # or selective: {extracted_at: true, source_file: false}\n</code></pre>"},{"location":"features/configuration/#validation-configuration","title":"Validation Configuration","text":"<pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id, email]\n      on_fail: quarantine     # fail, warn, quarantine\n\n    - type: unique\n      columns: [customer_id]\n\n    - type: accepted_values\n      column: status\n      values: [\"active\", \"inactive\", \"pending\"]\n\n    - type: custom_sql\n      sql: \"COUNT(*) FILTER (WHERE age &lt; 0) = 0\"\n      message: \"Negative ages found\"\n\n  quarantine:\n    connection: \"silver\"\n    path: \"quarantine/customers\"\n\n  gate:\n    require_pass_rate: 0.95   # Block if &lt; 95% pass\n</code></pre>"},{"location":"features/configuration/#contracts-pre-conditions","title":"Contracts (Pre-conditions)","text":"<pre><code>contracts:\n  - type: row_count\n    min: 1000\n    on_fail: fail\n\n  - type: freshness\n    column: \"updated_at\"\n    max_age_hours: 24\n\n  - type: schema\n    columns:\n      id: \"int\"\n      name: \"string\"\n</code></pre>"},{"location":"features/configuration/#privacy-configuration","title":"Privacy Configuration","text":"<pre><code>privacy:\n  enabled: true\n  rules:\n    - column: \"email\"\n      method: \"hash\"          # hash, mask, redact, fake\n    - column: \"ssn\"\n      method: \"mask\"\n      params:\n        pattern: \"XXX-XX-####\"\n</code></pre>"},{"location":"features/configuration/#error-handling","title":"Error Handling","text":"<pre><code>on_error: \"fail_later\"        # fail_fast, fail_later, ignore\n</code></pre> Strategy Description <code>fail_fast</code> Stop pipeline immediately on error <code>fail_later</code> Continue pipeline, skip dependents (default) <code>ignore</code> Treat as success with warning, dependents run"},{"location":"features/configuration/#complete-node-example","title":"Complete Node Example","text":"<pre><code>- name: \"process_orders\"\n  description: \"Clean and deduplicate orders\"\n  tags: [\"daily\", \"critical\"]\n  depends_on: [\"load_orders\"]\n\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"order_id\"]\n    order_by: \"updated_at DESC\"\n\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status != 'cancelled'\"\n\n  validation:\n    tests:\n      - type: not_null\n        columns: [order_id, customer_id]\n        on_fail: quarantine\n    quarantine:\n      connection: \"silver\"\n      path: \"quarantine/orders\"\n    gate:\n      require_pass_rate: 0.98\n\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"orders_clean\"\n    mode: \"upsert\"\n\n  on_error: \"fail_fast\"\n  cache: true\n  log_level: \"DEBUG\"\n</code></pre>"},{"location":"features/configuration/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR_NAME}</code> syntax to inject environment variables:</p> <pre><code>connections:\n  azure_blob:\n    type: \"azure_blob\"\n    account_name: \"myaccount\"\n    container: \"data\"\n    auth:\n      mode: \"account_key\"\n      account_key: \"${AZURE_STORAGE_KEY}\"\n\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n</code></pre> <p>Variables are resolved at configuration load time. Missing variables raise an error.</p>"},{"location":"features/configuration/#global-variables","title":"Global Variables","text":"<p>Define reusable variables in <code>vars</code>:</p> <pre><code>vars:\n  env: \"production\"\n  team: \"data-platform\"\n\nlogging:\n  metadata:\n    environment: \"${vars.env}\"\n    team: \"${vars.team}\"\n</code></pre>"},{"location":"features/configuration/#environment-overrides","title":"Environment Overrides","text":"<p>Define environment-specific configurations that override base settings:</p> <pre><code>project: \"Customer360\"\nengine: \"pandas\"\n\nconnections:\n  database:\n    type: \"sql_server\"\n    host: \"dev-server.database.windows.net\"\n    database: \"dev_db\"\n\nenvironments:\n  staging:\n    connections:\n      database:\n        host: \"staging-server.database.windows.net\"\n        database: \"staging_db\"\n\n  production:\n    engine: \"spark\"\n    connections:\n      database:\n        host: \"prod-server.database.windows.net\"\n        database: \"prod_db\"\n    logging:\n      level: \"WARNING\"\n      structured: true\n</code></pre> <p>Select environment at runtime:</p> <pre><code>odibi run --env production\n</code></pre>"},{"location":"features/configuration/#validation","title":"Validation","text":"<p>Odibi uses Pydantic for configuration validation, providing:</p>"},{"location":"features/configuration/#type-checking","title":"Type Checking","text":"<pre><code># This will fail: max_attempts must be integer 1-10\nretry:\n  max_attempts: 100  # Error: ensure this value is less than or equal to 10\n</code></pre>"},{"location":"features/configuration/#required-field-validation","title":"Required Field Validation","text":"<pre><code># This will fail: 'project' is required\nengine: \"spark\"\npipelines: []\n# Error: field required - project\n</code></pre>"},{"location":"features/configuration/#cross-field-validation","title":"Cross-Field Validation","text":"<pre><code># This will fail: story.connection must exist in connections\nconnections:\n  bronze:\n    type: \"local\"\n    base_path: \"./data\"\n\nstory:\n  connection: \"silver\"  # Error: Story connection 'silver' not found\n  path: \"stories/\"\n</code></pre>"},{"location":"features/configuration/#node-validation","title":"Node Validation","text":"<pre><code># This will fail: node must have at least one operation\n- name: \"empty_node\"\n  # Error: Node 'empty_node' must have at least one of: read, transform, write, transformer\n</code></pre>"},{"location":"features/configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from odibi.config import load_config_from_file, ProjectConfig\n\n# From file (with env var substitution)\nconfig = load_config_from_file(\"odibi.yaml\")\n\n# From dict (programmatic)\nconfig = ProjectConfig(\n    project=\"MyProject\",\n    connections={\"local\": {\"type\": \"local\", \"base_path\": \"./data\"}},\n    pipelines=[...],\n    story={\"connection\": \"local\", \"path\": \"stories/\"},\n    system={\"connection\": \"local\"},\n)\n</code></pre>"},{"location":"features/configuration/#complete-example","title":"Complete Example","text":"<pre><code>project: \"E-Commerce Analytics\"\nversion: \"2.0.0\"\nengine: \"spark\"\nowner: \"data-team@company.com\"\n\nvars:\n  env: \"production\"\n\n# Resilience\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n\n# Observability\nlogging:\n  level: \"INFO\"\n  structured: true\n  metadata:\n    environment: \"${vars.env}\"\n\n# Alerting\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n\n# Performance\nperformance:\n  use_arrow: true\n\n# Lineage\nlineage:\n  url: \"http://marquez:5000\"\n  namespace: \"ecommerce\"\n\n# Connections\nconnections:\n  landing:\n    type: \"azure_blob\"\n    account_name: \"datalake\"\n    container: \"landing\"\n    auth:\n      mode: \"aad_msi\"\n\n  bronze:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"bronze\"\n\n  silver:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"silver\"\n\n  gold:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"gold\"\n\n# Story output\nstory:\n  connection: \"bronze\"\n  path: \"_stories/\"\n  max_sample_rows: 10\n  retention_days: 30\n\n# System catalog\nsystem:\n  connection: \"bronze\"\n  path: \"_odibi_system\"\n\n# Pipelines\npipelines:\n  - pipeline: \"orders_bronze\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: \"landing\"\n          format: \"json\"\n          path: \"orders/*.json\"\n          incremental:\n            mode: \"stateful\"\n            column: \"order_date\"\n        write:\n          connection: \"bronze\"\n          table: \"raw_orders\"\n          mode: \"append\"\n          add_metadata: true\n\n  - pipeline: \"orders_silver\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_orders\"\n        depends_on: [\"ingest_orders\"]\n\n        transformer: \"deduplicate\"\n        params:\n          keys: [\"order_id\"]\n          order_by: \"updated_at DESC\"\n\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_total &gt; 0\"\n            - function: \"clean_text\"\n              params:\n                columns: [\"customer_email\"]\n                case: \"lower\"\n\n        validation:\n          tests:\n            - type: not_null\n              columns: [order_id, customer_id]\n              on_fail: quarantine\n            - type: range\n              column: \"order_total\"\n              min: 0\n          quarantine:\n            connection: \"silver\"\n            path: \"quarantine/orders\"\n          gate:\n            require_pass_rate: 0.95\n\n        write:\n          connection: \"silver\"\n          table: \"orders\"\n          mode: \"upsert\"\n\n# Environment overrides\nenvironments:\n  dev:\n    engine: \"pandas\"\n    logging:\n      level: \"DEBUG\"\n    connections:\n      landing:\n        type: \"local\"\n        base_path: \"./test_data/landing\"\n      bronze:\n        type: \"local\"\n        base_path: \"./test_data/bronze\"\n</code></pre>"},{"location":"features/configuration/#related","title":"Related","text":"<ul> <li>YAML Schema Reference - Complete field reference</li> <li>Alerting - Alert configuration details</li> <li>Quality Gates - Validation and gates</li> <li>Quarantine Tables - Quarantine configuration</li> </ul>"},{"location":"features/connections/","title":"Connections","text":"<p>Unified connection system for accessing local filesystems, cloud storage, databases, and HTTP endpoints with pluggable authentication.</p>"},{"location":"features/connections/#overview","title":"Overview","text":"<p>Odibi's connection system provides: - Multiple backends: Local filesystem, Azure ADLS, Azure SQL, HTTP APIs - Flexible authentication: Service principals, managed identity, Key Vault, connection strings - Environment variables: Secure secret injection via <code>${VAR}</code> syntax - Plugin architecture: Register custom connection types via factory pattern</p>"},{"location":"features/connections/#built-in-connection-types","title":"Built-in Connection Types","text":"Type Description <code>local</code> Local filesystem or URI-based paths <code>local_dbfs</code> Databricks File System mock for local development <code>azure_adls</code> Azure Data Lake Storage Gen2 <code>azure_sql</code> Azure SQL Database <code>http</code> HTTP/REST API endpoints <code>delta</code> Delta Lake tables (path-based or catalog)"},{"location":"features/connections/#configuration","title":"Configuration","text":""},{"location":"features/connections/#basic-structure","title":"Basic Structure","text":"<pre><code>connections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n\n  silver:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    path_prefix: silver\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-key\n</code></pre>"},{"location":"features/connections/#connection-config-options","title":"Connection Config Options","text":"Field Type Required Description <code>type</code> string Yes Connection type (see table above) <code>auth</code> object No Authentication configuration <code>auth_mode</code> string No Authentication mode (auto-detected if omitted) <code>validation_mode</code> string No <code>eager</code> or <code>lazy</code> validation (default: <code>lazy</code>)"},{"location":"features/connections/#local-connection","title":"Local Connection","text":"<p>Simple filesystem connection for local development or mounted volumes.</p> <pre><code>connections:\n  raw_data:\n    type: local\n    base_path: ./data/raw\n\n  mounted_volume:\n    type: local\n    base_path: /mnt/storage/data\n</code></pre>"},{"location":"features/connections/#uri-based-paths","title":"URI-Based Paths","text":"<p>Supports URI schemes like <code>file://</code> or <code>dbfs:/</code>:</p> <pre><code>connections:\n  dbfs_data:\n    type: local\n    base_path: dbfs:/FileStore/data\n</code></pre>"},{"location":"features/connections/#config-options","title":"Config Options","text":"Field Type Default Description <code>base_path</code> string <code>./data</code> Base directory for all paths"},{"location":"features/connections/#local-dbfs-connection","title":"Local DBFS Connection","text":"<p>Mock DBFS for testing Databricks pipelines locally.</p> <pre><code>connections:\n  dbfs:\n    type: local_dbfs\n    root: .dbfs\n</code></pre> <p>Maps <code>dbfs:/FileStore/data.csv</code> to <code>.dbfs/FileStore/data.csv</code>.</p>"},{"location":"features/connections/#config-options_1","title":"Config Options","text":"Field Type Default Description <code>root</code> string <code>.dbfs</code> Local directory to use as DBFS root"},{"location":"features/connections/#azure-data-lake-storage-adls-connection","title":"Azure Data Lake Storage (ADLS) Connection","text":"<p>Azure Data Lake Storage Gen2 with multi-mode authentication.</p> <pre><code>connections:\n  datalake:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: datalake\n    path_prefix: bronze\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n</code></pre>"},{"location":"features/connections/#config-options_2","title":"Config Options","text":"Field Type Required Description <code>account_name</code> string Yes Storage account name <code>container</code> string Yes Container/filesystem name <code>path_prefix</code> string No Optional prefix for all paths <code>auth_mode</code> string No Authentication mode (auto-detected)"},{"location":"features/connections/#authentication-modes","title":"Authentication Modes","text":""},{"location":"features/connections/#key-vault-recommended","title":"Key Vault (Recommended)","text":"<p>Retrieves storage account key from Azure Key Vault:</p> <pre><code>connections:\n  secure_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n</code></pre>"},{"location":"features/connections/#service-principal","title":"Service Principal","text":"<p>OAuth authentication with Azure AD service principal:</p> <pre><code>connections:\n  sp_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: service_principal\n    auth:\n      tenant_id: ${AZURE_TENANT_ID}\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n</code></pre>"},{"location":"features/connections/#managed-identity","title":"Managed Identity","text":"<p>Use Azure Managed Identity (recommended for Azure-hosted workloads):</p> <pre><code>connections:\n  msi_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: managed_identity\n</code></pre>"},{"location":"features/connections/#sas-token","title":"SAS Token","text":"<p>Shared Access Signature for time-limited access:</p> <pre><code>connections:\n  sas_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: sas_token\n    auth:\n      sas_token: ${STORAGE_SAS_TOKEN}\n</code></pre>"},{"location":"features/connections/#direct-key-development-only","title":"Direct Key (Development Only)","text":"<p>\u26a0\ufe0f Not recommended for production</p> <pre><code>connections:\n  dev_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: direct_key\n    auth:\n      account_key: ${STORAGE_ACCOUNT_KEY}\n</code></pre>"},{"location":"features/connections/#path-resolution","title":"Path Resolution","text":"<p>ADLS connections generate <code>abfss://</code> URIs:</p> <pre><code>conn.get_path(\"folder/file.parquet\")\n# Returns: abfss://data@mystorageaccount.dfs.core.windows.net/bronze/folder/file.parquet\n</code></pre>"},{"location":"features/connections/#azure-sql-connection","title":"Azure SQL Connection","text":"<p>Azure SQL Database with SQL auth, Managed Identity, or Key Vault.</p> <pre><code>connections:\n  warehouse:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: analytics\n    auth_mode: aad_msi\n</code></pre>"},{"location":"features/connections/#config-options_3","title":"Config Options","text":"Field Type Default Description <code>host</code> / <code>server</code> string Required SQL Server hostname <code>database</code> string Required Database name <code>driver</code> string <code>ODBC Driver 18 for SQL Server</code> ODBC driver <code>port</code> int <code>1433</code> SQL Server port <code>timeout</code> int <code>30</code> Connection timeout (seconds) <code>auth_mode</code> string Auto <code>sql</code>, <code>aad_msi</code>, <code>key_vault</code>"},{"location":"features/connections/#authentication-modes_1","title":"Authentication Modes","text":""},{"location":"features/connections/#sql-authentication","title":"SQL Authentication","text":"<pre><code>connections:\n  sql_auth:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: sql\n    auth:\n      username: ${SQL_USERNAME}\n      password: ${SQL_PASSWORD}\n</code></pre>"},{"location":"features/connections/#managed-identity_1","title":"Managed Identity","text":"<pre><code>connections:\n  msi_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: aad_msi\n</code></pre>"},{"location":"features/connections/#key-vault","title":"Key Vault","text":"<pre><code>connections:\n  keyvault_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: key_vault\n    auth:\n      username: sqladmin\n      key_vault_name: my-keyvault\n      secret_name: sql-password\n</code></pre>"},{"location":"features/connections/#usage","title":"Usage","text":"<pre><code>from odibi.connections.azure_sql import AzureSQL\n\nconn = AzureSQL(\n    server=\"myserver.database.windows.net\",\n    database=\"analytics\",\n    auth_mode=\"aad_msi\",\n)\n\n# Read data\ndf = conn.read_sql(\"SELECT * FROM customers WHERE region = 'US'\")\n\n# Read entire table\ndf = conn.read_table(\"orders\", schema=\"dbo\")\n\n# Write data\nconn.write_table(df, \"processed_orders\", if_exists=\"replace\")\n\n# Execute statements\nconn.execute(\"DELETE FROM staging WHERE processed = 1\")\n</code></pre>"},{"location":"features/connections/#http-connection","title":"HTTP Connection","text":"<p>Connect to REST APIs with various authentication methods.</p> <pre><code>connections:\n  api:\n    type: http\n    base_url: https://api.example.com/v1/\n    auth:\n      token: ${API_TOKEN}\n</code></pre>"},{"location":"features/connections/#config-options_4","title":"Config Options","text":"Field Type Required Description <code>base_url</code> string Yes Base URL for API <code>headers</code> object No Default request headers <code>auth</code> object No Authentication configuration"},{"location":"features/connections/#authentication-methods","title":"Authentication Methods","text":""},{"location":"features/connections/#bearer-token","title":"Bearer Token","text":"<pre><code>connections:\n  bearer_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      token: ${API_BEARER_TOKEN}\n</code></pre>"},{"location":"features/connections/#basic-auth","title":"Basic Auth","text":"<pre><code>connections:\n  basic_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      username: ${API_USER}\n      password: ${API_PASSWORD}\n</code></pre>"},{"location":"features/connections/#api-key","title":"API Key","text":"<pre><code>connections:\n  apikey_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      api_key: ${API_KEY}\n      header_name: X-API-Key  # Optional, defaults to X-API-Key\n</code></pre>"},{"location":"features/connections/#custom-headers","title":"Custom Headers","text":"<pre><code>connections:\n  custom_api:\n    type: http\n    base_url: https://api.example.com/\n    headers:\n      Content-Type: application/json\n      X-Custom-Header: custom-value\n    auth:\n      token: ${API_TOKEN}\n</code></pre>"},{"location":"features/connections/#delta-connection","title":"Delta Connection","text":"<p>Delta Lake tables via path or Unity Catalog.</p>"},{"location":"features/connections/#path-based-delta","title":"Path-Based Delta","text":"<pre><code>connections:\n  delta_lake:\n    type: delta\n    path: /mnt/delta/tables\n</code></pre>"},{"location":"features/connections/#catalog-based-delta-spark","title":"Catalog-Based Delta (Spark)","text":"<pre><code>connections:\n  unity_catalog:\n    type: delta\n    catalog: main\n    schema: analytics\n</code></pre>"},{"location":"features/connections/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR}</code> syntax to inject secrets from environment variables:</p> <pre><code>connections:\n  secure:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: data\n    auth:\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n      tenant_id: ${AZURE_TENANT_ID}\n</code></pre> <p>Environment variables are resolved at runtime, keeping secrets out of configuration files.</p>"},{"location":"features/connections/#connection-factory","title":"Connection Factory","text":"<p>Odibi uses a plugin system for connection types. Built-in types are registered automatically.</p>"},{"location":"features/connections/#registering-custom-connections","title":"Registering Custom Connections","text":"<pre><code>from odibi.plugins import register_connection_factory\nfrom odibi.connections.base import BaseConnection\n\nclass MyCustomConnection(BaseConnection):\n    def __init__(self, endpoint: str, api_key: str):\n        self.endpoint = endpoint\n        self.api_key = api_key\n\n    def get_path(self, relative_path: str) -&gt; str:\n        return f\"{self.endpoint}/{relative_path}\"\n\n    def validate(self) -&gt; None:\n        if not self.endpoint:\n            raise ValueError(\"Endpoint is required\")\n\ndef create_custom_connection(name: str, config: dict):\n    return MyCustomConnection(\n        endpoint=config[\"endpoint\"],\n        api_key=config.get(\"api_key\", \"\"),\n    )\n\n# Register the factory\nregister_connection_factory(\"my_custom\", create_custom_connection)\n</code></pre> <p>Then use in YAML:</p> <pre><code>connections:\n  custom:\n    type: my_custom\n    endpoint: https://custom-service.example.com\n    api_key: ${CUSTOM_API_KEY}\n</code></pre>"},{"location":"features/connections/#built-in-factory-registration","title":"Built-in Factory Registration","text":"<p>Built-in connections are registered via <code>register_builtins()</code>:</p> Factory Name Connection Class <code>local</code> <code>LocalConnection</code> <code>http</code> <code>HttpConnection</code> <code>azure_blob</code> <code>AzureADLS</code> <code>azure_adls</code> <code>AzureADLS</code> <code>delta</code> <code>LocalConnection</code> or <code>DeltaCatalogConnection</code> <code>sql_server</code> <code>AzureSQL</code> <code>azure_sql</code> <code>AzureSQL</code>"},{"location":"features/connections/#complete-examples","title":"Complete Examples","text":""},{"location":"features/connections/#multi-environment-setup","title":"Multi-Environment Setup","text":"<pre><code>project: DataPipeline\nengine: spark\n\nconnections:\n  # Local development\n  local_bronze:\n    type: local\n    base_path: ./data/bronze\n\n  local_silver:\n    type: local\n    base_path: ./data/silver\n\n  # Azure production\n  azure_bronze:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    path_prefix: bronze\n    auth_mode: managed_identity\n\n  azure_silver:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    path_prefix: silver\n    auth_mode: managed_identity\n\n  # SQL database\n  warehouse:\n    type: azure_sql\n    host: ${SQL_SERVER}\n    database: analytics\n    auth_mode: aad_msi\n\n  # External API\n  weather_api:\n    type: http\n    base_url: https://api.weather.com/v1/\n    auth:\n      api_key: ${WEATHER_API_KEY}\n\npipelines:\n  - pipeline: ingest_orders\n    nodes:\n      - name: read_orders\n        source:\n          connection: azure_bronze\n          path: orders/\n        # ...\n</code></pre>"},{"location":"features/connections/#service-principal-authentication","title":"Service Principal Authentication","text":"<pre><code>connections:\n  adls_sp:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    path_prefix: ingestion\n    auth_mode: service_principal\n    auth:\n      tenant_id: ${AZURE_TENANT_ID}\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n\n  sql_sp:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: warehouse\n    auth_mode: sql\n    auth:\n      username: ${SQL_USER}\n      password: ${SQL_PASSWORD}\n</code></pre>"},{"location":"features/connections/#key-vault-integration","title":"Key Vault Integration","text":"<pre><code>connections:\n  secure_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: sensitive-data\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n\n  secure_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: secure_db\n    auth_mode: key_vault\n    auth:\n      username: sqladmin\n      key_vault_name: my-keyvault\n      secret_name: sql-admin-password\n</code></pre>"},{"location":"features/connections/#best-practices","title":"Best Practices","text":"<ol> <li>Use Managed Identity - Preferred for Azure-hosted workloads (no secrets to manage)</li> <li>Use Key Vault - Store secrets in Key Vault, not config files</li> <li>Environment variables - Use <code>${VAR}</code> for any sensitive values</li> <li>Lazy validation - Default <code>validation_mode: lazy</code> defers validation until first use</li> <li>Separate connections - Use different connections for different security zones</li> <li>Register secrets - Secrets are automatically registered for log redaction</li> </ol>"},{"location":"features/connections/#related","title":"Related","text":"<ul> <li>YAML Schema Reference</li> <li>Pipeline Configuration</li> <li>Security Best Practices</li> </ul>"},{"location":"features/cross-pipeline-dependencies/","title":"Cross-Pipeline Dependencies","text":"<p>Last Updated: 2025-12-03 Status: \u2705 Implemented</p>"},{"location":"features/cross-pipeline-dependencies/#overview","title":"Overview","text":"<p>Cross-pipeline dependencies enable pipelines to reference outputs from other pipelines using the <code>$pipeline.node</code> syntax. This is essential for implementing the medallion architecture pattern where silver nodes depend on bronze outputs, and gold nodes depend on silver outputs.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Bronze    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Silver    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    Gold     \u2502\n\u2502  Pipeline   \u2502     \u2502  Pipeline   \u2502     \u2502  Pipeline   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n   writes to          reads from          reads from\n   meta_outputs       $read_bronze.*      $transform_silver.*\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#use-cases","title":"Use Cases","text":""},{"location":"features/cross-pipeline-dependencies/#1-medallion-architecture-bronze-silver-gold","title":"1. Medallion Architecture (Bronze \u2192 Silver \u2192 Gold)","text":"<p>The most common pattern: ingest raw data in bronze, clean/enrich in silver, aggregate for business in gold.</p> <pre><code># Bronze: Raw ingestion\npipeline: read_bronze\nnodes:\n  - name: raw_orders\n    read:\n      connection: source_db\n      table: sales.orders\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"bronze/orders\"\n\n# Silver: Enriched data\npipeline: transform_silver\nnodes:\n  - name: enriched_orders\n    inputs:\n      orders: $read_bronze.raw_orders        # \u2190 Cross-pipeline reference\n      customers: $read_bronze.raw_customers\n    transform:\n      steps:\n        - operation: join\n          left: orders\n          right: customers\n          on: [customer_id]\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"silver/enriched_orders\"\n\n# Gold: Business aggregates\npipeline: build_gold\nnodes:\n  - name: daily_sales\n    inputs:\n      orders: $transform_silver.enriched_orders\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              DATE(order_date) as date,\n              SUM(amount) as total_sales\n            FROM orders\n            GROUP BY 1\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"gold/daily_sales\"\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#2-multi-source-joins","title":"2. Multi-Source Joins","text":"<p>When a node needs to join data from multiple sources:</p> <pre><code>- name: enriched_downtime\n  inputs:\n    events: $read_bronze.shift_events\n    calendar: $read_bronze.calendar_dim\n    plant: $read_bronze.plant_dim\n  transform:\n    steps:\n      - operation: join\n        left: events\n        right: calendar\n        on: [date_id]\n      - operation: join\n        right: plant\n        on: [plant_id]\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#3-mixing-references-with-explicit-reads","title":"3. Mixing References with Explicit Reads","text":"<p>You can combine cross-pipeline references with explicit read configs:</p> <pre><code>- name: combined_data\n  inputs:\n    # Cross-pipeline reference\n    events: $read_bronze.events\n\n    # Explicit read (for data not from another pipeline)\n    reference_data:\n      connection: static_files\n      path: \"reference/lookup_table.csv\"\n      format: csv\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#yaml-syntax","title":"YAML Syntax","text":""},{"location":"features/cross-pipeline-dependencies/#the-inputs-block","title":"The <code>inputs</code> Block","text":"<pre><code>nodes:\n  - name: node_name\n    inputs:\n      &lt;input_name&gt;: $&lt;pipeline_name&gt;.&lt;node_name&gt;    # Cross-pipeline reference\n      &lt;input_name&gt;:                                  # Explicit read config\n        connection: &lt;connection_name&gt;\n        path: &lt;path&gt;\n        format: &lt;format&gt;\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#reference-syntax-pipelinenode","title":"Reference Syntax: <code>$pipeline.node</code>","text":"Component Description <code>$</code> Prefix indicating a cross-pipeline reference <code>pipeline</code> Name of the source pipeline (from <code>pipeline:</code> field) <code>.</code> Separator <code>node</code> Name of the source node (from <code>name:</code> field) <p>Examples: - <code>$read_bronze.orders</code> \u2192 Output from node <code>orders</code> in pipeline <code>read_bronze</code> - <code>$ingest_daily.customers</code> \u2192 Output from node <code>customers</code> in pipeline <code>ingest_daily</code></p>"},{"location":"features/cross-pipeline-dependencies/#how-the-meta_outputs-catalog-table-works","title":"How the <code>meta_outputs</code> Catalog Table Works","text":"<p>When a node with a <code>write</code> block completes, its output metadata is recorded in the system catalog.</p>"},{"location":"features/cross-pipeline-dependencies/#schema","title":"Schema","text":"Column Type Description <code>pipeline_name</code> STRING Pipeline identifier <code>node_name</code> STRING Node identifier <code>output_type</code> STRING <code>\"external_table\"</code> or <code>\"managed_table\"</code> <code>connection_name</code> STRING Connection used (for external tables) <code>path</code> STRING Storage path <code>format</code> STRING Data format (delta, parquet, etc.) <code>table_name</code> STRING Registered table name (if any) <code>last_run</code> TIMESTAMP Last execution time <code>row_count</code> LONG Row count at last write <code>updated_at</code> TIMESTAMP Record update time"},{"location":"features/cross-pipeline-dependencies/#resolution-flow","title":"Resolution Flow","text":"<pre><code>1. Silver node has: inputs: {events: $read_bronze.shift_events}\n\n2. At load time, Odibi queries meta_outputs:\n   SELECT * FROM meta_outputs\n   WHERE pipeline_name = 'read_bronze' AND node_name = 'shift_events'\n\n3. Returns: {connection: 'goat_prod', path: 'bronze/OEE/shift_events', format: 'delta'}\n\n4. At runtime: engine.read(connection='goat_prod', path='bronze/OEE/shift_events', format='delta')\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#performance-notes","title":"Performance Notes","text":""},{"location":"features/cross-pipeline-dependencies/#batch-writes-only","title":"Batch Writes Only","text":"<p>Output metadata is collected in-memory during pipeline execution and written to the catalog once at pipeline completion. This avoids per-node I/O overhead.</p> <p>Before optimization: 17 nodes \u00d7 ~2-3s = ~40s overhead After optimization: Single batch MERGE = ~2s total</p>"},{"location":"features/cross-pipeline-dependencies/#caching","title":"Caching","text":"<p>The <code>get_node_output()</code> method uses caching to avoid repeated catalog queries within the same session.</p>"},{"location":"features/cross-pipeline-dependencies/#validate-early","title":"Validate Early","text":"<p>All <code>$references</code> are validated at pipeline load time (fail fast), not at execution time. This provides immediate feedback if a referenced pipeline hasn't run.</p>"},{"location":"features/cross-pipeline-dependencies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/cross-pipeline-dependencies/#error-no-output-found-for-pipelinenode","title":"Error: \"No output found for $pipeline.node\"","text":"<pre><code>ReferenceResolutionError: No output found for $read_bronze.shift_events.\nEnsure pipeline 'read_bronze' has run and node 'shift_events' has a write block.\n</code></pre> <p>Causes: 1. The referenced pipeline hasn't been run yet 2. The referenced node doesn't have a <code>write</code> block 3. Typo in pipeline or node name</p> <p>Solutions: 1. Run the source pipeline first: <code>odibi run bronze.yaml</code> 2. Add a <code>write</code> block to the source node 3. Check spelling matches exactly (case-sensitive)</p>"},{"location":"features/cross-pipeline-dependencies/#error-cannot-have-both-read-and-inputs","title":"Error: \"Cannot have both 'read' and 'inputs'\"","text":"<pre><code>ValidationError: Node 'my_node': Cannot have both 'read' and 'inputs'.\nUse 'read' for single-source nodes or 'inputs' for multi-source cross-pipeline dependencies.\n</code></pre> <p>Solution: Choose one approach: - Use <code>read</code> for simple single-source reads - Use <code>inputs</code> for multi-source or cross-pipeline reads</p>"},{"location":"features/cross-pipeline-dependencies/#error-invalid-reference-format","title":"Error: \"Invalid reference format\"","text":"<pre><code>ValueError: Invalid reference format: $read_bronze. Expected $pipeline.node\n</code></pre> <p>Solution: Ensure the reference includes both pipeline and node names separated by a dot.</p>"},{"location":"features/cross-pipeline-dependencies/#engine-compatibility","title":"Engine Compatibility","text":"Feature Spark Pandas Polars <code>meta_outputs</code> writes \u2705 \u2705 \u2705 <code>$pipeline.node</code> (path-based) \u2705 \u2705 \u2705 <code>$pipeline.node</code> (managed table) \u2705 \u274c \u274c <code>inputs:</code> block \u2705 \u2705 \u2705 <p>Best Practice: Always use <code>path:</code> in write config for cross-engine compatibility.</p>"},{"location":"features/cross-pipeline-dependencies/#files-changed-in-implementation","title":"Files Changed in Implementation","text":"File Changes <code>odibi/catalog.py</code> Added <code>meta_outputs</code> table, <code>register_outputs_batch()</code>, <code>get_node_output()</code> <code>odibi/config.py</code> Added <code>inputs</code> field to <code>NodeConfig</code> <code>odibi/node.py</code> Added <code>_execute_inputs_phase()</code>, <code>_create_output_record()</code> <code>odibi/pipeline.py</code> Added batch output registration at pipeline end <code>odibi/references.py</code> New module for reference resolution <code>tests/unit/test_cross_pipeline_dependencies.py</code> 26 new tests"},{"location":"features/cross-pipeline-dependencies/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Reference - NodeConfig with <code>inputs</code> field</li> <li>YAML Schema Reference - Full schema documentation</li> <li>Catalog Feature - System catalog details</li> <li>Pipelines - Pipeline execution flow</li> </ul>"},{"location":"features/diagnostics/","title":"Diagnostics","text":"<p>Tools for debugging, monitoring, and comparing pipeline runs with Delta Lake version analysis and data diff capabilities.</p>"},{"location":"features/diagnostics/#overview","title":"Overview","text":"<p>Odibi's diagnostics module provides: - Delta Diagnostics: Table history, version comparison, metrics extraction - Data Diff: Row-level comparison, schema comparison, change detection - Run Comparison: Compare pipeline executions to identify drift - History Management: Access and analyze historical pipeline runs</p>"},{"location":"features/diagnostics/#delta-diagnostics","title":"Delta Diagnostics","text":""},{"location":"features/diagnostics/#table-version-comparison","title":"Table Version Comparison","text":"<p>Compare two versions of a Delta table to understand what changed:</p> <pre><code>from odibi.diagnostics import get_delta_diff\n\n# Basic comparison (metadata only)\ndiff = get_delta_diff(\n    table_path=\"/path/to/delta/table\",\n    version_a=5,\n    version_b=10,\n    spark=spark,  # Optional: use Spark or deltalake (Pandas)\n)\n\nprint(f\"Rows changed: {diff.rows_change}\")\nprint(f\"Files changed: {diff.files_change}\")\nprint(f\"Size change: {diff.size_change_bytes} bytes\")\nprint(f\"Operations: {diff.operations}\")\n</code></pre>"},{"location":"features/diagnostics/#deltadiffresult-fields","title":"DeltaDiffResult Fields","text":"Field Type Description <code>table_path</code> str Path to the Delta table <code>version_a</code> int Start version <code>version_b</code> int End version <code>rows_change</code> int Net row count change <code>files_change</code> int Net file count change <code>size_change_bytes</code> int Net size change in bytes <code>schema_added</code> List[str] Columns added between versions <code>schema_removed</code> List[str] Columns removed between versions <code>schema_current</code> List[str] Current schema columns <code>schema_previous</code> List[str] Previous schema columns <code>operations</code> List[str] Operations that occurred between versions"},{"location":"features/diagnostics/#deep-diff-mode","title":"Deep Diff Mode","text":"<p>Enable row-level comparison for detailed analysis:</p> <pre><code># Deep comparison with key-based diff\ndiff = get_delta_diff(\n    table_path=\"/path/to/delta/table\",\n    version_a=5,\n    version_b=10,\n    spark=spark,\n    deep=True,\n    keys=[\"order_id\"],  # Primary key columns for update detection\n)\n\nprint(f\"Rows added: {diff.rows_added}\")\nprint(f\"Rows removed: {diff.rows_removed}\")\nprint(f\"Rows updated: {diff.rows_updated}\")\n\n# Sample data\nprint(\"Added rows:\", diff.sample_added[:5])\nprint(\"Removed rows:\", diff.sample_removed[:5])\nprint(\"Updated rows:\", diff.sample_updated[:5])\n</code></pre>"},{"location":"features/diagnostics/#drift-detection","title":"Drift Detection","text":"<p>Automatically detect significant changes between versions:</p> <pre><code>from odibi.diagnostics import detect_drift\n\nwarning = detect_drift(\n    table_path=\"/path/to/delta/table\",\n    current_version=10,\n    baseline_version=5,\n    spark=spark,\n    threshold_pct=10.0,  # Alert if &gt;10% row count change\n)\n\nif warning:\n    print(f\"Drift detected: {warning}\")\n</code></pre> <p>Drift detection checks for: - Schema drift: Columns added or removed - Data volume drift: Row count changes exceeding threshold</p>"},{"location":"features/diagnostics/#data-diff","title":"Data Diff","text":""},{"location":"features/diagnostics/#node-comparison","title":"Node Comparison","text":"<p>Compare two executions of the same node:</p> <pre><code>from odibi.diagnostics import diff_nodes\n\ndiff = diff_nodes(node_a, node_b)\n\nprint(f\"Status change: {diff.status_change}\")\nprint(f\"Rows diff: {diff.rows_diff}\")\nprint(f\"Schema changed: {diff.schema_change}\")\nprint(f\"SQL changed: {diff.sql_changed}\")\nprint(f\"Has drift: {diff.has_drift}\")\n</code></pre>"},{"location":"features/diagnostics/#nodediffresult-fields","title":"NodeDiffResult Fields","text":"Field Type Description <code>node_name</code> str Name of the node <code>status_change</code> str Status change (e.g., \"success -&gt; failed\") <code>rows_out_a</code> int Output rows from run A <code>rows_out_b</code> int Output rows from run B <code>rows_diff</code> int Row count difference (B - A) <code>schema_change</code> bool Whether schema changed <code>columns_added</code> List[str] Columns added in run B <code>columns_removed</code> List[str] Columns removed in run B <code>sql_changed</code> bool Whether SQL logic changed <code>config_changed</code> bool Whether configuration changed <code>transformation_changed</code> bool Whether transformation stack changed <code>delta_version_change</code> str Delta version change (e.g., \"v1 -&gt; v2\") <code>has_drift</code> bool True if any significant drift occurred"},{"location":"features/diagnostics/#run-comparison","title":"Run Comparison","text":"<p>Compare two complete pipeline runs:</p> <pre><code>from odibi.diagnostics import diff_runs\n\nrun_diff = diff_runs(run_a, run_b)\n\nprint(f\"Nodes added: {run_diff.nodes_added}\")\nprint(f\"Nodes removed: {run_diff.nodes_removed}\")\nprint(f\"Drift sources: {run_diff.drift_source_nodes}\")\nprint(f\"Impacted downstream: {run_diff.impacted_downstream_nodes}\")\n\n# Examine individual node diffs\nfor name, node_diff in run_diff.node_diffs.items():\n    if node_diff.has_drift:\n        print(f\"  {name}: {node_diff.status_change or 'data drift'}\")\n</code></pre>"},{"location":"features/diagnostics/#rundiffresult-fields","title":"RunDiffResult Fields","text":"Field Type Description <code>run_id_a</code> str Run ID of baseline <code>run_id_b</code> str Run ID of current run <code>node_diffs</code> Dict[str, NodeDiffResult] Per-node comparison results <code>nodes_added</code> List[str] Nodes present in B but not A <code>nodes_removed</code> List[str] Nodes present in A but not B <code>drift_source_nodes</code> List[str] Nodes where logic changed <code>impacted_downstream_nodes</code> List[str] Nodes affected by upstream drift"},{"location":"features/diagnostics/#historymanager","title":"HistoryManager","text":"<p>Manage and access pipeline run history:</p> <pre><code>from odibi.diagnostics import HistoryManager\n\nmanager = HistoryManager(history_path=\"stories/\")\n\n# List available runs\nruns = manager.list_runs(\"process_orders\")\nfor run in runs:\n    print(f\"Run: {run['run_id']} at {run['timestamp']}\")\n\n# Get specific runs\nlatest = manager.get_latest_run(\"process_orders\")\nspecific = manager.get_run_by_id(\"process_orders\", \"20240130_101500\")\nprevious = manager.get_previous_run(\"process_orders\", \"20240130_101500\")\n</code></pre>"},{"location":"features/diagnostics/#historymanager-methods","title":"HistoryManager Methods","text":"Method Description <code>list_runs(pipeline_name)</code> List all runs for a pipeline (newest first) <code>get_latest_run(pipeline_name)</code> Get the most recent run metadata <code>get_run_by_id(pipeline_name, run_id)</code> Get specific run by ID <code>get_previous_run(pipeline_name, run_id)</code> Get the run immediately before specified run <code>load_run(path)</code> Load run metadata from JSON file"},{"location":"features/diagnostics/#examples","title":"Examples","text":""},{"location":"features/diagnostics/#debugging-a-failed-pipeline","title":"Debugging a Failed Pipeline","text":"<pre><code>from odibi.diagnostics import HistoryManager, diff_runs\n\nmanager = HistoryManager(\"stories/\")\n\n# Get the failed run and the last successful run\nfailed_run = manager.get_latest_run(\"process_orders\")\nprevious_run = manager.get_previous_run(\"process_orders\", failed_run.run_id)\n\nif previous_run:\n    diff = diff_runs(previous_run, failed_run)\n\n    # Find what changed\n    print(\"Changes that may have caused failure:\")\n    for node in diff.drift_source_nodes:\n        node_diff = diff.node_diffs[node]\n        if node_diff.sql_changed:\n            print(f\"  - {node}: SQL logic changed\")\n        if node_diff.config_changed:\n            print(f\"  - {node}: Configuration changed\")\n</code></pre>"},{"location":"features/diagnostics/#monitoring-data-quality-over-time","title":"Monitoring Data Quality Over Time","text":"<pre><code>from odibi.diagnostics import get_delta_diff, detect_drift\n\n# Check for unexpected changes after a pipeline run\ntable_path = \"/delta/silver/orders\"\n\n# Compare with yesterday's version\ndrift_warning = detect_drift(\n    table_path=table_path,\n    current_version=100,\n    baseline_version=95,\n    spark=spark,\n    threshold_pct=5.0,  # Alert on &gt;5% change\n)\n\nif drift_warning:\n    # Get detailed diff\n    diff = get_delta_diff(\n        table_path=table_path,\n        version_a=95,\n        version_b=100,\n        spark=spark,\n        deep=True,\n    )\n\n    print(f\"Warning: {drift_warning}\")\n    print(f\"Details: +{diff.rows_added} / -{diff.rows_removed} rows\")\n\n    if diff.schema_added:\n        print(f\"New columns: {diff.schema_added}\")\n</code></pre>"},{"location":"features/diagnostics/#comparing-spark-vs-pandas-execution","title":"Comparing Spark vs Pandas Execution","text":"<p>The diagnostics module supports both Spark and Pandas (via <code>deltalake</code> library):</p> <pre><code>from odibi.diagnostics import get_delta_diff\n\n# With Spark (for large tables)\ndiff_spark = get_delta_diff(\n    table_path=\"/delta/table\",\n    version_a=1,\n    version_b=5,\n    spark=spark,\n    deep=True,\n)\n\n# With Pandas/deltalake (for local development)\ndiff_pandas = get_delta_diff(\n    table_path=\"/delta/table\",\n    version_a=1,\n    version_b=5,\n    spark=None,  # Uses deltalake library\n    deep=True,\n)\n</code></pre>"},{"location":"features/diagnostics/#tracking-schema-evolution","title":"Tracking Schema Evolution","text":"<pre><code>from odibi.diagnostics import get_delta_diff\n\ndiff = get_delta_diff(\n    table_path=\"/delta/silver/customers\",\n    version_a=0,  # Initial version\n    version_b=50,  # Current version\n    spark=spark,\n)\n\nprint(\"Schema Evolution:\")\nprint(f\"  Initial columns: {diff.schema_previous}\")\nprint(f\"  Current columns: {diff.schema_current}\")\nprint(f\"  Added over time: {diff.schema_added}\")\nprint(f\"  Removed over time: {diff.schema_removed}\")\n</code></pre>"},{"location":"features/diagnostics/#best-practices","title":"Best Practices","text":"<ol> <li>Use deep mode sparingly - Deep diff is expensive; use metadata-only diffs for routine monitoring</li> <li>Define primary keys - Key-based diff enables update detection, not just add/remove</li> <li>Set appropriate thresholds - Tune drift detection thresholds based on expected data patterns</li> <li>Store history - Enable story persistence to enable run comparisons over time</li> <li>Automate drift checks - Integrate drift detection into pipeline post-run hooks</li> </ol>"},{"location":"features/diagnostics/#related","title":"Related","text":"<ul> <li>Stories - Pipeline execution history</li> <li>Schema Tracking - Schema change monitoring</li> <li>Quality Gates - Data quality validation</li> <li>Lineage - Data lineage tracking</li> </ul>"},{"location":"features/engines/","title":"Execution Engines","text":"<p>Multi-engine architecture for flexible data processing across local development, high-performance workloads, and big data environments.</p>"},{"location":"features/engines/#overview","title":"Overview","text":"<p>Odibi's engine system provides: - Multiple backends: Pandas, Spark, Polars - Unified API: Consistent interface across engines - Automatic selection: Choose based on workload and environment - Performance tuning: Engine-specific optimizations</p>"},{"location":"features/engines/#supported-engines","title":"Supported Engines","text":"Engine Best For Dependencies <code>pandas</code> Local development, small datasets (&lt;1GB) <code>pip install odibi</code> <code>spark</code> Big data, Databricks, distributed processing <code>pip install odibi[spark]</code> <code>polars</code> High-performance local processing, medium datasets <code>pip install polars</code>"},{"location":"features/engines/#pandasengine","title":"PandasEngine","text":"<p>Default engine for local development with broad format support.</p> <p>Strengths: - Extensive format support (CSV, Parquet, JSON, Excel, Avro, Delta) - Rich ecosystem integration - Familiar API for data scientists - SQL support via DuckDB (optional)</p> <p>Best for: - Local development and testing - Small to medium datasets (&lt;1GB) - Complex transformations with pandas operations</p>"},{"location":"features/engines/#sparkengine","title":"SparkEngine","text":"<p>Distributed processing engine for big data workloads.</p> <p>Strengths: - Horizontal scalability - Native Databricks integration - Delta Lake support with ACID transactions - Streaming pipelines - Multi-account ADLS support</p> <p>Best for: - Large datasets (&gt;1GB) - Production Databricks workflows - Distributed processing - Real-time streaming</p>"},{"location":"features/engines/#polarsengine","title":"PolarsEngine","text":"<p>High-performance engine with lazy evaluation.</p> <p>Strengths: - Extremely fast (Rust-based) - Memory efficient with lazy execution - Multi-threaded by default - Native scan operations (scan_csv, scan_parquet)</p> <p>Best for: - High-performance local processing - Medium to large datasets (1GB-10GB) - CPU-bound transformations</p>"},{"location":"features/engines/#configuration","title":"Configuration","text":""},{"location":"features/engines/#basic-engine-setup","title":"Basic Engine Setup","text":"<pre><code>project: DataPipeline\nengine: pandas  # or spark, polars\n\nconnections:\n  # ...\n\npipelines:\n  # ...\n</code></pre>"},{"location":"features/engines/#engine-options","title":"Engine Options","text":"Field Type Description <code>engine</code> string Engine type: <code>pandas</code>, <code>spark</code>, <code>polars</code> <code>performance</code> object Performance tuning options"},{"location":"features/engines/#engine-selection-guide","title":"Engine Selection Guide","text":"Scenario Recommended Engine Local development <code>pandas</code> Unit testing <code>pandas</code> Databricks production <code>spark</code> Large datasets (&gt;1GB) <code>spark</code> or <code>polars</code> CPU-bound local processing <code>polars</code> Streaming pipelines <code>spark</code> Quick prototyping <code>pandas</code>"},{"location":"features/engines/#engine-api","title":"Engine API","text":"<p>All engines implement the same core interface defined in <code>Engine</code> base class.</p>"},{"location":"features/engines/#core-methods","title":"Core Methods","text":"Method Description <code>read()</code> Read data from source <code>write()</code> Write data to destination <code>execute_sql()</code> Execute SQL query <code>execute_operation()</code> Execute built-in operation (pivot, sort, etc.) <code>get_schema()</code> Get DataFrame schema <code>get_shape()</code> Get DataFrame dimensions <code>count_rows()</code> Count rows in DataFrame <code>count_nulls()</code> Count nulls in specified columns"},{"location":"features/engines/#data-operations","title":"Data Operations","text":"Method Description <code>validate_schema()</code> Validate DataFrame schema against rules <code>validate_data()</code> Validate data against validation config <code>get_sample()</code> Get sample rows as dictionaries <code>profile_nulls()</code> Calculate null percentage per column <code>harmonize_schema()</code> Match DataFrame to target schema <code>anonymize()</code> Anonymize columns (hash, mask, redact)"},{"location":"features/engines/#table-operations","title":"Table Operations","text":"Method Description <code>table_exists()</code> Check if table/path exists <code>get_table_schema()</code> Get schema of existing table <code>maintain_table()</code> Run maintenance (optimize, vacuum) <code>materialize()</code> Materialize lazy dataset into memory"},{"location":"features/engines/#custom-format-support","title":"Custom Format Support","text":"<pre><code>from odibi.engine import PandasEngine\n\ndef read_netcdf(path, **options):\n    import xarray as xr\n    return xr.open_dataset(path).to_dataframe()\n\ndef write_netcdf(df, path, **options):\n    import xarray as xr\n    xr.Dataset.from_dataframe(df).to_netcdf(path)\n\nPandasEngine.register_format(\"netcdf\", reader=read_netcdf, writer=write_netcdf)\n</code></pre>"},{"location":"features/engines/#performance-configuration","title":"Performance Configuration","text":""},{"location":"features/engines/#pandas-performance","title":"Pandas Performance","text":"<pre><code>engine: pandas\nperformance:\n  use_arrow: true    # Use PyArrow backend (faster, less memory)\n  use_duckdb: false  # Use DuckDB for SQL (experimental)\n</code></pre> <p>Arrow backend benefits: - Faster I/O for Parquet files - Reduced memory usage - Better type preservation</p>"},{"location":"features/engines/#spark-performance","title":"Spark Performance","text":"<pre><code>engine: spark\n</code></pre> <p>Spark is automatically configured with: - Arrow-based PySpark conversions - Adaptive Query Execution (AQE) - Dynamic partition overwrite mode</p> <p>Additional optimizations via write options:</p> <pre><code>pipelines:\n  - pipeline: optimize_example\n    nodes:\n      - name: write_optimized\n        write:\n          connection: silver\n          format: delta\n          path: optimized_table\n          options:\n            optimize_write: true\n            zorder_by: [customer_id, date]\n</code></pre>"},{"location":"features/engines/#polars-performance","title":"Polars Performance","text":"<pre><code>engine: polars\n</code></pre> <p>Polars features: - Lazy evaluation by default (scan operations) - Automatic query optimization - Multi-threaded execution - Streaming writes (sink operations)</p>"},{"location":"features/engines/#examples","title":"Examples","text":""},{"location":"features/engines/#switching-engines","title":"Switching Engines","text":"<p>Same pipeline, different engines:</p> <pre><code># Local development\nproject: DataPipeline\nengine: pandas\n\nconnections:\n  bronze:\n    type: local\n    path: ./data/bronze\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          format: csv\n          path: orders.csv\n</code></pre> <pre><code># Production (Databricks)\nproject: DataPipeline\nengine: spark\n\nconnections:\n  bronze:\n    type: azure_adls\n    storage_account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          format: delta\n          path: orders\n</code></pre>"},{"location":"features/engines/#using-different-connections","title":"Using Different Connections","text":"<pre><code>project: MultiSource\nengine: spark\n\nconnections:\n  raw_data:\n    type: azure_adls\n    storage_account: rawstorage\n    container: raw\n\n  processed:\n    type: azure_adls\n    storage_account: procstorage\n    container: silver\n\n  sql_source:\n    type: azure_sql\n    server: myserver.database.windows.net\n    database: mydb\n\npipelines:\n  - pipeline: ingest_sql\n    nodes:\n      - name: read_sql\n        read:\n          connection: sql_source\n          format: sql\n          table: dbo.customers\n\n      - name: write_delta\n        write:\n          connection: processed\n          format: delta\n          path: customers\n</code></pre>"},{"location":"features/engines/#lazy-vs-eager-execution","title":"Lazy vs Eager Execution","text":"<pre><code># Polars with lazy execution (default)\nengine: polars\n\npipelines:\n  - pipeline: lazy_example\n    nodes:\n      - name: scan_data\n        read:\n          connection: bronze\n          format: parquet\n          path: large_dataset/*.parquet\n        # Returns LazyFrame - no data loaded yet\n\n      - name: filter_transform\n        sql: |\n          SELECT * FROM scan_data WHERE status = 'active'\n        # Still lazy - builds query plan\n\n      - name: write_result\n        write:\n          connection: silver\n          format: parquet\n          path: filtered_data\n        # Execution happens here (sink_parquet)\n</code></pre>"},{"location":"features/engines/#engine-specific-features","title":"Engine-Specific Features","text":"<p>Pandas with Delta Time Travel:</p> <pre><code>engine: pandas\n\npipelines:\n  - pipeline: time_travel\n    nodes:\n      - name: read_historical\n        read:\n          connection: bronze\n          format: delta\n          path: orders\n          options:\n            versionAsOf: 5  # Read version 5\n</code></pre> <p>Spark with Streaming:</p> <pre><code>engine: spark\n\npipelines:\n  - pipeline: streaming_ingest\n    nodes:\n      - name: stream_read\n        read:\n          connection: bronze\n          format: delta\n          path: events\n          streaming: true\n</code></pre>"},{"location":"features/engines/#programmatic-engine-usage","title":"Programmatic Engine Usage","text":"<pre><code>from odibi.engine import get_engine_class\n\n# Get engine by name\nEngineClass = get_engine_class(\"pandas\")\nengine = EngineClass(connections=my_connections)\n\n# Read data\ndf = engine.read(\n    connection=my_connection,\n    format=\"parquet\",\n    path=\"data/*.parquet\"\n)\n\n# Execute SQL\nfrom odibi.context import PandasContext\nctx = PandasContext()\nctx.register(\"orders\", df)\nresult = engine.execute_sql(\"SELECT * FROM orders WHERE total &gt; 100\", ctx)\n\n# Write data\nengine.write(\n    df=result,\n    connection=output_connection,\n    format=\"delta\",\n    path=\"filtered_orders\",\n    mode=\"overwrite\"\n)\n</code></pre>"},{"location":"features/engines/#register-custom-engine","title":"Register Custom Engine","text":"<pre><code>from odibi.engine import Engine, register_engine\n\nclass DuckDBEngine(Engine):\n    name = \"duckdb\"\n\n    def read(self, connection, format, **kwargs):\n        # Custom implementation\n        pass\n\n    # Implement other required methods...\n\nregister_engine(\"duckdb\", DuckDBEngine)\n</code></pre>"},{"location":"features/engines/#best-practices","title":"Best Practices","text":"<ol> <li>Match engine to workload - Use pandas for development, spark for production</li> <li>Use lazy execution - Polars and Spark defer computation until needed</li> <li>Enable Arrow - Faster I/O and reduced memory for Pandas</li> <li>Partition large tables - Use <code>partition_by</code> for write performance</li> <li>Run maintenance - Enable auto-optimize for Delta tables</li> <li>Test locally first - Develop with pandas, deploy with spark</li> </ol>"},{"location":"features/engines/#related","title":"Related","text":"<ul> <li>Connections - Data source configuration</li> <li>Pipelines - Pipeline definition</li> <li>YAML Schema Reference - Full configuration options</li> </ul>"},{"location":"features/lineage/","title":"Cross-Pipeline Lineage","text":"<p>Track table-level lineage relationships across pipelines for impact analysis and data governance.</p>"},{"location":"features/lineage/#overview","title":"Overview","text":"<p>Odibi tracks lineage at two levels: - OpenLineage integration: Standards-based lineage emission - Cross-pipeline lineage: Table-to-table relationships in the System Catalog</p> <p>This document covers the cross-pipeline lineage tracking stored in <code>meta_lineage</code>.</p>"},{"location":"features/lineage/#how-it-works","title":"How It Works","text":"<ol> <li>During pipeline execution, read/write operations are recorded</li> <li>Source \u2192 Target relationships are stored in <code>meta_lineage</code></li> <li>CLI commands query the lineage graph</li> <li>Impact analysis identifies affected downstream tables</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   bronze/   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   silver/   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   gold/     \u2502\n\u2502  customers  \u2502     \u2502 dim_customer\u2502     \u2502customer_360 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2502                   \u2502                   \u2502\n   Pipeline A          Pipeline B          Pipeline C\n</code></pre>"},{"location":"features/lineage/#cli-commands","title":"CLI Commands","text":""},{"location":"features/lineage/#trace-upstream-lineage","title":"Trace Upstream Lineage","text":"<p>Find all sources for a table:</p> <pre><code>odibi lineage upstream &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi lineage upstream gold/customer_360 --config pipeline.yaml\n\nUpstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre> <p>Options:</p> <pre><code>odibi lineage upstream gold/customer_360 --config config.yaml \\\n    --depth 5 \\         # Traverse up to 5 levels (default: 3)\n    --format json       # Output as JSON\n</code></pre>"},{"location":"features/lineage/#trace-downstream-lineage","title":"Trace Downstream Lineage","text":"<p>Find all consumers of a table:</p> <pre><code>odibi lineage downstream &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi lineage downstream bronze/customers_raw --config pipeline.yaml\n\nDownstream Lineage: bronze/customers_raw\n============================================================\nbronze/customers_raw\n\u251c\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n\u2502   \u251c\u2500\u2500 gold/customer_360 (gold_pipeline.build_360)\n\u2502   \u2514\u2500\u2500 gold/churn_features (ml_pipeline.build_features)\n\u2514\u2500\u2500 silver/customer_events (silver_pipeline.process_events)\n</code></pre>"},{"location":"features/lineage/#impact-analysis","title":"Impact Analysis","text":"<p>Assess the impact of changes to a table:</p> <pre><code>odibi lineage impact &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi lineage impact bronze/customers_raw --config pipeline.yaml\n\n\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n    - silver/customer_events (pipeline: silver_pipeline)\n\n  Summary:\n    Total: 4 downstream table(s) in 3 pipeline(s)\n</code></pre>"},{"location":"features/lineage/#programmatic-access","title":"Programmatic Access","text":""},{"location":"features/lineage/#using-lineagetracker","title":"Using LineageTracker","text":"<pre><code>from odibi.lineage import LineageTracker\nfrom odibi.catalog import CatalogManager\n\n# Initialize\ncatalog = CatalogManager(spark, config, base_path, engine)\ntracker = LineageTracker(catalog)\n\n# Record lineage manually\ntracker.record_lineage(\n    read_config=node.read,\n    write_config=node.write,\n    pipeline=\"my_pipeline\",\n    node=\"process_data\",\n    run_id=\"run-12345\",\n    connections=connections,\n)\n\n# Query upstream\nupstream = tracker.get_upstream(\"gold/customer_360\", depth=3)\nfor record in upstream:\n    print(f\"{record['source_table']} \u2192 {record['target_table']}\")\n\n# Query downstream\ndownstream = tracker.get_downstream(\"bronze/customers_raw\", depth=3)\n\n# Impact analysis\nimpact = tracker.get_impact_analysis(\"bronze/customers_raw\")\nprint(f\"Affected tables: {impact['affected_tables']}\")\nprint(f\"Affected pipelines: {impact['affected_pipelines']}\")\n</code></pre>"},{"location":"features/lineage/#direct-catalog-access","title":"Direct Catalog Access","text":"<pre><code># Record lineage directly\ncatalog.record_lineage(\n    source_table=\"bronze/customers_raw\",\n    target_table=\"silver/dim_customers\",\n    target_pipeline=\"silver_pipeline\",\n    target_node=\"process_customers\",\n    run_id=\"run-12345\",\n    relationship=\"feeds\",\n)\n\n# Query upstream\nupstream = catalog.get_upstream(\"gold/customer_360\", depth=3)\n\n# Query downstream\ndownstream = catalog.get_downstream(\"bronze/customers_raw\", depth=3)\n</code></pre>"},{"location":"features/lineage/#lineage-record-structure","title":"Lineage Record Structure","text":"<p>Each lineage record includes:</p> Field Description <code>source_table</code> Source table path <code>target_table</code> Target table path <code>source_pipeline</code> Pipeline reading from source <code>source_node</code> Node reading from source <code>target_pipeline</code> Pipeline writing to target <code>target_node</code> Node writing to target <code>relationship</code> Type: \"feeds\" or \"derived_from\" <code>last_observed</code> Last time this relationship was seen <code>run_id</code> Run ID when recorded"},{"location":"features/lineage/#automatic-tracking","title":"Automatic Tracking","text":"<p>Lineage is automatically tracked when: 1. A node has both <code>read</code> and <code>write</code> configurations 2. The System Catalog is configured 3. The pipeline runs successfully</p> <pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n      format: delta\n\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE active = true\"\n\n    write:\n      connection: silver\n      path: dim_customers\n      format: delta\n</code></pre> <p>This automatically records: <code>bronze/customers_raw \u2192 silver/dim_customers</code></p>"},{"location":"features/lineage/#dependency-based-lineage","title":"Dependency-Based Lineage","text":"<p>Lineage is also tracked for <code>depends_on</code> relationships:</p> <pre><code>nodes:\n  - name: source_node\n    read: { connection: bronze, path: raw_data }\n    write: { connection: silver, path: processed_data }\n\n  - name: consumer_node\n    depends_on: [source_node]  # Lineage tracked!\n    transform:\n      steps:\n        - sql: \"SELECT * FROM source_node\"\n    write: { connection: gold, path: final_data }\n</code></pre>"},{"location":"features/lineage/#storage-location","title":"Storage Location","text":"<p>Lineage is stored in the System Catalog:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n</code></pre> <p>Location: <code>{connection_base_path}/_odibi_system/meta_lineage/</code></p>"},{"location":"features/lineage/#example-pre-deployment-impact-check","title":"Example: Pre-Deployment Impact Check","text":"<p>Before deploying schema changes, check impact:</p> <pre><code>def pre_deployment_check(catalog, table_to_change):\n    \"\"\"Check impact before deploying changes.\"\"\"\n    downstream = catalog.get_downstream(table_to_change, depth=5)\n\n    if not downstream:\n        print(f\"\u2705 No downstream dependencies for {table_to_change}\")\n        return True\n\n    affected_tables = set()\n    affected_pipelines = set()\n\n    for record in downstream:\n        affected_tables.add(record['target_table'])\n        if record.get('target_pipeline'):\n            affected_pipelines.add(record['target_pipeline'])\n\n    print(f\"\u26a0\ufe0f  Changes to {table_to_change} will affect:\")\n    print(f\"   - {len(affected_tables)} tables\")\n    print(f\"   - {len(affected_pipelines)} pipelines\")\n\n    for table in sorted(affected_tables):\n        print(f\"     \u2022 {table}\")\n\n    return len(downstream) == 0\n</code></pre>"},{"location":"features/lineage/#integration-with-schema-tracking","title":"Integration with Schema Tracking","text":"<p>Combine lineage with schema tracking for comprehensive governance:</p> <pre><code>def assess_schema_change_impact(catalog, table_path):\n    \"\"\"Assess impact of recent schema changes.\"\"\"\n    # Get schema changes\n    history = catalog.get_schema_history(table_path, limit=2)\n    if len(history) &lt; 2:\n        return\n\n    latest = history[0]\n    removed = latest.get('columns_removed', [])\n\n    if removed:\n        # Check downstream impact\n        downstream = catalog.get_downstream(table_path)\n        print(f\"\u26a0\ufe0f  Columns {removed} were removed from {table_path}\")\n        print(f\"   This may break {len(downstream)} downstream tables\")\n</code></pre>"},{"location":"features/lineage/#best-practices","title":"Best Practices","text":"<ol> <li>Run impact analysis before changes - Know what you'll affect</li> <li>Use consistent table naming - Makes lineage easier to follow</li> <li>Document cross-pipeline boundaries - Clarify ownership</li> <li>Monitor lineage depth - Deep chains may indicate complexity</li> <li>Integrate with CI/CD - Block deployments with unknown impact</li> </ol>"},{"location":"features/lineage/#related","title":"Related","text":"<ul> <li>Schema Version Tracking - Track schema changes</li> <li>OpenLineage Integration - Standards-based lineage</li> </ul>"},{"location":"features/orchestration/","title":"Orchestration","text":"<p>Generate production-ready workflow definitions for Apache Airflow and Dagster from your Odibi pipelines.</p>"},{"location":"features/orchestration/#overview","title":"Overview","text":"<p>Odibi's orchestration module provides: - Airflow Integration: Generate DAG files with proper task dependencies - Dagster Integration: Create asset definitions with dependency graphs - Automatic Dependency Mapping: Node dependencies become task/asset dependencies - CLI Execution: Each node runs via <code>odibi run</code> for isolation</p>"},{"location":"features/orchestration/#airflow-integration","title":"Airflow Integration","text":""},{"location":"features/orchestration/#airflowexporter-class","title":"AirflowExporter Class","text":"<p>The <code>AirflowExporter</code> generates Airflow DAG Python files from Odibi pipeline configurations.</p> <pre><code>from odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\nconfig = load_config(\"odibi.yaml\")\nexporter = AirflowExporter(config)\n\n# Generate DAG code for a specific pipeline\ndag_code = exporter.generate_code(\"process_orders\")\n\n# Write to Airflow DAGs folder\nwith open(\"/airflow/dags/odibi_process_orders.py\", \"w\") as f:\n    f.write(dag_code)\n</code></pre>"},{"location":"features/orchestration/#generated-dag-structure","title":"Generated DAG Structure","text":"<p>The exporter creates a DAG with: - <code>BashOperator</code> tasks for each node - Proper upstream/downstream dependencies - Configurable retries from your Odibi config - Tags for filtering (<code>odibi</code>, layer name)</p> <pre><code># Generated DAG example\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\nwith DAG(\n    'odibi_process_orders',\n    default_args=default_args,\n    description='Process incoming orders',\n    schedule_interval=None,\n    catchup=False,\n    tags=['odibi', 'silver'],\n) as dag:\n\n    ingest_orders = BashOperator(\n        task_id='ingest_orders',\n        bash_command='odibi run --pipeline process_orders --node ingest_orders',\n    )\n\n    validate_orders = BashOperator(\n        task_id='validate_orders',\n        bash_command='odibi run --pipeline process_orders --node validate_orders',\n    )\n\n    # Dependencies\n    [ingest_orders] &gt;&gt; validate_orders\n</code></pre>"},{"location":"features/orchestration/#airflow-configuration-options","title":"Airflow Configuration Options","text":"Option Source Description <code>dag_id</code> Auto-generated <code>odibi_{pipeline_name}</code> <code>owner</code> <code>config.owner</code> DAG owner for Airflow UI <code>retries</code> <code>config.retry.max_attempts</code> Retry count (0 if disabled) <code>tags</code> <code>pipeline.layer</code> Includes <code>odibi</code> and layer name <code>description</code> <code>pipeline.description</code> Pipeline description"},{"location":"features/orchestration/#dagster-integration","title":"Dagster Integration","text":""},{"location":"features/orchestration/#dagsterfactory-class","title":"DagsterFactory Class","text":"<p>The <code>DagsterFactory</code> creates Dagster asset definitions directly from your Odibi configuration.</p> <pre><code># definitions.py\nfrom odibi.config import load_config\nfrom odibi.orchestration.dagster import DagsterFactory\n\nconfig = load_config(\"odibi.yaml\")\ndefs = DagsterFactory(config).create_definitions()\n</code></pre>"},{"location":"features/orchestration/#asset-features","title":"Asset Features","text":"<p>Each Odibi node becomes a Dagster asset with: - Dependency tracking: <code>depends_on</code> becomes asset dependencies - Grouping: Assets grouped by pipeline name - Compute kind: Tagged as <code>odibi</code> for UI identification - Op tags: Pipeline and node names for filtering</p>"},{"location":"features/orchestration/#generated-assets","title":"Generated Assets","text":"<pre><code># Dagster creates assets like:\n@asset(\n    name=\"validate_orders\",\n    deps=[\"ingest_orders\"],\n    group_name=\"process_orders\",\n    description=\"Validate order data quality\",\n    compute_kind=\"odibi\",\n    op_tags={\"odibi/pipeline\": \"process_orders\", \"odibi/node\": \"validate_orders\"},\n)\ndef validate_orders(context: AssetExecutionContext):\n    # Runs: odibi run --pipeline process_orders --node validate_orders\n    ...\n</code></pre>"},{"location":"features/orchestration/#dagster-installation","title":"Dagster Installation","text":"<p>Dagster is an optional dependency:</p> <pre><code>pip install dagster dagster-webserver\n</code></pre>"},{"location":"features/orchestration/#configuration","title":"Configuration","text":""},{"location":"features/orchestration/#project-configuration-for-orchestration","title":"Project Configuration for Orchestration","text":"<pre><code>project: DataPipeline\nowner: data-team      # Used as Airflow DAG owner\n\nretry:\n  enabled: true\n  max_attempts: 3      # Airflow retry count\n\npipelines:\n  - pipeline: process_orders\n    layer: silver      # Used as Airflow tag\n    description: \"Process incoming orders\"\n    nodes:\n      - name: ingest_orders\n        # ...\n\n      - name: validate_orders\n        depends_on:\n          - ingest_orders\n        # ...\n\n      - name: transform_orders\n        depends_on:\n          - validate_orders\n        # ...\n</code></pre>"},{"location":"features/orchestration/#dependency-mapping","title":"Dependency Mapping","text":"<p>Node dependencies in Odibi map directly to orchestrator dependencies:</p> Odibi Config Airflow Dagster <code>depends_on: [node_a]</code> <code>[node_a] &gt;&gt; node_b</code> <code>deps=[\"node_a\"]</code> <code>depends_on: [a, b]</code> <code>[a, b] &gt;&gt; node_c</code> <code>deps=[\"a\", \"b\"]</code> No dependencies First task No deps"},{"location":"features/orchestration/#examples","title":"Examples","text":""},{"location":"features/orchestration/#complete-airflow-integration","title":"Complete Airflow Integration","text":"<pre><code># scripts/generate_dags.py\nfrom pathlib import Path\nfrom odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\ndef generate_all_dags(config_path: str, output_dir: str):\n    config = load_config(config_path)\n    exporter = AirflowExporter(config)\n    output = Path(output_dir)\n\n    for pipeline in config.pipelines:\n        dag_code = exporter.generate_code(pipeline.pipeline)\n        dag_file = output / f\"odibi_{pipeline.pipeline}.py\"\n        dag_file.write_text(dag_code)\n        print(f\"Generated: {dag_file}\")\n\nif __name__ == \"__main__\":\n    generate_all_dags(\"odibi.yaml\", \"/opt/airflow/dags\")\n</code></pre>"},{"location":"features/orchestration/#complete-dagster-integration","title":"Complete Dagster Integration","text":"<pre><code># definitions.py\nfrom odibi.config import load_config\nfrom odibi.orchestration.dagster import DagsterFactory\n\n# Load Odibi configuration\nconfig = load_config(\"odibi.yaml\")\n\n# Create Dagster definitions\ndefs = DagsterFactory(config).create_definitions()\n\n# Run with: dagster dev -f definitions.py\n</code></pre>"},{"location":"features/orchestration/#multi-pipeline-setup","title":"Multi-Pipeline Setup","text":"<pre><code># odibi.yaml\nproject: DataWarehouse\nowner: platform-team\n\npipelines:\n  - pipeline: bronze_ingestion\n    layer: bronze\n    nodes:\n      - name: ingest_customers\n        source:\n          connection: raw_db\n          path: customers\n\n      - name: ingest_orders\n        source:\n          connection: raw_db\n          path: orders\n\n  - pipeline: silver_transformation\n    layer: silver\n    nodes:\n      - name: clean_customers\n        depends_on: []\n        source:\n          connection: bronze\n          path: customers\n\n      - name: clean_orders\n        depends_on: []\n        source:\n          connection: bronze\n          path: orders\n\n      - name: join_customer_orders\n        depends_on:\n          - clean_customers\n          - clean_orders\n</code></pre> <pre><code># Generate DAGs for all pipelines\nfrom odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\nconfig = load_config(\"odibi.yaml\")\nexporter = AirflowExporter(config)\n\n# Generates separate DAGs:\n# - odibi_bronze_ingestion\n# - odibi_silver_transformation\nfor pipeline in config.pipelines:\n    code = exporter.generate_code(pipeline.pipeline)\n    print(f\"--- {pipeline.pipeline} ---\")\n    print(code)\n</code></pre>"},{"location":"features/orchestration/#best-practices","title":"Best Practices","text":"<ol> <li>Use CLI execution - Both adapters use <code>odibi run</code> for process isolation</li> <li>Set owner - Configure <code>owner</code> in YAML for Airflow ownership</li> <li>Enable retries - Configure retry settings in Odibi config</li> <li>Layer tags - Use <code>layer</code> field for organizing DAGs in Airflow</li> <li>Generate on deploy - Regenerate DAG files during CI/CD deployment</li> </ol>"},{"location":"features/orchestration/#related","title":"Related","text":"<ul> <li>Pipeline Configuration - YAML schema reference</li> <li>CLI Reference - <code>odibi run</code> command details</li> <li>Retry Configuration - Retry settings used by orchestrators</li> </ul>"},{"location":"features/patterns/","title":"Loading Patterns","text":"<p>Pre-built execution patterns for common data warehouse loading scenarios including SCD2 and Merge operations.</p>"},{"location":"features/patterns/#overview","title":"Overview","text":"<p>Odibi's pattern system provides: - Declarative loading: Configure complex loading logic via YAML - Engine agnostic: Works with Spark and Pandas engines - Built-in validation: Patterns validate required parameters before execution - Extensible: Create custom patterns by extending the <code>Pattern</code> base class</p>"},{"location":"features/patterns/#available-patterns","title":"Available Patterns","text":"Pattern Description Use Case <code>scd2</code> Slowly Changing Dimension Type 2 Historical tracking of dimension changes <code>merge</code> Upsert/merge operations Incremental updates, CDC <p>Note: For simple append or overwrite operations, use <code>write.mode: append</code> or <code>write.mode: overwrite</code> directly\u2014no pattern needed.</p>"},{"location":"features/patterns/#configuration","title":"Configuration","text":"<p>Patterns are configured via the <code>transformer</code> field in node configuration:</p> <pre><code>nodes:\n  - name: load_customers\n    transformer: scd2\n    params:\n      target: \"gold/customers\"\n      keys: [\"customer_id\"]\n      track_cols: [\"address\", \"tier\"]\n      effective_time_col: \"updated_at\"\n</code></pre>"},{"location":"features/patterns/#config-options","title":"Config Options","text":"Field Type Required Description <code>transformer</code> string Yes Pattern name: <code>scd2</code>, <code>merge</code> <code>params</code> object Yes Pattern-specific parameters"},{"location":"features/patterns/#pattern-parameters","title":"Pattern Parameters","text":""},{"location":"features/patterns/#scd2-pattern","title":"SCD2 Pattern","text":"<p>Tracks history by creating new rows for updates. When a tracked column changes, the old record is closed and a new record is inserted.</p> Parameter Type Required Default Description <code>target</code> string Yes - Target table name or path containing history <code>keys</code> list Yes - Natural keys to identify unique entities <code>track_cols</code> list Yes - Columns to monitor for changes <code>effective_time_col</code> string Yes - Source column indicating when the change occurred <code>end_time_col</code> string No <code>valid_to</code> Name of the end timestamp column <code>current_flag_col</code> string No <code>is_current</code> Name of the current record flag column <code>delete_col</code> string No <code>null</code> Column indicating soft deletion (boolean)"},{"location":"features/patterns/#merge-pattern","title":"Merge Pattern","text":"<p>Upsert/merge logic with support for multiple strategies.</p> Parameter Type Required Default Description <code>target</code> string Yes - Target table name or path <code>keys</code> list Yes - Join keys for matching records <code>strategy</code> string No <code>upsert</code> <code>upsert</code>, <code>append_only</code>, <code>delete_match</code> <code>audit_cols</code> object No <code>null</code> <code>{created_col: \"...\", updated_col: \"...\"}</code> <code>update_condition</code> string No <code>null</code> SQL condition for update clause <code>insert_condition</code> string No <code>null</code> SQL condition for insert clause <code>delete_condition</code> string No <code>null</code> SQL condition for delete clause <code>optimize_write</code> bool No <code>false</code> Run OPTIMIZE after write (Spark only) <code>zorder_by</code> list No <code>null</code> Columns to Z-Order by <code>cluster_by</code> list No <code>null</code> Columns to Liquid Cluster by (Delta)"},{"location":"features/patterns/#merge-strategies","title":"Merge Strategies","text":"Strategy Description <code>upsert</code> Update existing records, insert new ones <code>append_only</code> Ignore duplicates, only insert new keys <code>delete_match</code> Delete records in target that match keys in source"},{"location":"features/patterns/#pattern-api","title":"Pattern API","text":"<p>All patterns extend the <code>Pattern</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\nfrom odibi.config import NodeConfig\nfrom odibi.context import EngineContext\nfrom odibi.engine.base import Engine\n\nclass Pattern(ABC):\n    \"\"\"Base class for Execution Patterns.\"\"\"\n\n    def __init__(self, engine: Engine, config: NodeConfig):\n        self.engine = engine\n        self.config = config\n        self.params = config.params\n\n    @abstractmethod\n    def execute(self, context: EngineContext) -&gt; Any:\n        \"\"\"\n        Execute the pattern logic.\n\n        Args:\n            context: EngineContext containing current DataFrame and helpers.\n\n        Returns:\n            The transformed DataFrame.\n        \"\"\"\n        pass\n\n    def validate(self) -&gt; None:\n        \"\"\"\n        Validate pattern configuration.\n        Raises ValueError if invalid.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"features/patterns/#creating-custom-patterns","title":"Creating Custom Patterns","text":"<ol> <li>Extend the <code>Pattern</code> base class</li> <li>Implement <code>execute()</code> method</li> <li>Optionally override <code>validate()</code> for parameter validation</li> </ol> <pre><code>from odibi.patterns.base import Pattern\nfrom odibi.context import EngineContext\n\nclass MyCustomPattern(Pattern):\n\n    def validate(self) -&gt; None:\n        if not self.params.get(\"required_param\"):\n            raise ValueError(\"MyCustomPattern: 'required_param' is required.\")\n\n    def execute(self, context: EngineContext):\n        df = context.df\n        # Custom transformation logic\n        return df\n</code></pre>"},{"location":"features/patterns/#examples","title":"Examples","text":""},{"location":"features/patterns/#scd2-customer-dimension-with-history","title":"SCD2: Customer Dimension with History","text":"<p>Track customer address and tier changes over time:</p> <pre><code>project: CustomerDW\nengine: spark\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/gold\n\npipelines:\n  - pipeline: load_customer_dim\n    nodes:\n      - name: customer_scd2\n        read:\n          connection: bronze\n          path: customers\n        transformer: scd2\n        params:\n          target: \"gold/customers\"\n          keys: [\"customer_id\"]\n          track_cols: [\"address\", \"city\", \"state\", \"tier\"]\n          effective_time_col: \"updated_at\"\n          end_time_col: \"valid_to\"\n          current_flag_col: \"is_active\"\n</code></pre>"},{"location":"features/patterns/#merge-incremental-customer-updates","title":"Merge: Incremental Customer Updates","text":"<p>Upsert with audit columns:</p> <pre><code>pipelines:\n  - pipeline: sync_customers\n    nodes:\n      - name: customers_merge\n        read:\n          connection: bronze\n          path: customer_updates\n        transformer: merge\n        params:\n          target: \"silver.customers\"\n          keys: [\"customer_id\"]\n          strategy: upsert\n          audit_cols:\n            created_col: \"dw_created_at\"\n            updated_col: \"dw_updated_at\"\n</code></pre>"},{"location":"features/patterns/#merge-gdpr-delete-request","title":"Merge: GDPR Delete Request","text":"<p>Delete records matching source keys:</p> <pre><code>pipelines:\n  - pipeline: gdpr_delete\n    nodes:\n      - name: delete_customers\n        read:\n          connection: compliance\n          path: deletion_requests\n        transformer: merge\n        params:\n          target: \"silver.customers\"\n          keys: [\"customer_id\"]\n          strategy: delete_match\n</code></pre>"},{"location":"features/patterns/#merge-conditional-update","title":"Merge: Conditional Update","text":"<p>Only update if source record is newer:</p> <pre><code>pipelines:\n  - pipeline: sync_products\n    nodes:\n      - name: products_merge\n        read:\n          connection: bronze\n          path: product_updates\n        transformer: merge\n        params:\n          target: \"silver.products\"\n          keys: [\"product_id\"]\n          strategy: upsert\n          update_condition: \"source.updated_at &gt; target.updated_at\"\n          insert_condition: \"source.is_deleted = false\"\n</code></pre>"},{"location":"features/patterns/#simple-append-no-pattern-needed","title":"Simple Append (No Pattern Needed)","text":"<p>For event/fact data, just use write mode:</p> <pre><code>pipelines:\n  - pipeline: load_events\n    nodes:\n      - name: events\n        read:\n          connection: bronze\n          path: events\n        write:\n          connection: gold\n          path: events\n          mode: append\n</code></pre>"},{"location":"features/patterns/#simple-overwrite-no-pattern-needed","title":"Simple Overwrite (No Pattern Needed)","text":"<p>For full refresh of small tables:</p> <pre><code>pipelines:\n  - pipeline: refresh_products\n    nodes:\n      - name: products\n        read:\n          connection: bronze\n          path: products\n        write:\n          connection: gold\n          path: products\n          mode: overwrite\n</code></pre>"},{"location":"features/patterns/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right pattern - Use SCD2 for dimensions needing history, Merge for incremental CDC</li> <li>Use write.mode for simple cases - <code>append</code> for events, <code>overwrite</code> for full refresh</li> <li>Define business keys - Ensure <code>keys</code> uniquely identify records in your domain</li> <li>Monitor tracked columns - For SCD2, only track columns that represent meaningful business changes</li> <li>Use audit columns - Track <code>created_at</code> and <code>updated_at</code> for debugging and lineage</li> <li>Optimize large tables - Use <code>zorder_by</code> or <code>cluster_by</code> for frequently queried columns</li> </ol>"},{"location":"features/patterns/#related","title":"Related","text":"<ul> <li>Transformers - Built-in transformation functions</li> <li>Pipelines - Pipeline configuration</li> <li>YAML Schema Reference - Full schema documentation</li> </ul>"},{"location":"features/pipelines/","title":"Pipelines","text":"<p>Orchestrate complex data workflows with dependency-aware execution, parallel processing, and intelligent error handling.</p>"},{"location":"features/pipelines/#overview","title":"Overview","text":"<p>Odibi's pipeline system provides: - DAG-based execution: Automatic dependency resolution with cycle detection - Parallel processing: Execute independent nodes concurrently - Error strategies: Fine-grained control over failure behavior - Resume capability: Skip successfully completed nodes on retry - Drift detection: Compare local config against deployed definitions</p>"},{"location":"features/pipelines/#pipeline-vs-pipelinemanager","title":"Pipeline vs PipelineManager","text":"Component Purpose <code>Pipeline</code> Executes a single pipeline (nodes, graph, engine) <code>PipelineManager</code> Manages multiple pipelines from a YAML config file"},{"location":"features/pipelines/#core-concepts","title":"Core Concepts","text":""},{"location":"features/pipelines/#pipeline","title":"Pipeline","text":"<p>The <code>Pipeline</code> class is the executor and orchestrator for a single pipeline. It: - Builds a dependency graph from node configurations - Resolves execution order via topological sort - Manages the execution engine (Pandas or Spark) - Generates execution stories for observability</p>"},{"location":"features/pipelines/#node","title":"Node","text":"<p>A <code>Node</code> is the atomic unit of work. Each node follows a four-phase execution pattern:</p> <pre><code>Read \u2192 Transform \u2192 Validate \u2192 Write\n</code></pre> Phase Description Read Load data from a connection (file, table, API) Transform Apply transformations (SQL, functions, patterns) Validate Run data quality tests, quarantine bad rows Write Persist output to a connection"},{"location":"features/pipelines/#dependencygraph","title":"DependencyGraph","text":"<p>The <code>DependencyGraph</code> class builds and validates the DAG:</p> Feature Description Missing dependency check Fails if <code>depends_on</code> references undefined nodes Cycle detection Detects circular dependencies before execution Topological sort Returns nodes in valid execution order Execution layers Groups independent nodes for parallel execution"},{"location":"features/pipelines/#pipelineresults","title":"PipelineResults","text":"<p>Execution results are captured in <code>PipelineResults</code>:</p> Field Type Description <code>pipeline_name</code> string Name of the executed pipeline <code>completed</code> list Successfully completed node names <code>failed</code> list Failed node names <code>skipped</code> list Skipped node names (dependency failures) <code>node_results</code> dict Detailed <code>NodeResult</code> per node <code>duration</code> float Total execution time in seconds <code>story_path</code> string Path to generated execution story"},{"location":"features/pipelines/#configuration","title":"Configuration","text":""},{"location":"features/pipelines/#yaml-structure","title":"YAML Structure","text":"<pre><code>project: MyDataPipeline\nengine: spark  # or 'pandas'\n\nconnections:\n  bronze:\n    type: local\n    path: ./data/bronze\n  silver:\n    type: local\n    path: ./data/silver\n\npipelines:\n  - pipeline: bronze_to_silver\n    nodes:\n      - name: load_orders\n        read:\n          connection: bronze\n          path: orders.parquet\n          format: parquet\n\n      - name: clean_orders\n        depends_on: [load_orders]\n        transform:\n          steps:\n            - function: drop_nulls\n              params:\n                columns: [order_id, customer_id]\n\n      - name: write_orders\n        depends_on: [clean_orders]\n        write:\n          connection: silver\n          path: orders_clean.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"features/pipelines/#pipeline-config-options","title":"Pipeline Config Options","text":"Field Type Required Description <code>pipeline</code> string Yes Unique pipeline name <code>nodes</code> list Yes List of node configurations"},{"location":"features/pipelines/#node-config-options","title":"Node Config Options","text":"Field Type Required Description <code>name</code> string Yes Unique node name within pipeline <code>depends_on</code> list No List of upstream node names <code>read</code> object No Read configuration <code>transform</code> object No Transform steps configuration <code>validation</code> object No Data quality tests <code>write</code> object No Write configuration <code>on_error</code> string No Error strategy: <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code> <code>cache</code> bool No Cache output in memory"},{"location":"features/pipelines/#execution-modes","title":"Execution Modes","text":""},{"location":"features/pipelines/#serial-vs-parallel","title":"Serial vs Parallel","text":"<pre><code># Serial execution (default)\nresults = manager.run()\n\n# Parallel execution with 4 workers\nresults = manager.run(parallel=True, max_workers=4)\n</code></pre> <p>In parallel mode, nodes are grouped into execution layers. Nodes within the same layer have no dependencies on each other and execute concurrently.</p>"},{"location":"features/pipelines/#dry-run-mode","title":"Dry Run Mode","text":"<p>Simulate execution without performing actual read/write operations:</p> <pre><code>results = manager.run(dry_run=True)\n</code></pre> <p>Dry run validates: - Configuration syntax - Dependency graph structure - Connection availability</p>"},{"location":"features/pipelines/#resume-from-failure","title":"Resume from Failure","text":"<p>Skip nodes that completed successfully in the previous run:</p> <pre><code>results = manager.run(resume_from_failure=True)\n</code></pre> <p>Resume capability: - Tracks node version hashes to detect config changes - Restores output from persisted writes - Invalidates downstream nodes when upstream re-executes</p>"},{"location":"features/pipelines/#error-strategies","title":"Error Strategies","text":"<p>Control how the pipeline handles node failures:</p> Strategy Behavior <code>fail_fast</code> Stop immediately on first failure <code>fail_later</code> Complete current layer, then stop <code>ignore</code> Log error and continue execution <pre><code>nodes:\n  - name: optional_enrichment\n    on_error: ignore  # Continue even if this fails\n    # ...\n</code></pre> <p>Override at runtime:</p> <pre><code>results = manager.run(on_error=\"fail_fast\")\n</code></pre>"},{"location":"features/pipelines/#features","title":"Features","text":""},{"location":"features/pipelines/#dependency-resolution","title":"Dependency Resolution","text":"<p>The pipeline automatically determines execution order:</p> <pre><code># Get execution order\norder = pipeline.graph.topological_sort()\n# ['load_orders', 'clean_orders', 'write_orders']\n\n# Visualize the graph\nprint(pipeline.visualize())\n</code></pre> <p>Output:</p> <pre><code>Dependency Graph:\n\nLayer 1:\n  - load_orders\n\nLayer 2:\n  - clean_orders (depends on: load_orders)\n\nLayer 3:\n  - write_orders (depends on: clean_orders)\n</code></pre>"},{"location":"features/pipelines/#execution-layers","title":"Execution Layers","text":"<p>For parallel execution, nodes are grouped into layers:</p> <pre><code>layers = pipeline.get_execution_layers()\n# [['load_orders'], ['clean_orders'], ['write_orders']]\n</code></pre> <p>Nodes in the same layer can run simultaneously.</p>"},{"location":"features/pipelines/#drift-detection","title":"Drift Detection","text":"<p>When a System Catalog is configured, the pipeline detects drift between local and deployed configurations:</p> <pre><code>\u26a0\ufe0f DRIFT DETECTED: Local pipeline definition differs from Catalog.\n   Local Hash: a1b2c3d4\n   Catalog Hash: e5f6g7h8\n   Advice: Deploy changes using 'odibi deploy' before running in production.\n</code></pre> <p>Deploy to sync:</p> <pre><code>manager.deploy()  # Deploy all pipelines\nmanager.deploy(\"bronze_to_silver\")  # Deploy specific pipeline\n</code></pre>"},{"location":"features/pipelines/#lineage-integration","title":"Lineage Integration","text":"<p>Pipelines automatically emit OpenLineage events when configured:</p> <pre><code>lineage:\n  enabled: true\n  backend: file\n  path: ./lineage\n</code></pre> <p>Events include: - Pipeline start/complete - Node start/complete - Input/output datasets - Schema information</p>"},{"location":"features/pipelines/#api-examples","title":"API Examples","text":""},{"location":"features/pipelines/#create-from-yaml","title":"Create from YAML","text":"<pre><code>from odibi.pipeline import Pipeline, PipelineManager\n\n# Recommended: Use Pipeline.from_yaml() for convenience\nmanager = Pipeline.from_yaml(\"config.yaml\")\n\n# Or directly use PipelineManager\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# With environment overrides\nmanager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n</code></pre>"},{"location":"features/pipelines/#run-pipelines","title":"Run Pipelines","text":"<pre><code># Run all pipelines\nresults = manager.run()\n\n# Run specific pipeline\nresults = manager.run(\"bronze_to_silver\")\n\n# Run multiple pipelines\nresults = manager.run([\"bronze_to_silver\", \"silver_to_gold\"])\n\n# Run with options\nresults = manager.run(\n    parallel=True,\n    max_workers=8,\n    dry_run=False,\n    resume_from_failure=True,\n    on_error=\"fail_fast\"\n)\n</code></pre>"},{"location":"features/pipelines/#check-results","title":"Check Results","text":"<pre><code># Single pipeline returns PipelineResults\nif not results.failed:\n    print(f\"Success! Processed {len(results.completed)} nodes in {results.duration:.2f}s\")\nelse:\n    print(f\"Failed nodes: {results.failed}\")\n\n# Access individual node results\nfor node_name, node_result in results.node_results.items():\n    print(f\"{node_name}: {node_result.rows_processed} rows in {node_result.duration:.2f}s\")\n\n# Get story path\nif results.story_path:\n    print(f\"Execution story: {results.story_path}\")\n</code></pre>"},{"location":"features/pipelines/#pipeline-validation","title":"Pipeline Validation","text":"<pre><code># Validate without executing\nvalidation = pipeline.validate()\n\nif validation[\"valid\"]:\n    print(f\"Pipeline valid with {validation['node_count']} nodes\")\n    print(f\"Execution order: {validation['execution_order']}\")\nelse:\n    print(f\"Errors: {validation['errors']}\")\n\nif validation[\"warnings\"]:\n    print(f\"Warnings: {validation['warnings']}\")\n</code></pre>"},{"location":"features/pipelines/#list-and-access-pipelines","title":"List and Access Pipelines","text":"<pre><code># List available pipelines\nprint(manager.list_pipelines())\n# ['bronze_to_silver', 'silver_to_gold']\n\n# Get specific pipeline instance\npipeline = manager.get_pipeline(\"bronze_to_silver\")\n\n# Execute single node (for debugging)\nresult = pipeline.execute_node(\"clean_orders\")\n</code></pre>"},{"location":"features/pipelines/#complete-example","title":"Complete Example","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nconnections:\n  raw:\n    type: azure_adls\n    account: ${AZURE_STORAGE_ACCOUNT}\n    container: raw\n  silver:\n    type: delta\n    path: abfss://silver@${AZURE_STORAGE_ACCOUNT}.dfs.core.windows.net/\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    on_events: [on_failure]\n\npipelines:\n  - pipeline: sales_daily\n    nodes:\n      - name: ingest_transactions\n        read:\n          connection: raw\n          path: transactions/\n          format: parquet\n          incremental:\n            mode: rolling_window\n            column: transaction_date\n            lookback: 7\n            unit: day\n\n      - name: validate_transactions\n        depends_on: [ingest_transactions]\n        validation:\n          tests:\n            - type: not_null\n              columns: [transaction_id, amount]\n            - type: positive\n              columns: [amount]\n          on_fail: quarantine\n          quarantine:\n            connection: silver\n            path: quarantine/transactions\n\n      - name: aggregate_daily\n        depends_on: [validate_transactions]\n        transform:\n          steps:\n            - function: group_by_sum\n              params:\n                group_cols: [store_id, transaction_date]\n                sum_cols: [amount]\n        on_error: fail_fast\n\n      - name: write_daily_sales\n        depends_on: [aggregate_daily]\n        write:\n          connection: silver\n          path: sales/daily\n          format: delta\n          mode: merge\n          merge_keys: [store_id, transaction_date]\n</code></pre> <pre><code>from odibi.pipeline import Pipeline\n\nmanager = Pipeline.from_yaml(\"sales_config.yaml\")\nresults = manager.run(parallel=True, max_workers=4)\n\nif results.failed:\n    print(f\"Pipeline failed: {results.failed}\")\nelse:\n    print(f\"Daily sales updated: {results.story_path}\")\n</code></pre>"},{"location":"features/pipelines/#related","title":"Related","text":"<ul> <li>Alerting - Configure notifications for pipeline events</li> <li>Quality Gates - Block pipelines on data quality failures</li> <li>Quarantine Tables - Handle invalid rows</li> <li>Lineage - Track data flow across pipelines</li> </ul>"},{"location":"features/plugins/","title":"Plugins &amp; Extensibility","text":"<p>Extend Odibi with custom connections, transforms, and engines through a flexible plugin system.</p>"},{"location":"features/plugins/#overview","title":"Overview","text":"<p>Odibi's plugin system provides: - Connection plugins: Add custom data source connectors - Transform plugins: Register custom data transformation functions - Engine plugins: Support for different processing engines - Auto-discovery: Automatic loading of <code>transforms.py</code> and <code>plugins.py</code> - Entry points: Standard Python packaging for distributable plugins</p>"},{"location":"features/plugins/#plugin-types","title":"Plugin Types","text":""},{"location":"features/plugins/#connection-plugins","title":"Connection Plugins","text":"<p>Add support for new data sources by registering connection factories:</p> <pre><code>from odibi.plugins import register_connection_factory\n\ndef create_my_connection(name: str, config: dict):\n    \"\"\"Factory function for custom connection.\"\"\"\n    return MyCustomConnection(\n        host=config.get(\"host\"),\n        port=config.get(\"port\", 5432),\n    )\n\nregister_connection_factory(\"my_db\", create_my_connection)\n</code></pre> <p>Once registered, use in YAML config:</p> <pre><code>connections:\n  my_source:\n    type: my_db\n    host: localhost\n    port: 5432\n</code></pre>"},{"location":"features/plugins/#transform-plugins","title":"Transform Plugins","text":"<p>Register custom data transformation functions using the <code>@transform</code> decorator:</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef clean_phone_numbers(context, current, country_code=\"US\"):\n    \"\"\"Standardize phone number format.\"\"\"\n    df = current\n    # Transform logic here\n    return df\n\n@transform(\"custom_name\")\ndef my_transform(context, current):\n    \"\"\"Transform with custom registration name.\"\"\"\n    return current\n</code></pre> <p>Use in pipeline YAML:</p> <pre><code>pipelines:\n  - pipeline: process_customers\n    nodes:\n      - name: standardize\n        transform: clean_phone_numbers\n        params:\n          country_code: \"UK\"\n</code></pre>"},{"location":"features/plugins/#engine-plugins","title":"Engine Plugins","text":"<p>Odibi supports multiple processing engines. The engine is specified at the project level:</p> <pre><code>project: MyProject\nengine: spark  # or 'pandas', 'polars'\n</code></pre>"},{"location":"features/plugins/#functionregistry","title":"FunctionRegistry","text":"<p>The <code>FunctionRegistry</code> is the central registry for transform functions.</p>"},{"location":"features/plugins/#registration-methods","title":"Registration Methods","text":"<pre><code>from odibi.registry import FunctionRegistry, transform\n\n# Method 1: Using decorator\n@transform\ndef my_transform(context, current, param1: str):\n    return current\n\n# Method 2: Direct registration\ndef another_transform(context, current):\n    return current\n\nFunctionRegistry.register(another_transform, name=\"alt_transform\")\n</code></pre>"},{"location":"features/plugins/#registry-api","title":"Registry API","text":"Method Description <code>register(func, name, param_model)</code> Register a function with optional name and Pydantic model <code>get(name)</code> Retrieve a registered function <code>validate_params(name, params)</code> Validate parameters against signature <code>list_functions()</code> List all registered function names <code>get_function_info(name)</code> Get function metadata and signature <code>get_param_model(name)</code> Get Pydantic model for parameter validation"},{"location":"features/plugins/#parameter-validation","title":"Parameter Validation","text":"<p>Use Pydantic models for strict parameter validation:</p> <pre><code>from pydantic import BaseModel\nfrom odibi.registry import transform, FunctionRegistry\n\nclass FilterParams(BaseModel):\n    column: str\n    min_value: float\n    max_value: float = 100.0\n\n@transform(param_model=FilterParams)\ndef filter_range(context, current, column: str, min_value: float, max_value: float = 100.0):\n    return current.filter((current[column] &gt;= min_value) &amp; (current[column] &lt;= max_value))\n\n# Validation happens automatically\nFunctionRegistry.validate_params(\"filter_range\", {\"column\": \"price\", \"min_value\": 10})\n</code></pre>"},{"location":"features/plugins/#connection-factory","title":"Connection Factory","text":"<p>The connection factory system allows registering custom connection types.</p>"},{"location":"features/plugins/#built-in-connections","title":"Built-in Connections","text":"Type Description <code>local</code> Local filesystem <code>http</code> HTTP/REST endpoints <code>azure_blob</code> / <code>azure_adls</code> Azure Blob Storage / ADLS Gen2 <code>delta</code> Delta Lake tables <code>sql_server</code> / <code>azure_sql</code> SQL Server / Azure SQL"},{"location":"features/plugins/#custom-factory-pattern","title":"Custom Factory Pattern","text":"<pre><code>from odibi.plugins import register_connection_factory, get_connection_factory\n\ndef create_postgres_connection(name: str, config: dict):\n    \"\"\"Create a PostgreSQL connection.\"\"\"\n    from my_connections import PostgresConnection\n\n    return PostgresConnection(\n        host=config[\"host\"],\n        port=config.get(\"port\", 5432),\n        database=config[\"database\"],\n        username=config.get(\"username\"),\n        password=config.get(\"password\"),\n    )\n\n# Register the factory\nregister_connection_factory(\"postgres\", create_postgres_connection)\n\n# Retrieve a factory (if needed)\nfactory = get_connection_factory(\"postgres\")\n</code></pre>"},{"location":"features/plugins/#auto-discovery","title":"Auto-Discovery","text":"<p>Odibi automatically loads extension files from your project directory.</p>"},{"location":"features/plugins/#supported-files","title":"Supported Files","text":"File Purpose <code>transforms.py</code> Custom transform functions <code>plugins.py</code> Connection factories and other plugins"},{"location":"features/plugins/#search-locations","title":"Search Locations","text":"<ol> <li>Config directory: Same directory as your YAML config</li> <li>Current working directory: Where you run the CLI</li> </ol>"},{"location":"features/plugins/#example-structure","title":"Example Structure","text":"<pre><code>my_project/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 transforms.py      # Auto-loaded\n\u251c\u2500\u2500 plugins.py         # Auto-loaded\n\u2514\u2500\u2500 data/\n</code></pre>"},{"location":"features/plugins/#transformspy-example","title":"transforms.py Example","text":"<pre><code>\"\"\"Custom transforms for my project.\"\"\"\n\nfrom odibi.registry import transform\n\n@transform\ndef calculate_metrics(context, current, metrics: list):\n    \"\"\"Calculate custom business metrics.\"\"\"\n    df = current\n    for metric in metrics:\n        df = df.withColumn(f\"{metric}_calculated\", ...)\n    return df\n\n@transform\ndef apply_business_rules(context, current, rule_set: str):\n    \"\"\"Apply business rules based on rule set name.\"\"\"\n    # Implementation\n    return current\n</code></pre>"},{"location":"features/plugins/#pluginspy-example","title":"plugins.py Example","text":"<pre><code>\"\"\"Custom plugins for my project.\"\"\"\n\nfrom odibi.plugins import register_connection_factory\n\ndef create_snowflake_connection(name, config):\n    from snowflake.connector import connect\n    # Create connection\n    return SnowflakeWrapper(connect(**config))\n\nregister_connection_factory(\"snowflake\", create_snowflake_connection)\n</code></pre>"},{"location":"features/plugins/#plugin-configuration","title":"Plugin Configuration","text":""},{"location":"features/plugins/#entry-points-setup","title":"Entry Points Setup","text":"<p>For distributable plugins, use Python entry points in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"odibi.connections\"]\npostgres = \"my_plugin.connections:create_postgres_connection\"\nsnowflake = \"my_plugin.connections:create_snowflake_connection\"\n</code></pre> <p>Or in <code>setup.py</code>:</p> <pre><code>setup(\n    name=\"odibi-postgres-plugin\",\n    entry_points={\n        \"odibi.connections\": [\n            \"postgres = my_plugin.connections:create_postgres_connection\",\n        ],\n    },\n)\n</code></pre>"},{"location":"features/plugins/#entry-point-groups","title":"Entry Point Groups","text":"Group Purpose <code>odibi.connections</code> Connection factory functions"},{"location":"features/plugins/#loading-plugins","title":"Loading Plugins","text":"<p>Plugins are loaded automatically at startup:</p> <pre><code>from odibi.plugins import load_plugins\n\n# Called automatically, but can be invoked manually\nload_plugins()\n</code></pre>"},{"location":"features/plugins/#complete-example","title":"Complete Example","text":""},{"location":"features/plugins/#project-structure","title":"Project Structure","text":"<pre><code>my_etl_project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 transforms.py\n\u251c\u2500\u2500 plugins.py\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_etl/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 connections/\n            \u2514\u2500\u2500 custom.py\n</code></pre>"},{"location":"features/plugins/#transformspy","title":"transforms.py","text":"<pre><code>\"\"\"Project-specific transforms.\"\"\"\n\nfrom odibi.registry import transform\nfrom pydantic import BaseModel\n\nclass EnrichmentParams(BaseModel):\n    lookup_table: str\n    key_column: str\n    value_columns: list[str]\n\n@transform(param_model=EnrichmentParams)\ndef enrich_with_lookup(\n    context,\n    current,\n    lookup_table: str,\n    key_column: str,\n    value_columns: list[str],\n):\n    \"\"\"Enrich data with lookup table values.\"\"\"\n    lookup_df = context.read(lookup_table)\n    return current.join(\n        lookup_df.select(key_column, *value_columns),\n        on=key_column,\n        how=\"left\",\n    )\n\n@transform\ndef deduplicate(context, current, key_columns: list, order_by: str = None):\n    \"\"\"Remove duplicate rows based on key columns.\"\"\"\n    if order_by:\n        from pyspark.sql import Window\n        from pyspark.sql.functions import row_number\n\n        window = Window.partitionBy(*key_columns).orderBy(order_by)\n        return current.withColumn(\"_rn\", row_number().over(window)) \\\n                      .filter(\"_rn = 1\") \\\n                      .drop(\"_rn\")\n    return current.dropDuplicates(key_columns)\n</code></pre>"},{"location":"features/plugins/#pluginspy","title":"plugins.py","text":"<pre><code>\"\"\"Connection plugins.\"\"\"\n\nfrom odibi.plugins import register_connection_factory\n\ndef create_redis_connection(name: str, config: dict):\n    \"\"\"Redis cache connection.\"\"\"\n    import redis\n\n    return redis.Redis(\n        host=config.get(\"host\", \"localhost\"),\n        port=config.get(\"port\", 6379),\n        db=config.get(\"db\", 0),\n        password=config.get(\"password\"),\n    )\n\nregister_connection_factory(\"redis\", create_redis_connection)\n</code></pre>"},{"location":"features/plugins/#configyaml","title":"config.yaml","text":"<pre><code>project: CustomerETL\nengine: spark\n\nconnections:\n  source:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: raw\n\n  cache:\n    type: redis\n    host: localhost\n    port: 6379\n\npipelines:\n  - pipeline: process_customers\n    nodes:\n      - name: load_customers\n        source: source\n        path: customers/\n\n      - name: enrich\n        input: load_customers\n        transform: enrich_with_lookup\n        params:\n          lookup_table: reference/regions\n          key_column: region_code\n          value_columns:\n            - region_name\n            - country\n\n      - name: dedupe\n        input: enrich\n        transform: deduplicate\n        params:\n          key_columns:\n            - customer_id\n          order_by: updated_at desc\n</code></pre>"},{"location":"features/plugins/#best-practices","title":"Best Practices","text":"<ol> <li>Use Pydantic models - Validate transform parameters with type safety</li> <li>Keep plugins focused - One connection type per factory function</li> <li>Handle imports lazily - Import heavy dependencies inside factory functions</li> <li>Log appropriately - Use <code>logger.info()</code> for successful loads</li> <li>Provide good defaults - Make configuration optional where sensible</li> <li>Document parameters - Use docstrings for transform functions</li> </ol>"},{"location":"features/plugins/#related","title":"Related","text":"<ul> <li>Connections - Built-in connection types</li> <li>Transformers - Built-in transform functions</li> <li>Engines - Supported processing engines</li> <li>Configuration - YAML configuration reference</li> </ul>"},{"location":"features/quality_gates/","title":"Quality Gates","text":"<p>Batch-level quality validation that evaluates the entire dataset before writing.</p>"},{"location":"features/quality_gates/#overview","title":"Overview","text":"<p>While validation tests run per-row, quality gates evaluate aggregate metrics: - Overall pass rate - What percentage of rows passed all tests? - Per-test thresholds - Different requirements for different tests - Row count anomalies - Detect unexpected batch sizes</p>"},{"location":"features/quality_gates/#configuration","title":"Configuration","text":""},{"location":"features/quality_gates/#basic-gate-setup","title":"Basic Gate Setup","text":"<pre><code>nodes:\n  - name: load_silver_customers\n    read:\n      connection: bronze\n      path: customers\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n        - type: unique\n          columns: [customer_id]\n\n      gate:\n        require_pass_rate: 0.95  # 95% must pass\n        on_fail: abort           # Stop if gate fails\n</code></pre>"},{"location":"features/quality_gates/#gate-config-options","title":"Gate Config Options","text":"Field Type Required Default Description <code>require_pass_rate</code> float No 0.95 Minimum % of rows passing ALL tests <code>on_fail</code> string No \"abort\" Action on failure <code>thresholds</code> list No [] Per-test thresholds <code>row_count</code> object No null Row count validation"},{"location":"features/quality_gates/#on-fail-actions","title":"On-Fail Actions","text":"Action Description <code>abort</code> Stop pipeline, write nothing (default) <code>warn_and_write</code> Log warning, write all rows anyway <code>write_valid_only</code> Write only rows that passed validation"},{"location":"features/quality_gates/#per-test-thresholds","title":"Per-Test Thresholds","text":"<p>Set different requirements for specific tests:</p> <pre><code>gate:\n  require_pass_rate: 0.95  # Global: 95% must pass all tests\n\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99  # 99% for not_null (stricter)\n    - test: unique\n      min_pass_rate: 1.0   # 100% unique (no duplicates allowed)\n    - test: email_format   # Named test\n      min_pass_rate: 0.90  # 90% for email format (more lenient)\n</code></pre>"},{"location":"features/quality_gates/#row-count-validation","title":"Row Count Validation","text":"<p>Detect anomalies in batch size:</p> <pre><code>gate:\n  row_count:\n    min: 100              # Fail if fewer than 100 rows\n    max: 1000000          # Fail if more than 1M rows\n    change_threshold: 0.5 # Fail if count changes &gt;50% vs last run\n</code></pre>"},{"location":"features/quality_gates/#row-count-options","title":"Row Count Options","text":"Field Type Description <code>min</code> int Minimum expected row count <code>max</code> int Maximum expected row count <code>change_threshold</code> float Max allowed change vs previous run (0.5 = 50%)"},{"location":"features/quality_gates/#complete-example","title":"Complete Example","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: bronze\n      path: orders_raw\n\n    validation:\n      tests:\n        # Critical fields\n        - type: not_null\n          name: required_fields\n          columns: [order_id, customer_id, order_date]\n\n        # Uniqueness\n        - type: unique\n          name: unique_orders\n          columns: [order_id]\n\n        # Business rules\n        - type: range\n          name: valid_amount\n          column: amount\n          min: 0\n\n        - type: accepted_values\n          name: valid_status\n          column: status\n          values: [pending, completed, cancelled]\n\n      gate:\n        # Global threshold\n        require_pass_rate: 0.95\n\n        # Per-test overrides\n        thresholds:\n          - test: required_fields\n            min_pass_rate: 0.99\n          - test: unique_orders\n            min_pass_rate: 1.0\n\n        # Row count checks\n        row_count:\n          min: 1000\n          change_threshold: 0.3\n\n        # What to do on failure\n        on_fail: abort\n\n    write:\n      connection: silver\n      path: orders\n      format: delta\n</code></pre>"},{"location":"features/quality_gates/#combining-gates-with-quarantine","title":"Combining Gates with Quarantine","text":"<p>Use both for comprehensive data quality:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine  # Route failures to quarantine\n\n    - type: unique\n      columns: [customer_id]\n      on_fail: fail        # Critical - must pass\n\n  quarantine:\n    connection: silver\n    path: quarantine/customers\n\n  gate:\n    require_pass_rate: 0.95  # Still need 95% overall\n    on_fail: abort\n</code></pre> <p>Flow: 1. Rows failing <code>not_null</code> are quarantined 2. Gate evaluates remaining rows 3. If &lt;95% pass, pipeline aborts 4. Otherwise, valid rows are written</p>"},{"location":"features/quality_gates/#gate-failure-alerts","title":"Gate Failure Alerts","text":"<p>Get notified when gates fail:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n</code></pre> <p>Alert payload includes: - Pass rate achieved vs required - Number of failed rows - Failure reasons</p>"},{"location":"features/quality_gates/#gatefailederror","title":"GateFailedError","text":"<p>When a gate fails with <code>on_fail: abort</code>, a <code>GateFailedError</code> is raised:</p> <pre><code>from odibi.exceptions import GateFailedError\n\ntry:\n    pipeline.run()\nexcept GateFailedError as e:\n    print(f\"Gate failed: {e.pass_rate:.1%} &lt; {e.required_rate:.1%}\")\n    print(f\"Reasons: {e.failure_reasons}\")\n</code></pre>"},{"location":"features/quality_gates/#best-practices","title":"Best Practices","text":"<ol> <li>Start with high thresholds - Be strict initially, relax as needed</li> <li>Use per-test thresholds - Critical tests (uniqueness) should be 100%</li> <li>Monitor row count changes - Sudden changes often indicate problems</li> <li>Combine with quarantine - Don't lose failed data, route it for analysis</li> <li>Set up alerts - Know immediately when gates fail</li> </ol>"},{"location":"features/quality_gates/#related","title":"Related","text":"<ul> <li>Quarantine Tables - Route failed rows</li> <li>Alerting - Alert on gate failures</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/quarantine/","title":"Quarantine Tables","text":"<p>Route failed validation rows to a dedicated quarantine table with rejection metadata for later analysis and reprocessing.</p>"},{"location":"features/quarantine/#overview","title":"Overview","text":"<p>When validation tests fail, Odibi provides three options via <code>on_fail</code>: - <code>fail</code> - Stop the entire pipeline (default) - <code>warn</code> - Log and continue with all rows - <code>quarantine</code> - Route failed rows to a quarantine table, continue with valid rows</p> <p>The quarantine option preserves bad data for debugging without blocking production pipelines.</p>"},{"location":"features/quarantine/#configuration","title":"Configuration","text":""},{"location":"features/quarantine/#basic-quarantine-setup","title":"Basic Quarantine Setup","text":"<pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id, email]\n          on_fail: quarantine  # Route failures to quarantine\n        - type: regex_match\n          column: email\n          pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n          on_fail: quarantine\n\n      quarantine:\n        connection: silver\n        path: quarantine/customers\n        add_columns:\n          _rejection_reason: true\n          _rejected_at: true\n          _source_batch_id: true\n          _failed_tests: true\n</code></pre>"},{"location":"features/quarantine/#quarantine-config-options","title":"Quarantine Config Options","text":"Field Type Required Description <code>connection</code> string Yes Connection for quarantine writes <code>path</code> string No* Path for quarantine data <code>table</code> string No* Table name for quarantine <code>add_columns</code> object No Metadata columns to add <code>retention_days</code> int No Days to retain (default: 90) <p>*Either <code>path</code> or <code>table</code> is required.</p>"},{"location":"features/quarantine/#metadata-columns","title":"Metadata Columns","text":"<p>Control which metadata columns are added to quarantined rows:</p> <pre><code>quarantine:\n  connection: silver\n  path: quarantine/customers\n  add_columns:\n    _rejection_reason: true    # Description of why row failed\n    _rejected_at: true         # UTC timestamp of rejection\n    _source_batch_id: true     # Run ID for traceability\n    _failed_tests: true        # Comma-separated list of failed tests\n    _original_node: false      # Node name (disabled by default)\n</code></pre>"},{"location":"features/quarantine/#how-it-works","title":"How It Works","text":"<ol> <li>Test Evaluation: Each test with <code>on_fail: quarantine</code> is evaluated per-row</li> <li>Row Splitting: DataFrame is split into valid and invalid portions</li> <li>Metadata Addition: Failed rows receive metadata columns</li> <li>Quarantine Write: Invalid rows are appended to the quarantine table</li> <li>Pipeline Continues: Valid rows proceed through the pipeline</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Input Data    \u2502\n\u2502   (100 rows)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Validation    \u2502\n\u2502   Tests Run     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n    \u25bc         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Valid \u2502  \u2502  Invalid  \u2502\n\u2502(95)   \u2502  \u2502   (5)     \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502            \u2502\n    \u25bc            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Target \u2502  \u2502Quarantine \u2502\n\u2502 Table \u2502  \u2502  Table    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/quarantine/#example-complete-quarantine-pipeline","title":"Example: Complete Quarantine Pipeline","text":"<pre><code>project: CustomerData\nengine: spark\n\nconnections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n  silver:\n    type: local\n    base_path: ./data/silver\n\npipelines:\n  - pipeline: ingest_customers\n    layer: silver\n    nodes:\n      - name: validate_customers\n        read:\n          connection: bronze\n          path: customers_raw\n          format: parquet\n\n        validation:\n          tests:\n            # Required fields\n            - type: not_null\n              columns: [customer_id, email, created_at]\n              on_fail: quarantine\n\n            # Email format\n            - type: regex_match\n              column: email\n              pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n              on_fail: quarantine\n\n            # Age validation\n            - type: range\n              column: age\n              min: 0\n              max: 150\n              on_fail: quarantine\n\n          quarantine:\n            connection: silver\n            path: quarantine/customers\n            add_columns:\n              _rejection_reason: true\n              _rejected_at: true\n              _source_batch_id: true\n              _failed_tests: true\n\n        write:\n          connection: silver\n          path: customers\n          format: delta\n          mode: append\n</code></pre>"},{"location":"features/quarantine/#querying-quarantine-data","title":"Querying Quarantine Data","text":"<p>After running the pipeline, query the quarantine table to analyze failures:</p> <pre><code>-- View recent quarantined rows\nSELECT\n    customer_id,\n    email,\n    _rejection_reason,\n    _failed_tests,\n    _rejected_at,\n    _source_batch_id\nFROM quarantine.customers\nWHERE _rejected_at &gt;= current_date() - INTERVAL 7 DAYS\nORDER BY _rejected_at DESC;\n\n-- Count failures by test type\nSELECT\n    _failed_tests,\n    COUNT(*) as count\nFROM quarantine.customers\nGROUP BY _failed_tests\nORDER BY count DESC;\n</code></pre>"},{"location":"features/quarantine/#alerts-for-quarantine-events","title":"Alerts for Quarantine Events","text":"<p>Configure alerts to notify when rows are quarantined:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-quality\"\n</code></pre>"},{"location":"features/quarantine/#best-practices","title":"Best Practices","text":"<ol> <li>Use meaningful test names - Helps identify failures in quarantine data</li> <li>Set appropriate retention - Balance storage costs vs debugging needs</li> <li>Monitor quarantine rates - High rates may indicate upstream data issues</li> <li>Combine with gates - Use quality gates to abort if too many rows are quarantined</li> <li>Automate reprocessing - Build workflows to reprocess fixed quarantine data</li> </ol>"},{"location":"features/quarantine/#related","title":"Related","text":"<ul> <li>Quality Gates - Batch-level validation</li> <li>Alerting - Alert on quarantine events</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/schema_tracking/","title":"Schema Version Tracking","text":"<p>Track schema changes over time with automatic versioning, change detection, and CLI tools.</p>"},{"location":"features/schema_tracking/#overview","title":"Overview","text":"<p>Odibi automatically tracks schema changes in the System Catalog: - Version history: Every schema change creates a new version - Change detection: Identifies added, removed, and modified columns - CLI tools: Query history and compare versions</p>"},{"location":"features/schema_tracking/#how-it-works","title":"How It Works","text":"<ol> <li>After each pipeline run, the output schema is captured</li> <li>Schema is hashed and compared to the previous version</li> <li>If changed, a new version is recorded with change details</li> <li>History is stored in <code>meta_schemas</code> table</li> </ol>"},{"location":"features/schema_tracking/#automatic-tracking","title":"Automatic Tracking","text":"<p>Schema tracking happens automatically during pipeline execution when: - A node writes to a table/path - The System Catalog is configured</p> <p>No additional configuration is required.</p>"},{"location":"features/schema_tracking/#cli-commands","title":"CLI Commands","text":""},{"location":"features/schema_tracking/#view-schema-history","title":"View Schema History","text":"<pre><code>odibi schema history &lt;table&gt; --config config.yaml\n</code></pre> <p>Example:</p> <pre><code>$ odibi schema history silver/customers --config pipeline.yaml\n\nSchema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre> <p>Options:</p> <pre><code>odibi schema history &lt;table&gt; --config config.yaml \\\n    --limit 20 \\        # Show last 20 versions (default: 10)\n    --format json       # Output as JSON\n</code></pre>"},{"location":"features/schema_tracking/#compare-schema-versions","title":"Compare Schema Versions","text":"<pre><code>odibi schema diff &lt;table&gt; --config config.yaml --from-version 3 --to-version 5\n</code></pre> <p>Example:</p> <pre><code>$ odibi schema diff silver/customers --config pipeline.yaml --from-version 3 --to-version 5\n\nSchema Diff: silver/customers\nFrom v3 \u2192 v5\n============================================================\n  customer_id                  STRING               (unchanged)\n  email                        STRING               (unchanged)\n  name                         STRING               (unchanged)\n- legacy_id                    STRING               (removed in v5)\n+ loyalty_tier                 STRING               (added in v5)\n+ created_at                   TIMESTAMP            (added in v5)\n+ updated_at                   TIMESTAMP            (added in v5)\n</code></pre> <p>Without versions (compares latest two):</p> <pre><code>odibi schema diff silver/customers --config pipeline.yaml\n</code></pre>"},{"location":"features/schema_tracking/#programmatic-access","title":"Programmatic Access","text":""},{"location":"features/schema_tracking/#track-schema-manually","title":"Track Schema Manually","text":"<pre><code>from odibi.catalog import CatalogManager\n\ncatalog = CatalogManager(spark, config, base_path, engine)\n\n# Track a schema change\nresult = catalog.track_schema(\n    table_path=\"silver/customers\",\n    schema={\"customer_id\": \"STRING\", \"email\": \"STRING\", \"age\": \"INT\"},\n    pipeline=\"customer_pipeline\",\n    node=\"process_customers\",\n    run_id=\"run-12345\",\n)\n\nprint(f\"Changed: {result['changed']}\")\nprint(f\"Version: {result['version']}\")\nprint(f\"Columns added: {result.get('columns_added', [])}\")\nprint(f\"Columns removed: {result.get('columns_removed', [])}\")\nprint(f\"Types changed: {result.get('columns_type_changed', [])}\")\n</code></pre>"},{"location":"features/schema_tracking/#query-schema-history","title":"Query Schema History","text":"<pre><code># Get history for a table\nhistory = catalog.get_schema_history(\"silver/customers\", limit=10)\n\nfor record in history:\n    print(f\"v{record['schema_version']}: {record['captured_at']}\")\n    if record.get('columns_added'):\n        print(f\"  Added: {record['columns_added']}\")\n</code></pre>"},{"location":"features/schema_tracking/#schema-record-structure","title":"Schema Record Structure","text":"<p>Each schema version record includes:</p> Field Description <code>table_path</code> Full path to the table <code>schema_version</code> Auto-incrementing version number <code>schema_hash</code> MD5 hash of column definitions <code>columns</code> JSON of column names and types <code>captured_at</code> Timestamp of capture <code>pipeline</code> Pipeline that made the change <code>node</code> Node that made the change <code>run_id</code> Execution run ID <code>columns_added</code> List of new columns <code>columns_removed</code> List of removed columns <code>columns_type_changed</code> List of columns with type changes"},{"location":"features/schema_tracking/#storage-location","title":"Storage Location","text":"<p>Schema history is stored in the System Catalog:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n</code></pre> <p>Location: <code>{connection_base_path}/_odibi_system/meta_schemas/</code></p>"},{"location":"features/schema_tracking/#example-detecting-breaking-changes","title":"Example: Detecting Breaking Changes","text":"<p>Use schema tracking to detect breaking changes before they impact downstream:</p> <pre><code>def check_for_breaking_changes(catalog, table_path):\n    \"\"\"Check if recent schema changes might break downstream.\"\"\"\n    history = catalog.get_schema_history(table_path, limit=2)\n\n    if len(history) &lt; 2:\n        return False  # No previous version\n\n    latest = history[0]\n    removed = latest.get('columns_removed', [])\n    type_changes = latest.get('columns_type_changed', [])\n\n    if removed or type_changes:\n        print(f\"\u26a0\ufe0f Potential breaking changes in {table_path}\")\n        if removed:\n            print(f\"  Removed columns: {removed}\")\n        if type_changes:\n            print(f\"  Type changes: {type_changes}\")\n        return True\n\n    return False\n</code></pre>"},{"location":"features/schema_tracking/#integration-with-lineage","title":"Integration with Lineage","text":"<p>Combine schema tracking with lineage to assess impact:</p> <pre><code># Check what would be affected by a schema change\nodibi lineage impact silver/customers --config config.yaml\n</code></pre> <pre><code>\u26a0\ufe0f  Impact Analysis: silver/customers\n============================================================\n\nChanges to silver/customers would affect:\n\n  Affected Tables:\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 2 downstream table(s) in 2 pipeline(s)\n</code></pre>"},{"location":"features/schema_tracking/#best-practices","title":"Best Practices","text":"<ol> <li>Review schema changes - Check history after deployments</li> <li>Monitor for removals - Removed columns often break downstream</li> <li>Document type changes - Type changes may affect queries</li> <li>Use lineage for impact - Know what's affected before changing</li> <li>Automate checks - Add schema validation to CI/CD</li> </ol>"},{"location":"features/schema_tracking/#related","title":"Related","text":"<ul> <li>Cross-Pipeline Lineage - Impact analysis</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/stories/","title":"Execution Stories","text":"<p>Auto-generated pipeline execution documentation with rich metadata, sample data, and multiple output formats.</p>"},{"location":"features/stories/#overview","title":"Overview","text":"<p>Odibi's Story system provides: - Execution timeline: Complete record of pipeline runs with timestamps - Node-level metrics: Duration, row counts, schema changes per node - Sample data capture: Input/output samples with automatic redaction - Multiple renderers: HTML, Markdown, JSON output formats - Themes: Customizable styling for HTML reports - Retention policies: Automatic cleanup of old stories</p>"},{"location":"features/stories/#configuration","title":"Configuration","text":""},{"location":"features/stories/#basic-story-setup","title":"Basic Story Setup","text":"<pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre>"},{"location":"features/stories/#story-config-options","title":"Story Config Options","text":"Field Type Required Default Description <code>connection</code> string Yes - Connection name for story output <code>path</code> string Yes - Path for stories (relative to connection base_path) <code>max_sample_rows</code> int No <code>10</code> Maximum rows to include in samples <code>retention_days</code> int No <code>30</code> Days to keep stories before cleanup <code>retention_count</code> int No <code>100</code> Maximum number of stories to retain <code>failure_sample_size</code> int No <code>100</code> Rows to capture per validation failure <code>max_failure_samples</code> int No <code>500</code> Total failed rows across all validations <code>max_sampled_validations</code> int No <code>5</code> After this many validations, show only counts <code>theme</code> string No <code>default</code> Theme name or path to YAML theme file <code>include_samples</code> bool No <code>true</code> Whether to include data samples"},{"location":"features/stories/#remote-storage","title":"Remote Storage","text":"<p>Stories can be written to remote storage (ADLS, S3) using fsspec:</p> <pre><code>story:\n  output_path: abfss://container@account.dfs.core.windows.net/stories/\n  storage_options:\n    account_key: \"${STORAGE_ACCOUNT_KEY}\"\n</code></pre>"},{"location":"features/stories/#story-contents","title":"Story Contents","text":"<p>Each story captures comprehensive execution metadata:</p>"},{"location":"features/stories/#execution-timeline","title":"Execution Timeline","text":"Metric Description <code>started_at</code> ISO timestamp when pipeline started <code>completed_at</code> ISO timestamp when pipeline finished <code>duration</code> Total execution time in seconds <code>run_id</code> Unique identifier for the run"},{"location":"features/stories/#node-results","title":"Node Results","text":"<p>For each node in the pipeline:</p> Metric Description <code>node_name</code> Name of the node <code>operation</code> Operation type (read, transform, write) <code>status</code> Execution status: <code>success</code>, <code>failed</code>, <code>skipped</code> <code>duration</code> Node execution time in seconds <code>rows_in</code> Input row count <code>rows_out</code> Output row count <code>rows_change</code> Row count difference <code>rows_change_pct</code> Percentage change in row count"},{"location":"features/stories/#sample-data","title":"Sample Data","text":"<p>Sample data is captured with automatic redaction of sensitive values:</p> <pre><code>sample_data:\n  - order_id: 12345\n    customer_email: \"[REDACTED]\"\n    amount: 99.99\n  - order_id: 12346\n    customer_email: \"[REDACTED]\"\n    amount: 149.99\n</code></pre> <p>Configure sample capture:</p> <pre><code>story:\n  max_sample_rows: 5      # Limit sample size\n  include_samples: true   # Enable/disable samples\n</code></pre>"},{"location":"features/stories/#schema-changes","title":"Schema Changes","text":"<p>Stories track schema evolution:</p> Field Description <code>schema_in</code> Input column names <code>schema_out</code> Output column names <code>columns_added</code> New columns added <code>columns_removed</code> Columns removed <code>columns_renamed</code> Renamed columns"},{"location":"features/stories/#validation-results","title":"Validation Results","text":"<p>Validation warnings and errors are captured:</p> <pre><code>validation_warnings:\n  - \"Column 'email' has 5% null values\"\n  - \"Date range extends beyond expected bounds\"\n</code></pre> <p>Error details for failed nodes:</p> <pre><code>error_type: ValueError\nerror_message: \"Column 'order_id' contains duplicate values\"\nerror_traceback: \"Full Python traceback...\"\nerror_traceback_cleaned: \"Cleaned traceback (Spark/Java noise removed)\"\n</code></pre>"},{"location":"features/stories/#execution-steps","title":"Execution Steps","text":"<p>Stories capture the execution steps taken during node processing for debugging:</p> <pre><code>execution_steps:\n  - \"Read from bronze_db\"\n  - \"Applied pattern 'deduplicate'\"\n  - \"Executed 2 pre-SQL statement(s)\"\n  - \"Passed 3 contract checks\"\n</code></pre>"},{"location":"features/stories/#failed-rows-samples","title":"Failed Rows Samples","text":"<p>When validations fail, stories capture sample rows that failed each validation:</p> <pre><code>failed_rows_samples:\n  not_null_customer_id:\n    - { order_id: 123, customer_id: null, amount: 50.00 }\n    - { order_id: 456, customer_id: null, amount: 75.00 }\n  positive_amount:\n    - { order_id: 789, customer_id: \"C001\", amount: -10.00 }\n\nfailed_rows_counts:\n  not_null_customer_id: 150\n  positive_amount: 25\n</code></pre> <p>Configure failure sample limits:</p> <pre><code>story:\n  failure_sample_size: 100        # Max rows per validation\n  max_failure_samples: 500        # Total rows across all validations\n  max_sampled_validations: 5      # After 5 validations, show only counts\n</code></pre>"},{"location":"features/stories/#retry-history","title":"Retry History","text":"<p>When retries occur, the full history is captured:</p> <pre><code>retry_history:\n  - attempt: 1\n    success: false\n    error: \"Connection timeout\"\n    error_type: \"TimeoutError\"\n    duration: 1.2\n  - attempt: 2\n    success: false\n    error: \"Connection timeout\"\n    error_type: \"TimeoutError\"\n    duration: 2.4\n  - attempt: 3\n    success: true\n    duration: 0.8\n</code></pre>"},{"location":"features/stories/#delta-lake-info","title":"Delta Lake Info","text":"<p>For Delta Lake writes, version and operation metrics are captured:</p> <pre><code>delta_info:\n  version: 42\n  operation: MERGE\n  operation_metrics:\n    numTargetRowsInserted: 150\n    numTargetRowsUpdated: 25\n</code></pre>"},{"location":"features/stories/#themes","title":"Themes","text":"<p>Customize HTML story appearance with built-in or custom themes.</p>"},{"location":"features/stories/#built-in-themes","title":"Built-in Themes","text":"Theme Description <code>default</code> Clean, professional blue theme <code>corporate</code> Traditional business styling with serif headings <code>dark</code> Dark mode with high-contrast colors <code>minimal</code> Simple black and white, compact layout"},{"location":"features/stories/#using-themes","title":"Using Themes","text":"<pre><code>story:\n  theme: dark\n</code></pre>"},{"location":"features/stories/#custom-theme-file","title":"Custom Theme File","text":"<p>Create a custom theme YAML file:</p> <pre><code># my_theme.yaml\nname: company_brand\nprimary_color: \"#003366\"\nsuccess_color: \"#2e7d32\"\nerror_color: \"#c62828\"\nwarning_color: \"#ff9900\"\nbg_color: \"#ffffff\"\ntext_color: \"#333333\"\nfont_family: \"Arial, sans-serif\"\nheading_font: \"Georgia, serif\"\nlogo_url: \"https://example.com/logo.png\"\ncompany_name: \"Acme Corp\"\nfooter_text: \"Confidential - Internal Use Only\"\n</code></pre> <p>Reference in config:</p> <pre><code>story:\n  theme: path/to/my_theme.yaml\n</code></pre>"},{"location":"features/stories/#theme-options","title":"Theme Options","text":"Option Type Description <code>name</code> string Theme identifier <code>primary_color</code> hex Main accent color <code>success_color</code> hex Success status color <code>error_color</code> hex Error status color <code>warning_color</code> hex Warning status color <code>bg_color</code> hex Background color <code>text_color</code> hex Primary text color <code>border_color</code> hex Border color <code>code_bg</code> hex Code block background <code>font_family</code> string Body font stack <code>heading_font</code> string Heading font stack <code>code_font</code> string Monospace font stack <code>font_size</code> string Base font size <code>max_width</code> string Container max width <code>logo_url</code> string URL to company logo <code>company_name</code> string Company name for branding <code>footer_text</code> string Custom footer text <code>custom_css</code> string Additional CSS rules"},{"location":"features/stories/#renderers","title":"Renderers","text":"<p>Stories can be rendered in multiple formats.</p>"},{"location":"features/stories/#html-renderer","title":"HTML Renderer","text":"<p>Default format with interactive, responsive design:</p> <pre><code>from odibi.story.renderers import HTMLStoryRenderer, get_renderer\nfrom odibi.story.themes import get_theme\n\n# Using the factory\nrenderer = get_renderer(\"html\")\nhtml = renderer.render(metadata)\n\n# With custom theme\ntheme = get_theme(\"dark\")\nrenderer = HTMLStoryRenderer(theme=theme)\nhtml = renderer.render(metadata)\n</code></pre> <p>Features: - Collapsible node sections - Status indicators with color coding - Summary statistics dashboard - Responsive layout</p>"},{"location":"features/stories/#json-renderer","title":"JSON Renderer","text":"<p>Machine-readable format for API integration:</p> <pre><code>from odibi.story.renderers import JSONStoryRenderer\n\nrenderer = JSONStoryRenderer()\njson_str = renderer.render(metadata)\n</code></pre> <p>Output structure:</p> <pre><code>{\n  \"pipeline_name\": \"process_orders\",\n  \"run_id\": \"20240130_101500\",\n  \"started_at\": \"2024-01-30T10:15:00\",\n  \"completed_at\": \"2024-01-30T10:15:45\",\n  \"duration\": 45.23,\n  \"total_nodes\": 5,\n  \"completed_nodes\": 4,\n  \"failed_nodes\": 1,\n  \"skipped_nodes\": 0,\n  \"success_rate\": 80.0,\n  \"total_rows_processed\": 15000,\n  \"nodes\": [...]\n}\n</code></pre>"},{"location":"features/stories/#markdown-renderer","title":"Markdown Renderer","text":"<p>GitHub-flavored markdown for documentation:</p> <pre><code>from odibi.story.renderers import MarkdownStoryRenderer\n\nrenderer = MarkdownStoryRenderer()\nmd = renderer.render(metadata)\n</code></pre>"},{"location":"features/stories/#renderer-factory","title":"Renderer Factory","text":"<p>Use the factory function to get a renderer by format:</p> <pre><code>from odibi.story.renderers import get_renderer\n\n# Supported formats: \"html\", \"markdown\", \"md\", \"json\"\nrenderer = get_renderer(\"json\")\noutput = renderer.render(metadata)\n</code></pre>"},{"location":"features/stories/#retention","title":"Retention","text":"<p>Stories are automatically cleaned up based on retention policies.</p>"},{"location":"features/stories/#retention-configuration","title":"Retention Configuration","text":"<pre><code>story:\n  retention_days: 30    # Delete stories older than 30 days\n  retention_count: 100  # Keep maximum 100 stories per pipeline\n</code></pre>"},{"location":"features/stories/#how-retention-works","title":"How Retention Works","text":"<ol> <li>Count-based: When story count exceeds <code>retention_count</code>, oldest stories are deleted first</li> <li>Time-based: Stories older than <code>retention_days</code> are deleted</li> <li>Both apply: A story is deleted if it exceeds either limit</li> </ol>"},{"location":"features/stories/#storage-structure","title":"Storage Structure","text":"<p>Stories are organized by pipeline and date:</p> <pre><code>stories/\n\u251c\u2500\u2500 process_orders/\n\u2502   \u251c\u2500\u2500 2024-01-30/\n\u2502   \u2502   \u251c\u2500\u2500 run_10-15-00.html\n\u2502   \u2502   \u251c\u2500\u2500 run_10-15-00.json\n\u2502   \u2502   \u251c\u2500\u2500 run_14-30-00.html\n\u2502   \u2502   \u2514\u2500\u2500 run_14-30-00.json\n\u2502   \u2514\u2500\u2500 2024-01-31/\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 process_customers/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"features/stories/#remote-storage-cleanup","title":"Remote Storage Cleanup","text":"<p>Note: Automatic cleanup for remote storage (ADLS, S3) is not yet implemented. Monitor storage usage manually.</p>"},{"location":"features/stories/#examples","title":"Examples","text":""},{"location":"features/stories/#complete-story-configuration","title":"Complete Story Configuration","text":"<pre><code>project: DataPipeline\nengine: spark\n\nstory:\n  output_path: stories/\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n  theme: corporate\n  include_samples: true\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          path: orders/\n\n      - name: transform_orders\n        transform:\n          operation: sql\n          query: |\n            SELECT order_id, customer_id, amount\n            FROM {read_orders}\n            WHERE amount &gt; 0\n\n      - name: write_orders\n        write:\n          connection: silver\n          path: orders/\n          mode: merge\n</code></pre>"},{"location":"features/stories/#generated-story-output-json","title":"Generated Story Output (JSON)","text":"<pre><code>{\n  \"pipeline_name\": \"process_orders\",\n  \"pipeline_layer\": \"silver\",\n  \"run_id\": \"20240130_101500\",\n  \"started_at\": \"2024-01-30T10:15:00\",\n  \"completed_at\": \"2024-01-30T10:15:45\",\n  \"duration\": 45.23,\n  \"total_nodes\": 3,\n  \"completed_nodes\": 3,\n  \"failed_nodes\": 0,\n  \"skipped_nodes\": 0,\n  \"success_rate\": 100.0,\n  \"total_rows_processed\": 15000,\n  \"project\": \"DataPipeline\",\n  \"nodes\": [\n    {\n      \"node_name\": \"read_orders\",\n      \"operation\": \"read\",\n      \"status\": \"success\",\n      \"duration\": 5.12,\n      \"rows_out\": 15500,\n      \"schema_out\": [\"order_id\", \"customer_id\", \"amount\", \"created_at\"]\n    },\n    {\n      \"node_name\": \"transform_orders\",\n      \"operation\": \"transform\",\n      \"status\": \"success\",\n      \"duration\": 2.34,\n      \"rows_in\": 15500,\n      \"rows_out\": 15000,\n      \"rows_change\": -500,\n      \"rows_change_pct\": -3.2,\n      \"columns_removed\": [\"created_at\"]\n    },\n    {\n      \"node_name\": \"write_orders\",\n      \"operation\": \"write\",\n      \"status\": \"success\",\n      \"duration\": 37.77,\n      \"rows_out\": 15000,\n      \"delta_info\": {\n        \"version\": 42,\n        \"operation\": \"MERGE\",\n        \"operation_metrics\": {\n          \"numTargetRowsInserted\": 500,\n          \"numTargetRowsUpdated\": 14500\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"features/stories/#programmatic-story-generation","title":"Programmatic Story Generation","text":"<pre><code>from odibi.story.generator import StoryGenerator\nfrom odibi.story.metadata import PipelineStoryMetadata\nfrom odibi.story.themes import get_theme\n\n# Create generator\ngenerator = StoryGenerator(\n    pipeline_name=\"process_orders\",\n    max_sample_rows=10,\n    output_path=\"stories/\",\n    retention_days=30,\n    retention_count=100,\n)\n\n# Generate story after pipeline execution\nstory_path = generator.generate(\n    node_results=node_results,\n    completed=[\"read_orders\", \"transform_orders\", \"write_orders\"],\n    failed=[],\n    skipped=[],\n    duration=45.23,\n    start_time=\"2024-01-30T10:15:00\",\n    end_time=\"2024-01-30T10:15:45\",\n)\n\n# Get summary for alerts\nalert_summary = generator.get_alert_summary()\n</code></pre>"},{"location":"features/stories/#documentation-stories","title":"Documentation Stories","text":"<p>Generate stakeholder-ready documentation from pipeline config:</p> <pre><code>from odibi.story.doc_story import DocStoryGenerator\nfrom odibi.config import PipelineConfig\n\n# Load pipeline config\npipeline_config = PipelineConfig.from_yaml(\"pipeline.yaml\")\n\n# Generate documentation\ndoc_generator = DocStoryGenerator(pipeline_config)\ndoc_path = doc_generator.generate(\n    output_path=\"docs/pipeline_doc.html\",\n    format=\"html\",\n    include_flow_diagram=True,\n)\n</code></pre>"},{"location":"features/stories/#related","title":"Related","text":"<ul> <li>Alerting - Stories linked in alert payloads</li> <li>Quality Gates - Gate results captured in stories</li> <li>Schema Tracking - Schema changes in stories</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/transformers/","title":"Transformers","text":"<p>Declarative data transformations with SQL-first semantics, dual-engine support (Spark/Pandas), and extensible custom transforms.</p>"},{"location":"features/transformers/#overview","title":"Overview","text":"<p>Odibi's transformer system provides: - SQL-First Design: All core operations leverage SQL for optimal engine performance - Dual-Engine Support: Seamless execution on Spark or Pandas/DuckDB - Built-in Library: 30+ production-ready transformers - Extensibility: Register custom transforms with the <code>@transform</code> decorator - Chained Operations: Compose multiple transforms in <code>transform.steps</code></p>"},{"location":"features/transformers/#configuration","title":"Configuration","text":""},{"location":"features/transformers/#basic-transformer-usage","title":"Basic Transformer Usage","text":"<pre><code>nodes:\n  - name: clean_orders\n    source: raw_orders\n    transformer: \"filter_rows\"\n    params:\n      condition: \"status = 'active'\"\n</code></pre>"},{"location":"features/transformers/#transformer-config-options","title":"Transformer Config Options","text":"Field Type Required Description <code>transformer</code> string Yes Transformer name (e.g., <code>filter_rows</code>, <code>scd2</code>) <code>params</code> object Yes Transformer-specific parameters"},{"location":"features/transformers/#transform-steps","title":"Transform Steps","text":"<p>Chain multiple transformations in sequence using <code>transform.steps</code>:</p> <pre><code>nodes:\n  - name: process_customers\n    source: raw_customers\n    transform:\n      steps:\n        - transformer: \"clean_text\"\n          params:\n            columns: [\"email\", \"name\"]\n            trim: true\n            case: \"lower\"\n\n        - transformer: \"filter_rows\"\n          params:\n            condition: \"email IS NOT NULL\"\n\n        - transformer: \"derive_columns\"\n          params:\n            derivations:\n              full_name: \"concat(first_name, ' ', last_name)\"\n\n        - transformer: \"deduplicate\"\n          params:\n            keys: [\"customer_id\"]\n            order_by: \"updated_at DESC\"\n</code></pre>"},{"location":"features/transformers/#built-in-transformers","title":"Built-in Transformers","text":""},{"location":"features/transformers/#sql-core-transformers","title":"SQL Core Transformers","text":"<p>Basic SQL operations that work across all engines.</p>"},{"location":"features/transformers/#filter_rows","title":"filter_rows","text":"<p>Filter rows using SQL WHERE conditions.</p> <pre><code>transformer: \"filter_rows\"\nparams:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre>"},{"location":"features/transformers/#derive_columns","title":"derive_columns","text":"<p>Add new columns using SQL expressions.</p> <pre><code>transformer: \"derive_columns\"\nparams:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre>"},{"location":"features/transformers/#cast_columns","title":"cast_columns","text":"<p>Cast columns to different types.</p> <pre><code>transformer: \"cast_columns\"\nparams:\n  casts:\n    age: \"int\"\n    salary: \"double\"\n    created_at: \"timestamp\"\n</code></pre>"},{"location":"features/transformers/#clean_text","title":"clean_text","text":"<p>Apply text cleaning operations (trim, case conversion).</p> <pre><code>transformer: \"clean_text\"\nparams:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"  # Options: lower, upper, preserve\n</code></pre>"},{"location":"features/transformers/#extract_date_parts","title":"extract_date_parts","text":"<p>Extract year, month, day, hour from timestamps.</p> <pre><code>transformer: \"extract_date_parts\"\nparams:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\", \"day\"]\n</code></pre>"},{"location":"features/transformers/#normalize_schema","title":"normalize_schema","text":"<p>Rename, drop, and reorder columns.</p> <pre><code>transformer: \"normalize_schema\"\nparams:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre>"},{"location":"features/transformers/#sort","title":"sort","text":"<p>Sort data by columns.</p> <pre><code>transformer: \"sort\"\nparams:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre>"},{"location":"features/transformers/#limit-sample","title":"limit / sample","text":"<p>Limit or randomly sample rows.</p> <pre><code># Limit\ntransformer: \"limit\"\nparams:\n  n: 100\n  offset: 0\n\n# Sample\ntransformer: \"sample\"\nparams:\n  fraction: 0.1\n  seed: 42\n</code></pre>"},{"location":"features/transformers/#distinct","title":"distinct","text":"<p>Remove duplicate rows.</p> <pre><code>transformer: \"distinct\"\nparams:\n  columns: [\"category\", \"status\"]  # Optional: subset of columns\n</code></pre>"},{"location":"features/transformers/#fill_nulls","title":"fill_nulls","text":"<p>Replace null values with defaults.</p> <pre><code>transformer: \"fill_nulls\"\nparams:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre>"},{"location":"features/transformers/#split_part","title":"split_part","text":"<p>Extract parts of strings by delimiter.</p> <pre><code>transformer: \"split_part\"\nparams:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre>"},{"location":"features/transformers/#date_add-date_trunc-date_diff","title":"date_add / date_trunc / date_diff","text":"<p>Date arithmetic operations.</p> <pre><code># Add interval\ntransformer: \"date_add\"\nparams:\n  col: \"created_at\"\n  value: 7\n  unit: \"day\"\n\n# Truncate to precision\ntransformer: \"date_trunc\"\nparams:\n  col: \"created_at\"\n  unit: \"month\"\n\n# Calculate difference\ntransformer: \"date_diff\"\nparams:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre>"},{"location":"features/transformers/#case_when","title":"case_when","text":"<p>Conditional logic.</p> <pre><code>transformer: \"case_when\"\nparams:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre>"},{"location":"features/transformers/#convert_timezone","title":"convert_timezone","text":"<p>Convert timestamps between timezones.</p> <pre><code>transformer: \"convert_timezone\"\nparams:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre>"},{"location":"features/transformers/#concat_columns","title":"concat_columns","text":"<p>Concatenate multiple columns.</p> <pre><code>transformer: \"concat_columns\"\nparams:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre>"},{"location":"features/transformers/#relational-transformers","title":"Relational Transformers","text":"<p>Operations involving multiple datasets.</p>"},{"location":"features/transformers/#join","title":"join","text":"<p>Join with another dataset.</p> <pre><code>transformer: \"join\"\nparams:\n  right_dataset: \"customers\"  # Must be in depends_on\n  on: [\"customer_id\"]\n  how: \"left\"  # inner, left, right, full, cross\n  prefix: \"cust\"  # Prefix for right columns (avoid collisions)\n</code></pre>"},{"location":"features/transformers/#union","title":"union","text":"<p>Union multiple datasets.</p> <pre><code>transformer: \"union\"\nparams:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true  # Match columns by name\n</code></pre>"},{"location":"features/transformers/#pivot","title":"pivot","text":"<p>Pivot rows into columns.</p> <pre><code>transformer: \"pivot\"\nparams:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n  values: [\"Jan\", \"Feb\", \"Mar\"]  # Optional: explicit pivot values\n</code></pre>"},{"location":"features/transformers/#unpivot","title":"unpivot","text":"<p>Unpivot (melt) columns into rows.</p> <pre><code>transformer: \"unpivot\"\nparams:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre>"},{"location":"features/transformers/#aggregate","title":"aggregate","text":"<p>Group and aggregate data.</p> <pre><code>transformer: \"aggregate\"\nparams:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre>"},{"location":"features/transformers/#advanced-transformers","title":"Advanced Transformers","text":"<p>Complex data processing operations.</p>"},{"location":"features/transformers/#deduplicate","title":"deduplicate","text":"<p>Remove duplicates using window functions.</p> <pre><code>transformer: \"deduplicate\"\nparams:\n  keys: [\"customer_id\"]\n  order_by: \"updated_at DESC\"  # Keep most recent\n</code></pre>"},{"location":"features/transformers/#explode_list_column","title":"explode_list_column","text":"<p>Flatten array/list columns into rows.</p> <pre><code>transformer: \"explode_list_column\"\nparams:\n  column: \"items\"\n  outer: true  # Keep rows with empty lists\n</code></pre>"},{"location":"features/transformers/#dict_based_mapping","title":"dict_based_mapping","text":"<p>Map values using a dictionary.</p> <pre><code>transformer: \"dict_based_mapping\"\nparams:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre>"},{"location":"features/transformers/#regex_replace","title":"regex_replace","text":"<p>Replace patterns using regex.</p> <pre><code>transformer: \"regex_replace\"\nparams:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre>"},{"location":"features/transformers/#unpack_struct","title":"unpack_struct","text":"<p>Flatten struct/dict columns.</p> <pre><code>transformer: \"unpack_struct\"\nparams:\n  column: \"user_info\"\n</code></pre>"},{"location":"features/transformers/#hash_columns","title":"hash_columns","text":"<p>Hash columns for PII anonymization.</p> <pre><code>transformer: \"hash_columns\"\nparams:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"  # or \"md5\"\n</code></pre>"},{"location":"features/transformers/#generate_surrogate_key","title":"generate_surrogate_key","text":"<p>Create deterministic surrogate keys.</p> <pre><code>transformer: \"generate_surrogate_key\"\nparams:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre>"},{"location":"features/transformers/#parse_json","title":"parse_json","text":"<p>Parse JSON strings into structured data.</p> <pre><code>transformer: \"parse_json\"\nparams:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre>"},{"location":"features/transformers/#validate_and_flag","title":"validate_and_flag","text":"<p>Flag rows that fail validation rules.</p> <pre><code>transformer: \"validate_and_flag\"\nparams:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre>"},{"location":"features/transformers/#window_calculation","title":"window_calculation","text":"<p>Apply window functions.</p> <pre><code>transformer: \"window_calculation\"\nparams:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre>"},{"location":"features/transformers/#normalize_json","title":"normalize_json","text":"<p>Flatten nested JSON/struct into columns.</p> <pre><code>transformer: \"normalize_json\"\nparams:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre>"},{"location":"features/transformers/#sessionize","title":"sessionize","text":"<p>Assign session IDs based on inactivity threshold.</p> <pre><code>transformer: \"sessionize\"\nparams:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800  # 30 minutes\n  session_col: \"session_id\"\n</code></pre>"},{"location":"features/transformers/#scd-slowly-changing-dimensions","title":"SCD (Slowly Changing Dimensions)","text":"<p>Track historical changes with SCD Type 2.</p> <pre><code>transformer: \"scd2\"\nparams:\n  target: \"gold/customers\"        # Path to existing history\n  keys: [\"customer_id\"]           # Entity keys\n  track_cols: [\"address\", \"tier\"] # Columns to monitor for changes\n  effective_time_col: \"txn_date\"  # When change occurred\n  end_time_col: \"valid_to\"        # End timestamp column\n  current_flag_col: \"is_current\"  # Current record flag\n</code></pre> <p>How SCD2 Works: 1. Match: Finds existing records using <code>keys</code> 2. Compare: Checks <code>track_cols</code> to detect changes 3. Close: Updates old record's <code>end_time_col</code> if changed 4. Insert: Adds new record with open-ended validity</p>"},{"location":"features/transformers/#merge-transformer","title":"Merge Transformer","text":"<p>Upsert, append, or delete records in target tables.</p> <pre><code># Upsert (Update + Insert)\ntransformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"upsert\"\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre> <p>Merge Strategies:</p> Strategy Description <code>upsert</code> Update existing, insert new (default) <code>append_only</code> Only insert new keys, ignore duplicates <code>delete_match</code> Delete records matching source keys <p>Advanced Merge Options:</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n  insert_condition: \"source.is_deleted = false\"\n  delete_condition: \"source.is_deleted = true\"\n  optimize_write: true\n  zorder_by: [\"customer_id\"]\n  cluster_by: [\"region\"]\n</code></pre>"},{"location":"features/transformers/#validation-transformers","title":"Validation Transformers","text":"<p>Cross-dataset validation checks.</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"  # or \"schema_match\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre>"},{"location":"features/transformers/#delete-detection","title":"Delete Detection","text":"<p>Detect deleted records for CDC-like behavior.</p> <pre><code>transformer: \"detect_deletes\"\nparams:\n  mode: \"snapshot_diff\"  # Compare Delta versions\n  keys: [\"customer_id\"]\n  soft_delete_col: \"is_deleted\"  # Add flag column\n  max_delete_percent: 10.0  # Safety threshold\n  on_threshold_breach: \"error\"  # error, warn, skip\n</code></pre> <p>Delete Detection Modes:</p> Mode Description <code>none</code> Disabled <code>snapshot_diff</code> Compare current vs previous Delta version <code>sql_compare</code> Compare against live source via JDBC"},{"location":"features/transformers/#creating-custom-transformers","title":"Creating Custom Transformers","text":"<p>Use the <code>@transform</code> decorator with <code>FunctionRegistry</code> to create custom transformers.</p>"},{"location":"features/transformers/#basic-custom-transformer","title":"Basic Custom Transformer","text":"<pre><code>from pydantic import BaseModel, Field\nfrom odibi.context import EngineContext\nfrom odibi.registry import transform\n\n\nclass MyTransformParams(BaseModel):\n    \"\"\"Parameters for my custom transform.\"\"\"\n    column: str = Field(..., description=\"Column to process\")\n    multiplier: float = Field(default=1.0, description=\"Multiplier value\")\n\n\n@transform(\"my_custom_transform\", param_model=MyTransformParams)\ndef my_custom_transform(context: EngineContext, **params) -&gt; EngineContext:\n    \"\"\"My custom transformation.\"\"\"\n    config = MyTransformParams(**params)\n\n    # Use SQL for cross-engine compatibility\n    sql_query = f\"\"\"\n        SELECT *, {config.column} * {config.multiplier} AS {config.column}_scaled\n        FROM df\n    \"\"\"\n    return context.sql(sql_query)\n</code></pre>"},{"location":"features/transformers/#using-custom-transformers-in-yaml","title":"Using Custom Transformers in YAML","text":"<pre><code>nodes:\n  - name: process_data\n    source: raw_data\n    transformer: \"my_custom_transform\"\n    params:\n      column: \"price\"\n      multiplier: 1.1\n</code></pre>"},{"location":"features/transformers/#engine-specific-logic","title":"Engine-Specific Logic","text":"<pre><code>from odibi.enums import EngineType\n\n@transform(\"dual_engine_transform\", param_model=MyParams)\ndef dual_engine_transform(context: EngineContext, **params) -&gt; EngineContext:\n    config = MyParams(**params)\n\n    if context.engine_type == EngineType.SPARK:\n        # Spark-specific implementation\n        import pyspark.sql.functions as F\n        df = context.df.withColumn(\"new_col\", F.lit(\"spark\"))\n        return context.with_df(df)\n\n    elif context.engine_type == EngineType.PANDAS:\n        # Pandas-specific implementation\n        df = context.df.copy()\n        df[\"new_col\"] = \"pandas\"\n        return context.with_df(df)\n</code></pre>"},{"location":"features/transformers/#complete-example","title":"Complete Example","text":"<pre><code>project: ECommerceETL\nengine: spark\n\nconnections:\n  bronze:\n    type: delta\n    path: \"dbfs:/bronze\"\n  silver:\n    type: delta\n    path: \"dbfs:/silver\"\n  gold:\n    type: delta\n    path: \"dbfs:/gold\"\n\npipelines:\n  - pipeline: orders_to_gold\n    nodes:\n      # Clean raw data\n      - name: clean_orders\n        source:\n          connection: bronze\n          path: orders\n        transform:\n          steps:\n            - transformer: \"clean_text\"\n              params:\n                columns: [\"customer_email\"]\n                trim: true\n                case: \"lower\"\n\n            - transformer: \"cast_columns\"\n              params:\n                casts:\n                  order_date: \"timestamp\"\n                  total_amount: \"double\"\n\n            - transformer: \"filter_rows\"\n              params:\n                condition: \"total_amount &gt; 0\"\n\n      # Deduplicate and enrich\n      - name: enriched_orders\n        source: clean_orders\n        depends_on: [clean_orders, customers]\n        transform:\n          steps:\n            - transformer: \"deduplicate\"\n              params:\n                keys: [\"order_id\"]\n                order_by: \"updated_at DESC\"\n\n            - transformer: \"join\"\n              params:\n                right_dataset: \"customers\"\n                on: [\"customer_id\"]\n                how: \"left\"\n\n            - transformer: \"derive_columns\"\n              params:\n                derivations:\n                  order_year: \"YEAR(order_date)\"\n                  order_month: \"MONTH(order_date)\"\n\n      # Final merge to gold\n      - name: gold_orders\n        source: enriched_orders\n        transformer: \"merge\"\n        params:\n          target: \"gold.orders\"\n          keys: [\"order_id\"]\n          strategy: \"upsert\"\n          audit_cols:\n            created_col: \"dw_created_at\"\n            updated_col: \"dw_updated_at\"\n        destination:\n          connection: gold\n          path: orders\n</code></pre>"},{"location":"features/transformers/#best-practices","title":"Best Practices","text":"<ol> <li>Use SQL-first transforms - They push computation to the engine for optimal performance</li> <li>Chain with transform.steps - Compose multiple operations declaratively</li> <li>Prefer built-in transforms - They're tested for dual-engine compatibility</li> <li>Use Pydantic models - Define parameter schemas for custom transforms</li> <li>Handle nulls explicitly - Use <code>fill_nulls</code> or <code>COALESCE</code> in derivations</li> <li>Document custom transforms - Include docstrings and param descriptions</li> </ol>"},{"location":"features/transformers/#related","title":"Related","text":"<ul> <li>Quality Gates - Validate transform outputs</li> <li>Quarantine Tables - Handle failed validations</li> <li>YAML Schema Reference - Complete configuration options</li> </ul>"},{"location":"guides/MIGRATION_GUIDE/","title":"Odibi V3 Migration Guide","text":""},{"location":"guides/MIGRATION_GUIDE/#1-privacy-inheritance-safety-upgrade","title":"1. Privacy Inheritance (Safety Upgrade)","text":"<p>Change: PII status now inherits from upstream nodes. If a column is marked as <code>pii: true</code> in a source node, it will remain PII in all downstream nodes unless explicitly declassified.</p> <p>Impact: *   Existing Pipelines: Pipelines that relied on implicit declassification (i.e., assuming PII status is lost after one node) may now trigger anonymization in downstream nodes if privacy is configured. *   New Behavior: Safer by default. You cannot accidentally expose PII by forgetting to re-tag it.</p> <p>Action Required: If you have columns that are no longer PII (e.g., you hashed them or dropped the sensitive part), you must now explicitly declassify them in the node configuration if you want to stop tracking them as PII.</p> <pre><code>- name: downstream_node\n  privacy:\n    method: \"hash\"\n    declassify:\n      - \"hashed_email\"  # Stop tracking this as PII\n</code></pre>"},{"location":"guides/MIGRATION_GUIDE/#2-spark-write-modes","title":"2. Spark Write Modes","text":"<p>Change: The Spark engine now supports <code>upsert</code> and <code>append_once</code> modes, bringing it to parity with the Pandas engine.</p> <p>Usage: These modes require <code>keys</code> to be defined in the <code>write.options</code> (or <code>params</code> if using a transformer that passes them). They are supported only for Delta Lake format.</p> <pre><code>- name: merge_users\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"users\"\n    mode: \"upsert\"\n    options:\n      keys: [\"user_id\"]\n</code></pre>"},{"location":"guides/MIGRATION_GUIDE/#3-context-api","title":"3. Context API","text":"<p>Change: The <code>NodeExecutionContext</code> (available in custom transformers as <code>ctx</code>) has been updated. *   Added <code>ctx.schema</code>: Returns a dictionary of column types. *   Added <code>ctx.pii_metadata</code>: Dictionary of active PII columns.</p>"},{"location":"guides/avoiding_the_builder_trap/","title":"Avoiding the Builder Trap","text":"<p>A Guide for Odibi Maintainers and Contributors</p>"},{"location":"guides/avoiding_the_builder_trap/#what-is-the-builder-trap","title":"What is the Builder Trap?","text":"<p>The Builder Trap is the tendency to continuously add features, refactor code, and \"improve\" a framework without ever using it on real problems. It feels productive, but it creates:</p> <ul> <li>Blind spots: Features that seem useful but aren't</li> <li>Complexity: Code that solves imaginary problems</li> <li>Burnout: Endless work with no validation</li> <li>Drift: The framework diverges from real user needs</li> </ul> <p>The antidote: Use the framework more than you build it.</p>"},{"location":"guides/avoiding_the_builder_trap/#the-golden-ratio","title":"The Golden Ratio","text":"<p>Aim for this balance:</p> Activity Time Allocation Using Odibi (real pipelines, real data) 60% Fixing/Improving (based on usage pain) 30% New Features (planned, prioritized) 10% <p>If you're spending more than 30% of your time building new features, you're probably in the trap.</p>"},{"location":"guides/avoiding_the_builder_trap/#issue-driven-development","title":"Issue-Driven Development","text":""},{"location":"guides/avoiding_the_builder_trap/#the-rule","title":"The Rule","text":"<p>If it's not critical, create an issue instead of fixing it immediately.</p>"},{"location":"guides/avoiding_the_builder_trap/#the-decision-framework","title":"The Decision Framework","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Is this blocking you?           \u2502\n\u2502         (Tests fail, can't run)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502               \u2502\n        YES              NO\n         \u2502               \u2502\n         \u25bc               \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Fix Now \u2502    \u2502 Is it &lt; 5 min   \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 AND obvious?    \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502               \u2502\n                  YES              NO\n                   \u2502               \u2502\n                   \u25bc               \u25bc\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502 Fix Now \u2502    \u2502 CREATE      \u2502\n             \u2502 No Issue\u2502    \u2502 AN ISSUE    \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/avoiding_the_builder_trap/#why-this-works","title":"Why This Works","text":"<ol> <li>Prevents scope creep - \"Just one quick fix\" becomes 3 hours of yak shaving</li> <li>Forces prioritization - Is this actually important, or just visible right now?</li> <li>Creates documentation - Future you can see what was decided and why</li> <li>Enables batching - Related issues can be fixed together efficiently</li> <li>Protects focus - You stay on your current task</li> </ol>"},{"location":"guides/avoiding_the_builder_trap/#issue-hygiene","title":"Issue Hygiene","text":"<p>When creating issues, include:</p> <pre><code>## Problem\nWhat's broken or missing?\n\n## Impact\nWho is affected? How badly?\n\n## Proposed Solution (optional)\nQuick sketch of the fix\n\n## Effort Estimate\n- Trivial (&lt; 30 min)\n- Small (1-2 hours)\n- Medium (half day)\n- Large (1+ days)\n</code></pre> <p>Use labels consistently: - <code>bug</code> - Something is broken - <code>enhancement</code> - New feature or improvement - <code>documentation</code> - Docs updates - <code>good-first-issue</code> - Easy wins for new contributors - <code>priority:high</code> - Needs attention soon - <code>priority:low</code> - Nice to have</p>"},{"location":"guides/avoiding_the_builder_trap/#dogfooding-practices","title":"Dogfooding Practices","text":""},{"location":"guides/avoiding_the_builder_trap/#1-build-real-pipelines","title":"1. Build Real Pipelines","text":"<p>Don't just test with toy data. Use Odibi for actual work:</p> <ul> <li>Personal projects: Analyze your finances, fitness data, reading list</li> <li>Work tasks: If appropriate, use Odibi for real ETL jobs</li> <li>Side projects: Build something you actually need</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#2-experience-the-onboarding","title":"2. Experience the Onboarding","text":"<p>Periodically, pretend you're a new user:</p> <pre><code># Start fresh\nrm -rf .venv\npython -m venv .venv\npip install odibi\n\n# Follow your own Getting Started guide\n# Note every point of friction\n</code></pre>"},{"location":"guides/avoiding_the_builder_trap/#3-run-the-full-workflow","title":"3. Run the Full Workflow","text":"<p>Regularly exercise the complete user journey:</p> <pre><code>odibi init-pipeline mytest\nodibi validate odibi.yaml\nodibi run odibi.yaml --dry-run\nodibi run odibi.yaml\nodibi doctor odibi.yaml\nodibi story list\n</code></pre> <p>Ask yourself: - Was anything confusing? - Did error messages help or frustrate? - What would I Google if I got stuck?</p>"},{"location":"guides/avoiding_the_builder_trap/#4-break-it-on-purpose","title":"4. Break It On Purpose","text":"<p>Try to make Odibi fail:</p> <ul> <li>Missing connections</li> <li>Circular dependencies</li> <li>Invalid YAML</li> <li>Wrong column names in SQL</li> <li>Network failures (disconnect wifi mid-run)</li> </ul> <p>Good frameworks fail gracefully. Bad ones fail mysteriously.</p>"},{"location":"guides/avoiding_the_builder_trap/#signs-youre-in-the-builder-trap","title":"Signs You're in the Builder Trap","text":"<p>Watch for these warning signs:</p> Sign Reality Check \"I need to refactor X before I can use it\" No, you don't. Use it messy. \"Just one more feature, then it's ready\" It's ready now. Ship it. \"Nobody can use this until I fix Y\" Let them try. Their feedback &gt; your assumptions. \"I'll write docs after I finish building\" Docs ARE building. Write them now. \"This code isn't clean enough\" Clean code that isn't used is worthless."},{"location":"guides/avoiding_the_builder_trap/#the-cure","title":"The Cure","text":"<p>When you catch yourself building instead of using:</p> <ol> <li>Stop immediately</li> <li>Create an issue for what you were about to do</li> <li>Open <code>odibi.yaml</code> and run a real pipeline</li> <li>Note what actually bothers you during usage</li> <li>Those are your real priorities</li> </ol>"},{"location":"guides/avoiding_the_builder_trap/#weekly-review-ritual","title":"Weekly Review Ritual","text":"<p>Every week, spend 30 minutes on this:</p>"},{"location":"guides/avoiding_the_builder_trap/#1-triage-issues-10-min","title":"1. Triage Issues (10 min)","text":"<ul> <li>Review new issues</li> <li>Close duplicates or \"won't fix\"</li> <li>Prioritize the backlog</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#2-usage-reflection-10-min","title":"2. Usage Reflection (10 min)","text":"<ul> <li>What pipelines did I run this week?</li> <li>What frustrated me?</li> <li>What worked well?</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#3-plan-next-week-10-min","title":"3. Plan Next Week (10 min)","text":"<ul> <li>Pick 1-2 issues to address</li> <li>Schedule time for USING, not just building</li> <li>Resist the urge to add \"just one more thing\"</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#the-north-star-question","title":"The North Star Question","text":"<p>Before any work session, ask:</p> <p>\"Am I building this because a real user (including myself) hit this problem, or because I think someone might need it someday?\"</p> <p>If the answer is \"someday\" \u2192 Create an issue and move on.</p> <p>If the answer is \"I hit this yesterday\" \u2192 Fix it.</p>"},{"location":"guides/avoiding_the_builder_trap/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>The Mom Test - How to validate ideas through usage</li> <li>Shape Up - Basecamp's approach to shipping</li> <li>Just Fucking Ship - Amy Hoy on finishing things</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#summary","title":"Summary","text":"Do This Not This Use Odibi on real data Build features in isolation Create issues for future work Fix everything immediately Experience your own onboarding Assume the UX is fine Ship small, validate, iterate Wait until it's \"perfect\" Ask \"did I hit this problem?\" Ask \"might someone need this?\" <p>The framework is ready. Go use it.</p>"},{"location":"guides/best_practices/","title":"Odibi Best Practices Guide","text":"<p>Version: 2.4.0 Last Updated: 2025-12-03 Audience: Data Engineers, Analytics Engineers, Team Leads</p> <p>This guide covers recommended patterns for building maintainable, scalable, and production-ready Odibi pipelines.</p>"},{"location":"guides/best_practices/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Project Organization</li> <li>Pipeline Design</li> <li>Node Design</li> <li>Naming Conventions</li> <li>Configuration Management</li> <li>Performance</li> <li>Data Quality</li> <li>Cross-Pipeline Dependencies</li> <li>Security</li> <li>Version Control</li> </ol>"},{"location":"guides/best_practices/#1-project-organization","title":"1. Project Organization","text":""},{"location":"guides/best_practices/#recommended-folder-structure","title":"Recommended Folder Structure","text":"<pre><code>my-odibi-project/\n\u251c\u2500\u2500 project.yaml                    # Core config (connections, settings)\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 bronze/\n\u2502   \u2502   \u2514\u2500\u2500 read_bronze.yaml        # Bronze layer pipeline\n\u2502   \u251c\u2500\u2500 silver/\n\u2502   \u2502   \u2514\u2500\u2500 transform_silver.yaml   # Silver layer pipeline\n\u2502   \u2514\u2500\u2500 gold/\n\u2502       \u2514\u2500\u2500 build_gold.yaml         # Gold layer pipeline\n\u251c\u2500\u2500 transformations/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 custom_transforms.py        # Custom Python transformations\n\u251c\u2500\u2500 sql/\n\u2502   \u2514\u2500\u2500 complex_queries.sql         # Complex SQL (optional)\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_pipelines.py           # Pipeline tests\n\u251c\u2500\u2500 .env                            # Local secrets (git-ignored)\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"guides/best_practices/#separation-of-concerns","title":"Separation of Concerns","text":"File Contains Does NOT Contain <code>project.yaml</code> Connections, system config, story config, imports Pipeline definitions <code>pipelines/*.yaml</code> Pipeline and node definitions Connection details <code>transformations/</code> Custom Python logic YAML configuration"},{"location":"guides/best_practices/#example-projectyaml","title":"Example <code>project.yaml</code>","text":"<pre><code>project: OEE\ndescription: \"OEE Analytics Platform\"\nengine: spark\nversion: \"1.0.0\"\nowner: \"Data Team\"\n\n# === Connections (defined once, used everywhere) ===\nconnections:\n  source_db:\n    type: sql_server\n    host: ${DB_HOST}\n    database: ${DB_NAME}\n    auth:\n      mode: sql_login\n      username: ${DB_USER}\n      password: ${DB_PASS}\n\n  lakehouse:\n    type: azure_blob\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    auth:\n      mode: account_key\n      account_key: ${STORAGE_KEY}\n\n# === System Catalog ===\nsystem:\n  connection: lakehouse\n  path: _odibi_system\n\n# === Story Configuration ===\nstory:\n  connection: lakehouse\n  path: stories/\n  retention_days: 30\n  auto_generate: true\n\n# === Global Settings ===\nperformance:\n  use_arrow: true\n  skip_null_profiling: true\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\nlogging:\n  level: INFO\n  structured: true\n\n# === Import Pipelines ===\nimports:\n  - pipelines/bronze/read_bronze.yaml\n  - pipelines/silver/transform_silver.yaml\n  - pipelines/gold/build_gold.yaml\n</code></pre>"},{"location":"guides/best_practices/#example-pipeline-file","title":"Example Pipeline File","text":"<p>pipelines/bronze/read_bronze.yaml:</p> <pre><code>pipelines:\n  - pipeline: read_bronze\n    description: \"Ingest raw data from source systems\"\n    layer: bronze\n    nodes:\n      - name: orders\n        description: \"Raw orders from ERP\"\n        read:\n          connection: source_db\n          format: sql\n          table: sales.orders\n          incremental:\n            mode: stateful\n            column: updated_at\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/orders\"\n          mode: append\n          add_metadata: true\n\n      - name: customers\n        description: \"Customer master data\"\n        read:\n          connection: source_db\n          format: sql\n          table: sales.customers\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/customers\"\n          mode: append\n          add_metadata: true\n          skip_if_unchanged: true\n          skip_hash_columns: [customer_id]\n</code></pre>"},{"location":"guides/best_practices/#2-pipeline-design","title":"2. Pipeline Design","text":""},{"location":"guides/best_practices/#one-pipeline-per-layer-per-domain","title":"One Pipeline Per Layer Per Domain","text":"<p>\u2705 Good:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 bronze/\n\u2502   \u2514\u2500\u2500 read_bronze.yaml           # All bronze ingestion\n\u251c\u2500\u2500 silver/\n\u2502   \u2514\u2500\u2500 transform_silver.yaml      # All silver transformations\n\u2514\u2500\u2500 gold/\n    \u251c\u2500\u2500 gold_sales.yaml            # Sales domain aggregates\n    \u2514\u2500\u2500 gold_inventory.yaml        # Inventory domain aggregates\n</code></pre> <p>\u274c Avoid:</p> <pre><code>pipelines/\n\u251c\u2500\u2500 orders_bronze_silver_gold.yaml  # Too many concerns in one file\n\u2514\u2500\u2500 everything.yaml                 # Unmaintainable\n</code></pre>"},{"location":"guides/best_practices/#pipeline-sizing-guidelines","title":"Pipeline Sizing Guidelines","text":"Node Count Recommendation 1-20 nodes Single pipeline file 20-50 nodes Consider splitting by sub-domain 50+ nodes Split into multiple pipelines"},{"location":"guides/best_practices/#keep-nodes-with-their-pipeline","title":"Keep Nodes with Their Pipeline","text":"<p>\u274c Don't split nodes into separate files:</p> <pre><code># nodes/orders.yaml - BAD: nodes scattered across files\npipelines:\n  - pipeline: read_bronze\n    nodes:\n      - name: orders\n</code></pre> <p>\u2705 Keep nodes together:</p> <pre><code># read_bronze.yaml - GOOD: all nodes in one place\npipelines:\n  - pipeline: read_bronze\n    nodes:\n      - name: orders\n        # ...\n      - name: customers\n        # ...\n      - name: products\n        # ...\n</code></pre> <p>Why? - <code>depends_on</code> relationships are visible in one file - Easier to understand the full pipeline flow - One file = one pipeline = one commit for changes</p>"},{"location":"guides/best_practices/#3-node-design","title":"3. Node Design","text":""},{"location":"guides/best_practices/#single-responsibility","title":"Single Responsibility","text":"<p>Each node should do one thing well:</p> <p>\u2705 Good:</p> <pre><code>- name: load_orders\n  read: ...\n  write: ...\n\n- name: clean_orders\n  depends_on: [load_orders]\n  transform:\n    steps:\n      - sql: \"SELECT * FROM load_orders WHERE status IS NOT NULL\"\n  write: ...\n\n- name: enrich_orders\n  depends_on: [clean_orders, customers]\n  transform:\n    steps:\n      - operation: join\n        left: clean_orders\n        right: customers\n        on: [customer_id]\n  write: ...\n</code></pre> <p>\u274c Avoid:</p> <pre><code>- name: do_everything\n  read: ...\n  transform:\n    steps:\n      - sql: \"...\"  # 500 lines of SQL doing everything\n  write: ...\n</code></pre>"},{"location":"guides/best_practices/#use-descriptions","title":"Use Descriptions","text":"<p>Always add descriptions for documentation and debugging:</p> <pre><code>- name: calculate_daily_revenue\n  description: \"Aggregates order amounts by day for finance reporting\"\n  tags: [daily, finance, critical]\n</code></pre>"},{"location":"guides/best_practices/#cache-strategically","title":"Cache Strategically","text":"<p>Use <code>cache: true</code> for nodes that are: - Read by multiple downstream nodes - Expensive to compute - Small enough to fit in memory</p> <pre><code>- name: dimension_products\n  description: \"Product dimension - cached for multiple joins\"\n  read: ...\n  cache: true  # Multiple nodes will join to this\n</code></pre>"},{"location":"guides/best_practices/#4-naming-conventions","title":"4. Naming Conventions","text":""},{"location":"guides/best_practices/#pipeline-names","title":"Pipeline Names","text":"<p>Use <code>snake_case</code> with layer prefix:</p> Pattern Example <code>{action}_{layer}</code> <code>read_bronze</code>, <code>transform_silver</code>, <code>build_gold</code> <code>{layer}_{domain}</code> <code>bronze_sales</code>, <code>silver_inventory</code>"},{"location":"guides/best_practices/#node-names","title":"Node Names","text":"<p>Use descriptive <code>snake_case</code>:</p> Pattern Example Source nodes <code>orders</code>, <code>customers</code>, <code>products</code> Transformed nodes <code>clean_orders</code>, <code>enriched_customers</code> Aggregated nodes <code>daily_sales</code>, <code>monthly_revenue</code> Dimension nodes <code>dim_product</code>, <code>dim_customer</code> Fact nodes <code>fact_orders</code>, <code>fact_inventory</code>"},{"location":"guides/best_practices/#connection-names","title":"Connection Names","text":"<p>Use environment + purpose:</p> <pre><code>connections:\n  prod_source_db:    # Production source database\n  prod_lakehouse:    # Production data lake\n  dev_lakehouse:     # Development data lake\n</code></pre>"},{"location":"guides/best_practices/#5-configuration-management","title":"5. Configuration Management","text":""},{"location":"guides/best_practices/#environment-variables-for-secrets","title":"Environment Variables for Secrets","text":"<p>\u2705 Always use environment variables for sensitive data:</p> <pre><code>connections:\n  database:\n    host: ${DB_HOST}\n    username: ${DB_USER}\n    password: ${DB_PASSWORD}\n</code></pre> <p>\u274c Never hardcode secrets:</p> <pre><code>connections:\n  database:\n    password: \"my_secret_password\"  # NEVER DO THIS\n</code></pre>"},{"location":"guides/best_practices/#use-env-for-local-development","title":"Use <code>.env</code> for Local Development","text":"<pre><code># .env (git-ignored)\nDB_HOST=localhost\nDB_USER=dev_user\nDB_PASSWORD=dev_password\nSTORAGE_ACCOUNT=devaccount\nSTORAGE_KEY=abc123...\n</code></pre>"},{"location":"guides/best_practices/#environment-specific-overrides","title":"Environment-Specific Overrides","text":"<pre><code># In project.yaml\nenvironments:\n  dev:\n    connections:\n      lakehouse:\n        container: dev-datalake\n  prod:\n    logging:\n      level: WARNING\n    connections:\n      lakehouse:\n        container: prod-datalake\n</code></pre> <p>Run with: <code>odibi run project.yaml --env prod</code></p>"},{"location":"guides/best_practices/#6-performance","title":"6. Performance","text":""},{"location":"guides/best_practices/#enable-arrow-for-pandas","title":"Enable Arrow for Pandas","text":"<pre><code>performance:\n  use_arrow: true  # Major speedup for Parquet I/O\n</code></pre>"},{"location":"guides/best_practices/#use-incremental-loading","title":"Use Incremental Loading","text":"<p>Don't reload full tables every time:</p> <pre><code>read:\n  connection: source_db\n  table: orders\n  incremental:\n    mode: stateful\n    column: updated_at\n    watermark_lag: \"1d\"\n</code></pre>"},{"location":"guides/best_practices/#skip-unchanged-data","title":"Skip Unchanged Data","text":"<p>For dimension tables that rarely change:</p> <pre><code>write:\n  mode: append\n  skip_if_unchanged: true\n  skip_hash_columns: [id]\n</code></pre>"},{"location":"guides/best_practices/#optimize-delta-writes-spark","title":"Optimize Delta Writes (Spark)","text":"<pre><code>write:\n  format: delta\n  options:\n    optimize_write: true\n    cluster_by: [date, region]\n</code></pre>"},{"location":"guides/best_practices/#skip-null-profiling-for-large-tables","title":"Skip Null Profiling for Large Tables","text":"<pre><code>performance:\n  skip_null_profiling: true  # Faster for very large DataFrames\n</code></pre>"},{"location":"guides/best_practices/#7-data-quality-validation","title":"7. Data Quality &amp; Validation","text":""},{"location":"guides/best_practices/#validation-strategy-overview","title":"Validation Strategy Overview","text":"<p>Odibi provides three validation mechanisms for different use cases:</p> Mechanism When Executed Purpose On Failure Contracts Before processing Input validation Always stops pipeline Validation After transformation Output checks Configurable (warn/error) Gates Before write Critical path checks Blocks downstream nodes"},{"location":"guides/best_practices/#use-contracts-for-input-validation","title":"Use Contracts for Input Validation","text":"<p>Fail fast if source data is bad:</p> <pre><code>- name: process_orders\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read: ...\n  transform: ...\n</code></pre>"},{"location":"guides/best_practices/#use-validation-for-output-checks","title":"Use Validation for Output Checks","text":"<p>Warn (or fail) if output doesn't meet expectations:</p> <pre><code>- name: daily_revenue\n  transform: ...\n  validation:\n    tests:\n      - type: not_null\n        columns: [date, revenue]\n      - type: unique\n        columns: [date]\n      - type: range\n        column: revenue\n        min: 0\n    on_failure: warn  # or \"error\" to fail the pipeline\n</code></pre>"},{"location":"guides/best_practices/#available-validation-types","title":"Available Validation Types","text":"Type Description Example <code>not_null</code> Check for null values <code>columns: [id, name]</code> <code>unique</code> Check for duplicates <code>columns: [id]</code> <code>row_count</code> Validate row counts <code>min: 100, max: 1000000</code> <code>freshness</code> Check data recency <code>column: updated_at, max_age: \"24h\"</code> <code>range</code> Numeric bounds <code>column: amount, min: 0, max: 10000</code> <code>regex</code> Pattern matching <code>column: email, pattern: \"^.+@.+$\"</code> <code>referential</code> FK validation <code>column: customer_id, reference: dim_customer.id</code> <code>custom</code> Custom Python function <code>function: my_validation_func</code>"},{"location":"guides/best_practices/#use-quality-gates-for-critical-paths","title":"Use Quality Gates for Critical Paths","text":"<pre><code>- name: load_orders\n  gate:\n    - type: row_count\n      min: 1000\n      on_failure: block  # Stops pipeline if &lt; 1000 rows\n</code></pre>"},{"location":"guides/best_practices/#fk-validation-for-fact-tables","title":"FK Validation for Fact Tables","text":"<p>Ensure referential integrity before loading fact tables:</p> <pre><code>- name: fact_orders\n  depends_on: [dim_customer, dim_product]\n  read:\n    connection: staging\n    path: orders\n  validation:\n    tests:\n      - type: referential\n        column: customer_id\n        reference: dim_customer.customer_id\n        on_orphan: warn\n      - type: referential\n        column: product_id\n        reference: dim_product.product_id\n        on_orphan: filter  # Remove orphan rows\n  write:\n    connection: warehouse\n    path: fact_orders\n</code></pre>"},{"location":"guides/best_practices/#custom-validation-functions","title":"Custom Validation Functions","text":"<p>Register custom validation logic:</p> <pre><code>from odibi import transform\n\n@transform(\"validate_business_rules\")\ndef validate_business_rules(context, current):\n    \"\"\"Custom business rule validation.\"\"\"\n    errors = []\n\n    # Rule 1: Order amount must match line items\n    mismatched = current[current['total'] != current['line_items_sum']]\n    if len(mismatched) &gt; 0:\n        errors.append(f\"{len(mismatched)} orders with mismatched totals\")\n\n    # Rule 2: Future dates not allowed\n    future_orders = current[current['order_date'] &gt; pd.Timestamp.now()]\n    if len(future_orders) &gt; 0:\n        errors.append(f\"{len(future_orders)} orders with future dates\")\n\n    if errors:\n        context.log_warning(f\"Validation issues: {'; '.join(errors)}\")\n\n    return current\n</code></pre> <p>Use in YAML:</p> <pre><code>transform:\n  steps:\n    - function: validate_business_rules\n</code></pre>"},{"location":"guides/best_practices/#quarantine-bad-records","title":"Quarantine Bad Records","text":"<p>Separate bad data for review instead of failing:</p> <pre><code>- name: process_orders\n  validation:\n    tests:\n      - type: not_null\n        columns: [order_id, amount]\n    on_failure: quarantine\n    quarantine:\n      connection: warehouse\n      path: quarantine/orders\n      include_reason: true  # Adds _quarantine_reason column\n</code></pre>"},{"location":"guides/best_practices/#8-cross-pipeline-dependencies","title":"8. Cross-Pipeline Dependencies","text":""},{"location":"guides/best_practices/#use-pipelinenode-references","title":"Use <code>$pipeline.node</code> References","text":"<p>When silver needs bronze outputs:</p> <pre><code># pipelines/silver/transform_silver.yaml\npipelines:\n  - pipeline: transform_silver\n    nodes:\n      - name: enriched_orders\n        inputs:\n          orders: $read_bronze.orders           # Cross-pipeline reference\n          customers: $read_bronze.customers\n        transform:\n          steps:\n            - operation: join\n              left: orders\n              right: customers\n              on: [customer_id]\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"silver/enriched_orders\"\n</code></pre>"},{"location":"guides/best_practices/#run-pipelines-in-order","title":"Run Pipelines in Order","text":"<pre><code># Bronze first\nodibi run project.yaml --pipeline read_bronze\n\n# Then silver (references bronze outputs)\nodibi run project.yaml --pipeline transform_silver\n</code></pre>"},{"location":"guides/best_practices/#best-practices-for-references","title":"Best Practices for References","text":"<ol> <li>Always use <code>path:</code> in write config \u2014 ensures cross-engine compatibility</li> <li>Run source pipeline first \u2014 references require catalog entries</li> <li>Use meaningful node names \u2014 <code>$read_bronze.orders</code> is clearer than <code>$p1.n1</code></li> </ol>"},{"location":"guides/best_practices/#9-security","title":"9. Security","text":""},{"location":"guides/best_practices/#mask-sensitive-columns-in-stories","title":"Mask Sensitive Columns in Stories","text":"<pre><code>- name: process_users\n  sensitive: [email, ssn, phone]  # Masked in Data Stories\n</code></pre>"},{"location":"guides/best_practices/#full-node-masking-for-pii-heavy-nodes","title":"Full Node Masking for PII-Heavy Nodes","text":"<pre><code>- name: medical_records\n  sensitive: true  # Entire sample redacted\n</code></pre>"},{"location":"guides/best_practices/#use-key-vault-in-production","title":"Use Key Vault in Production","text":"<pre><code>connections:\n  lakehouse:\n    auth:\n      mode: key_vault\n      key_vault: my-key-vault\n      secret: storage-account-key\n</code></pre>"},{"location":"guides/best_practices/#never-log-secrets","title":"Never Log Secrets","text":"<p>Odibi automatically redacts values that look like secrets, but be careful in custom transformations:</p> <pre><code>@transform\ndef my_transform(context, params):\n    # \u274c NEVER do this\n    print(f\"Using password: {params['password']}\")\n\n    # \u2705 Do this instead\n    logger.info(\"Connecting to database...\")\n</code></pre>"},{"location":"guides/best_practices/#10-version-control","title":"10. Version Control","text":""},{"location":"guides/best_practices/#git-ignore-list","title":"Git Ignore List","text":"<pre><code># .gitignore\n.env\n*.pyc\n__pycache__/\n.odibi/\nstories/\n*.log\n.venv/\n</code></pre>"},{"location":"guides/best_practices/#commit-guidelines","title":"Commit Guidelines","text":"Change Type Commit Message New pipeline <code>feat(bronze): add customer ingestion pipeline</code> New node <code>feat(silver): add order enrichment node</code> Bug fix <code>fix(gold): correct revenue calculation</code> Config change <code>chore: update retry settings</code>"},{"location":"guides/best_practices/#branch-strategy","title":"Branch Strategy","text":"<pre><code>main           # Production-ready pipelines\n\u251c\u2500\u2500 develop    # Integration branch\n\u251c\u2500\u2500 feature/*  # New pipelines/nodes\n\u2514\u2500\u2500 fix/*      # Bug fixes\n</code></pre>"},{"location":"guides/best_practices/#pr-checklist","title":"PR Checklist","text":"<ul> <li>[ ] Pipeline runs locally without errors</li> <li>[ ] Node descriptions added</li> <li>[ ] Sensitive columns marked</li> <li>[ ] Incremental config for large tables</li> <li>[ ] Tests pass</li> </ul>"},{"location":"guides/best_practices/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/best_practices/#project-organization-cheat-sheet","title":"Project Organization Cheat Sheet","text":"<pre><code>project.yaml          \u2192 Connections, settings, imports (NO pipelines)\npipelines/{layer}/    \u2192 One YAML per pipeline\ntransformations/      \u2192 Custom Python code\n.env                  \u2192 Local secrets (git-ignored)\n</code></pre>"},{"location":"guides/best_practices/#node-checklist","title":"Node Checklist","text":"<ul> <li>[ ] Descriptive name (<code>clean_orders</code> not <code>node_1</code>)</li> <li>[ ] Description explaining purpose</li> <li>[ ] Tags for filtering (<code>daily</code>, <code>critical</code>)</li> <li>[ ] <code>cache: true</code> if used by multiple nodes</li> <li>[ ] <code>sensitive</code> for PII columns</li> <li>[ ] Incremental config for large tables</li> </ul>"},{"location":"guides/best_practices/#performance-checklist","title":"Performance Checklist","text":"<ul> <li>[ ] <code>use_arrow: true</code> for Pandas</li> <li>[ ] Incremental loading for large sources</li> <li>[ ] <code>skip_if_unchanged</code> for dimensions</li> <li>[ ] <code>skip_null_profiling</code> for very large tables</li> <li>[ ] <code>cluster_by</code> for Spark/Delta</li> </ul>"},{"location":"guides/best_practices/#related-documentation","title":"Related Documentation","text":"<ul> <li>The Definitive Guide \u2014 Deep dive into architecture</li> <li>Performance Tuning \u2014 Optimization details</li> <li>Production Deployment \u2014 Going to production</li> <li>Cross-Pipeline Dependencies \u2014 <code>$pipeline.node</code> references</li> <li>Configuration Reference \u2014 Full YAML schema</li> </ul>"},{"location":"guides/cli_master_guide/","title":"\ud83d\udcbb Odibi CLI: Zero to Hero","text":"<p>Ultimate Cheatsheet &amp; Reference (v2.4.0)</p> <p>The Command Line Interface (CLI) is your primary tool for managing Odibi projects.</p>"},{"location":"guides/cli_master_guide/#level-1-the-basics","title":"\ud83d\udfe2 Level 1: The Basics","text":""},{"location":"guides/cli_master_guide/#1-create-a-new-pipeline-file","title":"1. Create a New Pipeline File","text":"<p>Generate a \"Master Kitchen Sink\" reference file with all features enabled.</p> <pre><code>odibi create my_pipeline.yaml\n</code></pre>"},{"location":"guides/cli_master_guide/#2-run-a-pipeline","title":"2. Run a Pipeline","text":"<p>Execute the pipeline defined in your YAML file.</p> <pre><code>odibi run my_pipeline.yaml\n</code></pre> <p>Common Flags: *   <code>--dry-run</code>: Simulate execution (don't write data). *   <code>--resume</code>: Resume from the last failure (skips successful nodes). *   <code>--env prod</code>: Load production environment variables.</p>"},{"location":"guides/cli_master_guide/#level-2-intermediate-management","title":"\ud83d\udfe1 Level 2: Intermediate (Management)","text":""},{"location":"guides/cli_master_guide/#1-initialize-a-full-project","title":"1. Initialize a Full Project","text":"<p>Don't just create a file; create a full folder structure with best practices (Bronze/Silver/Gold layers).</p> <pre><code># Creates folder 'my_project' with organized subfolders\nodibi init-pipeline my_project --template kitchen-sink\n</code></pre>"},{"location":"guides/cli_master_guide/#2-validate-configuration","title":"2. Validate Configuration","text":"<p>Check if your YAML is valid before running it.</p> <pre><code>odibi validate my_pipeline.yaml\n</code></pre>"},{"location":"guides/cli_master_guide/#3-visualize-dependencies","title":"3. Visualize Dependencies","text":"<p>Generate a dependency graph to understand flow.</p> <pre><code># ASCII Art (Default)\nodibi graph my_pipeline.yaml\n\n# Mermaid Diagram (for Markdown)\nodibi graph my_pipeline.yaml --format mermaid\n</code></pre>"},{"location":"guides/cli_master_guide/#level-3-hero-advanced-tools","title":"\ud83d\udd34 Level 3: Hero (Advanced Tools)","text":""},{"location":"guides/cli_master_guide/#1-deep-diff-compare-runs","title":"1. Deep Diff (Compare Runs)","text":"<p>Did a pipeline run suddenly output fewer rows? Use <code>story diff</code> to compare two runs.</p> <pre><code># List available runs\nodibi story list\n\n# Compare two story JSON files\nodibi story diff stories/runs/20231027_120000.json stories/runs/20231027_120500.json\n</code></pre> <p>Output: Shows execution time differences, row count changes, and success rates.</p>"},{"location":"guides/cli_master_guide/#2-manage-secrets","title":"2. Manage Secrets","text":"<p>Securely manage local secrets for your pipelines.</p> <pre><code># Initialize secrets store (creates .env.template)\nodibi secrets init\n\n# Validate all secrets are configured\nodibi secrets validate\n</code></pre>"},{"location":"guides/cli_master_guide/#level-4-system-catalog-the-brain","title":"\ud83e\udde0 Level 4: System Catalog (The Brain)","text":""},{"location":"guides/cli_master_guide/#query-the-system-catalog","title":"Query the System Catalog","text":"<p>The System Catalog stores metadata about all your runs, pipelines, nodes, and state. Query it without manually reading Delta tables.</p> <pre><code># List recent runs\nodibi catalog runs config.yaml\n\n# Filter by pipeline and status\nodibi catalog runs config.yaml --pipeline my_etl --status SUCCESS --days 14\n\n# List registered pipelines\nodibi catalog pipelines config.yaml\n\n# List nodes (optionally filter by pipeline)\nodibi catalog nodes config.yaml --pipeline my_etl\n\n# View HWM state checkpoints\nodibi catalog state config.yaml\n\n# Get execution statistics\nodibi catalog stats config.yaml --days 30\n</code></pre> <p>Catalog Subcommands: | Subcommand | Description | | :--- | :--- | | <code>runs</code> | List execution runs from <code>meta_runs</code> | | <code>pipelines</code> | List registered pipelines from <code>meta_pipelines</code> | | <code>nodes</code> | List registered nodes from <code>meta_nodes</code> | | <code>state</code> | List HWM state checkpoints from <code>meta_state</code> | | <code>tables</code> | List registered assets from <code>meta_tables</code> | | <code>metrics</code> | List metrics definitions from <code>meta_metrics</code> | | <code>patterns</code> | List pattern compliance from <code>meta_patterns</code> | | <code>stats</code> | Show execution statistics (success rate, avg duration, etc.) |</p> <p>Common Flags: * <code>--format json</code>: Output as JSON instead of ASCII table * <code>--pipeline &lt;name&gt;</code>: Filter by pipeline name * <code>--days &lt;n&gt;</code>: Show data from last N days (default: 7) * <code>--limit &lt;n&gt;</code>: Limit number of results (default: 20)</p>"},{"location":"guides/cli_master_guide/#level-5-schema-lineage-tracking","title":"\ud83d\udd0d Level 5: Schema &amp; Lineage Tracking","text":""},{"location":"guides/cli_master_guide/#schema-version-history","title":"Schema Version History","text":"<p>Track how table schemas evolve over time.</p> <pre><code># View schema history for a table\nodibi schema history silver/customers --config config.yaml\n\n# Compare two schema versions\nodibi schema diff silver/customers --config config.yaml --from-version 3 --to-version 5\n\n# Output as JSON\nodibi schema history silver/customers --config config.yaml --format json\n</code></pre> <p>Example Output:</p> <pre><code>Schema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre>"},{"location":"guides/cli_master_guide/#cross-pipeline-lineage","title":"Cross-Pipeline Lineage","text":"<p>Trace data dependencies across pipelines.</p> <pre><code># Trace upstream sources\nodibi lineage upstream gold/customer_360 --config config.yaml\n\n# Trace downstream consumers\nodibi lineage downstream bronze/customers_raw --config config.yaml\n\n# Impact analysis - what would be affected by changes?\nodibi lineage impact bronze/customers_raw --config config.yaml\n</code></pre> <p>Example Output (upstream):</p> <pre><code>Upstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre> <p>Example Output (impact):</p> <pre><code>\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 3 downstream table(s) in 2 pipeline(s)\n</code></pre> <p>Schema Subcommands: | Subcommand | Description | | :--- | :--- | | <code>history</code> | View schema version history for a table | | <code>diff</code> | Compare two schema versions |</p> <p>Lineage Subcommands: | Subcommand | Description | | :--- | :--- | | <code>upstream</code> | Trace upstream sources of a table | | <code>downstream</code> | Trace downstream consumers of a table | | <code>impact</code> | Impact analysis for schema changes |</p> <p>Common Flags: * <code>--config &lt;path&gt;</code>: Path to YAML config file (required) * <code>--depth &lt;n&gt;</code>: Maximum depth to traverse (default: 3) * <code>--format json</code>: Output as JSON * <code>--limit &lt;n&gt;</code>: Limit results (schema history only)</p>"},{"location":"guides/cli_master_guide/#command-reference","title":"\ud83d\udcc4 Command Reference","text":"Command Description <code>run</code> Execute a pipeline. <code>create</code> Create a single YAML config file. <code>init-pipeline</code> Scaffold a full project directory. <code>validate</code> Check YAML syntax and logic. <code>graph</code> Visualize pipeline dependencies. <code>story</code> Manage and compare execution reports (<code>generate</code>, <code>diff</code>, <code>list</code>). <code>secrets</code> Manage local secure secrets (<code>init</code>, <code>validate</code>). <code>catalog</code> Query System Catalog (<code>runs</code>, <code>pipelines</code>, <code>nodes</code>, <code>state</code>, <code>stats</code>). <code>schema</code> Schema version tracking (<code>history</code>, <code>diff</code>). <code>lineage</code> Cross-pipeline lineage (<code>upstream</code>, <code>downstream</code>, <code>impact</code>). <code>init-vscode</code> Setup VS Code environment."},{"location":"guides/dimensional_modeling_guide/","title":"Dimensional Modeling Guide for ODIBI","text":"<p>A practical reference for building data warehouses with ODIBI.</p>"},{"location":"guides/dimensional_modeling_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Problem We're Solving</li> <li>Facts and Dimensions</li> <li>The Star Schema</li> <li>Natural Keys vs Surrogate Keys</li> <li>Bronze \u2192 Silver \u2192 Gold Flow</li> <li>Where Do IDs Come From?</li> <li>Lookup Tables vs Dimension Tables</li> <li>Slowly Changing Dimensions (SCD)</li> <li>The Date Dimension</li> <li>Aggregations</li> <li>Common Mistakes to Avoid</li> <li>ODIBI Current State vs Target</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#the-problem-were-solving","title":"The Problem We're Solving","text":""},{"location":"guides/dimensional_modeling_guide/#raw-data-is-hard-to-query","title":"Raw Data is Hard to Query","text":"<p>Your source systems store data for operations, not analysis:</p> <pre><code>orders.csv\n| customer_email    | product_name | quantity | price | timestamp           |\n|-------------------|--------------|----------|-------|---------------------|\n| john@mail.com     | Latte        | 2        | 11.00 | 2024-01-15 09:15:00 |\n</code></pre> <p>To answer \"What was revenue by product category for Q4 weekends?\", you need: - Product category (not in orders) - Whether it's a weekend (calculated from timestamp) - Q4 filter (calculated from timestamp)</p> <p>Dimensional modeling pre-calculates and organizes this context.</p>"},{"location":"guides/dimensional_modeling_guide/#facts-and-dimensions","title":"Facts and Dimensions","text":""},{"location":"guides/dimensional_modeling_guide/#fact-table-what-happened","title":"Fact Table = What Happened","text":"<p>Events, transactions, measurements. Numbers you can add/count/average.</p> Contains Example Measures (numbers) <code>quantity</code>, <code>price</code>, <code>total_amount</code> Foreign keys (pointers to dimensions) <code>customer_sk</code>, <code>product_sk</code>, <code>date_sk</code> Degenerate dimensions (IDs with no table) <code>order_id</code>, <code>invoice_number</code> <p>Key insight: Facts are usually append-only. Once it happened, it happened.</p>"},{"location":"guides/dimensional_modeling_guide/#dimension-table-context-about-what-happened","title":"Dimension Table = Context About What Happened","text":"<p>Who, what, when, where, why. Descriptive attributes.</p> Contains Example Primary key <code>customer_sk</code> Natural key <code>customer_id</code> (from source system) Descriptive attributes <code>name</code>, <code>email</code>, <code>city</code>, <code>segment</code> Hierarchies <code>city</code> \u2192 <code>state</code> \u2192 <code>country</code> <p>Key insight: Dimensions change over time. A customer might move cities.</p>"},{"location":"guides/dimensional_modeling_guide/#the-star-schema","title":"The Star Schema","text":"<pre><code>                    dim_customer\n                         \u2502\n                         \u2502 customer_sk\n                         \u2502\ndim_product \u2500\u2500\u2500\u2500\u2500\u2500\u2500 fact_orders \u2500\u2500\u2500\u2500\u2500\u2500\u2500 dim_date\n         product_sk      \u2502        date_sk\n                         \u2502\n                    dim_region\n                         \u2502\n                    region_sk\n</code></pre> <p>Why this shape?</p> <ol> <li>Storage efficiency \u2014 Store \"John Smith, Premium, NYC\" once, reference by ID</li> <li>Query speed \u2014 Filter small dimension tables first, then join to facts</li> <li>Flexibility \u2014 Add new attributes to dimensions, all reports get them</li> <li>Single source of truth \u2014 Customer's segment defined in ONE place</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#natural-keys-vs-surrogate-keys","title":"Natural Keys vs Surrogate Keys","text":"Type What It Is Example Who Creates It Natural Key Business identifier <code>customer_id</code>, <code>email</code>, <code>product_sku</code> Source system or you (via hash) Surrogate Key Warehouse-generated integer <code>customer_sk = 1001</code> Data warehouse (auto-increment)"},{"location":"guides/dimensional_modeling_guide/#why-use-surrogate-keys","title":"Why Use Surrogate Keys?","text":"<ol> <li>Faster JOINs \u2014 Integers are faster than strings</li> <li>SCD2 support \u2014 One <code>customer_id</code> can have multiple <code>customer_sk</code> values (history)</li> <li>Survives source changes \u2014 If source system changes IDs, yours don't</li> <li>Unknown member \u2014 <code>customer_sk = 0</code> for orphan records</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#the-unknown-member","title":"The \"Unknown\" Member","text":"<p>What if an order references <code>customer_id = 999</code> but that customer doesn't exist?</p> <p>Solution: Create a special row in every dimension:</p> customer_sk customer_id name city 0 -1 Unknown Unknown 1001 47 John NYC <p>Orphan facts JOIN to <code>customer_sk = 0</code> \u2014 you never lose data.</p>"},{"location":"guides/dimensional_modeling_guide/#bronze-silver-gold-flow","title":"Bronze \u2192 Silver \u2192 Gold Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           BRONZE                                     \u2502\n\u2502   Raw data, as-is from source. Messy, duplicates, nulls, no IDs.    \u2502\n\u2502   Example: raw_orders.csv, raw_customers.json                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           SILVER                                     \u2502\n\u2502   Cleaned, validated, deduplicated. NATURAL KEYS assigned.          \u2502\n\u2502   Example: clean_orders, clean_customers                             \u2502\n\u2502   Keys: customer_id (hash or from source), product_id, order_id     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            GOLD                                      \u2502\n\u2502   Dimensional model. SURROGATE KEYS, SCD history, aggregates.       \u2502\n\u2502   Example: dim_customer, dim_product, fact_orders, agg_daily_sales  \u2502\n\u2502   Keys: customer_sk, product_sk, date_sk (integers)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#key-point-silver-uses-natural-keys-gold-uses-surrogate-keys","title":"Key Point: Silver Uses Natural Keys, Gold Uses Surrogate Keys","text":"Layer Keys Used Bronze Whatever source provides (emails, names, raw IDs) Silver Natural keys \u2014 business identifiers (customer_id, product_id) Gold Surrogate keys \u2014 warehouse-generated integers (customer_sk)"},{"location":"guides/dimensional_modeling_guide/#where-do-ids-come-from","title":"Where Do IDs Come From?","text":""},{"location":"guides/dimensional_modeling_guide/#scenario-a-source-system-has-ids-common","title":"Scenario A: Source System Has IDs (Common)","text":"<p>Your CRM already has <code>customer_id = 47</code>. Just pass it through.</p> <pre><code>Source \u2192 Bronze \u2192 Silver \u2192 Gold\n  47       47       47      + customer_sk = 1001\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#scenario-b-source-system-has-no-ids-your-data","title":"Scenario B: Source System Has NO IDs (Your Data)","text":"<p>You only have <code>email</code> and <code>product_name</code>. You must generate IDs.</p> <p>Option 1: Hash (Recommended)</p> <pre><code>SELECT MD5(LOWER(TRIM(email))) as customer_id, email, name\nFROM clean_customers\n</code></pre> <ul> <li>Same email = same ID, forever</li> <li>Deterministic, stateless, simple</li> <li>Ugly IDs but who cares</li> </ul> <p>Option 2: Persistent Lookup Table</p> <ul> <li>Store <code>email \u2192 customer_id</code> mapping</li> <li>On each run, only assign new IDs to new emails</li> <li>Nice sequential numbers (1, 2, 3...)</li> <li>More complex, requires state</li> </ul> <p>\u26a0\ufe0f NEVER use RANK() or ROW_NUMBER() alone \u2014 IDs will shift when data changes!</p>"},{"location":"guides/dimensional_modeling_guide/#lookup-tables-vs-dimension-tables","title":"Lookup Tables vs Dimension Tables","text":"<p>These are NOT the same thing.</p> Lookup Table Dimension Table Silver layer Gold layer Maps <code>email \u2192 customer_id</code> Has <code>customer_sk, customer_id, name, city, ...</code> Just a helper for ID generation Official, versioned, surrogate-keyed entity No history SCD2 history tracking Simple key-value Rich with attributes"},{"location":"guides/dimensional_modeling_guide/#the-flow","title":"The Flow","text":"<pre><code>Bronze: raw_customers (email, name, city)\n    \u2502\n    \u25bc\nSilver: customer_lookup (email \u2192 customer_id via hash)\n    \u2502\n    \u25bc\nSilver: clean_customers (customer_id, email, name, city)\n    \u2502\n    \u25bc\nGold: dim_customer (customer_SK, customer_id, name, city, valid_from, valid_to, is_current)\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>What happens when a customer moves from NYC to LA?</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-0-keep-original","title":"SCD Type 0: Keep Original","text":"<p>Never update. Always shows original value.</p> <p>Use case: Birth date, original signup date</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-1-overwrite","title":"SCD Type 1: Overwrite","text":"<p>Update in place. Lose history.</p> customer_sk customer_id city 1001 47 LA <p>Use case: Correcting typos, current-state-only reporting</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-2-track-history-most-common","title":"SCD Type 2: Track History (Most Common)","text":"<p>Create new row, close old row.</p> customer_sk customer_id city valid_from valid_to is_current 1001 47 NYC 2020-01-01 2023-03-15 false 1002 47 LA 2023-03-15 NULL true <p>Use case: Historical analysis, \"What was their city when they ordered?\"</p>"},{"location":"guides/dimensional_modeling_guide/#looking-up-surrogate-keys-for-facts","title":"Looking Up Surrogate Keys for Facts","text":"<p>When building fact tables, filter by <code>is_current = true</code>:</p> <pre><code>SELECT \n  o.order_id,\n  dc.customer_sk\nFROM clean_orders o\nLEFT JOIN dim_customer dc \n  ON o.customer_id = dc.customer_id \n  AND dc.is_current = true  -- Only match current version!\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#the-date-dimension","title":"The Date Dimension","text":"<p>Every business question involves time. Pre-calculate all date attributes.</p> Column Example Why Useful date_sk 20240115 Primary key full_date 2024-01-15 Actual date day_of_week Monday Filter by weekday is_weekend false Weekend analysis month 1 Monthly aggregation month_name January Display-friendly quarter 1 Quarterly reporting year 2024 Annual comparison is_holiday false Holiday impact"},{"location":"guides/dimensional_modeling_guide/#why-pre-calculate","title":"Why Pre-Calculate?","text":"<p>Without date dimension:</p> <pre><code>-- Calculates for every row\nWHERE DAYOFWEEK(order_date) IN (1, 7)\n</code></pre> <p>With date dimension:</p> <pre><code>-- Pre-calculated, indexed, fast\nWHERE d.is_weekend = true\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#aggregations","title":"Aggregations","text":""},{"location":"guides/dimensional_modeling_guide/#why-pre-aggregate","title":"Why Pre-Aggregate?","text":"<p>Without aggregates:</p> <pre><code>-- Scans 1 BILLION rows every time\nSELECT SUM(revenue) FROM fact_sales WHERE month = 'January'\n</code></pre> <p>With aggregates:</p> <pre><code>-- Scans 31 rows\nSELECT SUM(daily_revenue) FROM agg_daily_sales WHERE month = 'January'\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#the-grain-concept","title":"The Grain Concept","text":"<p>Grain = what one row represents</p> Table Grain fact_sales One order line item agg_daily_sales All sales for a day + product + region agg_monthly_sales All sales for a month + product + region <p>Rule: You can roll UP (daily \u2192 monthly) but not drill DOWN (monthly \u2192 daily) without the source.</p>"},{"location":"guides/dimensional_modeling_guide/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":""},{"location":"guides/dimensional_modeling_guide/#using-rank-to-generate-ids","title":"\u274c Using RANK() to Generate IDs","text":"<pre><code>-- WRONG: IDs will shift when new data arrives!\nSELECT DENSE_RANK() OVER (ORDER BY email) as customer_id\n</code></pre> <p>Fix: Use hash or persistent lookup table.</p>"},{"location":"guides/dimensional_modeling_guide/#building-lookups-from-bronze-dirty-data","title":"\u274c Building Lookups from Bronze (Dirty Data)","text":"<pre><code>-- WRONG: Bronze has duplicates, nulls, deleted records\nSELECT DISTINCT email FROM raw_customers\n</code></pre> <p>Fix: Build lookups from Silver (cleaned data).</p>"},{"location":"guides/dimensional_modeling_guide/#joining-fact-to-dimension-without-is_current-filter","title":"\u274c Joining Fact to Dimension Without is_current Filter","text":"<pre><code>-- WRONG: May get multiple matches (historical + current)\nSELECT * FROM fact_orders f\nJOIN dim_customer dc ON f.customer_id = dc.customer_id\n</code></pre> <p>Fix: Add <code>AND dc.is_current = true</code>.</p>"},{"location":"guides/dimensional_modeling_guide/#confusing-lookup-tables-with-dimension-tables","title":"\u274c Confusing Lookup Tables with Dimension Tables","text":"<ul> <li>Lookup = Silver, just maps keys</li> <li>Dimension = Gold, has surrogate keys + history + attributes</li> </ul>"},{"location":"guides/dimensional_modeling_guide/#not-having-an-unknown-member","title":"\u274c Not Having an Unknown Member","text":"<p>If a fact references a customer that doesn't exist, the JOIN fails and you lose data.</p> <p>Fix: Every dimension should have <code>SK = 0, name = 'Unknown'</code>.</p>"},{"location":"guides/dimensional_modeling_guide/#odibi-current-state-vs-target","title":"ODIBI Current State vs Target","text":""},{"location":"guides/dimensional_modeling_guide/#what-odibi-has-now","title":"What ODIBI Has Now","text":"Pattern What It Does Limitation <code>FactPattern</code> Dedup + pass through No SK lookup, no orphan handling <code>SCD2Pattern</code> Track history No auto surrogate key <code>MergePattern</code> Upsert logic \u2014 <code>SnapshotPattern</code> Point-in-time capture \u2014 <code>generate_surrogate_key</code> Hash-based key Not integrated into patterns"},{"location":"guides/dimensional_modeling_guide/#what-were-adding","title":"What We're Adding","text":"Pattern What It Will Do <code>DimensionPattern</code> Auto SK + SCD + unknown member + audit columns <code>DateDimensionPattern</code> Generate complete date dimension <code>Enhanced FactPattern</code> Auto SK lookups + orphan handling + grain validation <code>AggregationPattern</code> Declarative GROUP BY + time rollups"},{"location":"guides/dimensional_modeling_guide/#target-declarative-dimensional-modeling","title":"Target: Declarative Dimensional Modeling","text":"<pre><code># This should just work\n- name: dim_customer\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      unknown_member: true\n\n- name: fact_orders\n  pattern:\n    type: fact\n    params:\n      grain: [order_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          surrogate_key: customer_sk\n      orphan_handling: unknown\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#quick-reference-the-mental-model","title":"Quick Reference: The Mental Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        BUSINESS QUESTION                             \u2502\n\u2502   \"What was revenue by product category for Q4 weekends?\"           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         GOLD LAYER                                   \u2502\n\u2502   fact_orders \u2190\u2192 dim_product \u2190\u2192 dim_date                            \u2502\n\u2502   Surrogate keys, SCD2 history, pre-aggregated                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        SILVER LAYER                                  \u2502\n\u2502   clean_orders, clean_customers, customer_lookup                    \u2502\n\u2502   Natural keys (hash or source), validated, deduplicated            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        BRONZE LAYER                                  \u2502\n\u2502   raw_orders.csv, raw_customers.json                                \u2502\n\u2502   As-is from source, messy, no IDs if source doesn't have them     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#glossary","title":"Glossary","text":"Term Definition Fact Table of events/transactions with measures Dimension Table of context (who, what, when, where) Grain What one row represents Natural Key Business identifier (customer_id) Surrogate Key Warehouse-generated integer (customer_sk) SCD Slowly Changing Dimension Star Schema Fact in center, dimensions around it Lookup Table Helper table to map values to IDs Unknown Member Row with SK=0 for orphan handling Aggregate Pre-calculated summary table"},{"location":"guides/dogfooding/","title":"\ud83d\udc36 Dogfooding Guide","text":"<p>\"Eat your own dog food.\"</p> <p>This guide explains how we use Odibi to build Odibi, and how you can use the <code>odibi-metrics</code> project to validate your own environment.</p>"},{"location":"guides/dogfooding/#what-is-dogfooding","title":"What is Dogfooding?","text":"<p>Dogfooding means using your own product to do your actual job.</p> <p>Instead of testing Odibi with synthetic data (like \"foo\", \"bar\", \"test_1\"), we use it to track the development velocity of the Odibi framework itself. This forces us to encounter real-world problems\u2014messy API data, rate limits, Unicode errors, schema drift\u2014before our users do.</p>"},{"location":"guides/dogfooding/#the-odibi-metrics-pipeline","title":"The <code>odibi-metrics</code> Pipeline","text":"<p>We have included a reference implementation in <code>examples/odibi-metrics</code>. This is a real pipeline that:</p> <ol> <li>Extracts: Connects to the GitHub API to fetch Issues and PRs from <code>henryodibi11/Odibi</code>.</li> <li>Transforms: Cleans the data, handles timezones, and calculates weekly velocity (opened vs. closed tasks).</li> <li>Loads: Saves the results to a Gold layer (<code>velocity.csv</code>) and updates the System Catalog.</li> </ol>"},{"location":"guides/dogfooding/#how-to-run-it","title":"How to Run It","text":"<pre><code># 1. Go to the example directory\ncd examples/odibi-metrics\n\n# 2. Run the pipeline\nodibi run odibi.yaml\n</code></pre>"},{"location":"guides/dogfooding/#what-it-teaches-you","title":"What It Teaches You","text":"<p>If this pipeline fails, it means Odibi is not stable enough for production. We found (and fixed) the following issues using this exact pipeline:</p> <ul> <li>\u274c Unicode Errors: Windows consoles crashing on emoji output.</li> <li>\u274c Schema Drift: The System Catalog failing when new metadata fields were added.</li> <li>\u274c Timezone Bugs: Pandas crashing when grouping TZ-aware dates.</li> </ul>"},{"location":"guides/dogfooding/#how-to-file-issues","title":"How to File Issues","text":"<p>When you find a bug while running Odibi (or the dogfood pipeline), you should track it using GitHub Issues.</p> <ol> <li>Go to the Issues Tab on GitHub.</li> <li>Click New Issue.</li> <li>Title: Short summary (e.g., \"Crash on Windows 11\").</li> <li>Body: Paste the error log and your <code>odibi.yaml</code>.</li> </ol> <p>The Meta-Loop: Once you file the issue, the <code>odibi-metrics</code> pipeline will actually download that issue the next time it runs, adding it to your project statistics. You are using Odibi to measure how fast you are fixing Odibi.</p>"},{"location":"guides/environments/","title":"Managing Environments","text":"<p>Odibi allows you to define a single pipeline configuration that adapts to different contexts (e.g., Local Development, Testing, Production) using the <code>environments</code> block. This prevents configuration drift and ensures your pipeline logic remains consistent while infrastructure details change.</p>"},{"location":"guides/environments/#how-it-works","title":"How it Works","text":"<p>Odibi uses a Base Configuration + Override model: 1.  Base Configuration: Defines your default settings (typically for local development). 2.  Environment Overrides: Specific blocks that patch or replace values in the base configuration when that environment is active.</p>"},{"location":"guides/environments/#configuration-structure","title":"Configuration Structure","text":"<p>Odibi supports two ways to define environments: 1.  Inline Block: Using an <code>environments</code> block in your main config file. 2.  External Files: Using separate <code>env.{env}.yaml</code> files (e.g., <code>env.prod.yaml</code>).</p>"},{"location":"guides/environments/#method-1-inline-block","title":"Method 1: Inline Block","text":"<p>Add an <code>environments</code> section to your <code>project.yaml</code>:</p> <pre><code># ... base config ...\nenvironments:\n  prod:\n    engine: spark\n</code></pre>"},{"location":"guides/environments/#method-2-external-files-recommended-for-large-configs","title":"Method 2: External Files (Recommended for large configs)","text":"<p>Keep your main <code>odibi.yaml</code> clean by putting overrides in separate files.</p> <p>File: <code>odibi.yaml</code></p> <pre><code>project: Sales Data Pipeline\nengine: pandas\nconnections:\n  data_lake:\n    type: local\n    base_path: ./data\n</code></pre> <p>File: <code>env.prod.yaml</code></p> <pre><code># Automatically merged when running with --env prod\nengine: spark\nconnections:\n  data_lake:\n    type: azure_adls\n    account: prod_acc\n</code></pre> <p>When you run <code>odibi run odibi.yaml --env prod</code>, Odibi will: 1. Load <code>odibi.yaml</code>. 2. Look for <code>env.prod.yaml</code> in the same directory. 3. Merge the prod config on top of the base config.</p>"},{"location":"guides/environments/#inline-example-method-1","title":"Inline Example (Method 1)","text":"<pre><code># --- 1. Base Configuration (Default / Local) ---\nproject: Sales Data Pipeline\nengine: pandas\nretry:\n  enabled: false\n\nconnections:\n  data_lake:\n    type: local\n    base_path: ./data/raw\n\npipelines:\n  - pipeline: ingest_sales\n    nodes:\n      - name: read_csv\n        read:\n          connection: data_lake\n          path: sales.csv\n\n# --- 2. Environment Overrides ---\nenvironments:\n  # Production Environment\n  prod:\n    engine: spark  # Switch to Spark for scale\n    retry:\n      enabled: true\n      max_attempts: 3\n    connections:\n      data_lake:\n        type: azure_adls\n        account: mycompanyprod\n        container: sales-data\n        auth_mode: managed_identity\n    story:\n      max_sample_rows: 0 # Disable data sampling for security\n\n  # Testing Environment\n  test:\n    connections:\n      data_lake:\n        type: local\n        base_path: ./data/test_fixtures\n</code></pre>"},{"location":"guides/environments/#usage","title":"Usage","text":""},{"location":"guides/environments/#cli","title":"CLI","text":"<p>Use the <code>--env</code> flag to activate an environment.</p> <p>Run in Default (Base) Environment:</p> <pre><code>odibi run project.yaml\n</code></pre> <p>Run in Production:</p> <pre><code>odibi run project.yaml --env prod\n</code></pre>"},{"location":"guides/environments/#python-api","title":"Python API","text":"<p>Pass the <code>env</code> parameter when initializing the <code>PipelineManager</code>.</p> <pre><code>from odibi.pipeline import PipelineManager\n\n# Load Prod Configuration\nmanager = PipelineManager.from_yaml(\"project.yaml\", env=\"prod\")\n\n# Run Pipeline\nmanager.run(\"ingest_sales\")\n</code></pre>"},{"location":"guides/environments/#databricks-example","title":"Databricks Example","text":"<p>In a Databricks notebook, you can use widgets to switch environments dynamically without changing code.</p> <pre><code># 1. Create Widget\ndbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"test\", \"prod\"])\n\n# 2. Get Selection\ncurrent_env = dbutils.widgets.get(\"environment\")\n\n# 3. Run Pipeline\nmanager = PipelineManager.from_yaml(\"/dbfs/project.yaml\", env=current_env)\nmanager.run()\n</code></pre>"},{"location":"guides/environments/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/environments/#1-swapping-storage-local-vs-cloud","title":"1. Swapping Storage (Local vs. Cloud)","text":"<p>Develop locally with CSVs, deploy to ADLS/S3 without changing pipeline code.</p> <pre><code>connections:\n  storage: { type: local, base_path: ./data }\n\nenvironments:\n  prod:\n    connections:\n      storage: { type: azure_adls, account: prod_acc, container: data }\n</code></pre>"},{"location":"guides/environments/#2-scaling-engines-pandas-vs-spark","title":"2. Scaling Engines (Pandas vs. Spark)","text":"<p>Use Pandas for fast local iteration and unit tests, but switch to Spark for distributed processing in production.</p> <pre><code>engine: pandas\n\nenvironments:\n  prod:\n    engine: spark\n</code></pre>"},{"location":"guides/environments/#3-security-privacy","title":"3. Security &amp; Privacy","text":"<p>Disable data sampling in stories for production to prevent PII leakage, while keeping it enabled in dev for debugging.</p> <pre><code>story:\n  max_sample_rows: 20\n\nenvironments:\n  prod:\n    story:\n      max_sample_rows: 0\n</code></pre>"},{"location":"guides/environments/#4-alerting","title":"4. Alerting","text":"<p>Only send Slack/Teams notifications when running in production.</p> <pre><code>alerts: []  # No alerts in dev\n\nenvironments:\n  prod:\n    alerts:\n      - type: slack\n        url: ${SLACK_WEBHOOK}\n</code></pre>"},{"location":"guides/performance_tuning/","title":"Performance Tuning Guide","text":"<p>Odibi v2.2 introduces a \"High-Performance Core\" designed to handle everything from local laptop development to petabyte-scale Spark jobs. This guide explains the optimizations available and how to use them.</p>"},{"location":"guides/performance_tuning/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>For 90% of users, just add this to your <code>odibi.yaml</code>:</p> <pre><code>performance:\n  use_arrow: true  # Massive speedup for Pandas I/O\n</code></pre>"},{"location":"guides/performance_tuning/#1-pandas-engine-optimizations","title":"1. Pandas Engine Optimizations","text":"<p>The Pandas engine is designed for speed on a single machine.</p>"},{"location":"guides/performance_tuning/#apache-arrow-backend-use_arrow-true","title":"\ud83c\udff9 Apache Arrow Backend (<code>use_arrow: true</code>)","text":"<p>What it does: Replaces standard NumPy memory layout with Apache Arrow. Arrow is a columnar memory format that allows \"Zero-Copy\" data transfer.</p> <p>Why use it? -   Speed: Reading Parquet files becomes nearly instant because the data maps directly from disk to memory without conversion overhead. -   Memory: Reduces RAM usage by ~50% for string-heavy datasets (no more Python objects for strings).</p> <p>Configuration:</p> <pre><code># odibi.yaml\nperformance:\n  use_arrow: true\n</code></pre>"},{"location":"guides/performance_tuning/#parallel-file-io-multi-threading","title":"\u26a1 Parallel File I/O (Multi-Threading)","text":"<p>What it does: When reading multiple files (e.g., <code>path: data/sales_*.csv</code>), Odibi now uses a thread pool to read them in parallel instead of one by one.</p> <p>Why use it? Pandas is normally single-threaded. If you have 8 CPU cores, reading 50 CSV files sequentially wastes 7 of them. Parallel I/O saturates your CPU/Disk bandwidth for linear speedups.</p> <p>How to use: Automatic! Just use a glob pattern in your path:</p> <pre><code>read:\n  path: raw/data_*.csv  # &lt;--- Parallel reading activates automatically\n</code></pre>"},{"location":"guides/performance_tuning/#2-spark-engine-optimizations","title":"2. Spark Engine Optimizations","text":"<p>The Spark engine focuses on \"Data Layout\" optimizations\u2014making sure downstream queries are fast.</p>"},{"location":"guides/performance_tuning/#liquid-clustering-cluster_by","title":"\ud83d\udca7 Liquid Clustering (<code>cluster_by</code>)","text":"<p>What it does: Replaces traditional Hive-style partitioning (<code>year=2023/month=01</code>) with a flexible, dynamic clustering system. It physically groups related data together in the files.</p> <p>Why use it? -   No \"Small File\" Problem: Traditional partitioning creates too many tiny files if you pick the wrong column (e.g., <code>user_id</code>). Liquid handles this automatically. -   Skew Resistance: Handles uneven data (e.g., 90% of users in US, 1% in JP) without performance cliffs. -   Query Speed: Massive data skipping. Queries filtering by clustered columns skip 99% of the file scans.</p> <p>How to use: Add <code>cluster_by</code> to your write node. If the table doesn't exist, Odibi creates it with clustering enabled.</p> <pre><code>- name: write_sales\n  write:\n    table: silver.sales\n    mode: append\n    options:\n      cluster_by: [region, date]  # &lt;--- Enables Liquid Clustering\n      optimize_write: true        # &lt;--- Keeps clustering healthy\n</code></pre>"},{"location":"guides/performance_tuning/#auto-optimization-optimize_write","title":"\ud83e\uddf9 Auto-Optimization (<code>optimize_write</code>)","text":"<p>What it does: Runs the Delta Lake <code>OPTIMIZE</code> command immediately after a write/merge operation.</p> <p>Why use it? Streaming and frequent batch jobs create \"small files\" (fragmentation) which kill read performance. This option compacts them into larger, efficient files (Bin-packing) and enforces clustering (Z-Order/Liquid).</p> <p>Configuration:</p> <pre><code># In a standard Write node\noptions:\n  optimize_write: true\n\n# In a Merge Transformer\nparams:\n  optimize_write: true\n</code></pre>"},{"location":"guides/performance_tuning/#streaming-support","title":"\ud83c\udf0a Streaming Support","text":"<p>What it does: Allows you to switch from Batch (<code>read</code>/<code>write</code>) to Streaming (<code>readStream</code>/<code>writeStream</code>) with a single flag.</p> <p>Why use it? For real-time latency or processing infinite datasets (Kafka, Auto-Loader) without managing state manually.</p> <p>How to use:</p> <pre><code>- name: read_stream\n  read:\n    streaming: true  # &lt;--- Activates Spark Structured Streaming\n    format: cloudFiles\n    path: raw_landing/\n</code></pre>"},{"location":"guides/performance_tuning/#3-polars-engine-optimizations","title":"3. Polars Engine Optimizations","text":"<p>The Polars engine is a lightweight, fast alternative to Pandas with native Rust performance.</p>"},{"location":"guides/performance_tuning/#lazy-execution","title":"\ud83e\uddba Lazy Execution","text":"<p>What it does: Polars uses a lazy execution model where queries are not executed until you call <code>.collect()</code>. This allows the query optimizer to reorder, combine, and skip operations.</p> <p>Why use it? -   Query Optimization: Predicate pushdown, projection pruning, and filter hoisting happen automatically. -   Memory Efficiency: Only columns you need are loaded into memory. -   Performance: Often 5-10x faster than Pandas for analytical workloads.</p> <p>How to use: Set your engine to Polars and Odibi handles lazy evaluation automatically:</p> <pre><code>engine: polars\n</code></pre>"},{"location":"guides/performance_tuning/#scan-methods-streaming-large-files","title":"\ud83d\udcc2 Scan Methods (Streaming Large Files)","text":"<p>What it does: Uses <code>scan_csv</code>, <code>scan_parquet</code>, and <code>scan_ndjson</code> to read files lazily without loading them entirely into memory.</p> <p>Why use it? Process files larger than RAM by streaming them in chunks.</p> <p>Automatic: Odibi's Polars engine uses scan methods by default for supported formats.</p>"},{"location":"guides/performance_tuning/#native-parallelism","title":"\u26a1 Native Parallelism","text":"<p>What it does: Polars uses all available CPU cores automatically\u2014no configuration needed.</p> <p>Why use it? Unlike Pandas (single-threaded), Polars parallelizes operations like groupby, join, and filter across all cores.</p>"},{"location":"guides/performance_tuning/#summary-cheat-sheet","title":"Summary Cheat Sheet","text":"Optimization Engine Use Case Impact <code>use_arrow: true</code> Pandas Local processing, large Parquet files High (Speed + Memory) Parallel I/O Pandas Reading split CSV/JSON files High (Linear I/O speedup) <code>cluster_by</code> Spark High-cardinality filters, skewed data High (Read performance) <code>optimize_write</code> Spark Frequent writes, streaming, \"small files\" High (Prevents degradation) <code>streaming: true</code> Spark Real-time ingestion Architectural Lazy Execution Polars Analytical workloads, large datasets High (Speed + Memory) Scan Methods Polars Files larger than RAM High (Streaming) Native Parallelism Polars Multi-core utilization High (Automatic)"},{"location":"guides/production_deployment/","title":"\ud83c\udfed Production Deployment","text":"<p>Moving from your laptop to production (e.g., Databricks, Azure Data Factory, Airflow) requires handling secrets, environments, and logging differently.</p>"},{"location":"guides/production_deployment/#1-secrets-management","title":"1. Secrets Management","text":"<p>NEVER commit passwords to Git.</p> <p>Odibi supports environment variable substitution in <code>odibi.yaml</code>. Use the <code>${VAR_NAME}</code> syntax.</p> <p>Bad:</p> <pre><code>connections:\n  db:\n    password: \"super_secret_password\"  # \u274c Security Risk\n</code></pre> <p>Good:</p> <pre><code>connections:\n  db:\n    password: \"${DB_PASSWORD}\"         # \u2705 Safe\n</code></pre> <p>Then, set the environment variable <code>DB_PASSWORD</code> in your production environment (or <code>.env</code> file locally).</p>"},{"location":"guides/production_deployment/#automatic-redaction","title":"Automatic Redaction","text":"<p>Odibi automatically detects values that look like secrets (keys, tokens, passwords) and replaces them with <code>[REDACTED]</code> in logs and Data Stories.</p>"},{"location":"guides/production_deployment/#2-data-privacy-pii","title":"2. Data Privacy &amp; PII","text":"<p>When processing personal data (GDPR/HIPAA), you must ensure that sensitive data does not leak into your logs or execution reports.</p>"},{"location":"guides/production_deployment/#column-level-redaction","title":"Column-Level Redaction","text":"<p>If you want to see non-sensitive data in your reports but hide PII (Personally Identifiable Information), specify the columns list.</p> <pre><code>nodes:\n  - name: ingest_users\n    read: ...\n    # Only masks these columns in the HTML report\n    sensitive: [\"email\", \"ssn\", \"phone\", \"credit_card\"]\n</code></pre>"},{"location":"guides/production_deployment/#full-node-redaction","title":"Full Node Redaction","text":"<p>For highly sensitive nodes (e.g., medical records, financial transactions), you can mask the entire sample.</p> <pre><code>nodes:\n  - name: process_health_records\n    transform: ...\n    # Replaces entire sample with \"[REDACTED: Sensitive Data]\"\n    sensitive: true\n</code></pre> <p>Note: This only affects the Data Story (logs/html). The actual data moving through the pipeline is not modified.</p>"},{"location":"guides/production_deployment/#3-azure-integration","title":"3. Azure Integration","text":"<p>Odibi has native support for Azure resources.</p>"},{"location":"guides/production_deployment/#authentication","title":"Authentication","text":"<p>We support DefaultAzureCredential. This means you don't need to manage keys manually. 1.  Local: It uses your Azure CLI login (<code>az login</code>). 2.  Production: It uses the Managed Identity of the VM/Pod.</p> <pre><code>connections:\n  data_lake:\n    type: azure_adls\n    account: mydatalake\n    auth_mode: key_vault  # Fetches keys from Key Vault automatically\n    key_vault: my-key-vault-name\n</code></pre>"},{"location":"guides/production_deployment/#3-running-on-databricks","title":"3. Running on Databricks","text":"<p>Odibi runs natively on Databricks clusters.</p> <ol> <li>Install: Add <code>odibi[spark,azure]</code> to your cluster libraries.</li> <li>Deploy: Copy your project folder (YAML + SQL) to DBFS or git checkout.</li> <li>Job: Create a job that runs:     <code>bash     odibi run odibi.yaml</code></li> </ol> <p>Tip: Use the \"Spark\" engine for clusters or \"Polars\" engine for high-performance single-node tasks.</p> <pre><code>project: My Big Data Project\nengine: spark  # Options: pandas, polars, spark\n</code></pre>"},{"location":"guides/production_deployment/#4-system-catalog-unified-state","title":"4. System Catalog (Unified State)","text":"<p>Odibi uses a System Catalog (Delta Tables) to track execution history, high-water marks, and metadata. This unifies state management for both local and distributed environments.</p>"},{"location":"guides/production_deployment/#1-local-development-default","title":"1. Local Development (Default)","text":"<p>When running locally, the catalog is automatically created in a hidden directory (<code>.odibi/system/</code>). This uses the <code>deltalake</code> library (Rust core) for high-performance ACID transactions without needing Spark.</p>"},{"location":"guides/production_deployment/#2-production-distributed","title":"2. Production (Distributed)","text":"<p>In production (e.g., Databricks, Kubernetes), you should configure the System Catalog to store state in your Data Lake (ADLS/S3). This allows multiple concurrent pipelines to share state safely.</p> <pre><code>system:\n  connection: \"adls_bronze\"  # Points to your data lake connection\n  path: \"_odibi_system\"      # Directory for system tables\n</code></pre> <p>If utilizing Spark, Odibi leverages Delta Lake's optimistic concurrency control automatically.</p>"},{"location":"guides/production_deployment/#5-monitoring-observability","title":"5. Monitoring &amp; Observability","text":""},{"location":"guides/production_deployment/#openlineage-integration","title":"OpenLineage Integration","text":"<p>Odibi emits standard OpenLineage events. To integrate with DataHub, Marquez, or Atlan:</p> <pre><code>lineage:\n  url: \"http://marquez-api:5000\"\n  namespace: \"odibi-production\"\n</code></pre>"},{"location":"guides/production_deployment/#logging","title":"Logging","text":"<p>Odibi logs structured JSON to stdout by default in production. This is easily ingested by Datadog, Splunk, or Azure Monitor.</p> <pre><code># Force JSON logging\nexport ODIBI_LOG_FORMAT=json\nodibi run odibi.yaml\n</code></pre>"},{"location":"guides/production_deployment/#data-stories-as-artifacts","title":"Data Stories as Artifacts","text":"<p>Configure Odibi to save Data Stories to a permanent location (like an S3 bucket or ADLS container) so you have a permanent audit trail.</p> <pre><code>story:\n  connection: data_lake  # Save reports to the cloud\n  path: audit_reports/\n</code></pre>"},{"location":"guides/python_api_guide/","title":"\ud83d\udc0d Odibi Python API: Zero to Hero","text":"<p>Ultimate Cheatsheet &amp; Reference (v2.4.0)</p> <p>Welcome to the Python API guide. While the CLI is great for running pipelines, the Python API allows you to automate, test, and extend Odibi deeply into your infrastructure.</p>"},{"location":"guides/python_api_guide/#level-1-the-basics-running-pipelines","title":"\ud83d\udfe2 Level 1: The Basics (Running Pipelines)","text":"<p>The core entry point is the <code>PipelineManager</code>. It reads your YAML configuration and manages execution.</p>"},{"location":"guides/python_api_guide/#1-load-and-run","title":"1. Load and Run","text":"<pre><code>from odibi.pipeline import PipelineManager\n\n# 1. Load your project configuration\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\n\n# 2. Run EVERYTHING (All pipelines defined in yaml)\nresults = manager.run()\n\n# 3. Check if it worked\nif results['main_pipeline'].failed:\n    print(\"\u274c Pipeline Failed!\")\nelse:\n    print(\"\u2705 Success!\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-run-a-specific-pipeline","title":"2. Run a Specific Pipeline","text":"<p>If your YAML has multiple pipelines (e.g., <code>ingest</code>, <code>transform</code>, <code>export</code>), run just one:</p> <pre><code># Returns a single PipelineResults object instead of a dict\nresult = manager.run(\"ingest\")\n\nprint(f\"Duration: {result.duration:.2f}s\")\n</code></pre>"},{"location":"guides/python_api_guide/#3-dry-run-simulation","title":"3. Dry Run (Simulation)","text":"<p>Check logic without moving data:</p> <pre><code>manager.run(\"ingest\", dry_run=True)\n</code></pre>"},{"location":"guides/python_api_guide/#level-2-intermediate-inspection-automation","title":"\ud83d\udfe1 Level 2: Intermediate (Inspection &amp; Automation)","text":"<p>Once you have <code>PipelineResults</code>, you can inspect exactly what happened.</p>"},{"location":"guides/python_api_guide/#1-inspect-node-results","title":"1. Inspect Node Results","text":"<pre><code>result = manager.run(\"ingest\")\n\nfor node_name, node_result in result.node_results.items():\n    status = \"\u2705\" if node_result.success else \"\u274c\"\n    print(f\"{status} {node_name}: {node_result.duration:.2f}s\")\n\n    # See metadata (row counts, schema output, etc.)\n    if node_result.metadata:\n        print(f\"   Rows: {node_result.metadata.get('rows_out', 0)}\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-resume-from-failure","title":"2. Resume from Failure","text":"<p>If a pipeline fails at step 5 of 10, you don't want to re-run steps 1-4.</p> <pre><code># Automatically skips successfully completed nodes from the last run\nmanager.run(\"ingest\", resume_from_failure=True)\n</code></pre>"},{"location":"guides/python_api_guide/#level-3-hero-advanced-usage","title":"\ud83d\udd34 Level 3: Hero (Advanced Usage)","text":"<p>This is where Odibi shines. You can unit test individual logic units without running the full pipeline.</p>"},{"location":"guides/python_api_guide/#1-unit-testing-nodes","title":"1. Unit Testing Nodes","text":"<p>You don't need to run the whole pipeline to test one complex SQL transformation.</p> <pre><code>from odibi.pipeline import PipelineManager\nimport pandas as pd\n\n# 1. Setup Manager\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\npipeline = manager.get_pipeline(\"main_etl\")\n\n# 2. Mock Input Data (Inject test data into context)\nmock_data = {\n    \"read_customers\": pd.DataFrame([\n        {\"id\": 1, \"email\": \"BAD_EMAIL\"},\n        {\"id\": 2, \"email\": \"good@test.com\"}\n    ])\n}\n\n# 3. Run ONE specific node with mocked input\nresult = pipeline.run_node(\"clean_customers\", mock_data=mock_data)\n\n# 4. Assertions\noutput_df = pipeline.context.get(\"clean_customers\")\nassert len(output_df) == 1  # Should have filtered bad email\nassert output_df.iloc[0]['email'] == \"good@test.com\"\nprint(\"\u2705 Unit Test Passed\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-accessing-story-data","title":"2. Accessing Story Data","text":"<p>Want to send the pipeline report to Slack or Email programmatically?</p> <pre><code>result = manager.run(\"ingest\")\n\nif result.story_path:\n    print(f\"HTML Report generated at: {result.story_path}\")\n\n    # Read the HTML content\n    with open(result.story_path, \"r\") as f:\n        html_content = f.read()\n\n    # send_email(to=\"team@company.com\", subject=\"Pipeline Report\", body=html_content)\n</code></pre>"},{"location":"guides/python_api_guide/#3-deep-diff-pipeline-runs","title":"3. Deep Diff (Pipeline Runs)","text":"<p>Programmatically detect changes between two pipeline runs (schema drift, row count changes, logic changes).</p> <pre><code>from odibi.diagnostics.diff import diff_runs\nfrom odibi.story.metadata import PipelineStoryMetadata\n\n# Load run metadata (generated by odibi run in odibi_stories/metadata/)\n# Note: You need to know the paths to the JSON files\nrun_a = PipelineStoryMetadata.from_json(\"odibi_stories/metadata/run_20231027_120000.json\")\nrun_b = PipelineStoryMetadata.from_json(\"odibi_stories/metadata/run_20231027_120500.json\")\n\n# Calculate differences\ndiff = diff_runs(run_a, run_b)\n\n# Inspect Results\nif diff.nodes_added:\n    print(f\"New Nodes: {diff.nodes_added}\")\n\nfor node_name, node_diff in diff.node_diffs.items():\n    if node_diff.has_drift:\n        print(f\"\u26a0\ufe0f DRIFT in {node_name}:\")\n        if node_diff.schema_change:\n            print(f\"   - Schema changed! Added: {node_diff.columns_added}\")\n        if node_diff.sql_changed:\n            print(f\"   - SQL Logic changed\")\n        if node_diff.rows_diff != 0:\n            print(f\"   - Row count changed by {node_diff.rows_diff}\")\n</code></pre>"},{"location":"guides/python_api_guide/#4-deep-diff-delta-lake","title":"4. Deep Diff (Delta Lake)","text":"<p>Directly compare two versions of a Delta table to see what changed (rows added, removed, updated).</p> <pre><code>from odibi.diagnostics.delta import get_delta_diff\n\ntable_path = \"data/delta_tables/silver/customers\"\n\n# Compare version 1 vs version 2\ndiff = get_delta_diff(\n    table_path=table_path,\n    version_a=1,\n    version_b=2,\n    deep=True,          # Perform row-by-row comparison\n    keys=[\"id\"]         # Primary key for detecting updates\n)\n\nprint(f\"Rows Added: {diff.rows_added}\")\nprint(f\"Rows Removed: {diff.rows_removed}\")\nprint(f\"Rows Updated: {diff.rows_updated}\")\n\nif diff.sample_updated:\n    print(\"Sample Updates:\", diff.sample_updated[0])\n</code></pre>"},{"location":"guides/python_api_guide/#reference-custom-transformations","title":"\ud83d\udcda Reference: Custom Transformations","text":"<p>To extend the Python API with your own functions, see the Writing Custom Transformations guide.</p> <p>Quick Snippet:</p> <pre><code>from odibi import transform\n\n@transform\ndef my_custom_logic(context, current, threshold=100):\n    return current[current['value'] &gt; threshold]\n</code></pre>"},{"location":"guides/recipes/","title":"\ud83c\udf73 Odibi Cookbook: Recipes for Common Patterns","text":"<p>This guide provides copy-pasteable solutions for real-world Data Engineering problems.</p>"},{"location":"guides/recipes/#recipe-1-the-unstable-api-ingestion","title":"Recipe 1: The \"Unstable API\" Ingestion \ud83c\udf2a\ufe0f","text":"<p>Problem: \"My source JSON adds new fields constantly and is deeply nested. My pipeline breaks whenever the schema changes.\"</p> <p>Solution: Use <code>schema_policy: { mode: \"evolve\" }</code> to automatically adapt to new columns, and <code>normalize_json</code> to flatten the structure.</p> <pre><code>- name: \"ingest_unstable_api\"\n  read:\n    connection: \"api_source\"\n    format: \"json\"\n    path: \"events/v1/*.json\"\n\n  # 1. Handle Drift: Automatically add new columns as NULLable\n  schema_policy:\n    mode: \"evolve\"\n    on_new_columns: \"add_nullable\"\n\n  # 2. Flatten: Convert nested JSON into columns (e.g. payload.id -&gt; payload_id)\n  transformer: \"normalize_json\"\n  params:\n    column: \"payload\"\n    sep: \"_\"\n\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"events_flat\"\n</code></pre>"},{"location":"guides/recipes/#recipe-2-the-privacy-first-customer-table","title":"Recipe 2: The \"Privacy-First\" Customer Table \ud83d\udd12","text":"<p>Problem: \"I need to ingest customer data but Hash emails and Mask credit card numbers for compliance (GDPR/CCPA).\"</p> <p>Solution: Use the <code>privacy</code> block for global anonymization and <code>sensitive</code> columns for masking in stories. You can also mix methods using <code>hash_columns</code> transformer.</p> <pre><code>- name: \"load_secure_customers\"\n  read:\n    connection: \"s3_raw\"\n    format: \"parquet\"\n    path: \"customers/\"\n\n  # 1. Global Privacy Policy (Applies to PII columns)\n  privacy:\n    method: \"hash\"\n    salt: \"${PRIVACY_SALT}\"  # Load from env var\n\n  # 2. Mark Columns as PII (Triggers Privacy Policy)\n  columns:\n    email:\n      pii: true\n    phone:\n      pii: true\n\n  # 3. Explicit Masking for Credit Cards (Transformers run before Write)\n  transform:\n    steps:\n      # Mask CCNs (keep last 4)\n      - function: \"regex_replace\"\n        params:\n          column: \"credit_card\"\n          pattern: \".(?=.{4})\"  # Regex to match all except last 4\n          replacement: \"*\"\n\n  # 4. Hide from Stories (Documentation)\n  sensitive: [\"email\", \"credit_card\", \"phone\"]\n\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"dim_customers_anonymized\"\n</code></pre>"},{"location":"guides/recipes/#recipe-3-sessionizing-clickstream-data","title":"Recipe 3: Sessionizing Clickstream Data \u23f1\ufe0f","text":"<p>Problem: \"I have raw events. I need to group them into User Sessions (30-minute timeout) and load them incrementally.\"</p> <p>Solution: Combine the <code>sessionize</code> transformer with <code>incremental: { mode: \"stateful\" }</code> to process only new data while maintaining session logic.</p> <pre><code>- name: \"clickstream_sessions\"\n  read:\n    connection: \"kafka_landing\"\n    format: \"json\"\n    path: \"clicks/\"\n\n    # 1. Incremental Loading (Stateful)\n    # Tracks the last processed timestamp to only read new events\n    incremental:\n      mode: \"stateful\"\n      state_key: \"clickstream_hwm\"\n      watermark_lag: \"1h\"  # Handle late arriving data\n\n  # 2. Session Logic (30 min timeout)\n  transformer: \"sessionize\"\n  params:\n    timestamp_col: \"event_time\"\n    user_col: \"user_id\"\n    threshold_seconds: 1800  # 30 minutes\n    session_col: \"session_id\"\n\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sessions\"\n    mode: \"append\"\n</code></pre>"},{"location":"guides/secrets/","title":"Managing Secrets","text":"<p>Odibi provides a unified way to handle secrets (API keys, database passwords, storage tokens) across local development and production environments. It supports <code>.env</code> files for local use and native Azure Key Vault integration for production.</p>"},{"location":"guides/secrets/#1-variable-substitution","title":"1. Variable Substitution","text":"<p>You can reference environment variables in your <code>project.yaml</code> using the <code>${VAR_NAME}</code> syntax.</p> <pre><code>connections:\n  my_database:\n    type: azure_sql\n    host: ${DB_HOST}\n    auth:\n      username: ${DB_USER}\n      password: ${DB_PASS}\n</code></pre>"},{"location":"guides/secrets/#2-local-development-env","title":"2. Local Development (<code>.env</code>)","text":"<p>For local development, store your secrets in a <code>.env</code> file in your project root. Odibi automatically loads this file.</p> <p>Note: Always add <code>.env</code> to your <code>.gitignore</code> to prevent committing secrets.</p>"},{"location":"guides/secrets/#cli-commands","title":"CLI Commands","text":"<p>Initialize a template: Generate a <code>.env.template</code> file based on the variables used in your config.</p> <pre><code>odibi secrets init project.yaml\n</code></pre> <p>Validate your environment: Check if all required variables are set in your current environment.</p> <pre><code>odibi secrets validate project.yaml\n</code></pre>"},{"location":"guides/secrets/#3-production-azure-key-vault","title":"3. Production (Azure Key Vault)","text":"<p>In production (e.g., Databricks, Azure Functions), relying on environment variables for everything can be insecure. Odibi supports fetching secrets directly from Azure Key Vault.</p>"},{"location":"guides/secrets/#configuration","title":"Configuration","text":"<p>To use Key Vault, specify <code>key_vault_name</code> and <code>secret_name</code> in your connection config. Odibi will automatically fetch the secret securely using <code>DefaultAzureCredential</code> (Managed Identity / Service Principal).</p> <pre><code>connections:\n  adls_prod:\n    type: azure_adls\n    account: myprodstorage\n    container: data\n    # Instead of a hardcoded key or env var:\n    key_vault_name: \"my-key-vault\"\n    secret_name: \"adls-prod-key\"\n</code></pre>"},{"location":"guides/secrets/#how-it-works","title":"How it Works","text":"<ol> <li>Auth Detection: If <code>key_vault_name</code> is present, Odibi attempts to authenticate with Azure using the environment's identity (e.g., the Databricks cluster's Managed Identity).</li> <li>Parallel Fetching: If multiple connections use Key Vault, Odibi fetches them in parallel during startup to minimize latency.</li> <li>Caching: Secrets are cached in memory for the duration of the run.</li> </ol>"},{"location":"guides/secrets/#best-practices","title":"Best Practices","text":"<ol> <li>Never Commit Secrets: Do not put actual passwords in <code>project.yaml</code>. Use <code>${VAR}</code> placeholders.</li> <li>Use <code>.env.template</code>: Commit a template file with empty values so other developers know which variables they need to set.</li> <li>Use Key Vault in Prod: Avoid setting sensitive environment variables in cloud compute configurations if possible. Use Key Vault integration for rotation and auditing.</li> <li>Redaction: Odibi automatically attempts to redact known secret values from logs and generated stories.</li> </ol>"},{"location":"guides/setup_azure/","title":"Azure Integration Setup Guide (v2.1.0)","text":"<p>This guide covers authenticating and connecting Odibi to Azure services (ADLS Gen2, Azure SQL, Key Vault) using the latest v2.1.0 standards.</p> <p>Key Features in v2.1.0: - Auto-Auth: Zero-config authentication using Managed Identity or Environment Variables (<code>DefaultAzureCredential</code>). - Universal Key Vault: Retrieve ANY secret (Account Key, SAS Token, SQL Password) from Key Vault by referencing it in the config.</p>"},{"location":"guides/setup_azure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription</li> <li>Azure CLI installed: Install Azure CLI</li> <li>Odibi installed with Azure extras: <code>pip install \"odibi[azure]\"</code></li> </ul>"},{"location":"guides/setup_azure/#1-azure-data-lake-storage-gen2-adls","title":"1. Azure Data Lake Storage Gen2 (ADLS)","text":"<p>Use the <code>azure_blob</code> connection type.</p>"},{"location":"guides/setup_azure/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/setup_azure/#option-a-auto-auth-recommended","title":"Option A: Auto-Auth (Recommended)","text":"<p>Best for: Production (Managed Identity) or Local Dev (Azure CLI login). How it works: Odibi uses <code>DefaultAzureCredential</code> to find your identity automatically. No secrets in the config!</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    # No 'auth' section needed!\n    # Odibi will automatically try Managed Identity, CLI, or Env Vars.\n</code></pre> <p>Setup: 1. Azure: Grant your identity (User or Managed Identity) the Storage Blob Data Contributor role on the storage account. 2. Local: Run <code>az login</code>. 3. Production: Assign Managed Identity to the VM/Function/Databricks cluster.</p>"},{"location":"guides/setup_azure/#option-b-universal-key-vault-account-key","title":"Option B: Universal Key Vault (Account Key)","text":"<p>Best for: Scenarios where Managed Identity is not possible. How it works: Store the Account Key in Key Vault, and tell Odibi where to find it.</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    auth:\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"adls-account-key\"  # The secret containing the Account Key\n</code></pre>"},{"location":"guides/setup_azure/#option-c-universal-key-vault-sas-token","title":"Option C: Universal Key Vault (SAS Token)","text":"<p>Best for: Restricted access with SAS tokens.</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    auth:\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"adls-sas-token\"  # The secret containing the SAS Token\n</code></pre>"},{"location":"guides/setup_azure/#2-azure-sql-database","title":"2. Azure SQL Database","text":"<p>Use the <code>sql_server</code> connection type.</p>"},{"location":"guides/setup_azure/#configuration-patterns_1","title":"Configuration Patterns","text":""},{"location":"guides/setup_azure/#option-a-auto-auth-managed-identity","title":"Option A: Auto-Auth (Managed Identity)","text":"<p>Best for: Production pipelines running in Azure.</p> <pre><code>connections:\n  my_sql_db:\n    type: sql_server\n    host: \"odibi-sql.database.windows.net\"\n    database: \"analytics_db\"\n    port: 1433\n    auth: {}  # Empty auth dict signals \"Use Default Driver Auth / Managed Identity\"\n</code></pre> <p>Setup: 1. Enable Managed Identity on your compute resource. 2. In Azure SQL, create a user for the identity: <code>CREATE USER [my-identity] FROM EXTERNAL PROVIDER;</code>. 3. Grant permissions: <code>ALTER ROLE db_datareader ADD MEMBER [my-identity];</code>.</p>"},{"location":"guides/setup_azure/#option-b-universal-key-vault-sql-password","title":"Option B: Universal Key Vault (SQL Password)","text":"<p>Best for: Legacy SQL Auth (Username/Password).</p> <pre><code>connections:\n  my_sql_db:\n    type: sql_server\n    host: \"odibi-sql.database.windows.net\"\n    database: \"analytics_db\"\n    port: 1433\n    auth:\n      username: \"sqladmin\"\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"sql-password\"  # The secret containing the password\n</code></pre>"},{"location":"guides/setup_azure/#3-key-vault-setup","title":"3. Key Vault Setup","text":"<p>If you use Key Vault references, Odibi needs to authenticate to the Key Vault first. It uses Auto-Auth for this too!</p> <ol> <li> <p>Create Key Vault: <code>bash    az keyvault create --name my-keyvault --resource-group my-rg</code></p> </li> <li> <p>Grant Access:    Grant your identity (User or Managed Identity) the Key Vault Secrets User role.    <code>bash    az role assignment create \\      --role \"Key Vault Secrets User\" \\      --assignee &lt;your-email-or-identity-id&gt; \\      --scope /subscriptions/.../resourceGroups/my-rg/providers/Microsoft.KeyVault/vaults/my-keyvault</code></p> </li> <li> <p>Store Secrets: <code>bash    az keyvault secret set --vault-name my-keyvault --name adls-account-key --value \"your-key-here\"</code></p> </li> </ol>"},{"location":"guides/setup_azure/#summary-of-changes-v20-v21","title":"Summary of Changes (v2.0 -&gt; v2.1)","text":"Feature v2.0 (Old) v2.1 (New) Connection Type <code>azure_adls</code>, <code>azure_sql</code> <code>azure_blob</code>, <code>sql_server</code> Auth Mode <code>auth_mode: key_vault</code> (top-level) Removed. Use <code>auth: { key_vault_name: ... }</code> Managed Identity Explicit <code>auth_mode: managed_identity</code> Implicit (Auto-Auth) via <code>DefaultAzureCredential</code> Key Vault Limited to specific auth modes Universal (works for any secret in <code>auth</code> dict) <p>For a complete configuration reference, see docs/reference/configuration.md.</p>"},{"location":"guides/the_definitive_guide/","title":"The Definitive Guide to Odibi","text":"<p>Version: 2.4.0 Audience: Data Engineers, Analytics Engineers, Architects Prerequisites: Basic Python and SQL knowledge Goal: From \"Hello World\" to Enterprise Production</p>"},{"location":"guides/the_definitive_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction &amp; Philosophy</li> <li>Architecture Deep Dive</li> <li>The Object Model (Python API)</li> <li>Context, State, and Dependencies</li> <li>Transformation Engineering</li> <li>The Dual Engine: Pandas &amp; Spark</li> <li>Declarative Configuration (YAML)</li> <li>Observability: Data Stories</li> <li>Reliability &amp; Governance</li> <li>Production Deployment Patterns</li> <li>Comprehensive End-to-End Example</li> </ol>"},{"location":"guides/the_definitive_guide/#1-introduction-philosophy","title":"1. Introduction &amp; Philosophy","text":"<p>Odibi is a Declarative Data Framework designed to solve the \"Two Language Problem\" in data engineering: 1.  Local Development is often done in Python/Pandas on a laptop (fast iteration, small data). 2.  Production runs on distributed clusters like Spark/Databricks (high latency, massive data).</p> <p>Traditionally, this requires rewriting code or wrapping Spark in complex local docker containers. Odibi provides a unified abstraction layer that allows the exact same pipeline definition to run on your MacBook (using Pandas) and on a 100-node Databricks cluster (using Spark).</p>"},{"location":"guides/the_definitive_guide/#core-principles","title":"Core Principles","text":"<ol> <li>Declarative over Imperative: Define what you want (Input -&gt; Transform -&gt; Output), not how to loop through files.</li> <li>Code First, Configuration Second: Learn the Python API to understand the system; use YAML for deployment.</li> <li>Engine Agnostic: Logic written for Odibi runs on Pandas (Local) and Spark (Scale) without code changes.</li> <li>Observability by Default: Every run generates a rich HTML \"Data Story\" with lineage, profile, and logs.</li> </ol>"},{"location":"guides/the_definitive_guide/#2-architecture-deep-dive","title":"2. Architecture Deep Dive","text":"<p>Odibi is built on a \"Three Layer\" Architecture. Understanding these layers helps you debug and extend the framework.</p> <pre><code>graph TD\n    subgraph \"Layer 1: Declarative (User Interface)\"\n        A[odibi.yaml] --&gt;|Parsed by| B(ProjectConfig)\n        CLI[odibi run] --&gt;|Invokes| B\n    end\n\n    subgraph \"Layer 2: Object Model (The Brain)\"\n        B --&gt; C[PipelineConfig]\n        C --&gt; D[NodeConfig]\n        D --&gt;|Validates| E[DependencyGraph]\n        E --&gt;|Orchestrates| F[Pipeline Executor]\n    end\n\n    subgraph \"Layer 3: Runtime Engine (The Muscle)\"\n        F --&gt;|Delegates to| G{Engine Interface}\n        G --&gt;|Local Mode| H[Pandas Engine]\n        G --&gt;|Scale Mode| I[Spark Engine]\n        H --&gt; J[(Local Files / DuckDB)]\n        I --&gt; K[(Delta Lake / ADLS / Databricks)]\n    end\n</code></pre> <ul> <li>Layer 1 (YAML): Simple configuration files.</li> <li>Layer 2 (Pydantic Objects): The <code>odibi.config</code> module. This is where validation happens. If your graph has a cycle, or you miss a parameter, Layer 2 catches it before execution starts.</li> <li>Layer 3 (Engines): The <code>odibi.engine</code> module. This adapts your abstract \"Read CSV\" command into <code>pd.read_csv(...)</code> or <code>spark.read.csv(...)</code>.</li> </ul>"},{"location":"guides/the_definitive_guide/#3-the-object-model-python-api","title":"3. The Object Model (Python API)","text":"<p>The most effective way to learn Odibi is to build a pipeline using the Python classes directly. This demystifies the YAML tags.</p>"},{"location":"guides/the_definitive_guide/#31-the-hierarchy","title":"3.1 The Hierarchy","text":"<ol> <li><code>ProjectConfig</code>: Top-level container. Holds global settings (Engine type, Retry policy, Connections).</li> <li><code>PipelineConfig</code>: A logical grouping of tasks (e.g., \"Daily Sales Batch\").</li> <li><code>NodeConfig</code>: A single step in the pipeline.</li> <li>Operations: <code>ReadConfig</code>, <code>TransformConfig</code>, <code>WriteConfig</code>.</li> </ol>"},{"location":"guides/the_definitive_guide/#32-building-a-complex-node-programmatically","title":"3.2 Building a Complex Node Programmatically","text":"<p>Let's look at a feature-rich node configuration.</p> <pre><code>from odibi.config import NodeConfig, ReadConfig, TransformConfig, WriteConfig, ErrorStrategy\n\nnode = NodeConfig(\n    name=\"process_orders\",\n    description=\"Cleans orders and calculates tax\",\n    tags=[\"daily\", \"critical\"],\n\n    # 1. Dependency Management\n    depends_on=[\"raw_orders\", \"ref_tax_rates\"],\n\n    # 2. Transformation Logic\n    transform=TransformConfig(\n        steps=[\n            # Step A: Filter (SQL)\n            \"SELECT * FROM raw_orders WHERE status != 'CANCELLED'\",\n            # Step B: Custom Python Function\n            {\"function\": \"calculate_tax\", \"params\": {\"rate_source\": \"ref_tax_rates\"}}\n        ]\n    ),\n\n    # 3. Output Configuration\n    write=WriteConfig(\n        connection=\"silver_db\",\n        format=\"delta\",\n        table=\"orders_cleaned\",\n        mode=\"append\"\n    ),\n\n    # 4. Reliability &amp; Governance\n    on_error=ErrorStrategy.FAIL_FAST,  # Stop whole pipeline if this fails\n    sensitive=[\"customer_email\"],      # Mask this column in logs/stories\n    cache=True                         # Cache result in memory for downstream nodes\n)\n</code></pre>"},{"location":"guides/the_definitive_guide/#33-node-operations-guide","title":"3.3 Node Operations Guide","text":"Operation Description Key Fields Read Ingests data from a connection. <code>connection</code>, <code>format</code> (<code>csv</code>, <code>parquet</code>, <code>delta</code>, <code>sql</code>), <code>path</code>, <code>options</code> Transform Runs a sequence of steps. <code>steps</code> (List of SQL strings or <code>{function: name, params: {}}</code> dicts) Transformer Runs a single \"App-like\" transformation. <code>transformer</code> (Name string), <code>params</code> (Dict). Used for complex logic like SCD2 or Deduplication. Write Saves the result. <code>connection</code>, <code>format</code>, <code>path</code>/<code>table</code>, <code>mode</code> (<code>overwrite</code>, <code>append</code>, <code>upsert</code>)"},{"location":"guides/the_definitive_guide/#4-context-state-and-dependencies","title":"4. Context, State, and Dependencies","text":"<p>In Odibi, you don't pass variables between functions. You rely on the Dependency Graph.</p>"},{"location":"guides/the_definitive_guide/#41-the-global-context-odibicontextcontext","title":"4.1 The Global Context (<code>odibi.context.Context</code>)","text":"<p>The Pipeline Executor maintains a Global Context. *   Registry: When a node named <code>raw_orders</code> finishes, its result (DataFrame) is registered in the context under the key <code>\"raw_orders\"</code>. *   Access: Downstream nodes request data from the context by name. *   Memory Management: In the Pandas engine, this is a dictionary of DataFrames in RAM. In Spark, these are Temp Views registered in the Spark Session.</p>"},{"location":"guides/the_definitive_guide/#42-dependency-resolution","title":"4.2 Dependency Resolution","text":"<p>When you define <code>depends_on=[\"node_A\"]</code> for <code>node_B</code>: 1.  Graph Construction: Odibi builds a DAG (Directed Acyclic Graph). 2.  Topological Sort: It determines the execution order (A -&gt; B). 3.  Execution:     *   <code>node_A</code> runs. Output registered as <code>\"node_A\"</code>.     *   <code>node_B</code> starts. It can now execute <code>SELECT * FROM node_A</code>.</p>"},{"location":"guides/the_definitive_guide/#43-the-enginecontext-odibicontextenginecontext","title":"4.3 The EngineContext (<code>odibi.context.EngineContext</code>)","text":"<p>When writing custom transformations, you interact with <code>EngineContext</code>, not the Global Context directly. This wraps the global state and provides uniform APIs.</p> Method Description <code>context.df</code> The DataFrame resulting from the previous step in the current node. <code>context.get(name)</code> Retrieve a DataFrame from an upstream node (Global Context). <code>context.sql(query)</code> Run SQL on <code>context.df</code>. Returns a new Context with updated <code>df</code>. <code>context.register_temp_view(name, df)</code> Register a DF manually for complex SQL joins."},{"location":"guides/the_definitive_guide/#5-transformation-engineering","title":"5. Transformation Engineering","text":"<p>While SQL is great for filtering and projection, complex logic belongs in Python. Odibi uses a registry pattern.</p>"},{"location":"guides/the_definitive_guide/#51-the-transform-decorator","title":"5.1 The <code>@transform</code> Decorator","text":"<p>You must register functions so Odibi can find them by name from the YAML configuration.</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef categorize_users(context, params: dict):\n    \"\"\"\n    Categorizes users based on spend.\n\n    YAML:\n      function: categorize_users\n      params:\n        threshold: 1000\n    \"\"\"\n    threshold = params.get(\"threshold\", 1000)\n\n    # 1. Get Input\n    df = context.df\n\n    # 2. Use Engine-Native commands (Pandas/Spark agnostic via SQL)\n    # Or check context.engine_type if you need specific optimization\n\n    return context.sql(f\"\"\"\n        SELECT\n            *,\n            CASE WHEN total_spend &gt; {threshold} THEN 'VIP' ELSE 'Regular' END as category\n        FROM df\n    \"\"\").df\n</code></pre>"},{"location":"guides/the_definitive_guide/#52-transformer-vs-transform-steps","title":"5.2 Transformer vs. Transform Steps","text":"<p>There are two ways to define logic in a node:</p> <ol> <li> <p>Top-Level Transformer (<code>transformer: \"name\"</code>):</p> <ul> <li>The node acts as a specific \"App\" (e.g., <code>deduplicate</code>, <code>scd2</code>).</li> <li>It usually takes the dependencies as input and produces one output.</li> <li>Use this for heavy, reusable logic (Slowly Changing Dimensions, Merges).</li> </ul> </li> <li> <p>Transform Steps (<code>transform: { steps: [...] }</code>):</p> <ul> <li>A \"Script\" of sequential operations.</li> <li>Mixes SQL and lightweight Python functions.</li> <li>Use this for business logic specific to that pipeline (Filter -&gt; Join -&gt; Map -&gt; Reduce).</li> </ul> </li> </ol>"},{"location":"guides/the_definitive_guide/#6-the-dual-engine-pandas-spark","title":"6. The Dual Engine: Pandas &amp; Spark","text":"<p>Odibi abstracts the engine, but knowing how they differ is crucial for performance.</p>"},{"location":"guides/the_definitive_guide/#61-comparison","title":"6.1 Comparison","text":"Feature Pandas Engine Spark Engine Compute Single Node (CPU/RAM bound) Distributed (Cluster) SQL DuckDB / PandasQL Spark SQL (Catalyst) IO Local FS, S3/Blob (via fsspec) HDFS, S3, ADLS (Native) Setup <code>pip install odibi</code> <code>pip install odibi[spark]</code> Best For Dev, Testing, Small Data (&lt;10GB) Prod, Big Data (TB/PB)"},{"location":"guides/the_definitive_guide/#62-spark-specific-features","title":"6.2 Spark-Specific Features","text":"<p>The Spark engine enables advanced Data Lakehouse features via <code>odibi.engine.spark_engine.SparkEngine</code>.</p>"},{"location":"guides/the_definitive_guide/#delta-lake-integration","title":"Delta Lake Integration","text":"<p>Odibi treats Delta Lake as a first-class citizen.</p> <ul> <li> <p>Time Travel: <code>python     ReadConfig(..., options={\"as_of_version\": 5})     # OR     ReadConfig(..., options={\"as_of_timestamp\": \"2023-10-01\"})</code></p> </li> <li> <p>Upserts (MERGE): <code>python     WriteConfig(         ...,         format=\"delta\",         mode=\"upsert\",         options={\"keys\": [\"user_id\"]} # Matches on key, updates all other cols     )</code></p> </li> <li> <p>Optimization (Write Options): <code>python     WriteConfig(..., options={         \"optimize_write\": \"true\",  # Auto-compaction         \"zorder_by\": [\"region\"]    # Spatial indexing     })</code></p> </li> </ul>"},{"location":"guides/the_definitive_guide/#7-declarative-configuration-yaml","title":"7. Declarative Configuration (YAML)","text":"<p>The YAML configuration is the deployment artifact. It maps 1:1 to the Pydantic models.</p>"},{"location":"guides/the_definitive_guide/#71-project-structure","title":"7.1 Project Structure","text":"<p>Recommended folder structure for an Odibi project:</p> <pre><code>my_project/\n\u251c\u2500\u2500 odibi.yaml            # Main entry point\n\u251c\u2500\u2500 transforms.py         # Custom python logic (@transform)\n\u251c\u2500\u2500 data/                 # Local data (for dev)\n\u251c\u2500\u2500 stories/              # Generated reports\n\u2514\u2500\u2500 .env                  # Secrets (API keys)\n</code></pre>"},{"location":"guides/the_definitive_guide/#72-advanced-yaml-features","title":"7.2 Advanced YAML Features","text":""},{"location":"guides/the_definitive_guide/#environment-variables","title":"Environment Variables","text":"<p>You can inject secrets or environment-specific paths using <code>${VAR_NAME}</code>.</p> <pre><code>connections:\n  snowflake:\n    type: \"sql_server\"\n    host: \"${DB_HOST}\"\n    password: \"${DB_PASSWORD}\" # Redacted in logs automatically\n</code></pre>"},{"location":"guides/the_definitive_guide/#yaml-anchors-aliases","title":"YAML Anchors &amp; Aliases","text":"<p>Reuse configuration blocks to keep YAML DRY (Don't Repeat Yourself).</p> <pre><code># Define a template\n.default_write: &amp;default_write\n  connection: \"datalake\"\n  format: \"delta\"\n  mode: \"overwrite\"\n\nnodes:\n  - name: \"customers\"\n    write:\n      &lt;&lt;: *default_write\n      table: \"dim_customers\"\n\n  - name: \"orders\"\n    write:\n      &lt;&lt;: *default_write\n      table: \"fact_orders\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#8-observability-data-stories","title":"8. Observability: Data Stories","text":"<p>Running a pipeline blindly is dangerous. Odibi generates Data Stories.</p>"},{"location":"guides/the_definitive_guide/#81-what-is-a-story","title":"8.1 What is a Story?","text":"<p>A Story is an HTML file generated at the end of a run. It answers: 1.  Lineage: \"Where did this data come from?\" (Visual Graph) 2.  Profile: \"How many rows? How many nulls?\" (Schema &amp; Stats) 3.  Sample: \"What does the data look like?\" (Preview rows) 4.  Logic: \"What code actually ran?\" (SQL/Python snippet)</p>"},{"location":"guides/the_definitive_guide/#82-configuration","title":"8.2 Configuration","text":"<p>Enable story generation in <code>ProjectConfig</code>.</p> <pre><code>story:\n  connection: \"local_data\" # Where to save the HTML\n  path: \"stories/\"\n  max_sample_rows: 20      # Number of rows to preview\n  retention_days: 30       # Auto-cleanup old reports\n</code></pre>"},{"location":"guides/the_definitive_guide/#83-openlineage","title":"8.3 OpenLineage","text":"<p>Odibi supports the OpenLineage standard for integration with tools like Marquez or Atlan.</p> <pre><code>lineage:\n  url: \"http://localhost:5000\" # Marquez URL\n  namespace: \"odibi_prod\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#9-reliability-governance","title":"9. Reliability &amp; Governance","text":""},{"location":"guides/the_definitive_guide/#91-retries-backoff","title":"9.1 Retries &amp; Backoff","text":"<p>Network blips happen. Configure retries globally or per node.</p> <pre><code># Global setting in project config\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\" # linear, constant\n</code></pre>"},{"location":"guides/the_definitive_guide/#92-alerting","title":"9.2 Alerting","text":"<p>Send notifications when pipelines fail or succeed.</p> <pre><code>alerts:\n  - type: \"slack\"\n    url: \"${SLACK_WEBHOOK}\"\n    on_events: [\"on_failure\"] # on_start, on_success\n    metadata:\n      env: \"production\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#93-pii-protection","title":"9.3 PII Protection","text":"<p>Prevent sensitive data from leaking into logs or Data Stories.</p> <pre><code>nodes:\n  - name: \"load_users\"\n    # Masks columns in the 'Sample Data' section of the Story\n    sensitive: [\"email\", \"ssn\", \"phone_number\"]\n</code></pre>"},{"location":"guides/the_definitive_guide/#10-production-deployment-patterns","title":"10. Production Deployment Patterns","text":""},{"location":"guides/the_definitive_guide/#101-the-hybrid-pattern","title":"10.1 The \"Hybrid\" Pattern","text":"<ol> <li>Develop Locally: Use <code>engine: pandas</code> and local CSVs. Iterate fast.</li> <li>Deploy to Cloud:<ul> <li>Switch <code>engine: spark</code>.</li> <li>Change connections to ADLS/S3.</li> <li>Use <code>odibi run</code> via a job scheduler (Airflow, Databricks Workflows).</li> </ul> </li> </ol>"},{"location":"guides/the_definitive_guide/#102-cicd","title":"10.2 CI/CD","text":"<p>Since Odibi logic is Python code and YAML config: 1.  Linting: Run <code>black</code> and <code>ruff</code> on your <code>transforms.py</code>. 2.  Validation: Run a script that loads <code>ProjectConfig(path=\"odibi.yaml\")</code> to validate the graph structure before deployment. 3.  Testing: Use the Python API to run single nodes with mock data (Unit Tests).</p>"},{"location":"guides/the_definitive_guide/#11-comprehensive-end-to-end-example","title":"11. Comprehensive End-to-End Example","text":"<p>This script demonstrates a complete workflow: 1.  Mocking data generation. 2.  Defining custom logic. 3.  Configuring a multi-stage pipeline (Bronze -&gt; Silver -&gt; Gold). 4.  Executing with error handling.</p> <pre><code>import pandas as pd\nimport os\nimport logging\nfrom odibi.config import (\n    PipelineConfig, NodeConfig, ReadConfig, TransformConfig, WriteConfig,\n    RetryConfig, ErrorStrategy\n)\nfrom odibi.pipeline import Pipeline\nfrom odibi.registry import transform\nfrom odibi.connections import LocalConnection\n\n# ==========================================\n# 0. Setup Environment\n# ==========================================\n# Create dummy data for the example\nos.makedirs(\"data/landing\", exist_ok=True)\nos.makedirs(\"data/silver\", exist_ok=True)\nos.makedirs(\"data/gold\", exist_ok=True)\n\n# Generate Raw JSON Data (Simulating an API dump)\npd.DataFrame([\n    {\"id\": 1, \"user\": \"Alice\", \"tx_amount\": 150.0, \"ts\": \"2023-10-01T10:00:00\"},\n    {\"id\": 2, \"user\": \"Bob\",   \"tx_amount\": 20.0,  \"ts\": \"2023-10-01T10:05:00\"},\n    {\"id\": 3, \"user\": \"Alice\", \"tx_amount\": -50.0, \"ts\": \"2023-10-01T10:10:00\"}, # Invalid\n    {\"id\": 4, \"user\": \"Eve\",   \"tx_amount\": 1000.0, \"ts\": \"2023-10-01T10:15:00\"},\n]).to_json(\"data/landing/transactions.json\", orient=\"records\")\n\n# ==========================================\n# 1. Custom Logic (Business Rules)\n# ==========================================\n@transform\ndef anomaly_detection(context, params):\n    \"\"\"\n    Flags transactions that are 3 std devs above the mean.\n    \"\"\"\n    df = context.df\n\n    # Using SQL for statistical window function\n    # This works in DuckDB (Pandas) and Spark SQL\n    query = \"\"\"\n        SELECT\n            *,\n            AVG(tx_amount) OVER () as mean_amount,\n            STDDEV(tx_amount) OVER () as std_amount,\n            CASE\n                WHEN tx_amount &gt; (AVG(tx_amount) OVER () + 3 * STDDEV(tx_amount) OVER ())\n                THEN true\n                ELSE false\n            END as is_anomaly\n        FROM df\n    \"\"\"\n    return context.sql(query).df\n\n# ==========================================\n# 2. Pipeline Definition\n# ==========================================\npipeline_conf = PipelineConfig(\n    pipeline=\"fraud_detection_batch\",\n    description=\"Ingests transactions and flags anomalies\",\n    nodes=[\n        # --- Bronze Layer: Ingest Raw ---\n        NodeConfig(\n            name=\"bronze_tx\",\n            read=ReadConfig(\n                connection=\"landing\",\n                format=\"json\",\n                path=\"transactions.json\"\n            ),\n            # Keep raw data safe, minimal transform\n            write=WriteConfig(\n                connection=\"silver\",\n                format=\"parquet\",\n                path=\"bronze_tx.parquet\",\n                mode=\"overwrite\"\n            )\n        ),\n\n        # --- Silver Layer: Clean &amp; Enrich ---\n        NodeConfig(\n            name=\"silver_tx\",\n            depends_on=[\"bronze_tx\"],\n            transform=TransformConfig(\n                steps=[\n                    # 1. Filter invalid amounts\n                    \"SELECT * FROM bronze_tx WHERE tx_amount &gt; 0\",\n                    # 2. Run Anomaly Detection\n                    {\"function\": \"anomaly_detection\", \"params\": {}}\n                ]\n            ),\n            write=WriteConfig(\n                connection=\"silver\",\n                format=\"parquet\",\n                path=\"silver_tx.parquet\",\n                mode=\"overwrite\"\n            )\n        ),\n\n        # --- Gold Layer: Business Aggregates ---\n        NodeConfig(\n            name=\"gold_user_summary\",\n            depends_on=[\"silver_tx\"],\n            transform=TransformConfig(\n                steps=[\n                    \"\"\"\n                    SELECT\n                        user,\n                        COUNT(*) as tx_count,\n                        SUM(tx_amount) as total_volume,\n                        SUM(CASE WHEN is_anomaly THEN 1 ELSE 0 END) as suspicious_tx_count\n                    FROM silver_tx\n                    GROUP BY user\n                    \"\"\"\n                ]\n            ),\n            write=WriteConfig(\n                connection=\"gold\",\n                format=\"csv\",\n                path=\"user_risk_report.csv\",\n                mode=\"overwrite\"\n            ),\n            # Fail fast if report generation breaks\n            on_error=ErrorStrategy.FAIL_FAST\n        )\n    ]\n)\n\n# ==========================================\n# 3. Execution Wrapper\n# ==========================================\ndef run_pipeline():\n    print(\"\ud83d\ude80 Starting Fraud Detection Pipeline...\")\n\n    # Connections Definition\n    connections = {\n        \"landing\": LocalConnection(base_path=\"./data/landing\"),\n        \"silver\":  LocalConnection(base_path=\"./data/silver\"),\n        \"gold\":    LocalConnection(base_path=\"./data/gold\")\n    }\n\n    # Initialize Pipeline\n    pipeline = Pipeline(\n        pipeline_conf,\n        connections=connections,\n        generate_story=True,\n        story_config={\n            \"output_path\": \"data/stories\",\n            \"max_sample_rows\": 50\n        },\n        retry_config=RetryConfig(enabled=True, max_attempts=2)\n    )\n\n    # Run\n    results = pipeline.run()\n\n    # Logging Results\n    print(f\"\\n\u23f1\ufe0f Duration: {results.duration:.2f}s\")\n\n    if results.failed:\n        print(f\"\u274c Failed Nodes: {results.failed}\")\n        # Inspect specific error\n        for node in results.failed:\n            res = results.get_node_result(node)\n            print(f\"   Reason ({node}): {res.error}\")\n    else:\n        print(f\"\u2705 Success! Report generated at: {results.story_path}\")\n\n        # Verify output\n        print(\"\\n--- Risk Report ---\")\n        df = pd.read_csv(\"data/gold/user_risk_report.csv\")\n        print(df.to_string(index=False))\n\nif __name__ == \"__main__\":\n    run_pipeline()\n</code></pre> <p>This guide provides a solid foundation for using Odibi. Start with the Python API to understand the mechanics, and transition to YAML for production operations.</p>"},{"location":"guides/writing_transformations/","title":"Writing Transformation Functions in Odibi","text":"<p>This guide explains how to write custom Python transformation functions for Odibi pipelines, focusing on how to access data and manage state.</p>"},{"location":"guides/writing_transformations/#the-basics","title":"The Basics","text":"<p>Every transformation function in Odibi must be decorated with <code>@transform</code>. The Odibi engine automatically injects dependencies based on your function signature.</p>"},{"location":"guides/writing_transformations/#the-context-object","title":"The <code>context</code> Object","text":"<p>The first argument to any transformation function is always <code>context</code>. This object is your gateway to the entire state of the pipeline execution.</p> <p>Through <code>context</code>, you can: - Access the output of any previous node. - Retrieve datasets declared in <code>depends_on</code>. - Inspect available data using <code>context.list_names()</code>.</p>"},{"location":"guides/writing_transformations/#the-current-argument","title":"The <code>current</code> Argument","text":"<p>If your function includes an argument named <code>current</code>, Odibi will automatically pass the output of the immediately preceding step to it.</p> <ul> <li>With <code>current</code>: Continues the \"chain\" of data transformation.</li> <li>Without <code>current</code>: Breaks the chain (useful for generators or starting fresh logic).</li> </ul>"},{"location":"guides/writing_transformations/#accessing-other-datasets","title":"Accessing Other Datasets","text":"<p>While <code>current</code> is great for linear transformations (A \u2192 B \u2192 C), complex logic often requires accessing multiple datasets (e.g., for joins, lookups, or comparisons). You do this using <code>context.get()</code>.</p>"},{"location":"guides/writing_transformations/#pattern-explicit-data-fetching","title":"Pattern: Explicit Data Fetching","text":"<ol> <li>Define the Function: Add a parameter for the dataset name you want to fetch.</li> <li>Fetch from Context: Use <code>context.get(name)</code>.</li> <li>Configure in YAML: Pass the node name as a parameter.</li> </ol>"},{"location":"guides/writing_transformations/#python-implementation-transformspy","title":"Python Implementation (<code>transforms.py</code>)","text":"<pre><code>from odibi import transform\nimport pandas as pd\n\n@transform\ndef enrich_with_lookup(context, current: pd.DataFrame, lookup_node: str):\n    \"\"\"\n    Enriches the current stream with data from a lookup node.\n\n    Args:\n        context: The Odibi execution context.\n        current: The dataframe from the previous step.\n        lookup_node: The name of the node containing lookup data (passed from YAML).\n    \"\"\"\n    # 1. Fetch the other dataset using context\n    if not context.has(lookup_node):\n        raise ValueError(f\"Lookup node '{lookup_node}' not found in context!\")\n\n    lookup_df = context.get(lookup_node)\n\n    # 2. Perform the logic (e.g., merge)\n    # Note: For simple merges, SQL is often preferred, but Python is useful\n    # for fuzzy matching, complex logic, or API-based enrichment.\n    result = current.merge(\n        lookup_df,\n        on=\"common_id\",\n        how=\"left\",\n        suffixes=(\"\", \"_lookup\")\n    )\n\n    return result\n</code></pre>"},{"location":"guides/writing_transformations/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>nodes:\n  - name: main_process\n    depends_on:\n      - raw_orders      # The 'current' stream\n      - customer_info   # The lookup table\n    transform:\n      steps:\n        - function: enrich_with_lookup\n          params:\n            lookup_node: \"customer_info\"\n</code></pre>"},{"location":"guides/writing_transformations/#sql-vs-python-when-to-use-what","title":"SQL vs. Python: When to use what?","text":"<p>Odibi supports mixing SQL and Python steps in the same node.</p> Use SQL when... Use Python when... Joining tables (Standard Joins) Making API calls (e.g., Geocoding, REST APIs) Aggregations (GROUP BY, SUM) Complex loops or procedural logic Filtering (WHERE clauses) Using libraries (NumPy, SciPy, AI models) Renaming/Reordering columns File operations or custom parsing <p>Example of SQL for Multi-Dataset Access: If you just need a standard join, you don't need a Python function. You can reference nodes directly in SQL:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        SELECT o.*, c.email\n        FROM current_df AS o\n        LEFT JOIN customer_info AS c ON o.id = c.id\n</code></pre>"},{"location":"guides/writing_transformations/#sql-transformations","title":"SQL Transformations","text":"<p>For standard data transformations, SQL is often cleaner than Python. Odibi supports inline SQL and SQL file references.</p>"},{"location":"guides/writing_transformations/#inline-sql","title":"Inline SQL","text":"<pre><code>nodes:\n  - name: clean_orders\n    depends_on: [raw_orders]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              order_id,\n              customer_id,\n              UPPER(TRIM(status)) AS status,\n              CAST(amount AS DECIMAL(10,2)) AS amount,\n              COALESCE(discount, 0) AS discount\n            FROM raw_orders\n            WHERE order_id IS NOT NULL\n</code></pre>"},{"location":"guides/writing_transformations/#multi-table-sql-joins","title":"Multi-Table SQL Joins","text":"<p>Reference any node from <code>depends_on</code>:</p> <pre><code>nodes:\n  - name: enriched_orders\n    depends_on: [clean_orders, customers, products]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              o.*,\n              c.customer_name,\n              c.segment,\n              p.product_name,\n              p.category\n            FROM clean_orders o\n            LEFT JOIN customers c ON o.customer_id = c.id\n            LEFT JOIN products p ON o.product_id = p.id\n</code></pre>"},{"location":"guides/writing_transformations/#sql-file-reference","title":"SQL File Reference","text":"<p>For complex queries, use external SQL files:</p> <pre><code>transform:\n  steps:\n    - sql_file: sql/complex_aggregation.sql\n</code></pre> <p>sql/complex_aggregation.sql:</p> <pre><code>WITH daily_totals AS (\n    SELECT \n        DATE(order_date) AS order_day,\n        customer_id,\n        SUM(amount) AS daily_amount\n    FROM orders\n    GROUP BY DATE(order_date), customer_id\n)\nSELECT \n    order_day,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    SUM(daily_amount) AS revenue\nFROM daily_totals\nGROUP BY order_day\n</code></pre>"},{"location":"guides/writing_transformations/#window-functions-in-sql","title":"Window Functions in SQL","text":"<pre><code>transform:\n  steps:\n    - sql: |\n        SELECT \n          *,\n          ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS order_rank,\n          SUM(amount) OVER (PARTITION BY customer_id) AS customer_lifetime_value,\n          LAG(amount) OVER (PARTITION BY customer_id ORDER BY order_date) AS prev_order_amount\n        FROM orders\n</code></pre>"},{"location":"guides/writing_transformations/#combining-sql-and-python-steps","title":"Combining SQL and Python Steps","text":"<pre><code>transform:\n  steps:\n    # Step 1: SQL for standard transformations\n    - sql: |\n        SELECT * FROM raw_orders \n        WHERE status != 'CANCELLED'\n\n    # Step 2: Python for complex logic\n    - function: enrich_with_api_data\n      params:\n        api_endpoint: \"https://api.example.com/enrichment\"\n\n    # Step 3: SQL for final shaping\n    - sql: |\n        SELECT order_id, customer_id, amount, enriched_data\n        FROM current_df\n        ORDER BY order_date\n</code></pre>"},{"location":"guides/writing_transformations/#registering-custom-transforms-with-transform","title":"Registering Custom Transforms with @transform","text":"<p>The <code>@transform</code> decorator registers your function so Odibi can find it by name in YAML configurations.</p>"},{"location":"guides/writing_transformations/#basic-registration","title":"Basic Registration","text":"<pre><code>from odibi import transform\n\n@transform\ndef clean_names(context, current):\n    \"\"\"Function is registered as 'clean_names' (uses function name).\"\"\"\n    current['name'] = current['name'].str.strip().str.title()\n    return current\n</code></pre>"},{"location":"guides/writing_transformations/#custom-name-registration","title":"Custom Name Registration","text":"<pre><code>@transform(\"normalize_addresses\")\ndef my_address_normalizer(context, current):\n    \"\"\"Function is registered as 'normalize_addresses'.\"\"\"\n    # ... address normalization logic\n    return current\n</code></pre>"},{"location":"guides/writing_transformations/#registration-with-category-and-parameter-model","title":"Registration with Category and Parameter Model","text":"<pre><code>from pydantic import BaseModel\n\nclass EnrichmentParams(BaseModel):\n    lookup_table: str\n    join_key: str\n    columns: list[str]\n\n@transform(name=\"enrich_data\", category=\"enrichment\", param_model=EnrichmentParams)\ndef enrich_data(context, current, lookup_table: str, join_key: str, columns: list):\n    \"\"\"\n    Registered as 'enrich_data' with parameter validation.\n\n    Parameters are validated against EnrichmentParams before execution.\n    \"\"\"\n    lookup_df = context.get(lookup_table)\n    return current.merge(lookup_df[columns + [join_key]], on=join_key, how='left')\n</code></pre>"},{"location":"guides/writing_transformations/#where-to-put-your-transforms","title":"Where to Put Your Transforms","text":"<ol> <li>Project-level: Create <code>transformations/custom_transforms.py</code></li> <li>Import in project.yaml:    ```yaml    python_imports:<ul> <li>transformations.custom_transforms    ```</li> </ul> </li> <li>Use in nodes:    ```yaml    transform:      steps:<ul> <li>function: normalize_addresses    ```</li> </ul> </li> </ol>"},{"location":"guides/writing_transformations/#summary-of-function-signature-rules","title":"Summary of Function Signature Rules","text":"Signature Behavior <code>def func(context):</code> Receives context only. Does not receive previous step output. <code>def func(context, current):</code> Receives context AND the result of the previous step. <code>def func(context, my_param):</code> Receives context and a parameter from YAML. <code>def func(context, current, my_param):</code> Receives all three."},{"location":"guides/writing_transformations/#see-also","title":"See Also","text":"<ul> <li>Patterns Overview - Built-in transformation patterns</li> <li>Best Practices - Code organization guidelines</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"guides/wsl_setup/","title":"\ud83d\udc27 WSL Setup Guide","text":"<p>If you are on Windows, Windows Subsystem for Linux (WSL 2) is the only supported way to develop with Odibi (especially for Spark compatibility).</p>"},{"location":"guides/wsl_setup/#the-golden-rule","title":"The Golden Rule","text":"<p>Edit code in Windows. Run code in WSL.</p> <ul> <li>VS Code: Runs in Windows, but connects to WSL.</li> <li>Terminal: You type commands in the WSL (Ubuntu) terminal.</li> <li>Files: Live in the Linux file system (or <code>/mnt/d/</code>).</li> </ul>"},{"location":"guides/wsl_setup/#1-install-requirements-inside-wsl","title":"1. Install Requirements (Inside WSL)","text":"<p>Open your Ubuntu terminal and run:</p> <pre><code># 1. Update system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# 2. Install Python 3.10\nsudo apt install -y python3.10 python3.10-venv python3.10-dev\n\n# 3. Install Java (Required for Spark)\nsudo apt install -y openjdk-17-jdk\n</code></pre>"},{"location":"guides/wsl_setup/#2-setup-environment","title":"2. Setup Environment","text":"<ol> <li> <p>Clone/Go to your project: <code>bash     cd /mnt/d/odibi  # Accessing D: drive from Linux</code></p> </li> <li> <p>Create Virtual Environment: <code>bash     python3.10 -m venv .venv     source .venv/bin/activate</code></p> </li> <li> <p>Install Odibi: <code>bash     pip install \"odibi[spark]\"</code></p> </li> </ol>"},{"location":"guides/wsl_setup/#3-configure-vs-code","title":"3. Configure VS Code","text":"<ol> <li>Install the \"WSL\" extension in VS Code.</li> <li>Open your folder in Windows.</li> <li>Click the green button in the bottom-left corner (\"Open a Remote Window\").</li> <li>Select \"Reopen in WSL\".</li> </ol> <p>Now your terminal inside VS Code is a Linux terminal!</p>"},{"location":"guides/wsl_setup/#troubleshooting","title":"Troubleshooting","text":"<p>\"Java not found\" Make sure you installed <code>openjdk-17-jdk</code>. Add this to your <code>~/.bashrc</code>:</p> <pre><code>export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n</code></pre> <p>\"Command not found: odibi\" Did you activate your virtual environment?</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"patterns/","title":"Odibi Data Patterns","text":"<p>This directory contains documentation for common data pipeline patterns used in Odibi. Each pattern solves a specific problem and includes step-by-step examples.</p>"},{"location":"patterns/#patterns","title":"Patterns","text":""},{"location":"patterns/#1-append-only-raw-layer","title":"1. Append-Only Raw Layer","text":"<p>Problem: How do I safely ingest data without losing audit trails?</p> <p>Pattern: All data from sources is appended to the Raw layer without modification. Raw is immutable and append-only.</p> <p>When to use: Always, for all source ingestion. Raw is your safety net.</p>"},{"location":"patterns/#2-high-water-mark-smart-read","title":"2. High Water Mark (Smart Read)","text":"<p>Problem: My source table has millions of rows. How do I efficiently read only new/changed data?</p> <p>Pattern: Use the <code>incremental</code> configuration (Smart Read) to automatically filter the source query. Odibi manages the state for you: First Run = Full Load, Subsequent Runs = Incremental Load.</p> <p>When to use: When your source has timestamps (created_at, updated_at) and you want incremental reads. Essential for daily incremental loads.</p> <p>See also: Manual HWM Guide - understanding the underlying SQL pattern.</p>"},{"location":"patterns/#3-mergeupsert-silver-layer","title":"3. Merge/Upsert (Silver Layer)","text":"<p>Problem: How do I deduplicate and keep the latest version of each record?</p> <p>Pattern: Use Delta Lake's MERGE operation (or Merge Transformer) to upsert records by key, with audit columns tracking created/updated timestamps.</p> <p>When to use: Refining Raw \u2192 Silver. Always use for stateful transformations.</p>"},{"location":"patterns/#4-scd-type-2-history-tracking","title":"4. SCD Type 2 (History Tracking)","text":"<p>Problem: \"I need to know what the address was last month, not just now.\"</p> <p>Pattern: Track full history. Old records are closed (valid_to set), new records are opened (valid_to NULL). Preserves point-in-time accuracy.</p> <p>When to use: Slowly Changing Dimensions (Customer address, Product category).</p>"},{"location":"patterns/#5-windowed-reprocess-gold-layer-aggregates","title":"5. Windowed Reprocess (Gold Layer Aggregates)","text":"<p>Problem: Late-arriving data can break my aggregates. How do I fix them without double-counting?</p> <p>Pattern: Instead of patching aggregates with updates, recalculate the entire time window and overwrite that partition.</p> <p>When to use: Building Gold-layer aggregates (KPIs, star schemas). Ensures idempotency and correctness.</p>"},{"location":"patterns/#6-skip-if-unchanged-snapshot-optimization","title":"6. Skip If Unchanged (Snapshot Optimization)","text":"<p>Problem: My hourly pipeline appends identical data 24 times/day when the source hasn't changed.</p> <p>Pattern: Compute a hash of the DataFrame content before writing. If hash matches previous write, skip the append entirely.</p> <p>When to use: Snapshot tables without timestamps, reference data that changes infrequently, or when change frequency is unknown.</p>"},{"location":"patterns/#dimensional-modeling-patterns","title":"Dimensional Modeling Patterns","text":"<p>These patterns are designed for building star schemas and data warehouses. Use them via <code>transformer: pattern_name</code> in your node config.</p>"},{"location":"patterns/#7-dimension-pattern","title":"7. Dimension Pattern","text":"<p>Problem: How do I build dimension tables with surrogate keys and SCD support?</p> <p>Pattern: Use <code>transformer: dimension</code> to auto-generate surrogate keys and handle SCD Type 0/1/2 with optional unknown member rows.</p> <p>When to use: Building any dimension table (dim_customer, dim_product, etc.)</p> <pre><code>transformer: dimension\nparams:\n  natural_key: customer_id\n  surrogate_key: customer_sk\n  scd_type: 2\n  track_columns: [name, email, address]\n</code></pre>"},{"location":"patterns/#8-date-dimension-pattern","title":"8. Date Dimension Pattern","text":"<p>Problem: How do I generate a complete date dimension with fiscal calendars?</p> <p>Pattern: Use <code>transformer: date_dimension</code> to generate dates with 19 pre-calculated columns including fiscal year/quarter.</p> <p>When to use: Every data warehouse needs a date dimension. Generate once with a wide range (2015-2035).</p> <pre><code>transformer: date_dimension\nparams:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 7\n  unknown_member: true\n</code></pre>"},{"location":"patterns/#9-fact-pattern","title":"9. Fact Pattern","text":"<p>Problem: How do I build fact tables with automatic surrogate key lookups?</p> <p>Pattern: Use <code>transformer: fact</code> to join source data to dimensions, retrieve SKs, handle orphans, and validate grain.</p> <p>When to use: Building any fact table that references dimensions.</p> <pre><code>transformer: fact\nparams:\n  grain: [order_id, line_item_id]\n  dimensions:\n    - source_column: customer_id\n      dimension_table: dim_customer\n      dimension_key: customer_id\n      surrogate_key: customer_sk\n  orphan_handling: unknown\n</code></pre>"},{"location":"patterns/#10-aggregation-pattern","title":"10. Aggregation Pattern","text":"<p>Problem: How do I build aggregate tables with declarative GROUP BY and incremental refresh?</p> <p>Pattern: Use <code>transformer: aggregation</code> with grain (GROUP BY) and measure expressions.</p> <p>When to use: Building aggregate/summary tables, KPI tables, or materializing metrics.</p> <pre><code>transformer: aggregation\nparams:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n</code></pre>"},{"location":"patterns/#design-principles","title":"Design Principles","text":"<p>These patterns are built on the Odibi Architecture Manifesto:</p> <ol> <li>Robots Remember, Humans Forget \u2192 Use checkpoint bookkeeping, not manual state tracking</li> <li>Raw is Sacred \u2192 Append-only, immutable history. Never destroy original data.</li> <li>Rebuild the Bucket, Don't Patch the Hole \u2192 Reprocess entire time windows, don't patch aggregates</li> <li>SQL is for Humans, ADLS is for Robots \u2192 ADLS stores everything; SQL serves BI</li> <li>No Duplication \u2192 Test against production data; don't duplicate datasets</li> </ol>"},{"location":"patterns/#quick-reference","title":"Quick Reference","text":"Pattern Input Output Write Mode Idempotent? Append-Only Raw Source Raw <code>append</code> Yes (duplicates OK) High Water Mark Source + Timestamp Raw <code>append</code> Yes (filtered by timestamp) Smart Read Source + Timestamp Raw <code>append</code> Yes (auto-managed) Merge/Upsert Raw (micro-batch) Silver <code>merge</code> Yes (by key) SCD Type 2 Raw (micro-batch) Silver/Gold <code>overwrite</code> Yes (full history) Windowed Reprocess Silver (window) Gold <code>overwrite</code> (partition) Yes (recalculated) Skip If Unchanged Snapshot Source Raw <code>append</code> (conditional) Yes (hash-based) Dimension Staging Gold (dim_*) <code>overwrite</code> Yes (SK-based) Date Dimension Generated Gold (dim_date) <code>overwrite</code> Yes (no input) Fact Staging + Dims Gold (fact_*) <code>overwrite</code> Yes (grain-based) Aggregation Fact Gold (agg_*) <code>overwrite</code> Yes (grain-based)"},{"location":"patterns/#further-reading","title":"Further Reading","text":"<ul> <li>Odibi Architecture Manifesto</li> <li>Merge Transformer Spec</li> <li>Databricks: \"Incremental Processing\" documentation</li> <li>Book: Fundamentals of Data Engineering by Joe Reis &amp; Matt Housley</li> </ul>"},{"location":"patterns/aggregation/","title":"Aggregation Pattern","text":"<p>The <code>aggregation</code> pattern provides declarative aggregation with configurable grain, measures, and incremental merge strategies.</p>"},{"location":"patterns/aggregation/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The aggregation pattern works on data from the read block or a dependency, applies GROUP BY aggregation, and writes the results.</p> <pre><code>project: sales_analytics\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    nodes:\n      - name: agg_daily_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n\n        transformer: aggregation\n        params:\n          grain: [date_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          having: \"COUNT(*) &gt; 0\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#features","title":"Features","text":"<ul> <li>Declarative grain (GROUP BY columns)</li> <li>Flexible measure expressions (SUM, COUNT, AVG, etc.)</li> <li>Incremental aggregation (merge new data with existing)</li> <li>HAVING clause support</li> <li>Audit columns</li> </ul>"},{"location":"patterns/aggregation/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>grain</code> list Yes - Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes - Measure definitions with name and expr <code>having</code> str No - Optional HAVING clause <code>incremental</code> dict No - Incremental merge configuration <code>target</code> str For incremental - Target table for incremental merge <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/aggregation/#measure-definition","title":"Measure Definition","text":"<pre><code>params:\n  measures:\n    - name: total_revenue      # Output column name\n      expr: \"SUM(line_total)\"  # SQL aggregation expression\n</code></pre>"},{"location":"patterns/aggregation/#incremental-config","title":"Incremental Config","text":"<pre><code>params:\n  incremental:\n    timestamp_column: order_date  # Column to identify new data\n    merge_strategy: replace       # \"replace\" or \"sum\"\n  target: warehouse.agg_daily_sales\n</code></pre>"},{"location":"patterns/aggregation/#measure-expressions","title":"Measure Expressions","text":"<p>Use standard SQL aggregation functions:</p> <pre><code>params:\n  measures:\n    # Basic aggregations\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n\n    - name: max_order\n      expr: \"MAX(line_total)\"\n\n    # Complex expressions\n    - name: total_with_discount\n      expr: \"SUM(line_total - discount_amount)\"\n\n    - name: discount_rate\n      expr: \"SUM(discount_amount) / SUM(line_total)\"\n</code></pre>"},{"location":"patterns/aggregation/#incremental-merge-strategies","title":"Incremental Merge Strategies","text":""},{"location":"patterns/aggregation/#replace-strategy","title":"Replace Strategy","text":"<p>New aggregates overwrite existing for matching grain keys:</p> <pre><code>nodes:\n  - name: agg_daily_sales\n    read:\n      connection: warehouse\n      path: fact_orders\n    transformer: aggregation\n    params:\n      grain: [date_sk, product_sk]\n      measures:\n        - name: total_revenue\n          expr: \"SUM(line_total)\"\n      incremental:\n        timestamp_column: order_date\n        merge_strategy: replace\n      target: warehouse.agg_daily_sales\n    write:\n      connection: warehouse\n      path: agg_daily_sales\n      mode: overwrite\n</code></pre> <p>Use case: Full recalculation of affected grains (idempotent, handles late data)</p>"},{"location":"patterns/aggregation/#sum-strategy","title":"Sum Strategy","text":"<p>Add new measure values to existing aggregates:</p> <pre><code>params:\n  incremental:\n    timestamp_column: order_date\n    merge_strategy: sum\n</code></pre> <p>Use case: Additive metrics only (counts, sums) where data is append-only</p> <p>Warning: Don't use for AVG, DISTINCT counts, or ratios.</p>"},{"location":"patterns/aggregation/#time-rollups","title":"Time Rollups","text":"<p>Build aggregate hierarchies at multiple time grains:</p> <pre><code>pipelines:\n  - pipeline: build_aggregates\n    nodes:\n      # Daily aggregate (from fact)\n      - name: agg_daily_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n        transformer: aggregation\n        params:\n          grain: [date_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          mode: overwrite\n\n      # Monthly rollup (from daily aggregate)\n      - name: agg_monthly_sales\n        depends_on: [agg_daily_sales]\n        transform:\n          steps:\n            - sql: |\n                SELECT \n                  FLOOR(date_sk / 100) AS month_sk,\n                  product_sk,\n                  total_revenue,\n                  order_count\n                FROM df\n        transformer: aggregation\n        params:\n          grain: [month_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(total_revenue)\"\n            - name: order_count\n              expr: \"SUM(order_count)\"\n        write:\n          connection: warehouse\n          path: agg_monthly_sales\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#full-yaml-example","title":"Full YAML Example","text":"<p>Complete aggregation pipeline with incremental refresh:</p> <pre><code>project: sales_analytics\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_aggregates\n    description: \"Build daily and monthly sales aggregates\"\n    nodes:\n      - name: agg_daily_product_sales\n        description: \"Daily product sales aggregate\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n        transformer: aggregation\n        params:\n          grain:\n            - date_sk\n            - product_sk\n            - region\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: total_cost\n              expr: \"SUM(cost_amount)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: units_sold\n              expr: \"SUM(quantity)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n            - name: avg_unit_price\n              expr: \"AVG(unit_price)\"\n          having: \"SUM(line_total) &gt; 0\"\n          incremental:\n            timestamp_column: load_timestamp\n            merge_strategy: replace\n          target: warehouse.agg_daily_product_sales\n          audit:\n            load_timestamp: true\n            source_system: \"aggregation_pipeline\"\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#see-also","title":"See Also","text":"<ul> <li>Fact Pattern - Build fact tables</li> <li>Semantic Layer - Define metrics for ad-hoc queries</li> <li>Materializing Metrics - Schedule metric materialization</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/append_only_raw/","title":"Pattern: Append-Only Raw Layer (Landing \u2192 Raw)","text":"<p>Status: Core Pattern Layer: Raw (Bronze) Engine: Spark Structured Streaming or Batch Write Mode: <code>append</code> </p>"},{"location":"patterns/append_only_raw/#problem","title":"Problem","text":"<p>You have source data coming from multiple places (SQL databases, APIs, files). You need to: - Preserve the complete history of what arrived and when - Enable replay/reconstruction if downstream logic breaks - Avoid data loss from overwriting mistakes</p>"},{"location":"patterns/append_only_raw/#solution","title":"Solution","text":"<p>Append every piece of data you receive to the Raw layer without modification. Raw is the immutable audit log.</p>"},{"location":"patterns/append_only_raw/#key-principles","title":"Key Principles","text":"<ol> <li>Raw is Sacred \u2192 Never delete or overwrite Raw data</li> <li>Append-Only \u2192 Each run appends new rows (possibly duplicates)</li> <li>One-to-One Copy \u2192 Raw mirrors the source schema, nothing more</li> <li>Permanent Retention \u2192 Keep forever; storage is cheap</li> </ol>"},{"location":"patterns/append_only_raw/#pattern-flow","title":"Pattern Flow","text":"<pre><code>Source System (SQL, API, Files)\n         \u2193\n    Read Data\n         \u2193\n   Append to Raw\n    (append mode)\n         \u2193\nRaw Layer (history preserved)\n         \u2193\n[Next: Merge into Silver for dedup/cleanup]\n</code></pre>"},{"location":"patterns/append_only_raw/#example-sql-source-raw-layer","title":"Example: SQL Source \u2192 Raw Layer","text":""},{"location":"patterns/append_only_raw/#scenario","title":"Scenario","text":"<p>You have a SQL table <code>dbo.orders</code> with new/updated rows arriving daily.</p> <p>First Run (Day 1):</p> <p>Source (<code>dbo.orders</code>):</p> <pre><code>order_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n</code></pre> <p>After Append to Raw:</p> <pre><code>Raw.orders (Delta Table)\norder_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n</code></pre> <p>Second Run (Day 2):</p> <p>Source now has 2 new rows + 1 duplicate:</p> <pre><code>order_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00  \u2190 Already seen\n2        | Widget B     | 5   | 2025-11-01 11:00  \u2190 Already seen\n3        | Widget A     | 2   | 2025-11-01 12:00  \u2190 Already seen\n4        | Widget C     | 8   | 2025-11-02 09:00  \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00  \u2190 NEW\n</code></pre> <p>After Append to Raw:</p> <pre><code>Raw.orders (all 8 rows)\norder_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n1        | Widget A     | 10  | 2025-11-01 10:00  \u2190 DUPLICATE (OK)\n2        | Widget B     | 5   | 2025-11-01 11:00  \u2190 DUPLICATE (OK)\n3        | Widget A     | 2   | 2025-11-01 12:00  \u2190 DUPLICATE (OK)\n4        | Widget C     | 8   | 2025-11-02 09:00\n5        | Widget B     | 12  | 2025-11-02 10:00\n</code></pre> <p>Note: Duplicates in Raw are OK. Silver's merge will deduplicate them.</p>"},{"location":"patterns/append_only_raw/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/append_only_raw/#using-high-water-mark-incremental","title":"Using High Water Mark (Incremental)","text":"<pre><code>pipelines:\n  - pipeline: sql_to_raw\n    layer: bronze\n    nodes:\n      - id: load_orders_raw\n        name: \"Load Orders to Raw (Bronze)\"\n        read:\n          connection: sql_prod\n          format: sql\n          table: dbo.orders\n          options:\n            query: |\n              SELECT * FROM dbo.orders\n              WHERE COALESCE(updated_at, created_at) &gt; (\n                SELECT COALESCE(MAX(COALESCE(updated_at, created_at)), '1900-01-01')\n                FROM raw.orders\n              )\n        write:\n          connection: adls_prod\n          format: delta\n          table: raw.orders\n          mode: append\n</code></pre>"},{"location":"patterns/append_only_raw/#using-scheduled-batch-full-rescan","title":"Using Scheduled Batch (Full Rescan)","text":"<p>If your source doesn't have timestamps, rescan everything and append:</p> <pre><code>      - id: load_orders_raw\n        name: \"Load Orders to Raw\"\n        read:\n          connection: sql_prod\n          format: sql\n          table: dbo.orders\n        write:\n          connection: adls_prod\n          format: delta\n          table: raw.orders\n          mode: append\n</code></pre> <p>Warning: This will append the entire table each run, creating duplicates. Use High Water Mark pattern when possible.</p>"},{"location":"patterns/append_only_raw/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/append_only_raw/#advantages","title":"Advantages","text":"<p>\u2713 Complete audit trail (when did each row arrive?) \u2713 Safe replay/reconstruction if Silver breaks \u2713 Simple logic (no deduplication, no logic) \u2713 Idempotent (rerun is safe; creates duplicates but that's OK)  </p>"},{"location":"patterns/append_only_raw/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Storage grows unbounded (mitigated by cheap cloud storage) \u2717 Duplicates must be handled downstream (Silver's job) \u2717 Full rescans can be slow without timestamp filtering  </p>"},{"location":"patterns/append_only_raw/#when-to-use","title":"When to Use","text":"<p>Always use Append-Only for Raw ingestion. No exceptions.</p>"},{"location":"patterns/append_only_raw/#when-not-to-use","title":"When NOT to Use","text":"<p>Never overwrite Raw. Never delete from Raw. Never use <code>merge</code> in Raw.</p>"},{"location":"patterns/append_only_raw/#related-patterns","title":"Related Patterns","text":"<ul> <li>High Water Mark \u2192 Efficiently filter SQL sources for incremental reads</li> <li>Merge/Upsert \u2192 Deduplicate Raw data in Silver</li> </ul>"},{"location":"patterns/append_only_raw/#references","title":"References","text":"<ul> <li>Odibi Architecture Manifesto: Pattern A - Ingestion</li> <li>Raw is Sacred Principle</li> </ul>"},{"location":"patterns/date_dimension/","title":"Date Dimension Pattern","text":"<p>The <code>date_dimension</code> pattern generates a complete date dimension table with pre-calculated attributes useful for BI/reporting.</p>"},{"location":"patterns/date_dimension/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The date dimension pattern is unique - it generates data rather than transforming it. No <code>read:</code> block is needed.</p> <pre><code>project: my_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    nodes:\n      - name: dim_date\n        # No read block - pattern generates data\n        transformer: date_dimension\n        params:\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n          fiscal_year_start_month: 7  # July fiscal year\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/date_dimension/#features","title":"Features","text":"<ul> <li>Date range generation from start_date to end_date</li> <li>Fiscal calendar support with configurable fiscal year start month</li> <li>19 pre-calculated columns for flexible analysis</li> <li>Unknown member row (date_sk=0) for orphan FK handling</li> <li>Works with both Spark and Pandas</li> </ul>"},{"location":"patterns/date_dimension/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>start_date</code> str Yes - Start date in YYYY-MM-DD format <code>end_date</code> str Yes - End date in YYYY-MM-DD format <code>fiscal_year_start_month</code> int No 1 Month when fiscal year starts (1-12) <code>unknown_member</code> bool No false Add unknown date row with date_sk=0"},{"location":"patterns/date_dimension/#generated-columns","title":"Generated Columns","text":"<p>The pattern generates 19 columns automatically:</p> Column Type Description Example <code>date_sk</code> int Surrogate key (YYYYMMDD format) 20240115 <code>full_date</code> date The actual date 2024-01-15 <code>day_of_week</code> str Day name Monday <code>day_of_week_num</code> int Day number (1=Monday, 7=Sunday) 1 <code>day_of_month</code> int Day of month (1-31) 15 <code>day_of_year</code> int Day of year (1-366) 15 <code>is_weekend</code> bool Weekend flag false <code>week_of_year</code> int ISO week number (1-53) 3 <code>month</code> int Month number (1-12) 1 <code>month_name</code> str Month name January <code>quarter</code> int Calendar quarter (1-4) 1 <code>quarter_name</code> str Quarter name Q1 <code>year</code> int Calendar year 2024 <code>fiscal_year</code> int Fiscal year 2024 <code>fiscal_quarter</code> int Fiscal quarter (1-4) 3 <code>is_month_start</code> bool First day of month false <code>is_month_end</code> bool Last day of month false <code>is_year_start</code> bool First day of year false <code>is_year_end</code> bool Last day of year false"},{"location":"patterns/date_dimension/#fiscal-calendar-configuration","title":"Fiscal Calendar Configuration","text":"<p>Configure fiscal year start month for companies with non-calendar fiscal years:</p> <pre><code>nodes:\n  - name: dim_date\n    transformer: date_dimension\n    params:\n      start_date: \"2020-01-01\"\n      end_date: \"2030-12-31\"\n      fiscal_year_start_month: 7  # July 1st = FY start\n    write:\n      connection: warehouse\n      path: dim_date\n      mode: overwrite\n</code></pre> <p>Fiscal Year Calculation: - If <code>fiscal_year_start_month = 7</code> (July) - July 2024 \u2192 FY 2025 - June 2024 \u2192 FY 2024</p> <p>Fiscal Quarter Calculation: - Fiscal Q1: July, August, September - Fiscal Q2: October, November, December - Fiscal Q3: January, February, March - Fiscal Q4: April, May, June</p>"},{"location":"patterns/date_dimension/#unknown-member-row","title":"Unknown Member Row","text":"<p>Enable <code>unknown_member: true</code> to add a special row for orphan FK handling:</p> Column Value date_sk 0 full_date 1900-01-01 day_of_week Unknown day_of_week_num 0 month_name Unknown quarter_name Unknown year 0"},{"location":"patterns/date_dimension/#full-yaml-example","title":"Full YAML Example","text":"<p>Complete date dimension in a warehouse pipeline:</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_reference_dimensions\n    description: \"Build date and other reference dimensions\"\n    nodes:\n      - name: dim_date\n        description: \"Standard date dimension with fiscal calendar\"\n        transformer: date_dimension\n        params:\n          start_date: \"2015-01-01\"\n          end_date: \"2035-12-31\"\n          fiscal_year_start_month: 10  # October fiscal year (retail)\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n          partition_by: [year]  # Optional: partition by year\n</code></pre>"},{"location":"patterns/date_dimension/#common-fiscal-year-configurations","title":"Common Fiscal Year Configurations","text":""},{"location":"patterns/date_dimension/#retail-calendar-october-fy","title":"Retail Calendar (October FY)","text":"<pre><code>fiscal_year_start_month: 10\n</code></pre>"},{"location":"patterns/date_dimension/#government-calendar-october-fy","title":"Government Calendar (October FY)","text":"<pre><code>fiscal_year_start_month: 10\n</code></pre>"},{"location":"patterns/date_dimension/#education-calendar-july-fy","title":"Education Calendar (July FY)","text":"<pre><code>fiscal_year_start_month: 7\n</code></pre>"},{"location":"patterns/date_dimension/#standard-calendar-year","title":"Standard Calendar Year","text":"<pre><code>fiscal_year_start_month: 1  # Default\n</code></pre>"},{"location":"patterns/date_dimension/#complete-configuration-reference","title":"Complete Configuration Reference","text":""},{"location":"patterns/date_dimension/#all-parameters","title":"All Parameters","text":"Parameter Type Required Default Description <code>start_date</code> str Yes - Start date in YYYY-MM-DD format <code>end_date</code> str Yes - End date in YYYY-MM-DD format <code>fiscal_year_start_month</code> int No 1 Month when fiscal year starts (1-12) <code>unknown_member</code> bool No false Add unknown date row with date_sk=0 <code>date_format</code> str No \"%Y-%m-%d\" Format string for date parsing <code>week_start_day</code> int No 1 First day of week (1=Monday, 7=Sunday) <code>include_holidays</code> bool No false Generate is_holiday column (requires holiday_country) <code>holiday_country</code> str No \"US\" Country code for holiday calendar"},{"location":"patterns/date_dimension/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code>nodes:\n  - name: dim_date\n    transformer: date_dimension\n    params:\n      # Required\n      start_date: \"2015-01-01\"\n      end_date: \"2035-12-31\"\n\n      # Fiscal calendar\n      fiscal_year_start_month: 7      # July fiscal year\n\n      # Unknown member\n      unknown_member: true            # Add SK=0 row for orphans\n\n      # Week configuration\n      week_start_day: 1               # Monday (ISO standard)\n\n      # Holiday support (if holidays package installed)\n      include_holidays: true\n      holiday_country: \"US\"\n    write:\n      connection: warehouse\n      path: dim_date\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/date_dimension/#output-column-customization","title":"Output Column Customization","text":"<p>To customize output columns, add a SQL step after generation:</p> <pre><code>nodes:\n  - name: dim_date_raw\n    transformer: date_dimension\n    params:\n      start_date: \"2020-01-01\"\n      end_date: \"2030-12-31\"\n      fiscal_year_start_month: 10\n      unknown_member: true\n\n  - name: dim_date\n    depends_on: [dim_date_raw]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              date_sk,\n              full_date,\n              day_of_week,\n              month_name,\n              quarter_name,\n              year,\n              fiscal_year,\n              fiscal_quarter,\n              is_weekend,\n              -- Custom columns\n              CONCAT(year, '-', LPAD(month, 2, '0')) AS year_month,\n              CASE WHEN month IN (11, 12) THEN true ELSE false END AS is_holiday_season\n            FROM dim_date_raw\n    write:\n      connection: warehouse\n      path: dim_date\n</code></pre>"},{"location":"patterns/date_dimension/#see-also","title":"See Also","text":"<ul> <li>Dimension Pattern - Build regular dimensions</li> <li>Fact Pattern - Build fact tables with SK lookups</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/dimension/","title":"Dimension Pattern","text":"<p>The <code>dimension</code> pattern builds complete dimension tables with automatic surrogate key generation and SCD (Slowly Changing Dimension) support.</p>"},{"location":"patterns/dimension/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>Patterns are used via the <code>transformer:</code> field in a node config. The pattern name goes in <code>transformer:</code> and configuration goes in <code>params:</code>.</p> <pre><code>project: my_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dimensions\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n          format: delta\n\n        # Use dimension pattern via transformer\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns: [name, email, address]\n          target: warehouse.dim_customer\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#features","title":"Features","text":"<ul> <li>Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows)</li> <li>SCD Type 0 (static - never update existing records)</li> <li>SCD Type 1 (overwrite - update in place, no history)</li> <li>SCD Type 2 (history tracking - full audit trail)</li> <li>Unknown member row (SK=0) for orphan FK handling</li> <li>Audit columns (load_timestamp, source_system)</li> </ul>"},{"location":"patterns/dimension/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>natural_key</code> str Yes - Natural/business key column name <code>surrogate_key</code> str Yes - Surrogate key column name to generate <code>scd_type</code> int No 1 0=static, 1=overwrite, 2=history tracking <code>track_columns</code> list For SCD1/2 - Columns to track for changes <code>target</code> str For SCD2 - Target table path (required to read existing history) <code>unknown_member</code> bool No false Insert a row with SK=0 for orphan FK handling <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/dimension/#audit-config","title":"Audit Config","text":"<pre><code>params:\n  # ... other params ...\n  audit:\n    load_timestamp: true      # Add load_timestamp column\n    source_system: \"pos\"      # Add source_system column with this value\n</code></pre>"},{"location":"patterns/dimension/#scd-type-0-static","title":"SCD Type 0 (Static)","text":"<p>Static dimensions never update existing records. Only new records (not matching natural key) are inserted.</p> <p>Use case: Reference data that never changes (ISO country codes, fixed lookup values).</p> <pre><code>nodes:\n  - name: dim_country\n    read:\n      connection: staging\n      path: countries\n    transformer: dimension\n    params:\n      natural_key: country_code\n      surrogate_key: country_sk\n      scd_type: 0\n      target: warehouse.dim_country\n    write:\n      connection: warehouse\n      path: dim_country\n      mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#scd-type-1-overwrite","title":"SCD Type 1 (Overwrite)","text":"<p>Overwrite dimensions update existing records in place. No history is kept.</p> <p>Use case: Attributes where you only care about the current value (customer email, product price).</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: staging\n      path: customers\n    transformer: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 1\n      track_columns: [name, email, address]\n      target: warehouse.dim_customer\n      audit:\n        load_timestamp: true\n    write:\n      connection: warehouse\n      path: dim_customer\n      mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#scd-type-2-history-tracking","title":"SCD Type 2 (History Tracking)","text":"<p>History-tracking dimensions preserve full audit trail. Old records are closed, new versions are opened.</p> <p>Use case: Slowly changing attributes where historical accuracy matters (customer address for point-in-time reporting).</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: staging\n      path: customers\n    transformer: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_columns: [name, email, address, city, state]\n      target: warehouse.dim_customer\n      valid_from_col: valid_from     # Optional, default: valid_from\n      valid_to_col: valid_to         # Optional, default: valid_to\n      is_current_col: is_current     # Optional, default: is_current\n      unknown_member: true\n      audit:\n        load_timestamp: true\n        source_system: \"crm\"\n    write:\n      connection: warehouse\n      path: dim_customer\n      mode: overwrite\n</code></pre> <p>Generated Columns: - <code>valid_from</code>: Timestamp when this version became active - <code>valid_to</code>: Timestamp when this version was superseded (NULL for current) - <code>is_current</code>: Boolean flag (true for current version)</p>"},{"location":"patterns/dimension/#unknown-member-handling","title":"Unknown Member Handling","text":"<p>Enable <code>unknown_member: true</code> to automatically insert a row with SK=0. This allows fact tables to reference unknown dimensions without FK violations.</p> <p>Generated Unknown Member Row:</p> customer_sk customer_id name email valid_from is_current 0 -1 Unknown Unknown 1900-01-01 true"},{"location":"patterns/dimension/#full-star-schema-example","title":"Full Star Schema Example","text":"<p>Complete pipeline building dimensions for a star schema:</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_dimensions\n    nodes:\n      # Customer dimension with SCD2\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n          format: delta\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns:\n            - name\n            - email\n            - phone\n            - address_line_1\n            - city\n            - state\n            - postal_code\n          target: warehouse.dim_customer\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"salesforce\"\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n          mode: overwrite\n\n      # Product dimension with SCD1 (no history)\n      - name: dim_product\n        read:\n          connection: staging\n          path: products\n          format: delta\n        transformer: dimension\n        params:\n          natural_key: product_id\n          surrogate_key: product_sk\n          scd_type: 1\n          track_columns: [name, category, price, status]\n          target: warehouse.dim_product\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_product\n          format: delta\n          mode: overwrite\n\n      # Date dimension (generated, no source read needed)\n      - name: dim_date\n        transformer: date_dimension\n        params:\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n          fiscal_year_start_month: 7\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#python-api","title":"Python API","text":"<pre><code>from odibi.patterns.dimension import DimensionPattern\nfrom odibi.context import EngineContext\n\n# Create pattern instance\npattern = DimensionPattern(\n    engine=my_engine,\n    config=node_config  # NodeConfig with params\n)\n\n# Or directly with params dict\nfrom odibi.patterns.dimension import DimensionPattern\n\npattern = DimensionPattern(params={\n    \"natural_key\": \"customer_id\",\n    \"surrogate_key\": \"customer_sk\",\n    \"scd_type\": 2,\n    \"track_columns\": [\"name\", \"email\", \"address\"],\n    \"target\": \"gold.dim_customer\",\n    \"unknown_member\": True,\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"crm\"\n    }\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern\ncontext = EngineContext(df=source_df, engine_type=EngineType.SPARK)\nresult_df = pattern.execute(context)\n</code></pre>"},{"location":"patterns/dimension/#see-also","title":"See Also","text":"<ul> <li>Date Dimension Pattern - Generate date dimensions</li> <li>Fact Pattern - Build fact tables with SK lookups</li> <li>Aggregation Pattern - Build aggregate tables</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/fact/","title":"Fact Pattern","text":"<p>The <code>fact</code> pattern builds fact tables with automatic surrogate key lookups from dimension tables, orphan handling, grain validation, and measure calculations.</p>"},{"location":"patterns/fact/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The fact pattern looks up dimension tables from context - dimensions must be registered (either by running dimension nodes in the same pipeline with <code>depends_on</code>, or by reading them from storage).</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_star_schema\n    nodes:\n      # First, build or load dimensions\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n        # Just loading - no transform needed\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: delta\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: delta\n\n      # Then build fact table with SK lookups\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: staging\n          path: orders\n          format: delta\n\n        transformer: fact\n        params:\n          grain: [order_id, line_item_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer  # References node name\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/fact/#features","title":"Features","text":"<ul> <li>Automatic SK lookups from dimension tables</li> <li>Orphan handling (unknown member, reject, or quarantine)</li> <li>Grain validation (detect duplicates at PK level)</li> <li>Deduplication support</li> <li>Measure calculations and renaming</li> <li>Audit columns (load_timestamp, source_system)</li> <li>SCD2 dimension support (filter to is_current=true)</li> </ul>"},{"location":"patterns/fact/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>grain</code> list No - Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No [] Dimension lookup configurations <code>orphan_handling</code> str No \"unknown\" \"unknown\", \"reject\", or \"quarantine\" <code>measures</code> list No [] Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No false Remove duplicates before insert <code>keys</code> list Required if deduplicate - Keys for deduplication <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/fact/#dimension-lookup-config","title":"Dimension Lookup Config","text":"<pre><code>params:\n  dimensions:\n    - source_column: customer_id     # Column in source data\n      dimension_table: dim_customer  # Node name in context\n      dimension_key: customer_id     # Natural key column in dimension\n      surrogate_key: customer_sk     # Surrogate key to retrieve\n      scd2: true                     # If true, filter is_current=true\n</code></pre>"},{"location":"patterns/fact/#measures-config","title":"Measures Config","text":"<pre><code>params:\n  measures:\n    - quantity                           # Passthrough\n    - revenue: total_amount              # Rename\n    - line_total: \"quantity * unit_price\" # Calculate\n</code></pre>"},{"location":"patterns/fact/#orphan-handling","title":"Orphan Handling","text":"<p>Three strategies for handling source records that don't match any dimension:</p>"},{"location":"patterns/fact/#1-unknown-default","title":"1. Unknown (Default)","text":"<p>Map orphans to the unknown member (SK=0):</p> <pre><code>orphan_handling: unknown\n</code></pre>"},{"location":"patterns/fact/#2-reject","title":"2. Reject","text":"<p>Fail the pipeline if any orphans exist:</p> <pre><code>orphan_handling: reject\n</code></pre>"},{"location":"patterns/fact/#3-quarantine","title":"3. Quarantine","text":"<p>Route orphans to quarantine table:</p> <pre><code>orphan_handling: quarantine\n</code></pre>"},{"location":"patterns/fact/#grain-validation","title":"Grain Validation","text":"<p>Define the fact table grain to detect duplicate records:</p> <pre><code>params:\n  grain: [order_id, line_item_id]\n</code></pre> <p>If duplicates exist, the pattern raises an error with details.</p>"},{"location":"patterns/fact/#full-star-schema-example","title":"Full Star Schema Example","text":"<p>Complete pipeline building dimensions AND fact tables:</p> <pre><code>project: sales_star_schema\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  # Pipeline 1: Build dimensions\n  - pipeline: build_dimensions\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns: [name, email, region]\n          target: warehouse.dim_customer\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_customer\n          mode: overwrite\n\n      - name: dim_product\n        read:\n          connection: staging\n          path: products\n        transformer: dimension\n        params:\n          natural_key: product_id\n          surrogate_key: product_sk\n          scd_type: 1\n          track_columns: [name, category, price]\n          target: warehouse.dim_product\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_product\n          mode: overwrite\n\n      - name: dim_date\n        transformer: date_dimension\n        params:\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n          unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          mode: overwrite\n\n  # Pipeline 2: Build fact table (depends on dimensions existing)\n  - pipeline: build_facts\n    nodes:\n      # Load dimensions into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n\n      # Build fact table\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: staging\n          path: orders\n        transformer: fact\n        params:\n          grain: [order_id, line_item_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - discount_amount\n            - line_total: \"quantity * unit_price\"\n            - net_amount: \"quantity * unit_price - discount_amount\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n        write:\n          connection: warehouse\n          path: fact_orders\n          mode: overwrite\n</code></pre>"},{"location":"patterns/fact/#see-also","title":"See Also","text":"<ul> <li>Dimension Pattern - Build dimensions with SCD support</li> <li>Date Dimension Pattern - Generate date dimensions</li> <li>Aggregation Pattern - Build aggregate tables</li> <li>FK Validation - Additional FK validation</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/incremental_stateful/","title":"Stateful Incremental Loading","text":"<p>Stateful Incremental Loading is the \"Auto-Pilot\" mode for ingestion. Unlike Smart Read (Rolling Window) which blindly looks back X days, Stateful Mode remembers exactly where it left off.</p> <p>It tracks the High Water Mark (HWM)\u2014the maximum value of a column (e.g., <code>updated_at</code> or <code>id</code>) seen in the previous run\u2014and only fetches records greater than that value.</p>"},{"location":"patterns/incremental_stateful/#when-to-use-this","title":"When to use this?","text":"<ul> <li>CDC-like Ingestion: You want to sync a large table and only get new rows.</li> <li>Exactness: You don't want to guess a lookback window (e.g., \"3 days just to be safe\").</li> <li>Performance: You want to query the absolute minimum data required.</li> </ul>"},{"location":"patterns/incremental_stateful/#configuration","title":"Configuration","text":"<p>Enable it by setting <code>mode: stateful</code> in the <code>incremental</code> block.</p> <pre><code>- name: \"ingest_orders\"\n  read:\n    connection: \"postgres_prod\"\n    format: \"sql\"\n    table: \"public.orders\"\n\n    incremental:\n      mode: \"stateful\"              # Enable State Tracking\n      column: \"updated_at\"      # Column to track (max value is saved)\n      fallback_column: \"created_at\" # Optional: Use this if key_column is NULL\n      watermark_lag: \"30m\"          # Safety buffer (overlaps the window)\n      state_key: \"orders_ingest\"    # Optional: Custom ID for the state file\n\n  write:\n    connection: \"bronze\"\n    format: \"delta\"\n    table: \"orders_bronze\"\n    mode: \"append\"\n</code></pre>"},{"location":"patterns/incremental_stateful/#how-it-works","title":"How It Works","text":"<ol> <li> <p>First Run (Bootstrap)</p> <ul> <li>Odibi checks the state backend (Delta table or local JSON).</li> <li>No state found? $\\rightarrow$ Full Load (<code>SELECT * FROM table</code>).</li> <li>After success, it saves <code>MAX(updated_at)</code> as the HWM.</li> </ul> </li> <li> <p>Subsequent Runs (Incremental)</p> <ul> <li>Odibi retrieves the last HWM (e.g., <code>2023-10-25 10:00:00</code>).</li> <li>It subtracts the <code>watermark_lag</code> (e.g., 30 mins) $\\rightarrow$ <code>09:30:00</code>.</li> <li>Generates query: <code>SELECT * FROM table WHERE updated_at &gt; '2023-10-25 09:30:00'</code>.</li> <li>After success, it updates the HWM with the new maximum from the fetched batch.</li> </ul> </li> </ol>"},{"location":"patterns/incremental_stateful/#key-features","title":"Key Features","text":""},{"location":"patterns/incremental_stateful/#watermark-lag","title":"\ud83c\udf0a Watermark Lag","text":"<p>Data often arrives late or out of order. If you run your pipeline at 10:00, you might miss a record timestamped 09:59 that gets committed at 10:01.</p> <p>The <code>watermark_lag</code> creates a safety overlap. *   Lag: \"30m\" implies: \"Give me everything since the last run, but re-read the last 30 minutes just in case.\" *   This ensures At-Least-Once delivery. *   Note: This causes duplicates in the Bronze layer. This is expected! Your Silver layer (Merge/Upsert) handles deduplication.</p>"},{"location":"patterns/incremental_stateful/#state-backends","title":"\ud83d\udee1\ufe0f State Backends","text":"<p>Odibi automatically chooses the best backend: *   Spark/Databricks: Uses a Delta table (<code>odibi_meta.state</code>) to track HWMs. This is robust and supports concurrency. *   Pandas/Local: Uses a local JSON file (<code>.odibi/state.json</code>).</p>"},{"location":"patterns/incremental_stateful/#resets","title":"\ud83d\udd04 Resets","text":"<p>To reset the state and force a full reload: 1.  Delete the target table/file. 2.  Clear the state entry (manually or via CLI - CLI command coming soon).</p>"},{"location":"patterns/incremental_stateful/#comparison-rolling-window-vs-stateful","title":"Comparison: Rolling Window vs. Stateful","text":"Feature Rolling Window (<code>smart_read</code>) Stateful (<code>stateful</code>) Logic <code>NOW() - lookback</code> <code>&gt; Last HWM</code> State Stateless (Time-based) Stateful (Persisted) Best For Reporting windows (\"Last 30 days\") Ingestion / Replication (\"Sync table\") Complexity Low Medium Safety Good (if lookback is large) Excellent (Exact tracking)"},{"location":"patterns/incremental_stateful/#example-cdc-ingestion-pipeline","title":"Example: CDC Ingestion Pipeline","text":"<p>Here is a robust pattern for database replication:</p> <pre><code>nodes:\n  # 1. Ingest (Bronze) - Accumulates history with duplicates\n  - name: \"ingest_users\"\n    read:\n      connection: \"db_prod\"\n      table: \"users\"\n      incremental:\n        mode: \"stateful\"\n        key_column: \"updated_at\"\n        watermark_lag: \"15m\"\n    write:\n      connection: \"lake\"\n      format: \"delta\"\n      table: \"bronze_users\"\n      mode: \"append\"\n\n  # 2. Merge (Silver) - Deduplicates and keeps current state\n  - name: \"dim_users\"\n    depends_on: [\"ingest_users\"] # Reads ONLY the new batch\n    transformer: \"merge\"\n    params:\n      keys: [\"user_id\"]\n      order_by: \"updated_at DESC\"\n    write:\n      connection: \"lake\"\n      format: \"delta\"\n      table: \"silver_users\"\n      mode: \"upsert\"\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/","title":"[Legacy] Manual High Water Mark (HWM)","text":"<p>\u26a0\ufe0f Deprecated Pattern</p> <p>This manual pattern is no longer recommended. Please use the new Stateful Incremental Loading feature which handles this automatically.</p>"},{"location":"patterns/legacy_hwm_manual/#what-is-hwm","title":"What Is HWM?","text":"<p>A pattern for incremental data loading: load all data once on the first run, then load only new/changed data on each subsequent run.</p> <p>Day 1: Load 10 years of history Day 2+: Load only today's new records</p>"},{"location":"patterns/legacy_hwm_manual/#the-pattern","title":"The Pattern","text":""},{"location":"patterns/legacy_hwm_manual/#configuration","title":"Configuration","text":"<pre><code>nodes:\n  load_orders:\n    read:\n      connection: my_sql_server\n      format: sql_server\n      query: |\n        SELECT\n          order_id,\n          customer_id,\n          amount,\n          created_at,\n          updated_at\n        FROM dbo.orders\n        WHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n\n    write:\n      connection: adls\n      format: delta\n      path: bronze/orders\n      mode: append\n      first_run_query: |\n        SELECT\n          order_id,\n          customer_id,\n          amount,\n          created_at,\n          updated_at\n        FROM dbo.orders\n      options:\n        cluster_by: [created_at]\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#how-it-works","title":"How It Works","text":"<p>Day 1 (table doesn't exist): - Odibi runs <code>first_run_query</code> - Loads ALL orders from source - Creates table with clustering - Write mode: OVERWRITE</p> <p>Day 2+ (table exists): - Odibi runs regular <code>query</code> - Loads only orders modified in last 1 day - Appends to table - Write mode: APPEND</p>"},{"location":"patterns/legacy_hwm_manual/#the-two-queries","title":"The Two Queries","text":""},{"location":"patterns/legacy_hwm_manual/#first_run_query","title":"<code>first_run_query</code>","text":"<pre><code>SELECT * FROM dbo.orders\n</code></pre> <ul> <li>Loads everything</li> <li>Runs once on day 1</li> <li>Takes longer, but happens only once</li> </ul>"},{"location":"patterns/legacy_hwm_manual/#query-incremental","title":"<code>query</code> (incremental)","text":"<pre><code>SELECT * FROM dbo.orders\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n</code></pre> <ul> <li>Loads only new/updated records</li> <li>Runs every day from day 2 onward</li> <li>Takes seconds</li> </ul>"},{"location":"patterns/legacy_hwm_manual/#key-concepts","title":"Key Concepts","text":""},{"location":"patterns/legacy_hwm_manual/#1-track-changes","title":"1. Track Changes","text":"<p>Use <code>updated_at</code> if available. If not, use <code>created_at</code>.</p> <pre><code>-- Prefer updated_at (catches changes)\nWHERE updated_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- Fallback to created_at (new records only)\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- Use both (catches new + modified)\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#2-time-ranges","title":"2. Time Ranges","text":"<p>Use overlapping time ranges (2 days instead of 1) to catch late-arriving data:</p> <pre><code>-- 1 day: misses records that arrived late\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- 2 days: safer, catches late arrivals\nWHERE created_at &gt;= DATEADD(DAY, -2, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#3-clustering","title":"3. Clustering","text":"<p>On first run, apply clustering for query performance:</p> <pre><code>options:\n  cluster_by: [created_at]  # Applied day 1, speeds up incremental queries\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#setup-steps","title":"Setup Steps","text":"<ol> <li>Identify HWM column (<code>updated_at</code> or <code>created_at</code>)</li> <li>Write first_run_query (<code>SELECT * FROM table</code>)</li> <li>Write incremental query (<code>SELECT * WHERE timestamp &gt;= date_function</code>)</li> <li>Add options (<code>cluster_by</code> for performance)</li> <li>Deploy and let Odibi handle the rest</li> </ol>"},{"location":"patterns/legacy_hwm_manual/#example-orders-table","title":"Example: Orders Table","text":""},{"location":"patterns/legacy_hwm_manual/#source-data-dboorders","title":"Source Data (dbo.orders)","text":"<pre><code>order_id | customer_id | amount | created_at          | updated_at\n1        | 100         | 99.99  | 2025-01-20 10:00:00 | NULL\n2        | 101         | 49.99  | 2025-01-21 14:30:00 | 2025-01-21 15:00:00\n3        | 102         | 199.99 | 2025-01-23 09:00:00 | NULL\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#odibi-configuration","title":"Odibi Configuration","text":"<pre><code>nodes:\n  load_orders:\n    read:\n      connection: sql_server\n      format: sql_server\n      query: |\n        SELECT order_id, customer_id, amount, created_at, updated_at\n        FROM dbo.orders\n        WHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n\n    write:\n      connection: adls\n      format: delta\n      path: bronze/orders\n      mode: append\n      first_run_query: |\n        SELECT order_id, customer_id, amount, created_at, updated_at\n        FROM dbo.orders\n      options:\n        cluster_by: [created_at]\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#execution-timeline","title":"Execution Timeline","text":"<p>Day 1 (2025-01-20):</p> <pre><code>Run: first_run_query\nLoads: All 3 orders\nWrite: OVERWRITE (creates table)\nBronze table: 3 rows\n</code></pre> <p>Day 2 (2025-01-21):</p> <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-20)\nLoads: Order 1 (updated), Order 2 (new)\nWrite: APPEND\nBronze table: 5 rows (with duplicates of 1, 2)\n</code></pre> <p>Day 3 (2025-01-22):</p> <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-21)\nLoads: Order 2 (updated timestamp)\nWrite: APPEND\nBronze table: 6 rows\n</code></pre> <p>Day 4 (2025-01-23):</p> <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-22)\nLoads: Order 3 (new)\nWrite: APPEND\nBronze table: 7 rows\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#handling-duplicates","title":"Handling Duplicates","text":"<p>Since we append each day, you'll get duplicates in Bronze (which is fine\u2014that's what Raw/Bronze is for):</p> <pre><code>order_id | created_at          | load_date\n1        | 2025-01-20 10:00:00 | 2025-01-20  \u2190 Day 1\n1        | 2025-01-20 10:00:00 | 2025-01-21  \u2190 Day 2 (duplicate)\n2        | 2025-01-21 14:30:00 | 2025-01-21  \u2190 Day 2\n2        | 2025-01-21 14:30:00 | 2025-01-22  \u2190 Day 3 (duplicate)\n3        | 2025-01-23 09:00:00 | 2025-01-23  \u2190 Day 4\n</code></pre> <p>Silver layer (merge/upsert) deduplicates later using the merge transformer.</p>"},{"location":"patterns/legacy_hwm_manual/#sql-date-functions-by-database","title":"SQL Date Functions (by Database)","text":"Database Syntax SQL Server <code>DATEADD(DAY, -1, CAST(GETDATE() AS DATE))</code> PostgreSQL <code>CURRENT_DATE - INTERVAL '1 day'</code> MySQL <code>DATE_SUB(CURDATE(), INTERVAL 1 DAY)</code> Snowflake <code>DATEADD(day, -1, CURRENT_DATE())</code> BigQuery <code>DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)</code>"},{"location":"patterns/legacy_hwm_manual/#common-mistakes","title":"Common Mistakes","text":""},{"location":"patterns/legacy_hwm_manual/#single-query-for-everything","title":"\u274c Single query for everything","text":"<pre><code># Wrong: Won't capture first-run history\nquery: WHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#two-queries","title":"\u2705 Two queries","text":"<pre><code>first_run_query: SELECT * FROM table\nquery: WHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-instead-of","title":"\u274c Using <code>&gt;</code> instead of <code>&gt;=</code>","text":"<pre><code>-- Wrong: filters out today's data\nWHERE created_at &gt; DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-for-inclusive-range","title":"\u2705 Using <code>&gt;=</code> for inclusive range","text":"<pre><code>-- Right: includes today\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#only-using-created_at-misses-updates","title":"\u274c Only using created_at (misses updates)","text":"<pre><code>-- Wrong: updated records not captured\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-coalesceupdated_at-created_at","title":"\u2705 Using COALESCE(updated_at, created_at)","text":"<pre><code>-- Right: captures new AND updated\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#thats-it","title":"That's It","text":"<p>HWM is simple: two queries, Odibi chooses which one runs.</p> <ul> <li>First run: Odibi detects table doesn't exist, runs <code>first_run_query</code></li> <li>Subsequent runs: Odibi detects table exists, runs regular <code>query</code></li> <li>Automatic mode override: First run uses OVERWRITE, subsequent runs use APPEND</li> </ul> <p>Works great for loading from any SQL database into Bronze.</p>"},{"location":"patterns/merge_upsert/","title":"Pattern: Merge/Upsert (Raw \u2192 Silver)","text":"<p>Status: Core Pattern Layer: Silver (refined/cleaned) Engine: Spark (Delta) or Pandas Strategy: Merge (MERGE INTO in Spark) Idempotent: Yes (by key)  </p>"},{"location":"patterns/merge_upsert/#problem","title":"Problem","text":"<p>Raw contains duplicates and historical versions of records. You need: - One current version per key (deduplication) - Audit columns to track when each record was created/updated - Idempotency (rerunning doesn't create duplicates or double-count)</p> <p>How do you efficiently merge new/changed raw data into Silver while maintaining a clean current state?</p>"},{"location":"patterns/merge_upsert/#solution","title":"Solution","text":"<p>Use Delta Lake's MERGE operation to upsert records by key. Odibi provides the Merge Transformer to make this configuration-driven.</p>"},{"location":"patterns/merge_upsert/#how-it-works","title":"How It Works","text":"<p>MERGE Logic: 1. Read a batch of Raw data (new/changed rows) 2. Join with Silver by key columns 3. If matched: Update the Silver row with the new data 4. If not matched: Insert the new row 5. Auto-inject audit columns (created_at, updated_at)</p>"},{"location":"patterns/merge_upsert/#step-by-step-example","title":"Step-by-Step Example","text":""},{"location":"patterns/merge_upsert/#scenario-orders-table","title":"Scenario: Orders Table","text":"<p>Raw (after 2 runs; has duplicates):</p> <pre><code>order_id | product      | qty | created_at          | updated_at\n---------|--------------|-----|---------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL              \u2190 Duplicate\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 \u2190 Duplicate\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL\n</code></pre> <p>Silver (before merge):</p> <pre><code>order_id | product      | qty | created_at          | updated_at          | _created_ts        | _updated_ts\n---------|--------------|-----|---------------------|---------------------|--------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL                | 2025-11-01 10:30   | 2025-11-01 10:30\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL                | 2025-11-01 11:30   | 2025-11-01 11:30\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 | 2025-11-01 12:30   | 2025-11-01 13:30\n</code></pre>"},{"location":"patterns/merge_upsert/#merge-operation","title":"Merge Operation","text":"<p>New micro-batch from Raw (after dedup by timestamp):</p> <pre><code>order_id | product      | qty | created_at          | updated_at\n---------|--------------|-----|---------------------|-------------------\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL              \u2190 Duplicate, older timestamp\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL              \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL              \u2190 NEW\n</code></pre> <p>Merge By Key (<code>order_id</code>):</p> <pre><code>Row (order_id=2):\n  - Matches Silver row (order_id=2)\n  - Source timestamp: 2025-11-01 11:00\n  - Silver timestamp: 2025-11-01 13:30\n  - Source is older, skip? OR update anyway?\n  \u2192 MERGE strategy: UPDATE (keep latest by source created_at/updated_at)\n  \u2192 Actually: Insert latest version FIRST, then merge handles it\n  \u2192 Result: Silver row 2 unchanged (already has latest)\n\nRow (order_id=4):\n  - No match in Silver\n  - NEW row\n  \u2192 INSERT into Silver\n  \u2192 Set _created_ts = now(), _updated_ts = now()\n\nRow (order_id=5):\n  - No match in Silver\n  - NEW row\n  \u2192 INSERT into Silver\n  \u2192 Set _created_ts = now(), _updated_ts = now()\n</code></pre> <p>Silver (after merge):</p> <pre><code>order_id | product      | qty | created_at          | updated_at          | _created_ts        | _updated_ts\n---------|--------------|-----|---------------------|---------------------|--------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL                | 2025-11-01 10:30   | 2025-11-01 10:30\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL                | 2025-11-01 11:30   | 2025-11-01 11:30  (unchanged)\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 | 2025-11-01 12:30   | 2025-11-01 13:30  (unchanged)\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL                | 2025-11-02 09:30   | 2025-11-02 09:30  \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL                | 2025-11-02 10:30   | 2025-11-02 10:30  \u2190 NEW\n</code></pre> <p>Result: - Silver has exactly 5 unique orders (1 per key) - Duplicates from Raw are deduplicated - Audit columns track when Odibi processed each row - Idempotent: rerunning the same batch produces the same result</p>"},{"location":"patterns/merge_upsert/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/merge_upsert/#minimal-config","title":"Minimal Config","text":"<pre><code>- id: merge_orders_silver\n  name: \"Merge Orders to Silver\"\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.orders\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    strategy: upsert\n</code></pre>"},{"location":"patterns/merge_upsert/#full-config-with-audit-columns","title":"Full Config (with Audit Columns)","text":"<pre><code>- id: merge_orders_silver\n  name: \"Merge Orders: Raw \u2192 Silver\"\n  description: \"Deduplicate and upsert orders from raw layer\"\n  depends_on: [load_orders_raw]\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.orders\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    strategy: upsert\n    audit_cols:\n      created_col: _created_at\n      updated_col: _updated_at\n  validation:\n    not_empty: true\n    schema:\n      order_id:\n        type: integer\n        nullable: false\n      product:\n        type: string\n        nullable: false\n      qty:\n        type: integer\n        nullable: false\n</code></pre>"},{"location":"patterns/merge_upsert/#multi-key-example-composite-key","title":"Multi-Key Example (Composite Key)","text":"<pre><code>- id: merge_inventory_silver\n  name: \"Merge Inventory to Silver\"\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.inventory\n  transformer: merge\n  params:\n    target: silver.inventory\n    keys: [plant_id, material_id]  \u2190 Composite key\n    strategy: upsert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre>"},{"location":"patterns/merge_upsert/#merge-transformer-behavior","title":"Merge Transformer Behavior","text":""},{"location":"patterns/merge_upsert/#spark-delta","title":"Spark (Delta)","text":"<p>Uses native <code>DeltaTable.merge()</code>:</p> <pre><code># Pseudo-code\ndelta_table = DeltaTable.forName(\"silver.orders\")\ndelta_table.merge(\n    source_df,\n    condition=\"target.order_id = source.order_id\"\n) \\\n.whenMatchedUpdateAll() \\\n.whenNotMatchedInsertAll() \\\n.execute()\n\n# Auto-inject audit columns:\n# If insert: created_at = now(), updated_at = now()\n# If update: updated_at = now(), created_at unchanged\n</code></pre>"},{"location":"patterns/merge_upsert/#pandas","title":"Pandas","text":"<p>Loads, merges, overwrites:</p> <pre><code># Pseudo-code\ntarget = pd.read_parquet(\"silver/orders\")\nsource = df  # Input DataFrame\n\n# Merge indicator\nmerged = target.merge(source, on=['order_id'], how='outer', indicator=True)\n\n# Apply logic:\n# - Rows in target only: keep\n# - Rows in source only: insert\n# - Rows in both: update source values\n\n# Overwrite\nmerged.to_parquet(\"silver/orders\", mode=\"overwrite\")\n</code></pre>"},{"location":"patterns/merge_upsert/#strategy-options","title":"Strategy Options","text":"Strategy Behavior Best For <code>upsert</code> Insert new, update existing Standard use case (Raw \u2192 Silver) <code>append_only</code> Insert new, ignore duplicates Append-only tables (no updates) <code>delete_match</code> Delete matching rows Tombstones, soft deletes"},{"location":"patterns/merge_upsert/#audit-columns","title":"Audit Columns","text":""},{"location":"patterns/merge_upsert/#auto-injected-columns","title":"Auto-Injected Columns","text":"<p>When <code>audit_cols</code> is specified, Odibi adds two columns:</p> <pre><code># On INSERT\n_created_at = CURRENT_TIMESTAMP()\n_updated_at = CURRENT_TIMESTAMP()\n\n# On UPDATE\n_created_at = [unchanged]\n_updated_at = CURRENT_TIMESTAMP()\n</code></pre> <p>This lets you track when Odibi processed each record, separate from the source's created/updated columns.</p>"},{"location":"patterns/merge_upsert/#example","title":"Example","text":"<pre><code>audit_cols:\n  created_col: _sys_created_ts\n  updated_col: _sys_updated_ts\n</code></pre> <p>Result:</p> <pre><code>order_id | product | _sys_created_ts        | _sys_updated_ts\n---------|---------|------------------------|-------------------\n1        | Widget  | 2025-11-01 10:30:00    | 2025-11-01 10:30:00\n2        | Gadget  | 2025-11-01 11:30:00    | 2025-11-02 14:45:00  \u2190 Updated\n</code></pre>"},{"location":"patterns/merge_upsert/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/merge_upsert/#advantages","title":"Advantages","text":"<p>\u2713 Deduplicates Raw data automatically \u2713 Idempotent (safe to rerun) \u2713 Tracks data lineage (audit columns) \u2713 Handles both new and changed rows efficiently \u2713 Spark merge is fast (native Delta operation)  </p>"},{"location":"patterns/merge_upsert/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Requires primary key (what makes each row unique?) \u2717 Overwrites previous values (no history of all versions) \u2717 Pandas merge is slower than Spark (pandas mode not recommended for large tables)  </p>"},{"location":"patterns/merge_upsert/#common-patterns","title":"Common Patterns","text":""},{"location":"patterns/merge_upsert/#pattern-scd-type-1-current-state-only","title":"Pattern: SCD Type 1 (Current State Only)","text":"<p>Keep only the latest version of each record. This is the default merge pattern.</p> <pre><code>transformer: merge\nparams:\n  target: silver.customers\n  keys: [customer_id]\n  strategy: upsert\n</code></pre>"},{"location":"patterns/merge_upsert/#pattern-scd-type-2-full-history","title":"Pattern: SCD Type 2 (Full History)","text":"<p>Keep all historical versions with effective dates. NOT supported by standard merge. Use a separate <code>dim_customers</code> table with: - <code>customer_id</code> - <code>effective_from</code>, <code>effective_to</code> - <code>is_current</code> flag</p> <p>Then maintain it with a separate pipeline.</p>"},{"location":"patterns/merge_upsert/#pattern-append-only-no-duplicates","title":"Pattern: Append-Only (No Duplicates)","text":"<p>If your table should never have duplicates and you want to avoid updates:</p> <pre><code>transformer: merge\nparams:\n  target: silver.events\n  keys: [event_id]\n  strategy: append_only\n</code></pre> <p>This inserts new rows but ignores duplicates instead of updating.</p>"},{"location":"patterns/merge_upsert/#debugging","title":"Debugging","text":""},{"location":"patterns/merge_upsert/#check-for-duplicates-in-silver","title":"Check for Duplicates in Silver","text":"<pre><code>SELECT order_id, COUNT(*) as count\nFROM silver.orders\nGROUP BY order_id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If you see duplicates, your merge key is wrong.</p>"},{"location":"patterns/merge_upsert/#check-merge-history","title":"Check Merge History","text":"<pre><code>DESCRIBE HISTORY silver.orders\n</code></pre> <p>Shows every merge operation, versions, and row counts.</p>"},{"location":"patterns/merge_upsert/#when-to-use","title":"When to Use","text":"<ul> <li>Always for Raw \u2192 Silver refinement</li> <li>Multiple sources merging into same table</li> <li>Need to track data lineage (audit columns)</li> <li>Want idempotent transformations</li> </ul>"},{"location":"patterns/merge_upsert/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Audit tables (keep appending)</li> <li>SCD Type 2 (need version history)</li> <li>Data that should be immutable (use append instead)</li> </ul>"},{"location":"patterns/merge_upsert/#related-patterns","title":"Related Patterns","text":"<ul> <li>Append-Only Raw \u2192 Source layer (unmerged, duplicates OK)</li> <li>High Water Mark \u2192 How to efficiently feed Raw with new data</li> </ul>"},{"location":"patterns/merge_upsert/#references","title":"References","text":"<ul> <li>Odibi Merge Transformer Spec</li> <li>Databricks: Delta Lake MERGE</li> <li>Fundamentals of Data Engineering: Chapter on SCD</li> </ul>"},{"location":"patterns/scd2/","title":"SCD Type 2 (Slowly Changing Dimensions)","text":"<p>The SCD Type 2 pattern allows you to track the full history of changes for a record over time. Unlike a simple update (which overwrites the old value), SCD2 keeps the old version and adds a new version, managing effective dates for you.</p>"},{"location":"patterns/scd2/#the-time-machine-concept","title":"The \"Time Machine\" Concept","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: Each record has an \"effective window\" (<code>effective_time</code> to <code>end_time</code>) and a flag (<code>is_current</code>) indicating if it is the latest version.</p>"},{"location":"patterns/scd2/#visual-example","title":"Visual Example","text":"<p>Input (Source Update): Customer 101 moved to NY on Feb 1st.</p> customer_id address tier txn_date 101 NY Gold 2024-02-01 <p>Target Table (Before): Customer 101 lived in CA since Jan 1st.</p> customer_id address tier txn_date valid_to is_active 101 CA Gold 2024-01-01 NULL true <p>Target Table (After SCD2): Old record CLOSED (valid_to set). New record OPEN (is_active=true).</p> customer_id address tier txn_date valid_to is_active 101 CA Gold 2024-01-01 2024-02-01 false 101 NY Gold 2024-02-01 NULL true"},{"location":"patterns/scd2/#configuration","title":"Configuration","text":"<p>Use the <code>scd2</code> transformer in your pipeline node.</p>"},{"location":"patterns/scd2/#minimal-example","title":"Minimal Example","text":"<pre><code>nodes:\n  - name: \"dim_customers\"\n    # ... (read from source) ...\n\n    transformer: \"scd2\"\n    params:\n      target: \"gold/dim_customers\"     # Path to existing history\n      keys: [\"customer_id\"]            # Unique ID\n      track_cols: [\"address\", \"tier\"]  # Changes here trigger a new version\n      effective_time_col: \"txn_date\"   # When the change happened\n\n    write:\n      connection: \"gold_lake\"\n      format: \"delta\"\n      path: \"gold/dim_customers\"\n      mode: \"overwrite\"                # Important: SCD2 returns FULL history, so we overwrite\n</code></pre>"},{"location":"patterns/scd2/#full-configuration","title":"Full Configuration","text":"<pre><code>transformer: \"scd2\"\nparams:\n  target: \"gold/dim_customers\"\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\", \"email\"]\n\n  # Source column for start date\n  effective_time_col: \"updated_at\"\n\n  # Target columns to manage (optional defaults shown)\n  end_time_col: \"valid_to\"\n  current_flag_col: \"is_current\"\n</code></pre>"},{"location":"patterns/scd2/#how-it-works","title":"How It Works","text":"<p>The <code>scd2</code> transformer performs a complex set of operations automatically:</p> <ol> <li>Match: Finds existing records in the <code>target</code> table using <code>keys</code>.</li> <li>Compare: Checks <code>track_cols</code> to see if any data has changed.</li> <li>Close: If a record changed, it updates the old record's <code>end_time_col</code> to equal the new record's <code>effective_time_col</code>, and sets <code>is_current = false</code>.</li> <li>Insert: It adds the new record with <code>effective_time_col</code> as the start date, <code>NULL</code> as the end date, and <code>is_current = true</code>.</li> <li>Preserve: It keeps all unchanged history records as they are.</li> </ol>"},{"location":"patterns/scd2/#important-notes","title":"Important Notes","text":"<ul> <li>Write Mode: You must use <code>mode: overwrite</code> for the write operation following this transformer. The transformer constructs the complete new state of the history table (including old closed records and new open records).</li> <li>Target Existence: If the target table doesn't exist (first run), the transformer simply prepares the source data (adds valid_to/is_current columns) and returns it.</li> <li>Engine Support: Works on both Spark (Delta Lake) and Pandas (Parquet/CSV).</li> </ul>"},{"location":"patterns/scd2/#when-to-use","title":"When to Use","text":"<ul> <li>Dimension Tables: Customer dimensions, Product dimensions where attributes change slowly over time.</li> <li>Audit Trails: When you need exact historical state reconstruction.</li> </ul>"},{"location":"patterns/scd2/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Fact Tables: Events (Transactions, Logs) are immutable; they don't change state, they just occur. Use <code>append</code> instead.</li> <li>Rapidly Changing Data: If a record changes 100 times a day, SCD2 will explode your storage size. Use a snapshot or aggregate approach instead.</li> </ul>"},{"location":"patterns/skip_if_unchanged/","title":"Skip If Unchanged Pattern","text":"<p>Use Case: Avoid redundant writes for snapshot tables that may not change between pipeline runs.</p>"},{"location":"patterns/skip_if_unchanged/#the-problem","title":"The Problem","text":"<p>When ingesting snapshot data (full table extracts without timestamps), an hourly pipeline will append the same 192k rows 24 times per day if the source data doesn't change. This wastes:</p> <ul> <li>Storage: 24\u00d7 the necessary data</li> <li>Compute: Unnecessary write operations</li> <li>Query performance: More files to scan</li> </ul>"},{"location":"patterns/skip_if_unchanged/#the-solution","title":"The Solution","text":"<p>The <code>skip_if_unchanged</code> feature computes a hash of the DataFrame content before writing. If the hash matches the previous write, the write is skipped entirely.</p> <pre><code>nodes:\n  - name: bronze_data\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.MySnapshotTable\n    write:\n      connection: bronze\n      format: delta\n      table: my_snapshot_table\n      mode: append\n      skip_if_unchanged: true\n      skip_hash_sort_columns: [P_ID, DateId]  # For deterministic ordering\n</code></pre>"},{"location":"patterns/skip_if_unchanged/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    A[Read source data] --&gt; B[Compute SHA256 hash of DataFrame]\n    B --&gt; C{Previous hash exists?}\n    C --&gt;|No| D[Write data]\n    D --&gt; E[Store hash in Delta metadata]\n    C --&gt;|Yes| F{Hashes match?}\n    F --&gt;|Yes| G[Skip write, log 'unchanged']\n    F --&gt;|No| D\n</code></pre> <ol> <li>Hash Computation: Before writing, the entire DataFrame is converted to CSV bytes and hashed with SHA256</li> <li>Hash Storage: The hash is stored in Delta table properties (<code>odibi.content_hash</code>)</li> <li>Comparison: On subsequent runs, the new hash is compared to the stored hash</li> <li>Skip or Write: If hashes match, the write is skipped; otherwise, data is written and hash updated</li> </ol>"},{"location":"patterns/skip_if_unchanged/#configuration-options","title":"Configuration Options","text":"Option Type Description <code>skip_if_unchanged</code> bool Enable hash-based skip detection <code>skip_hash_columns</code> list Subset of columns to hash (default: all) <code>skip_hash_sort_columns</code> list Columns to sort by before hashing (for determinism)"},{"location":"patterns/skip_if_unchanged/#when-to-use","title":"When to Use","text":"<p>\u2705 Good fit: - Snapshot tables without <code>updated_at</code> timestamps - Reference/dimension data that changes infrequently - Tables where you don't know the change frequency</p> <p>\u274c Not recommended: - Tables with reliable <code>updated_at</code> (use HWM instead) - Append-only fact tables (new data every run) - Very large tables (hash computation is expensive)</p>"},{"location":"patterns/skip_if_unchanged/#example-global-manufacturing-data","title":"Example: Global Manufacturing Data","text":"<pre><code># Pipeline runs hourly for global coverage\n# But this table only changes 1-2 times per day\n\nnodes:\n  - name: bronze_osmdssds_detail\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.vw_OSMDSSDSEDetail\n    write:\n      connection: bronze\n      format: delta\n      table: osmdssds_detail\n      mode: append\n      add_metadata: true\n      skip_if_unchanged: true\n      skip_hash_sort_columns: [P_ID, DateId]\n</code></pre> <p>Result: - Pipeline checks every hour (data is fresh when needed) - Only writes when data actually changes (storage efficient) - Logs show \"Skipping write - content unchanged\" for skipped runs</p>"},{"location":"patterns/skip_if_unchanged/#storage-impact","title":"Storage Impact","text":"Scenario Daily Writes Annual Rows (192k/snapshot) No skip (hourly) 24 1.7 billion With skip (2 changes/day) 2 140 million Savings 92% 92%"},{"location":"patterns/skip_if_unchanged/#limitations","title":"Limitations","text":"<ol> <li>Delta only: Currently only supported for Delta format</li> <li>Full DataFrame hash: Computes hash of entire DataFrame (not row-by-row)</li> <li>Memory: DataFrame must fit in driver memory for hashing</li> <li>First run: Always writes on first run (no previous hash to compare)</li> </ol>"},{"location":"patterns/skip_if_unchanged/#related-patterns","title":"Related Patterns","text":"<ul> <li>Append-Only Raw Layer - Bronze layer best practices</li> <li>Incremental Stateful - For tables with timestamps</li> <li>Smart Read - Full load pattern</li> </ul>"},{"location":"patterns/smart_read/","title":"Smart Read (Rolling Window)","text":"<p>The \"Smart Read\" feature simplifies incremental data loading by automatically generating the correct SQL query based on time windows.</p> <p>Note: This page describes the Rolling Window mode (Stateless). For exact state tracking (HWM), see Stateful Incremental Loading.</p> <p>It eliminates the need to write complex SQL with <code>first_run_query</code> and dialect-specific date math.</p> <p>!!! warning \"Requirement: Write Configuration\"     Smart Read requires a <code>write</code> block in the same node.</p> <pre><code>It determines whether to run a **Full Load** or **Incremental Load** by checking if the destination defined in `write` already exists.\n\n*   If you only want to read data (without writing), use the standard `query` option with explicit date filters instead.\n*   Ensure your `write` mode is set correctly (usually `append`) to preserve history.\n</code></pre>"},{"location":"patterns/smart_read/#write-modes-for-incremental","title":"Write Modes for Incremental","text":"Mode Suitability Why? <code>append</code> \u2705 Recommended Safely adds new records to the lake. Preserves history. <code>upsert</code> \u26a0\ufe0f Advanced Use only if you are merging directly into a Silver layer table and have defined keys. <code>overwrite</code> \u274c Dangerous Do NOT use. This would replace your entire historical dataset with just the latest batch (e.g., the last 3 days)."},{"location":"patterns/smart_read/#how-it-works","title":"How It Works","text":"<p>Odibi checks if your Write target exists:</p> <ol> <li> <p>Target Missing (First Run):</p> <ul> <li>It assumes this is a historical load.</li> <li>Generates: <code>SELECT * FROM source_table</code></li> <li>Result: Loads all history.</li> </ul> </li> <li> <p>Target Exists (Subsequent Runs):</p> <ul> <li>It assumes this is an incremental load.</li> <li>Generates: <code>SELECT * FROM source_table WHERE column &gt;= [Calculated Date]</code></li> <li>Result: Loads only new/changed data.</li> </ul> </li> </ol>"},{"location":"patterns/smart_read/#the-standard-pattern-ingest-to-bronze","title":"The Standard Pattern: \"Ingest to Bronze\"","text":"<p>The most common use case for Smart Read is the Ingestion Node. This node acts as a bridge between your external source (SQL, API) and your Data Lake (Bronze Layer).</p>"},{"location":"patterns/smart_read/#why-use-this-pattern","title":"Why use this pattern?","text":"<ol> <li>State Management: The node uses the Write Target (e.g., <code>bronze_orders</code>) as its state.<ul> <li>Target Empty? $\\rightarrow$ Run <code>SELECT *</code> (Full History)</li> <li>Target Exists? $\\rightarrow$ Run <code>SELECT * ... WHERE date &gt; X</code> (Incremental)</li> </ul> </li> <li>Efficiency: Downstream nodes (e.g., \"clean_orders\") can simply depend on this node. They will receive the dataframe containing only the data that was just ingested (the incremental batch), allowing your entire pipeline to process only new data efficiently.</li> </ol>"},{"location":"patterns/smart_read/#example-node","title":"Example Node","text":"<pre><code>- name: \"ingest_orders\"\n  description: \"Incrementally load orders from SQL to Delta\"\n\n  # 1. READ (Source)\n  read:\n    connection: \"sql_db\"\n    format: \"sql\"\n    table: \"orders\"\n    incremental:\n      column: \"updated_at\"\n      lookback: 3\n      unit: \"day\"\n\n  # 2. WRITE (Target - Required for state tracking)\n  write:\n    connection: \"data_lake\"\n    format: \"delta\"\n    table: \"bronze_orders\"\n    mode: \"append\"  # Append new rows from the incremental batch\n</code></pre>"},{"location":"patterns/smart_read/#configuration","title":"Configuration","text":"<p>Use the <code>incremental</code> block in your <code>read</code> configuration.</p>"},{"location":"patterns/smart_read/#example-handling-updates-inserts","title":"Example: Handling Updates &amp; Inserts","text":"<p>This pattern handles both new records (<code>created_at</code>) and updates (<code>updated_at</code>).</p> <pre><code>nodes:\n  - name: \"load_orders\"\n    read:\n      connection: \"sql_server_prod\"\n      format: \"sql\"\n      table: \"dbo.orders\"\n\n      incremental:\n        column: \"updated_at\"         # Primary check\n        fallback_column: \"created_at\" # If updated_at is NULL\n        lookback: 1\n        unit: \"day\"\n\n    write:\n      connection: \"bronze\"\n      format: \"delta\"\n      table: \"orders_raw\"\n      mode: \"append\"\n</code></pre> <p>This generates:</p> <pre><code>SELECT * FROM dbo.orders\nWHERE COALESCE(updated_at, created_at) &gt;= '2023-10-25 10:00:00'\n</code></pre>"},{"location":"patterns/smart_read/#example-simple-append-only","title":"Example: Simple Append-Only","text":"<p>Perfect for pipelines that run every hour but want a 4-hour safety window for late-arriving data.</p> <pre><code>    read:\n      connection: \"postgres_db\"\n      format: \"sql\"\n      table: \"public.events\"\n      incremental:\n        column: \"event_time\"\n        lookback: 4\n        unit: \"hour\"\n\n    write:\n      connection: \"bronze\"\n      format: \"delta\"\n      table: \"events_raw\"\n      mode: \"append\"\n</code></pre>"},{"location":"patterns/smart_read/#advanced-merging-directly-to-silver-upsert","title":"Advanced: Merging directly to Silver (Upsert)","text":"<p>If you are bypassing Bronze and merging directly into a Silver table, you can use <code>upsert</code>. Note: This requires defining the primary <code>keys</code> to match on.</p> <pre><code>    read:\n      connection: \"crm_db\"\n      format: \"sql\"\n      table: \"customers\"\n      incremental:\n        column: \"last_modified\"\n        lookback: 1\n        unit: \"day\"\n\n    write:\n      connection: \"silver\"\n      format: \"delta\"\n      table: \"dim_customers\"\n      mode: \"upsert\"\n      options:\n        keys: [\"customer_id\"]\n</code></pre>"},{"location":"patterns/smart_read/#supported-units","title":"Supported Units","text":"Unit Description <code>hour</code> Looks back N hours from <code>now()</code> <code>day</code> Looks back N days from <code>now()</code> <code>month</code> Looks back N * 30 days (approx) <code>year</code> Looks back N * 365 days (approx)"},{"location":"patterns/smart_read/#comparison-with-legacy-pattern","title":"Comparison with Legacy Pattern","text":""},{"location":"patterns/smart_read/#old-way-manual","title":"\u274c Old Way (Manual)","text":"<p>You had to write two queries and know the SQL dialect.</p> <pre><code>read:\n  query: \"SELECT * FROM orders WHERE updated_at &gt;= DATEADD(DAY, -1, GETDATE())\"\nwrite:\n  first_run_query: \"SELECT * FROM orders\"\n</code></pre>"},{"location":"patterns/smart_read/#new-way-smart-read","title":"\u2705 New Way (Smart Read)","text":"<p>Configuration is declarative and dialect-agnostic.</p> <pre><code>read:\n  table: \"orders\"\n  incremental:\n    column: \"updated_at\"\n    lookback: 1\n    unit: \"day\"\n</code></pre>"},{"location":"patterns/smart_read/#faq","title":"FAQ","text":"<p>Q: What if I want to reload all history manually? A: You can simply delete the target table (or folder) in your data lake. The next run will detect it's missing and trigger the full historical load.</p> <p>Q: Does this work with <code>depends_on</code>? A: This feature is for Ingestion Nodes (Node 1) that read from external systems. Downstream nodes automatically benefit because they receive the data frame produced by Node 1.</p> <p>Q: Can I mix this with custom SQL? A: No. If you provide a <code>query</code> in the <code>read</code> section, Odibi respects your manual query and ignores the <code>incremental</code> block.</p>"},{"location":"patterns/windowed_reprocess/","title":"Pattern: Windowed Reprocess (Silver \u2192 Gold Aggregates)","text":"<p>Status: Core Pattern Layer: Gold (aggregated/BI-ready) Engine: Spark Batch Write Mode: <code>overwrite</code> (partition-specific) Idempotent: Yes (recalculated)  </p>"},{"location":"patterns/windowed_reprocess/#problem","title":"Problem","text":"<p>You have a Gold aggregate table (e.g., daily sales summary). Late-arriving data in Silver invalidates yesterday's numbers. You need to: - Fix aggregates when new data arrives - Avoid double-counting (can't just add new rows) - Keep calculations simple (always correct, never patched)</p> <p>How do you maintain accurate aggregates without complex update logic?</p>"},{"location":"patterns/windowed_reprocess/#solution","title":"Solution","text":"<p>Instead of patching aggregates with updates (error-prone), recalculate the entire time window and replace it.</p> <p>Principle: \"Rebuild the Bucket, Don't Patch the Hole\"</p>"},{"location":"patterns/windowed_reprocess/#how-it-works","title":"How It Works","text":""},{"location":"patterns/windowed_reprocess/#the-reprocess-pattern","title":"The Reprocess Pattern","text":"<ol> <li>Identify the window (e.g., \"Last 7 days\", \"This month\")</li> <li>Read Silver filtered to that window</li> <li>Recalculate aggregate (SUM, COUNT, AVG, etc.)</li> <li>Write to Gold with Dynamic Partition Overwrite</li> </ol> <p>If late data arrives in the last 7 days, next run recalculates those days\u2014automatically fixing aggregates.</p>"},{"location":"patterns/windowed_reprocess/#step-by-step-example","title":"Step-by-Step Example","text":""},{"location":"patterns/windowed_reprocess/#scenario-daily-sales-summary","title":"Scenario: Daily Sales Summary","text":"<p>Silver Table (Orders, with timestamps):</p> <p>Day 1 (Initial load on 2025-11-01 at 10:00):</p> <pre><code>order_id | order_date | amount | created_at\n---------|------------|--------|-------------------\n1        | 2025-11-01 | 100    | 2025-11-01 10:00\n2        | 2025-11-01 | 50     | 2025-11-01 10:30\n3        | 2025-10-31 | 200    | 2025-11-01 10:45\n</code></pre> <p>Run 1 (Calculate last 7 days: 2025-10-25 to 2025-11-01):</p> <pre><code>SELECT\n  DATE(order_date) as order_date,\n  SUM(amount) as total_sales,\n  COUNT(*) as order_count\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n</code></pre> <p>Gold Result (Days 25-Oct to 1-Nov):</p> <pre><code>order_date | total_sales | order_count\n-----------|-------------|-------------\n2025-10-31 | 200         | 1\n2025-11-01 | 150         | 2\n</code></pre> <p>Partition written: <code>order_date=2025-10-31</code>, <code>order_date=2025-11-01</code></p>"},{"location":"patterns/windowed_reprocess/#day-2-late-data-arrives","title":"Day 2: Late Data Arrives","text":"<p>Silver Table (New data arrived at 14:00):</p> <pre><code>order_id | order_date | amount | created_at\n---------|------------|--------|-------------------\n1        | 2025-11-01 | 100    | 2025-11-01 10:00\n2        | 2025-11-01 | 50     | 2025-11-01 10:30\n3        | 2025-10-31 | 200    | 2025-11-01 10:45\n4        | 2025-11-01 | 75     | 2025-11-02 14:00  \u2190 LATE DATA (same day, arrived late)\n</code></pre> <p>Run 2 (Recalculate last 7 days: 2025-10-25 to 2025-11-02):</p> <pre><code>SELECT\n  DATE(order_date) as order_date,\n  SUM(amount) as total_sales,\n  COUNT(*) as order_count\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n</code></pre> <p>Gold Result (Days 25-Oct to 2-Nov):</p> <pre><code>order_date | total_sales | order_count\n-----------|-------------|-------------\n2025-10-31 | 200         | 1\n2025-11-01 | 225         | 3              \u2190 UPDATED (was 150, now 225)\n</code></pre> <p>Write Mode: Dynamic Partition Overwrite - Existing partition <code>order_date=2025-10-31</code> is untouched - Partition <code>order_date=2025-11-01</code> is replaced entirely (was 2 rows, now 3 rows)</p> <p>No double-counting: The aggregate is recalculated from scratch, not patched.</p>"},{"location":"patterns/windowed_reprocess/#why-this-works","title":"Why This Works","text":""},{"location":"patterns/windowed_reprocess/#without-windowed-reprocess-wrong","title":"Without Windowed Reprocess (WRONG)","text":"<pre><code>-- Don't do this\nUPDATE gold.sales\nSET total_sales = total_sales + 75,\n    order_count = order_count + 1\nWHERE order_date = '2025-11-01'\n</code></pre> <p>Problems: - If this query runs twice, you add 75 twice (double-counting) - If you run it out-of-order, you corrupt data - Requires tracking \"what did I update?\"</p>"},{"location":"patterns/windowed_reprocess/#with-windowed-reprocess-right","title":"With Windowed Reprocess (RIGHT)","text":"<pre><code>-- Recalculate the entire 7-day window\nSELECT\n  DATE(order_date),\n  SUM(amount),\n  COUNT(*)\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n\n-- Write with Dynamic Partition Overwrite\n-- Entire partition is replaced\n</code></pre> <p>Advantages: - Idempotent (run 10 times = same result) - No double-counting (always fresh calculation) - Simple logic (standard SQL aggregate)</p>"},{"location":"patterns/windowed_reprocess/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/windowed_reprocess/#simple-daily-aggregate","title":"Simple Daily Aggregate","text":"<pre><code>- id: gold_daily_sales\n  name: \"Daily Sales Summary (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            DATE(order_date) as order_date,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            MIN(order_date) as first_order_ts,\n            MAX(order_date) as last_order_ts\n          FROM silver.orders\n          WHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\n          GROUP BY DATE(order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.daily_sales\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#monthly-aggregate-wider-window","title":"Monthly Aggregate (Wider Window)","text":"<pre><code>- id: gold_monthly_sales\n  name: \"Monthly Sales Summary (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            DATE_TRUNC('month', order_date) as month,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            COUNT(DISTINCT customer_id) as unique_customers\n          FROM silver.orders\n          WHERE DATE_TRUNC('month', order_date) &gt;= DATE_TRUNC('month', DATE_SUB(CURRENT_DATE(), 90))\n          GROUP BY DATE_TRUNC('month', order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.monthly_sales\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#multi-grain-aggregates","title":"Multi-Grain Aggregates","text":"<pre><code>- id: gold_sales_by_region_day\n  name: \"Sales by Region &amp; Day (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            region,\n            DATE(order_date) as order_date,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            AVG(amount) as avg_order_value\n          FROM silver.orders\n          WHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 30)\n          GROUP BY region, DATE(order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.sales_by_region_day\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#window-size-strategy","title":"Window Size Strategy","text":""},{"location":"patterns/windowed_reprocess/#how-far-back-should-the-window-be","title":"How Far Back Should the Window Be?","text":"<p>Rule of Thumb: 2-3x your SLA for late data.</p> SLA Window Example Same-day delivery (next day processed) 3-7 days Daily aggregate 1-week SLA 14-21 days Weekly aggregate End-of-month close (3-5 days) 30-45 days Monthly aggregate <p>Conservative approach: Recalculate 30 days back, even if only aggregating daily. It costs minimal compute.</p>"},{"location":"patterns/windowed_reprocess/#dynamic-partition-overwrite","title":"Dynamic Partition Overwrite","text":""},{"location":"patterns/windowed_reprocess/#why-it-matters","title":"Why It Matters","text":"<p>Scenario: Your table is partitioned by <code>order_date</code>:</p> <pre><code>gold/sales/\n\u251c\u2500\u2500 order_date=2025-11-01/\n\u251c\u2500\u2500 order_date=2025-10-31/\n\u251c\u2500\u2500 order_date=2025-10-30/\n\u2514\u2500\u2500 ... (30 days of data)\n</code></pre> <p>If you use full overwrite (default): - Entire table is replaced - All 30 days are rewritten (slow) - Other columns lose their data</p> <p>If you use dynamic partition overwrite: - Only <code>order_date=2025-11-01</code> (and other affected dates) are replaced - Unaffected dates remain untouched - Much faster</p>"},{"location":"patterns/windowed_reprocess/#enabling-in-odibi","title":"Enabling in Odibi","text":"<pre><code>write:\n  connection: adls_prod\n  format: delta\n  table: gold.daily_sales\n  mode: overwrite\n  options:\n    partitionOverwriteMode: dynamic\n</code></pre> <p>This is automatically enabled by Odibi's safe defaults (per Architecture Manifesto).</p>"},{"location":"patterns/windowed_reprocess/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/windowed_reprocess/#problem-aggregate-is-still-wrong","title":"Problem: Aggregate is Still Wrong","text":"<p>Causes: 1. Window is too short \u2192 Late data arriving outside window. Increase window size. 2. Wrong grouping \u2192 Missing a dimension (e.g., region). Check Silver data. 3. Stale Silver data \u2192 No new orders merged in. Check merge pipeline.</p> <p>Debug:</p> <pre><code>-- Check what's in Silver for the window\nSELECT DATE(order_date), COUNT(*)\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\nORDER BY order_date DESC;\n\n-- Compare to Gold\nSELECT order_date, COUNT(*) as count\nFROM gold.daily_sales\nWHERE order_date &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY order_date\nORDER BY order_date DESC;\n</code></pre>"},{"location":"patterns/windowed_reprocess/#problem-slow-rewrites","title":"Problem: Slow Rewrites","text":"<p>Causes: 1. Window too large \u2192 Recalculating 365 days every run. Reduce window or run less frequently. 2. No partitioning \u2192 Entire table is scanned. Add <code>partition by order_date</code> to Silver.</p> <p>Solution:</p> <pre><code># Smaller window for frequent runs\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 3)\n\n# Larger window for nightly runs\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 30)\n</code></pre>"},{"location":"patterns/windowed_reprocess/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/windowed_reprocess/#advantages","title":"Advantages","text":"<p>\u2713 Always correct (fresh calculation, not patched) \u2713 Idempotent (run multiple times = same result) \u2713 Self-healing (late data automatically fixes aggregates) \u2713 Simple logic (standard SQL, no complex update logic) \u2713 Fast (recalculate 7 days vs. maintain entire history)  </p>"},{"location":"patterns/windowed_reprocess/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Requires recomputation (slower than patches, but worth it) \u2717 Assumes partitioning (without partitioning, rewrites entire table) \u2717 Assumes stateless logic (can't use row-level updates)  </p>"},{"location":"patterns/windowed_reprocess/#when-to-use","title":"When to Use","text":"<ul> <li>Always for Gold aggregates (KPIs, fact tables, summaries)</li> <li>Late-arriving data possible</li> <li>Queries can be re-executed without side effects</li> <li>Need guaranteed correctness over minimal compute</li> </ul>"},{"location":"patterns/windowed_reprocess/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Audit tables (use append)</li> <li>Streaming aggregates with sub-second latency (use Structured Streaming)</li> <li>Data with complex, stateful dependencies</li> </ul>"},{"location":"patterns/windowed_reprocess/#related-patterns","title":"Related Patterns","text":"<ul> <li>Merge/Upsert \u2192 Maintains clean Silver data that aggregates read from</li> <li>Append-Only Raw \u2192 Source of truth if aggregates need replay</li> </ul>"},{"location":"patterns/windowed_reprocess/#references","title":"References","text":"<ul> <li>Odibi Architecture Manifesto: Pattern C - Aggregation</li> <li>Databricks: Dynamic Partition Overwrite</li> <li>Fundamentals of Data Engineering: Chapter on Aggregation</li> </ul>"},{"location":"plans/gold_layer_implementation_plan/","title":"ODIBI Gold Layer Enhancement Plan","text":""},{"location":"plans/gold_layer_implementation_plan/#executive-summary","title":"Executive Summary","text":"<p>Enhance ODIBI's Gold layer to support declarative dimensional modeling. Currently, users must write manual SQL for surrogate key lookups, aggregations, and FK validation. This plan adds patterns that make dimensional modeling as easy as Bronze/Silver.</p>"},{"location":"plans/gold_layer_implementation_plan/#current-state-vs-target-state","title":"Current State vs Target State","text":"Capability Current Target Fact table with SK lookups Manual SQL <code>FactPattern</code> auto-lookups Dimension with surrogate keys Manual + SCD2 <code>DimensionPattern</code> Date dimension Manual <code>DateDimensionPattern</code> Aggregations Manual SQL <code>AggregationPattern</code> FK validation None Built into FactPattern Semantic layer Storage only Execute + materialize"},{"location":"plans/gold_layer_implementation_plan/#implementation-phases","title":"Implementation Phases","text":""},{"location":"plans/gold_layer_implementation_plan/#phase-1-enhanced-dimensionpattern","title":"Phase 1: Enhanced DimensionPattern","text":"<p>Goal: Single pattern to build a complete dimension table with surrogate keys and SCD2.</p> <p>Location: <code>odibi/patterns/dimension.py</code></p> <p>Features: 1. Auto-generate surrogate key (sequential integer) 2. Support SCD Type 0, 1, 2 (configurable) 3. Add \"Unknown\" member automatically (SK = 0 or -1) 4. Add audit columns (load_timestamp, source_system) 5. Column aliasing for BI-friendly names</p> <p>Config Example:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id           # Business key (from Silver)\n    surrogate_key: customer_sk         # Generated (auto-increment)\n    scd_type: 2                        # 0=static, 1=overwrite, 2=history\n    track_columns: [name, email, city] # Columns to track for changes\n    unknown_member: true               # Add row for orphan FKs\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre> <p>Implementation Tasks: - [ ] Create <code>DimensionPattern</code> class extending <code>Pattern</code> - [ ] Implement surrogate key generation (max + row_number) - [ ] Integrate with existing SCD2 logic (reuse <code>scd.py</code>) - [ ] Add unknown member row insertion - [ ] Add audit column injection - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-2-datedimensionpattern","title":"Phase 2: DateDimensionPattern","text":"<p>Goal: Generate a complete date dimension table with pre-calculated attributes.</p> <p>Location: <code>odibi/patterns/date_dimension.py</code></p> <p>Features: 1. Generate date range (start_date to end_date) 2. Pre-calculate: day_of_week, is_weekend, month, quarter, year 3. Support fiscal calendar offset 4. Holiday integration (optional)</p> <p>Config Example:</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    date_key_format: \"yyyyMMdd\"        # 20240115\n    fiscal_year_start_month: 7          # July = FY start\n    include_time: false                 # Date only, no time grain\n</code></pre> <p>Generated Columns: - date_sk (INT, primary key) - full_date (DATE) - day_of_week (STRING: Monday, Tuesday...) - day_of_week_num (INT: 1-7) - is_weekend (BOOLEAN) - week_of_year (INT) - month (INT) - month_name (STRING) - quarter (INT: 1-4) - quarter_name (STRING: Q1, Q2...) - year (INT) - fiscal_year (INT) - fiscal_quarter (INT) - is_month_start (BOOLEAN) - is_month_end (BOOLEAN)</p> <p>Implementation Tasks: - [ ] Create <code>DateDimensionPattern</code> class - [ ] Date range generator (Pandas date_range / Spark sequence) - [ ] Attribute calculators - [ ] Fiscal calendar logic - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-3-enhanced-factpattern","title":"Phase 3: Enhanced FactPattern","text":"<p>Goal: Declarative fact table building with automatic SK lookups and validation.</p> <p>Location: <code>odibi/patterns/fact.py</code> (enhance existing)</p> <p>Features: 1. Declare dimension relationships 2. Auto-lookup surrogate keys from dimensions 3. Handle orphans (use unknown member or reject) 4. Validate grain (no duplicates at PK level) 5. Add audit columns 6. Enforce append-only mode</p> <p>Config Example:</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]                  # What makes a row unique\n\n    dimensions:\n      - source_column: customer_id     # Column in Silver data\n        dimension_table: dim_customer  # Gold dimension to lookup\n        dimension_key: customer_id     # Natural key in dimension\n        surrogate_key: customer_sk     # SK to retrieve\n        scd2: true                     # Filter is_current = true\n\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n\n      - source_column: order_date\n        dimension_table: dim_date\n        dimension_key: full_date\n        surrogate_key: date_sk\n\n    orphan_handling: unknown           # unknown | reject | quarantine\n\n    measures:\n      - quantity                       # Pass through\n      - unit_price: price              # Rename\n      - total_amount: \"quantity * price\"  # Calculated\n\n    audit:\n      load_timestamp: true\n      source_system: \"pos\"\n</code></pre> <p>Implementation Tasks: - [ ] Extend <code>FactPattern</code> class - [ ] Dimension lookup logic (generate JOIN SQL) - [ ] Orphan handling (COALESCE to unknown SK) - [ ] Grain validation (check for duplicates) - [ ] Measure calculation/renaming - [ ] Audit column injection - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-4-aggregationpattern","title":"Phase 4: AggregationPattern","text":"<p>Goal: Declarative aggregation with time-grain rollups.</p> <p>Location: <code>odibi/patterns/aggregation.py</code></p> <p>Features: 1. Declare grain (GROUP BY columns) 2. Declare measures with aggregation functions 3. Incremental aggregation (merge new data) 4. Time rollups (daily \u2192 weekly \u2192 monthly)</p> <p>Config Example:</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    source: fact_orders\n    grain: [date_sk, product_sk, region_sk]\n\n    measures:\n      - name: total_revenue\n        expr: \"SUM(total_amount)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n      - name: avg_order_value\n        expr: \"AVG(total_amount)\"\n      - name: unique_customers\n        expr: \"COUNT(DISTINCT customer_sk)\"\n\n    incremental:\n      timestamp_column: date_sk\n      merge_strategy: replace           # replace | sum\n\n    rollups:                            # Optional: generate multiple grains\n      - grain: [month, product_sk]\n        output: agg_monthly_product\n</code></pre> <p>Implementation Tasks: - [ ] Create <code>AggregationPattern</code> class - [ ] SQL generation for GROUP BY + aggregates - [ ] Incremental merge logic - [ ] Rollup generation - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-5-semantic-layer-execution","title":"Phase 5: Semantic Layer Execution","text":"<p>Goal: Execute metric definitions stored in meta_metrics.</p> <p>Location: <code>odibi/semantics/</code> (new module)</p> <p>Features: 1. Define metrics in YAML (not just SQL strings) 2. Query interface: \"revenue BY region, month\" 3. Materialize metrics on schedule 4. Serve via API (optional, future)</p> <p>Config Example (in odibi.yaml):</p> <pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"revenue / COUNT(*)\"\n    type: derived\n\ndimensions:\n  - name: order_date\n    source: dim_date\n    hierarchy: [year, quarter, month, full_date]\n\n  - name: product\n    source: dim_product\n    hierarchy: [category, subcategory, product_name]\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    schedule: \"0 2 1 * *\"              # 2am on 1st of month\n    output: gold/agg_monthly_revenue\n</code></pre> <p>Implementation Tasks: - [ ] Create <code>odibi/semantics/</code> module - [ ] Metric definition parser - [ ] Query generator (metrics BY dimensions) - [ ] Materialization executor - [ ] CLI: <code>odibi query \"revenue BY region\"</code> - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#phase-6-relationship-registry-fk-validation","title":"Phase 6: Relationship Registry &amp; FK Validation","text":"<p>Goal: Declare and validate star schema relationships.</p> <p>Location: <code>odibi/catalog.py</code> (extend) + <code>odibi/validation/fk.py</code> (new)</p> <p>Features: 1. Declare relationships in YAML 2. Validate referential integrity on fact load 3. Detect orphan records 4. Generate lineage from relationships</p> <p>Config Example:</p> <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n</code></pre> <p>Implementation Tasks: - [ ] Relationship config schema - [ ] FK validation logic - [ ] Orphan detection reporting - [ ] Integration with FactPattern - [ ] Unit tests - [ ] Documentation</p>"},{"location":"plans/gold_layer_implementation_plan/#implementation-order","title":"Implementation Order","text":"<pre><code>Phase 1: DimensionPattern     \u2190\u2500\u2500 Start here (foundation)\n    \u2193\nPhase 2: DateDimensionPattern \u2190\u2500\u2500 Quick win, very useful\n    \u2193\nPhase 3: Enhanced FactPattern \u2190\u2500\u2500 Core value (auto SK lookups)\n    \u2193\nPhase 4: AggregationPattern   \u2190\u2500\u2500 Performance optimization\n    \u2193\nPhase 5: Semantic Layer       \u2190\u2500\u2500 Advanced (can defer)\n    \u2193\nPhase 6: FK Validation        \u2190\u2500\u2500 Polish (can defer)\n</code></pre>"},{"location":"plans/gold_layer_implementation_plan/#file-structure","title":"File Structure","text":"<pre><code>odibi/\n\u251c\u2500\u2500 patterns/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py              # Existing\n\u2502   \u251c\u2500\u2500 scd2.py              # Existing\n\u2502   \u251c\u2500\u2500 fact.py              # Enhance\n\u2502   \u251c\u2500\u2500 merge.py             # Existing\n\u2502   \u251c\u2500\u2500 snapshot.py          # Existing\n\u2502   \u251c\u2500\u2500 dimension.py         # NEW (Phase 1)\n\u2502   \u251c\u2500\u2500 date_dimension.py    # NEW (Phase 2)\n\u2502   \u2514\u2500\u2500 aggregation.py       # NEW (Phase 4)\n\u251c\u2500\u2500 semantics/               # NEW (Phase 5)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 metrics.py\n\u2502   \u251c\u2500\u2500 query.py\n\u2502   \u2514\u2500\u2500 materialize.py\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 engine.py            # Existing\n\u2502   \u251c\u2500\u2500 gate.py              # Existing\n\u2502   \u2514\u2500\u2500 fk.py                # NEW (Phase 6)\n\u2514\u2500\u2500 config.py                # Add new config schemas\n</code></pre>"},{"location":"plans/gold_layer_implementation_plan/#testing-strategy","title":"Testing Strategy","text":"<p>Each pattern needs: 1. Unit tests with mock data (Pandas engine) 2. Integration tests (if Spark available) 3. Example YAML configs in <code>examples/</code> 4. Documentation in <code>docs/</code></p>"},{"location":"plans/gold_layer_implementation_plan/#success-criteria","title":"Success Criteria","text":"<p>After implementation, this YAML should work:</p> <pre><code>pipelines:\n  - pipeline: gold_dimensional\n    nodes:\n      # Date dimension (generated)\n      - name: dim_date\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2020-01-01\"\n            end_date: \"2030-12-31\"\n        write:\n          connection: gold\n          path: dim_date\n\n      # Customer dimension (from Silver)\n      - name: dim_customer\n        depends_on: [clean_customers]\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_columns: [name, city]\n            unknown_member: true\n        write:\n          connection: gold\n          path: dim_customer\n\n      # Fact table (auto SK lookups)\n      - name: fact_orders\n        depends_on: [clean_orders, dim_customer, dim_product, dim_date]\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - total_amount: \"quantity * price\"\n        write:\n          connection: gold\n          path: fact_orders\n          mode: append\n\n      # Daily aggregation\n      - name: agg_daily_sales\n        depends_on: [fact_orders]\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: revenue\n                expr: \"SUM(total_amount)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n        write:\n          connection: gold\n          path: agg_daily_sales\n</code></pre>"},{"location":"plans/gold_layer_implementation_plan/#timeline-estimate","title":"Timeline Estimate","text":"Phase Effort Priority Phase 1: DimensionPattern 2-3 days High Phase 2: DateDimensionPattern 1 day High Phase 3: Enhanced FactPattern 3-4 days High Phase 4: AggregationPattern 2 days Medium Phase 5: Semantic Layer 3-5 days Low (defer) Phase 6: FK Validation 1-2 days Low (defer) <p>Total for Phases 1-4: ~8-10 days</p>"},{"location":"plans/gold_layer_implementation_prompt/","title":"Prompt for Next Thread: Implement ODIBI Gold Layer Patterns","text":"<p>Copy and paste this into a new Amp thread:</p>"},{"location":"plans/gold_layer_implementation_prompt/#implement-phase-1-dimensionpattern-for-odibi","title":"Implement Phase 1: DimensionPattern for ODIBI","text":""},{"location":"plans/gold_layer_implementation_prompt/#context","title":"Context","text":"<p>I'm enhancing ODIBI's Gold layer to support declarative dimensional modeling. See the full plan at <code>docs/plans/gold_layer_implementation_plan.md</code>.</p> <p>ODIBI already has: - <code>odibi/patterns/scd2.py</code> - SCD Type 2 logic - <code>odibi/patterns/fact.py</code> - Basic fact pattern (just dedup) - <code>odibi/patterns/base.py</code> - Base Pattern class - <code>odibi/transformers/advanced.py</code> - <code>generate_surrogate_key</code> transformer</p>"},{"location":"plans/gold_layer_implementation_prompt/#task","title":"Task","text":"<p>Create <code>odibi/patterns/dimension.py</code> - a new <code>DimensionPattern</code> that builds a complete dimension table.</p>"},{"location":"plans/gold_layer_implementation_prompt/#requirements","title":"Requirements","text":"<ol> <li>Surrogate Key Generation</li> <li>Auto-generate integer surrogate key (e.g., <code>customer_sk</code>)</li> <li>Use <code>MAX(existing_sk) + ROW_NUMBER()</code> for new rows</li> <li> <p>Handle first run (no existing table)</p> </li> <li> <p>SCD Support</p> </li> <li><code>scd_type: 0</code> - Static (never update)</li> <li><code>scd_type: 1</code> - Overwrite (no history)</li> <li> <p><code>scd_type: 2</code> - Track history (reuse existing <code>scd2.py</code> logic)</p> </li> <li> <p>Unknown Member</p> </li> <li>If <code>unknown_member: true</code>, insert a row with SK=0 and \"Unknown\" values</li> <li> <p>This row is used for orphan FK handling in fact tables</p> </li> <li> <p>Audit Columns</p> </li> <li>Add <code>load_timestamp</code> (current timestamp)</li> <li> <p>Add <code>source_system</code> (from config)</p> </li> <li> <p>Config Schema <code>yaml    pattern:      type: dimension      params:        natural_key: customer_id         # Required        surrogate_key: customer_sk       # Required        scd_type: 2                      # 0, 1, or 2        track_columns: [name, city]      # For SCD1/2        target: gold/dim_customer        # For SCD2 (existing table)        unknown_member: true             # Optional        audit:          load_timestamp: true          source_system: \"crm\"</code></p> </li> </ol>"},{"location":"plans/gold_layer_implementation_prompt/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Create <code>odibi/patterns/dimension.py</code></li> <li>Create <code>DimensionPattern</code> class extending <code>Pattern</code></li> <li>Implement <code>validate()</code> method</li> <li>Implement <code>execute()</code> method with:</li> <li>SK generation logic</li> <li>SCD routing (0/1/2)</li> <li>Unknown member insertion</li> <li>Audit column injection</li> <li>Register pattern in <code>odibi/patterns/__init__.py</code></li> <li>Add config schema in <code>odibi/config.py</code> if needed</li> <li>Create unit tests in <code>tests/patterns/test_dimension.py</code></li> <li>Test with both Pandas and Spark engines</li> </ol>"},{"location":"plans/gold_layer_implementation_prompt/#reference-files","title":"Reference Files","text":"<p>Look at these for patterns to follow: - <code>odibi/patterns/scd2.py</code> - SCD2 logic to reuse - <code>odibi/patterns/fact.py</code> - Simple pattern structure - <code>odibi/patterns/base.py</code> - Base class - <code>odibi/transformers/advanced.py</code> - <code>generate_surrogate_key</code> for reference</p>"},{"location":"plans/gold_layer_implementation_prompt/#acceptance-criteria","title":"Acceptance Criteria","text":"<p>This config should work:</p> <pre><code>- name: dim_customer\n  depends_on: [clean_customers]\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      track_columns: [name, email, city]\n      target: gold/dim_customer\n      unknown_member: true\n      audit:\n        load_timestamp: true\n        source_system: \"crm\"\n  write:\n    connection: gold\n    path: dim_customer\n</code></pre> <p>And produce:</p> customer_sk customer_id name email city valid_from valid_to is_current load_timestamp source_system 0 -1 Unknown unknown@unknown.com Unknown 1900-01-01 NULL true 2024-01-15 crm 1 abc123... John john@mail.com NYC 2024-01-15 NULL true 2024-01-15 crm <p>After Phase 1 is complete, we'll move to Phase 2 (DateDimensionPattern) and Phase 3 (Enhanced FactPattern).</p>"},{"location":"reference/PARITY_TABLE/","title":"Engine Parity Table","text":"<p>Comparison of Pandas, Spark, and Polars engine capabilities as of V3.</p>"},{"location":"reference/PARITY_TABLE/#write-modes","title":"Write Modes","text":"Mode Pandas Spark Polars Notes <code>overwrite</code> \u2705 \u2705 \u2705 Default mode. <code>append</code> \u2705 \u2705 \u2705 <code>error</code> \u2705 \u2705 \u2705 Fails if table exists. <code>ignore</code> \u2705 \u2705 \u2705 Skips if table exists. <code>upsert</code> \u2705 \u2705 \u2705 Requires <code>keys</code>. Spark: Delta Lake only. <code>append_once</code> \u2705 \u2705 \u2705 Requires <code>keys</code>. Spark: Delta Lake only."},{"location":"reference/PARITY_TABLE/#core-features","title":"Core Features","text":"Feature Pandas Spark Polars Notes Reading (CSV, Parquet, JSON) \u2705 \u2705 \u2705 Reading (Delta Lake) \u2705 \u2705 \u2705 Pandas/Polars require <code>deltalake</code> package. Reading (SQL Server) \u2705 \u2705 \u2705 SQL Transformations \u2705 \u2705 \u2705 Pandas uses DuckDB/SQLite. Polars uses native SQL context. PII Anonymization \u2705 \u2705 \u2705 Hash, Mask, Redact. Schema Validation \u2705 \u2705 \u2705 Data Contracts \u2705 \u2705 \u2705 Incremental Loading \u2705 \u2705 \u2705 Rolling Window &amp; Stateful. Time Travel \u2705 \u2705 \u2705 Delta Lake only."},{"location":"reference/PARITY_TABLE/#execution-context","title":"Execution Context","text":"Property Pandas Spark Polars Notes <code>df</code> \u2705 \u2705 \u2705 Native DataFrame. <code>columns</code> \u2705 \u2705 \u2705 <code>schema</code> \u2705 \u2705 \u2705 Dictionary of types. <code>pii_metadata</code> \u2705 \u2705 \u2705 Active PII columns."},{"location":"reference/cheatsheet/","title":"\u26a1 Odibi Cheatsheet","text":""},{"location":"reference/cheatsheet/#cli-commands","title":"CLI Commands","text":"Command Description <code>odibi run odibi.yaml</code> Run the pipeline. <code>odibi run odibi.yaml --dry-run</code> Validate connections without moving data. <code>odibi doctor odibi.yaml</code> Check environment and config health. <code>odibi stress odibi.yaml</code> Run fuzz tests (random data) to find bugs. <code>odibi story view --latest</code> Open the latest run report. <code>odibi secrets init odibi.yaml</code> Create .env template for secrets. <code>odibi graph odibi.yaml</code> Visualize pipeline dependencies. <code>odibi generate-project</code> Scaffold a new project from files."},{"location":"reference/cheatsheet/#odibiyaml-structure","title":"<code>odibi.yaml</code> Structure","text":"<pre><code>version: 1\nproject: My Project\nengine: pandas          # or 'spark'\n\nconnections:\n  raw_data:\n    type: local\n    base_path: ./data\n\nstory:\n  connection: raw_data\n  path: stories/\n\npipelines:\n  - pipeline: main_etl\n    nodes:\n      # 1. Read\n      - name: load_csv\n        read:\n          connection: raw_data\n          path: input.csv\n          format: csv\n\n      # 2. Transform (SQL)\n      - name: clean_data\n        depends_on: [load_csv]\n        transform:\n          steps:\n            - \"SELECT * FROM load_csv WHERE id IS NOT NULL\"\n\n      # 3. Transform (Python)\n      - name: advanced_clean\n        depends_on: [clean_data]\n        transform:\n          steps:\n            - operation: my_custom_func  # defined in python\n              params:\n                threshold: 10\n\n      # 4. Write\n      - name: save_parquet\n        depends_on: [advanced_clean]\n        write:\n          connection: raw_data\n          path: output.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"reference/cheatsheet/#python-transformation","title":"Python Transformation","text":"<pre><code>from odibi.transformations import transformation\n\n@transformation(\"my_custom_func\")\ndef my_func(df, threshold=10):\n    \"\"\"Docstrings are required!\"\"\"\n    return df[df['val'] &gt; threshold]\n</code></pre>"},{"location":"reference/cheatsheet/#cross-pipeline-dependencies","title":"Cross-Pipeline Dependencies","text":"<p>Reference outputs from other pipelines using <code>$pipeline.node</code> syntax:</p> <pre><code>nodes:\n  - name: enriched_data\n    inputs:\n      # Cross-pipeline reference\n      events: $read_bronze.shift_events\n\n      # Explicit read config\n      calendar:\n        connection: prod\n        path: \"bronze/calendar\"\n        format: delta\n    transform:\n      steps:\n        - operation: join\n          left: events\n          right: calendar\n          on: [date_id]\n</code></pre> Syntax Example Description <code>$pipeline.node</code> <code>$read_bronze.orders</code> Reference node output from another pipeline <p>Requirements: - Referenced pipeline must have run first - Referenced node must have a <code>write</code> block - Cannot use both <code>read</code> and <code>inputs</code> in same node</p>"},{"location":"reference/cheatsheet/#sql-templates","title":"SQL Templates","text":"Variable Value <code>${source}</code> The path of the source file (if reading). <code>${SELF}</code> The name of the current node."},{"location":"reference/configuration/","title":"ODIBI Configuration System Explained","text":"<p>Last Updated: 2025-11-21 Author: Henry Odibi Purpose: Demystify how YAML configs, Python classes, and execution flow together</p>"},{"location":"reference/configuration/#table-of-contents","title":"\ud83d\udcd1 Table of Contents","text":"<ol> <li>The Big Picture</li> <li>Configuration Flow (Concept \u2192 Execution)</li> <li>Three Layers of Configuration</li> <li>Example: Tracing a Pipeline from YAML to Execution</li> <li>Key Concepts Explained</li> <li>Common Confusion Points</li> <li>Decision Trees</li> <li>Quick Reference</li> </ol>"},{"location":"reference/configuration/#the-big-picture","title":"The Big Picture","text":"<p>ODIBI has three distinct layers that work together:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 1: YAML Configuration (What You Write)              \u2502\n\u2502  - Declarative syntax (project.yaml, pipelines/)            \u2502\n\u2502  - Human-readable, version-controlled                       \u2502\n\u2502  - Defines WHAT to do, not HOW                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Parsed by\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 2: Pydantic Models (config.py)                      \u2502\n\u2502  - Validates YAML structure                                 \u2502\n\u2502  - Enforces required fields, types, constraints             \u2502\n\u2502  - Converts YAML \u2192 Python objects                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Used by\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 3: Runtime Classes (pipeline.py, node.py, etc.)     \u2502\n\u2502  - Executes the actual work                                 \u2502\n\u2502  - Manages context, engines, connections                    \u2502\n\u2502  - Generates stories, handles errors                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Insight: You interact with Layer 1 (YAML), but under the hood, Pydantic (Layer 2) ensures correctness before Runtime (Layer 3) does the work.</p>"},{"location":"reference/configuration/#configuration-flow-concept-execution","title":"Configuration Flow (Concept \u2192 Execution)","text":"<p>Let's trace a real pipeline from file to execution:</p>"},{"location":"reference/configuration/#step-1-you-write-yaml-user","title":"Step 1: You Write YAML (User)","text":"<pre><code># examples/templates/example_local.yaml\nproject: My Pipeline\nengine: pandas\nconnections:\n  local:\n    type: local\n    base_path: ./data\npipelines:\n  - pipeline: bronze_to_silver\n    nodes:\n      - name: load_raw_sales\n        read:\n          connection: local\n          path: bronze/sales.csv\n          format: csv\n</code></pre>"},{"location":"reference/configuration/#step-2-yaml-pydantic-models-automatic","title":"Step 2: YAML \u2192 Pydantic Models (Automatic)","text":"<p>When you call <code>Pipeline.from_yaml(\"examples/templates/example_local.yaml\")</code>:</p> <pre><code># pipeline.py (line 317)\nwith open(yaml_path, \"r\") as f:\n    config = yaml.safe_load(f)  # Loads YAML as Python dict\n\n# Parse into Pydantic models (config.py)\nproject_config = ProjectConfig(**config)  # Validates project/engine/connections\npipeline_config = PipelineConfig(**config['pipelines'][0])  # Validates pipeline structure\n</code></pre> <p>What Pydantic does: - Checks that <code>project</code>, <code>engine</code>, <code>connections</code> exist - Ensures <code>engine</code> is \"pandas\", \"spark\", or \"polars\" (not \"panda\" or \"pands\") - Validates each node has required fields (<code>name</code>, at least one operation) - Converts strings to enums where needed (e.g., <code>engine: \"pandas\"</code> \u2192 <code>EngineType.PANDAS</code>)</p> <p>If validation fails:</p> <pre><code>ValidationError: 1 validation error for PipelineConfig\nnodes.0.read.connection\n  field required (type=value_error.missing)\n</code></pre> <p>This is Layer 2 catching errors before execution.</p>"},{"location":"reference/configuration/#step-3-create-connections-automatic","title":"Step 3: Create Connections (Automatic)","text":"<pre><code># pipeline.py (lines 342-351) - PipelineManager.from_yaml()\nconnections = {}\nfor conn_name, conn_config in config.get(\"connections\", {}).items():\n    if conn_config.get(\"type\") == \"local\":\n        connections[conn_name] = LocalConnection(\n            base_path=conn_config.get(\"base_path\", \"./data\")\n        )\n</code></pre> <p>Result: Python objects ready to use:</p> <pre><code>connections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n</code></pre>"},{"location":"reference/configuration/#step-4-create-pipelinemanager-automatic","title":"Step 4: Create PipelineManager (Automatic)","text":"<pre><code># pipeline.py (lines 280-314)\nmanager = PipelineManager(\n    yaml_config=config,\n    engine=\"pandas\",\n    connections=connections,\n    story_config=config.get(\"story\", {})\n)\n</code></pre> <p>What PipelineManager does: - Creates a <code>Pipeline</code> object for each pipeline in the YAML - Stores them in <code>self._pipelines</code> dictionary keyed by pipeline name - Example: <code>manager._pipelines = {\"bronze_to_silver\": Pipeline(...), \"silver_to_gold\": Pipeline(...)}</code></p>"},{"location":"reference/configuration/#step-5-run-pipelines-user","title":"Step 5: Run Pipelines (User)","text":"<pre><code># You call:\nresults = manager.run()  # Runs ALL pipelines\n\n# Or:\nresult = manager.run('bronze_to_silver')  # Runs specific pipeline\n</code></pre> <p>What happens: 1. <code>PipelineManager.run()</code> looks up the pipeline by name 2. Calls <code>Pipeline.run()</code> (line 134) 3. <code>Pipeline.run()</code> orchestrates node execution:    - Builds dependency graph    - Topologically sorts nodes    - Executes each node in order    - Passes data via Context    - Generates story</p>"},{"location":"reference/configuration/#three-layers-of-configuration","title":"Three Layers of Configuration","text":""},{"location":"reference/configuration/#layer-1-yaml-files-declarative","title":"Layer 1: YAML Files (Declarative)","text":"<p>Purpose: Human-readable, version-controlled pipeline definitions</p> <p>Key Files: - Project-level: <code>project.yaml</code> or any YAML with <code>project</code> + <code>connections</code> + <code>pipelines</code> - Pipeline-level: Individual YAML files with specific pipelines</p> <p>What You Define: | Section | Purpose | Required | |---------|---------|----------| | <code>project</code> | Project name | \u2705 Yes | | <code>engine</code> | Execution engine (pandas/spark/polars) | \u2705 Yes | | <code>connections</code> | Where data lives | \u2705 Yes | | <code>pipelines</code> | List of pipelines | \u2705 Yes | | <code>story</code> | Story generation config | \u2705 Yes |</p> <p>Example:</p> <pre><code>project: Sales Analytics\nengine: pandas\nconnections:\n  warehouse:\n    type: local\n    base_path: /data/warehouse\npipelines:\n  - pipeline: daily_sales\n    nodes: [...]\n</code></pre>"},{"location":"reference/configuration/#layer-2-pydantic-models-validation","title":"Layer 2: Pydantic Models (Validation)","text":"<p>Purpose: Type-safe, validated Python objects</p> <p>Key File: <code>odibi/config.py</code></p> <p>Main Models:</p>"},{"location":"reference/configuration/#projectconfig-line-266","title":"ProjectConfig (Line 266)","text":"<pre><code>class ProjectConfig(BaseModel):\n    project: str  # Required\n    version: str = \"1.0.0\"  # Default\n    engine: EngineType = EngineType.PANDAS  # Default\n    connections: Dict[str, Dict[str, Any]]  # Required\n    story: StoryConfig  # Required\n    pipelines: List[PipelineConfig]  # Required\n    retry: RetryConfig = RetryConfig()  # Default\n    logging: LoggingConfig = LoggingConfig()  # Default\n</code></pre> <p>Maps to YAML:</p> <pre><code>project: My Pipeline      # \u2192 project\nversion: \"2.0.0\"          # \u2192 version\nengine: pandas            # \u2192 engine (validated as EngineType.PANDAS)\nconnections:\n  data:                   # \u2192 connections[\"data\"]\n    type: local\n    base_path: ./data\n  outputs:                # \u2192 connections[\"outputs\"]\n    type: local\n    base_path: ./outputs\n  api_source:             # \u2192 connections[\"api_source\"]\n    type: http\n    base_url: \"https://api.example.com/v1\"\n    headers:\n      Authorization: \"Bearer ${API_TOKEN}\"\nstory:\n  connection: outputs     # \u2192 story.connection (required)\n  path: stories/          # \u2192 story.path\n  auto_generate: true     # \u2192 story.auto_generate\n  max_sample_rows: 10     # \u2192 story.max_sample_rows\n  retention_days: 30      # \u2192 story.retention_days\n  retention_count: 100    # \u2192 story.retention_count\nretry:\n  enabled: true           # \u2192 retry.enabled\n  max_attempts: 3         # \u2192 retry.max_attempts\n  backoff: exponential    # \u2192 retry.backoff\nlogging:\n  level: INFO             # \u2192 logging.level\npipelines:                # \u2192 pipelines (list of pipeline configs)\n  - pipeline: example\n    nodes: [...]\n</code></pre>"},{"location":"reference/configuration/#pipelineconfig-line-203","title":"PipelineConfig (Line 203)","text":"<pre><code>class PipelineConfig(BaseModel):\n    pipeline: str  # Required (pipeline name)\n    description: Optional[str] = None\n    layer: Optional[str] = None\n    nodes: List[NodeConfig]  # Required (at least one node)\n</code></pre> <p>Maps to YAML:</p> <pre><code>pipelines:\n  - pipeline: bronze_to_silver  # \u2192 pipeline\n    layer: transformation       # \u2192 layer\n    description: \"Clean data\"   # \u2192 description\n    nodes: [...]               # \u2192 nodes\n</code></pre>"},{"location":"reference/configuration/#nodeconfig-line-172","title":"NodeConfig (Line 172)","text":"<pre><code>class NodeConfig(BaseModel):\n    name: str  # Required (unique within pipeline)\n    depends_on: List[str] = []\n    read: Optional[ReadConfig] = None\n    inputs: Optional[Dict[str, Union[str, Dict[str, Any]]]] = None  # Cross-pipeline dependencies\n    transform: Optional[TransformConfig] = None\n    write: Optional[WriteConfig] = None\n    cache: bool = False\n    sensitive: Union[bool, List[str]] = False  # PII Masking\n</code></pre> <p>Maps to YAML:</p> <pre><code>nodes:\n  - name: load_raw_sales         # \u2192 name\n    depends_on: [prev_node]      # \u2192 depends_on\n    sensitive: [\"email\"]         # \u2192 sensitive (Redact email in reports)\n    read:                        # \u2192 read (ReadConfig)\n      connection: local\n      path: bronze/sales.csv\n      format: csv\n    cache: true                  # \u2192 cache\n</code></pre> <p>Cross-Pipeline Dependencies (<code>inputs</code> block):</p> <p>For multi-input nodes that read from other pipelines, use the <code>inputs</code> block instead of <code>read</code>:</p> <pre><code>nodes:\n  - name: enriched_data\n    inputs:\n      events: $read_bronze.shift_events      # Cross-pipeline reference\n      calendar:                               # Explicit read config\n        connection: goat_prod\n        path: \"bronze/calendar\"\n        format: delta\n    transform:\n      steps:\n        - operation: join\n          left: events\n          right: calendar\n          on: [date_id]\n</code></pre> <p>Reference Syntax: <code>$pipeline_name.node_name</code> - The <code>$</code> prefix indicates a cross-pipeline reference - References are resolved via the Odibi Catalog (<code>meta_outputs</code> table) - The referenced node must have a <code>write</code> block and the pipeline must have run previously</p> <p>Validation Rules: - Node must have at least one of: <code>read</code>, <code>inputs</code>, <code>transform</code>, <code>write</code> - All node names must be unique within a pipeline - Connections referenced in <code>read.connection</code> or <code>write.connection</code> should exist (warned, not enforced) - Cannot have both <code>read</code> and <code>inputs</code> \u2014 use <code>read</code> for single-source nodes or <code>inputs</code> for multi-source cross-pipeline dependencies</p>"},{"location":"reference/configuration/#layer-3-runtime-classes-execution","title":"Layer 3: Runtime Classes (Execution)","text":"<p>Purpose: Execute the actual work</p> <p>Key Files: - <code>odibi/pipeline.py</code> - Pipeline orchestration - <code>odibi/node.py</code> - Individual node execution - <code>odibi/context.py</code> - Data passing between nodes - <code>odibi/engine/pandas_engine.py</code> - Actual read/write/transform logic</p> <p>Main Classes:</p>"},{"location":"reference/configuration/#pipelinemanager-line-280","title":"PipelineManager (Line 280)","text":"<pre><code>class PipelineManager:\n    def __init__(self, yaml_config, engine, connections, story_config):\n        self._pipelines = {}  # Dict[pipeline_name -&gt; Pipeline]\n        for pipeline_config_dict in yaml_config[\"pipelines\"]:\n            pipeline_config = PipelineConfig(**pipeline_config_dict)\n            self._pipelines[pipeline_config.pipeline] = Pipeline(...)\n\n    def run(self, pipelines=None):\n        # Run all or specific pipelines\n</code></pre> <p>Responsibilities: - Load and validate YAML - Instantiate connections - Create Pipeline objects for each pipeline - Orchestrate multi-pipeline execution - Return results</p>"},{"location":"reference/configuration/#pipeline-line-63","title":"Pipeline (Line 63)","text":"<pre><code>class Pipeline:\n    def __init__(self, pipeline_config, engine, connections, ...):\n        self.config = pipeline_config  # PipelineConfig from Layer 2\n        self.engine = PandasEngine()   # Or SparkEngine\n        self.context = create_context(engine)\n        self.graph = DependencyGraph(pipeline_config.nodes)\n\n    def run(self):\n        execution_order = self.graph.topological_sort()\n        for node_name in execution_order:\n            node = Node(...)\n            node_result = node.execute()\n</code></pre> <p>Responsibilities: - Build dependency graph - Determine execution order - Execute nodes sequentially (or parallel in future) - Manage context for data passing - Generate stories</p>"},{"location":"reference/configuration/#node-odibinodepy","title":"Node (odibi/node.py)","text":"<pre><code>class Node:\n    def execute(self):\n        # 1. Read data (if read config exists)\n        if self.config.read:\n            data = self.engine.read(...)\n            self.context.register(self.config.name, data)\n\n        # 2. Transform data (if transform config exists)\n        if self.config.transform:\n            data = self.engine.execute_transform(...)\n            self.context.register(self.config.name, data)\n\n        # 3. Write data (if write config exists)\n        if self.config.write:\n            self.engine.write(...)\n</code></pre> <p>Responsibilities: - Execute read \u2192 transform \u2192 write for a single node - Use engine for actual operations - Register results in context - Return NodeResult</p>"},{"location":"reference/configuration/#example-tracing-a-pipeline-from-yaml-to-execution","title":"Example: Tracing a Pipeline from YAML to Execution","text":"<p>Let's trace this simple YAML:</p> <pre><code>project: Simple Pipeline\nengine: pandas\nconnections:\n  local:\n    type: local\n    base_path: ./data\n  outputs:\n    type: local\n    base_path: ./outputs\nstory:\n  connection: outputs\n  path: stories/\n  enabled: true\npipelines:\n  - pipeline: example\n    nodes:\n      - name: load_data\n        read:\n          connection: local\n          path: input.csv\n          format: csv\n        cache: true\n\n      - name: clean_data\n        depends_on: [load_data]\n        transform:\n          steps:\n            - \"SELECT * FROM load_data WHERE amount &gt; 0\"\n\n      - name: save_data\n        depends_on: [clean_data]\n        write:\n          connection: local\n          path: output.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"reference/configuration/#execution-trace","title":"Execution Trace","text":"<p>1. User calls:</p> <pre><code>from odibi.pipeline import Pipeline\nmanager = Pipeline.from_yaml(\"simple.yaml\")\n</code></pre> <p>2. <code>Pipeline.from_yaml()</code> delegates to <code>PipelineManager.from_yaml()</code> (line 109)</p> <p>3. <code>PipelineManager.from_yaml()</code> (line 317):</p> <pre><code># Load YAML\nwith open(\"simple.yaml\") as f:\n    config = yaml.safe_load(f)\n# config = {\n#     \"project\": \"Simple Pipeline\",\n#     \"engine\": \"pandas\",\n#     \"connections\": {\"local\": {\"type\": \"local\", \"base_path\": \"./data\"}},\n#     \"pipelines\": [{\"pipeline\": \"example\", \"nodes\": [...]}]\n# }\n\n# Validate project config (entire YAML - single source of truth)\nproject_config = ProjectConfig(**config)\n# \u2705 Validation passed - checks:\n#    - Required fields: project, connections, story, pipelines\n#    - story.connection exists in connections\n#    - engine is valid (pandas, spark, or polars)\n\n# Create connections\nconnections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n\n# Create PipelineManager\nmanager = PipelineManager(\n    project_config=project_config,\n    engine=\"pandas\",\n    connections=connections\n)\n</code></pre> <p>4. <code>PipelineManager.__init__()</code> (line 283):</p> <pre><code># Loop through pipelines in YAML\nfor pipeline_config_dict in config[\"pipelines\"]:\n    # Validate pipeline config\n    pipeline_config = PipelineConfig(\n        pipeline=\"example\",\n        nodes=[\n            NodeConfig(name=\"load_data\", read=ReadConfig(...), cache=True),\n            NodeConfig(name=\"clean_data\", depends_on=[\"load_data\"], transform=TransformConfig(...)),\n            NodeConfig(name=\"save_data\", depends_on=[\"clean_data\"], write=WriteConfig(...))\n        ]\n    )\n    # \u2705 Validation passed (all nodes have unique names, at least one operation each)\n\n    # Create Pipeline instance\n    self._pipelines[\"example\"] = Pipeline(\n        pipeline_config=pipeline_config,\n        engine=\"pandas\",\n        connections={\"local\": LocalConnection(...)},\n        story_config={}\n    )\n</code></pre> <p>5. User runs:</p> <pre><code>results = manager.run()  # Run all pipelines\n</code></pre> <p>6. <code>PipelineManager.run()</code> (line 363):</p> <pre><code># pipelines=None means run all\npipeline_names = list(self._pipelines.keys())  # [\"example\"]\n\n# Run each pipeline\nfor name in pipeline_names:\n    results[name] = self._pipelines[name].run()\n</code></pre> <p>7. <code>Pipeline.run()</code> (line 134):</p> <pre><code># Get execution order from dependency graph\nexecution_order = self.graph.topological_sort()\n# Returns: [\"load_data\", \"clean_data\", \"save_data\"]\n\n# Execute nodes in order\nfor node_name in execution_order:  # \"load_data\"\n    node_config = self.graph.nodes[\"load_data\"]\n    node = Node(\n        config=node_config,\n        context=self.context,\n        engine=self.engine,  # PandasEngine\n        connections={\"local\": LocalConnection(...)}\n    )\n    node_result = node.execute()\n</code></pre> <p>8. <code>Node.execute()</code> for \"load_data\" (odibi/node.py):</p> <pre><code># Node has read config\nif self.config.read:\n    # Get connection\n    conn = self.connections[\"local\"]  # LocalConnection(base_path=\"./data\")\n    full_path = conn.get_path(\"input.csv\")  # \"./data/input.csv\"\n\n    # Read using engine\n    data = self.engine.read(\n        path=full_path,\n        format=\"csv\",\n        options={}\n    )\n    # data = pandas.DataFrame(...)\n\n    # Register in context\n    self.context.register(\"load_data\", data)\n</code></pre> <p>9. <code>Node.execute()</code> for \"clean_data\":</p> <pre><code># Node has transform config\nif self.config.transform:\n    sql = \"SELECT * FROM load_data WHERE amount &gt; 0\"\n    data = self.engine.execute_sql(sql, self.context)\n    # Engine gets \"load_data\" DataFrame from context\n    # Executes SQL using pandasql or duckdb\n    # Returns filtered DataFrame\n\n    self.context.register(\"clean_data\", data)\n</code></pre> <p>10. <code>Node.execute()</code> for \"save_data\":</p> <pre><code># Node has write config\nif self.config.write:\n    data = self.context.get(\"clean_data\")  # Get from previous node\n    conn = self.connections[\"local\"]\n    full_path = conn.get_path(\"output.parquet\")  # \"./data/output.parquet\"\n\n    self.engine.write(\n        data=data,\n        path=full_path,\n        format=\"parquet\",\n        mode=\"overwrite\",\n        options={}\n    )\n    # Writes DataFrame to ./data/output.parquet\n</code></pre> <p>11. Story generation (if enabled):</p> <pre><code>story_path = self.story_generator.generate(\n    node_results={...},\n    completed=[\"load_data\", \"clean_data\", \"save_data\"],\n    failed=[],\n    ...\n)\n# Generates markdown story in ./stories/\n</code></pre> <p>12. Return results to user:</p> <pre><code>results = {\n    \"example\": PipelineResults(\n        pipeline_name=\"example\",\n        completed=[\"load_data\", \"clean_data\", \"save_data\"],\n        failed=[],\n        duration=2.3,\n        story_path=\"./stories/example_20251107_143025.md\"\n    )\n}\n</code></pre>"},{"location":"reference/configuration/#key-concepts-explained","title":"Key Concepts Explained","text":""},{"location":"reference/configuration/#1-config-vs-runtime","title":"1. Config vs Runtime","text":"<p>Config (Layer 1 + 2): - What you declare in YAML - Validated by Pydantic - Immutable once loaded - Example: <code>ReadConfig(connection=\"local\", path=\"input.csv\", format=\"csv\")</code></p> <p>Runtime (Layer 3): - What executes the work - Uses config to make decisions - Mutable state (context, results) - Example: <code>PandasEngine.read(path=\"./data/input.csv\", format=\"csv\")</code> \u2192 returns DataFrame</p> <p>Why separate? - Validation happens early (before execution) - Config is reusable (can run same config multiple times) - Easier testing (mock runtime, test config separately)</p>"},{"location":"reference/configuration/#2-connection-config-vs-object","title":"2. Connection: Config vs Object","text":"<p>Connection Config (YAML):</p> <pre><code>connections:\n  local:\n    type: local\n    base_path: ./data\n</code></pre> <p>Connection Object (Python):</p> <pre><code>connections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n</code></pre> <p>What's the difference? - Config is declarative (YAML dict) - Object is executable (Python class with methods like <code>.get_path()</code>)</p> <p>Why both? - YAML is portable (version controlled, shareable) - Objects are functional (can call methods, maintain state)</p>"},{"location":"reference/configuration/#3-pipeline-vs-pipelinemanager","title":"3. Pipeline vs PipelineManager","text":"<p>Pipeline: - Represents one pipeline - Has nodes, dependencies, execution logic - Example: <code>bronze_to_silver</code> pipeline</p> <p>PipelineManager: - Manages multiple pipelines - Loads YAML, creates connections, instantiates Pipelines - Provides unified API: <code>manager.run()</code> runs all, <code>manager.run('bronze_to_silver')</code> runs one</p> <p>Why <code>Pipeline.from_yaml()</code> returns PipelineManager? - Convenience: Most YAMLs have multiple pipelines - Backward compatible: Users can still call <code>Pipeline.from_yaml()</code> - Unified API: <code>manager.run()</code> works for 1 or 10 pipelines</p>"},{"location":"reference/configuration/#4-from_yaml-the-boilerplate-eliminator","title":"4. <code>from_yaml()</code> - The Boilerplate Eliminator","text":"<p>Before (manual setup):</p> <pre><code>import yaml\nfrom odibi.pipeline import Pipeline\nfrom odibi.config import PipelineConfig\nfrom odibi.connections import LocalConnection\n\nwith open(\"config.yaml\") as f:\n    config = yaml.safe_load(f)\n\npipeline_config = PipelineConfig(**config['pipelines'][0])\nconnections = {\n    'local': LocalConnection(base_path=config['connections']['local']['base_path'])\n}\npipeline = Pipeline(\n    pipeline_config=pipeline_config,\n    engine=\"pandas\",\n    connections=connections\n)\nresults = pipeline.run()\n</code></pre> <p>After (<code>from_yaml()</code>):</p> <pre><code>from odibi.pipeline import Pipeline\n\nmanager = Pipeline.from_yaml(\"config.yaml\")\nresults = manager.run()\n</code></pre> <p>What <code>from_yaml()</code> does: 1. Load YAML 2. Validate with Pydantic 3. Create connection objects 4. Instantiate PipelineManager 5. Return ready-to-run manager</p> <p>Result: 2 lines instead of 15!</p>"},{"location":"reference/configuration/#5-context-the-data-bus","title":"5. Context - The Data Bus","text":"<p>Purpose: Pass data between nodes without explicit function calls</p> <p>How it works:</p> <pre><code># Node 1: load_data\ndata = engine.read(...)\ncontext.register(\"load_data\", data)  # Store DataFrame\n\n# Node 2: clean_data (depends_on: [load_data])\ndata = context.get(\"load_data\")  # Retrieve DataFrame\ncleaned = engine.execute_sql(\"SELECT * FROM load_data WHERE ...\", context)\ncontext.register(\"clean_data\", cleaned)\n\n# Node 3: save_data (depends_on: [clean_data])\ndata = context.get(\"clean_data\")\nengine.write(data, ...)\n</code></pre> <p>Why not return values? - Nodes execute sequentially but independently - SQL transforms reference DataFrames by name (not variable) - Context provides unified API across Pandas and Spark</p>"},{"location":"reference/configuration/#common-confusion-points","title":"Common Confusion Points","text":""},{"location":"reference/configuration/#confusion-1-why-do-i-see-both-pipeline-and-name","title":"Confusion #1: \"Why do I see both <code>pipeline</code> and <code>name</code>?\"","text":"<p>Answer: Different levels of abstraction!</p> <pre><code>pipelines:                   # List of pipelines\n  - pipeline: bronze_to_silver  # \u2190 Pipeline NAME (identifies the pipeline)\n    nodes:                     # List of nodes in THIS pipeline\n      - name: load_data        # \u2190 Node NAME (identifies the node)\n</code></pre> <p>Analogy: - <code>pipeline</code> is like a book title (\"Harry Potter\") - <code>name</code> is like a chapter name (\"The Boy Who Lived\")</p> <p>In code: - <code>PipelineConfig.pipeline</code> \u2192 pipeline name - <code>NodeConfig.name</code> \u2192 node name</p>"},{"location":"reference/configuration/#confusion-2-whats-the-difference-between-connection-local-and-type-local","title":"Confusion #2: \"What's the difference between <code>connection: local</code> and <code>type: local</code>?\"","text":"<p>Answer: Different contexts!</p> <p>In <code>connections</code> section (defining connections):</p> <pre><code>connections:\n  local:           # \u2190 Connection NAME (you choose this)\n    type: local    # \u2190 Connection TYPE (system type: local, azure_adls, etc.)\n    base_path: ./data\n</code></pre> <p>In <code>read</code>/<code>write</code> section (using connections):</p> <pre><code>nodes:\n  - name: load_data\n    read:\n      connection: local  # \u2190 References the CONNECTION NAME from above\n      path: input.csv\n</code></pre> <p>Analogy: - <code>connections</code> section: \"Define a car named 'my_car' of type 'sedan'\" - <code>read.connection</code>: \"Use the car named 'my_car' to drive somewhere\"</p>"},{"location":"reference/configuration/#confusion-3-why-does-from_yaml-return-a-manager-instead-of-a-pipeline","title":"Confusion #3: \"Why does <code>from_yaml()</code> return a manager instead of a pipeline?\"","text":"<p>Answer: YAML files typically have multiple pipelines!</p> <pre><code>pipelines:\n  - pipeline: bronze_to_silver  # Pipeline 1\n    nodes: [...]\n  - pipeline: silver_to_gold    # Pipeline 2\n    nodes: [...]\n</code></pre> <p>If it returned a single Pipeline: - Which one? The first? All? - How to run specific pipelines?</p> <p>By returning PipelineManager: - Access all pipelines - Run all: <code>manager.run()</code> - Run one: <code>manager.run('bronze_to_silver')</code> - Run some: <code>manager.run(['bronze_to_silver', 'silver_to_gold'])</code></p> <p>For single pipeline YAMLs:</p> <pre><code>result = manager.run()  # If only 1 pipeline, returns PipelineResults (not dict)\n</code></pre>"},{"location":"reference/configuration/#confusion-4-whats-the-difference-between-options-and-params","title":"Confusion #4: \"What's the difference between <code>options</code> and <code>params</code>?\"","text":"<p>Answer: Different operation types!</p> <p><code>options</code> (in read/write):</p> <pre><code>read:\n  connection: local\n  path: data.csv\n  format: csv\n  options:           # \u2190 Format-specific options (passed to pandas.read_csv())\n    header: 0\n    dtype:\n      id: str\n</code></pre> <p>Maps to: <code>pandas.read_csv(path, header=0, dtype={\"id\": str})</code></p> <p><code>params</code> (in transform):</p> <pre><code>transform:\n  steps:\n    - function: my_custom_function\n      params:        # \u2190 Function arguments\n        threshold: 0.5\n        mode: strict\n</code></pre> <p>Maps to: <code>my_custom_function(context, threshold=0.5, mode='strict')</code></p> <p>Key difference: - <code>options</code> \u2192 passed to engine (Pandas/Spark I/O functions) - <code>params</code> \u2192 passed to your function</p>"},{"location":"reference/configuration/#confusion-5-where-do-stories-get-written","title":"Confusion #5: \"Where do stories get written?\"","text":"<p>Answer: Stories use the connection pattern, just like data!</p> <p>Story configuration (required):</p> <pre><code>connections:\n  outputs:\n    type: local\n    base_path: ./outputs\n\nstory:\n  connection: outputs  # \u2190 References connection name\n  path: stories/       # \u2190 Path within connection\n  enabled: true\n</code></pre> <p>Resolved path: <code>./outputs/stories/pipeline_name_20251107_143025.md</code></p> <p>Why this pattern? - Explicit: Clear where stories are written (no hidden defaults) - Traceable: Connection-based paths preserve truth - Consistent: Same pattern as <code>read.connection</code> and <code>write.connection</code> - Flexible: Stories can go to ADLS, DBFS, or local storage</p> <p>Before v1.1 (confusing):</p> <pre><code># Story path was implicit - where is \"stories/\" relative to?\nconnections:\n  local:\n    type: local\n    base_path: ./data\n</code></pre> <p>After v1.1 (explicit):</p> <pre><code>connections:\n  outputs:\n    type: local\n    base_path: ./outputs\n\nstory:\n  connection: outputs  # Required - must exist in connections\n  path: stories/\n</code></pre>"},{"location":"reference/configuration/#confusion-6-how-does-sql-find-the-dataframes","title":"Confusion #6: \"How does SQL find the DataFrames?\"","text":"<p>Answer: The engine looks them up in the context!</p> <p>Your SQL:</p> <pre><code>SELECT * FROM load_data WHERE amount &gt; 0\n</code></pre> <p>What the engine does (simplified):</p> <pre><code># PandasEngine.execute_sql()\ndef execute_sql(self, sql: str, context: Context):\n    # 1. Find all table references in SQL\n    tables = extract_table_names(sql)  # [\"load_data\"]\n\n    # 2. Get DataFrames from context\n    load_data = context.get(\"load_data\")  # The DataFrame from earlier node\n\n    # 3. Execute SQL using pandasql or duckdb\n    result = duckdb.query(sql).to_df()\n\n    return result\n</code></pre> <p>Key insight: Table names in SQL must match node names in the pipeline!</p>"},{"location":"reference/configuration/#decision-trees","title":"Decision Trees","text":""},{"location":"reference/configuration/#which-class-do-i-use","title":"\"Which class do I use?\"","text":"<pre><code>\u250c\u2500 Need to load and run a YAML file?\n\u2502  \u251c\u2500 YES \u2192 Use `Pipeline.from_yaml(\"config.yaml\")`\n\u2502  \u2502         Returns PipelineManager\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Are you building custom integrations?\n\u2502     \u251c\u2500 YES \u2192 Use `PipelineManager(...)` or `Pipeline(...)` directly\n\u2502     \u2514\u2500 NO \u2192 Use `Pipeline.from_yaml()` (recommended)\n</code></pre>"},{"location":"reference/configuration/#how-do-i-run-my-pipelines","title":"\"How do I run my pipelines?\"","text":"<pre><code>\u250c\u2500 How many pipelines in YAML?\n\u2502  \u251c\u2500 ONE \u2192 `manager.run()` returns PipelineResults\n\u2502  \u251c\u2500 MANY \u2192 `manager.run()` returns Dict[name -&gt; PipelineResults]\n\u2502  \u2502\n\u2502  \u2514\u2500 Want to run specific pipeline(s)?\n\u2502     \u251c\u2500 ONE \u2192 `manager.run('pipeline_name')` returns PipelineResults\n\u2502     \u2514\u2500 MULTIPLE \u2192 `manager.run(['pipe1', 'pipe2'])` returns Dict\n</code></pre>"},{"location":"reference/configuration/#where-does-my-configuration-live","title":"\"Where does my configuration live?\"","text":"<pre><code>\u250c\u2500 Is it about the OVERALL project?\n\u2502  \u251c\u2500 YES \u2192 Top level (project, engine, connections, story)\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Is it about a PIPELINE?\n\u2502     \u251c\u2500 YES \u2192 Under `pipelines:` (pipeline, layer, nodes)\n\u2502     \u2502\n\u2502     \u2514\u2500 NO \u2192 Is it about a NODE?\n\u2502        \u251c\u2500 YES \u2192 Under `nodes:` (name, read, transform, write)\n\u2502        \u2502\n\u2502        \u2514\u2500 NO \u2192 Is it about an OPERATION?\n\u2502           \u251c\u2500 READ \u2192 Under `read:` (connection, path, format, options)\n\u2502           \u251c\u2500 TRANSFORM \u2192 Under `transform:` (steps)\n\u2502           \u2514\u2500 WRITE \u2192 Under `write:` (connection, path, format, mode, options)\n</code></pre>"},{"location":"reference/configuration/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/configuration/#yaml-structure","title":"YAML Structure","text":"<pre><code># PROJECT LEVEL (required)\nproject: string               # Project name\nengine: pandas|spark|polars   # Execution engine\n\n# GLOBAL SETTINGS (optional)\nretry:\n  enabled: bool\n  max_attempts: int\n  backoff: exponential|linear|constant\nlogging:\n  level: DEBUG|INFO|WARNING|ERROR\n  structured: bool\n  metadata: dict\n\n# CONNECTIONS (required, at least one)\nconnections:\n  &lt;connection_name&gt;:          # Your choice of name\n    type: local|azure_blob|delta|sql_server|http\n    validation_mode: lazy|eager   # optional, defaults to 'lazy'\n    &lt;type-specific-config&gt;\n\n# ENVIRONMENTS (optional)\nenvironments:\n  &lt;env_name&gt;:\n    &lt;overrides&gt;: ...\n    # Or use external file: env.&lt;env_name&gt;.yaml\n\n# STORY (required)\nstory:\n  connection: string        # Name of connection to write stories\n  path: string              # Relative path under that connection\n  auto_generate: bool\n  max_sample_rows: int\n  retention_days: int (optional)\n  retention_count: int (optional)\n\n# PIPELINES (required, at least one)\npipelines:\n  - pipeline: string          # Pipeline name\n    layer: string (optional)\n    description: string (optional)\n    nodes:                    # At least one node\n      - name: string          # Unique node name\n        depends_on: [string]  # List of node names (optional)\n        cache: bool (optional)\n\n        # At least ONE of these:\n        read:\n          connection: string  # Connection name\n          path: string        # Relative to connection base_path (Required unless 'query' used)\n          table: string       # Table name (alternative to path)\n          format: csv|parquet|json|excel|avro|sql_server\n          options: dict       # Format-specific (optional)\n            query: string     # SQL query (substitutes for path/table in sql_server)\n\n        transform:\n          steps:              # List of SQL strings or function calls\n            - string (SQL)\n            - function: string\n              params: dict\n\n        write:\n          connection: string\n          path: string\n          table: string       # Table name (alternative to path)\n          register_table: string # Register file output as external table (Spark/Delta only)\n          format: csv|parquet|json|excel|avro|delta\n          mode: overwrite|append\n          options: dict (optional)\n</code></pre>"},{"location":"reference/configuration/#python-api-quick-reference","title":"Python API Quick Reference","text":"<pre><code># === RECOMMENDED: Simple Usage ===\nfrom odibi.pipeline import Pipeline\n\n# Load and run all pipelines\nmanager = Pipeline.from_yaml(\"examples/templates/template_full.yaml\")\nresults = manager.run()  # Dict[name -&gt; PipelineResults] or single PipelineResults\n\n# Run specific pipeline\nresult = manager.run('bronze_to_silver')\n\n# List available pipelines\nprint(manager.list_pipelines())  # ['bronze_to_silver', 'silver_to_gold']\n\n# === ADVANCED: Direct PipelineManager ===\nfrom odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\nresults = manager.run()\n\n# Access specific pipeline\npipeline = manager.get_pipeline('bronze_to_silver')\nresult = pipeline.run()\n\n# === ADVANCED: Manual Construction ===\nfrom odibi.pipeline import Pipeline\nfrom odibi.config import PipelineConfig\nfrom odibi.connections import LocalConnection\n\npipeline_config = PipelineConfig(\n    pipeline=\"my_pipeline\",\n    nodes=[...]\n)\nconnections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\npipeline = Pipeline(\n    pipeline_config=pipeline_config,\n    engine=\"pandas\",\n    connections=connections\n)\nresult = pipeline.run()\n</code></pre>"},{"location":"reference/configuration/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: Single pipeline in YAML</p> <pre><code>manager = Pipeline.from_yaml(\"simple.yaml\")\nresult = manager.run()  # Returns PipelineResults (not dict)\nprint(f\"Completed: {result.completed}\")\n</code></pre> <p>Pattern 2: Multiple pipelines, run all</p> <pre><code>manager = Pipeline.from_yaml(\"multi.yaml\")\nresults = manager.run()  # Returns Dict[name -&gt; PipelineResults]\nfor name, result in results.items():\n    print(f\"{name}: {len(result.completed)} nodes\")\n</code></pre> <p>Pattern 3: Multiple pipelines, run specific</p> <pre><code>manager = Pipeline.from_yaml(\"multi.yaml\")\nresult = manager.run('bronze_to_silver')  # Returns PipelineResults\nprint(result.to_dict())\n</code></pre>"},{"location":"reference/configuration/#summary","title":"Summary","text":"<p>The Three Layers: 1. YAML (Layer 1): What you write (declarative) 2. Pydantic Models (Layer 2): Validation (automatic) 3. Runtime Classes (Layer 3): Execution (automatic)</p> <p>The Flow:</p> <pre><code>YAML file\n  \u2192 yaml.safe_load()\n  \u2192 Pydantic validation\n  \u2192 PipelineManager/Pipeline creation\n  \u2192 manager.run()\n  \u2192 Node execution\n  \u2192 Results\n</code></pre> <p>Key Takeaways: - <code>Pipeline.from_yaml()</code> returns <code>PipelineManager</code> (not <code>Pipeline</code>) - <code>manager.run()</code> runs all pipelines (or specific ones by name) - Configs are validated before execution (fail fast) - Context passes data between nodes (SQL references node names) - Connections are defined once, referenced many times</p> <p>Questions? Confusion? Open an issue on GitHub or check: - examples/templates/template_full.yaml - Complete YAML reference - docs/tutorials/walkthroughs/01_local_pipeline_pandas.ipynb - Interactive tutorial</p> <p>This document evolves with the framework. Last updated: 2025-11-20</p>"},{"location":"reference/supported_formats/","title":"Supported File Formats","text":"<p>ODIBI's PandasEngine supports multiple file formats with both local and cloud storage (ADLS, S3, etc.).</p>"},{"location":"reference/supported_formats/#format-support-matrix","title":"Format Support Matrix","text":"Format Read Write ADLS S3 Local Dependencies CSV \u2705 \u2705 \u2705 \u2705 \u2705 pandas (built-in) Parquet \u2705 \u2705 \u2705 \u2705 \u2705 pyarrow or fastparquet JSON \u2705 \u2705 \u2705 \u2705 \u2705 pandas (built-in) Excel \u2705 \u2705 \u2705 \u2705 \u2705 openpyxl Avro \u2705 \u2705 \u2705 \u2705 \u2705 fastavro"},{"location":"reference/supported_formats/#format-details","title":"Format Details","text":""},{"location":"reference/supported_formats/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>Use Case: Simple tabular data, human-readable format</p> <p>Read Example:</p> <pre><code>- name: load_csv\n  read:\n    connection: bronze\n    path: data/sales.csv\n    format: csv\n    options:\n      sep: \",\"\n      header: true\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_csv\n  write:\n    connection: bronze\n    path: output/results.csv\n    format: csv\n    mode: overwrite\n</code></pre>"},{"location":"reference/supported_formats/#parquet-apache-parquet","title":"Parquet (Apache Parquet)","text":"<p>Use Case: Data lake storage, recommended for production</p> <p>Benefits: - Columnar format (efficient for analytics) - Built-in compression - Type preservation - Fast reads for column-based queries</p> <p>Read Example:</p> <pre><code>- name: load_parquet\n  read:\n    connection: bronze\n    path: data/sales.parquet\n    format: parquet\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_parquet\n  write:\n    connection: silver\n    path: output/sales.parquet\n    format: parquet\n    options:\n      compression: snappy  # or gzip, brotli\n</code></pre>"},{"location":"reference/supported_formats/#json-json-lines","title":"JSON (JSON Lines)","text":"<p>Use Case: Semi-structured data, nested objects</p> <p>Format: JSON lines (newline-delimited JSON objects)</p> <p>Read Example:</p> <pre><code>- name: load_json\n  read:\n    connection: bronze\n    path: data/events.json\n    format: json\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_json\n  write:\n    connection: bronze\n    path: output/events.json\n    format: json\n    options:\n      orient: records\n</code></pre>"},{"location":"reference/supported_formats/#excel-microsoft-excel","title":"Excel (Microsoft Excel)","text":"<p>Use Case: Business reports, spreadsheets</p> <p>Supported: <code>.xlsx</code> files</p> <p>Dependencies: Requires <code>openpyxl</code></p> <pre><code>pip install openpyxl\n</code></pre> <p>Read Example:</p> <pre><code>- name: load_excel\n  read:\n    connection: bronze\n    path: reports/sales.xlsx\n    format: excel\n    options:\n      sheet_name: \"Sheet1\"\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_excel\n  write:\n    connection: bronze\n    path: output/report.xlsx\n    format: excel\n</code></pre>"},{"location":"reference/supported_formats/#avro-apache-avro","title":"Avro (Apache Avro)","text":"<p>Use Case: Event streaming, schema evolution</p> <p>Benefits: - Binary format (compact) - Schema included in file - Supports schema evolution - Efficient for serialization</p> <p>Dependencies: Requires <code>fastavro</code></p> <pre><code>pip install fastavro\n</code></pre> <p>Read Example:</p> <pre><code>- name: load_avro\n  read:\n    connection: bronze\n    path: events/stream.avro\n    format: avro\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_avro\n  write:\n    connection: bronze\n    path: output/events.avro\n    format: avro\n</code></pre> <p>Note: Avro schema is automatically inferred from DataFrame dtypes.</p>"},{"location":"reference/supported_formats/#cloud-storage-support","title":"Cloud Storage Support","text":"<p>All formats work seamlessly with cloud storage:</p> <p>ADLS (Azure Data Lake Storage):</p> <pre><code>connections:\n  bronze:\n    type: azure_adls\n    account: mystorageaccount\n    container: bronze\n    auth_mode: key_vault\n    key_vault_name: my-vault\n    secret_name: storage-key\n\npipelines:\n  - pipeline: multi_format\n    nodes:\n      - name: read_csv_from_adls\n        read:\n          connection: bronze\n          path: data/sales.csv\n          format: csv\n\n      - name: write_avro_to_adls\n        depends_on: [read_csv_from_adls]\n        write:\n          connection: bronze\n          path: output/sales.avro\n          format: avro\n</code></pre> <p>All formats support: - \u2705 Multi-account connections - \u2705 Key Vault authentication - \u2705 Storage options pass-through - \u2705 Remote URI handling (<code>abfss://</code>, <code>s3://</code>)</p>"},{"location":"reference/supported_formats/#best-practices","title":"Best Practices","text":""},{"location":"reference/supported_formats/#for-data-lakes-recommended","title":"For Data Lakes (Recommended)","text":"<ol> <li>Use Parquet for production data</li> <li>Efficient storage</li> <li>Fast analytics</li> <li> <p>Type preservation</p> </li> <li> <p>Use CSV for human-readable data</p> </li> <li>Easy to inspect</li> <li>Compatible with all tools</li> <li> <p>Good for small datasets</p> </li> <li> <p>Use Avro for event streams</p> </li> <li>Schema evolution support</li> <li>Compact binary format</li> <li>Good for append-only logs</li> </ol>"},{"location":"reference/supported_formats/#performance-tips","title":"Performance Tips","text":"<p>Parquet: - Use <code>snappy</code> compression (good balance of speed/size) - Enable column pruning (read only needed columns) - Consider partitioning for large datasets (Phase 2B)</p> <p>CSV: - Use chunking for large files - Specify dtypes explicitly to avoid inference</p> <p>Avro: - Best for write-once, read-many workloads - Schema is embedded (no separate schema files needed)</p>"},{"location":"reference/supported_formats/#delta-lake-databricks-open-source","title":"Delta Lake (Databricks / Open Source)","text":"<p>Use Case: ACID transactions, time travel, data lakehouse</p> <p>Benefits: - ACID Transactions: No partial writes or corruption - Time Travel: Query previous versions of data - Schema Evolution: Safely evolve schema over time - Audit History: Track all changes to the table</p> <p>Dependencies: Requires <code>delta-spark</code> (for Spark engine) or <code>deltalake</code> (for Pandas engine).</p> <p>Read Example:</p> <pre><code>- name: load_delta\n  read:\n    connection: bronze\n    path: data/sales.delta\n    format: delta\n    options:\n      version_as_of: 5  # Time travel!\n</code></pre> <p>Write Example:</p> <pre><code>- name: save_delta\n  write:\n    connection: silver\n    path: output/sales.delta\n    format: delta\n    mode: append  # or overwrite\n</code></pre> <p>See the Delta Lake Guide for advanced features like VACUUM and Restore.</p>"},{"location":"reference/yaml_schema/","title":"Odibi Configuration Reference","text":"<p>This manual details the YAML configuration schema for Odibi projects. Auto-generated from Pydantic models.</p>"},{"location":"reference/yaml_schema/#project-structure","title":"Project Structure","text":""},{"location":"reference/yaml_schema/#projectconfig","title":"<code>ProjectConfig</code>","text":"<p>Complete project configuration from YAML.</p>"},{"location":"reference/yaml_schema/#enterprise-setup-guide","title":"\ud83c\udfe2 \"Enterprise Setup\" Guide","text":"<p>Business Problem: \"We need a robust production environment with alerts, retries, and proper logging.\"</p> <p>Recipe: Production Ready</p> <pre><code>project: \"Customer360\"\nengine: \"spark\"\n\n# 1. Resilience\nretry:\n    enabled: true\n    max_attempts: 3\n    backoff: \"exponential\"\n\n# 2. Observability\nlogging:\n    level: \"INFO\"\n    structured: true  # JSON logs for Splunk/Datadog\n\n# 3. Alerting\nalerts:\n    - type: \"slack\"\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events: [\"on_failure\"]\n\n# ... connections and pipelines ...\n</code></pre> Field Type Required Default Description project str Yes - Project name engine EngineType No <code>EngineType.PANDAS</code> Execution engine connections Dict[str, LocalConnectionConfig | AzureBlobConnectionConfig | DeltaConnectionConfig | SQLServerConnectionConfig | HttpConnectionConfig | CustomConnectionConfig] Yes - Named connections (at least one required)Options: LocalConnectionConfig, AzureBlobConnectionConfig, DeltaConnectionConfig, SQLServerConnectionConfig, HttpConnectionConfig pipelines List[PipelineConfig] Yes - Pipeline definitions (at least one required) story StoryConfig Yes - Story generation configuration (mandatory) system SystemConfig Yes - System Catalog configuration (mandatory) lineage Optional[LineageConfig] No - OpenLineage configuration description Optional[str] No - Project description version str No <code>1.0.0</code> Project version owner Optional[str] No - Project owner/contact vars Dict[str, Any] No <code>PydanticUndefined</code> Global variables for substitution (e.g. ${vars.env}) retry RetryConfig No <code>PydanticUndefined</code> - logging LoggingConfig No <code>PydanticUndefined</code> - alerts List[AlertConfig] No <code>PydanticUndefined</code> Alert configurations performance PerformanceConfig No <code>PydanticUndefined</code> Performance tuning environments Optional[Dict[str, Dict[str, Any]]] No - Structure: same as ProjectConfig but with only overridden fields. Not yet validated strictly. semantic Optional[Dict[str, Any]] No - Semantic layer configuration. Can be inline or reference external file. Contains metrics, dimensions, and materializations for self-service analytics. Example: semantic: { config: 'semantic_config.yaml' } or inline definitions."},{"location":"reference/yaml_schema/#pipelineconfig","title":"<code>PipelineConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for a pipeline.</p> <p>Example:</p> <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"node1\"\n        ...\n</code></pre> Field Type Required Default Description pipeline str Yes - Pipeline name description Optional[str] No - Pipeline description layer Optional[str] No - Logical layer (bronze/silver/gold) nodes List[NodeConfig] Yes - List of nodes in this pipeline"},{"location":"reference/yaml_schema/#nodeconfig","title":"<code>NodeConfig</code>","text":"<p>Used in: PipelineConfig</p> <p>Configuration for a single node.</p>"},{"location":"reference/yaml_schema/#the-smart-node-pattern","title":"\ud83e\udde0 \"The Smart Node\" Pattern","text":"<p>Business Problem: \"We need complex dependencies, caching for heavy computations, and the ability to run only specific parts of the pipeline.\"</p> <p>The Solution: Nodes are the building blocks. They handle dependencies (<code>depends_on</code>), execution control (<code>tags</code>, <code>enabled</code>), and performance (<code>cache</code>).</p>"},{"location":"reference/yaml_schema/#dag-dependencies","title":"\ud83d\udd78\ufe0f DAG &amp; Dependencies","text":"<p>The Glue of the Pipeline. Nodes don't run in isolation. They form a Directed Acyclic Graph (DAG).</p> <ul> <li><code>depends_on</code>: Critical! If Node B reads from Node A (in memory), you MUST list <code>[\"Node A\"]</code>.<ul> <li>Implicit Data Flow: If a node has no <code>read</code> block, it automatically picks up the DataFrame from its first dependency.</li> </ul> </li> </ul>"},{"location":"reference/yaml_schema/#smart-read-incremental-loading","title":"\ud83e\udde0 Smart Read &amp; Incremental Loading","text":"<p>Automated History Management.</p> <p>Odibi intelligently determines whether to perform a Full Load or an Incremental Load based on the state of the target.</p> <p>The \"Smart Read\" Logic: 1.  First Run (Full Load): If the target table (defined in <code>write</code>) does not exist:     *   Incremental filtering rules are ignored.     *   The entire source dataset is read.     *   Use <code>write.first_run_query</code> (optional) to override the read query for this initial bootstrap (e.g., to backfill only 1 year of history instead of all time).</p> <ol> <li>Subsequent Runs (Incremental Load): If the target table exists:<ul> <li>Rolling Window: Filters source data where <code>column &gt;= NOW() - lookback</code>.</li> <li>Stateful: Filters source data where <code>column &gt; last_high_water_mark</code>.</li> </ul> </li> </ol> <p>This ensures you don't need separate \"init\" and \"update\" pipelines. One config handles both lifecycle states.</p>"},{"location":"reference/yaml_schema/#orchestration-tags","title":"\ud83c\udff7\ufe0f Orchestration Tags","text":"<p>Run What You Need. Tags allow you to execute slices of your pipeline. *   <code>odibi run --tag daily</code> -&gt; Runs all nodes with \"daily\" tag. *   <code>odibi run --tag critical</code> -&gt; Runs high-priority nodes.</p>"},{"location":"reference/yaml_schema/#choosing-your-logic-transformer-vs-transform","title":"\ud83e\udd16 Choosing Your Logic: Transformer vs. Transform","text":"<p>1. The \"Transformer\" (Top-Level) *   What it is: A pre-packaged, heavy-duty operation that defines the entire purpose of the node. *   When to use: When applying a standard Data Engineering pattern (e.g., SCD2, Merge, Deduplicate). *   Analogy: \"Run this App.\" *   Syntax: <code>transformer: \"scd2\"</code> + <code>params: {...}</code></p> <p>2. The \"Transform Steps\" (Process Chain) *   What it is: A sequence of smaller steps (SQL, functions, operations) executed in order. *   When to use: For custom business logic, data cleaning, or feature engineering pipelines. *   Analogy: \"Run this Script.\" *   Syntax: <code>transform: { steps: [...] }</code></p> <p>Note: You can use both! The <code>transformer</code> runs first, then <code>transform</code> steps refine the result.</p>"},{"location":"reference/yaml_schema/#chaining-operations","title":"\ud83d\udd17 Chaining Operations","text":"<p>You can mix and match! The execution order is always: 1.  Read (or Dependency Injection) 2.  Transformer (The \"App\" logic, e.g., Deduplicate) 3.  Transform Steps (The \"Script\" logic, e.g., cleanup) 4.  Validation 5.  Write</p> <p>Constraint: You must define at least one of <code>read</code>, <code>transformer</code>, <code>transform</code>, or <code>write</code>.</p>"},{"location":"reference/yaml_schema/#example-app-vs-script","title":"\u26a1 Example: App vs. Script","text":"<p>Scenario 1: The Full ETL Flow (Chained) Shows explicit Read, Transform Chain, and Write.</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]\n\n  # \"clean_text\" is a registered function from the Transformer Catalog\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Top-Level Transformer) Shows a node that applies a pattern (Deduplicate) to incoming data.</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication (From Transformer Catalog)\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner (Reporting) Shows how tags allow running specific slices (e.g., <code>odibi run --tag daily</code>).</p> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  depends_on: [\"deduped_users\"]\n\n  # Ad-hoc aggregation script\n  transform:\n    steps:\n      - sql: \"SELECT date_trunc('day', updated_at) as day, count(*) as total FROM df GROUP BY 1\"\n\n  write: { connection: \"local_data\", format: \"csv\", path: \"reports/daily_stats.csv\" }\n</code></pre> <p>Scenario 4: The \"Kitchen Sink\" (All Operations) Shows Read -&gt; Transformer -&gt; Transform -&gt; Write execution order.</p> <p>Why this works: 1.  Internal Chaining (<code>df</code>): In every step (Transformer or SQL), <code>df</code> refers to the output of the previous step. 2.  External Access (<code>depends_on</code>): If you added <code>depends_on: [\"other_node\"]</code>, you could also run <code>SELECT * FROM other_node</code> in your SQL steps!</p> <pre><code>- name: \"complex_flow\"\n  # 1. Read -&gt; Creates initial 'df'\n  read: { connection: \"bronze\", format: \"parquet\", path: \"users\" }\n\n  # 2. Transformer (The \"App\": Deduplicate first)\n  # Takes 'df' (from Read), dedups it, returns new 'df'\n  transformer: \"deduplicate\"\n  params: { keys: [\"user_id\"], order_by: \"updated_at DESC\" }\n\n  # 3. Transform Steps (The \"Script\": Filter AFTER deduplication)\n  # SQL sees the deduped data as 'df'\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n  # 4. Write -&gt; Saves the final filtered 'df'\n  write: { connection: \"silver\", format: \"delta\", table: \"active_unique_users\" }\n</code></pre>"},{"location":"reference/yaml_schema/#transformer-catalog","title":"\ud83d\udcda Transformer Catalog","text":"<p>These are the built-in functions you can use in two ways:</p> <ol> <li>As a Top-Level Transformer: <code>transformer: \"name\"</code> (Defines the node's main logic)</li> <li>As a Step in a Chain: <code>transform: { steps: [{ function: \"name\" }] }</code> (Part of a sequence)</li> </ol> <p>Note: <code>merge</code> and <code>scd2</code> are special \"Heavy Lifters\" and should generally be used as Top-Level Transformers.</p> <p>Data Engineering Patterns *   <code>merge</code>: Upsert/Merge into target (Delta/SQL). (Params) *   <code>scd2</code>: Slowly Changing Dimensions Type 2. (Params) *   <code>deduplicate</code>: Remove duplicates using window functions. (Params)</p> <p>Relational Algebra *   <code>join</code>: Join two datasets. (Params) *   <code>union</code>: Stack datasets vertically. (Params) *   <code>pivot</code>: Rotate rows to columns. (Params) *   <code>unpivot</code>: Rotate columns to rows (melt). (Params) *   <code>aggregate</code>: Group by and sum/count/avg. (Params)</p> <p>Data Quality &amp; Cleaning *   <code>validate_and_flag</code>: Check rules and flag invalid rows. (Params) *   <code>clean_text</code>: Trim and normalize case. (Params) *   <code>filter_rows</code>: SQL-based filtering. (Params) *   <code>fill_nulls</code>: Replace NULLs with defaults. (Params)</p> <p>Feature Engineering *   <code>derive_columns</code>: Create new cols via SQL expressions. (Params) *   <code>case_when</code>: Conditional logic (if-else). (Params) *   <code>generate_surrogate_key</code>: Create MD5 keys from columns. (Params) *   <code>date_diff</code>, <code>date_add</code>, <code>date_trunc</code>: Date arithmetic.</p> <p>Scenario 1: The Full ETL Flow (Show two nodes: one loader, one processor)</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]  # &lt;--- Explicit dependency\n\n  # Explicit Transformation Steps\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Transformer) (Show a node that is a Transformer, no read needed if it picks up from dependency)</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner Run only this with <code>odibi run --tag daily</code></p> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  # ...\n</code></pre> <p>Scenario 4: Pre/Post SQL Hooks Setup and cleanup with SQL statements.</p> <pre><code>- name: \"optimize_sales\"\n  depends_on: [\"load_sales\"]\n  pre_sql:\n    - \"SET spark.sql.shuffle.partitions = 200\"\n    - \"CREATE TEMP VIEW staging AS SELECT * FROM bronze.raw_sales\"\n  transform:\n    steps:\n      - sql: \"SELECT * FROM staging WHERE amount &gt; 0\"\n  post_sql:\n    - \"OPTIMIZE gold.fact_sales ZORDER BY (customer_id)\"\n    - \"VACUUM gold.fact_sales RETAIN 168 HOURS\"\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sales\"\n</code></pre> <p>Scenario 5: Materialization Strategies Choose how output is persisted.</p> <pre><code># Option 1: View (no physical storage, logical model)\n- name: \"vw_active_customers\"\n  materialized: \"view\"  # Creates SQL view instead of table\n  transform:\n    steps:\n      - sql: \"SELECT * FROM customers WHERE status = 'active'\"\n  write:\n    connection: \"gold\"\n    table: \"vw_active_customers\"\n\n# Option 2: Incremental (append to existing Delta table)\n- name: \"fact_events\"\n  materialized: \"incremental\"  # Uses APPEND mode\n  read:\n    connection: \"bronze\"\n    table: \"raw_events\"\n    incremental:\n      mode: \"stateful\"\n      column: \"event_time\"\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"fact_events\"\n\n# Option 3: Table (default - full overwrite)\n- name: \"dim_products\"\n  materialized: \"table\"  # Default behavior\n  # ...\n</code></pre> Field Type Required Default Description name str Yes - Unique node name description Optional[str] No - Human-readable description enabled bool No <code>True</code> If False, node is skipped during execution tags List[str] No <code>PydanticUndefined</code> Operational tags for selective execution (e.g., 'daily', 'critical'). Use with <code>odibi run --tag</code>. depends_on List[str] No <code>PydanticUndefined</code> List of parent nodes that must complete before this node runs. The output of these nodes is available for reading. columns Dict[str, ColumnMetadata] No <code>PydanticUndefined</code> Data Dictionary defining the output schema. Used for documentation, PII tagging, and validation. read Optional[ReadConfig] No - Input operation (Load). If missing, data is taken from the first dependency. inputs Optional[Dict[str, str | Dict[str, Any]]] No - Multi-input support for cross-pipeline dependencies. Map input names to either: (a) $pipeline.node reference (e.g., '$read_bronze.shift_events') (b) Explicit read config dict. Cannot be used with 'read'. Example: inputs: {events: '$read_bronze.events', calendar: {connection: 'goat', path: 'cal'}} transform Optional[TransformConfig] No - Chain of fine-grained transformation steps (SQL, functions). Runs after 'transformer' if both are present. write Optional[WriteConfig] No - Output operation (Save to file/table). streaming bool No <code>False</code> Enable streaming execution for this node (Spark only) transformer Optional[str] No - Name of the 'App' logic to run (e.g., 'deduplicate', 'scd2'). See Transformer Catalog for options. params Dict[str, Any] No <code>PydanticUndefined</code> Parameters for transformer pre_sql List[str] No <code>PydanticUndefined</code> List of SQL statements to execute before node runs. Use for setup: temp tables, variable initialization, grants. Example: ['SET spark.sql.shuffle.partitions=200', 'CREATE TEMP VIEW src AS SELECT * FROM raw'] post_sql List[str] No <code>PydanticUndefined</code> List of SQL statements to execute after node completes. Use for cleanup, optimization, or audit logging. Example: ['OPTIMIZE gold.fact_sales', 'VACUUM gold.fact_sales RETAIN 168 HOURS'] materialized Optional[Literal['table', 'view', 'incremental']] No - Materialization strategy. Options: 'table' (default physical write), 'view' (creates SQL view instead of table), 'incremental' (uses append mode for Delta tables). Views are useful for Gold layer logical models. cache bool No <code>False</code> Cache result for reuse log_level Optional[LogLevel] No - Override log level for this node on_error ErrorStrategy No <code>ErrorStrategy.FAIL_LATER</code> Failure handling strategy validation Optional[ValidationConfig] No - - contracts List[TestConfig] No <code>PydanticUndefined</code> Pre-condition contracts (Circuit Breakers). Runs on input data before transformation.Options: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract schema_policy Optional[SchemaPolicyConfig] No - Schema drift handling policy privacy Optional[PrivacyConfig] No - Privacy Suite: PII anonymization settings sensitive bool | List[str] No <code>False</code> If true or list of columns, masks sample data in stories"},{"location":"reference/yaml_schema/#columnmetadata","title":"<code>ColumnMetadata</code>","text":"<p>Used in: NodeConfig</p> <p>Metadata for a column in the data dictionary. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | description | Optional[str] | No | - | Column description | | pii | bool | No | <code>False</code> | Contains PII? | | tags | List[str] | No | <code>PydanticUndefined</code> | Tags (e.g. 'business_key', 'measure') |</p>"},{"location":"reference/yaml_schema/#systemconfig","title":"<code>SystemConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for the Odibi System Catalog (The Brain).</p> <p>Stores metadata, state, and pattern configurations. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection to store system tables (e.g., 'adls_bronze') | | path | str | No | <code>_odibi_system</code> | Path relative to connection root |</p>"},{"location":"reference/yaml_schema/#connections","title":"Connections","text":""},{"location":"reference/yaml_schema/#localconnectionconfig","title":"<code>LocalConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Local filesystem connection.</p> <p>When to Use: Development, testing, small datasets, local processing.</p> <p>See Also: AzureBlobConnectionConfig for cloud alternatives.</p> <p>Example:</p> <pre><code>local_data:\n  type: \"local\"\n  base_path: \"./data\"\n</code></pre> Field Type Required Default Description type Literal['local'] No <code>ConnectionType.LOCAL</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - base_path str No <code>./data</code> Base directory path"},{"location":"reference/yaml_schema/#deltaconnectionconfig","title":"<code>DeltaConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Delta Lake connection for ACID-compliant data lakes.</p> <p>When to Use: - Production data lakes on Azure/AWS/GCP - Need time travel, ACID transactions, schema evolution - Upsert/merge operations</p> <p>See Also: WriteConfig for Delta write options</p> <p>Scenario 1: Delta via metastore</p> <pre><code>delta_silver:\n  type: \"delta\"\n  catalog: \"spark_catalog\"\n  schema: \"silver_db\"\n</code></pre> <p>Scenario 2: Direct path + Node usage</p> <pre><code>delta_local:\n  type: \"local\"\n  base_path: \"dbfs:/mnt/delta\"\n\n# In pipeline:\n# read:\n#   connection: \"delta_local\"\n#   format: \"delta\"\n#   path: \"bronze/orders\"\n</code></pre> Field Type Required Default Description type Literal['delta'] No <code>ConnectionType.DELTA</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - catalog str Yes - Spark catalog name (e.g. 'spark_catalog') schema_name str Yes - Database/schema name table Optional[str] No - Optional default table name for this connection (used by story/pipeline helpers)"},{"location":"reference/yaml_schema/#azureblobconnectionconfig","title":"<code>AzureBlobConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Azure Blob Storage / ADLS Gen2 connection.</p> <p>When to Use: Azure-based data lakes, landing zones, raw data storage.</p> <p>See Also: DeltaConnectionConfig for Delta-specific options</p> <p>Scenario 1: Prod with Key Vault-managed key</p> <pre><code>adls_bronze:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"key_vault\"\n    key_vault: \"kv-data\"\n    secret: \"adls-account-key\"\n</code></pre> <p>Scenario 2: Local dev with inline account key</p> <pre><code>adls_dev:\n  type: \"azure_blob\"\n  account_name: \"devaccount\"\n  container: \"sandbox\"\n  auth:\n    mode: \"account_key\"\n    account_key: \"${ADLS_ACCOUNT_KEY}\"\n</code></pre> <p>Scenario 3: MSI (no secrets)</p> <pre><code>adls_msi:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"aad_msi\"\n    # optional: client_id for user-assigned identity\n    client_id: \"00000000-0000-0000-0000-000000000000\"\n</code></pre> Field Type Required Default Description type Literal['azure_blob'] No <code>ConnectionType.AZURE_BLOB</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - account_name str Yes - - container str Yes - - auth AzureBlobAuthConfig No <code>PydanticUndefined</code> Options: AzureBlobKeyVaultAuth, AzureBlobAccountKeyAuth, AzureBlobSasAuth, AzureBlobConnectionStringAuth, AzureBlobMsiAuth"},{"location":"reference/yaml_schema/#sqlserverconnectionconfig","title":"<code>SQLServerConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>SQL Server / Azure SQL Database connection.</p> <p>When to Use: Reading from SQL Server sources, Azure SQL DB, Azure Synapse.</p> <p>See Also: ReadConfig for query options</p> <p>Scenario 1: Managed identity (AAD MSI)</p> <pre><code>sql_dw_msi:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"aad_msi\"\n</code></pre> <p>Scenario 2: SQL login</p> <pre><code>sql_dw_login:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"sql_login\"\n    username: \"dw_writer\"\n    password: \"${DW_PASSWORD}\"\n</code></pre> Field Type Required Default Description type Literal['sql_server'] No <code>ConnectionType.SQL_SERVER</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - host str Yes - - database str Yes - - port int No <code>1433</code> - auth SQLServerAuthConfig No <code>PydanticUndefined</code> Options: SQLLoginAuth, SQLAadPasswordAuth, SQLMsiAuth, SQLConnectionStringAuth"},{"location":"reference/yaml_schema/#httpconnectionconfig","title":"<code>HttpConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>HTTP connection.</p> <p>Scenario: Bearer token via env var</p> <pre><code>api_source:\n  type: \"http\"\n  base_url: \"https://api.example.com\"\n  headers:\n    User-Agent: \"odibi-pipeline\"\n  auth:\n    mode: \"bearer\"\n    token: \"${API_TOKEN}\"\n</code></pre> Field Type Required Default Description type Literal['http'] No <code>ConnectionType.HTTP</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - base_url str Yes - - headers Dict[str, str] No <code>PydanticUndefined</code> - auth HttpAuthConfig No <code>PydanticUndefined</code> Options: HttpNoAuth, HttpBasicAuth, HttpBearerAuth, HttpApiKeyAuth"},{"location":"reference/yaml_schema/#node-operations","title":"Node Operations","text":""},{"location":"reference/yaml_schema/#readconfig","title":"<code>ReadConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for reading data into a node.</p> <p>When to Use: First node in a pipeline, or any node that reads from storage.</p> <p>Key Concepts: - <code>connection</code>: References a named connection from <code>connections:</code> section - <code>format</code>: File format (csv, parquet, delta, json, sql) - <code>incremental</code>: Enable incremental loading (only new data)</p> <p>See Also: - Incremental Loading - HWM-based loading - IncrementalConfig - Incremental loading options</p>"},{"location":"reference/yaml_schema/#universal-reader-guide","title":"\ud83d\udcd6 \"Universal Reader\" Guide","text":"<p>Business Problem: \"I need to read from files, databases, streams, and even travel back in time to see how data looked yesterday.\"</p> <p>Recipe 1: The Time Traveler (Delta/Iceberg) Reproduce a bug by seeing the data exactly as it was.</p> <pre><code>read:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  time_travel:\n    as_of_timestamp: \"2023-10-25T14:00:00Z\"\n</code></pre> <p>Recipe 2: The Streamer Process data in real-time.</p> <pre><code>read:\n  connection: \"event_hub\"\n  format: \"json\"\n  streaming: true\n</code></pre> <p>Recipe 3: The SQL Query Push down filtering to the source database.</p> <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  # Use the query option to filter at source!\n  query: \"SELECT * FROM huge_table WHERE date &gt;= '2024-01-01'\"\n</code></pre> <p>Recipe 4: Archive Bad Records (Spark) Capture malformed records for later inspection.</p> <pre><code>read:\n  connection: \"landing\"\n  format: \"json\"\n  path: \"events/*.json\"\n  archive_options:\n    badRecordsPath: \"/mnt/quarantine/bad_records\"\n</code></pre> <p>Recipe 5: Optimize JDBC Parallelism (Spark) Control partition count for SQL sources to reduce task overhead.</p> <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  table: \"small_lookup_table\"\n  options:\n    numPartitions: 1  # Single partition for small tables\n</code></pre> <p>Performance Tip: For small tables (&lt;100K rows), use <code>numPartitions: 1</code> to avoid excessive Spark task scheduling overhead. For large tables, increase partitions to enable parallel reads (requires partitionColumn, lowerBound, upperBound). | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name from project.yaml | | format | ReadFormat | str | Yes | - | Data format (csv, parquet, delta, etc.) | | table | Optional[str] | No | - | Table name for SQL/Delta | | path | Optional[str] | No | - | Path for file-based sources | | streaming | bool | No | <code>False</code> | Enable streaming read (Spark only) | | schema_ddl | Optional[str] | No | - | Schema for streaming reads from file sources (required for Avro, JSON, CSV). Use Spark DDL format: 'col1 STRING, col2 INT, col3 TIMESTAMP'. Not required for Delta (schema is inferred from table metadata). | | query | Optional[str] | No | - | SQL query to filter at source (pushdown). Mutually exclusive with table/path if supported by connector. | | filter | Optional[str] | No | - | SQL WHERE clause filter (pushed down to source for SQL formats). Example: \"DAY &gt; '2022-12-31'\" | | incremental | Optional[IncrementalConfig] | No | - | Automatic incremental loading strategy (CDC-like). If set, generates query based on target state (HWM). | | time_travel | Optional[TimeTravelConfig] | No | - | Time travel options (Delta only) | | archive_options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Options for archiving bad records (e.g. badRecordsPath for Spark) | | options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Format-specific options |</p>"},{"location":"reference/yaml_schema/#incrementalconfig","title":"<code>IncrementalConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for automatic incremental loading.</p> <p>When to Use: Load only new/changed data instead of full table scans.</p> <p>See Also: ReadConfig</p> <p>Modes: 1. Rolling Window (Default): Uses a time-based lookback from NOW().    Good for: Stateless loading where you just want \"recent\" data.    Args: <code>lookback</code>, <code>unit</code></p> <ol> <li>Stateful: Tracks the High-Water Mark (HWM) of the key column.    Good for: Exact incremental ingestion (e.g. CDC-like).    Args: <code>state_key</code> (optional), <code>watermark_lag</code> (optional)</li> </ol> <p>Generates SQL: - Rolling: <code>WHERE column &gt;= NOW() - lookback</code> - Stateful: <code>WHERE column &gt; :last_hwm</code></p> <p>Example (Rolling Window):</p> <pre><code>incremental:\n  mode: \"rolling_window\"\n  column: \"updated_at\"\n  lookback: 3\n  unit: \"day\"\n</code></pre> <p>Example (Stateful HWM):</p> <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"id\"\n  # Optional: track separate column for HWM state\n  state_key: \"last_processed_id\"\n</code></pre> <p>Example (Stateful with Watermark Lag):</p> <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"updated_at\"\n  # Handle late-arriving data: look back 2 hours from HWM\n  watermark_lag: \"2h\"\n</code></pre> Field Type Required Default Description mode IncrementalMode No <code>IncrementalMode.ROLLING_WINDOW</code> Incremental strategy: 'rolling_window' or 'stateful' column str Yes - Primary column to filter on (e.g., updated_at) fallback_column Optional[str] No - Backup column if primary is NULL (e.g., created_at). Generates COALESCE(col, fallback) &gt;= ... lookback Optional[int] No - Time units to look back (Rolling Window only) unit Optional[IncrementalUnit] No - Time unit for lookback (Rolling Window only). Options: 'hour', 'day', 'month', 'year' state_key Optional[str] No - Unique ID for state tracking. Defaults to node name if not provided. watermark_lag Optional[str] No - Safety buffer for late-arriving data in stateful mode. Subtracts this duration from the stored HWM when filtering. Format: '' where unit is 's', 'm', 'h', or 'd'. Examples: '2h' (2 hours), '30m' (30 minutes), '1d' (1 day). Use when source has replication lag or eventual consistency."},{"location":"reference/yaml_schema/#timetravelconfig","title":"<code>TimeTravelConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for time travel reading (Delta/Iceberg).</p> <p>Example:</p> <pre><code>time_travel:\n  as_of_version: 10\n  # OR\n  as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre> Field Type Required Default Description as_of_version Optional[int] No - Version number to time travel to as_of_timestamp Optional[str] No - Timestamp string to time travel to"},{"location":"reference/yaml_schema/#transformconfig","title":"<code>TransformConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for transformation steps within a node.</p> <p>When to Use: Custom business logic, data cleaning, SQL transformations.</p> <p>Key Concepts: - <code>steps</code>: Ordered list of operations (SQL, functions, or both) - Each step receives the DataFrame from the previous step - Steps execute in order: step1 \u2192 step2 \u2192 step3</p> <p>See Also: Transformer Catalog</p> <p>Transformer vs Transform: - <code>transformer</code>: Single heavy operation (scd2, merge, deduplicate) - <code>transform.steps</code>: Chain of lighter operations</p>"},{"location":"reference/yaml_schema/#transformation-pipeline-guide","title":"\ud83d\udd27 \"Transformation Pipeline\" Guide","text":"<p>Business Problem: \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"</p> <p>The Solution: Chain multiple steps together. Output of Step 1 becomes input of Step 2.</p> <p>Function Registry: The <code>function</code> step type looks up functions registered with <code>@transform</code> (or <code>@register</code>). This allows you to use the same registered functions as both top-level Transformers and steps in a chain.</p> <p>Recipe: The Mix-and-Match</p> <pre><code>transform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n</code></pre> Field Type Required Default Description steps List[str | TransformStep] Yes - List of transformation steps (SQL strings or TransformStep configs)"},{"location":"reference/yaml_schema/#deletedetectionconfig","title":"<code>DeleteDetectionConfig</code>","text":"<p>Configuration for delete detection in Silver layer.</p>"},{"location":"reference/yaml_schema/#cdc-without-cdc-guide","title":"\ud83d\udd0d \"CDC Without CDC\" Guide","text":"<p>Business Problem: \"Records are deleted in our Azure SQL source, but our Silver tables still show them.\"</p> <p>The Solution: Use delete detection to identify and flag records that no longer exist in the source.</p> <p>Recipe 1: SQL Compare (Recommended for HWM)</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n</code></pre> <p>Recipe 2: Snapshot Diff (For Full Snapshot Sources) Use ONLY with full snapshot ingestion, NOT with HWM incremental.</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [customer_id]\n</code></pre> <p>Recipe 3: Conservative Threshold</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: erp\n        source_table: dbo.Customers\n        max_delete_percent: 20.0\n        on_threshold_breach: error\n</code></pre> <p>Recipe 4: Hard Delete (Remove Rows)</p> <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n        soft_delete_col: null  # removes rows instead of flagging\n</code></pre> Field Type Required Default Description mode DeleteDetectionMode No <code>DeleteDetectionMode.NONE</code> Delete detection strategy: none, snapshot_diff, sql_compare keys List[str] No <code>PydanticUndefined</code> Business key columns for comparison soft_delete_col Optional[str] No <code>_is_deleted</code> Column to flag deletes (True = deleted). Set to null for hard-delete (removes rows). source_connection Optional[str] No - For sql_compare: connection name to query live source source_table Optional[str] No - For sql_compare: table to query for current keys source_query Optional[str] No - For sql_compare: custom SQL query for keys (overrides source_table) snapshot_column Optional[str] No - For snapshot_diff on non-Delta: column to identify snapshots. If None, uses Delta time travel (default). on_first_run FirstRunBehavior No <code>FirstRunBehavior.SKIP</code> Behavior when no previous version exists for snapshot_diff max_delete_percent Optional[float] No <code>50.0</code> Safety threshold: warn/error if more than X% of rows would be deleted on_threshold_breach ThresholdBreachAction No <code>ThresholdBreachAction.WARN</code> Behavior when delete percentage exceeds max_delete_percent"},{"location":"reference/yaml_schema/#validationconfig","title":"<code>ValidationConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for data validation (post-transform checks).</p> <p>When to Use: Output data quality checks that run after transformation but before writing.</p> <p>See Also: Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)</p>"},{"location":"reference/yaml_schema/#the-indestructible-pipeline-pattern","title":"\ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern","text":"<p>Business Problem: \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it before it lands.\"</p> <p>The Solution: A Quality Gate that runs after transformation but before writing.</p> <p>Recipe: The Quality Gate</p> <pre><code>validation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges &amp; Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at &lt;= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n</code></pre> <p>Recipe: Quarantine + Gate</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n</code></pre> Field Type Required Default Description mode ValidationAction No <code>ValidationAction.FAIL</code> Execution mode: 'fail' (stop pipeline) or 'warn' (log only) on_fail OnFailAction No <code>OnFailAction.ALERT</code> Action on failure: 'alert' (send notification) or 'ignore' tests List[TestConfig] No <code>PydanticUndefined</code> List of validation testsOptions: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract quarantine Optional[QuarantineConfig] No - Quarantine configuration for failed rows gate Optional[GateConfig] No - Quality gate configuration for batch-level validation"},{"location":"reference/yaml_schema/#quarantineconfig","title":"<code>QuarantineConfig</code>","text":"<p>Used in: ValidationConfig</p> <p>Configuration for quarantine table routing.</p> <p>When to Use: Capture invalid records for review/reprocessing instead of failing the pipeline.</p> <p>See Also: Quarantine Guide, ValidationConfig</p> <p>Routes rows that fail validation tests to a quarantine table with rejection metadata for later analysis/reprocessing.</p> <p>Example:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n</code></pre> Field Type Required Default Description connection str Yes - Connection for quarantine writes path Optional[str] No - Path for quarantine data table Optional[str] No - Table name for quarantine add_columns QuarantineColumnsConfig No <code>PydanticUndefined</code> Metadata columns to add to quarantined rows retention_days Optional[int] No <code>90</code> Days to retain quarantined data (auto-cleanup)"},{"location":"reference/yaml_schema/#quarantinecolumnsconfig","title":"<code>QuarantineColumnsConfig</code>","text":"<p>Used in: QuarantineConfig</p> <p>Columns added to quarantined rows for debugging and reprocessing.</p> <p>Example:</p> <pre><code>quarantine:\n  connection: silver\n  path: customers_quarantine\n  add_columns:\n    _rejection_reason: true\n    _rejected_at: true\n    _source_batch_id: true\n    _failed_tests: true\n    _original_node: false\n</code></pre> Field Type Required Default Description rejection_reason bool No <code>True</code> Add _rejection_reason column with test failure description rejected_at bool No <code>True</code> Add _rejected_at column with UTC timestamp source_batch_id bool No <code>True</code> Add _source_batch_id column with run ID for traceability failed_tests bool No <code>True</code> Add _failed_tests column with comma-separated list of failed test names original_node bool No <code>False</code> Add _original_node column with source node name"},{"location":"reference/yaml_schema/#gateconfig","title":"<code>GateConfig</code>","text":"<p>Used in: EnvironmentConfig, ValidationConfig</p> <p>Gate requirements for promoting changes to Master.</p> <p>All gates must pass before changes can be promoted. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | require_ruff_clean | bool | No | <code>True</code> | Require ruff linting to pass with no errors | | require_pytest_pass | bool | No | <code>True</code> | Require all pytest tests to pass | | require_odibi_validate | bool | No | <code>True</code> | Require odibi validate to pass on modified configs | | require_golden_projects | bool | No | <code>True</code> | Require all learning harness configs to pass |</p>"},{"location":"reference/yaml_schema/#gateconfig_1","title":"<code>GateConfig</code>","text":"<p>Used in: EnvironmentConfig, ValidationConfig</p> <p>Quality gate configuration for batch-level validation.</p> <p>When to Use: Pipeline-level pass/fail thresholds, row count limits, change detection.</p> <p>See Also: Quality Gates, ValidationConfig</p> <p>Gates evaluate the entire batch before writing, ensuring data quality thresholds are met.</p> <p>Example:</p> <pre><code>gate:\n  require_pass_rate: 0.95\n  on_fail: abort\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n  row_count:\n    min: 100\n    change_threshold: 0.5\n</code></pre> Field Type Required Default Description require_pass_rate float No <code>0.95</code> Minimum percentage of rows passing ALL tests on_fail GateOnFail No <code>GateOnFail.ABORT</code> Action when gate fails thresholds List[GateThreshold] No <code>PydanticUndefined</code> Per-test thresholds (overrides global require_pass_rate) row_count Optional[RowCountGate] No - Row count anomaly detection"},{"location":"reference/yaml_schema/#gatethreshold","title":"<code>GateThreshold</code>","text":"<p>Used in: GateConfig</p> <p>Per-test threshold configuration for quality gates.</p> <p>Allows setting different pass rate requirements for specific tests.</p> <p>Example:</p> <pre><code>gate:\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n    - test: unique\n      min_pass_rate: 1.0\n</code></pre> Field Type Required Default Description test str Yes - Test name or type to apply threshold to min_pass_rate float Yes - Minimum pass rate required (0.0-1.0, e.g., 0.99 = 99%)"},{"location":"reference/yaml_schema/#rowcountgate","title":"<code>RowCountGate</code>","text":"<p>Used in: GateConfig</p> <p>Row count anomaly detection for quality gates.</p> <p>Validates that batch size falls within expected bounds and detects significant changes from previous runs.</p> <p>Example:</p> <pre><code>gate:\n  row_count:\n    min: 100\n    max: 1000000\n    change_threshold: 0.5\n</code></pre> Field Type Required Default Description min Optional[int] No - Minimum expected row count max Optional[int] No - Maximum expected row count change_threshold Optional[float] No - Max allowed change vs previous run (e.g., 0.5 = 50% change triggers failure)"},{"location":"reference/yaml_schema/#writeconfig","title":"<code>WriteConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for writing data from a node.</p> <p>When to Use: Any node that persists data to storage.</p> <p>Key Concepts: - <code>mode</code>: How to handle existing data (overwrite, append, upsert) - <code>keys</code>: Required for upsert mode - columns that identify unique records - <code>partition_by</code>: Columns to partition output by (improves query performance)</p> <p>See Also: - Performance Tuning - Partitioning strategies</p>"},{"location":"reference/yaml_schema/#big-data-performance-guide","title":"\ud83d\ude80 \"Big Data Performance\" Guide","text":"<p>Business Problem: \"My dashboards are slow because the query scans terabytes of data just to find one day's sales.\"</p> <p>The Solution: Use Partitioning for coarse filtering (skipping huge chunks) and Z-Ordering for fine-grained skipping (colocating related data).</p> <p>Recipe: Lakehouse Optimized</p> <pre><code>write:\n  connection: \"gold_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  mode: \"append\"\n\n  # 1. Partitioning: Physical folders.\n  # Use for low-cardinality columns often used in WHERE clauses.\n  # WARNING: Do NOT partition by high-cardinality cols like ID or Timestamp!\n  partition_by: [\"country_code\", \"txn_year_month\"]\n\n  # 2. Z-Ordering: Data clustering.\n  # Use for high-cardinality columns often used in JOINs or predicates.\n  zorder_by: [\"customer_id\", \"product_id\"]\n\n  # 3. Table Properties: Engine tuning.\n  table_properties:\n    \"delta.autoOptimize.optimizeWrite\": \"true\"\n    \"delta.autoOptimize.autoCompact\": \"true\"\n</code></pre> Field Type Required Default Description connection str Yes - Connection name from project.yaml format ReadFormat | str Yes - Output format (csv, parquet, delta, etc.) table Optional[str] No - Table name for SQL/Delta path Optional[str] No - Path for file-based outputs register_table Optional[str] No - Register file output as external table (Spark/Delta only) mode WriteMode No <code>WriteMode.OVERWRITE</code> Write mode. Options: 'overwrite', 'append', 'upsert', 'append_once' partition_by List[str] No <code>PydanticUndefined</code> List of columns to physically partition the output by (folder structure). Use for low-cardinality columns (e.g. date, country). zorder_by List[str] No <code>PydanticUndefined</code> List of columns to Z-Order by. Improves read performance for high-cardinality columns used in filters/joins (Delta only). table_properties Dict[str, str] No <code>PydanticUndefined</code> Delta table properties. Overrides global performance.delta_table_properties. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. merge_schema bool No <code>False</code> Allow schema evolution (mergeSchema option in Delta) first_run_query Optional[str] No - SQL query for full-load on first run (High Water Mark pattern). If set, uses this query when target table doesn't exist, then switches to incremental. Only applies to SQL reads. options Dict[str, Any] No <code>PydanticUndefined</code> Format-specific options auto_optimize bool | AutoOptimizeConfig No - Auto-run OPTIMIZE and VACUUM after write (Delta only) add_metadata bool | WriteMetadataConfig No - Add metadata columns for Bronze layer lineage. Set to <code>true</code> to add all applicable columns, or provide a WriteMetadataConfig for selective columns. Columns: _extracted_at, _source_file (file sources), _source_connection, _source_table (SQL sources). skip_if_unchanged bool No <code>False</code> Skip write if DataFrame content is identical to previous write. Computes SHA256 hash of entire DataFrame and compares to stored hash in Delta table metadata. Useful for snapshot tables without timestamps to avoid redundant appends. Only supported for Delta format. skip_hash_columns Optional[List[str]] No - Columns to include in hash computation for skip_if_unchanged. If None, all columns are used. Specify a subset to ignore volatile columns like timestamps. skip_hash_sort_columns Optional[List[str]] No - Columns to sort by before hashing for deterministic comparison. Required if row order may vary between runs. Typically your business key columns. streaming Optional[StreamingWriteConfig] No - Streaming write configuration for Spark Structured Streaming. When set, uses writeStream instead of batch write. Requires a streaming DataFrame from a streaming read source."},{"location":"reference/yaml_schema/#writemetadataconfig","title":"<code>WriteMetadataConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for metadata columns added during Bronze writes.</p>"},{"location":"reference/yaml_schema/#bronze-metadata-guide","title":"\ud83d\udccb Bronze Metadata Guide","text":"<p>Business Problem: \"We need lineage tracking and debugging info for our Bronze layer data.\"</p> <p>The Solution: Add metadata columns during ingestion for traceability.</p> <p>Recipe 1: Add All Metadata (Recommended)</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n</code></pre> <p>Recipe 2: Selective Metadata</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: false\n    source_table: false\n</code></pre> <p>Available Columns: - <code>_extracted_at</code>: Pipeline execution timestamp (all sources) - <code>_source_file</code>: Source filename/path (file sources only) - <code>_source_connection</code>: Connection name used (all sources) - <code>_source_table</code>: Table or query name (SQL sources only) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | extracted_at | bool | No | <code>True</code> | Add _extracted_at column with pipeline execution timestamp | | source_file | bool | No | <code>True</code> | Add _source_file column with source filename (file sources only) | | source_connection | bool | No | <code>False</code> | Add _source_connection column with connection name | | source_table | bool | No | <code>False</code> | Add _source_table column with table/query name (SQL sources only) |</p>"},{"location":"reference/yaml_schema/#streamingwriteconfig","title":"<code>StreamingWriteConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Spark Structured Streaming writes.</p>"},{"location":"reference/yaml_schema/#real-time-pipeline-guide","title":"\ud83d\ude80 \"Real-Time Pipeline\" Guide","text":"<p>Business Problem: \"I need to process data continuously as it arrives from Kafka/Event Hubs and write it to Delta Lake in near real-time.\"</p> <p>The Solution: Configure streaming write with checkpoint location for fault tolerance and trigger interval for processing frequency.</p> <p>Recipe: Streaming Ingestion</p> <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_stream\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_stream\"\n    trigger:\n      processing_time: \"10 seconds\"\n</code></pre> <p>Recipe: One-Time Streaming (Batch-like)</p> <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_batch\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_batch\"\n    trigger:\n      available_now: true\n</code></pre> Field Type Required Default Description output_mode Literal['append', 'update', 'complete'] No <code>append</code> Output mode for streaming writes. 'append' - Only new rows. 'update' - Updated rows only. 'complete' - Entire result table (requires aggregation). checkpoint_location str Yes - Path for streaming checkpoints. Required for fault tolerance. Must be a reliable storage location (e.g., cloud storage, DBFS). trigger Optional[TriggerConfig] No - Trigger configuration. If not specified, processes data as fast as possible. Use 'processing_time' for micro-batch intervals, 'once' for single batch, 'available_now' for processing all available data then stopping. query_name Optional[str] No - Name for the streaming query (useful for monitoring and debugging) await_termination Optional[bool] No <code>False</code> Wait for the streaming query to terminate. Set to True for batch-like streaming with 'once' or 'available_now' triggers. timeout_seconds Optional[int] No - Timeout in seconds when await_termination is True. If None, waits indefinitely."},{"location":"reference/yaml_schema/#triggerconfig","title":"<code>TriggerConfig</code>","text":"<p>Used in: StreamingWriteConfig</p> <p>Configuration for streaming trigger intervals.</p> <p>Specify exactly one of the trigger options.</p> <p>Example:</p> <pre><code>trigger:\n  processing_time: \"10 seconds\"\n</code></pre> <p>Or for one-time processing:</p> <pre><code>trigger:\n  once: true\n</code></pre> Field Type Required Default Description processing_time Optional[str] No - Trigger interval as duration string (e.g., '10 seconds', '1 minute') once Optional[bool] No - Process all available data once and stop available_now Optional[bool] No - Process all available data in multiple batches, then stop continuous Optional[str] No - Continuous processing with checkpoint interval (e.g., '1 second')"},{"location":"reference/yaml_schema/#autooptimizeconfig","title":"<code>AutoOptimizeConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Delta Lake automatic optimization.</p> <p>Example:</p> <pre><code>auto_optimize:\n  enabled: true\n  vacuum_retention_hours: 168\n</code></pre> Field Type Required Default Description enabled bool No <code>True</code> Enable auto optimization vacuum_retention_hours int No <code>168</code> Hours to retain history for VACUUM (default 7 days). Set to 0 to disable VACUUM."},{"location":"reference/yaml_schema/#privacyconfig","title":"<code>PrivacyConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for PII anonymization.</p>"},{"location":"reference/yaml_schema/#privacy-pii-protection","title":"\ud83d\udd10 Privacy &amp; PII Protection","text":"<p>How It Works: 1. Mark columns as <code>pii: true</code> in the <code>columns</code> metadata 2. Configure a <code>privacy</code> block with the anonymization method 3. During node execution, all columns marked as PII (and inherited from dependencies) are anonymized 4. Upstream PII markings are inherited by downstream nodes</p> <p>Example:</p> <pre><code>columns:\n  customer_email:\n    pii: true  # Mark as PII\n  customer_id:\n    pii: false\n\nprivacy:\n  method: hash       # hash, mask, or redact\n  salt: \"secret_key\" # Optional: makes hash unique/secure\n  declassify: []     # Remove columns from PII protection\n</code></pre> <p>Methods: - <code>hash</code>: SHA256 hash (length 64). With salt, prevents pre-computed rainbow tables. - <code>mask</code>: Show only last 4 chars, replace rest with <code>*</code>. Example: <code>john@email.com</code> \u2192 <code>****@email.com</code> - <code>redact</code>: Replace entire value with <code>[REDACTED]</code></p> <p>Important: - <code>pii: true</code> alone does NOTHING. You must set a <code>privacy.method</code> to actually mask data. - PII inheritance: If dependency outputs PII columns, this node inherits them unless declassified. - Salt is optional but recommended for hash to prevent attacks. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | method | PrivacyMethod | Yes | - | Anonymization method: 'hash' (SHA256), 'mask' (show last 4), or 'redact' ([REDACTED]) | | salt | Optional[str] | No | - | Salt for hashing (optional but recommended). Appended before hashing to create unique hashes. Example: 'company_secret_key_2025' | | declassify | List[str] | No | <code>PydanticUndefined</code> | List of columns to remove from PII protection (stops inheritance from upstream). Example: ['customer_id'] |</p>"},{"location":"reference/yaml_schema/#contracts-data-quality-gates","title":"Contracts (Data Quality Gates)","text":""},{"location":"reference/yaml_schema/#contracts-pre-transform-checks","title":"Contracts (Pre-Transform Checks)","text":"<p>Contracts are fail-fast data quality checks that run on input data before transformation. They always halt execution on failure - use them to prevent bad data from entering the pipeline.</p> <p>Contracts vs Validation vs Quality Gates:</p> Feature When it Runs On Failure Use Case Contracts Before transform Always fails Input data quality (not-null, unique keys) Validation After transform Configurable (fail/warn/quarantine) Output data quality (ranges, formats) Quality Gates After validation Configurable (abort/warn) Pipeline-level thresholds (pass rate, row counts) Quarantine With validation Routes bad rows Capture invalid records for review <p>See Also: - Validation Guide - Full validation configuration - Quarantine Guide - Quarantine setup and review - Getting Started: Validation</p> <p>Example:</p> <pre><code>- name: \"process_orders\"\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read:\n    source: raw_orders\n</code></pre>"},{"location":"reference/yaml_schema/#acceptedvaluestest","title":"<code>AcceptedValuesTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures a column only contains values from an allowed list.</p> <p>When to Use: Enum-like fields, status columns, categorical data validation.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected]\n</code></pre> Field Type Required Default Description type Literal['accepted_values'] No <code>TestType.ACCEPTED_VALUES</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check values List[Any] Yes - Allowed values"},{"location":"reference/yaml_schema/#customsqltest","title":"<code>CustomSQLTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Runs a custom SQL condition and fails if too many rows violate it.</p> <pre><code>contracts:\n  - type: custom_sql\n    condition: \"amount &gt; 0\"\n    threshold: 0.01  # Allow up to 1% failures\n</code></pre> Field Type Required Default Description type Literal['custom_sql'] No <code>TestType.CUSTOM_SQL</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure condition str Yes - SQL condition that should be true for valid rows threshold float No <code>0.0</code> Failure rate threshold (0.0 = strictly no failures allowed)"},{"location":"reference/yaml_schema/#distributioncontract","title":"<code>DistributionContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if a column's statistical distribution is within expected bounds.</p> <p>When to Use: Detect data drift, anomaly detection, statistical monitoring.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \"&gt;100\"  # Mean must be &gt; 100\n    on_fail: warn\n</code></pre> Field Type Required Default Description type Literal['distribution'] No <code>TestType.DISTRIBUTION</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.WARN</code> - column str Yes - Column to analyze metric Literal['mean', 'min', 'max', 'null_percentage'] Yes - Statistical metric to check threshold str Yes - Threshold expression (e.g., '&gt;100', '&lt;0.05')"},{"location":"reference/yaml_schema/#freshnesscontract","title":"<code>FreshnessContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that data is not stale by checking a timestamp column.</p> <p>When to Use: Source systems that should update regularly, SLA monitoring.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n</code></pre> Field Type Required Default Description type Literal['freshness'] No <code>TestType.FRESHNESS</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> - column str No <code>updated_at</code> Timestamp column to check max_age str Yes - Maximum allowed age (e.g., '24h', '7d')"},{"location":"reference/yaml_schema/#notnulltest","title":"<code>NotNullTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns contain no NULL values.</p> <p>When to Use: Primary keys, required fields, foreign keys that must resolve.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n</code></pre> Field Type Required Default Description type Literal['not_null'] No <code>TestType.NOT_NULL</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure columns List[str] Yes - Columns that must not contain nulls"},{"location":"reference/yaml_schema/#rangetest","title":"<code>RangeTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values fall within a specified range.</p> <p>When to Use: Numeric bounds validation (ages, prices, quantities), date ranges.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n</code></pre> Field Type Required Default Description type Literal['range'] No <code>TestType.RANGE</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check min int | float | str No - Minimum value (inclusive) max int | float | str No - Maximum value (inclusive)"},{"location":"reference/yaml_schema/#regexmatchtest","title":"<code>RegexMatchTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values match a regex pattern.</p> <p>When to Use: Format validation (emails, phone numbers, IDs, codes).</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n</code></pre> Field Type Required Default Description type Literal['regex_match'] No <code>TestType.REGEX_MATCH</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check pattern str Yes - Regex pattern to match"},{"location":"reference/yaml_schema/#rowcounttest","title":"<code>RowCountTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that row count falls within expected bounds.</p> <p>When to Use: Ensure minimum data completeness, detect truncated loads, cap batch sizes.</p> <p>See Also: Contracts Overview, GateConfig</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000\n    max: 100000\n</code></pre> Field Type Required Default Description type Literal['row_count'] No <code>TestType.ROW_COUNT</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure min Optional[int] No - Minimum row count max Optional[int] No - Maximum row count"},{"location":"reference/yaml_schema/#schemacontract","title":"<code>SchemaContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that the DataFrame schema matches expected columns.</p> <p>When to Use: Enforce schema stability, detect upstream schema drift, ensure column presence.</p> <p>See Also: Contracts Overview, SchemaPolicyConfig</p> <p>Uses the <code>columns</code> metadata from NodeConfig to verify schema.</p> <pre><code>contracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n</code></pre> Field Type Required Default Description type Literal['schema'] No <code>TestType.SCHEMA</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> - strict bool No <code>True</code> If true, fail on unexpected columns"},{"location":"reference/yaml_schema/#uniquetest","title":"<code>UniqueTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns (or combination) contain unique values.</p> <p>When to Use: Primary keys, natural keys, deduplication verification.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: unique\n    columns: [order_id]  # Single column\n  # OR composite key:\n  - type: unique\n    columns: [customer_id, order_date]  # Composite uniqueness\n</code></pre> Field Type Required Default Description type Literal['unique'] No <code>TestType.UNIQUE</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure columns List[str] Yes - Columns that must be unique (composite key if multiple)"},{"location":"reference/yaml_schema/#volumedroptest","title":"<code>VolumeDropTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if row count dropped significantly compared to history.</p> <p>When to Use: Detect source outages, partial loads, or data pipeline issues.</p> <p>See Also: Contracts Overview, RowCountTest</p> <p>Formula: <code>(current - avg) / avg &lt; -threshold</code></p> <pre><code>contracts:\n  - type: volume_drop\n    threshold: 0.5  # Fail if &gt; 50% drop from 7-day average\n    lookback_days: 7\n</code></pre> Field Type Required Default Description type Literal['volume_drop'] No <code>TestType.VOLUME_DROP</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure threshold float No <code>0.5</code> Max allowed drop (0.5 = 50% drop) lookback_days int No <code>7</code> Days of history to average"},{"location":"reference/yaml_schema/#global-settings","title":"Global Settings","text":""},{"location":"reference/yaml_schema/#lineageconfig","title":"<code>LineageConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for OpenLineage integration.</p> <p>Example:</p> <pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n</code></pre> Field Type Required Default Description url Optional[str] No - OpenLineage API URL namespace str No <code>odibi</code> Namespace for jobs api_key Optional[str] No - API Key"},{"location":"reference/yaml_schema/#alertconfig","title":"<code>AlertConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for alerts with throttling support.</p> <p>Supports Slack, Teams, and generic webhooks with event-specific payloads.</p> <p>Available Events: - <code>on_start</code> - Pipeline started - <code>on_success</code> - Pipeline completed successfully - <code>on_failure</code> - Pipeline failed - <code>on_quarantine</code> - Rows were quarantined - <code>on_gate_block</code> - Quality gate blocked the pipeline - <code>on_threshold_breach</code> - A threshold was exceeded</p> <p>Example:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre> Field Type Required Default Description type AlertType Yes - - url str Yes - Webhook URL on_events List[AlertEvent] No <code>[&lt;AlertEvent.ON_FAILURE: 'on_failure'&gt;]</code> Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach metadata Dict[str, Any] No <code>PydanticUndefined</code> Extra metadata: throttle_minutes, max_per_hour, channel, etc."},{"location":"reference/yaml_schema/#loggingconfig","title":"<code>LoggingConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Logging configuration.</p> <p>Example:</p> <pre><code>logging:\n  level: \"INFO\"\n  structured: true\n</code></pre> Field Type Required Default Description level LogLevel No <code>LogLevel.INFO</code> - structured bool No <code>False</code> Output JSON logs metadata Dict[str, Any] No <code>PydanticUndefined</code> Extra metadata in logs"},{"location":"reference/yaml_schema/#performanceconfig","title":"<code>PerformanceConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Performance tuning configuration.</p> <p>Example:</p> <pre><code>performance:\n  use_arrow: true\n  spark_config:\n    \"spark.sql.shuffle.partitions\": \"200\"\n    \"spark.sql.adaptive.enabled\": \"true\"\n    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\"\n  delta_table_properties:\n    \"delta.columnMapping.mode\": \"name\"\n</code></pre> <p>Spark Config Notes: - Configs are applied via <code>spark.conf.set()</code> at runtime - For existing sessions (e.g., Databricks), only runtime-settable configs will take effect - Session-level configs (e.g., <code>spark.executor.memory</code>) require session restart - Common runtime-safe configs: shuffle partitions, adaptive query execution, Delta optimizations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | use_arrow | bool | No | <code>True</code> | Use Apache Arrow-backed DataFrames (Pandas only). Reduces memory and speeds up I/O. | | spark_config | Dict[str, str] | No | <code>PydanticUndefined</code> | Spark configuration settings applied at runtime via spark.conf.set(). Example: {'spark.sql.shuffle.partitions': '200', 'spark.sql.adaptive.enabled': 'true'}. Note: Some configs require session restart and cannot be set at runtime. | | delta_table_properties | Dict[str, str] | No | <code>PydanticUndefined</code> | Default table properties applied to all Delta writes. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. | | skip_null_profiling | bool | No | <code>False</code> | Skip null profiling in metadata collection phase. Reduces execution time for large DataFrames by avoiding an additional Spark job. | | skip_catalog_writes | bool | No | <code>False</code> | Skip catalog metadata writes (register_asset, track_schema, log_pattern, record_lineage) after each node write. Significantly improves performance for high-throughput pipelines like Bronze layer ingestion. Set to true when catalog tracking is not needed. | | skip_run_logging | bool | No | <code>False</code> | Skip batch catalog writes at pipeline end (log_runs_batch, register_outputs_batch). Saves 10-20s per pipeline run. Enable when you don't need run history in the catalog. Stories are still generated and contain full execution details. |</p>"},{"location":"reference/yaml_schema/#retryconfig","title":"<code>RetryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Retry configuration.</p> <p>Example:</p> <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n</code></pre> Field Type Required Default Description enabled bool No <code>True</code> - max_attempts int No <code>3</code> - backoff BackoffStrategy No <code>BackoffStrategy.EXPONENTIAL</code> -"},{"location":"reference/yaml_schema/#storyconfig","title":"<code>StoryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Story generation configuration.</p> <p>Stories are ODIBI's core value - execution reports with lineage. They must use a connection for consistent, traceable output.</p> <p>Example:</p> <pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre> <p>Failure Sample Settings: - <code>failure_sample_size</code>: Number of failed rows to capture per validation (default: 100) - <code>max_failure_samples</code>: Total failed rows across all validations (default: 500) - <code>max_sampled_validations</code>: After this many validations, show only counts (default: 5) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name for story output (uses connection's path resolution) | | path | str | Yes | - | Path for stories (relative to connection base_path) | | max_sample_rows | int | No | <code>10</code> | - | | auto_generate | bool | No | <code>True</code> | - | | retention_days | Optional[int] | No | <code>30</code> | Days to keep stories | | retention_count | Optional[int] | No | <code>100</code> | Max number of stories to keep | | failure_sample_size | int | No | <code>100</code> | Number of failed rows to capture per validation rule | | max_failure_samples | int | No | <code>500</code> | Maximum total failed rows across all validations | | max_sampled_validations | int | No | <code>5</code> | After this many validations, show only counts (no samples) | | async_generation | bool | No | <code>False</code> | Generate stories asynchronously (fire-and-forget). Pipeline returns immediately while story writes in background. Improves multi-pipeline performance by ~5-10s per pipeline. |</p>"},{"location":"reference/yaml_schema/#transformation-reference","title":"Transformation Reference","text":""},{"location":"reference/yaml_schema/#how-to-use-transformers","title":"How to Use Transformers","text":"<p>You can use any transformer in two ways:</p> <p>1. As a Top-Level Transformer (\"The App\") Use this for major operations that define the node's purpose (e.g. Merge, SCD2).</p> <pre><code>- name: \"my_node\"\n  transformer: \"&lt;transformer_name&gt;\"\n  params:\n    &lt;param_name&gt;: &lt;value&gt;\n</code></pre> <p>2. As a Step in a Chain (\"The Script\") Use this for smaller operations within a <code>transform</code> block (e.g. clean_text, filter).</p> <pre><code>- name: \"my_node\"\n  transform:\n    steps:\n      - function: \"&lt;transformer_name&gt;\"\n         params:\n           &lt;param_name&gt;: &lt;value&gt;\n</code></pre> <p>Available Transformers: The models below describe the <code>params</code> required for each transformer.</p>"},{"location":"reference/yaml_schema/#common-operations","title":"\ud83d\udcc2 Common Operations","text":""},{"location":"reference/yaml_schema/#casewhencase","title":"CaseWhenCase","text":"<p>Back to Catalog</p> Field Type Required Default Description condition str Yes - - value str Yes - -"},{"location":"reference/yaml_schema/#add_prefix-addprefixparams","title":"<code>add_prefix</code> (AddPrefixParams)","text":"<p>Adds a prefix to column names.</p> <p>Configuration for adding a prefix to column names.</p> <p>Example - All columns:</p> <pre><code>add_prefix:\n  prefix: \"src_\"\n</code></pre> <p>Example - Specific columns:</p> <pre><code>add_prefix:\n  prefix: \"raw_\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description prefix str Yes - Prefix to add to column names columns Optional[List[str]] No - Columns to prefix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from prefixing"},{"location":"reference/yaml_schema/#add_suffix-addsuffixparams","title":"<code>add_suffix</code> (AddSuffixParams)","text":"<p>Adds a suffix to column names.</p> <p>Configuration for adding a suffix to column names.</p> <p>Example - All columns:</p> <pre><code>add_suffix:\n  suffix: \"_raw\"\n</code></pre> <p>Example - Specific columns:</p> <pre><code>add_suffix:\n  suffix: \"_v2\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description suffix str Yes - Suffix to add to column names columns Optional[List[str]] No - Columns to suffix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from suffixing"},{"location":"reference/yaml_schema/#case_when-casewhenparams","title":"<code>case_when</code> (CaseWhenParams)","text":"<p>Implements structured CASE WHEN logic.</p> <p>Configuration for conditional logic.</p> <p>Example:</p> <pre><code>case_when:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description cases List[CaseWhenCase] Yes - List of conditional branches default str No <code>NULL</code> Default value if no condition met output_col str Yes - Name of the resulting column"},{"location":"reference/yaml_schema/#cast_columns-castcolumnsparams","title":"<code>cast_columns</code> (CastColumnsParams)","text":"<p>Casts specific columns to new types while keeping others intact.</p> <p>Configuration for column type casting.</p> <p>Example:</p> <pre><code>cast_columns:\n  casts:\n    age: \"int\"\n    salary: \"DOUBLE\"\n    created_at: \"TIMESTAMP\"\n    tags: \"ARRAY&lt;STRING&gt;\"  # Raw SQL types allowed\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description casts Dict[str, SimpleType | str] Yes - Map of column to target SQL type"},{"location":"reference/yaml_schema/#clean_text-cleantextparams","title":"<code>clean_text</code> (CleanTextParams)","text":"<p>Applies string cleaning operations (Trim/Case) via SQL.</p> <p>Configuration for text cleaning.</p> <p>Example:</p> <pre><code>clean_text:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to clean trim bool No <code>True</code> Apply TRIM() case Literal['lower', 'upper', 'preserve'] No <code>preserve</code> Case conversion"},{"location":"reference/yaml_schema/#coalesce_columns-coalescecolumnsparams","title":"<code>coalesce_columns</code> (CoalesceColumnsParams)","text":"<p>Returns the first non-null value from a list of columns. Useful for fallback/priority scenarios.</p> <p>Configuration for coalescing columns (first non-null value).</p> <p>Example - Phone number fallback:</p> <pre><code>coalesce_columns:\n  columns: [\"mobile_phone\", \"work_phone\", \"home_phone\"]\n  output_col: \"primary_phone\"\n</code></pre> <p>Example - Timestamp fallback:</p> <pre><code>coalesce_columns:\n  columns: [\"updated_at\", \"modified_at\", \"created_at\"]\n  output_col: \"last_change_at\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to coalesce (in priority order) output_col str Yes - Name of the output column drop_source bool No <code>False</code> Drop the source columns after coalescing"},{"location":"reference/yaml_schema/#concat_columns-concatcolumnsparams","title":"<code>concat_columns</code> (ConcatColumnsParams)","text":"<p>Concatenates multiple columns into one string. NULLs are skipped (treated as empty string) using CONCAT_WS behavior.</p> <p>Configuration for string concatenation.</p> <p>Example:</p> <pre><code>concat_columns:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to concatenate separator str No - Separator string output_col str Yes - Resulting column name"},{"location":"reference/yaml_schema/#convert_timezone-converttimezoneparams","title":"<code>convert_timezone</code> (ConvertTimezoneParams)","text":"<p>Converts a timestamp from one timezone to another. Assumes the input column is a naive timestamp representing time in source_tz, or a timestamp with timezone.</p> <p>Configuration for timezone conversion.</p> <p>Example:</p> <pre><code>convert_timezone:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - Timestamp column to convert source_tz str No <code>UTC</code> Source timezone (e.g., 'UTC', 'America/New_York') target_tz str Yes - Target timezone (e.g., 'America/Los_Angeles') output_col Optional[str] No - Name of the result column (default: {col}_{target_tz})"},{"location":"reference/yaml_schema/#date_add-dateaddparams","title":"<code>date_add</code> (DateAddParams)","text":"<p>Adds an interval to a date/timestamp column.</p> <p>Configuration for date addition.</p> <p>Example:</p> <pre><code>date_add:\n  col: \"created_at\"\n  value: 1\n  unit: \"day\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - - value int Yes - - unit Literal['day', 'month', 'year', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema/#date_diff-datediffparams","title":"<code>date_diff</code> (DateDiffParams)","text":"<p>Calculates difference between two dates/timestamps. Returns the elapsed time in the specified unit (as float for sub-day units).</p> <p>Configuration for date difference.</p> <p>Example:</p> <pre><code>date_diff:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description start_col str Yes - - end_col str Yes - - unit Literal['day', 'hour', 'minute', 'second'] No <code>day</code> -"},{"location":"reference/yaml_schema/#date_trunc-datetruncparams","title":"<code>date_trunc</code> (DateTruncParams)","text":"<p>Truncates a date/timestamp to the specified precision.</p> <p>Configuration for date truncation.</p> <p>Example:</p> <pre><code>date_trunc:\n  col: \"created_at\"\n  unit: \"month\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - - unit Literal['year', 'month', 'day', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema/#derive_columns-derivecolumnsparams","title":"<code>derive_columns</code> (DeriveColumnsParams)","text":"<p>Appends new columns based on SQL expressions.</p> <p>Design: - Uses projection to add fields. - Keeps all existing columns via <code>*</code>.</p> <p>Configuration for derived columns.</p> <p>Example:</p> <pre><code>derive_columns:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre> <p>Note: Engine will fail if expressions reference non-existent columns. Back to Catalog</p> Field Type Required Default Description derivations Dict[str, str] Yes - Map of column name to SQL expression"},{"location":"reference/yaml_schema/#distinct-distinctparams","title":"<code>distinct</code> (DistinctParams)","text":"<p>Returns unique rows (SELECT DISTINCT).</p> <p>Configuration for distinct rows.</p> <p>Example:</p> <pre><code>distinct:\n  columns: [\"category\", \"status\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to project (if None, keeps all columns unique)"},{"location":"reference/yaml_schema/#drop_columns-dropcolumnsparams","title":"<code>drop_columns</code> (DropColumnsParams)","text":"<p>Removes the specified columns from the DataFrame.</p> <p>Configuration for dropping specific columns (blacklist).</p> <p>Example:</p> <pre><code>drop_columns:\n  columns: [\"_internal_id\", \"_temp_flag\", \"_processing_date\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to drop"},{"location":"reference/yaml_schema/#extract_date_parts-extractdateparams","title":"<code>extract_date_parts</code> (ExtractDateParams)","text":"<p>Extracts date parts using ANSI SQL extract/functions.</p> <p>Configuration for extracting date parts.</p> <p>Example:</p> <pre><code>extract_date_parts:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description source_col str Yes - - prefix Optional[str] No - - parts Literal[typing.Literal['year', 'month', 'day', 'hour']] No <code>['year', 'month', 'day']</code> -"},{"location":"reference/yaml_schema/#fill_nulls-fillnullsparams","title":"<code>fill_nulls</code> (FillNullsParams)","text":"<p>Replaces null values with specified defaults using COALESCE.</p> <p>Configuration for filling null values.</p> <p>Example:</p> <pre><code>fill_nulls:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description values Dict[str, str | int | float | bool] Yes - Map of column to fill value"},{"location":"reference/yaml_schema/#filter_rows-filterrowsparams","title":"<code>filter_rows</code> (FilterRowsParams)","text":"<p>Filters rows using a standard SQL WHERE clause.</p> <p>Design: - SQL-First: Pushes filtering to the engine's optimizer. - Zero-Copy: No data movement to Python.</p> <p>Configuration for filtering rows.</p> <p>Example:</p> <pre><code>filter_rows:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre> <p>Example (Null Check):</p> <pre><code>filter_rows:\n  condition: \"email IS NOT NULL AND email != ''\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description condition str Yes - SQL WHERE clause (e.g., 'age &gt; 18 AND status = \"active\"')"},{"location":"reference/yaml_schema/#limit-limitparams","title":"<code>limit</code> (LimitParams)","text":"<p>Limits result size.</p> <p>Configuration for result limiting.</p> <p>Example:</p> <pre><code>limit:\n  n: 100\n  offset: 0\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description n int Yes - Number of rows to return offset int No <code>0</code> Number of rows to skip"},{"location":"reference/yaml_schema/#normalize_column_names-normalizecolumnnamesparams","title":"<code>normalize_column_names</code> (NormalizeColumnNamesParams)","text":"<p>Normalizes column names to a consistent style. Useful for cleaning up messy source data with spaces, mixed case, or special characters.</p> <p>Configuration for normalizing column names.</p> <p>Example:</p> <pre><code>normalize_column_names:\n  style: \"snake_case\"\n  lowercase: true\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description style Literal['snake_case', 'none'] No <code>snake_case</code> Naming style: 'snake_case' converts spaces/special chars to underscores lowercase bool No <code>True</code> Convert names to lowercase remove_special bool No <code>True</code> Remove special characters except underscores"},{"location":"reference/yaml_schema/#normalize_schema-normalizeschemaparams","title":"<code>normalize_schema</code> (NormalizeSchemaParams)","text":"<p>Structural transformation to rename, drop, and reorder columns.</p> <p>Note: This is one of the few that might behave better with native API in some cases, but SQL projection handles it perfectly and is consistent.</p> <p>Configuration for schema normalization.</p> <p>Example:</p> <pre><code>normalize_schema:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description rename Optional[Dict[str, str]] No <code>PydanticUndefined</code> old_name -&gt; new_name drop Optional[List[str]] No <code>PydanticUndefined</code> Columns to remove; ignored if not present select_order Optional[List[str]] No - Final column order; any missing columns appended after"},{"location":"reference/yaml_schema/#rename_columns-renamecolumnsparams","title":"<code>rename_columns</code> (RenameColumnsParams)","text":"<p>Renames columns according to the provided mapping. Columns not in the mapping are kept unchanged.</p> <p>Configuration for bulk column renaming.</p> <p>Example:</p> <pre><code>rename_columns:\n  mapping:\n    customer_id: cust_id\n    order_date: date\n    total_amount: amount\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description mapping Dict[str, str] Yes - Map of old column name to new column name"},{"location":"reference/yaml_schema/#replace_values-replacevaluesparams","title":"<code>replace_values</code> (ReplaceValuesParams)","text":"<p>Replaces values in specified columns according to the mapping. Supports replacing to NULL.</p> <p>Configuration for bulk value replacement.</p> <p>Example - Standardize nulls:</p> <pre><code>replace_values:\n  columns: [\"status\", \"category\"]\n  mapping:\n    \"N/A\": null\n    \"\": null\n    \"Unknown\": null\n</code></pre> <p>Example - Code replacement:</p> <pre><code>replace_values:\n  columns: [\"country_code\"]\n  mapping:\n    \"US\": \"USA\"\n    \"UK\": \"GBR\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to apply replacements to mapping Dict[str, Optional[str]] Yes - Map of old value to new value (use null for NULL)"},{"location":"reference/yaml_schema/#sample-sampleparams","title":"<code>sample</code> (SampleParams)","text":"<p>Samples data using random filtering.</p> <p>Configuration for random sampling.</p> <p>Example:</p> <pre><code>sample:\n  fraction: 0.1\n  seed: 42\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description fraction float Yes - Fraction of rows to return (0.0 to 1.0) seed Optional[int] No - -"},{"location":"reference/yaml_schema/#select_columns-selectcolumnsparams","title":"<code>select_columns</code> (SelectColumnsParams)","text":"<p>Keeps only the specified columns, dropping all others.</p> <p>Configuration for selecting specific columns (whitelist).</p> <p>Example:</p> <pre><code>select_columns:\n  columns: [\"id\", \"name\", \"created_at\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to keep"},{"location":"reference/yaml_schema/#sort-sortparams","title":"<code>sort</code> (SortParams)","text":"<p>Sorts the dataset.</p> <p>Configuration for sorting.</p> <p>Example:</p> <pre><code>sort:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description by str | List[str] Yes - Column(s) to sort by ascending bool No <code>True</code> Sort order"},{"location":"reference/yaml_schema/#split_part-splitpartparams","title":"<code>split_part</code> (SplitPartParams)","text":"<p>Extracts the Nth part of a string after splitting by a delimiter.</p> <p>Configuration for splitting strings.</p> <p>Example:</p> <pre><code>split_part:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - Column to split delimiter str Yes - Delimiter to split by index int Yes - 1-based index of the token to extract"},{"location":"reference/yaml_schema/#trim_whitespace-trimwhitespaceparams","title":"<code>trim_whitespace</code> (TrimWhitespaceParams)","text":"<p>Trims leading and trailing whitespace from string columns.</p> <p>Configuration for trimming whitespace from string columns.</p> <p>Example - All string columns:</p> <pre><code>trim_whitespace: {}\n</code></pre> <p>Example - Specific columns:</p> <pre><code>trim_whitespace:\n  columns: [\"name\", \"address\", \"city\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to trim (default: all string columns detected at runtime)"},{"location":"reference/yaml_schema/#relational-algebra","title":"\ud83d\udcc2 Relational Algebra","text":""},{"location":"reference/yaml_schema/#aggregate-aggregateparams","title":"<code>aggregate</code> (AggregateParams)","text":"<p>Performs grouping and aggregation via SQL.</p> <p>Configuration for aggregation.</p> <p>Example:</p> <pre><code>aggregate:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - Columns to group by aggregations Dict[str, AggFunc] Yes - Map of column to aggregation function (sum, avg, min, max, count)"},{"location":"reference/yaml_schema/#join-joinparams","title":"<code>join</code> (JoinParams)","text":"<p>Joins the current dataset with another dataset from the context.</p> <p>Configuration for joining datasets.</p> <p>Scenario 1: Simple Left Join</p> <pre><code>join:\n  right_dataset: \"customers\"\n  on: \"customer_id\"\n  how: \"left\"\n</code></pre> <p>Scenario 2: Join with Prefix (avoid collisions)</p> <pre><code>join:\n  right_dataset: \"orders\"\n  on: [\"user_id\"]\n  how: \"inner\"\n  prefix: \"ord\"  # Result cols: ord_date, ord_amount...\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description right_dataset str Yes - Name of the node/dataset to join with on str | List[str] Yes - Column(s) to join on how Literal['inner', 'left', 'right', 'full', 'cross', 'anti', 'semi'] No <code>left</code> Join type prefix Optional[str] No - Prefix for columns from right dataset to avoid collisions"},{"location":"reference/yaml_schema/#pivot-pivotparams","title":"<code>pivot</code> (PivotParams)","text":"<p>Pivots row values into columns.</p> <p>Configuration for pivoting data.</p> <p>Example:</p> <pre><code>pivot:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n</code></pre> <p>Example (Optimized for Spark):</p> <pre><code>pivot:\n  group_by: [\"id\"]\n  pivot_col: \"category\"\n  values: [\"A\", \"B\", \"C\"]  # Explicit values avoid extra pass\n  agg_col: \"amount\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - - pivot_col str Yes - - agg_col str Yes - - agg_func Literal['sum', 'count', 'avg', 'max', 'min', 'first'] No <code>sum</code> - values Optional[List[str]] No - Specific values to pivot (for Spark optimization)"},{"location":"reference/yaml_schema/#union-unionparams","title":"<code>union</code> (UnionParams)","text":"<p>Unions current dataset with others.</p> <p>Configuration for unioning datasets.</p> <p>Example (By Name - Default):</p> <pre><code>union:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true\n</code></pre> <p>Example (By Position):</p> <pre><code>union:\n  datasets: [\"legacy_data\"]\n  by_name: false\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description datasets List[str] Yes - List of node names to union with current by_name bool No <code>True</code> Match columns by name (UNION ALL BY NAME)"},{"location":"reference/yaml_schema/#unpivot-unpivotparams","title":"<code>unpivot</code> (UnpivotParams)","text":"<p>Unpivots columns into rows (Melt/Stack).</p> <p>Configuration for unpivoting (melting) data.</p> <p>Example:</p> <pre><code>unpivot:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description id_cols List[str] Yes - - value_vars List[str] Yes - - var_name str No <code>variable</code> - value_name str No <code>value</code> -"},{"location":"reference/yaml_schema/#data-quality","title":"\ud83d\udcc2 Data Quality","text":""},{"location":"reference/yaml_schema/#cross_check-crosscheckparams","title":"<code>cross_check</code> (CrossCheckParams)","text":"<p>Perform cross-node validation checks.</p> <p>Does not return a DataFrame (returns None). Raises ValidationError on failure.</p> <p>Configuration for cross-node validation checks.</p> <p>Example (Row Count Mismatch):</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre> <p>Example (Schema Match):</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"schema_match\"\n  inputs: [\"staging_orders\", \"prod_orders\"]\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description type str Yes - Check type: 'row_count_diff', 'schema_match' inputs List[str] Yes - List of node names to compare threshold float No <code>0.0</code> Threshold for diff (0.0-1.0)"},{"location":"reference/yaml_schema/#warehousing-patterns","title":"\ud83d\udcc2 Warehousing Patterns","text":""},{"location":"reference/yaml_schema/#auditcolumnsconfig","title":"AuditColumnsConfig","text":"<p>Back to Catalog</p> Field Type Required Default Description created_col Optional[str] No - Column to set only on first insert updated_col Optional[str] No - Column to update on every merge"},{"location":"reference/yaml_schema/#merge-mergeparams","title":"<code>merge</code> (MergeParams)","text":"<p>Merge transformer implementation. Handles Upsert, Append-Only, and Delete-Match strategies.</p> <p>Configuration for Merge transformer (Upsert/Append).</p>"},{"location":"reference/yaml_schema/#gdpr-compliance-guide","title":"\u2696\ufe0f \"GDPR &amp; Compliance\" Guide","text":"<p>Business Problem: \"A user exercised their 'Right to be Forgotten'. We need to remove them from our Silver tables immediately.\"</p> <p>The Solution: Use the <code>delete_match</code> strategy. The source dataframe contains the IDs to be deleted, and the transformer removes them from the target.</p> <p>Recipe 1: Right to be Forgotten (Delete)</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"delete_match\"\n</code></pre> <p>Recipe 2: Conditional Update (SCD Type 1) \"Only update if the source record is newer than the target record.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.products\"\n  keys: [\"product_id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n</code></pre> <p>Recipe 3: Safe Insert (Filter Bad Records) \"Only insert records that are not marked as deleted.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.orders\"\n  keys: [\"order_id\"]\n  strategy: \"append_only\"\n  insert_condition: \"source.is_deleted = false\"\n</code></pre> <p>Recipe 4: Audit Columns \"Track when records were created or updated.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.users\"\n  keys: [\"user_id\"]\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre> <p>Recipe 5: Full Sync (Insert + Update + Delete) \"Sync target with source: insert new, update changed, and remove soft-deleted.\"</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  # 1. Delete if source says so\n  delete_condition: \"source.is_deleted = true\"\n  # 2. Update if changed (and not deleted)\n  update_condition: \"source.hash != target.hash\"\n  # 3. Insert new (and not deleted)\n  insert_condition: \"source.is_deleted = false\"\n</code></pre> <p>Strategies: *   upsert (Default): Update existing records, insert new ones. *   append_only: Ignore duplicates, only insert new keys. *   delete_match: Delete records in target that match keys in source. Back to Catalog</p> Field Type Required Default Description target str Yes - Target table name or path keys List[str] Yes - List of join keys strategy MergeStrategy No <code>MergeStrategy.UPSERT</code> Merge behavior: 'upsert', 'append_only', 'delete_match' audit_cols Optional[AuditColumnsConfig] No - {'created_col': '...', 'updated_col': '...'} optimize_write bool No <code>False</code> Run OPTIMIZE after write (Spark) zorder_by Optional[List[str]] No - Columns to Z-Order by cluster_by Optional[List[str]] No - Columns to Liquid Cluster by (Delta) update_condition Optional[str] No - SQL condition for update clause (e.g. 'source.ver &gt; target.ver') insert_condition Optional[str] No - SQL condition for insert clause (e.g. 'source.status != \"deleted\"') delete_condition Optional[str] No - SQL condition for delete clause (e.g. 'source.status = \"deleted\"')"},{"location":"reference/yaml_schema/#scd2-scd2params","title":"<code>scd2</code> (SCD2Params)","text":"<p>Implements SCD Type 2 Logic.</p> <p>Returns the FULL history dataset (to be written via Overwrite).</p> <p>Parameters for SCD Type 2 (Slowly Changing Dimensions) transformer.</p>"},{"location":"reference/yaml_schema/#the-time-machine-pattern","title":"\ud83d\udd70\ufe0f The \"Time Machine\" Pattern","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: SCD Type 2 tracks the full history of changes. Each record has an \"effective window\" (start/end dates) and a flag indicating if it is the current version.</p> <p>Recipe:</p> <pre><code>transformer: \"scd2\"\nparams:\n  target: \"gold/customers\"         # Path to existing history\n  keys: [\"customer_id\"]            # How we identify the entity\n  track_cols: [\"address\", \"tier\"]  # What changes we care about\n  effective_time_col: \"txn_date\"   # When the change actually happened\n  end_time_col: \"valid_to\"         # (Optional) Name of closing timestamp\n  current_flag_col: \"is_active\"    # (Optional) Name of current flag\n</code></pre> <p>How it works: 1. Match: Finds existing records using <code>keys</code>. 2. Compare: Checks <code>track_cols</code> to see if data changed. 3. Close: If changed, updates the old record's <code>end_time_col</code> to the new <code>effective_time_col</code>. 4. Insert: Adds a new record with <code>effective_time_col</code> as start and open-ended end date. Back to Catalog</p> Field Type Required Default Description target str Yes - Target table name or path containing history keys List[str] Yes - Natural keys to identify unique entities track_cols List[str] Yes - Columns to monitor for changes effective_time_col str Yes - Source column indicating when the change occurred. end_time_col str No <code>valid_to</code> Name of the end timestamp column current_flag_col str No <code>is_current</code> Name of the current record flag column delete_col Optional[str] No - Column indicating soft deletion (boolean)"},{"location":"reference/yaml_schema/#advanced-feature-engineering","title":"\ud83d\udcc2 Advanced &amp; Feature Engineering","text":""},{"location":"reference/yaml_schema/#shiftdefinition","title":"ShiftDefinition","text":"<p>Definition of a single shift. Back to Catalog</p> Field Type Required Default Description name str Yes - Name of the shift (e.g., 'Day', 'Night') start str Yes - Start time in HH:MM format (e.g., '06:00') end str Yes - End time in HH:MM format (e.g., '14:00')"},{"location":"reference/yaml_schema/#deduplicate-deduplicateparams","title":"<code>deduplicate</code> (DeduplicateParams)","text":"<p>Deduplicates data using Window functions.</p> <p>Configuration for deduplication.</p> <p>Scenario: Keep latest record</p> <pre><code>deduplicate:\n  keys: [\"id\"]\n  order_by: \"updated_at DESC\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description keys List[str] Yes - List of columns to partition by (columns that define uniqueness) order_by Optional[str] No - SQL Order by clause (e.g. 'updated_at DESC') to determine which record to keep (first one is kept)"},{"location":"reference/yaml_schema/#dict_based_mapping-dictmappingparams","title":"<code>dict_based_mapping</code> (DictMappingParams)","text":"<p>Configuration for dictionary mapping.</p> <p>Scenario: Map status codes to labels</p> <pre><code>dict_based_mapping:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column to map values from mapping Dict[str, str | int | float | bool] Yes - Dictionary of source value -&gt; target value default str | int | float | bool No - Default value if source value is not found in mapping output_column Optional[str] No - Name of output column. If not provided, overwrites source column."},{"location":"reference/yaml_schema/#explode_list_column-explodeparams","title":"<code>explode_list_column</code> (ExplodeParams)","text":"<p>Configuration for exploding lists.</p> <p>Scenario: Flatten list of items per order</p> <pre><code>explode_list_column:\n  column: \"items\"\n  outer: true  # Keep orders with empty items list\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing the list/array to explode outer bool No <code>False</code> If True, keep rows with empty lists (explode_outer behavior). If False, drops them."},{"location":"reference/yaml_schema/#generate_surrogate_key-surrogatekeyparams","title":"<code>generate_surrogate_key</code> (SurrogateKeyParams)","text":"<p>Generates a deterministic surrogate key (MD5) from a combination of columns. Handles NULLs by treating them as empty strings to ensure consistency.</p> <p>Configuration for surrogate key generation.</p> <p>Example:</p> <pre><code>generate_surrogate_key:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to combine for the key separator str No <code>-</code> Separator between values output_col str No <code>surrogate_key</code> Name of the output column"},{"location":"reference/yaml_schema/#geocode-geocodeparams","title":"<code>geocode</code> (GeocodeParams)","text":"<p>Geocoding Stub.</p> <p>Configuration for geocoding.</p> <p>Example:</p> <pre><code>geocode:\n  address_col: \"full_address\"\n  output_col: \"lat_long\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description address_col str Yes - Column containing the address to geocode output_col str No <code>lat_long</code> Name of the output column for coordinates"},{"location":"reference/yaml_schema/#hash_columns-hashparams","title":"<code>hash_columns</code> (HashParams)","text":"<p>Hashes columns for PII/Anonymization.</p> <p>Configuration for column hashing.</p> <p>Example:</p> <pre><code>hash_columns:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to hash algorithm HashAlgorithm No <code>HashAlgorithm.SHA256</code> Hashing algorithm. Options: 'sha256', 'md5'"},{"location":"reference/yaml_schema/#normalize_json-normalizejsonparams","title":"<code>normalize_json</code> (NormalizeJsonParams)","text":"<p>Flattens a nested JSON/Struct column.</p> <p>Configuration for JSON normalization.</p> <p>Example:</p> <pre><code>normalize_json:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing nested JSON/Struct sep str No <code>_</code> Separator for nested fields (e.g., 'parent_child')"},{"location":"reference/yaml_schema/#parse_json-parsejsonparams","title":"<code>parse_json</code> (ParseJsonParams)","text":"<p>Parses a JSON string column into a Struct/Map column.</p> <p>Configuration for JSON parsing.</p> <p>Example:</p> <pre><code>parse_json:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - String column containing JSON json_schema str Yes - DDL schema string (e.g. 'a INT, b STRING') or Spark StructType DDL output_col Optional[str] No - -"},{"location":"reference/yaml_schema/#regex_replace-regexreplaceparams","title":"<code>regex_replace</code> (RegexReplaceParams)","text":"<p>SQL-based Regex replacement.</p> <p>Configuration for regex replacement.</p> <p>Example:</p> <pre><code>regex_replace:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column to apply regex replacement on pattern str Yes - Regex pattern to match replacement str Yes - String to replace matches with"},{"location":"reference/yaml_schema/#sessionize-sessionizeparams","title":"<code>sessionize</code> (SessionizeParams)","text":"<p>Assigns session IDs based on inactivity threshold.</p> <p>Configuration for sessionization.</p> <p>Example:</p> <pre><code>sessionize:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description timestamp_col str Yes - Timestamp column to calculate session duration from user_col str Yes - User identifier to partition sessions by threshold_seconds int No <code>1800</code> Inactivity threshold in seconds (default: 30 minutes). If gap &gt; threshold, new session starts. session_col str No <code>session_id</code> Output column name for the generated session ID"},{"location":"reference/yaml_schema/#split_events_by_period-spliteventsbyperiodparams","title":"<code>split_events_by_period</code> (SplitEventsByPeriodParams)","text":"<p>Splits events that span multiple time periods into individual segments.</p> <p>For events spanning multiple days/hours/shifts, this creates separate rows for each period with adjusted start/end times and recalculated durations.</p> <p>Configuration for splitting events that span multiple time periods.</p> <p>Splits events that span multiple days, hours, or shifts into individual segments per period. Useful for OEE/downtime analysis, billing, and time-based aggregations.</p> <p>Example - Split by day:</p> <pre><code>split_events_by_period:\n  start_col: \"Shutdown_Start_Time\"\n  end_col: \"Shutdown_End_Time\"\n  period: \"day\"\n  duration_col: \"Shutdown_Duration_Min\"\n</code></pre> <p>Example - Split by shift:</p> <pre><code>split_events_by_period:\n  start_col: \"event_start\"\n  end_col: \"event_end\"\n  period: \"shift\"\n  duration_col: \"duration_minutes\"\n  shifts:\n    - name: \"Day\"\n      start: \"06:00\"\n      end: \"14:00\"\n    - name: \"Swing\"\n      start: \"14:00\"\n      end: \"22:00\"\n    - name: \"Night\"\n      start: \"22:00\"\n      end: \"06:00\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description start_col str Yes - Column containing the event start timestamp end_col str Yes - Column containing the event end timestamp period str No <code>day</code> Period type to split by: 'day', 'hour', or 'shift' duration_col Optional[str] No - Output column name for duration in minutes. If not set, no duration column is added. shifts Optional[List[ShiftDefinition]] No - List of shift definitions (required when period='shift') shift_col Optional[str] No <code>shift_name</code> Output column name for shift name (only used when period='shift')"},{"location":"reference/yaml_schema/#unpack_struct-unpackstructparams","title":"<code>unpack_struct</code> (UnpackStructParams)","text":"<p>Flattens a struct/dict column into top-level columns.</p> <p>Configuration for unpacking structs.</p> <p>Example:</p> <pre><code>unpack_struct:\n  column: \"user_info\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Struct/Dictionary column to unpack/flatten into individual columns"},{"location":"reference/yaml_schema/#validate_and_flag-validateandflagparams","title":"<code>validate_and_flag</code> (ValidateAndFlagParams)","text":"<p>Validates rules and appends a column with a list/string of failed rule names.</p> <p>Configuration for validation flagging.</p> <p>Example:</p> <pre><code>validate_and_flag:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description rules Dict[str, str] Yes - Map of rule name to SQL condition (must be TRUE) flag_col str No <code>_issues</code> Name of the column to store failed rules"},{"location":"reference/yaml_schema/#window_calculation-windowcalculationparams","title":"<code>window_calculation</code> (WindowCalculationParams)","text":"<p>Generic wrapper for Window functions.</p> <p>Configuration for window functions.</p> <p>Example:</p> <pre><code>window_calculation:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre> <p>Back to Catalog</p> Field Type Required Default Description target_col str Yes - - function str Yes - Window function e.g. 'sum(amount)', 'rank()' partition_by List[str] No <code>PydanticUndefined</code> - order_by Optional[str] No - -"},{"location":"reference/yaml_schema/#semantic-layer","title":"Semantic Layer","text":""},{"location":"reference/yaml_schema/#semantic-layer_1","title":"Semantic Layer","text":"<p>The semantic layer provides a unified interface for defining and querying business metrics. Define metrics once, query them by name across dimensions.</p> <p>Core Components: - MetricDefinition: Define aggregation expressions (SUM, COUNT, AVG) - DimensionDefinition: Define grouping attributes with hierarchies - MaterializationConfig: Pre-compute metrics at specific grain - SemanticQuery: Execute queries like \"revenue BY region, month\" - Project: Unified API that connects pipelines and semantic layer</p> <p>Unified Project API (Recommended):</p> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>YAML Configuration:</p> <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: gold.fact_orders    # connection.table notation\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: gold.dim_customer\n      column: region\n\nmaterializations:\n  - name: monthly_revenue\n    metrics: [revenue]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre> <p>The <code>source: gold.fact_orders</code> notation resolves paths automatically from connections.</p>"},{"location":"reference/yaml_schema/#dimensiondefinition","title":"<code>DimensionDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic dimension.</p> <p>A dimension represents an attribute for grouping and filtering metrics (e.g., date, product, region).</p> <p>Attributes:     name: Unique dimension identifier     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.dim_customer</code>         - <code>connection.path</code>: e.g., <code>gold.dim_customer</code> or <code>gold.dims/customer</code>         - <code>table_name</code>: Uses default connection     column: Column name in source (defaults to name)     hierarchy: Optional ordered list of columns for drill-down     description: Human-readable description | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique dimension identifier | | source | str | Yes | - | Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.dim_customer), connection.path (e.g., gold.dim_customer or gold.dims/customer), or bare table_name | | column | Optional[str] | No | - | Column name (defaults to name) | | hierarchy | List[str] | No | <code>PydanticUndefined</code> | Drill-down hierarchy | | description | Optional[str] | No | - | Human-readable description |</p>"},{"location":"reference/yaml_schema/#materializationconfig","title":"<code>MaterializationConfig</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Configuration for materializing metrics to a table.</p> <p>Materialization pre-computes aggregated metrics at a specific grain and persists them for faster querying.</p> <p>Attributes:     name: Unique materialization identifier     metrics: List of metric names to include     dimensions: List of dimension names (determines grain)     output: Output table path     schedule: Optional cron schedule for refresh     incremental: Configuration for incremental refresh | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique materialization identifier | | metrics | List[str] | Yes | - | Metrics to materialize | | dimensions | List[str] | Yes | - | Dimensions for grouping | | output | str | Yes | - | Output table path | | schedule | Optional[str] | No | - | Cron schedule | | incremental | Optional[Dict[str, Any]] | No | - | Incremental refresh config |</p>"},{"location":"reference/yaml_schema/#metricdefinition","title":"<code>MetricDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic metric.</p> <p>A metric represents a measurable value that can be aggregated across dimensions (e.g., revenue, order_count, avg_order_value).</p> <p>Attributes:     name: Unique metric identifier     description: Human-readable description     expr: SQL aggregation expression (e.g., \"SUM(total_amount)\")     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.fact_orders</code>         - <code>connection.path</code>: e.g., <code>gold.fact_orders</code> or <code>gold.oee/plant_a/metrics</code>         - <code>table_name</code>: Uses default connection     filters: Optional WHERE conditions to apply     type: \"simple\" (direct aggregation) or \"derived\" (references other metrics) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique metric identifier | | description | Optional[str] | No | - | Human-readable description | | expr | str | Yes | - | SQL aggregation expression | | source | Optional[str] | No | - | Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.fact_orders), connection.path (e.g., gold.fact_orders or gold.oee/plant_a/table), or bare table_name | | filters | List[str] | No | <code>PydanticUndefined</code> | WHERE conditions | | type | MetricType | No | <code>MetricType.SIMPLE</code> | Metric type |</p>"},{"location":"reference/yaml_schema/#semanticlayerconfig","title":"<code>SemanticLayerConfig</code>","text":"<p>Complete semantic layer configuration.</p> <p>Contains all metrics, dimensions, and materializations for a semantic layer deployment.</p> <p>Attributes:     metrics: List of metric definitions     dimensions: List of dimension definitions     materializations: List of materialization configurations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | metrics | List[MetricDefinition] | No | <code>PydanticUndefined</code> | Metric definitions | | dimensions | List[DimensionDefinition] | No | <code>PydanticUndefined</code> | Dimension definitions | | materializations | List[MaterializationConfig] | No | <code>PydanticUndefined</code> | Materialization configs |</p>"},{"location":"reference/yaml_schema/#fk-validation","title":"FK Validation","text":""},{"location":"reference/yaml_schema/#fk-validation_1","title":"FK Validation","text":"<p>Declare and validate referential integrity between fact and dimension tables.</p> <p>Features: - Declare relationships in YAML - Validate FK constraints on fact load - Detect orphan records - Generate lineage from relationships</p> <p>Example:</p> <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    on_violation: error\n</code></pre>"},{"location":"reference/yaml_schema/#relationshipconfig","title":"<code>RelationshipConfig</code>","text":"<p>Used in: RelationshipRegistry</p> <p>Configuration for a foreign key relationship.</p> <p>Attributes:     name: Unique relationship identifier     fact: Fact table name     dimension: Dimension table name     fact_key: Foreign key column in fact table     dimension_key: Primary/surrogate key column in dimension     nullable: Whether nulls are allowed in fact_key     on_violation: Action on violation (\"warn\", \"error\", \"quarantine\") | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique relationship identifier | | fact | str | Yes | - | Fact table name | | dimension | str | Yes | - | Dimension table name | | fact_key | str | Yes | - | FK column in fact table | | dimension_key | str | Yes | - | PK/SK column in dimension | | nullable | bool | No | <code>False</code> | Allow nulls in fact_key | | on_violation | str | No | <code>error</code> | Action on violation |</p>"},{"location":"reference/yaml_schema/#relationshipregistry","title":"<code>RelationshipRegistry</code>","text":"<p>Registry of all declared relationships.</p> <p>Attributes:     relationships: List of relationship configurations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | relationships | List[RelationshipConfig] | No | <code>PydanticUndefined</code> | Relationship definitions |</p>"},{"location":"reference/yaml_schema/#data-patterns","title":"Data Patterns","text":""},{"location":"reference/yaml_schema/#data-patterns_1","title":"Data Patterns","text":"<p>Declarative patterns for common data warehouse building blocks. Patterns encapsulate best practices for dimensional modeling, ensuring consistent implementation across your data warehouse.</p>"},{"location":"reference/yaml_schema/#dimensionpattern","title":"DimensionPattern","text":"<p>Build complete dimension tables with surrogate keys and SCD (Slowly Changing Dimension) support.</p> <p>When to Use: - Building dimension tables from source systems (customers, products, locations) - Need surrogate keys for star schema joins - Need to track historical changes (SCD Type 2)</p> <p>Beginner Note: Dimensions are the \"who, what, where, when\" of your data warehouse. A customer dimension has customer_id (natural key) and customer_sk (surrogate key). Fact tables join to dimensions via surrogate keys.</p> <p>See Also: FactPattern, DateDimensionPattern</p> <p>Features: - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> <p>Params:</p> Parameter Type Required Description <code>natural_key</code> str Yes Natural/business key column name <code>surrogate_key</code> str Yes Surrogate key column name to generate <code>scd_type</code> int No 0=static, 1=overwrite, 2=history (default: 1) <code>track_columns</code> list SCD1/2 Columns to track for change detection <code>target</code> str SCD2 Target table path to read existing history <code>unknown_member</code> bool No Insert row with SK=0 for orphan handling <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column with value <p>Supported Target Formats: - Spark: catalog.table, Delta paths, .parquet, .csv, .json, .orc - Pandas: .parquet, .csv, .json, .xlsx, .feather, .pickle</p> <p>Example:</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_columns: [name, email, address, city]\n    target: warehouse.dim_customer\n    unknown_member: true\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre>"},{"location":"reference/yaml_schema/#datedimensionpattern","title":"DateDimensionPattern","text":"<p>Generate a complete date dimension table with pre-calculated attributes for BI/reporting.</p> <p>When to Use: - Every data warehouse needs a date dimension for time-based analytics - Enable date filtering, grouping by week/month/quarter, fiscal year reporting</p> <p>Beginner Note: The date dimension is foundational for any BI/reporting system. It lets you query \"sales by month\" or \"orders in fiscal Q2\" without complex date calculations.</p> <p>See Also: DimensionPattern</p> <p>Features: - Generates all dates in a range with rich attributes - Calendar and fiscal year support - ISO week numbering - Weekend/month-end flags</p> <p>Params:</p> Parameter Type Required Description <code>start_date</code> str Yes Start date (YYYY-MM-DD) <code>end_date</code> str Yes End date (YYYY-MM-DD) <code>date_key_format</code> str No Format for date_sk (default: yyyyMMdd) <code>fiscal_year_start_month</code> int No Month fiscal year starts (1-12, default: 1) <code>unknown_member</code> bool No Add unknown date row with date_sk=0 <p>Generated Columns: <code>date_sk</code>, <code>full_date</code>, <code>day_of_week</code>, <code>day_of_week_num</code>, <code>day_of_month</code>, <code>day_of_year</code>, <code>is_weekend</code>, <code>week_of_year</code>, <code>month</code>, <code>month_name</code>, <code>quarter</code>, <code>quarter_name</code>, <code>year</code>, <code>fiscal_year</code>, <code>fiscal_quarter</code>, <code>is_month_start</code>, <code>is_month_end</code>, <code>is_year_start</code>, <code>is_year_end</code></p> <p>Example:</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    fiscal_year_start_month: 7\n    unknown_member: true\n</code></pre>"},{"location":"reference/yaml_schema/#factpattern","title":"FactPattern","text":"<p>Build fact tables with automatic surrogate key lookups from dimensions.</p> <p>When to Use: - Building fact tables from transactional data (orders, events, transactions) - Need to look up surrogate keys from dimension tables - Need to handle orphan records (missing dimension matches)</p> <p>Beginner Note: Facts are the \"how much, how many\" of your data warehouse. An orders fact has measures (quantity, revenue) and dimension keys (customer_sk, product_sk). The pattern automatically looks up SKs from dimensions.</p> <p>See Also: DimensionPattern, QuarantineConfig</p> <p>Features: - Automatic SK lookups from dimension tables (with SCD2 current-record filtering) - Orphan handling: unknown (SK=0), reject (error), quarantine (route to table) - Grain validation (detect duplicates) - Calculated measures and column renaming - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list No Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No Dimension lookup configurations (see below) <code>orphan_handling</code> str No \"unknown\" | \"reject\" | \"quarantine\" (default: unknown) <code>quarantine</code> dict quarantine Quarantine config (see below) <code>measures</code> list No Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No Remove duplicates before processing <code>keys</code> list dedupe Keys for deduplication <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Dimension Lookup Config:</p> <pre><code>dimensions:\n  - source_column: customer_id      # Column in source fact\n    dimension_table: dim_customer   # Dimension in context\n    dimension_key: customer_id      # Natural key in dimension\n    surrogate_key: customer_sk      # SK to retrieve\n    scd2: true                      # Filter is_current=true\n</code></pre> <p>Quarantine Config (for orphan_handling: quarantine):</p> <pre><code>quarantine:\n  connection: silver                # Required: connection name\n  path: fact_orders_orphans         # OR table: quarantine_table\n  add_columns:\n    _rejection_reason: true         # Add rejection reason\n    _rejected_at: true              # Add rejection timestamp\n    _source_dimension: true         # Add dimension name\n</code></pre> <p>Example:</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n        scd2: true\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n    orphan_handling: unknown\n    measures:\n      - quantity\n      - revenue: \"quantity * unit_price\"\n    audit:\n      load_timestamp: true\n      source_system: \"pos\"\n</code></pre>"},{"location":"reference/yaml_schema/#aggregationpattern","title":"AggregationPattern","text":"<p>Declarative aggregation with GROUP BY and optional incremental merge.</p> <p>When to Use: - Building summary/aggregate tables (daily sales, monthly metrics) - Need incremental aggregation (update existing aggregates) - Gold layer reporting tables</p> <p>Beginner Note: Aggregations summarize facts at a higher grain. Example: daily_sales aggregates orders by date with SUM(revenue).</p> <p>See Also: FactPattern</p> <p>Features: - Declare grain (GROUP BY columns) - Define measures with SQL aggregation expressions - Optional HAVING filter - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list Yes Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes Measure definitions with name and expr <code>having</code> str No HAVING clause for filtering aggregates <code>incremental.timestamp_column</code> str No Column to identify new data <code>incremental.merge_strategy</code> str No \"replace\", \"sum\", \"min\", or \"max\" <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Example:</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk, region]\n    measures:\n      - name: total_revenue\n        expr: \"SUM(total_amount)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n      - name: avg_order_value\n        expr: \"AVG(total_amount)\"\n    having: \"COUNT(*) &gt; 0\"\n    audit:\n      load_timestamp: true\n</code></pre>"},{"location":"reference/yaml_schema/#auditconfig","title":"<code>AuditConfig</code>","text":"<p>Configuration for audit columns. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | load_timestamp | bool | No | <code>True</code> | Add load_timestamp column | | source_system | Optional[str] | No | - | Source system name for source_system column |</p>"},{"location":"reference/api/cli/","title":"CLI API","text":""},{"location":"reference/api/cli/#odibi.cli.main","title":"<code>odibi.cli.main</code>","text":"<p>Main CLI entry point.</p>"},{"location":"reference/api/cli/#odibi.cli.main.main","title":"<code>main()</code>","text":"<p>Main CLI entry point.</p> Source code in <code>odibi\\cli\\main.py</code> <pre><code>def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    # Configure telemetry early\n    setup_telemetry()\n\n    parser = argparse.ArgumentParser(\n        description=\"Odibi Data Pipeline Framework\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  odibi run config.yaml                    Run a pipeline\n  odibi validate config.yaml               Validate configuration\n  odibi graph config.yaml                  Visualize dependencies\n  odibi story generate config.yaml        Generate documentation\n  odibi story diff run1.json run2.json    Compare two runs\n  odibi story list                         List story files\n        \"\"\",\n    )\n\n    # Global arguments\n    parser.add_argument(\n        \"--log-level\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n        default=\"INFO\",\n        help=\"Set logging verbosity (default: INFO)\",\n    )\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # odibi run\n    run_parser = subparsers.add_parser(\"run\", help=\"Execute pipeline\")\n    run_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    run_parser.add_argument(\n        \"--env\", default=\"development\", help=\"Environment (development/production)\"\n    )\n    run_parser.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Simulate execution without running operations\"\n    )\n    run_parser.add_argument(\n        \"--resume\", action=\"store_true\", help=\"Resume from last failure (skip successful nodes)\"\n    )\n    run_parser.add_argument(\n        \"--parallel\", action=\"store_true\", help=\"Run independent nodes in parallel\"\n    )\n    run_parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=4,\n        help=\"Number of worker threads for parallel execution (default: 4)\",\n    )\n    run_parser.add_argument(\n        \"--on-error\",\n        choices=[\"fail_fast\", \"fail_later\", \"ignore\"],\n        help=\"Override error handling strategy\",\n    )\n    run_parser.add_argument(\n        \"--tag\",\n        help=\"Filter nodes by tag (e.g., --tag daily)\",\n    )\n    run_parser.add_argument(\n        \"--pipeline\",\n        dest=\"pipeline_name\",\n        help=\"Run specific pipeline by name\",\n    )\n    run_parser.add_argument(\n        \"--node\",\n        dest=\"node_name\",\n        help=\"Run specific node by name\",\n    )\n\n    # odibi deploy\n    deploy_parser = subparsers.add_parser(\"deploy\", help=\"Deploy definitions to System Catalog\")\n    deploy_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    deploy_parser.add_argument(\n        \"--env\", default=\"development\", help=\"Environment (development/production)\"\n    )\n\n    # odibi validate\n    validate_parser = subparsers.add_parser(\"validate\", help=\"Validate config\")\n    validate_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n\n    # odibi test\n    test_parser = subparsers.add_parser(\"test\", help=\"Run unit tests for transformations\")\n    test_parser.add_argument(\n        \"path\", nargs=\"?\", default=\"tests\", help=\"Path to tests directory or file (default: tests)\"\n    )\n    test_parser.add_argument(\"--snapshot\", action=\"store_true\", help=\"Update snapshots for tests\")\n\n    # odibi docs\n    subparsers.add_parser(\"docs\", help=\"Generate API documentation\")\n\n    # odibi graph\n    graph_parser = subparsers.add_parser(\"graph\", help=\"Visualize dependency graph\")\n    graph_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    graph_parser.add_argument(\"--pipeline\", help=\"Pipeline name (optional)\")\n    graph_parser.add_argument(\n        \"--format\",\n        choices=[\"ascii\", \"dot\", \"mermaid\"],\n        default=\"ascii\",\n        help=\"Output format (default: ascii)\",\n    )\n    graph_parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose output\")\n\n    # odibi story\n    add_story_parser(subparsers)\n\n    # odibi secrets\n    add_secrets_parser(subparsers)\n\n    # odibi init-pipeline (create/init)\n    add_init_parser(subparsers)\n\n    # odibi doctor\n    add_doctor_parser(subparsers)\n\n    # odibi ui\n    add_ui_parser(subparsers)\n\n    # odibi export\n    add_export_parser(subparsers)\n\n    # odibi catalog\n    add_catalog_parser(subparsers)\n\n    # odibi schema\n    add_schema_parser(subparsers)\n\n    # odibi lineage\n    add_lineage_parser(subparsers)\n\n    args = parser.parse_args()\n\n    # Configure logging\n    import logging\n\n    logging.basicConfig(\n        level=getattr(logging, args.log_level),\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    if args.command == \"run\":\n        return run_command(args)\n    elif args.command == \"deploy\":\n        from odibi.cli.deploy import deploy_command\n\n        return deploy_command(args)\n    elif args.command == \"docs\":\n        generate_docs()\n        return 0\n    elif args.command == \"validate\":\n        return validate_command(args)\n    elif args.command == \"test\":\n        return test_command(args)\n    elif args.command == \"graph\":\n        return graph_command(args)\n    elif args.command == \"story\":\n        return story_command(args)\n    elif args.command == \"secrets\":\n        return secrets_command(args)\n    elif args.command in [\"init-pipeline\", \"create\", \"init\", \"generate-project\"]:\n        return init_pipeline_command(args)\n    elif args.command == \"doctor\":\n        return doctor_command(args)\n    elif args.command == \"ui\":\n        return ui_command(args)\n    elif args.command == \"export\":\n        return export_command(args)\n    elif args.command == \"catalog\":\n        return catalog_command(args)\n    elif args.command == \"schema\":\n        return schema_command(args)\n    elif args.command == \"lineage\":\n        return lineage_command(args)\n    else:\n        parser.print_help()\n        return 1\n</code></pre>"},{"location":"reference/api/connections/","title":"Connections API","text":""},{"location":"reference/api/connections/#odibi.connections.base","title":"<code>odibi.connections.base</code>","text":"<p>Base connection interface.</p>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection","title":"<code>BaseConnection</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connections.</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>class BaseConnection(ABC):\n    \"\"\"Abstract base class for connections.\"\"\"\n\n    @abstractmethod\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full path for a relative path.\n\n        Args:\n            relative_path: Relative path or table name\n\n        Returns:\n            Full path to resource\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate(self) -&gt; None:\n        \"\"\"Validate connection configuration.\n\n        Raises:\n            ConnectionError: If validation fails\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection.get_path","title":"<code>get_path(relative_path)</code>  <code>abstractmethod</code>","text":"<p>Get full path for a relative path.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>Relative path or table name</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path to resource</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>@abstractmethod\ndef get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full path for a relative path.\n\n    Args:\n        relative_path: Relative path or table name\n\n    Returns:\n        Full path to resource\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection.validate","title":"<code>validate()</code>  <code>abstractmethod</code>","text":"<p>Validate connection configuration.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If validation fails</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>@abstractmethod\ndef validate(self) -&gt; None:\n    \"\"\"Validate connection configuration.\n\n    Raises:\n        ConnectionError: If validation fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local","title":"<code>odibi.connections.local</code>","text":"<p>Local filesystem connection.</p>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection","title":"<code>LocalConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>class LocalConnection(BaseConnection):\n    \"\"\"Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).\"\"\"\n\n    def __init__(self, base_path: str = \"./data\"):\n        \"\"\"Initialize local connection.\n\n        Args:\n            base_path: Base directory for all paths (can be local path or URI)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"local\",\n            connection_name=\"LocalConnection\",\n            action=\"init\",\n            base_path=base_path,\n        )\n\n        self.base_path_str = base_path\n        self.is_uri = \"://\" in base_path or \":/\" in base_path\n\n        if not self.is_uri:\n            self.base_path = Path(base_path)\n            ctx.debug(\n                \"LocalConnection initialized with filesystem path\",\n                base_path=base_path,\n                is_uri=False,\n            )\n        else:\n            self.base_path = None  # Not used for URIs\n            ctx.debug(\n                \"LocalConnection initialized with URI path\",\n                base_path=base_path,\n                is_uri=True,\n            )\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full path for a relative path.\n\n        Args:\n            relative_path: Relative path from base\n\n        Returns:\n            Full absolute path or URI\n        \"\"\"\n        ctx = get_logging_context()\n\n        if self.is_uri:\n            # Use os.path for simple string joining, handling slashes manually for consistency\n            # Strip leading slash from relative to avoid root replacement\n            clean_rel = relative_path.lstrip(\"/\").lstrip(\"\\\\\")\n            # Handle cases where base_path might not have trailing slash\n            if self.base_path_str.endswith(\"/\") or self.base_path_str.endswith(\"\\\\\"):\n                full_path = f\"{self.base_path_str}{clean_rel}\"\n            else:\n                # Use forward slash for URIs\n                full_path = f\"{self.base_path_str}/{clean_rel}\"\n\n            ctx.debug(\n                \"Resolved URI path\",\n                relative_path=relative_path,\n                full_path=full_path,\n            )\n            return full_path\n        else:\n            # Standard local path logic\n            full_path = self.base_path / relative_path\n            resolved = str(full_path.absolute())\n\n            ctx.debug(\n                \"Resolved local path\",\n                relative_path=relative_path,\n                full_path=resolved,\n            )\n            return resolved\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate that base path exists or can be created.\n\n        Raises:\n            ConnectionError: If validation fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating LocalConnection\",\n            base_path=self.base_path_str,\n            is_uri=self.is_uri,\n        )\n\n        if self.is_uri:\n            # Cannot validate/create URIs with local os module\n            # Assume valid or handled by engine\n            ctx.debug(\n                \"Skipping URI validation (handled by engine)\",\n                base_path=self.base_path_str,\n            )\n        else:\n            # Create base directory if it doesn't exist\n            try:\n                self.base_path.mkdir(parents=True, exist_ok=True)\n                ctx.info(\n                    \"LocalConnection validated successfully\",\n                    base_path=str(self.base_path.absolute()),\n                    created=not self.base_path.exists(),\n                )\n            except Exception as e:\n                ctx.error(\n                    \"LocalConnection validation failed\",\n                    base_path=self.base_path_str,\n                    error=str(e),\n                )\n                raise\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.__init__","title":"<code>__init__(base_path='./data')</code>","text":"<p>Initialize local connection.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Base directory for all paths (can be local path or URI)</p> <code>'./data'</code> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def __init__(self, base_path: str = \"./data\"):\n    \"\"\"Initialize local connection.\n\n    Args:\n        base_path: Base directory for all paths (can be local path or URI)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"local\",\n        connection_name=\"LocalConnection\",\n        action=\"init\",\n        base_path=base_path,\n    )\n\n    self.base_path_str = base_path\n    self.is_uri = \"://\" in base_path or \":/\" in base_path\n\n    if not self.is_uri:\n        self.base_path = Path(base_path)\n        ctx.debug(\n            \"LocalConnection initialized with filesystem path\",\n            base_path=base_path,\n            is_uri=False,\n        )\n    else:\n        self.base_path = None  # Not used for URIs\n        ctx.debug(\n            \"LocalConnection initialized with URI path\",\n            base_path=base_path,\n            is_uri=True,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get full path for a relative path.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>Relative path from base</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full absolute path or URI</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full path for a relative path.\n\n    Args:\n        relative_path: Relative path from base\n\n    Returns:\n        Full absolute path or URI\n    \"\"\"\n    ctx = get_logging_context()\n\n    if self.is_uri:\n        # Use os.path for simple string joining, handling slashes manually for consistency\n        # Strip leading slash from relative to avoid root replacement\n        clean_rel = relative_path.lstrip(\"/\").lstrip(\"\\\\\")\n        # Handle cases where base_path might not have trailing slash\n        if self.base_path_str.endswith(\"/\") or self.base_path_str.endswith(\"\\\\\"):\n            full_path = f\"{self.base_path_str}{clean_rel}\"\n        else:\n            # Use forward slash for URIs\n            full_path = f\"{self.base_path_str}/{clean_rel}\"\n\n        ctx.debug(\n            \"Resolved URI path\",\n            relative_path=relative_path,\n            full_path=full_path,\n        )\n        return full_path\n    else:\n        # Standard local path logic\n        full_path = self.base_path / relative_path\n        resolved = str(full_path.absolute())\n\n        ctx.debug(\n            \"Resolved local path\",\n            relative_path=relative_path,\n            full_path=resolved,\n        )\n        return resolved\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.validate","title":"<code>validate()</code>","text":"<p>Validate that base path exists or can be created.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If validation fails</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate that base path exists or can be created.\n\n    Raises:\n        ConnectionError: If validation fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating LocalConnection\",\n        base_path=self.base_path_str,\n        is_uri=self.is_uri,\n    )\n\n    if self.is_uri:\n        # Cannot validate/create URIs with local os module\n        # Assume valid or handled by engine\n        ctx.debug(\n            \"Skipping URI validation (handled by engine)\",\n            base_path=self.base_path_str,\n        )\n    else:\n        # Create base directory if it doesn't exist\n        try:\n            self.base_path.mkdir(parents=True, exist_ok=True)\n            ctx.info(\n                \"LocalConnection validated successfully\",\n                base_path=str(self.base_path.absolute()),\n                created=not self.base_path.exists(),\n            )\n        except Exception as e:\n            ctx.error(\n                \"LocalConnection validation failed\",\n                base_path=self.base_path_str,\n                error=str(e),\n            )\n            raise\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls","title":"<code>odibi.connections.azure_adls</code>","text":"<p>Azure Data Lake Storage Gen2 connection (Phase 2A: Multi-mode authentication).</p>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS","title":"<code>AzureADLS</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Azure Data Lake Storage Gen2 connection.</p> <p>Phase 2A: Multi-mode authentication + multi-account support Supports key_vault (recommended), direct_key, service_principal, and managed_identity.</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>class AzureADLS(BaseConnection):\n    \"\"\"Azure Data Lake Storage Gen2 connection.\n\n    Phase 2A: Multi-mode authentication + multi-account support\n    Supports key_vault (recommended), direct_key, service_principal, and managed_identity.\n    \"\"\"\n\n    def __init__(\n        self,\n        account: str,\n        container: str,\n        path_prefix: str = \"\",\n        auth_mode: str = \"key_vault\",\n        key_vault_name: Optional[str] = None,\n        secret_name: Optional[str] = None,\n        account_key: Optional[str] = None,\n        sas_token: Optional[str] = None,\n        tenant_id: Optional[str] = None,\n        client_id: Optional[str] = None,\n        client_secret: Optional[str] = None,\n        validate: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Initialize ADLS connection.\n\n        Args:\n            account: Storage account name (e.g., 'mystorageaccount')\n            container: Container/filesystem name\n            path_prefix: Optional prefix for all paths\n            auth_mode: Authentication mode\n                ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')\n            key_vault_name: Azure Key Vault name (required for key_vault mode)\n            secret_name: Secret name in Key Vault (required for key_vault mode)\n            account_key: Storage account key (required for direct_key mode)\n            sas_token: Shared Access Signature token (required for sas_token mode)\n            tenant_id: Azure Tenant ID (required for service_principal)\n            client_id: Service Principal Client ID (required for service_principal)\n            client_secret: Service Principal Client Secret (required for service_principal)\n            validate: Validate configuration on init\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"azure_adls\",\n            connection_name=f\"{account}/{container}\",\n            action=\"init\",\n            account=account,\n            container=container,\n            auth_mode=auth_mode,\n            path_prefix=path_prefix or \"(none)\",\n        )\n\n        self.account = account\n        self.container = container\n        self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n        self.auth_mode = auth_mode\n        self.key_vault_name = key_vault_name\n        self.secret_name = secret_name\n        self.account_key = account_key\n        self.sas_token = sas_token\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n        self._cached_key: Optional[str] = None\n        self._cache_lock = threading.Lock()\n\n        if validate:\n            self.validate()\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate ADLS connection configuration.\n\n        Raises:\n            ValueError: If required fields are missing for the selected auth_mode\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating AzureADLS connection\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        if not self.account:\n            ctx.error(\"ADLS connection validation failed: missing 'account'\")\n            raise ValueError(\"ADLS connection requires 'account'\")\n        if not self.container:\n            ctx.error(\n                \"ADLS connection validation failed: missing 'container'\",\n                account=self.account,\n            )\n            raise ValueError(\"ADLS connection requires 'container'\")\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"ADLS key_vault mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                    key_vault_name=self.key_vault_name or \"(missing)\",\n                    secret_name=self.secret_name or \"(missing)\",\n                )\n                raise ValueError(\n                    f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"direct_key\":\n            if not self.account_key:\n                ctx.error(\n                    \"ADLS direct_key mode validation failed: missing account_key\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"direct_key mode requires 'account_key' \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n\n            # Warn in production\n            if os.getenv(\"ODIBI_ENV\") == \"production\":\n                ctx.warning(\n                    \"Using direct_key in production is not recommended\",\n                    account=self.account,\n                    container=self.container,\n                )\n                warnings.warn(\n                    f\"\u26a0\ufe0f  Using direct_key in production is not recommended. \"\n                    f\"Use auth_mode: key_vault. Connection: {self.account}/{self.container}\",\n                    UserWarning,\n                )\n        elif self.auth_mode == \"sas_token\":\n            if not self.sas_token and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"ADLS sas_token mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"sas_token mode requires 'sas_token' (or key_vault_name/secret_name) \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"service_principal\":\n            if not self.tenant_id or not self.client_id:\n                ctx.error(\n                    \"ADLS service_principal mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                    missing=\"tenant_id and/or client_id\",\n                )\n                raise ValueError(\"service_principal mode requires 'tenant_id' and 'client_id'\")\n\n            if not self.client_secret and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"ADLS service_principal mode validation failed: missing client_secret\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"service_principal mode requires 'client_secret' \"\n                    f\"(or key_vault_name/secret_name) for {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"managed_identity\":\n            # No specific config required, but we might check if environment supports it\n            ctx.debug(\n                \"Using managed_identity auth mode\",\n                account=self.account,\n                container=self.container,\n            )\n        else:\n            ctx.error(\n                \"ADLS validation failed: unsupported auth_mode\",\n                account=self.account,\n                container=self.container,\n                auth_mode=self.auth_mode,\n            )\n            raise ValueError(\n                f\"Unsupported auth_mode: '{self.auth_mode}'. \"\n                f\"Use 'key_vault', 'direct_key', 'service_principal', or 'managed_identity'.\"\n            )\n\n        ctx.info(\n            \"AzureADLS connection validated successfully\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n    def get_storage_key(self, timeout: float = 30.0) -&gt; Optional[str]:\n        \"\"\"Get storage account key (cached).\n\n        Only relevant for 'key_vault' and 'direct_key' modes.\n\n        Args:\n            timeout: Timeout for Key Vault operations in seconds (default: 30.0)\n\n        Returns:\n            Storage account key or None if not applicable for auth_mode\n\n        Raises:\n            ImportError: If azure libraries not installed (key_vault mode)\n            TimeoutError: If Key Vault fetch exceeds timeout\n            Exception: If Key Vault access fails\n        \"\"\"\n        ctx = get_logging_context()\n\n        with self._cache_lock:\n            # Return cached key if available (double-check inside lock)\n            if self._cached_key:\n                ctx.debug(\n                    \"Using cached storage key\",\n                    account=self.account,\n                    container=self.container,\n                )\n                return self._cached_key\n\n            if self.auth_mode == \"key_vault\":\n                ctx.debug(\n                    \"Fetching storage key from Key Vault\",\n                    account=self.account,\n                    key_vault_name=self.key_vault_name,\n                    secret_name=self.secret_name,\n                    timeout=timeout,\n                )\n\n                try:\n                    import concurrent.futures\n\n                    from azure.identity import DefaultAzureCredential\n                    from azure.keyvault.secrets import SecretClient\n                except ImportError as e:\n                    ctx.error(\n                        \"Key Vault authentication failed: missing azure libraries\",\n                        account=self.account,\n                        error=str(e),\n                    )\n                    raise ImportError(\n                        \"Key Vault authentication requires 'azure-identity' and \"\n                        \"'azure-keyvault-secrets'. Install with: pip install odibi[azure]\"\n                    ) from e\n\n                # Create Key Vault client\n                credential = DefaultAzureCredential()\n                kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n                client = SecretClient(vault_url=kv_uri, credential=credential)\n\n                ctx.debug(\n                    \"Connecting to Key Vault\",\n                    key_vault_uri=kv_uri,\n                    secret_name=self.secret_name,\n                )\n\n                # Fetch secret with timeout protection\n                def _fetch():\n                    secret = client.get_secret(self.secret_name)\n                    return secret.value\n\n                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                    future = executor.submit(_fetch)\n                    try:\n                        self._cached_key = future.result(timeout=timeout)\n                        logger.register_secret(self._cached_key)\n                        ctx.info(\n                            \"Successfully fetched storage key from Key Vault\",\n                            account=self.account,\n                            key_vault_name=self.key_vault_name,\n                        )\n                        return self._cached_key\n                    except concurrent.futures.TimeoutError:\n                        ctx.error(\n                            \"Key Vault fetch timed out\",\n                            account=self.account,\n                            key_vault_name=self.key_vault_name,\n                            secret_name=self.secret_name,\n                            timeout=timeout,\n                        )\n                        raise TimeoutError(\n                            f\"Key Vault fetch timed out after {timeout}s for \"\n                            f\"vault '{self.key_vault_name}', secret '{self.secret_name}'\"\n                        )\n\n            elif self.auth_mode == \"direct_key\":\n                ctx.debug(\n                    \"Using direct account key\",\n                    account=self.account,\n                )\n                return self.account_key\n\n            elif self.auth_mode == \"sas_token\":\n                # Return cached key (fetched from KV) if available, else sas_token arg\n                ctx.debug(\n                    \"Using SAS token\",\n                    account=self.account,\n                    from_cache=bool(self._cached_key),\n                )\n                return self._cached_key or self.sas_token\n\n            # For other modes (SP, MI), we don't use an account key\n            ctx.debug(\n                \"No storage key required for auth_mode\",\n                account=self.account,\n                auth_mode=self.auth_mode,\n            )\n            return None\n\n    def get_client_secret(self) -&gt; Optional[str]:\n        \"\"\"Get Service Principal client secret (cached or literal).\"\"\"\n        return self._cached_key or self.client_secret\n\n    def pandas_storage_options(self) -&gt; Dict[str, Any]:\n        \"\"\"Get storage options for pandas/fsspec.\n\n        Returns:\n            Dictionary with appropriate authentication parameters for fsspec\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Building pandas storage options\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        base_options = {\"account_name\": self.account}\n\n        if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n            return {**base_options, \"account_key\": self.get_storage_key()}\n\n        elif self.auth_mode == \"sas_token\":\n            # Use get_storage_key() which handles KV fallback for SAS\n            return {**base_options, \"sas_token\": self.get_storage_key()}\n\n        elif self.auth_mode == \"service_principal\":\n            return {\n                **base_options,\n                \"tenant_id\": self.tenant_id,\n                \"client_id\": self.client_id,\n                \"client_secret\": self.get_client_secret(),\n            }\n\n        elif self.auth_mode == \"managed_identity\":\n            # adlfs supports using DefaultAzureCredential implicitly if anon=False\n            # and no other creds provided, assuming azure.identity is installed\n            return {**base_options, \"anon\": False}\n\n        return base_options\n\n    def configure_spark(self, spark) -&gt; None:\n        \"\"\"Configure Spark session with storage credentials.\n\n        Args:\n            spark: SparkSession instance\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Configuring Spark for AzureADLS\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n            config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(config_key, self.get_storage_key())\n            ctx.debug(\n                \"Set Spark config for account key\",\n                config_key=config_key,\n            )\n\n        elif self.auth_mode == \"sas_token\":\n            # SAS Token Configuration\n            # fs.azure.sas.token.provider.type -&gt; FixedSASTokenProvider\n            # fs.azure.sas.fixed.token -&gt; &lt;token&gt;\n            provider_key = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(provider_key, \"SAS\")\n\n            sas_provider_key = (\n                f\"fs.azure.sas.token.provider.type.{self.account}.dfs.core.windows.net\"\n            )\n            spark.conf.set(\n                sas_provider_key, \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n            )\n\n            sas_token = self.get_storage_key()\n\n            sas_token_key = f\"fs.azure.sas.fixed.token.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(sas_token_key, sas_token)\n\n            ctx.debug(\n                \"Set Spark config for SAS token\",\n                auth_type_key=provider_key,\n                provider_key=sas_provider_key,\n            )\n\n        elif self.auth_mode == \"service_principal\":\n            # Configure OAuth for ADLS Gen2\n            # Ref: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html\n            prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"OAuth\")\n\n            prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n            prefix = f\"fs.azure.account.oauth2.client.id.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, self.client_id)\n\n            prefix = f\"fs.azure.account.oauth2.client.secret.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, self.get_client_secret())\n\n            prefix = f\"fs.azure.account.oauth2.client.endpoint.{self.account}.dfs.core.windows.net\"\n            endpoint = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n            spark.conf.set(prefix, endpoint)\n\n            ctx.debug(\n                \"Set Spark config for service principal OAuth\",\n                tenant_id=self.tenant_id,\n                client_id=self.client_id,\n            )\n\n        elif self.auth_mode == \"managed_identity\":\n            prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"OAuth\")\n\n            prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n\n            ctx.debug(\n                \"Set Spark config for managed identity\",\n                account=self.account,\n            )\n\n        ctx.info(\n            \"Spark configuration complete\",\n            account=self.account,\n            auth_mode=self.auth_mode,\n        )\n\n    def uri(self, path: str) -&gt; str:\n        \"\"\"Build abfss:// URI for given path.\n\n        Args:\n            path: Relative path within container\n\n        Returns:\n            Full abfss:// URI\n\n        Example:\n            &gt;&gt;&gt; conn = AzureADLS(\n            ...     account=\"myaccount\", container=\"data\",\n            ...     auth_mode=\"direct_key\", account_key=\"key123\"\n            ... )\n            &gt;&gt;&gt; conn.uri(\"folder/file.csv\")\n            'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'\n        \"\"\"\n        if self.path_prefix:\n            full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n        else:\n            full_path = path.lstrip(\"/\")\n\n        return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full abfss:// URI for relative path.\"\"\"\n        ctx = get_logging_context()\n        full_uri = self.uri(relative_path)\n\n        ctx.debug(\n            \"Resolved ADLS path\",\n            account=self.account,\n            container=self.container,\n            relative_path=relative_path,\n            full_uri=full_uri,\n        )\n\n        return full_uri\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.__init__","title":"<code>__init__(account, container, path_prefix='', auth_mode='key_vault', key_vault_name=None, secret_name=None, account_key=None, sas_token=None, tenant_id=None, client_id=None, client_secret=None, validate=True, **kwargs)</code>","text":"<p>Initialize ADLS connection.</p> <p>Parameters:</p> Name Type Description Default <code>account</code> <code>str</code> <p>Storage account name (e.g., 'mystorageaccount')</p> required <code>container</code> <code>str</code> <p>Container/filesystem name</p> required <code>path_prefix</code> <code>str</code> <p>Optional prefix for all paths</p> <code>''</code> <code>auth_mode</code> <code>str</code> <p>Authentication mode ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')</p> <code>'key_vault'</code> <code>key_vault_name</code> <code>Optional[str]</code> <p>Azure Key Vault name (required for key_vault mode)</p> <code>None</code> <code>secret_name</code> <code>Optional[str]</code> <p>Secret name in Key Vault (required for key_vault mode)</p> <code>None</code> <code>account_key</code> <code>Optional[str]</code> <p>Storage account key (required for direct_key mode)</p> <code>None</code> <code>sas_token</code> <code>Optional[str]</code> <p>Shared Access Signature token (required for sas_token mode)</p> <code>None</code> <code>tenant_id</code> <code>Optional[str]</code> <p>Azure Tenant ID (required for service_principal)</p> <code>None</code> <code>client_id</code> <code>Optional[str]</code> <p>Service Principal Client ID (required for service_principal)</p> <code>None</code> <code>client_secret</code> <code>Optional[str]</code> <p>Service Principal Client Secret (required for service_principal)</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Validate configuration on init</p> <code>True</code> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def __init__(\n    self,\n    account: str,\n    container: str,\n    path_prefix: str = \"\",\n    auth_mode: str = \"key_vault\",\n    key_vault_name: Optional[str] = None,\n    secret_name: Optional[str] = None,\n    account_key: Optional[str] = None,\n    sas_token: Optional[str] = None,\n    tenant_id: Optional[str] = None,\n    client_id: Optional[str] = None,\n    client_secret: Optional[str] = None,\n    validate: bool = True,\n    **kwargs,\n):\n    \"\"\"Initialize ADLS connection.\n\n    Args:\n        account: Storage account name (e.g., 'mystorageaccount')\n        container: Container/filesystem name\n        path_prefix: Optional prefix for all paths\n        auth_mode: Authentication mode\n            ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')\n        key_vault_name: Azure Key Vault name (required for key_vault mode)\n        secret_name: Secret name in Key Vault (required for key_vault mode)\n        account_key: Storage account key (required for direct_key mode)\n        sas_token: Shared Access Signature token (required for sas_token mode)\n        tenant_id: Azure Tenant ID (required for service_principal)\n        client_id: Service Principal Client ID (required for service_principal)\n        client_secret: Service Principal Client Secret (required for service_principal)\n        validate: Validate configuration on init\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"azure_adls\",\n        connection_name=f\"{account}/{container}\",\n        action=\"init\",\n        account=account,\n        container=container,\n        auth_mode=auth_mode,\n        path_prefix=path_prefix or \"(none)\",\n    )\n\n    self.account = account\n    self.container = container\n    self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n    self.auth_mode = auth_mode\n    self.key_vault_name = key_vault_name\n    self.secret_name = secret_name\n    self.account_key = account_key\n    self.sas_token = sas_token\n    self.tenant_id = tenant_id\n    self.client_id = client_id\n    self.client_secret = client_secret\n\n    self._cached_key: Optional[str] = None\n    self._cache_lock = threading.Lock()\n\n    if validate:\n        self.validate()\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.configure_spark","title":"<code>configure_spark(spark)</code>","text":"<p>Configure Spark session with storage credentials.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <p>SparkSession instance</p> required Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def configure_spark(self, spark) -&gt; None:\n    \"\"\"Configure Spark session with storage credentials.\n\n    Args:\n        spark: SparkSession instance\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Configuring Spark for AzureADLS\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n        config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(config_key, self.get_storage_key())\n        ctx.debug(\n            \"Set Spark config for account key\",\n            config_key=config_key,\n        )\n\n    elif self.auth_mode == \"sas_token\":\n        # SAS Token Configuration\n        # fs.azure.sas.token.provider.type -&gt; FixedSASTokenProvider\n        # fs.azure.sas.fixed.token -&gt; &lt;token&gt;\n        provider_key = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(provider_key, \"SAS\")\n\n        sas_provider_key = (\n            f\"fs.azure.sas.token.provider.type.{self.account}.dfs.core.windows.net\"\n        )\n        spark.conf.set(\n            sas_provider_key, \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n        )\n\n        sas_token = self.get_storage_key()\n\n        sas_token_key = f\"fs.azure.sas.fixed.token.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(sas_token_key, sas_token)\n\n        ctx.debug(\n            \"Set Spark config for SAS token\",\n            auth_type_key=provider_key,\n            provider_key=sas_provider_key,\n        )\n\n    elif self.auth_mode == \"service_principal\":\n        # Configure OAuth for ADLS Gen2\n        # Ref: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html\n        prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"OAuth\")\n\n        prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n        prefix = f\"fs.azure.account.oauth2.client.id.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, self.client_id)\n\n        prefix = f\"fs.azure.account.oauth2.client.secret.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, self.get_client_secret())\n\n        prefix = f\"fs.azure.account.oauth2.client.endpoint.{self.account}.dfs.core.windows.net\"\n        endpoint = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n        spark.conf.set(prefix, endpoint)\n\n        ctx.debug(\n            \"Set Spark config for service principal OAuth\",\n            tenant_id=self.tenant_id,\n            client_id=self.client_id,\n        )\n\n    elif self.auth_mode == \"managed_identity\":\n        prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"OAuth\")\n\n        prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n\n        ctx.debug(\n            \"Set Spark config for managed identity\",\n            account=self.account,\n        )\n\n    ctx.info(\n        \"Spark configuration complete\",\n        account=self.account,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_client_secret","title":"<code>get_client_secret()</code>","text":"<p>Get Service Principal client secret (cached or literal).</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_client_secret(self) -&gt; Optional[str]:\n    \"\"\"Get Service Principal client secret (cached or literal).\"\"\"\n    return self._cached_key or self.client_secret\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get full abfss:// URI for relative path.</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full abfss:// URI for relative path.\"\"\"\n    ctx = get_logging_context()\n    full_uri = self.uri(relative_path)\n\n    ctx.debug(\n        \"Resolved ADLS path\",\n        account=self.account,\n        container=self.container,\n        relative_path=relative_path,\n        full_uri=full_uri,\n    )\n\n    return full_uri\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_storage_key","title":"<code>get_storage_key(timeout=30.0)</code>","text":"<p>Get storage account key (cached).</p> <p>Only relevant for 'key_vault' and 'direct_key' modes.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Timeout for Key Vault operations in seconds (default: 30.0)</p> <code>30.0</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Storage account key or None if not applicable for auth_mode</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If azure libraries not installed (key_vault mode)</p> <code>TimeoutError</code> <p>If Key Vault fetch exceeds timeout</p> <code>Exception</code> <p>If Key Vault access fails</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_storage_key(self, timeout: float = 30.0) -&gt; Optional[str]:\n    \"\"\"Get storage account key (cached).\n\n    Only relevant for 'key_vault' and 'direct_key' modes.\n\n    Args:\n        timeout: Timeout for Key Vault operations in seconds (default: 30.0)\n\n    Returns:\n        Storage account key or None if not applicable for auth_mode\n\n    Raises:\n        ImportError: If azure libraries not installed (key_vault mode)\n        TimeoutError: If Key Vault fetch exceeds timeout\n        Exception: If Key Vault access fails\n    \"\"\"\n    ctx = get_logging_context()\n\n    with self._cache_lock:\n        # Return cached key if available (double-check inside lock)\n        if self._cached_key:\n            ctx.debug(\n                \"Using cached storage key\",\n                account=self.account,\n                container=self.container,\n            )\n            return self._cached_key\n\n        if self.auth_mode == \"key_vault\":\n            ctx.debug(\n                \"Fetching storage key from Key Vault\",\n                account=self.account,\n                key_vault_name=self.key_vault_name,\n                secret_name=self.secret_name,\n                timeout=timeout,\n            )\n\n            try:\n                import concurrent.futures\n\n                from azure.identity import DefaultAzureCredential\n                from azure.keyvault.secrets import SecretClient\n            except ImportError as e:\n                ctx.error(\n                    \"Key Vault authentication failed: missing azure libraries\",\n                    account=self.account,\n                    error=str(e),\n                )\n                raise ImportError(\n                    \"Key Vault authentication requires 'azure-identity' and \"\n                    \"'azure-keyvault-secrets'. Install with: pip install odibi[azure]\"\n                ) from e\n\n            # Create Key Vault client\n            credential = DefaultAzureCredential()\n            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n            client = SecretClient(vault_url=kv_uri, credential=credential)\n\n            ctx.debug(\n                \"Connecting to Key Vault\",\n                key_vault_uri=kv_uri,\n                secret_name=self.secret_name,\n            )\n\n            # Fetch secret with timeout protection\n            def _fetch():\n                secret = client.get_secret(self.secret_name)\n                return secret.value\n\n            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                future = executor.submit(_fetch)\n                try:\n                    self._cached_key = future.result(timeout=timeout)\n                    logger.register_secret(self._cached_key)\n                    ctx.info(\n                        \"Successfully fetched storage key from Key Vault\",\n                        account=self.account,\n                        key_vault_name=self.key_vault_name,\n                    )\n                    return self._cached_key\n                except concurrent.futures.TimeoutError:\n                    ctx.error(\n                        \"Key Vault fetch timed out\",\n                        account=self.account,\n                        key_vault_name=self.key_vault_name,\n                        secret_name=self.secret_name,\n                        timeout=timeout,\n                    )\n                    raise TimeoutError(\n                        f\"Key Vault fetch timed out after {timeout}s for \"\n                        f\"vault '{self.key_vault_name}', secret '{self.secret_name}'\"\n                    )\n\n        elif self.auth_mode == \"direct_key\":\n            ctx.debug(\n                \"Using direct account key\",\n                account=self.account,\n            )\n            return self.account_key\n\n        elif self.auth_mode == \"sas_token\":\n            # Return cached key (fetched from KV) if available, else sas_token arg\n            ctx.debug(\n                \"Using SAS token\",\n                account=self.account,\n                from_cache=bool(self._cached_key),\n            )\n            return self._cached_key or self.sas_token\n\n        # For other modes (SP, MI), we don't use an account key\n        ctx.debug(\n            \"No storage key required for auth_mode\",\n            account=self.account,\n            auth_mode=self.auth_mode,\n        )\n        return None\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.pandas_storage_options","title":"<code>pandas_storage_options()</code>","text":"<p>Get storage options for pandas/fsspec.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with appropriate authentication parameters for fsspec</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def pandas_storage_options(self) -&gt; Dict[str, Any]:\n    \"\"\"Get storage options for pandas/fsspec.\n\n    Returns:\n        Dictionary with appropriate authentication parameters for fsspec\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Building pandas storage options\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    base_options = {\"account_name\": self.account}\n\n    if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n        return {**base_options, \"account_key\": self.get_storage_key()}\n\n    elif self.auth_mode == \"sas_token\":\n        # Use get_storage_key() which handles KV fallback for SAS\n        return {**base_options, \"sas_token\": self.get_storage_key()}\n\n    elif self.auth_mode == \"service_principal\":\n        return {\n            **base_options,\n            \"tenant_id\": self.tenant_id,\n            \"client_id\": self.client_id,\n            \"client_secret\": self.get_client_secret(),\n        }\n\n    elif self.auth_mode == \"managed_identity\":\n        # adlfs supports using DefaultAzureCredential implicitly if anon=False\n        # and no other creds provided, assuming azure.identity is installed\n        return {**base_options, \"anon\": False}\n\n    return base_options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.uri","title":"<code>uri(path)</code>","text":"<p>Build abfss:// URI for given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path within container</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full abfss:// URI</p> Example <p>conn = AzureADLS( ...     account=\"myaccount\", container=\"data\", ...     auth_mode=\"direct_key\", account_key=\"key123\" ... ) conn.uri(\"folder/file.csv\") 'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def uri(self, path: str) -&gt; str:\n    \"\"\"Build abfss:// URI for given path.\n\n    Args:\n        path: Relative path within container\n\n    Returns:\n        Full abfss:// URI\n\n    Example:\n        &gt;&gt;&gt; conn = AzureADLS(\n        ...     account=\"myaccount\", container=\"data\",\n        ...     auth_mode=\"direct_key\", account_key=\"key123\"\n        ... )\n        &gt;&gt;&gt; conn.uri(\"folder/file.csv\")\n        'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'\n    \"\"\"\n    if self.path_prefix:\n        full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n    else:\n        full_path = path.lstrip(\"/\")\n\n    return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.validate","title":"<code>validate()</code>","text":"<p>Validate ADLS connection configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing for the selected auth_mode</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate ADLS connection configuration.\n\n    Raises:\n        ValueError: If required fields are missing for the selected auth_mode\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating AzureADLS connection\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    if not self.account:\n        ctx.error(\"ADLS connection validation failed: missing 'account'\")\n        raise ValueError(\"ADLS connection requires 'account'\")\n    if not self.container:\n        ctx.error(\n            \"ADLS connection validation failed: missing 'container'\",\n            account=self.account,\n        )\n        raise ValueError(\"ADLS connection requires 'container'\")\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"ADLS key_vault mode validation failed\",\n                account=self.account,\n                container=self.container,\n                key_vault_name=self.key_vault_name or \"(missing)\",\n                secret_name=self.secret_name or \"(missing)\",\n            )\n            raise ValueError(\n                f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"direct_key\":\n        if not self.account_key:\n            ctx.error(\n                \"ADLS direct_key mode validation failed: missing account_key\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"direct_key mode requires 'account_key' \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n\n        # Warn in production\n        if os.getenv(\"ODIBI_ENV\") == \"production\":\n            ctx.warning(\n                \"Using direct_key in production is not recommended\",\n                account=self.account,\n                container=self.container,\n            )\n            warnings.warn(\n                f\"\u26a0\ufe0f  Using direct_key in production is not recommended. \"\n                f\"Use auth_mode: key_vault. Connection: {self.account}/{self.container}\",\n                UserWarning,\n            )\n    elif self.auth_mode == \"sas_token\":\n        if not self.sas_token and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"ADLS sas_token mode validation failed\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"sas_token mode requires 'sas_token' (or key_vault_name/secret_name) \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"service_principal\":\n        if not self.tenant_id or not self.client_id:\n            ctx.error(\n                \"ADLS service_principal mode validation failed\",\n                account=self.account,\n                container=self.container,\n                missing=\"tenant_id and/or client_id\",\n            )\n            raise ValueError(\"service_principal mode requires 'tenant_id' and 'client_id'\")\n\n        if not self.client_secret and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"ADLS service_principal mode validation failed: missing client_secret\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"service_principal mode requires 'client_secret' \"\n                f\"(or key_vault_name/secret_name) for {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"managed_identity\":\n        # No specific config required, but we might check if environment supports it\n        ctx.debug(\n            \"Using managed_identity auth mode\",\n            account=self.account,\n            container=self.container,\n        )\n    else:\n        ctx.error(\n            \"ADLS validation failed: unsupported auth_mode\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n        raise ValueError(\n            f\"Unsupported auth_mode: '{self.auth_mode}'. \"\n            f\"Use 'key_vault', 'direct_key', 'service_principal', or 'managed_identity'.\"\n        )\n\n    ctx.info(\n        \"AzureADLS connection validated successfully\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql","title":"<code>odibi.connections.azure_sql</code>","text":""},{"location":"reference/api/connections/#odibi.connections.azure_sql--azure-sql-database-connection","title":"Azure SQL Database Connection","text":"<p>Provides connectivity to Azure SQL databases with authentication support.</p>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL","title":"<code>AzureSQL</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Azure SQL Database connection.</p> <p>Supports: - SQL authentication (username/password) - Azure Active Directory Managed Identity - Connection pooling - Read/write operations via SQLAlchemy</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>class AzureSQL(BaseConnection):\n    \"\"\"\n    Azure SQL Database connection.\n\n    Supports:\n    - SQL authentication (username/password)\n    - Azure Active Directory Managed Identity\n    - Connection pooling\n    - Read/write operations via SQLAlchemy\n    \"\"\"\n\n    def __init__(\n        self,\n        server: str,\n        database: str,\n        driver: str = \"ODBC Driver 18 for SQL Server\",\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        auth_mode: str = \"aad_msi\",  # \"aad_msi\", \"sql\", \"key_vault\"\n        key_vault_name: Optional[str] = None,\n        secret_name: Optional[str] = None,\n        port: int = 1433,\n        timeout: int = 30,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize Azure SQL connection.\n\n        Args:\n            server: SQL server hostname (e.g., 'myserver.database.windows.net')\n            database: Database name\n            driver: ODBC driver name (default: ODBC Driver 18 for SQL Server)\n            username: SQL auth username (required if auth_mode='sql')\n            password: SQL auth password (required if auth_mode='sql')\n            auth_mode: Authentication mode ('aad_msi', 'sql', 'key_vault')\n            key_vault_name: Key Vault name (required if auth_mode='key_vault')\n            secret_name: Secret name containing password (required if auth_mode='key_vault')\n            port: SQL Server port (default: 1433)\n            timeout: Connection timeout in seconds (default: 30)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"azure_sql\",\n            connection_name=f\"{server}/{database}\",\n            action=\"init\",\n            server=server,\n            database=database,\n            auth_mode=auth_mode,\n            port=port,\n        )\n\n        self.server = server\n        self.database = database\n        self.driver = driver\n        self.username = username\n        self.password = password\n        self.auth_mode = auth_mode\n        self.key_vault_name = key_vault_name\n        self.secret_name = secret_name\n        self.port = port\n        self.timeout = timeout\n        self._engine = None\n        self._cached_key = None  # For consistency with ADLS / parallel fetch\n\n        ctx.debug(\n            \"AzureSQL connection initialized\",\n            server=server,\n            database=database,\n            auth_mode=auth_mode,\n            driver=driver,\n        )\n\n    def get_password(self) -&gt; Optional[str]:\n        \"\"\"Get password (cached).\"\"\"\n        ctx = get_logging_context()\n\n        if self.password:\n            ctx.debug(\n                \"Using provided password\",\n                server=self.server,\n                database=self.database,\n            )\n            return self.password\n\n        if self._cached_key:\n            ctx.debug(\n                \"Using cached password\",\n                server=self.server,\n                database=self.database,\n            )\n            return self._cached_key\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"Key Vault mode requires key_vault_name and secret_name\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"key_vault mode requires key_vault_name and secret_name\")\n\n            ctx.debug(\n                \"Fetching password from Key Vault\",\n                server=self.server,\n                key_vault_name=self.key_vault_name,\n                secret_name=self.secret_name,\n            )\n\n            try:\n                from azure.identity import DefaultAzureCredential\n                from azure.keyvault.secrets import SecretClient\n\n                credential = DefaultAzureCredential()\n                kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n                client = SecretClient(vault_url=kv_uri, credential=credential)\n                secret = client.get_secret(self.secret_name)\n                self._cached_key = secret.value\n                logger.register_secret(self._cached_key)\n\n                ctx.info(\n                    \"Successfully fetched password from Key Vault\",\n                    server=self.server,\n                    key_vault_name=self.key_vault_name,\n                )\n                return self._cached_key\n            except ImportError as e:\n                ctx.error(\n                    \"Key Vault support requires azure libraries\",\n                    server=self.server,\n                    error=str(e),\n                )\n                raise ImportError(\n                    \"Key Vault support requires 'azure-identity' and 'azure-keyvault-secrets'\"\n                )\n\n        ctx.debug(\n            \"No password required for auth_mode\",\n            server=self.server,\n            auth_mode=self.auth_mode,\n        )\n        return None\n\n    def odbc_dsn(self) -&gt; str:\n        \"\"\"Build ODBC connection string.\n\n        Returns:\n            ODBC DSN string\n\n        Example:\n            &gt;&gt;&gt; conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\")\n            &gt;&gt;&gt; conn.odbc_dsn()\n            'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Building ODBC connection string\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        dsn = (\n            f\"Driver={{{self.driver}}};\"\n            f\"Server=tcp:{self.server},1433;\"\n            f\"Database={self.database};\"\n            f\"Encrypt=yes;\"\n            f\"TrustServerCertificate=yes;\"\n            f\"Connection Timeout=30;\"\n        )\n\n        pwd = self.get_password()\n        if self.username and pwd:\n            dsn += f\"UID={self.username};PWD={pwd};\"\n            ctx.debug(\n                \"Using SQL authentication\",\n                server=self.server,\n                username=self.username,\n            )\n        elif self.auth_mode == \"aad_msi\":\n            dsn += \"Authentication=ActiveDirectoryMsi;\"\n            ctx.debug(\n                \"Using AAD Managed Identity authentication\",\n                server=self.server,\n            )\n        elif self.auth_mode == \"aad_service_principal\":\n            # Not fully supported via ODBC string simply without token usually\n            ctx.debug(\n                \"Using AAD Service Principal authentication\",\n                server=self.server,\n            )\n\n        return dsn\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get table reference for relative path.\"\"\"\n        return relative_path\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate Azure SQL connection configuration.\"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating AzureSQL connection\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        if not self.server:\n            ctx.error(\"AzureSQL validation failed: missing 'server'\")\n            raise ValueError(\"Azure SQL connection requires 'server'\")\n        if not self.database:\n            ctx.error(\n                \"AzureSQL validation failed: missing 'database'\",\n                server=self.server,\n            )\n            raise ValueError(\"Azure SQL connection requires 'database'\")\n\n        if self.auth_mode == \"sql\":\n            if not self.username:\n                ctx.error(\n                    \"AzureSQL validation failed: SQL auth requires username\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"Azure SQL with auth_mode='sql' requires username\")\n            if not self.password and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"AzureSQL validation failed: SQL auth requires password\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    \"Azure SQL with auth_mode='sql' requires password \"\n                    \"(or key_vault_name/secret_name)\"\n                )\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"AzureSQL validation failed: key_vault mode missing config\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    \"Azure SQL with auth_mode='key_vault' requires key_vault_name and secret_name\"\n                )\n            if not self.username:\n                ctx.error(\n                    \"AzureSQL validation failed: key_vault mode requires username\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"Azure SQL with auth_mode='key_vault' requires username\")\n\n        ctx.info(\n            \"AzureSQL connection validated successfully\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n    def get_engine(self):\n        \"\"\"\n        Get or create SQLAlchemy engine.\n\n        Returns:\n            SQLAlchemy engine instance\n\n        Raises:\n            ConnectionError: If connection fails or drivers missing\n        \"\"\"\n        ctx = get_logging_context()\n\n        if self._engine is not None:\n            ctx.debug(\n                \"Using cached SQLAlchemy engine\",\n                server=self.server,\n                database=self.database,\n            )\n            return self._engine\n\n        ctx.debug(\n            \"Creating SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n        )\n\n        try:\n            from urllib.parse import quote_plus\n\n            from sqlalchemy import create_engine\n        except ImportError as e:\n            ctx.error(\n                \"SQLAlchemy import failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=\"Required packages 'sqlalchemy' or 'pyodbc' not found.\",\n                suggestions=[\n                    \"Install required packages: pip install sqlalchemy pyodbc\",\n                    \"Or install odibi with azure extras: pip install 'odibi[azure]'\",\n                ],\n            )\n\n        try:\n            # Build connection string\n            conn_str = self.odbc_dsn()\n            connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}\"\n\n            ctx.debug(\n                \"Creating SQLAlchemy engine with connection pooling\",\n                server=self.server,\n                database=self.database,\n            )\n\n            # Create engine with connection pooling\n            self._engine = create_engine(\n                connection_url,\n                pool_pre_ping=True,  # Verify connections before use\n                pool_recycle=3600,  # Recycle connections after 1 hour\n                echo=False,\n            )\n\n            # Test connection\n            with self._engine.connect():\n                pass\n\n            ctx.info(\n                \"SQLAlchemy engine created successfully\",\n                server=self.server,\n                database=self.database,\n            )\n\n            return self._engine\n\n        except Exception as e:\n            suggestions = self._get_error_suggestions(str(e))\n            ctx.error(\n                \"Failed to create SQLAlchemy engine\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n                suggestions=suggestions,\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Failed to create engine: {str(e)}\",\n                suggestions=suggestions,\n            )\n\n    def read_sql(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Execute SQL query and return results as DataFrame.\n\n        Args:\n            query: SQL query string\n            params: Optional query parameters for parameterized queries\n\n        Returns:\n            Query results as pandas DataFrame\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Executing SQL query\",\n            server=self.server,\n            database=self.database,\n            query_length=len(query),\n        )\n\n        try:\n            engine = self.get_engine()\n            result = pd.read_sql(query, engine, params=params)\n\n            ctx.info(\n                \"SQL query executed successfully\",\n                server=self.server,\n                database=self.database,\n                rows_returned=len(result),\n            )\n            return result\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"SQL query execution failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Query execution failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def read_table(self, table_name: str, schema: Optional[str] = \"dbo\") -&gt; pd.DataFrame:\n        \"\"\"\n        Read entire table into DataFrame.\n\n        Args:\n            table_name: Name of the table\n            schema: Schema name (default: dbo)\n\n        Returns:\n            Table contents as pandas DataFrame\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Reading table\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            schema=schema,\n        )\n\n        if schema:\n            query = f\"SELECT * FROM [{schema}].[{table_name}]\"\n        else:\n            query = f\"SELECT * FROM [{table_name}]\"\n\n        return self.read_sql(query)\n\n    def write_table(\n        self,\n        df: pd.DataFrame,\n        table_name: str,\n        schema: Optional[str] = \"dbo\",\n        if_exists: str = \"replace\",\n        index: bool = False,\n        chunksize: Optional[int] = 1000,\n    ) -&gt; int:\n        \"\"\"\n        Write DataFrame to SQL table.\n\n        Args:\n            df: DataFrame to write\n            table_name: Name of the table\n            schema: Schema name (default: dbo)\n            if_exists: How to behave if table exists ('fail', 'replace', 'append')\n            index: Whether to write DataFrame index as column\n            chunksize: Number of rows to write in each batch (default: 1000)\n\n        Returns:\n            Number of rows written\n\n        Raises:\n            ConnectionError: If write fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Writing DataFrame to table\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            schema=schema,\n            rows=len(df),\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n\n        try:\n            engine = self.get_engine()\n\n            rows_written = df.to_sql(\n                name=table_name,\n                con=engine,\n                schema=schema,\n                if_exists=if_exists,\n                index=index,\n                chunksize=chunksize,\n                method=\"multi\",  # Use multi-row INSERT for better performance\n            )\n\n            result_rows = rows_written if rows_written is not None else len(df)\n            ctx.info(\n                \"Table write completed successfully\",\n                server=self.server,\n                database=self.database,\n                table_name=table_name,\n                rows_written=result_rows,\n            )\n            return result_rows\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"Table write failed\",\n                server=self.server,\n                database=self.database,\n                table_name=table_name,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Write operation failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n        \"\"\"\n        Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n        Args:\n            sql: SQL statement\n            params: Optional parameters for parameterized query\n\n        Returns:\n            Result from execution\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Executing SQL statement\",\n            server=self.server,\n            database=self.database,\n            statement_length=len(sql),\n        )\n\n        try:\n            engine = self.get_engine()\n            from sqlalchemy import text\n\n            with engine.connect() as conn:\n                result = conn.execute(text(sql), params or {})\n                conn.commit()\n\n                ctx.info(\n                    \"SQL statement executed successfully\",\n                    server=self.server,\n                    database=self.database,\n                )\n                return result\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"SQL statement execution failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Statement execution failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def close(self):\n        \"\"\"Close database connection and dispose of engine.\"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Closing AzureSQL connection\",\n            server=self.server,\n            database=self.database,\n        )\n\n        if self._engine:\n            self._engine.dispose()\n            self._engine = None\n            ctx.info(\n                \"AzureSQL connection closed\",\n                server=self.server,\n                database=self.database,\n            )\n\n    def _get_error_suggestions(self, error_msg: str) -&gt; List[str]:\n        \"\"\"Generate suggestions based on error message.\"\"\"\n        suggestions = []\n        error_lower = error_msg.lower()\n\n        if \"login failed\" in error_lower:\n            suggestions.append(\"Check username and password\")\n            suggestions.append(f\"Verify auth_mode is correct (current: {self.auth_mode})\")\n            if \"identity\" in error_lower:\n                suggestions.append(\"Ensure Managed Identity has access to the database\")\n\n        if \"firewall\" in error_lower or \"tcp provider\" in error_lower:\n            suggestions.append(\"Check Azure SQL Server firewall rules\")\n            suggestions.append(\"Ensure client IP is allowed\")\n\n        if \"driver\" in error_lower:\n            suggestions.append(f\"Verify ODBC driver '{self.driver}' is installed\")\n            suggestions.append(\"On Linux: sudo apt-get install msodbcsql18\")\n\n        return suggestions\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        \"\"\"Get Spark JDBC options.\n\n        Returns:\n            Dictionary of Spark JDBC options (url, user, password, etc.)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Building Spark JDBC options\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        jdbc_url = (\n            f\"jdbc:sqlserver://{self.server}:{self.port};\"\n            f\"databaseName={self.database};encrypt=true;trustServerCertificate=true;\"\n        )\n\n        if self.auth_mode == \"aad_msi\":\n            jdbc_url += (\n                \"hostNameInCertificate=*.database.windows.net;\"\n                \"loginTimeout=30;authentication=ActiveDirectoryMsi;\"\n            )\n            ctx.debug(\n                \"Configured JDBC URL for AAD MSI\",\n                server=self.server,\n            )\n        elif self.auth_mode == \"aad_service_principal\":\n            # Not fully implemented in init yet, but placeholder\n            ctx.debug(\n                \"Configured JDBC URL for AAD Service Principal\",\n                server=self.server,\n            )\n\n        options = {\n            \"url\": jdbc_url,\n            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n        }\n\n        if self.auth_mode == \"sql\" or self.auth_mode == \"key_vault\":\n            if self.username:\n                options[\"user\"] = self.username\n\n            pwd = self.get_password()\n            if pwd:\n                options[\"password\"] = pwd\n\n            ctx.debug(\n                \"Added SQL authentication to Spark options\",\n                server=self.server,\n                username=self.username,\n            )\n\n        ctx.info(\n            \"Spark JDBC options built successfully\",\n            server=self.server,\n            database=self.database,\n        )\n\n        return options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.__init__","title":"<code>__init__(server, database, driver='ODBC Driver 18 for SQL Server', username=None, password=None, auth_mode='aad_msi', key_vault_name=None, secret_name=None, port=1433, timeout=30, **kwargs)</code>","text":"<p>Initialize Azure SQL connection.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>SQL server hostname (e.g., 'myserver.database.windows.net')</p> required <code>database</code> <code>str</code> <p>Database name</p> required <code>driver</code> <code>str</code> <p>ODBC driver name (default: ODBC Driver 18 for SQL Server)</p> <code>'ODBC Driver 18 for SQL Server'</code> <code>username</code> <code>Optional[str]</code> <p>SQL auth username (required if auth_mode='sql')</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>SQL auth password (required if auth_mode='sql')</p> <code>None</code> <code>auth_mode</code> <code>str</code> <p>Authentication mode ('aad_msi', 'sql', 'key_vault')</p> <code>'aad_msi'</code> <code>key_vault_name</code> <code>Optional[str]</code> <p>Key Vault name (required if auth_mode='key_vault')</p> <code>None</code> <code>secret_name</code> <code>Optional[str]</code> <p>Secret name containing password (required if auth_mode='key_vault')</p> <code>None</code> <code>port</code> <code>int</code> <p>SQL Server port (default: 1433)</p> <code>1433</code> <code>timeout</code> <code>int</code> <p>Connection timeout in seconds (default: 30)</p> <code>30</code> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def __init__(\n    self,\n    server: str,\n    database: str,\n    driver: str = \"ODBC Driver 18 for SQL Server\",\n    username: Optional[str] = None,\n    password: Optional[str] = None,\n    auth_mode: str = \"aad_msi\",  # \"aad_msi\", \"sql\", \"key_vault\"\n    key_vault_name: Optional[str] = None,\n    secret_name: Optional[str] = None,\n    port: int = 1433,\n    timeout: int = 30,\n    **kwargs,\n):\n    \"\"\"\n    Initialize Azure SQL connection.\n\n    Args:\n        server: SQL server hostname (e.g., 'myserver.database.windows.net')\n        database: Database name\n        driver: ODBC driver name (default: ODBC Driver 18 for SQL Server)\n        username: SQL auth username (required if auth_mode='sql')\n        password: SQL auth password (required if auth_mode='sql')\n        auth_mode: Authentication mode ('aad_msi', 'sql', 'key_vault')\n        key_vault_name: Key Vault name (required if auth_mode='key_vault')\n        secret_name: Secret name containing password (required if auth_mode='key_vault')\n        port: SQL Server port (default: 1433)\n        timeout: Connection timeout in seconds (default: 30)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"azure_sql\",\n        connection_name=f\"{server}/{database}\",\n        action=\"init\",\n        server=server,\n        database=database,\n        auth_mode=auth_mode,\n        port=port,\n    )\n\n    self.server = server\n    self.database = database\n    self.driver = driver\n    self.username = username\n    self.password = password\n    self.auth_mode = auth_mode\n    self.key_vault_name = key_vault_name\n    self.secret_name = secret_name\n    self.port = port\n    self.timeout = timeout\n    self._engine = None\n    self._cached_key = None  # For consistency with ADLS / parallel fetch\n\n    ctx.debug(\n        \"AzureSQL connection initialized\",\n        server=server,\n        database=database,\n        auth_mode=auth_mode,\n        driver=driver,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.close","title":"<code>close()</code>","text":"<p>Close database connection and dispose of engine.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def close(self):\n    \"\"\"Close database connection and dispose of engine.\"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Closing AzureSQL connection\",\n        server=self.server,\n        database=self.database,\n    )\n\n    if self._engine:\n        self._engine.dispose()\n        self._engine = None\n        ctx.info(\n            \"AzureSQL connection closed\",\n            server=self.server,\n            database=self.database,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.execute","title":"<code>execute(sql, params=None)</code>","text":"<p>Execute SQL statement (INSERT, UPDATE, DELETE, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL statement</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for parameterized query</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result from execution</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n    Args:\n        sql: SQL statement\n        params: Optional parameters for parameterized query\n\n    Returns:\n        Result from execution\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Executing SQL statement\",\n        server=self.server,\n        database=self.database,\n        statement_length=len(sql),\n    )\n\n    try:\n        engine = self.get_engine()\n        from sqlalchemy import text\n\n        with engine.connect() as conn:\n            result = conn.execute(text(sql), params or {})\n            conn.commit()\n\n            ctx.info(\n                \"SQL statement executed successfully\",\n                server=self.server,\n                database=self.database,\n            )\n            return result\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"SQL statement execution failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Statement execution failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_engine","title":"<code>get_engine()</code>","text":"<p>Get or create SQLAlchemy engine.</p> <p>Returns:</p> Type Description <p>SQLAlchemy engine instance</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection fails or drivers missing</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_engine(self):\n    \"\"\"\n    Get or create SQLAlchemy engine.\n\n    Returns:\n        SQLAlchemy engine instance\n\n    Raises:\n        ConnectionError: If connection fails or drivers missing\n    \"\"\"\n    ctx = get_logging_context()\n\n    if self._engine is not None:\n        ctx.debug(\n            \"Using cached SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n        )\n        return self._engine\n\n    ctx.debug(\n        \"Creating SQLAlchemy engine\",\n        server=self.server,\n        database=self.database,\n    )\n\n    try:\n        from urllib.parse import quote_plus\n\n        from sqlalchemy import create_engine\n    except ImportError as e:\n        ctx.error(\n            \"SQLAlchemy import failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=\"Required packages 'sqlalchemy' or 'pyodbc' not found.\",\n            suggestions=[\n                \"Install required packages: pip install sqlalchemy pyodbc\",\n                \"Or install odibi with azure extras: pip install 'odibi[azure]'\",\n            ],\n        )\n\n    try:\n        # Build connection string\n        conn_str = self.odbc_dsn()\n        connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}\"\n\n        ctx.debug(\n            \"Creating SQLAlchemy engine with connection pooling\",\n            server=self.server,\n            database=self.database,\n        )\n\n        # Create engine with connection pooling\n        self._engine = create_engine(\n            connection_url,\n            pool_pre_ping=True,  # Verify connections before use\n            pool_recycle=3600,  # Recycle connections after 1 hour\n            echo=False,\n        )\n\n        # Test connection\n        with self._engine.connect():\n            pass\n\n        ctx.info(\n            \"SQLAlchemy engine created successfully\",\n            server=self.server,\n            database=self.database,\n        )\n\n        return self._engine\n\n    except Exception as e:\n        suggestions = self._get_error_suggestions(str(e))\n        ctx.error(\n            \"Failed to create SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n            suggestions=suggestions,\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Failed to create engine: {str(e)}\",\n            suggestions=suggestions,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_password","title":"<code>get_password()</code>","text":"<p>Get password (cached).</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_password(self) -&gt; Optional[str]:\n    \"\"\"Get password (cached).\"\"\"\n    ctx = get_logging_context()\n\n    if self.password:\n        ctx.debug(\n            \"Using provided password\",\n            server=self.server,\n            database=self.database,\n        )\n        return self.password\n\n    if self._cached_key:\n        ctx.debug(\n            \"Using cached password\",\n            server=self.server,\n            database=self.database,\n        )\n        return self._cached_key\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"Key Vault mode requires key_vault_name and secret_name\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"key_vault mode requires key_vault_name and secret_name\")\n\n        ctx.debug(\n            \"Fetching password from Key Vault\",\n            server=self.server,\n            key_vault_name=self.key_vault_name,\n            secret_name=self.secret_name,\n        )\n\n        try:\n            from azure.identity import DefaultAzureCredential\n            from azure.keyvault.secrets import SecretClient\n\n            credential = DefaultAzureCredential()\n            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n            client = SecretClient(vault_url=kv_uri, credential=credential)\n            secret = client.get_secret(self.secret_name)\n            self._cached_key = secret.value\n            logger.register_secret(self._cached_key)\n\n            ctx.info(\n                \"Successfully fetched password from Key Vault\",\n                server=self.server,\n                key_vault_name=self.key_vault_name,\n            )\n            return self._cached_key\n        except ImportError as e:\n            ctx.error(\n                \"Key Vault support requires azure libraries\",\n                server=self.server,\n                error=str(e),\n            )\n            raise ImportError(\n                \"Key Vault support requires 'azure-identity' and 'azure-keyvault-secrets'\"\n            )\n\n    ctx.debug(\n        \"No password required for auth_mode\",\n        server=self.server,\n        auth_mode=self.auth_mode,\n    )\n    return None\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get table reference for relative path.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get table reference for relative path.\"\"\"\n    return relative_path\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_spark_options","title":"<code>get_spark_options()</code>","text":"<p>Get Spark JDBC options.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary of Spark JDBC options (url, user, password, etc.)</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_spark_options(self) -&gt; Dict[str, str]:\n    \"\"\"Get Spark JDBC options.\n\n    Returns:\n        Dictionary of Spark JDBC options (url, user, password, etc.)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Building Spark JDBC options\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    jdbc_url = (\n        f\"jdbc:sqlserver://{self.server}:{self.port};\"\n        f\"databaseName={self.database};encrypt=true;trustServerCertificate=true;\"\n    )\n\n    if self.auth_mode == \"aad_msi\":\n        jdbc_url += (\n            \"hostNameInCertificate=*.database.windows.net;\"\n            \"loginTimeout=30;authentication=ActiveDirectoryMsi;\"\n        )\n        ctx.debug(\n            \"Configured JDBC URL for AAD MSI\",\n            server=self.server,\n        )\n    elif self.auth_mode == \"aad_service_principal\":\n        # Not fully implemented in init yet, but placeholder\n        ctx.debug(\n            \"Configured JDBC URL for AAD Service Principal\",\n            server=self.server,\n        )\n\n    options = {\n        \"url\": jdbc_url,\n        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n    }\n\n    if self.auth_mode == \"sql\" or self.auth_mode == \"key_vault\":\n        if self.username:\n            options[\"user\"] = self.username\n\n        pwd = self.get_password()\n        if pwd:\n            options[\"password\"] = pwd\n\n        ctx.debug(\n            \"Added SQL authentication to Spark options\",\n            server=self.server,\n            username=self.username,\n        )\n\n    ctx.info(\n        \"Spark JDBC options built successfully\",\n        server=self.server,\n        database=self.database,\n    )\n\n    return options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.odbc_dsn","title":"<code>odbc_dsn()</code>","text":"<p>Build ODBC connection string.</p> <p>Returns:</p> Type Description <code>str</code> <p>ODBC DSN string</p> Example <p>conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\") conn.odbc_dsn() 'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def odbc_dsn(self) -&gt; str:\n    \"\"\"Build ODBC connection string.\n\n    Returns:\n        ODBC DSN string\n\n    Example:\n        &gt;&gt;&gt; conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\")\n        &gt;&gt;&gt; conn.odbc_dsn()\n        'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Building ODBC connection string\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    dsn = (\n        f\"Driver={{{self.driver}}};\"\n        f\"Server=tcp:{self.server},1433;\"\n        f\"Database={self.database};\"\n        f\"Encrypt=yes;\"\n        f\"TrustServerCertificate=yes;\"\n        f\"Connection Timeout=30;\"\n    )\n\n    pwd = self.get_password()\n    if self.username and pwd:\n        dsn += f\"UID={self.username};PWD={pwd};\"\n        ctx.debug(\n            \"Using SQL authentication\",\n            server=self.server,\n            username=self.username,\n        )\n    elif self.auth_mode == \"aad_msi\":\n        dsn += \"Authentication=ActiveDirectoryMsi;\"\n        ctx.debug(\n            \"Using AAD Managed Identity authentication\",\n            server=self.server,\n        )\n    elif self.auth_mode == \"aad_service_principal\":\n        # Not fully supported via ODBC string simply without token usually\n        ctx.debug(\n            \"Using AAD Service Principal authentication\",\n            server=self.server,\n        )\n\n    return dsn\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_sql","title":"<code>read_sql(query, params=None)</code>","text":"<p>Execute SQL query and return results as DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional query parameters for parameterized queries</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Query results as pandas DataFrame</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_sql(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute SQL query and return results as DataFrame.\n\n    Args:\n        query: SQL query string\n        params: Optional query parameters for parameterized queries\n\n    Returns:\n        Query results as pandas DataFrame\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Executing SQL query\",\n        server=self.server,\n        database=self.database,\n        query_length=len(query),\n    )\n\n    try:\n        engine = self.get_engine()\n        result = pd.read_sql(query, engine, params=params)\n\n        ctx.info(\n            \"SQL query executed successfully\",\n            server=self.server,\n            database=self.database,\n            rows_returned=len(result),\n        )\n        return result\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"SQL query execution failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Query execution failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_table","title":"<code>read_table(table_name, schema='dbo')</code>","text":"<p>Read entire table into DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>schema</code> <code>Optional[str]</code> <p>Schema name (default: dbo)</p> <code>'dbo'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table contents as pandas DataFrame</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_table(self, table_name: str, schema: Optional[str] = \"dbo\") -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire table into DataFrame.\n\n    Args:\n        table_name: Name of the table\n        schema: Schema name (default: dbo)\n\n    Returns:\n        Table contents as pandas DataFrame\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Reading table\",\n        server=self.server,\n        database=self.database,\n        table_name=table_name,\n        schema=schema,\n    )\n\n    if schema:\n        query = f\"SELECT * FROM [{schema}].[{table_name}]\"\n    else:\n        query = f\"SELECT * FROM [{table_name}]\"\n\n    return self.read_sql(query)\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.validate","title":"<code>validate()</code>","text":"<p>Validate Azure SQL connection configuration.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate Azure SQL connection configuration.\"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating AzureSQL connection\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    if not self.server:\n        ctx.error(\"AzureSQL validation failed: missing 'server'\")\n        raise ValueError(\"Azure SQL connection requires 'server'\")\n    if not self.database:\n        ctx.error(\n            \"AzureSQL validation failed: missing 'database'\",\n            server=self.server,\n        )\n        raise ValueError(\"Azure SQL connection requires 'database'\")\n\n    if self.auth_mode == \"sql\":\n        if not self.username:\n            ctx.error(\n                \"AzureSQL validation failed: SQL auth requires username\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"Azure SQL with auth_mode='sql' requires username\")\n        if not self.password and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"AzureSQL validation failed: SQL auth requires password\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                \"Azure SQL with auth_mode='sql' requires password \"\n                \"(or key_vault_name/secret_name)\"\n            )\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"AzureSQL validation failed: key_vault mode missing config\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                \"Azure SQL with auth_mode='key_vault' requires key_vault_name and secret_name\"\n            )\n        if not self.username:\n            ctx.error(\n                \"AzureSQL validation failed: key_vault mode requires username\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"Azure SQL with auth_mode='key_vault' requires username\")\n\n    ctx.info(\n        \"AzureSQL connection validated successfully\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.write_table","title":"<code>write_table(df, table_name, schema='dbo', if_exists='replace', index=False, chunksize=1000)</code>","text":"<p>Write DataFrame to SQL table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>schema</code> <code>Optional[str]</code> <p>Schema name (default: dbo)</p> <code>'dbo'</code> <code>if_exists</code> <code>str</code> <p>How to behave if table exists ('fail', 'replace', 'append')</p> <code>'replace'</code> <code>index</code> <code>bool</code> <p>Whether to write DataFrame index as column</p> <code>False</code> <code>chunksize</code> <code>Optional[int]</code> <p>Number of rows to write in each batch (default: 1000)</p> <code>1000</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows written</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If write fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def write_table(\n    self,\n    df: pd.DataFrame,\n    table_name: str,\n    schema: Optional[str] = \"dbo\",\n    if_exists: str = \"replace\",\n    index: bool = False,\n    chunksize: Optional[int] = 1000,\n) -&gt; int:\n    \"\"\"\n    Write DataFrame to SQL table.\n\n    Args:\n        df: DataFrame to write\n        table_name: Name of the table\n        schema: Schema name (default: dbo)\n        if_exists: How to behave if table exists ('fail', 'replace', 'append')\n        index: Whether to write DataFrame index as column\n        chunksize: Number of rows to write in each batch (default: 1000)\n\n    Returns:\n        Number of rows written\n\n    Raises:\n        ConnectionError: If write fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Writing DataFrame to table\",\n        server=self.server,\n        database=self.database,\n        table_name=table_name,\n        schema=schema,\n        rows=len(df),\n        if_exists=if_exists,\n        chunksize=chunksize,\n    )\n\n    try:\n        engine = self.get_engine()\n\n        rows_written = df.to_sql(\n            name=table_name,\n            con=engine,\n            schema=schema,\n            if_exists=if_exists,\n            index=index,\n            chunksize=chunksize,\n            method=\"multi\",  # Use multi-row INSERT for better performance\n        )\n\n        result_rows = rows_written if rows_written is not None else len(df)\n        ctx.info(\n            \"Table write completed successfully\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            rows_written=result_rows,\n        )\n        return result_rows\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"Table write failed\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Write operation failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/engine/","title":"Engine API","text":""},{"location":"reference/api/engine/#odibi.engine.base","title":"<code>odibi.engine.base</code>","text":"<p>Base engine interface.</p>"},{"location":"reference/api/engine/#odibi.engine.base.Engine","title":"<code>Engine</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for execution engines.</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>class Engine(ABC):\n    \"\"\"Abstract base class for execution engines.\"\"\"\n\n    # Custom format registry\n    _custom_readers: Dict[str, Any] = {}\n    _custom_writers: Dict[str, Any] = {}\n\n    @classmethod\n    def register_format(cls, fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None):\n        \"\"\"Register custom format reader/writer.\n\n        Args:\n            fmt: Format name (e.g. 'netcdf')\n            reader: Function(path, **options) -&gt; DataFrame\n            writer: Function(df, path, **options) -&gt; None\n        \"\"\"\n        if reader:\n            cls._custom_readers[fmt] = reader\n        if writer:\n            cls._custom_writers[fmt] = writer\n\n    @abstractmethod\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n    ) -&gt; Any:\n        \"\"\"Read data from source.\n\n        Args:\n            connection: Connection object\n            format: Data format (csv, parquet, delta, etc.)\n            table: Table name (for SQL/Delta)\n            path: File path (for file-based sources)\n            options: Format-specific options\n\n        Returns:\n            DataFrame (engine-specific type)\n        \"\"\"\n        pass\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Materialized DataFrame\n        \"\"\"\n        return df\n\n    @abstractmethod\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Write data to destination.\n\n        Args:\n            df: DataFrame to write\n            connection: Connection object\n            format: Output format\n            table: Table name (for SQL/Delta)\n            path: File path (for file-based outputs)\n            mode: Write mode (overwrite/append)\n            options: Format-specific options\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_sql(self, sql: str, context: Context) -&gt; Any:\n        \"\"\"Execute SQL query.\n\n        Args:\n            sql: SQL query string\n            context: Execution context with registered DataFrames\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n        \"\"\"Execute built-in operation (pivot, etc.).\n\n        Args:\n            operation: Operation name\n            params: Operation parameters\n            df: Input DataFrame\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_schema(self, df: Any) -&gt; Any:\n        \"\"\"Get DataFrame schema.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dict[str, str] mapping column names to types, or List[str] of names (deprecated)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            (rows, columns)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Row count\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\n\n        Args:\n            df: DataFrame\n            columns: Columns to check\n\n        Returns:\n            Dictionary of column -&gt; null count\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\n\n        Args:\n            df: DataFrame\n            schema_rules: Validation rules\n\n        Returns:\n            List of validation failures (empty if valid)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate data against rules.\n\n        Args:\n            df: DataFrame to validate\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages (empty if valid)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\n\n        Args:\n            df: DataFrame\n            n: Number of rows to return\n\n        Returns:\n            List of row dictionaries\n        \"\"\"\n        pass\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            List of file paths (or empty list if not applicable/supported)\n        \"\"\"\n        return []\n\n    def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dictionary of {column_name: null_percentage} (0.0 to 1.0)\n        \"\"\"\n        return {}\n\n    @abstractmethod\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Args:\n            connection: Connection object\n            table: Table name (for catalog tables)\n            path: File path (for path-based tables)\n\n        Returns:\n            True if table/location exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\n\n        Args:\n            df: Input DataFrame\n            target_schema: Target schema (column name -&gt; type)\n            policy: SchemaPolicyConfig object\n\n        Returns:\n            Harmonized DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; Any:\n        \"\"\"Anonymize specified columns.\n\n        Args:\n            df: DataFrame to anonymize\n            columns: List of columns to anonymize\n            method: Method ('hash', 'mask', 'redact')\n            salt: Optional salt for hashing\n\n        Returns:\n            Anonymized DataFrame\n        \"\"\"\n        pass\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\n\n        Args:\n            connection: Connection object\n            table: Table name\n            path: File path\n            format: Data format (optional, helps with file-based sources)\n\n        Returns:\n            Schema dict or None if table doesn't exist or schema fetch fails.\n        \"\"\"\n        return None\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\n\n        Args:\n            connection: Connection object\n            format: Table format\n            table: Table name\n            path: Table path\n            config: AutoOptimizeConfig object\n        \"\"\"\n        pass\n\n    def add_write_metadata(\n        self,\n        df: Any,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; Any:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added (or unchanged if metadata_config is None/False)\n        \"\"\"\n        return df  # Default: no-op\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with metadata columns added (or unchanged if metadata_config is None/False)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: Any,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; Any:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added (or unchanged if metadata_config is None/False)\n    \"\"\"\n    return df  # Default: no-op\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>  <code>abstractmethod</code>","text":"<p>Anonymize specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to anonymize</p> required <code>columns</code> <code>List[str]</code> <p>List of columns to anonymize</p> required <code>method</code> <code>str</code> <p>Method ('hash', 'mask', 'redact')</p> required <code>salt</code> <code>Optional[str]</code> <p>Optional salt for hashing</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Anonymized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; Any:\n    \"\"\"Anonymize specified columns.\n\n    Args:\n        df: DataFrame to anonymize\n        columns: List of columns to anonymize\n        method: Method ('hash', 'mask', 'redact')\n        salt: Optional salt for hashing\n\n    Returns:\n        Anonymized DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.count_nulls","title":"<code>count_nulls(df, columns)</code>  <code>abstractmethod</code>","text":"<p>Count nulls in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>columns</code> <code>List[str]</code> <p>Columns to check</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary of column -&gt; null count</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\n\n    Args:\n        df: DataFrame\n        columns: Columns to check\n\n    Returns:\n        Dictionary of column -&gt; null count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.count_rows","title":"<code>count_rows(df)</code>  <code>abstractmethod</code>","text":"<p>Count rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>int</code> <p>Row count</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Row count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>  <code>abstractmethod</code>","text":"<p>Execute built-in operation (pivot, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Operation name</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Operation parameters</p> required <code>df</code> <code>Any</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n    \"\"\"Execute built-in operation (pivot, etc.).\n\n    Args:\n        operation: Operation name\n        params: Operation parameters\n        df: Input DataFrame\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.execute_sql","title":"<code>execute_sql(sql, context)</code>  <code>abstractmethod</code>","text":"<p>Execute SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context with registered DataFrames</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef execute_sql(self, sql: str, context: Context) -&gt; Any:\n    \"\"\"Execute SQL query.\n\n    Args:\n        sql: SQL query string\n        context: Execution context with registered DataFrames\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_sample","title":"<code>get_sample(df, n=10)</code>  <code>abstractmethod</code>","text":"<p>Get sample rows as list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>n</code> <code>int</code> <p>Number of rows to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of row dictionaries</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\n\n    Args:\n        df: DataFrame\n        n: Number of rows to return\n\n    Returns:\n        List of row dictionaries\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_schema","title":"<code>get_schema(df)</code>  <code>abstractmethod</code>","text":"<p>Get DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dict[str, str] mapping column names to types, or List[str] of names (deprecated)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_schema(self, df: Any) -&gt; Any:\n    \"\"\"Get DataFrame schema.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dict[str, str] mapping column names to types, or List[str] of names (deprecated)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_shape","title":"<code>get_shape(df)</code>  <code>abstractmethod</code>","text":"<p>Get DataFrame shape.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(rows, columns)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        (rows, columns)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths (or empty list if not applicable/supported)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        List of file paths (or empty list if not applicable/supported)\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Data format (optional, helps with file-based sources)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, str]]</code> <p>Schema dict or None if table doesn't exist or schema fetch fails.</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\n\n    Args:\n        connection: Connection object\n        table: Table name\n        path: File path\n        format: Data format (optional, helps with file-based sources)\n\n    Returns:\n        Schema dict or None if table doesn't exist or schema fetch fails.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>  <code>abstractmethod</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Input DataFrame</p> required <code>target_schema</code> <code>Dict[str, str]</code> <p>Target schema (column name -&gt; type)</p> required <code>policy</code> <code>Any</code> <p>SchemaPolicyConfig object</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Harmonized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\n\n    Args:\n        df: Input DataFrame\n        target_schema: Target schema (column name -&gt; type)\n        policy: SchemaPolicyConfig object\n\n    Returns:\n        Harmonized DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Table format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>Table path</p> <code>None</code> <code>config</code> <code>Optional[Any]</code> <p>AutoOptimizeConfig object</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\n\n    Args:\n        connection: Connection object\n        format: Table format\n        table: Table name\n        path: Table path\n        config: AutoOptimizeConfig object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset into memory (DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Materialized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Materialized DataFrame\n    \"\"\"\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of {column_name: null_percentage} (0.0 to 1.0)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dictionary of {column_name: null_percentage} (0.0 to 1.0)\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.read","title":"<code>read(connection, format, table=None, path=None, options=None)</code>  <code>abstractmethod</code>","text":"<p>Read data from source.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Data format (csv, parquet, delta, etc.)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for SQL/Delta)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for file-based sources)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame (engine-specific type)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n) -&gt; Any:\n    \"\"\"Read data from source.\n\n    Args:\n        connection: Connection object\n        format: Data format (csv, parquet, delta, etc.)\n        table: Table name (for SQL/Delta)\n        path: File path (for file-based sources)\n        options: Format-specific options\n\n    Returns:\n        DataFrame (engine-specific type)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.register_format","title":"<code>register_format(fmt, reader=None, writer=None)</code>  <code>classmethod</code>","text":"<p>Register custom format reader/writer.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>str</code> <p>Format name (e.g. 'netcdf')</p> required <code>reader</code> <code>Optional[Any]</code> <p>Function(path, **options) -&gt; DataFrame</p> <code>None</code> <code>writer</code> <code>Optional[Any]</code> <p>Function(df, path, **options) -&gt; None</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@classmethod\ndef register_format(cls, fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None):\n    \"\"\"Register custom format reader/writer.\n\n    Args:\n        fmt: Format name (e.g. 'netcdf')\n        reader: Function(path, **options) -&gt; DataFrame\n        writer: Function(df, path, **options) -&gt; None\n    \"\"\"\n    if reader:\n        cls._custom_readers[fmt] = reader\n    if writer:\n        cls._custom_writers[fmt] = writer\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>  <code>abstractmethod</code>","text":"<p>Check if table or location exists.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for catalog tables)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for path-based tables)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if table/location exists, False otherwise</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Args:\n        connection: Connection object\n        table: Table name (for catalog tables)\n        path: File path (for path-based tables)\n\n    Returns:\n        True if table/location exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.validate_data","title":"<code>validate_data(df, validation_config)</code>  <code>abstractmethod</code>","text":"<p>Validate data against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to validate</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages (empty if valid)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate data against rules.\n\n    Args:\n        df: DataFrame to validate\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages (empty if valid)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>  <code>abstractmethod</code>","text":"<p>Validate DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>schema_rules</code> <code>Dict[str, Any]</code> <p>Validation rules</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failures (empty if valid)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\n\n    Args:\n        df: DataFrame\n        schema_rules: Validation rules\n\n    Returns:\n        List of validation failures (empty if valid)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.write","title":"<code>write(df, connection, format, table=None, path=None, mode='overwrite', options=None, streaming_config=None)</code>  <code>abstractmethod</code>","text":"<p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to write</p> required <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Output format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for SQL/Delta)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for file-based outputs)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Write mode (overwrite/append)</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Write data to destination.\n\n    Args:\n        df: DataFrame to write\n        connection: Connection object\n        format: Output format\n        table: Table name (for SQL/Delta)\n        path: File path (for file-based outputs)\n        mode: Write mode (overwrite/append)\n        options: Format-specific options\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine","title":"<code>odibi.engine.pandas_engine</code>","text":"<p>Pandas engine implementation.</p>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.LazyDataset","title":"<code>LazyDataset</code>  <code>dataclass</code>","text":"<p>Lazy representation of a dataset (file) for out-of-core processing.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>@dataclass\nclass LazyDataset:\n    \"\"\"Lazy representation of a dataset (file) for out-of-core processing.\"\"\"\n\n    path: Union[str, List[str]]\n    format: str\n    options: Dict[str, Any]\n    connection: Optional[Any] = None  # To resolve path/credentials if needed\n\n    def __repr__(self):\n        return f\"LazyDataset(path={self.path}, format={self.format})\"\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine","title":"<code>PandasEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Pandas-based execution engine.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>class PandasEngine(Engine):\n    \"\"\"Pandas-based execution engine.\"\"\"\n\n    name = \"pandas\"\n    engine_type = EngineType.PANDAS\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Pandas engine.\n\n        Args:\n            connections: Dictionary of connection objects\n            config: Engine configuration (optional)\n        \"\"\"\n        self.connections = connections or {}\n        self.config = config or {}\n\n        # Suppress noisy delta-rs transaction conflict warnings (handled by retry)\n        if \"RUST_LOG\" not in os.environ:\n            os.environ[\"RUST_LOG\"] = \"deltalake_core::kernel::transaction=error\"\n\n        # Check for performance flags\n        performance = self.config.get(\"performance\", {})\n\n        # Determine desired state\n        if hasattr(performance, \"use_arrow\"):\n            desired_use_arrow = performance.use_arrow\n        elif isinstance(performance, dict):\n            desired_use_arrow = performance.get(\"use_arrow\", True)\n        else:\n            desired_use_arrow = True\n\n        # Verify availability\n        if desired_use_arrow:\n            try:\n                import pyarrow  # noqa: F401\n\n                self.use_arrow = True\n            except ImportError:\n                import logging\n\n                logger = logging.getLogger(__name__)\n                logger.warning(\n                    \"Apache Arrow not found. Disabling Arrow optimizations. \"\n                    \"Install 'pyarrow' to enable.\"\n                )\n                self.use_arrow = False\n        else:\n            self.use_arrow = False\n\n        # Check for DuckDB\n        self.use_duckdb = False\n        # Default to False to ensure stability with existing tests (Lazy Loading is opt-in)\n        if self.config.get(\"performance\", {}).get(\"use_duckdb\", False):\n            try:\n                import duckdb  # noqa: F401\n\n                self.use_duckdb = True\n            except ImportError:\n                pass\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset.\"\"\"\n        if isinstance(df, LazyDataset):\n            # Re-invoke read but force materialization (by bypassing Lazy check)\n            # We pass the resolved path directly\n            # Note: We need to handle the case where path was resolved.\n            # LazyDataset.path should be the FULL path.\n            return self._read_file(\n                full_path=df.path, format=df.format, options=df.options, connection=df.connection\n            )\n        return df\n\n    def _process_df(\n        self, df: Union[pd.DataFrame, Iterator[pd.DataFrame]], query: Optional[str]\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Apply post-read processing (filtering).\"\"\"\n        if query and df is not None:\n            # Handle Iterator\n            from collections.abc import Iterator\n\n            if isinstance(df, Iterator):\n                # Filter each chunk\n                return (chunk.query(query) for chunk in df)\n\n            if not df.empty:\n                try:\n                    return df.query(query)\n                except Exception as e:\n                    import logging\n\n                    logger = logging.getLogger(__name__)\n                    logger.warning(f\"Failed to apply query '{query}': {e}\")\n        return df\n\n    _CLOUD_URI_PREFIXES = (\"abfss://\", \"s3://\", \"gs://\", \"az://\", \"https://\")\n\n    def _retry_delta_operation(self, func, max_retries: int = 5, base_delay: float = 0.2):\n        \"\"\"Retry Delta operations with exponential backoff for concurrent conflicts.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                return func()\n            except Exception as e:\n                error_str = str(e).lower()\n                is_conflict = \"conflict\" in error_str or \"concurrent\" in error_str\n                if attempt == max_retries - 1 or not is_conflict:\n                    raise\n                delay = base_delay * (2**attempt) + random.uniform(0, 0.1)\n                time.sleep(delay)\n\n    def _resolve_path(self, path: Optional[str], connection: Any) -&gt; str:\n        \"\"\"Resolve path to full URI, avoiding double-prefixing for cloud URIs.\n\n        Args:\n            path: Relative or absolute path\n            connection: Connection object (may have get_path method)\n\n        Returns:\n            Full resolved path\n        \"\"\"\n        if not path:\n            raise ValueError(\"Path must be provided\")\n        if path.startswith(self._CLOUD_URI_PREFIXES):\n            return path\n        if connection:\n            return connection.get_path(path)\n        return path\n\n    def _merge_storage_options(\n        self, connection: Any, options: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Merge connection storage options with user options.\n\n        Args:\n            connection: Connection object (may have pandas_storage_options method)\n            options: User-provided options\n\n        Returns:\n            Merged options dictionary\n        \"\"\"\n        options = options or {}\n\n        # If connection provides storage_options (e.g., AzureADLS), merge them\n        if hasattr(connection, \"pandas_storage_options\"):\n            conn_storage_opts = connection.pandas_storage_options()\n            user_storage_opts = options.get(\"storage_options\", {})\n\n            # User options override connection options\n            merged_storage_opts = {**conn_storage_opts, **user_storage_opts}\n\n            # Return options with merged storage_options\n            return {**options, \"storage_options\": merged_storage_opts}\n\n        return options\n\n    def _read_parallel(self, read_func: Any, paths: List[str], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Read multiple files in parallel using threads.\n\n        Args:\n            read_func: Pandas read function (e.g. pd.read_csv)\n            paths: List of file paths\n            kwargs: Arguments to pass to read_func\n\n        Returns:\n            Concatenated DataFrame\n        \"\"\"\n        # Conservative worker count to avoid OOM on large files\n        max_workers = min(8, os.cpu_count() or 4)\n\n        dfs = []\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # map preserves order\n            results = executor.map(lambda p: read_func(p, **kwargs), paths)\n            dfs = list(results)\n\n        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        as_of_version: Optional[int] = None,\n        as_of_timestamp: Optional[str] = None,\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Read data using Pandas (or LazyDataset).\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        source = path or table\n        ctx.debug(\n            \"Starting read operation\",\n            format=format,\n            path=source,\n            streaming=streaming,\n            use_arrow=self.use_arrow,\n        )\n\n        if streaming:\n            ctx.error(\n                \"Streaming not supported in Pandas engine\",\n                format=format,\n                path=source,\n            )\n            raise ValueError(\n                \"Streaming is not supported in the Pandas engine. \"\n                \"Please use 'engine: spark' for streaming pipelines.\"\n            )\n\n        options = options or {}\n\n        # Resolve full path from connection\n        try:\n            full_path = self._resolve_path(path or table, connection)\n        except ValueError:\n            if table and not connection:\n                ctx.error(\"Connection required when specifying 'table'\", table=table)\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n            ctx.error(\"Neither path nor table provided for read operation\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Merge storage options for cloud connections\n        merged_options = self._merge_storage_options(connection, options)\n\n        # Sanitize options for pandas compatibility\n        if \"header\" in merged_options:\n            if merged_options[\"header\"] is True:\n                merged_options[\"header\"] = 0\n            elif merged_options[\"header\"] is False:\n                merged_options[\"header\"] = None\n\n        # Handle Time Travel options\n        if as_of_version is not None:\n            merged_options[\"versionAsOf\"] = as_of_version\n            ctx.debug(\"Time travel enabled\", version=as_of_version)\n        if as_of_timestamp is not None:\n            merged_options[\"timestampAsOf\"] = as_of_timestamp\n            ctx.debug(\"Time travel enabled\", timestamp=as_of_timestamp)\n\n        # Check for Lazy/DuckDB optimization\n        can_lazy_load = False\n\n        if can_lazy_load:\n            ctx.debug(\"Using lazy loading via DuckDB\", path=str(full_path))\n            if isinstance(full_path, (str, Path)):\n                return LazyDataset(\n                    path=str(full_path),\n                    format=format,\n                    options=merged_options,\n                    connection=connection,\n                )\n            elif isinstance(full_path, list):\n                return LazyDataset(\n                    path=full_path, format=format, options=merged_options, connection=connection\n                )\n\n        result = self._read_file(full_path, format, merged_options, connection)\n\n        # Log metrics for materialized DataFrames\n        elapsed = (time.time() - start) * 1000\n        if isinstance(result, pd.DataFrame):\n            row_count = len(result)\n            memory_mb = result.memory_usage(deep=True).sum() / (1024 * 1024)\n\n            ctx.log_file_io(\n                path=str(full_path) if not isinstance(full_path, list) else str(full_path[0]),\n                format=format,\n                mode=\"read\",\n                rows=row_count,\n            )\n            ctx.log_pandas_metrics(\n                memory_mb=memory_mb,\n                dtypes={col: str(dtype) for col, dtype in result.dtypes.items()},\n            )\n            ctx.info(\n                \"Read completed\",\n                format=format,\n                rows=row_count,\n                elapsed_ms=round(elapsed, 2),\n                memory_mb=round(memory_mb, 2),\n            )\n\n        return result\n\n    def _read_file(\n        self,\n        full_path: Union[str, List[str], Any],\n        format: str,\n        options: Dict[str, Any],\n        connection: Any = None,\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Internal file reading logic.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        ctx.debug(\n            \"Reading file\",\n            path=str(full_path) if not isinstance(full_path, list) else f\"{len(full_path)} files\",\n            format=format,\n        )\n\n        # Custom Readers\n        if format in self._custom_readers:\n            ctx.debug(f\"Using custom reader for format: {format}\")\n            return self._custom_readers[format](full_path, **options)\n\n        # Handle glob patterns for local files\n        is_glob = False\n        if isinstance(full_path, (str, Path)) and (\n            \"*\" in str(full_path) or \"?\" in str(full_path) or \"[\" in str(full_path)\n        ):\n            parsed = urlparse(str(full_path))\n            # Only expand for local files (no scheme, file://, or drive letter)\n            is_local = (\n                not parsed.scheme\n                or parsed.scheme == \"file\"\n                or (len(parsed.scheme) == 1 and parsed.scheme.isalpha())\n            )\n\n            if is_local:\n                glob_path = str(full_path)\n                if glob_path.startswith(\"file:///\"):\n                    glob_path = glob_path[8:]\n                elif glob_path.startswith(\"file://\"):\n                    glob_path = glob_path[7:]\n\n                matched_files = glob.glob(glob_path)\n                if not matched_files:\n                    ctx.error(\n                        \"No files matched glob pattern\",\n                        pattern=glob_path,\n                    )\n                    raise FileNotFoundError(f\"No files matched pattern: {glob_path}\")\n\n                ctx.info(\n                    \"Glob pattern expanded\",\n                    pattern=glob_path,\n                    matched_files=len(matched_files),\n                )\n                full_path = matched_files\n                is_glob = True\n\n        # Prepare read options (options already includes storage_options from caller)\n        read_kwargs = options.copy()\n\n        # Extract 'query' or 'filter' option for post-read filtering\n        post_read_query = read_kwargs.pop(\"query\", None) or read_kwargs.pop(\"filter\", None)\n\n        if self.use_arrow:\n            read_kwargs[\"dtype_backend\"] = \"pyarrow\"\n\n        # Read based on format\n        if format == \"csv\":\n            try:\n                if is_glob and isinstance(full_path, list):\n                    ctx.debug(\n                        \"Parallel CSV read\",\n                        file_count=len(full_path),\n                    )\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n            except UnicodeDecodeError:\n                ctx.warning(\n                    \"UnicodeDecodeError, retrying with latin1 encoding\",\n                    path=str(full_path),\n                )\n                read_kwargs[\"encoding\"] = \"latin1\"\n                if is_glob and isinstance(full_path, list):\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n            except pd.errors.ParserError:\n                ctx.warning(\n                    \"ParserError, retrying with on_bad_lines='skip'\",\n                    path=str(full_path),\n                )\n                read_kwargs[\"on_bad_lines\"] = \"skip\"\n                if is_glob and isinstance(full_path, list):\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n        elif format == \"parquet\":\n            ctx.debug(\"Reading parquet\", path=str(full_path))\n            df = pd.read_parquet(full_path, **read_kwargs)\n            if isinstance(full_path, list):\n                df.attrs[\"odibi_source_files\"] = full_path\n            else:\n                df.attrs[\"odibi_source_files\"] = [str(full_path)]\n            return self._process_df(df, post_read_query)\n        elif format == \"json\":\n            if is_glob and isinstance(full_path, list):\n                ctx.debug(\n                    \"Parallel JSON read\",\n                    file_count=len(full_path),\n                )\n                df = self._read_parallel(pd.read_json, full_path, **read_kwargs)\n                df.attrs[\"odibi_source_files\"] = full_path\n                return self._process_df(df, post_read_query)\n\n            df = pd.read_json(full_path, **read_kwargs)\n            if hasattr(df, \"attrs\"):\n                df.attrs[\"odibi_source_files\"] = [str(full_path)]\n            return self._process_df(df, post_read_query)\n        elif format == \"excel\":\n            ctx.debug(\"Reading Excel file\", path=str(full_path))\n            read_kwargs.pop(\"dtype_backend\", None)\n            return self._process_df(pd.read_excel(full_path, **read_kwargs), post_read_query)\n        elif format == \"delta\":\n            ctx.debug(\"Reading Delta table\", path=str(full_path))\n            try:\n                from deltalake import DeltaTable\n            except ImportError:\n                ctx.error(\n                    \"Delta Lake library not installed\",\n                    path=str(full_path),\n                )\n                raise ImportError(\n                    \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                    \"or 'pip install deltalake'. See README.md for installation instructions.\"\n                )\n\n            storage_opts = options.get(\"storage_options\", {})\n            version = options.get(\"versionAsOf\")\n            timestamp = options.get(\"timestampAsOf\")\n\n            if timestamp is not None:\n                from datetime import datetime as dt_module\n\n                if isinstance(timestamp, str):\n                    ts = dt_module.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n                else:\n                    ts = timestamp\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                dt.load_with_datetime(ts)\n                ctx.debug(\"Delta table loaded with timestamp\", timestamp=str(ts))\n            elif version is not None:\n                dt = DeltaTable(full_path, storage_options=storage_opts, version=version)\n                ctx.debug(\"Delta table loaded with version\", version=version)\n            else:\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                ctx.debug(\"Delta table loaded (latest version)\")\n\n            if self.use_arrow:\n                import inspect\n\n                sig = inspect.signature(dt.to_pandas)\n\n                if \"arrow_options\" in sig.parameters:\n                    return self._process_df(\n                        dt.to_pandas(\n                            partitions=None, arrow_options={\"types_mapper\": pd.ArrowDtype}\n                        ),\n                        post_read_query,\n                    )\n                else:\n                    return self._process_df(\n                        dt.to_pyarrow_table().to_pandas(types_mapper=pd.ArrowDtype),\n                        post_read_query,\n                    )\n            else:\n                return self._process_df(dt.to_pandas(), post_read_query)\n        elif format == \"avro\":\n            ctx.debug(\"Reading Avro file\", path=str(full_path))\n            try:\n                import fastavro\n            except ImportError:\n                ctx.error(\n                    \"fastavro library not installed\",\n                    path=str(full_path),\n                )\n                raise ImportError(\n                    \"Avro support requires 'pip install odibi[pandas]' \"\n                    \"or 'pip install fastavro'. See README.md for installation instructions.\"\n                )\n\n            parsed = urlparse(full_path)\n            if parsed.scheme and parsed.scheme not in [\"file\", \"\"]:\n                import fsspec\n\n                storage_opts = options.get(\"storage_options\", {})\n                with fsspec.open(full_path, \"rb\", **storage_opts) as f:\n                    reader = fastavro.reader(f)\n                    records = [record for record in reader]\n                return pd.DataFrame(records)\n            else:\n                with open(full_path, \"rb\") as f:\n                    reader = fastavro.reader(f)\n                    records = [record for record in reader]\n                return self._process_df(pd.DataFrame(records), post_read_query)\n        elif format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            ctx.debug(\"Reading SQL table\", table=str(full_path), format=format)\n            if not hasattr(connection, \"read_table\"):\n                ctx.error(\n                    \"Connection does not support SQL operations\",\n                    connection_type=type(connection).__name__,\n                )\n                raise ValueError(\n                    f\"Connection type '{type(connection).__name__}' does not support SQL operations\"\n                )\n\n            table_name = str(full_path)\n            if \".\" in table_name:\n                schema, tbl = table_name.split(\".\", 1)\n            else:\n                schema, tbl = \"dbo\", table_name\n\n            ctx.debug(\"Executing SQL read\", schema=schema, table=tbl)\n            return connection.read_table(table_name=tbl, schema=schema)\n        else:\n            ctx.error(\"Unsupported format\", format=format)\n            raise ValueError(f\"Unsupported format for Pandas engine: {format}\")\n\n    def write(\n        self,\n        df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Pandas.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        destination = path or table\n        ctx.debug(\n            \"Starting write operation\",\n            format=format,\n            destination=destination,\n            mode=mode,\n        )\n\n        # Ensure materialization if LazyDataset\n        df = self.materialize(df)\n\n        options = options or {}\n\n        # Handle iterator/generator input\n        from collections.abc import Iterator\n\n        if isinstance(df, Iterator):\n            ctx.debug(\"Writing iterator/generator input\")\n            return self._write_iterator(df, connection, format, table, path, mode, options)\n\n        row_count = len(df)\n        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n\n        ctx.log_pandas_metrics(\n            memory_mb=memory_mb,\n            dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n        )\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            ctx.debug(\"Writing to SQL\", table=table, mode=mode)\n            return self._write_sql(df, connection, table, mode, options)\n\n        # Resolve full path from connection\n        try:\n            full_path = self._resolve_path(path or table, connection)\n        except ValueError:\n            if table and not connection:\n                ctx.error(\"Connection required when specifying 'table'\", table=table)\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n            ctx.error(\"Neither path nor table provided for write operation\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Merge storage options for cloud connections\n        merged_options = self._merge_storage_options(connection, options)\n\n        # Custom Writers\n        if format in self._custom_writers:\n            ctx.debug(f\"Using custom writer for format: {format}\")\n            writer_options = merged_options.copy()\n            writer_options.pop(\"keys\", None)\n            self._custom_writers[format](df, full_path, mode=mode, **writer_options)\n            return None\n\n        # Ensure directory exists (local only)\n        self._ensure_directory(full_path)\n\n        # Warn about partitioning\n        self._check_partitioning(merged_options)\n\n        # Delta Lake Write\n        if format == \"delta\":\n            ctx.debug(\"Writing Delta table\", path=str(full_path), mode=mode)\n            result = self._write_delta(df, full_path, mode, merged_options)\n            elapsed = (time.time() - start) * 1000\n            ctx.log_file_io(\n                path=str(full_path),\n                format=format,\n                mode=mode,\n                rows=row_count,\n            )\n            ctx.info(\n                \"Write completed\",\n                format=format,\n                rows=row_count,\n                elapsed_ms=round(elapsed, 2),\n            )\n            return result\n\n        # Handle Generic Upsert/Append-Once for non-Delta\n        if mode in [\"upsert\", \"append_once\"]:\n            ctx.debug(f\"Handling {mode} mode for non-Delta format\")\n            df, mode = self._handle_generic_upsert(df, full_path, format, mode, merged_options)\n            row_count = len(df)\n\n        # Standard File Write\n        result = self._write_file(df, full_path, format, mode, merged_options)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.log_file_io(\n            path=str(full_path),\n            format=format,\n            mode=mode,\n            rows=row_count,\n        )\n        ctx.info(\n            \"Write completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return result\n\n    def _write_iterator(\n        self,\n        df_iter: Iterator[pd.DataFrame],\n        connection: Any,\n        format: str,\n        table: Optional[str],\n        path: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle writing of iterator/generator.\"\"\"\n        first_chunk = True\n        for chunk in df_iter:\n            # Determine mode for this chunk\n            current_mode = mode if first_chunk else \"append\"\n            current_options = options.copy()\n\n            # Handle CSV header for chunks\n            if not first_chunk and format == \"csv\":\n                if current_options.get(\"header\") is not False:\n                    current_options[\"header\"] = False\n\n            self.write(\n                chunk,\n                connection,\n                format,\n                table,\n                path,\n                mode=current_mode,\n                options=current_options,\n            )\n            first_chunk = False\n        return None\n\n    def _write_sql(\n        self,\n        df: pd.DataFrame,\n        connection: Any,\n        table: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle SQL writing.\"\"\"\n        if not hasattr(connection, \"write_table\"):\n            raise ValueError(\n                f\"Connection type '{type(connection).__name__}' does not support SQL operations\"\n            )\n\n        if not table:\n            raise ValueError(\"SQL format requires 'table' config\")\n\n        # Extract schema from table name if present\n        if \".\" in table:\n            schema, table_name = table.split(\".\", 1)\n        else:\n            schema, table_name = \"dbo\", table\n\n        # Map mode to if_exists\n        if_exists = \"replace\"  # overwrite\n        if mode == \"append\":\n            if_exists = \"append\"\n        elif mode == \"fail\":\n            if_exists = \"fail\"\n\n        chunksize = options.get(\"chunksize\", 1000)\n\n        connection.write_table(\n            df=df,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n        return None\n\n    def _ensure_directory(self, full_path: str) -&gt; None:\n        \"\"\"Ensure parent directory exists for local files.\"\"\"\n        parsed = urlparse(str(full_path))\n        is_windows_drive = (\n            len(parsed.scheme) == 1 and parsed.scheme.isalpha() if parsed.scheme else False\n        )\n\n        if not parsed.scheme or parsed.scheme == \"file\" or is_windows_drive:\n            Path(full_path).parent.mkdir(parents=True, exist_ok=True)\n\n    def _check_partitioning(self, options: Dict[str, Any]) -&gt; None:\n        \"\"\"Warn about potential partitioning issues.\"\"\"\n        partition_by = options.get(\"partition_by\") or options.get(\"partitionBy\")\n        if partition_by:\n            import warnings\n\n            warnings.warn(\n                \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n                \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n                \"and ensure each partition has &gt; 1000 rows.\",\n                UserWarning,\n            )\n\n    def _write_delta(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        mode: str,\n        merged_options: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Handle Delta Lake writing.\"\"\"\n        try:\n            from deltalake import DeltaTable, write_deltalake\n        except ImportError:\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' or 'pip install deltalake'. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        storage_opts = merged_options.get(\"storage_options\", {})\n\n        # Handle null-only columns: Delta Lake doesn't support Null dtype\n        # Cast columns with all-null values to string to avoid schema errors\n        for col in df.columns:\n            if df[col].isna().all():\n                df[col] = df[col].astype(\"string\")\n\n        # Map modes\n        delta_mode = \"overwrite\"\n        if mode == \"append\":\n            delta_mode = \"append\"\n        elif mode == \"error\" or mode == \"fail\":\n            delta_mode = \"error\"\n        elif mode == \"ignore\":\n            delta_mode = \"ignore\"\n\n        # Handle upsert/append_once logic\n        if mode == \"upsert\":\n            keys = merged_options.get(\"keys\")\n            if not keys:\n                raise ValueError(\"Upsert requires 'keys' in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            def do_upsert():\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                (\n                    dt.merge(\n                        source=df,\n                        predicate=\" AND \".join([f\"s.{k} = t.{k}\" for k in keys]),\n                        source_alias=\"s\",\n                        target_alias=\"t\",\n                    )\n                    .when_matched_update_all()\n                    .when_not_matched_insert_all()\n                    .execute()\n                )\n\n            self._retry_delta_operation(do_upsert)\n        elif mode == \"append_once\":\n            keys = merged_options.get(\"keys\")\n            if not keys:\n                raise ValueError(\"Append_once requires 'keys' in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            def do_append_once():\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                (\n                    dt.merge(\n                        source=df,\n                        predicate=\" AND \".join([f\"s.{k} = t.{k}\" for k in keys]),\n                        source_alias=\"s\",\n                        target_alias=\"t\",\n                    )\n                    .when_not_matched_insert_all()\n                    .execute()\n                )\n\n            self._retry_delta_operation(do_append_once)\n        else:\n            # Filter options supported by write_deltalake\n            write_kwargs = {\n                k: v\n                for k, v in merged_options.items()\n                if k\n                in [\n                    \"partition_by\",\n                    \"mode\",\n                    \"overwrite_schema\",\n                    \"schema_mode\",\n                    \"name\",\n                    \"description\",\n                    \"configuration\",\n                ]\n            }\n\n            def do_write():\n                write_deltalake(\n                    full_path, df, mode=delta_mode, storage_options=storage_opts, **write_kwargs\n                )\n\n            self._retry_delta_operation(do_write)\n\n        # Return commit info\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=1)\n        latest = history[0]\n\n        return {\n            \"version\": dt.version(),\n            \"timestamp\": datetime.fromtimestamp(latest.get(\"timestamp\", 0) / 1000),\n            \"operation\": latest.get(\"operation\"),\n            \"operation_metrics\": latest.get(\"operationMetrics\", {}),\n            \"read_version\": latest.get(\"readVersion\"),\n        }\n\n    def _handle_generic_upsert(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        format: str,\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; tuple[pd.DataFrame, str]:\n        \"\"\"Handle upsert/append_once for standard files by merging with existing data.\"\"\"\n        if \"keys\" not in options:\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        keys = options[\"keys\"]\n        if isinstance(keys, str):\n            keys = [keys]\n\n        # Try to read existing file\n        existing_df = None\n        try:\n            read_opts = options.copy()\n            read_opts.pop(\"keys\", None)\n\n            if format == \"csv\":\n                existing_df = pd.read_csv(full_path, **read_opts)\n            elif format == \"parquet\":\n                existing_df = pd.read_parquet(full_path, **read_opts)\n            elif format == \"json\":\n                existing_df = pd.read_json(full_path, **read_opts)\n            elif format == \"excel\":\n                existing_df = pd.read_excel(full_path, **read_opts)\n        except Exception:\n            # File doesn't exist or can't be read\n            return df, \"overwrite\"  # Treat as new write\n\n        if existing_df is None:\n            return df, \"overwrite\"\n\n        if mode == \"append_once\":\n            # Check if keys exist\n            missing_keys = set(keys) - set(df.columns)\n            if missing_keys:\n                raise KeyError(f\"Keys {missing_keys} not found in input data\")\n\n            # Identify new rows\n            merged = df.merge(existing_df[keys], on=keys, how=\"left\", indicator=True)\n            new_rows = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n            if format in [\"csv\", \"json\"]:\n                return new_rows, \"append\"\n            else:\n                # Rewrite everything\n                return pd.concat([existing_df, new_rows], ignore_index=True), \"overwrite\"\n\n        elif mode == \"upsert\":\n            # Check if keys exist\n            missing_keys = set(keys) - set(df.columns)\n            if missing_keys:\n                raise KeyError(f\"Keys {missing_keys} not found in input data\")\n\n            # 1. Remove rows from existing that are in input\n            merged_indicator = existing_df.merge(df[keys], on=keys, how=\"left\", indicator=True)\n            rows_to_keep = existing_df[merged_indicator[\"_merge\"] == \"left_only\"]\n\n            # 2. Concat rows_to_keep + input df\n            # 3. Write mode becomes overwrite\n            return pd.concat([rows_to_keep, df], ignore_index=True), \"overwrite\"\n\n        return df, mode\n\n    def _write_file(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        format: str,\n        mode: str,\n        merged_options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle standard file writing (CSV, Parquet, etc.).\"\"\"\n        writer_options = merged_options.copy()\n        writer_options.pop(\"keys\", None)\n\n        # Remove storage_options for local pandas writers usually?\n        # Some pandas writers accept storage_options (parquet, csv with fsspec)\n\n        if format == \"csv\":\n            mode_param = \"w\"\n            if mode == \"append\":\n                mode_param = \"a\"\n                if not os.path.exists(full_path):\n                    # If file doesn't exist, include header\n                    writer_options[\"header\"] = True\n                else:\n                    # If appending, don't write header unless explicit\n                    if \"header\" not in writer_options:\n                        writer_options[\"header\"] = False\n\n            df.to_csv(full_path, index=False, mode=mode_param, **writer_options)\n\n        elif format == \"parquet\":\n            if mode == \"append\":\n                # Pandas read_parquet doesn't support append directly usually.\n                # We implement simple read-concat-write for local files\n                if os.path.exists(full_path):\n                    existing = pd.read_parquet(full_path, **merged_options)\n                    df = pd.concat([existing, df], ignore_index=True)\n\n            df.to_parquet(full_path, index=False, **writer_options)\n\n        elif format == \"json\":\n            if mode == \"append\":\n                writer_options[\"mode\"] = \"a\"\n\n            # Default to records if not specified\n            if \"orient\" not in writer_options:\n                writer_options[\"orient\"] = \"records\"\n\n            # Include storage_options for cloud storage (ADLS, S3, GCS)\n            if \"storage_options\" in merged_options:\n                writer_options[\"storage_options\"] = merged_options[\"storage_options\"]\n\n            df.to_json(full_path, **writer_options)\n\n        elif format == \"excel\":\n            if mode == \"append\":\n                # Simple append for excel\n                if os.path.exists(full_path):\n                    with pd.ExcelWriter(full_path, mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n                        df.to_excel(writer, index=False, **writer_options)\n                    return\n\n            df.to_excel(full_path, index=False, **writer_options)\n\n        elif format == \"avro\":\n            try:\n                import fastavro\n            except ImportError:\n                raise ImportError(\"Avro support requires 'pip install fastavro'\")\n\n            # Convert datetime columns to microseconds for Avro timestamp-micros\n            df_avro = df.copy()\n            for col in df_avro.columns:\n                if pd.api.types.is_datetime64_any_dtype(df_avro[col].dtype):\n                    df_avro[col] = df_avro[col].apply(\n                        lambda x: int(x.timestamp() * 1_000_000) if pd.notna(x) else None\n                    )\n\n            records = df_avro.to_dict(\"records\")\n            schema = self._infer_avro_schema(df)\n\n            # Use fsspec for remote URIs (abfss://, s3://, etc.)\n            parsed = urlparse(full_path)\n            if parsed.scheme and parsed.scheme not in [\"file\", \"\"]:\n                # Remote file - use fsspec\n                import fsspec\n\n                storage_opts = merged_options.get(\"storage_options\", {})\n                write_mode = \"wb\" if mode == \"overwrite\" else \"ab\"\n                with fsspec.open(full_path, write_mode, **storage_opts) as f:\n                    fastavro.writer(f, schema, records)\n            else:\n                # Local file - use standard open\n                open_mode = \"wb\"\n                if mode == \"append\" and os.path.exists(full_path):\n                    open_mode = \"a+b\"\n\n                with open(full_path, open_mode) as f:\n                    fastavro.writer(f, schema, records)\n        else:\n            raise ValueError(f\"Unsupported format for Pandas engine: {format}\")\n\n    def add_write_metadata(\n        self,\n        df: pd.DataFrame,\n        metadata_config,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: Pandas DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added\n        \"\"\"\n        from odibi.config import WriteMetadataConfig\n\n        # Normalize config: True -&gt; all defaults\n        if metadata_config is True:\n            config = WriteMetadataConfig()\n        elif isinstance(metadata_config, WriteMetadataConfig):\n            config = metadata_config\n        else:\n            return df  # None or invalid -&gt; no metadata\n\n        # Work on a copy to avoid modifying original\n        df = df.copy()\n\n        # _extracted_at: always applicable\n        if config.extracted_at:\n            df[\"_extracted_at\"] = pd.Timestamp.now()\n\n        # _source_file: only for file sources\n        if config.source_file and is_file_source and source_path:\n            df[\"_source_file\"] = source_path\n\n        # _source_connection: all sources\n        if config.source_connection and source_connection:\n            df[\"_source_connection\"] = source_connection\n\n        # _source_table: SQL sources only\n        if config.source_table and source_table:\n            df[\"_source_table\"] = source_table\n\n        return df\n\n    def _register_lazy_view_unused(self, conn, name: str, df: Any) -&gt; None:\n        \"\"\"Register a LazyDataset as a DuckDB view.\"\"\"\n        duck_fmt = df.format\n        if duck_fmt == \"json\":\n            duck_fmt = \"json_auto\"\n\n        if isinstance(df.path, list):\n            paths = \", \".join([f\"'{p}'\" for p in df.path])\n            conn.execute(\n                f\"CREATE OR REPLACE VIEW {name} AS SELECT * FROM read_{duck_fmt}([{paths}])\"\n            )\n        else:\n            conn.execute(\n                f\"CREATE OR REPLACE VIEW {name} AS SELECT * FROM read_{duck_fmt}('{df.path}')\"\n            )\n\n    def execute_sql(self, sql: str, context: Context) -&gt; pd.DataFrame:\n        \"\"\"Execute SQL query using DuckDB (if available) or pandasql.\n\n        Args:\n            sql: SQL query string\n            context: Execution context\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        if not isinstance(context, PandasContext):\n            raise TypeError(\"PandasEngine requires PandasContext\")\n\n        # Try to use DuckDB for SQL\n        try:\n            import duckdb\n\n            # Create in-memory database\n            conn = duckdb.connect(\":memory:\")\n\n            # Register all DataFrames from context\n            for name in context.list_names():\n                dataset_obj = context.get(name)\n\n                # Debug check\n                # print(f\"DEBUG: Registering {name} type={type(dataset_obj)} LazyDataset={LazyDataset}\")\n\n                # Handle LazyDataset (DuckDB optimization)\n                # if isinstance(dataset_obj, LazyDataset):\n                #     self._register_lazy_view(conn, name, dataset_obj)\n                #     # Log that we used DuckDB on file\n                #     # logger.info(f\"Executing SQL via DuckDB on lazy file: {dataset_obj.path}\")\n                #     continue\n\n                # Handle chunked data (Iterator)\n                from collections.abc import Iterator\n\n                if isinstance(dataset_obj, Iterator):\n                    # Warning: Materializing iterator for SQL execution\n                    # Note: DuckDB doesn't support streaming from iterator yet\n                    dataset_obj = pd.concat(dataset_obj, ignore_index=True)\n\n                conn.register(name, dataset_obj)\n\n            # Execute query\n            result = conn.execute(sql).df()\n            conn.close()\n\n            return result\n\n        except ImportError:\n            # Fallback: try pandasql\n            try:\n                from pandasql import sqldf\n\n                # Build local namespace with DataFrames\n                locals_dict = {}\n                for name in context.list_names():\n                    df = context.get(name)\n\n                    # Handle chunked data (Iterator)\n                    from collections.abc import Iterator\n\n                    if isinstance(df, Iterator):\n                        df = pd.concat(df, ignore_index=True)\n\n                    locals_dict[name] = df\n\n                return sqldf(sql, locals_dict)\n\n            except ImportError:\n                raise TransformError(\n                    \"SQL execution requires 'duckdb' or 'pandasql'. \"\n                    \"Install with: pip install duckdb\"\n                )\n\n    def execute_operation(\n        self,\n        operation: str,\n        params: Dict[str, Any],\n        df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute built-in operation.\n\n        Args:\n            operation: Operation name\n            params: Operation parameters\n            df: Input DataFrame or Iterator\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        # Materialize LazyDataset\n        df = self.materialize(df)\n\n        # Handle chunked data (Iterator)\n        from collections.abc import Iterator\n\n        if isinstance(df, Iterator):\n            # Warning: Materializing iterator for operation execution\n            df = pd.concat(df, ignore_index=True)\n\n        if operation == \"pivot\":\n            return self._pivot(df, params)\n        elif operation == \"drop_duplicates\":\n            return df.drop_duplicates(**params)\n        elif operation == \"fillna\":\n            return df.fillna(**params)\n        elif operation == \"drop\":\n            return df.drop(**params)\n        elif operation == \"rename\":\n            return df.rename(**params)\n        elif operation == \"sort\":\n            return df.sort_values(**params)\n        elif operation == \"sample\":\n            return df.sample(**params)\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext, PandasContext\n            from odibi.registry import FunctionRegistry\n\n            if FunctionRegistry.has_function(operation):\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df\n                engine_ctx = EngineContext(\n                    context=PandasContext(),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n            raise ValueError(f\"Unsupported operation: {operation}\")\n\n    def _pivot(self, df: pd.DataFrame, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute pivot operation.\n\n        Args:\n            df: Input DataFrame\n            params: Pivot parameters\n\n        Returns:\n            Pivoted DataFrame\n        \"\"\"\n        group_by = params.get(\"group_by\", [])\n        pivot_column = params[\"pivot_column\"]\n        value_column = params[\"value_column\"]\n        agg_func = params.get(\"agg_func\", \"first\")\n\n        # Validate columns exist\n        required_columns = set()\n        if isinstance(group_by, list):\n            required_columns.update(group_by)\n        elif isinstance(group_by, str):\n            required_columns.add(group_by)\n            group_by = [group_by]\n\n        required_columns.add(pivot_column)\n        required_columns.add(value_column)\n\n        missing = required_columns - set(df.columns)\n        if missing:\n            raise KeyError(\n                f\"Columns not found in DataFrame for pivot operation: {missing}. \"\n                f\"Available: {list(df.columns)}\"\n            )\n\n        result = df.pivot_table(\n            index=group_by, columns=pivot_column, values=value_column, aggfunc=agg_func\n        ).reset_index()\n\n        # Flatten column names if multi-level\n        if isinstance(result.columns, pd.MultiIndex):\n            result.columns = [\"_\".join(col).strip(\"_\") for col in result.columns.values]\n\n        return result\n\n    def harmonize_schema(\n        self, df: pd.DataFrame, target_schema: Dict[str, str], policy: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        target_cols = list(target_schema.keys())\n        current_cols = df.columns.tolist()\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        # 1. Check Validations\n        if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # 2. Apply Transformations\n        if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n            # Evolve: Add missing columns, Keep new columns\n            for col in missing:\n                df[col] = None\n        else:\n            # Enforce / Ignore New: Project to target schema (Drops new, Adds missing)\n            # Note: reindex adds NaN for missing columns\n            df = df.reindex(columns=target_cols)\n\n        return df\n\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Anonymize specified columns.\"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        res = df.copy()\n\n        for col in columns:\n            if col not in res.columns:\n                continue\n\n            if method == \"hash\":\n                # Vectorized Hashing (via map/apply)\n                # Note: True vectorization requires C-level support (e.g. pyarrow.compute)\n                # Standard Pandas apply is the fallback but we can optimize string handling\n\n                # Convert to string, handling nulls\n                # s_col = res[col].astype(str)\n                # Nulls become 'nan'/'None' string, we want to preserve them or hash them consistently?\n                # Typically nulls should remain null.\n\n                mask_nulls = res[col].isna()\n\n                def _hash_val(val):\n                    to_hash = val\n                    if salt:\n                        to_hash += salt\n                    return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n                # Apply only to non-nulls\n                res.loc[~mask_nulls, col] = res.loc[~mask_nulls, col].astype(str).apply(_hash_val)\n\n            elif method == \"mask\":\n                # Vectorized Masking\n                # Mask all but last 4 characters\n\n                mask_nulls = res[col].isna()\n                s_valid = res.loc[~mask_nulls, col].astype(str)\n\n                # Use vectorized regex replacement\n                # Replace any character that is followed by 4 characters with '*'\n                res.loc[~mask_nulls, col] = s_valid.str.replace(r\".(?=.{4})\", \"*\", regex=True)\n\n            elif method == \"redact\":\n                res[col] = \"[REDACTED]\"\n\n        return res\n\n    def get_schema(self, df: Any) -&gt; Dict[str, str]:\n        \"\"\"Get DataFrame schema with types.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Dict[str, str]: Column name -&gt; Type string\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res = conn.execute(\"DESCRIBE SELECT * FROM df\").df()\n                    return dict(zip(res[\"column_name\"], res[\"column_type\"]))\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return {col: str(df[col].dtype) for col in df.columns}\n\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            (rows, columns)\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            cols = len(self.get_schema(df))\n            rows = self.count_rows(df)\n            return (rows, cols)\n        return df.shape\n\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Row count\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res = conn.execute(\"SELECT count(*) FROM df\").fetchone()\n                    return res[0] if res else 0\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return len(df)\n\n    def count_nulls(self, df: pd.DataFrame, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\n\n        Args:\n            df: DataFrame\n            columns: Columns to check\n\n        Returns:\n            Dictionary of column -&gt; null count\n        \"\"\"\n        null_counts = {}\n        for col in columns:\n            if col in df.columns:\n                null_counts[col] = int(df[col].isna().sum())\n            else:\n                raise ValueError(f\"Column '{col}' not found in DataFrame\")\n        return null_counts\n\n    def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\n\n        Args:\n            df: DataFrame\n            schema_rules: Validation rules\n\n        Returns:\n            List of validation failures\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        failures = []\n\n        # Check required columns\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(df.columns)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        # Check column types\n        if \"types\" in schema_rules:\n            type_map = {\n                \"int\": [\"int64\", \"int32\", \"int16\", \"int8\"],\n                \"float\": [\"float64\", \"float32\"],\n                \"str\": [\"object\", \"string\"],\n                \"bool\": [\"bool\"],\n            }\n\n            for col, expected_type in schema_rules[\"types\"].items():\n                if col not in df.columns:\n                    failures.append(f\"Column '{col}' not found for type validation\")\n                    continue\n\n                actual_type = str(df[col].dtype)\n                # Handle pyarrow types (e.g. int64[pyarrow])\n                if \"[\" in actual_type and \"pyarrow\" in actual_type:\n                    actual_type = actual_type.split(\"[\")[0]\n\n                expected_dtypes = type_map.get(expected_type, [expected_type])\n\n                if actual_type not in expected_dtypes:\n                    failures.append(\n                        f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def _infer_avro_schema(self, df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Infer Avro schema from pandas DataFrame.\n\n        Args:\n            df: DataFrame to infer schema from\n\n        Returns:\n            Avro schema dictionary\n        \"\"\"\n        type_mapping = {\n            \"int64\": \"long\",\n            \"int32\": \"int\",\n            \"float64\": \"double\",\n            \"float32\": \"float\",\n            \"bool\": \"boolean\",\n            \"object\": \"string\",\n            \"string\": \"string\",\n        }\n\n        fields = []\n        for col in df.columns:\n            dtype = df[col].dtype\n            dtype_str = str(dtype)\n\n            # Handle datetime types with Avro logical types\n            if pd.api.types.is_datetime64_any_dtype(dtype):\n                avro_type = {\n                    \"type\": \"long\",\n                    \"logicalType\": \"timestamp-micros\",\n                }\n            elif dtype_str == \"date\" or (hasattr(dtype, \"name\") and \"date\" in dtype.name.lower()):\n                avro_type = {\n                    \"type\": \"int\",\n                    \"logicalType\": \"date\",\n                }\n            elif pd.api.types.is_timedelta64_dtype(dtype):\n                avro_type = {\n                    \"type\": \"long\",\n                    \"logicalType\": \"time-micros\",\n                }\n            else:\n                avro_type = type_mapping.get(dtype_str, \"string\")\n\n            # Handle nullable columns\n            if df[col].isnull().any():\n                avro_type = [\"null\", avro_type]\n\n            fields.append({\"name\": col, \"type\": avro_type})\n\n        return {\"type\": \"record\", \"name\": \"DataFrame\", \"fields\": fields}\n\n    def validate_data(self, df: pd.DataFrame, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate DataFrame against rules.\n\n        Args:\n            df: DataFrame\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        failures = []\n\n        # Check not empty\n        if validation_config.not_empty:\n            if len(df) == 0:\n                failures.append(\"DataFrame is empty\")\n\n        # Check for nulls in specified columns\n        if validation_config.no_nulls:\n            null_counts = self.count_nulls(df, validation_config.no_nulls)\n            for col, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col}' has {count} null values\")\n\n        # Schema validation\n        if validation_config.schema_validation:\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        # Range validation\n        if validation_config.ranges:\n            for col, bounds in validation_config.ranges.items():\n                if col in df.columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    if min_val is not None:\n                        min_violations = df[df[col] &lt; min_val]\n                        if len(min_violations) &gt; 0:\n                            failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        max_violations = df[df[col] &gt; max_val]\n                        if len(max_violations) &gt; 0:\n                            failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col}' not found for range validation\")\n\n        # Allowed values validation\n        if validation_config.allowed_values:\n            for col, allowed in validation_config.allowed_values.items():\n                if col in df.columns:\n                    # Check for values not in allowed list\n                    invalid = df[~df[col].isin(allowed)]\n                    if len(invalid) &gt; 0:\n                        failures.append(f\"Column '{col}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n        return failures\n\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\n\n        Args:\n            df: DataFrame or LazyDataset\n            n: Number of rows to return\n\n        Returns:\n            List of row dictionaries\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res_df = conn.execute(f\"SELECT * FROM df LIMIT {n}\").df()\n                    return res_df.to_dict(\"records\")\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return df.head(n).to_dict(\"records\")\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Args:\n            connection: Connection object\n            table: Table name (not used in Pandas\u2014no catalog)\n            path: File path\n\n        Returns:\n            True if file/directory exists, False otherwise\n        \"\"\"\n        if path:\n            full_path = connection.get_path(path)\n            return os.path.exists(full_path)\n        return False\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\"\"\"\n        try:\n            if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n                # SQL Server: Read empty result\n                query = f\"SELECT TOP 0 * FROM {table}\"\n                df = connection.read_sql(query)\n                return self.get_schema(df)\n\n            if path:\n                full_path = connection.get_path(path)\n                if not os.path.exists(full_path):\n                    return None\n\n                if format == \"delta\":\n                    from deltalake import DeltaTable\n\n                    dt = DeltaTable(full_path)\n                    # Use pyarrow schema to pandas schema to avoid reading data\n                    arrow_schema = dt.schema().to_pyarrow()\n                    empty_df = arrow_schema.empty_table().to_pandas()\n                    return self.get_schema(empty_df)\n\n                elif format == \"parquet\":\n                    import pyarrow.parquet as pq\n\n                    target_path = full_path\n                    if os.path.isdir(full_path):\n                        # Find first parquet file\n                        files = glob.glob(os.path.join(full_path, \"*.parquet\"))\n                        if not files:\n                            return None\n                        target_path = files[0]\n\n                    schema = pq.read_schema(target_path)\n                    empty_df = schema.empty_table().to_pandas()\n                    return self.get_schema(empty_df)\n\n                elif format == \"csv\":\n                    df = pd.read_csv(full_path, nrows=0)\n                    return self.get_schema(df)\n\n        except (FileNotFoundError, PermissionError):\n            return None\n        except ImportError as e:\n            # Log missing optional dependency\n            import logging\n\n            logging.getLogger(__name__).warning(\n                f\"Could not infer schema due to missing dependency: {e}\"\n            )\n            return None\n        except Exception as e:\n            import logging\n\n            logging.getLogger(__name__).warning(f\"Failed to infer schema for {table or path}: {e}\")\n            return None\n        return None\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n        dry_run: bool = False,\n        enforce_retention_duration: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"VACUUM a Delta table to remove old files.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            retention_hours: Retention period (default 168 = 7 days)\n            dry_run: If True, only show files to be deleted\n            enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n        Returns:\n            Dictionary with files_deleted count\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n        )\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        deleted_files = dt.vacuum(\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n            enforce_retention_duration=enforce_retention_duration,\n        )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=str(full_path),\n            files_deleted=len(deleted_files),\n            dry_run=dry_run,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return {\"files_deleted\": len(deleted_files)}\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            limit: Maximum number of versions to return\n\n        Returns:\n            List of version metadata dictionaries\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=limit)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=str(full_path),\n            versions_returned=len(history) if history else 0,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n\n    def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n        \"\"\"Restore Delta table to a specific version.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            version: Version number to restore to\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.info(\"Starting Delta table restore\", path=path, target_version=version)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        dt.restore(version)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta table restored\",\n            path=str(full_path),\n            restored_to_version=version,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        if format != \"delta\" or not config or not config.enabled:\n            return\n\n        if not path and not table:\n            return\n\n        full_path = connection.get_path(path if path else table)\n        start = time.time()\n\n        ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.warning(\n                \"Auto-optimize skipped: 'deltalake' library not installed\",\n                path=str(full_path),\n            )\n            return\n\n        try:\n            storage_opts = {}\n            if hasattr(connection, \"pandas_storage_options\"):\n                storage_opts = connection.pandas_storage_options()\n\n            dt = DeltaTable(full_path, storage_options=storage_opts)\n\n            ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n            dt.optimize.compact()\n\n            retention = config.vacuum_retention_hours\n            if retention is not None and retention &gt; 0:\n                ctx.info(\n                    \"Running Delta VACUUM\",\n                    path=str(full_path),\n                    retention_hours=retention,\n                )\n                dt.vacuum(\n                    retention_hours=retention,\n                    enforce_retention_duration=True,\n                    dry_run=False,\n                )\n\n            elapsed = (time.time() - start) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                path=str(full_path),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            ctx.warning(\n                \"Auto-optimize failed\",\n                path=str(full_path),\n                error=str(e),\n            )\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            List of file paths\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if isinstance(df.path, list):\n                return df.path\n            return [str(df.path)]\n\n        if hasattr(df, \"attrs\"):\n            return df.attrs.get(\"odibi_source_files\", [])\n        return []\n\n    def profile_nulls(self, df: pd.DataFrame) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dictionary of {column_name: null_percentage}\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        # mean() of boolean DataFrame gives the percentage of True values\n        return df.isna().mean().to_dict()\n\n    def filter_greater_than(self, df: pd.DataFrame, column: str, value: Any) -&gt; pd.DataFrame:\n        \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n        if column not in df.columns:\n            raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n        try:\n            # Handle timestamp string comparison\n            if pd.api.types.is_datetime64_any_dtype(df[column]) and isinstance(value, str):\n                value = pd.to_datetime(value)\n\n            return df[df[column] &gt; value]\n        except Exception as e:\n            raise ValueError(f\"Failed to filter {column} &gt; {value}: {e}\")\n\n    def filter_coalesce(\n        self, df: pd.DataFrame, col1: str, col2: str, op: str, value: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n        if col1 not in df.columns:\n            # If fallback only? No, usually primary must exist.\n            raise ValueError(f\"Column '{col1}' not found\")\n\n        # If col2 missing, behave like col1\n        if col2 not in df.columns:\n            s = df[col1]\n        else:\n            s = df[col1].combine_first(df[col2])\n\n        try:\n            if pd.api.types.is_datetime64_any_dtype(s) and isinstance(value, str):\n                value = pd.to_datetime(value)\n\n            if op == \"&gt;=\":\n                return df[s &gt;= value]\n            elif op == \"&gt;\":\n                return df[s &gt; value]\n            elif op == \"&lt;=\":\n                return df[s &lt;= value]\n            elif op == \"&lt;\":\n                return df[s &lt; value]\n            elif op == \"==\" or op == \"=\":\n                return df[s == value]\n            else:\n                raise ValueError(f\"Unsupported operator: {op}\")\n        except Exception as e:\n            raise ValueError(f\"Failed to filter COALESCE({col1}, {col2}) {op} {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.__init__","title":"<code>__init__(connections=None, config=None)</code>","text":"<p>Initialize Pandas engine.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Pandas engine.\n\n    Args:\n        connections: Dictionary of connection objects\n        config: Engine configuration (optional)\n    \"\"\"\n    self.connections = connections or {}\n    self.config = config or {}\n\n    # Suppress noisy delta-rs transaction conflict warnings (handled by retry)\n    if \"RUST_LOG\" not in os.environ:\n        os.environ[\"RUST_LOG\"] = \"deltalake_core::kernel::transaction=error\"\n\n    # Check for performance flags\n    performance = self.config.get(\"performance\", {})\n\n    # Determine desired state\n    if hasattr(performance, \"use_arrow\"):\n        desired_use_arrow = performance.use_arrow\n    elif isinstance(performance, dict):\n        desired_use_arrow = performance.get(\"use_arrow\", True)\n    else:\n        desired_use_arrow = True\n\n    # Verify availability\n    if desired_use_arrow:\n        try:\n            import pyarrow  # noqa: F401\n\n            self.use_arrow = True\n        except ImportError:\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"Apache Arrow not found. Disabling Arrow optimizations. \"\n                \"Install 'pyarrow' to enable.\"\n            )\n            self.use_arrow = False\n    else:\n        self.use_arrow = False\n\n    # Check for DuckDB\n    self.use_duckdb = False\n    # Default to False to ensure stability with existing tests (Lazy Loading is opt-in)\n    if self.config.get(\"performance\", {}).get(\"use_duckdb\", False):\n        try:\n            import duckdb  # noqa: F401\n\n            self.use_duckdb = True\n        except ImportError:\n            pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <code>metadata_config</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with metadata columns added</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: pd.DataFrame,\n    metadata_config,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: Pandas DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added\n    \"\"\"\n    from odibi.config import WriteMetadataConfig\n\n    # Normalize config: True -&gt; all defaults\n    if metadata_config is True:\n        config = WriteMetadataConfig()\n    elif isinstance(metadata_config, WriteMetadataConfig):\n        config = metadata_config\n    else:\n        return df  # None or invalid -&gt; no metadata\n\n    # Work on a copy to avoid modifying original\n    df = df.copy()\n\n    # _extracted_at: always applicable\n    if config.extracted_at:\n        df[\"_extracted_at\"] = pd.Timestamp.now()\n\n    # _source_file: only for file sources\n    if config.source_file and is_file_source and source_path:\n        df[\"_source_file\"] = source_path\n\n    # _source_connection: all sources\n    if config.source_connection and source_connection:\n        df[\"_source_connection\"] = source_connection\n\n    # _source_table: SQL sources only\n    if config.source_table and source_table:\n        df[\"_source_table\"] = source_table\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize specified columns.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Anonymize specified columns.\"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    res = df.copy()\n\n    for col in columns:\n        if col not in res.columns:\n            continue\n\n        if method == \"hash\":\n            # Vectorized Hashing (via map/apply)\n            # Note: True vectorization requires C-level support (e.g. pyarrow.compute)\n            # Standard Pandas apply is the fallback but we can optimize string handling\n\n            # Convert to string, handling nulls\n            # s_col = res[col].astype(str)\n            # Nulls become 'nan'/'None' string, we want to preserve them or hash them consistently?\n            # Typically nulls should remain null.\n\n            mask_nulls = res[col].isna()\n\n            def _hash_val(val):\n                to_hash = val\n                if salt:\n                    to_hash += salt\n                return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n            # Apply only to non-nulls\n            res.loc[~mask_nulls, col] = res.loc[~mask_nulls, col].astype(str).apply(_hash_val)\n\n        elif method == \"mask\":\n            # Vectorized Masking\n            # Mask all but last 4 characters\n\n            mask_nulls = res[col].isna()\n            s_valid = res.loc[~mask_nulls, col].astype(str)\n\n            # Use vectorized regex replacement\n            # Replace any character that is followed by 4 characters with '*'\n            res.loc[~mask_nulls, col] = s_valid.str.replace(r\".(?=.{4})\", \"*\", regex=True)\n\n        elif method == \"redact\":\n            res[col] = \"[REDACTED]\"\n\n    return res\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>columns</code> <code>List[str]</code> <p>Columns to check</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary of column -&gt; null count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def count_nulls(self, df: pd.DataFrame, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\n\n    Args:\n        df: DataFrame\n        columns: Columns to check\n\n    Returns:\n        Dictionary of column -&gt; null count\n    \"\"\"\n    null_counts = {}\n    for col in columns:\n        if col in df.columns:\n            null_counts[col] = int(df[col].isna().sum())\n        else:\n            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n    return null_counts\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>int</code> <p>Row count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Row count\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res = conn.execute(\"SELECT count(*) FROM df\").fetchone()\n                return res[0] if res else 0\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return len(df)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Operation name</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Operation parameters</p> required <code>df</code> <code>Union[DataFrame, Iterator[DataFrame]]</code> <p>Input DataFrame or Iterator</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def execute_operation(\n    self,\n    operation: str,\n    params: Dict[str, Any],\n    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n) -&gt; pd.DataFrame:\n    \"\"\"Execute built-in operation.\n\n    Args:\n        operation: Operation name\n        params: Operation parameters\n        df: Input DataFrame or Iterator\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    # Materialize LazyDataset\n    df = self.materialize(df)\n\n    # Handle chunked data (Iterator)\n    from collections.abc import Iterator\n\n    if isinstance(df, Iterator):\n        # Warning: Materializing iterator for operation execution\n        df = pd.concat(df, ignore_index=True)\n\n    if operation == \"pivot\":\n        return self._pivot(df, params)\n    elif operation == \"drop_duplicates\":\n        return df.drop_duplicates(**params)\n    elif operation == \"fillna\":\n        return df.fillna(**params)\n    elif operation == \"drop\":\n        return df.drop(**params)\n    elif operation == \"rename\":\n        return df.rename(**params)\n    elif operation == \"sort\":\n        return df.sort_values(**params)\n    elif operation == \"sample\":\n        return df.sample(**params)\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext, PandasContext\n        from odibi.registry import FunctionRegistry\n\n        if FunctionRegistry.has_function(operation):\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df\n            engine_ctx = EngineContext(\n                context=PandasContext(),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n        raise ValueError(f\"Unsupported operation: {operation}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.execute_sql","title":"<code>execute_sql(sql, context)</code>","text":"<p>Execute SQL query using DuckDB (if available) or pandasql.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Context) -&gt; pd.DataFrame:\n    \"\"\"Execute SQL query using DuckDB (if available) or pandasql.\n\n    Args:\n        sql: SQL query string\n        context: Execution context\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    if not isinstance(context, PandasContext):\n        raise TypeError(\"PandasEngine requires PandasContext\")\n\n    # Try to use DuckDB for SQL\n    try:\n        import duckdb\n\n        # Create in-memory database\n        conn = duckdb.connect(\":memory:\")\n\n        # Register all DataFrames from context\n        for name in context.list_names():\n            dataset_obj = context.get(name)\n\n            # Debug check\n            # print(f\"DEBUG: Registering {name} type={type(dataset_obj)} LazyDataset={LazyDataset}\")\n\n            # Handle LazyDataset (DuckDB optimization)\n            # if isinstance(dataset_obj, LazyDataset):\n            #     self._register_lazy_view(conn, name, dataset_obj)\n            #     # Log that we used DuckDB on file\n            #     # logger.info(f\"Executing SQL via DuckDB on lazy file: {dataset_obj.path}\")\n            #     continue\n\n            # Handle chunked data (Iterator)\n            from collections.abc import Iterator\n\n            if isinstance(dataset_obj, Iterator):\n                # Warning: Materializing iterator for SQL execution\n                # Note: DuckDB doesn't support streaming from iterator yet\n                dataset_obj = pd.concat(dataset_obj, ignore_index=True)\n\n            conn.register(name, dataset_obj)\n\n        # Execute query\n        result = conn.execute(sql).df()\n        conn.close()\n\n        return result\n\n    except ImportError:\n        # Fallback: try pandasql\n        try:\n            from pandasql import sqldf\n\n            # Build local namespace with DataFrames\n            locals_dict = {}\n            for name in context.list_names():\n                df = context.get(name)\n\n                # Handle chunked data (Iterator)\n                from collections.abc import Iterator\n\n                if isinstance(df, Iterator):\n                    df = pd.concat(df, ignore_index=True)\n\n                locals_dict[name] = df\n\n            return sqldf(sql, locals_dict)\n\n        except ImportError:\n            raise TransformError(\n                \"SQL execution requires 'duckdb' or 'pandasql'. \"\n                \"Install with: pip install duckdb\"\n            )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.filter_coalesce","title":"<code>filter_coalesce(df, col1, col2, op, value)</code>","text":"<p>Filter using COALESCE(col1, col2) op value.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def filter_coalesce(\n    self, df: pd.DataFrame, col1: str, col2: str, op: str, value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n    if col1 not in df.columns:\n        # If fallback only? No, usually primary must exist.\n        raise ValueError(f\"Column '{col1}' not found\")\n\n    # If col2 missing, behave like col1\n    if col2 not in df.columns:\n        s = df[col1]\n    else:\n        s = df[col1].combine_first(df[col2])\n\n    try:\n        if pd.api.types.is_datetime64_any_dtype(s) and isinstance(value, str):\n            value = pd.to_datetime(value)\n\n        if op == \"&gt;=\":\n            return df[s &gt;= value]\n        elif op == \"&gt;\":\n            return df[s &gt; value]\n        elif op == \"&lt;=\":\n            return df[s &lt;= value]\n        elif op == \"&lt;\":\n            return df[s &lt; value]\n        elif op == \"==\" or op == \"=\":\n            return df[s == value]\n        else:\n            raise ValueError(f\"Unsupported operator: {op}\")\n    except Exception as e:\n        raise ValueError(f\"Failed to filter COALESCE({col1}, {col2}) {op} {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.filter_greater_than","title":"<code>filter_greater_than(df, column, value)</code>","text":"<p>Filter DataFrame where column &gt; value.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def filter_greater_than(self, df: pd.DataFrame, column: str, value: Any) -&gt; pd.DataFrame:\n    \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n    try:\n        # Handle timestamp string comparison\n        if pd.api.types.is_datetime64_any_dtype(df[column]) and isinstance(value, str):\n            value = pd.to_datetime(value)\n\n        return df[df[column] &gt; value]\n    except Exception as e:\n        raise ValueError(f\"Failed to filter {column} &gt; {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>limit</code> <code>Optional[int]</code> <p>Maximum number of versions to return</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of version metadata dictionaries</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        limit: Maximum number of versions to return\n\n    Returns:\n        List of version metadata dictionaries\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    history = dt.history(limit=limit)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta history retrieved\",\n        path=str(full_path),\n        versions_returned=len(history) if history else 0,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return history\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <code>n</code> <code>int</code> <p>Number of rows to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of row dictionaries</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\n\n    Args:\n        df: DataFrame or LazyDataset\n        n: Number of rows to return\n\n    Returns:\n        List of row dictionaries\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res_df = conn.execute(f\"SELECT * FROM df LIMIT {n}\").df()\n                return res_df.to_dict(\"records\")\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return df.head(n).to_dict(\"records\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema with types.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Column name -&gt; Type string</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_schema(self, df: Any) -&gt; Dict[str, str]:\n    \"\"\"Get DataFrame schema with types.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Dict[str, str]: Column name -&gt; Type string\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res = conn.execute(\"DESCRIBE SELECT * FROM df\").df()\n                return dict(zip(res[\"column_name\"], res[\"column_type\"]))\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return {col: str(df[col].dtype) for col in df.columns}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(rows, columns)</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        (rows, columns)\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        cols = len(self.get_schema(df))\n        rows = self.count_rows(df)\n        return (rows, cols)\n    return df.shape\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        List of file paths\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if isinstance(df.path, list):\n            return df.path\n        return [str(df.path)]\n\n    if hasattr(df, \"attrs\"):\n        return df.attrs.get(\"odibi_source_files\", [])\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\"\"\"\n    try:\n        if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            # SQL Server: Read empty result\n            query = f\"SELECT TOP 0 * FROM {table}\"\n            df = connection.read_sql(query)\n            return self.get_schema(df)\n\n        if path:\n            full_path = connection.get_path(path)\n            if not os.path.exists(full_path):\n                return None\n\n            if format == \"delta\":\n                from deltalake import DeltaTable\n\n                dt = DeltaTable(full_path)\n                # Use pyarrow schema to pandas schema to avoid reading data\n                arrow_schema = dt.schema().to_pyarrow()\n                empty_df = arrow_schema.empty_table().to_pandas()\n                return self.get_schema(empty_df)\n\n            elif format == \"parquet\":\n                import pyarrow.parquet as pq\n\n                target_path = full_path\n                if os.path.isdir(full_path):\n                    # Find first parquet file\n                    files = glob.glob(os.path.join(full_path, \"*.parquet\"))\n                    if not files:\n                        return None\n                    target_path = files[0]\n\n                schema = pq.read_schema(target_path)\n                empty_df = schema.empty_table().to_pandas()\n                return self.get_schema(empty_df)\n\n            elif format == \"csv\":\n                df = pd.read_csv(full_path, nrows=0)\n                return self.get_schema(df)\n\n    except (FileNotFoundError, PermissionError):\n        return None\n    except ImportError as e:\n        # Log missing optional dependency\n        import logging\n\n        logging.getLogger(__name__).warning(\n            f\"Could not infer schema due to missing dependency: {e}\"\n        )\n        return None\n    except Exception as e:\n        import logging\n\n        logging.getLogger(__name__).warning(f\"Failed to infer schema for {table or path}: {e}\")\n        return None\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def harmonize_schema(\n    self, df: pd.DataFrame, target_schema: Dict[str, str], policy: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    target_cols = list(target_schema.keys())\n    current_cols = df.columns.tolist()\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    # 1. Check Validations\n    if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # 2. Apply Transformations\n    if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n        # Evolve: Add missing columns, Keep new columns\n        for col in missing:\n            df[col] = None\n    else:\n        # Enforce / Ignore New: Project to target schema (Drops new, Adds missing)\n        # Note: reindex adds NaN for missing columns\n        df = df.reindex(columns=target_cols)\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n\n    if format != \"delta\" or not config or not config.enabled:\n        return\n\n    if not path and not table:\n        return\n\n    full_path = connection.get_path(path if path else table)\n    start = time.time()\n\n    ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.warning(\n            \"Auto-optimize skipped: 'deltalake' library not installed\",\n            path=str(full_path),\n        )\n        return\n\n    try:\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n\n        ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n        dt.optimize.compact()\n\n        retention = config.vacuum_retention_hours\n        if retention is not None and retention &gt; 0:\n            ctx.info(\n                \"Running Delta VACUUM\",\n                path=str(full_path),\n                retention_hours=retention,\n            )\n            dt.vacuum(\n                retention_hours=retention,\n                enforce_retention_duration=True,\n                dry_run=False,\n            )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            path=str(full_path),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        ctx.warning(\n            \"Auto-optimize failed\",\n            path=str(full_path),\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset.\"\"\"\n    if isinstance(df, LazyDataset):\n        # Re-invoke read but force materialization (by bypassing Lazy check)\n        # We pass the resolved path directly\n        # Note: We need to handle the case where path was resolved.\n        # LazyDataset.path should be the FULL path.\n        return self._read_file(\n            full_path=df.path, format=df.format, options=df.options, connection=df.connection\n        )\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of {column_name: null_percentage}</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def profile_nulls(self, df: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dictionary of {column_name: null_percentage}\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    # mean() of boolean DataFrame gives the percentage of True values\n    return df.isna().mean().to_dict()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, as_of_version=None, as_of_timestamp=None)</code>","text":"<p>Read data using Pandas (or LazyDataset).</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    as_of_version: Optional[int] = None,\n    as_of_timestamp: Optional[str] = None,\n) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Read data using Pandas (or LazyDataset).\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    source = path or table\n    ctx.debug(\n        \"Starting read operation\",\n        format=format,\n        path=source,\n        streaming=streaming,\n        use_arrow=self.use_arrow,\n    )\n\n    if streaming:\n        ctx.error(\n            \"Streaming not supported in Pandas engine\",\n            format=format,\n            path=source,\n        )\n        raise ValueError(\n            \"Streaming is not supported in the Pandas engine. \"\n            \"Please use 'engine: spark' for streaming pipelines.\"\n        )\n\n    options = options or {}\n\n    # Resolve full path from connection\n    try:\n        full_path = self._resolve_path(path or table, connection)\n    except ValueError:\n        if table and not connection:\n            ctx.error(\"Connection required when specifying 'table'\", table=table)\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n        ctx.error(\"Neither path nor table provided for read operation\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Merge storage options for cloud connections\n    merged_options = self._merge_storage_options(connection, options)\n\n    # Sanitize options for pandas compatibility\n    if \"header\" in merged_options:\n        if merged_options[\"header\"] is True:\n            merged_options[\"header\"] = 0\n        elif merged_options[\"header\"] is False:\n            merged_options[\"header\"] = None\n\n    # Handle Time Travel options\n    if as_of_version is not None:\n        merged_options[\"versionAsOf\"] = as_of_version\n        ctx.debug(\"Time travel enabled\", version=as_of_version)\n    if as_of_timestamp is not None:\n        merged_options[\"timestampAsOf\"] = as_of_timestamp\n        ctx.debug(\"Time travel enabled\", timestamp=as_of_timestamp)\n\n    # Check for Lazy/DuckDB optimization\n    can_lazy_load = False\n\n    if can_lazy_load:\n        ctx.debug(\"Using lazy loading via DuckDB\", path=str(full_path))\n        if isinstance(full_path, (str, Path)):\n            return LazyDataset(\n                path=str(full_path),\n                format=format,\n                options=merged_options,\n                connection=connection,\n            )\n        elif isinstance(full_path, list):\n            return LazyDataset(\n                path=full_path, format=format, options=merged_options, connection=connection\n            )\n\n    result = self._read_file(full_path, format, merged_options, connection)\n\n    # Log metrics for materialized DataFrames\n    elapsed = (time.time() - start) * 1000\n    if isinstance(result, pd.DataFrame):\n        row_count = len(result)\n        memory_mb = result.memory_usage(deep=True).sum() / (1024 * 1024)\n\n        ctx.log_file_io(\n            path=str(full_path) if not isinstance(full_path, list) else str(full_path[0]),\n            format=format,\n            mode=\"read\",\n            rows=row_count,\n        )\n        ctx.log_pandas_metrics(\n            memory_mb=memory_mb,\n            dtypes={col: str(dtype) for col, dtype in result.dtypes.items()},\n        )\n        ctx.info(\n            \"Read completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n            memory_mb=round(memory_mb, 2),\n        )\n\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.restore_delta","title":"<code>restore_delta(connection, path, version)</code>","text":"<p>Restore Delta table to a specific version.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>version</code> <code>int</code> <p>Version number to restore to</p> required Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n    \"\"\"Restore Delta table to a specific version.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        version: Version number to restore to\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.info(\"Starting Delta table restore\", path=path, target_version=version)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    dt.restore(version)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta table restored\",\n        path=str(full_path),\n        restored_to_version=version,\n        elapsed_ms=round(elapsed, 2),\n    )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (not used in Pandas\u2014no catalog)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if file/directory exists, False otherwise</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Args:\n        connection: Connection object\n        table: Table name (not used in Pandas\u2014no catalog)\n        path: File path\n\n    Returns:\n        True if file/directory exists, False otherwise\n    \"\"\"\n    if path:\n        full_path = connection.get_path(path)\n        return os.path.exists(full_path)\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168, dry_run=False, enforce_retention_duration=True)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>retention_hours</code> <code>int</code> <p>Retention period (default 168 = 7 days)</p> <code>168</code> <code>dry_run</code> <code>bool</code> <p>If True, only show files to be deleted</p> <code>False</code> <code>enforce_retention_duration</code> <code>bool</code> <p>If False, allows retention &lt; 168 hours (testing only)</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with files_deleted count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n    dry_run: bool = False,\n    enforce_retention_duration: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"VACUUM a Delta table to remove old files.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        retention_hours: Retention period (default 168 = 7 days)\n        dry_run: If True, only show files to be deleted\n        enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n    Returns:\n        Dictionary with files_deleted count\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n    )\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    deleted_files = dt.vacuum(\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n        enforce_retention_duration=enforce_retention_duration,\n    )\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta VACUUM completed\",\n        path=str(full_path),\n        files_deleted=len(deleted_files),\n        dry_run=dry_run,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return {\"files_deleted\": len(deleted_files)}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate DataFrame against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def validate_data(self, df: pd.DataFrame, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate DataFrame against rules.\n\n    Args:\n        df: DataFrame\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    failures = []\n\n    # Check not empty\n    if validation_config.not_empty:\n        if len(df) == 0:\n            failures.append(\"DataFrame is empty\")\n\n    # Check for nulls in specified columns\n    if validation_config.no_nulls:\n        null_counts = self.count_nulls(df, validation_config.no_nulls)\n        for col, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col}' has {count} null values\")\n\n    # Schema validation\n    if validation_config.schema_validation:\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    # Range validation\n    if validation_config.ranges:\n        for col, bounds in validation_config.ranges.items():\n            if col in df.columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                if min_val is not None:\n                    min_violations = df[df[col] &lt; min_val]\n                    if len(min_violations) &gt; 0:\n                        failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    max_violations = df[df[col] &gt; max_val]\n                    if len(max_violations) &gt; 0:\n                        failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col}' not found for range validation\")\n\n    # Allowed values validation\n    if validation_config.allowed_values:\n        for col, allowed in validation_config.allowed_values.items():\n            if col in df.columns:\n                # Check for values not in allowed list\n                invalid = df[~df[col].isin(allowed)]\n                if len(invalid) &gt; 0:\n                    failures.append(f\"Column '{col}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>schema_rules</code> <code>Dict[str, Any]</code> <p>Validation rules</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failures</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\n\n    Args:\n        df: DataFrame\n        schema_rules: Validation rules\n\n    Returns:\n        List of validation failures\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    failures = []\n\n    # Check required columns\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(df.columns)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    # Check column types\n    if \"types\" in schema_rules:\n        type_map = {\n            \"int\": [\"int64\", \"int32\", \"int16\", \"int8\"],\n            \"float\": [\"float64\", \"float32\"],\n            \"str\": [\"object\", \"string\"],\n            \"bool\": [\"bool\"],\n        }\n\n        for col, expected_type in schema_rules[\"types\"].items():\n            if col not in df.columns:\n                failures.append(f\"Column '{col}' not found for type validation\")\n                continue\n\n            actual_type = str(df[col].dtype)\n            # Handle pyarrow types (e.g. int64[pyarrow])\n            if \"[\" in actual_type and \"pyarrow\" in actual_type:\n                actual_type = actual_type.split(\"[\")[0]\n\n            expected_dtypes = type_map.get(expected_type, [expected_type])\n\n            if actual_type not in expected_dtypes:\n                failures.append(\n                    f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.write","title":"<code>write(df, connection, format, table=None, path=None, register_table=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Pandas.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def write(\n    self,\n    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    register_table: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Pandas.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    destination = path or table\n    ctx.debug(\n        \"Starting write operation\",\n        format=format,\n        destination=destination,\n        mode=mode,\n    )\n\n    # Ensure materialization if LazyDataset\n    df = self.materialize(df)\n\n    options = options or {}\n\n    # Handle iterator/generator input\n    from collections.abc import Iterator\n\n    if isinstance(df, Iterator):\n        ctx.debug(\"Writing iterator/generator input\")\n        return self._write_iterator(df, connection, format, table, path, mode, options)\n\n    row_count = len(df)\n    memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n\n    ctx.log_pandas_metrics(\n        memory_mb=memory_mb,\n        dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n    )\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        ctx.debug(\"Writing to SQL\", table=table, mode=mode)\n        return self._write_sql(df, connection, table, mode, options)\n\n    # Resolve full path from connection\n    try:\n        full_path = self._resolve_path(path or table, connection)\n    except ValueError:\n        if table and not connection:\n            ctx.error(\"Connection required when specifying 'table'\", table=table)\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n        ctx.error(\"Neither path nor table provided for write operation\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Merge storage options for cloud connections\n    merged_options = self._merge_storage_options(connection, options)\n\n    # Custom Writers\n    if format in self._custom_writers:\n        ctx.debug(f\"Using custom writer for format: {format}\")\n        writer_options = merged_options.copy()\n        writer_options.pop(\"keys\", None)\n        self._custom_writers[format](df, full_path, mode=mode, **writer_options)\n        return None\n\n    # Ensure directory exists (local only)\n    self._ensure_directory(full_path)\n\n    # Warn about partitioning\n    self._check_partitioning(merged_options)\n\n    # Delta Lake Write\n    if format == \"delta\":\n        ctx.debug(\"Writing Delta table\", path=str(full_path), mode=mode)\n        result = self._write_delta(df, full_path, mode, merged_options)\n        elapsed = (time.time() - start) * 1000\n        ctx.log_file_io(\n            path=str(full_path),\n            format=format,\n            mode=mode,\n            rows=row_count,\n        )\n        ctx.info(\n            \"Write completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n        )\n        return result\n\n    # Handle Generic Upsert/Append-Once for non-Delta\n    if mode in [\"upsert\", \"append_once\"]:\n        ctx.debug(f\"Handling {mode} mode for non-Delta format\")\n        df, mode = self._handle_generic_upsert(df, full_path, format, mode, merged_options)\n        row_count = len(df)\n\n    # Standard File Write\n    result = self._write_file(df, full_path, format, mode, merged_options)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.log_file_io(\n        path=str(full_path),\n        format=format,\n        mode=mode,\n        rows=row_count,\n    )\n    ctx.info(\n        \"Write completed\",\n        format=format,\n        rows=row_count,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine","title":"<code>odibi.engine.spark_engine</code>","text":"<p>Spark execution engine (Phase 2B: Delta Lake support).</p> <p>Status: Phase 2B implemented - Delta Lake read/write, VACUUM, history, restore</p>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine","title":"<code>SparkEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Spark execution engine with PySpark backend.</p> <p>Phase 2A: Basic read/write + ADLS multi-account support Phase 2B: Delta Lake support</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>class SparkEngine(Engine):\n    \"\"\"Spark execution engine with PySpark backend.\n\n    Phase 2A: Basic read/write + ADLS multi-account support\n    Phase 2B: Delta Lake support\n    \"\"\"\n\n    name = \"spark\"\n    engine_type = EngineType.SPARK\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        spark_session=None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Spark engine with import guard.\n\n        Args:\n            connections: Dictionary of connection objects (for multi-account config)\n            spark_session: Existing SparkSession (optional, creates new if None)\n            config: Engine configuration (optional)\n\n        Raises:\n            ImportError: If pyspark not installed\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        ctx.debug(\"Initializing SparkEngine\", connections_count=len(connections or {}))\n\n        try:\n            from pyspark.sql import SparkSession\n        except ImportError as e:\n            ctx.error(\n                \"PySpark not installed\",\n                error_type=\"ImportError\",\n                suggestion=\"pip install odibi[spark]\",\n            )\n            raise ImportError(\n                \"Spark support requires 'pip install odibi[spark]'. \"\n                \"See docs/setup_databricks.md for setup instructions.\"\n            ) from e\n\n        start_time = time.time()\n\n        # Configure Delta Lake support\n        try:\n            from delta import configure_spark_with_delta_pip\n\n            builder = SparkSession.builder.appName(\"odibi\").config(\n                \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n            )\n\n            # Performance Optimizations\n            builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n            builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n            # Reduce Verbosity\n            builder = builder.config(\n                \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n            builder = builder.config(\n                \"spark.executor.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n\n            self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n            self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n            ctx.debug(\"Delta Lake support enabled\")\n\n        except ImportError:\n            ctx.debug(\"Delta Lake not available, using standard Spark\")\n            builder = SparkSession.builder.appName(\"odibi\").config(\n                \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n            )\n\n            # Performance Optimizations\n            builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n            builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n            # Reduce Verbosity\n            builder = builder.config(\n                \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n\n            self.spark = spark_session or builder.getOrCreate()\n            self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n        self.config = config or {}\n        self.connections = connections or {}\n\n        # Configure all ADLS connections upfront\n        self._configure_all_connections()\n\n        # Apply user-defined Spark configs from performance settings\n        self._apply_spark_config()\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"SparkEngine initialized\",\n            elapsed_ms=round(elapsed, 2),\n            app_name=self.spark.sparkContext.appName,\n            spark_version=self.spark.version,\n            connections_configured=len(self.connections),\n            using_existing_session=spark_session is not None,\n        )\n\n    def _configure_all_connections(self) -&gt; None:\n        \"\"\"Configure Spark with all ADLS connection credentials.\n\n        This sets all storage account keys upfront so Spark can access\n        multiple accounts. Keys are scoped by account name, so no conflicts.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        for conn_name, connection in self.connections.items():\n            if hasattr(connection, \"configure_spark\"):\n                ctx.log_connection(\n                    connection_type=type(connection).__name__,\n                    connection_name=conn_name,\n                    action=\"configure_spark\",\n                )\n                try:\n                    connection.configure_spark(self.spark)\n                    ctx.debug(f\"Configured ADLS connection: {conn_name}\")\n                except Exception as e:\n                    ctx.error(\n                        f\"Failed to configure ADLS connection: {conn_name}\",\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                    )\n                    raise\n\n    def _apply_spark_config(self) -&gt; None:\n        \"\"\"Apply user-defined Spark configurations from performance settings.\n\n        Applies configs via spark.conf.set() for runtime-settable options.\n        For existing sessions (e.g., Databricks), only modifiable configs take effect.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        performance = self.config.get(\"performance\", {})\n        spark_config = performance.get(\"spark_config\", {})\n\n        if not spark_config:\n            return\n\n        ctx.debug(\"Applying Spark configuration\", config_count=len(spark_config))\n\n        for key, value in spark_config.items():\n            try:\n                self.spark.conf.set(key, value)\n                ctx.debug(\n                    f\"Applied Spark config: {key}={value}\", config_key=key, config_value=value\n                )\n            except Exception as e:\n                ctx.warning(\n                    f\"Failed to set Spark config '{key}'\",\n                    config_key=key,\n                    error_message=str(e),\n                    suggestion=\"This config may require session restart\",\n                )\n\n    def _apply_table_properties(\n        self, target: str, properties: Dict[str, str], is_table: bool = False\n    ) -&gt; None:\n        \"\"\"Apply table properties to a Delta table.\n\n        Performance: Batches all properties into a single ALTER TABLE statement\n        to avoid multiple round-trips to the catalog.\n        \"\"\"\n        if not properties:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            table_ref = target if is_table else f\"delta.`{target}`\"\n            ctx.debug(\n                f\"Applying table properties to {target}\",\n                properties_count=len(properties),\n                is_table=is_table,\n            )\n\n            props_list = [f\"'{k}' = '{v}'\" for k, v in properties.items()]\n            props_str = \", \".join(props_list)\n            sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n            self.spark.sql(sql)\n            ctx.debug(f\"Set {len(properties)} table properties in single statement\")\n\n        except Exception as e:\n            ctx.warning(\n                f\"Failed to set table properties on {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n            )\n\n    def _optimize_delta_write(\n        self, target: str, options: Dict[str, Any], is_table: bool = False\n    ) -&gt; None:\n        \"\"\"Run Delta Lake optimization (OPTIMIZE / ZORDER).\"\"\"\n        should_optimize = options.get(\"optimize_write\", False)\n        zorder_by = options.get(\"zorder_by\")\n\n        if not should_optimize and not zorder_by:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        try:\n            if is_table:\n                sql = f\"OPTIMIZE {target}\"\n            else:\n                sql = f\"OPTIMIZE delta.`{target}`\"\n\n            if zorder_by:\n                if isinstance(zorder_by, str):\n                    zorder_by = [zorder_by]\n                cols = \", \".join(zorder_by)\n                sql += f\" ZORDER BY ({cols})\"\n\n            ctx.debug(\"Running Delta optimization\", sql=sql, target=target)\n            self.spark.sql(sql)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta optimization completed\",\n                target=target,\n                zorder_by=zorder_by,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.warning(\n                f\"Optimization failed for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n    def _get_last_delta_commit_info(\n        self, target: str, is_table: bool = False\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for the most recent Delta commit.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            from delta.tables import DeltaTable\n\n            if is_table:\n                dt = DeltaTable.forName(self.spark, target)\n            else:\n                dt = DeltaTable.forPath(self.spark, target)\n\n            last_commit = dt.history(1).collect()[0]\n\n            def safe_get(row, field):\n                if hasattr(row, field):\n                    return getattr(row, field)\n                if hasattr(row, \"__getitem__\"):\n                    try:\n                        return row[field]\n                    except (KeyError, ValueError):\n                        return None\n                return None\n\n            commit_info = {\n                \"version\": safe_get(last_commit, \"version\"),\n                \"timestamp\": safe_get(last_commit, \"timestamp\"),\n                \"operation\": safe_get(last_commit, \"operation\"),\n                \"operation_metrics\": safe_get(last_commit, \"operationMetrics\"),\n                \"read_version\": safe_get(last_commit, \"readVersion\"),\n            }\n\n            ctx.debug(\n                \"Delta commit metadata retrieved\",\n                target=target,\n                version=commit_info.get(\"version\"),\n                operation=commit_info.get(\"operation\"),\n            )\n\n            return commit_info\n\n        except Exception as e:\n            ctx.warning(\n                f\"Failed to fetch Delta commit info for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n            )\n            return None\n\n    def harmonize_schema(self, df, target_schema: Dict[str, str], policy: Any):\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n        from pyspark.sql.functions import col, lit\n\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        target_cols = list(target_schema.keys())\n        current_cols = df.columns\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        ctx.debug(\n            \"Schema harmonization\",\n            target_columns=len(target_cols),\n            current_columns=len(current_cols),\n            missing_columns=list(missing) if missing else None,\n            new_columns=list(new_cols) if new_cols else None,\n            policy_mode=policy.mode.value if hasattr(policy.mode, \"value\") else str(policy.mode),\n        )\n\n        # Check Validations\n        if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n            ctx.error(\n                f\"Schema Policy Violation: Missing columns {missing}\",\n                missing_columns=list(missing),\n            )\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n            ctx.error(\n                f\"Schema Policy Violation: New columns {new_cols}\",\n                new_columns=list(new_cols),\n            )\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # Apply Transformations\n        if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n            res = df\n            for c in missing:\n                res = res.withColumn(c, lit(None))\n            ctx.debug(\"Schema evolved: added missing columns as null\")\n            return res\n        else:\n            select_exprs = []\n            for c in target_cols:\n                if c in current_cols:\n                    select_exprs.append(col(c))\n                else:\n                    select_exprs.append(lit(None).alias(c))\n\n            ctx.debug(\"Schema enforced: projected to target schema\")\n            return df.select(*select_exprs)\n\n    def anonymize(self, df, columns: List[str], method: str, salt: Optional[str] = None):\n        \"\"\"Anonymize columns using Spark functions.\"\"\"\n        from pyspark.sql.functions import col, concat, lit, regexp_replace, sha2\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        ctx.debug(\n            \"Anonymizing columns\",\n            columns=columns,\n            method=method,\n            has_salt=salt is not None,\n        )\n\n        res = df\n        for c in columns:\n            if c not in df.columns:\n                ctx.warning(f\"Column '{c}' not found for anonymization, skipping\", column=c)\n                continue\n\n            if method == \"hash\":\n                if salt:\n                    res = res.withColumn(c, sha2(concat(col(c), lit(salt)), 256))\n                else:\n                    res = res.withColumn(c, sha2(col(c), 256))\n\n            elif method == \"mask\":\n                res = res.withColumn(c, regexp_replace(col(c), \".(?=.{4})\", \"*\"))\n\n            elif method == \"redact\":\n                res = res.withColumn(c, lit(\"[REDACTED]\"))\n\n        ctx.debug(f\"Anonymization completed using {method}\")\n        return res\n\n    def get_schema(self, df) -&gt; Dict[str, str]:\n        \"\"\"Get DataFrame schema with types.\"\"\"\n        return {f.name: f.dataType.simpleString() for f in df.schema}\n\n    def get_shape(self, df) -&gt; Tuple[int, int]:\n        \"\"\"Get DataFrame shape as (rows, columns).\"\"\"\n        return (df.count(), len(df.columns))\n\n    def count_rows(self, df) -&gt; int:\n        \"\"\"Count rows in DataFrame.\"\"\"\n        return df.count()\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        as_of_version: Optional[int] = None,\n        as_of_timestamp: Optional[str] = None,\n    ):\n        \"\"\"Read data using Spark.\n\n        Args:\n            connection: Connection object (with get_path method)\n            format: Data format (csv, parquet, json, delta, sql_server)\n            table: Table name\n            path: File path\n            streaming: Whether to read as a stream (readStream)\n            schema: Schema string in DDL format (required for streaming file sources)\n            options: Format-specific options (including versionAsOf for Delta time travel)\n            as_of_version: Time travel version\n            as_of_timestamp: Time travel timestamp\n\n        Returns:\n            Spark DataFrame (or Streaming DataFrame)\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        source_identifier = table or path or \"unknown\"\n        ctx.debug(\n            \"Starting Spark read\",\n            format=format,\n            source=source_identifier,\n            streaming=streaming,\n            as_of_version=as_of_version,\n            as_of_timestamp=as_of_timestamp,\n        )\n\n        # Handle Time Travel options\n        if as_of_version is not None:\n            options[\"versionAsOf\"] = as_of_version\n            ctx.debug(f\"Time travel enabled: version {as_of_version}\")\n        if as_of_timestamp is not None:\n            options[\"timestampAsOf\"] = as_of_timestamp\n            ctx.debug(f\"Time travel enabled: timestamp {as_of_timestamp}\")\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if streaming:\n                ctx.error(\"Streaming not supported for SQL Server / Azure SQL\")\n                raise ValueError(\"Streaming not supported for SQL Server / Azure SQL yet.\")\n\n            if not hasattr(connection, \"get_spark_options\"):\n                conn_type = type(connection).__name__\n                msg = f\"Connection type '{conn_type}' does not support Spark SQL read\"\n                ctx.error(msg, connection_type=conn_type)\n                raise ValueError(msg)\n\n            jdbc_options = connection.get_spark_options()\n            merged_options = {**jdbc_options, **options}\n\n            # Extract filter for SQL pushdown\n            sql_filter = merged_options.pop(\"filter\", None)\n\n            if \"query\" in merged_options:\n                merged_options.pop(\"dbtable\", None)\n                # If filter provided with query, append to WHERE clause\n                if sql_filter:\n                    existing_query = merged_options[\"query\"]\n                    # Wrap existing query and add filter\n                    if \"WHERE\" in existing_query.upper():\n                        merged_options[\"query\"] = f\"({existing_query}) AND ({sql_filter})\"\n                    else:\n                        subquery = f\"SELECT * FROM ({existing_query}) AS _subq WHERE {sql_filter}\"\n                        merged_options[\"query\"] = subquery\n                    ctx.debug(f\"Applied SQL pushdown filter to query: {sql_filter}\")\n            elif table:\n                # Build query with filter pushdown instead of using dbtable\n                if sql_filter:\n                    merged_options.pop(\"dbtable\", None)\n                    merged_options[\"query\"] = f\"SELECT * FROM {table} WHERE {sql_filter}\"\n                    ctx.debug(f\"Applied SQL pushdown filter: {sql_filter}\")\n                else:\n                    merged_options[\"dbtable\"] = table\n            elif \"dbtable\" not in merged_options:\n                ctx.error(\"SQL format requires 'table' config or 'query' option\")\n                raise ValueError(\"SQL format requires 'table' config or 'query' option\")\n\n            ctx.debug(\"Executing JDBC read\", has_query=\"query\" in merged_options)\n\n            try:\n                df = self.spark.read.format(\"jdbc\").options(**merged_options).load()\n                elapsed = (time.time() - start_time) * 1000\n                partition_count = df.rdd.getNumPartitions()\n\n                ctx.log_file_io(path=source_identifier, format=format, mode=\"read\")\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.info(\n                    \"JDBC read completed\",\n                    source=source_identifier,\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                )\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"JDBC read failed\",\n                    source=source_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Read based on format\n        if table:\n            # Managed/External Table (Catalog)\n            ctx.debug(f\"Reading from catalog table: {table}\")\n\n            if streaming:\n                reader = self.spark.readStream.format(format)\n            else:\n                reader = self.spark.read.format(format)\n\n            for key, value in options.items():\n                reader = reader.option(key, value)\n\n            try:\n                df = reader.table(table)\n\n                if \"filter\" in options:\n                    df = df.filter(options[\"filter\"])\n                    ctx.debug(f\"Applied filter: {options['filter']}\")\n\n                elapsed = (time.time() - start_time) * 1000\n\n                if not streaming:\n                    partition_count = df.rdd.getNumPartitions()\n                    ctx.log_spark_metrics(partition_count=partition_count)\n                    ctx.log_file_io(path=table, format=format, mode=\"read\")\n                    ctx.info(\n                        f\"Table read completed: {table}\",\n                        elapsed_ms=round(elapsed, 2),\n                        partitions=partition_count,\n                    )\n                else:\n                    ctx.info(f\"Streaming read started: {table}\", elapsed_ms=round(elapsed, 2))\n\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"Table read failed: {table}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        elif path:\n            # File Path\n            full_path = connection.get_path(path)\n            ctx.debug(f\"Reading from path: {full_path}\")\n\n            # Auto-detect encoding for CSV (Batch only)\n            if not streaming and format == \"csv\" and options.get(\"auto_encoding\"):\n                options = options.copy()\n                options.pop(\"auto_encoding\")\n\n                if \"encoding\" not in options:\n                    try:\n                        from odibi.utils.encoding import detect_encoding\n\n                        detected = detect_encoding(connection, path)\n                        if detected:\n                            options[\"encoding\"] = detected\n                            ctx.debug(f\"Detected encoding: {detected}\", path=path)\n                    except ImportError:\n                        pass\n                    except Exception as e:\n                        ctx.warning(\n                            f\"Encoding detection failed for {path}\",\n                            error_message=str(e),\n                        )\n\n            if streaming:\n                reader = self.spark.readStream.format(format)\n                if schema:\n                    reader = reader.schema(schema)\n                    ctx.debug(f\"Applied schema for streaming read: {schema[:100]}...\")\n                else:\n                    # Determine if we should warn about missing schema\n                    # Formats that can infer schema: delta, parquet, avro (embedded schema)\n                    # cloudFiles with schemaLocation or self-describing formats (avro, parquet) are fine\n                    should_warn = True\n\n                    if format in [\"delta\", \"parquet\"]:\n                        should_warn = False\n                    elif format == \"cloudFiles\":\n                        cloud_format = options.get(\"cloudFiles.format\", \"\")\n                        has_schema_location = \"cloudFiles.schemaLocation\" in options\n                        # avro and parquet have embedded schemas\n                        if cloud_format in [\"avro\", \"parquet\"] or has_schema_location:\n                            should_warn = False\n\n                    if should_warn:\n                        ctx.warning(\n                            f\"Streaming read from '{format}' format without schema. \"\n                            \"Schema inference is not supported for streaming sources. \"\n                            \"Consider adding 'schema' to your read config.\"\n                        )\n            else:\n                reader = self.spark.read.format(format)\n                if schema:\n                    reader = reader.schema(schema)\n\n            for key, value in options.items():\n                if key == \"header\" and isinstance(value, bool):\n                    value = str(value).lower()\n                reader = reader.option(key, value)\n\n            try:\n                df = reader.load(full_path)\n\n                if \"filter\" in options:\n                    df = df.filter(options[\"filter\"])\n                    ctx.debug(f\"Applied filter: {options['filter']}\")\n\n                elapsed = (time.time() - start_time) * 1000\n\n                if not streaming:\n                    partition_count = df.rdd.getNumPartitions()\n                    ctx.log_spark_metrics(partition_count=partition_count)\n                    ctx.log_file_io(path=path, format=format, mode=\"read\")\n                    ctx.info(\n                        f\"File read completed: {path}\",\n                        elapsed_ms=round(elapsed, 2),\n                        partitions=partition_count,\n                        format=format,\n                    )\n                else:\n                    ctx.info(f\"Streaming read started: {path}\", elapsed_ms=round(elapsed, 2))\n\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"File read failed: {path}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                    format=format,\n                )\n                raise\n        else:\n            ctx.error(\"Either path or table must be provided\")\n            raise ValueError(\"Either path or table must be provided\")\n\n    def write(\n        self,\n        df,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Spark.\n\n        Args:\n            df: Spark DataFrame to write\n            connection: Connection object\n            format: Output format (csv, parquet, json, delta)\n            table: Table name\n            path: File path\n            register_table: Name to register as external table (if path is used)\n            mode: Write mode (overwrite, append, error, ignore, upsert, append_once)\n            options: Format-specific options (including partition_by for partitioning)\n            streaming_config: StreamingWriteConfig for streaming DataFrames\n\n        Returns:\n            Optional dictionary containing Delta commit metadata (if format=delta),\n            or streaming query info (if streaming)\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        if getattr(df, \"isStreaming\", False) is True:\n            return self._write_streaming(\n                df=df,\n                connection=connection,\n                format=format,\n                table=table,\n                path=path,\n                register_table=register_table,\n                options=options,\n                streaming_config=streaming_config,\n            )\n\n        target_identifier = table or path or \"unknown\"\n        try:\n            partition_count = df.rdd.getNumPartitions()\n        except Exception:\n            partition_count = 1  # Fallback for mocks or unsupported DataFrames\n\n        # Auto-coalesce DataFrames for Delta writes to reduce file overhead\n        # Use coalesce_partitions option to explicitly set target partitions\n        # NOTE: We avoid df.count() here as it would trigger double-evaluation of lazy DataFrames\n        coalesce_partitions = options.pop(\"coalesce_partitions\", None)\n        if (\n            coalesce_partitions\n            and isinstance(partition_count, int)\n            and partition_count &gt; coalesce_partitions\n        ):\n            df = df.coalesce(coalesce_partitions)\n            ctx.debug(\n                f\"Coalesced DataFrame to {coalesce_partitions} partition(s)\",\n                original_partitions=partition_count,\n            )\n            partition_count = coalesce_partitions\n\n        ctx.debug(\n            \"Starting Spark write\",\n            format=format,\n            target=target_identifier,\n            mode=mode,\n            partitions=partition_count,\n        )\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if not hasattr(connection, \"get_spark_options\"):\n                conn_type = type(connection).__name__\n                msg = f\"Connection type '{conn_type}' does not support Spark SQL write\"\n                ctx.error(msg, connection_type=conn_type)\n                raise ValueError(msg)\n\n            jdbc_options = connection.get_spark_options()\n            merged_options = {**jdbc_options, **options}\n\n            if table:\n                merged_options[\"dbtable\"] = table\n            elif \"dbtable\" not in merged_options:\n                ctx.error(\"SQL format requires 'table' config or 'dbtable' option\")\n                raise ValueError(\"SQL format requires 'table' config or 'dbtable' option\")\n\n            if mode not in [\"overwrite\", \"append\", \"ignore\", \"error\"]:\n                if mode == \"fail\":\n                    mode = \"error\"\n                else:\n                    ctx.error(f\"Write mode '{mode}' not supported for Spark SQL write\")\n                    raise ValueError(f\"Write mode '{mode}' not supported for Spark SQL write\")\n\n            ctx.debug(\"Executing JDBC write\", target=table or merged_options.get(\"dbtable\"))\n\n            try:\n                df.write.format(\"jdbc\").options(**merged_options).mode(mode).save()\n                elapsed = (time.time() - start_time) * 1000\n                ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                ctx.info(\n                    \"JDBC write completed\",\n                    target=target_identifier,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"JDBC write failed\",\n                    target=target_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Handle Upsert/AppendOnce (Delta Only)\n        if mode in [\"upsert\", \"append_once\"]:\n            if format != \"delta\":\n                ctx.error(f\"Mode '{mode}' only supported for Delta format\")\n                raise NotImplementedError(\n                    f\"Mode '{mode}' only supported for Delta format in Spark engine.\"\n                )\n\n            keys = options.get(\"keys\")\n            if not keys:\n                ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n                raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            exists = self.table_exists(connection, table, path)\n            ctx.debug(\"Table existence check for merge\", target=target_identifier, exists=exists)\n\n            if not exists:\n                mode = \"overwrite\"\n                ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n            else:\n                from delta.tables import DeltaTable\n\n                target_dt = None\n                target_name = \"\"\n                is_table_target = False\n\n                if table:\n                    target_dt = DeltaTable.forName(self.spark, table)\n                    target_name = table\n                    is_table_target = True\n                elif path:\n                    full_path = connection.get_path(path)\n                    target_dt = DeltaTable.forPath(self.spark, full_path)\n                    target_name = full_path\n                    is_table_target = False\n\n                condition = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in keys])\n                ctx.debug(\"Executing Delta merge\", merge_mode=mode, keys=keys, condition=condition)\n\n                merge_builder = target_dt.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n                try:\n                    if mode == \"upsert\":\n                        merge_builder.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                    elif mode == \"append_once\":\n                        merge_builder.whenNotMatchedInsertAll().execute()\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Delta merge completed\",\n                        target=target_name,\n                        mode=mode,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    self._optimize_delta_write(target_name, options, is_table=is_table_target)\n                    commit_info = self._get_last_delta_commit_info(\n                        target_name, is_table=is_table_target\n                    )\n\n                    if commit_info:\n                        ctx.debug(\n                            \"Delta commit info\",\n                            version=commit_info.get(\"version\"),\n                            operation=commit_info.get(\"operation\"),\n                        )\n\n                    return commit_info\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Delta merge failed\",\n                        target=target_name,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n        # Get output location\n        if table:\n            # Managed/External Table (Catalog)\n            ctx.debug(f\"Writing to catalog table: {table}\")\n            writer = df.write.format(format).mode(mode)\n\n            partition_by = options.get(\"partition_by\")\n            if partition_by:\n                if isinstance(partition_by, str):\n                    partition_by = [partition_by]\n                writer = writer.partitionBy(*partition_by)\n                ctx.debug(f\"Partitioning by: {partition_by}\")\n\n            for key, value in options.items():\n                writer = writer.option(key, value)\n\n            try:\n                writer.saveAsTable(table)\n                elapsed = (time.time() - start_time) * 1000\n\n                ctx.log_file_io(\n                    path=table,\n                    format=format,\n                    mode=mode,\n                    partitions=partition_by,\n                )\n                ctx.info(\n                    f\"Table write completed: {table}\",\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if format == \"delta\":\n                    self._optimize_delta_write(table, options, is_table=True)\n                    return self._get_last_delta_commit_info(table, is_table=True)\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"Table write failed: {table}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        elif path:\n            full_path = connection.get_path(path)\n        else:\n            ctx.error(\"Either path or table must be provided\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Extract partition_by option\n        partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n\n        # Extract cluster_by option (Liquid Clustering)\n        cluster_by = options.pop(\"cluster_by\", None)\n\n        # Warn about partitioning anti-patterns\n        if partition_by and cluster_by:\n            import warnings\n\n            ctx.warning(\n                \"Conflict: Both 'partition_by' and 'cluster_by' are set\",\n                partition_by=partition_by,\n                cluster_by=cluster_by,\n            )\n            warnings.warn(\n                \"\u26a0\ufe0f  Conflict: Both 'partition_by' and 'cluster_by' (Liquid Clustering) are set. \"\n                \"Liquid Clustering supersedes partitioning. 'partition_by' will be ignored \"\n                \"if the table is being created now.\",\n                UserWarning,\n            )\n\n        elif partition_by:\n            import warnings\n\n            ctx.warning(\n                \"Partitioning warning: ensure low-cardinality columns\",\n                partition_by=partition_by,\n            )\n            warnings.warn(\n                \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n                \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n                \"and ensure each partition has &gt; 1000 rows.\",\n                UserWarning,\n            )\n\n        # Handle Upsert/Append-Once for Delta Lake (Path-based only for now)\n        if format == \"delta\" and mode in [\"upsert\", \"append_once\"]:\n            try:\n                from delta.tables import DeltaTable\n            except ImportError:\n                ctx.error(\"Delta Lake support requires 'delta-spark'\")\n                raise ImportError(\"Delta Lake support requires 'delta-spark'\")\n\n            if \"keys\" not in options:\n                ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n                raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n            if DeltaTable.isDeltaTable(self.spark, full_path):\n                ctx.debug(f\"Performing Delta merge at path: {full_path}\")\n                delta_table = DeltaTable.forPath(self.spark, full_path)\n                keys = options[\"keys\"]\n                if isinstance(keys, str):\n                    keys = [keys]\n\n                condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in keys])\n                merger = delta_table.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n                try:\n                    if mode == \"upsert\":\n                        merger.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                    else:\n                        merger.whenNotMatchedInsertAll().execute()\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Delta merge completed at path\",\n                        path=path,\n                        mode=mode,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    if register_table:\n                        try:\n                            table_in_catalog = self.spark.catalog.tableExists(register_table)\n                            needs_registration = not table_in_catalog\n\n                            # Handle orphan catalog entries (only for path-not-found errors)\n                            if table_in_catalog:\n                                try:\n                                    self.spark.table(register_table).limit(0).collect()\n                                    ctx.debug(\n                                        f\"Table '{register_table}' already registered and valid\"\n                                    )\n                                except Exception as verify_err:\n                                    error_str = str(verify_err)\n                                    is_orphan = (\n                                        \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                                        or \"Path does not exist\" in error_str\n                                        or \"FileNotFoundException\" in error_str\n                                    )\n                                    if is_orphan:\n                                        ctx.warning(\n                                            f\"Table '{register_table}' is orphan, re-registering\"\n                                        )\n                                        try:\n                                            self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                                        except Exception:\n                                            pass\n                                        needs_registration = True\n                                    else:\n                                        ctx.debug(\n                                            f\"Table '{register_table}' verify failed, \"\n                                            \"skipping registration\"\n                                        )\n\n                            if needs_registration:\n                                create_sql = (\n                                    f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                    f\"USING DELTA LOCATION '{full_path}'\"\n                                )\n                                self.spark.sql(create_sql)\n                                ctx.info(f\"Registered table: {register_table}\", path=full_path)\n                        except Exception as e:\n                            ctx.error(\n                                f\"Failed to register external table '{register_table}'\",\n                                error_message=str(e),\n                            )\n\n                    self._optimize_delta_write(full_path, options, is_table=False)\n                    return self._get_last_delta_commit_info(full_path, is_table=False)\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Delta merge failed at path\",\n                        path=path,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n            else:\n                mode = \"overwrite\"\n                ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n\n        # Write based on format (Path-based)\n        ctx.debug(f\"Writing to path: {full_path}\")\n\n        # Handle Liquid Clustering (New Table Creation via SQL)\n        if format == \"delta\" and cluster_by:\n            should_create = False\n            target_name = None\n\n            if table:\n                target_name = table\n                if mode == \"overwrite\":\n                    should_create = True\n                elif mode == \"append\":\n                    if not self.spark.catalog.tableExists(table):\n                        should_create = True\n            elif path:\n                full_path = connection.get_path(path)\n                target_name = f\"delta.`{full_path}`\"\n                if mode == \"overwrite\":\n                    should_create = True\n                elif mode == \"append\":\n                    try:\n                        from delta.tables import DeltaTable\n\n                        if not DeltaTable.isDeltaTable(self.spark, full_path):\n                            should_create = True\n                    except ImportError:\n                        pass\n\n            if should_create:\n                if isinstance(cluster_by, str):\n                    cluster_by = [cluster_by]\n\n                cols = \", \".join(cluster_by)\n                temp_view = f\"odibi_temp_writer_{abs(hash(str(target_name)))}\"\n                df.createOrReplaceTempView(temp_view)\n\n                create_cmd = (\n                    \"CREATE OR REPLACE TABLE\"\n                    if mode == \"overwrite\"\n                    else \"CREATE TABLE IF NOT EXISTS\"\n                )\n\n                sql = (\n                    f\"{create_cmd} {target_name} USING DELTA CLUSTER BY ({cols}) \"\n                    f\"AS SELECT * FROM {temp_view}\"\n                )\n\n                ctx.debug(\"Creating clustered Delta table\", sql=sql, cluster_by=cluster_by)\n\n                try:\n                    self.spark.sql(sql)\n                    self.spark.catalog.dropTempView(temp_view)\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Clustered Delta table created\",\n                        target=target_name,\n                        cluster_by=cluster_by,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    if register_table and path:\n                        try:\n                            reg_sql = (\n                                f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                f\"USING DELTA LOCATION '{full_path}'\"\n                            )\n                            self.spark.sql(reg_sql)\n                            ctx.info(f\"Registered table: {register_table}\")\n                        except Exception:\n                            pass\n\n                    if format == \"delta\":\n                        self._optimize_delta_write(\n                            target_name if table else full_path, options, is_table=bool(table)\n                        )\n                        return self._get_last_delta_commit_info(\n                            target_name if table else full_path, is_table=bool(table)\n                        )\n                    return None\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Failed to create clustered Delta table\",\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n        # Extract table_properties from options\n        table_properties = options.pop(\"table_properties\", None)\n\n        # For column mapping and other properties that must be set BEFORE write\n        original_configs = {}\n        if table_properties and format == \"delta\":\n            for prop_name, prop_value in table_properties.items():\n                spark_conf_key = (\n                    f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n                )\n                try:\n                    original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n                except Exception:\n                    original_configs[spark_conf_key] = None\n                self.spark.conf.set(spark_conf_key, prop_value)\n            ctx.debug(\n                \"Applied table properties as session defaults\",\n                properties=list(table_properties.keys()),\n            )\n\n        writer = df.write.format(format).mode(mode)\n\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            writer.save(full_path)\n            elapsed = (time.time() - start_time) * 1000\n\n            ctx.log_file_io(\n                path=path,\n                format=format,\n                mode=mode,\n                partitions=partition_by,\n            )\n            ctx.info(\n                f\"File write completed: {path}\",\n                format=format,\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"File write failed: {path}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n        finally:\n            for conf_key, original_value in original_configs.items():\n                if original_value is None:\n                    self.spark.conf.unset(conf_key)\n                else:\n                    self.spark.conf.set(conf_key, original_value)\n\n        if format == \"delta\":\n            self._optimize_delta_write(full_path, options, is_table=False)\n\n        if register_table and format == \"delta\":\n            try:\n                table_in_catalog = self.spark.catalog.tableExists(register_table)\n                needs_registration = not table_in_catalog\n\n                # Handle orphan catalog entries: table exists but points to deleted path\n                # Only treat as orphan if it's specifically a DELTA_PATH_DOES_NOT_EXIST error\n                if table_in_catalog:\n                    try:\n                        self.spark.table(register_table).limit(0).collect()\n                        ctx.debug(\n                            f\"Table '{register_table}' already registered and valid, \"\n                            \"skipping registration\"\n                        )\n                    except Exception as verify_err:\n                        error_str = str(verify_err)\n                        is_orphan = (\n                            \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                            or \"Path does not exist\" in error_str\n                            or \"FileNotFoundException\" in error_str\n                        )\n\n                        if is_orphan:\n                            # Orphan entry - table in catalog but path was deleted\n                            ctx.warning(\n                                f\"Table '{register_table}' is orphan (path deleted), \"\n                                \"dropping and re-registering\",\n                                error_message=error_str[:200],\n                            )\n                            try:\n                                self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                            except Exception:\n                                pass  # Best effort cleanup\n                            needs_registration = True\n                        else:\n                            # Other error (auth, network, etc.) - don't drop, just log\n                            ctx.debug(\n                                f\"Table '{register_table}' exists but verify failed \"\n                                \"(not orphan), skipping registration\",\n                                error_message=error_str[:200],\n                            )\n\n                if needs_registration:\n                    ctx.debug(f\"Registering table '{register_table}' at '{full_path}'\")\n                    reg_sql = (\n                        f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                        f\"USING DELTA LOCATION '{full_path}'\"\n                    )\n                    self.spark.sql(reg_sql)\n                    ctx.info(f\"Registered table: {register_table}\", path=full_path)\n            except Exception as e:\n                ctx.error(\n                    f\"Failed to register table '{register_table}'\",\n                    error_message=str(e),\n                )\n                raise RuntimeError(\n                    f\"Failed to register external table '{register_table}': {e}\"\n                ) from e\n\n        if format == \"delta\":\n            return self._get_last_delta_commit_info(full_path, is_table=False)\n\n        return None\n\n    def _write_streaming(\n        self,\n        df,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Write streaming DataFrame using Spark Structured Streaming.\n\n        Args:\n            df: Streaming Spark DataFrame\n            connection: Connection object\n            format: Output format (delta, kafka, etc.)\n            table: Table name\n            path: File path\n            register_table: Name to register as external table (if path is used)\n            options: Format-specific options\n            streaming_config: StreamingWriteConfig with streaming parameters\n\n        Returns:\n            Dictionary with streaming query information\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        if streaming_config is None:\n            ctx.error(\"Streaming DataFrame requires streaming_config\")\n            raise ValueError(\n                \"Streaming DataFrame detected but no streaming_config provided. \"\n                \"Add a 'streaming' section to your write config with at least \"\n                \"'checkpoint_location' specified.\"\n            )\n\n        target_identifier = table or path or \"unknown\"\n\n        checkpoint_location = streaming_config.checkpoint_location\n        if checkpoint_location and connection:\n            if not checkpoint_location.startswith(\n                (\"abfss://\", \"s3://\", \"gs://\", \"dbfs://\", \"hdfs://\", \"wasbs://\")\n            ):\n                checkpoint_location = connection.get_path(checkpoint_location)\n                ctx.debug(\n                    \"Resolved checkpoint location through connection\",\n                    original=streaming_config.checkpoint_location,\n                    resolved=checkpoint_location,\n                )\n\n        ctx.debug(\n            \"Starting streaming write\",\n            format=format,\n            target=target_identifier,\n            output_mode=streaming_config.output_mode,\n            checkpoint=checkpoint_location,\n        )\n\n        writer = df.writeStream.format(format)\n        writer = writer.outputMode(streaming_config.output_mode)\n        writer = writer.option(\"checkpointLocation\", checkpoint_location)\n\n        if streaming_config.query_name:\n            writer = writer.queryName(streaming_config.query_name)\n\n        if streaming_config.trigger:\n            trigger = streaming_config.trigger\n            if trigger.once:\n                writer = writer.trigger(once=True)\n            elif trigger.available_now:\n                writer = writer.trigger(availableNow=True)\n            elif trigger.processing_time:\n                writer = writer.trigger(processingTime=trigger.processing_time)\n            elif trigger.continuous:\n                writer = writer.trigger(continuous=trigger.continuous)\n\n        partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            if table:\n                query = writer.toTable(table)\n                ctx.info(\n                    f\"Streaming query started: writing to table {table}\",\n                    query_id=str(query.id),\n                    query_name=query.name,\n                )\n            elif path:\n                full_path = connection.get_path(path)\n                query = writer.start(full_path)\n                ctx.info(\n                    f\"Streaming query started: writing to path {path}\",\n                    query_id=str(query.id),\n                    query_name=query.name,\n                )\n            else:\n                ctx.error(\"Either path or table must be provided for streaming write\")\n                raise ValueError(\"Either path or table must be provided for streaming write\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            result = {\n                \"streaming\": True,\n                \"query_id\": str(query.id),\n                \"query_name\": query.name,\n                \"status\": \"running\",\n                \"target\": target_identifier,\n                \"output_mode\": streaming_config.output_mode,\n                \"checkpoint_location\": streaming_config.checkpoint_location,\n                \"elapsed_ms\": round(elapsed, 2),\n            }\n\n            should_wait = streaming_config.await_termination\n            if streaming_config.trigger:\n                trigger = streaming_config.trigger\n                if trigger.once or trigger.available_now:\n                    should_wait = True\n\n            if should_wait:\n                ctx.info(\n                    \"Awaiting streaming query termination\",\n                    timeout_seconds=streaming_config.timeout_seconds,\n                )\n                query.awaitTermination(streaming_config.timeout_seconds)\n                result[\"status\"] = \"terminated\"\n                elapsed = (time.time() - start_time) * 1000\n                result[\"elapsed_ms\"] = round(elapsed, 2)\n                ctx.info(\n                    \"Streaming query terminated\",\n                    query_id=str(query.id),\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table and path and format == \"delta\":\n                    full_path = connection.get_path(path)\n                    try:\n                        self.spark.sql(\n                            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                            f\"USING DELTA LOCATION '{full_path}'\"\n                        )\n                        ctx.info(\n                            f\"Registered external table: {register_table}\",\n                            path=full_path,\n                        )\n                        result[\"registered_table\"] = register_table\n                    except Exception as reg_err:\n                        ctx.warning(\n                            f\"Failed to register external table '{register_table}'\",\n                            error=str(reg_err),\n                        )\n            else:\n                result[\"streaming_query\"] = query\n                if register_table:\n                    ctx.warning(\n                        \"register_table ignored for continuous streaming. \"\n                        \"Table will be registered after query terminates or manually.\"\n                    )\n\n            return result\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Streaming write failed\",\n                target=target_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def execute_sql(self, sql: str, context: Any = None):\n        \"\"\"Execute SQL query in Spark.\n\n        Args:\n            sql: SQL query string\n            context: Execution context (optional, not used for Spark)\n\n        Returns:\n            Spark DataFrame with query results\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Executing Spark SQL\", query_preview=sql[:200] if len(sql) &gt; 200 else sql)\n\n        try:\n            result = self.spark.sql(sql)\n            elapsed = (time.time() - start_time) * 1000\n            partition_count = result.rdd.getNumPartitions()\n\n            ctx.log_spark_metrics(partition_count=partition_count)\n            ctx.info(\n                \"Spark SQL executed\",\n                elapsed_ms=round(elapsed, 2),\n                partitions=partition_count,\n            )\n\n            return result\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            error_type = type(e).__name__\n            clean_message = _extract_spark_error_message(e)\n\n            if \"AnalysisException\" in error_type:\n                ctx.error(\n                    \"Spark SQL Analysis Error\",\n                    error_type=error_type,\n                    error_message=clean_message,\n                    query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise TransformError(f\"Spark SQL Analysis Error: {clean_message}\")\n\n            if \"ParseException\" in error_type:\n                ctx.error(\n                    \"Spark SQL Parse Error\",\n                    error_type=error_type,\n                    error_message=clean_message,\n                    query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise TransformError(f\"Spark SQL Parse Error: {clean_message}\")\n\n            ctx.error(\n                \"Spark SQL execution failed\",\n                error_type=error_type,\n                error_message=clean_message,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Error: {clean_message}\")\n\n    def execute_transform(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"SparkEngine.execute_transform() will be implemented in Phase 2B. \"\n            \"See PHASES.md for implementation plan.\"\n        )\n\n    def execute_operation(self, operation: str, params: Dict[str, Any], df) -&gt; Any:\n        \"\"\"Execute built-in operation on Spark DataFrame.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        params = params or {}\n\n        ctx.debug(f\"Executing operation: {operation}\", params=list(params.keys()))\n\n        if operation == \"pivot\":\n            group_by = params.get(\"group_by\", [])\n            pivot_column = params.get(\"pivot_column\")\n            value_column = params.get(\"value_column\")\n            agg_func = params.get(\"agg_func\", \"first\")\n\n            if not pivot_column or not value_column:\n                ctx.error(\"Pivot requires 'pivot_column' and 'value_column'\")\n                raise ValueError(\"Pivot requires 'pivot_column' and 'value_column'\")\n\n            if isinstance(group_by, str):\n                group_by = [group_by]\n\n            agg_expr = {value_column: agg_func}\n            return df.groupBy(*group_by).pivot(pivot_column).agg(agg_expr)\n\n        elif operation == \"drop_duplicates\":\n            subset = params.get(\"subset\")\n            if subset:\n                if isinstance(subset, str):\n                    subset = [subset]\n                return df.dropDuplicates(subset=subset)\n            return df.dropDuplicates()\n\n        elif operation == \"fillna\":\n            value = params.get(\"value\")\n            subset = params.get(\"subset\")\n            return df.fillna(value, subset=subset)\n\n        elif operation == \"drop\":\n            columns = params.get(\"columns\")\n            if not columns:\n                return df\n            if isinstance(columns, str):\n                columns = [columns]\n            return df.drop(*columns)\n\n        elif operation == \"rename\":\n            columns = params.get(\"columns\")\n            if not columns:\n                return df\n\n            res = df\n            for old_name, new_name in columns.items():\n                res = res.withColumnRenamed(old_name, new_name)\n            return res\n\n        elif operation == \"sort\":\n            by = params.get(\"by\")\n            ascending = params.get(\"ascending\", True)\n\n            if not by:\n                return df\n\n            if isinstance(by, str):\n                by = [by]\n\n            if not ascending:\n                from pyspark.sql.functions import desc\n\n                sort_cols = [desc(c) for c in by]\n                return df.orderBy(*sort_cols)\n\n            return df.orderBy(*by)\n\n        elif operation == \"sample\":\n            fraction = params.get(\"frac\", 0.1)\n            seed = params.get(\"random_state\")\n            with_replacement = params.get(\"replace\", False)\n            return df.sample(withReplacement=with_replacement, fraction=fraction, seed=seed)\n\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext\n            from odibi.registry import FunctionRegistry\n\n            ctx.debug(\n                f\"Checking registry for operation: {operation}\",\n                registered_functions=list(FunctionRegistry._functions.keys())[:10],\n                has_function=FunctionRegistry.has_function(operation),\n            )\n\n            if FunctionRegistry.has_function(operation):\n                ctx.debug(f\"Executing registered transformer as operation: {operation}\")\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df\n                from odibi.context import SparkContext\n\n                engine_ctx = EngineContext(\n                    context=SparkContext(self.spark),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n            ctx.error(f\"Unsupported operation for Spark engine: {operation}\")\n            raise ValueError(f\"Unsupported operation for Spark engine: {operation}\")\n\n    def count_nulls(self, df, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\"\"\"\n        from pyspark.sql.functions import col, count, when\n\n        missing = set(columns) - set(df.columns)\n        if missing:\n            raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing)}\")\n\n        aggs = [count(when(col(c).isNull(), c)).alias(c) for c in columns]\n        result = df.select(*aggs).collect()[0].asDict()\n        return result\n\n    def validate_schema(self, df, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\"\"\"\n        failures = []\n\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(df.columns)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        if \"types\" in schema_rules:\n            type_map = {\n                \"int\": [\"integer\", \"long\", \"short\", \"byte\", \"bigint\"],\n                \"float\": [\"double\", \"float\"],\n                \"str\": [\"string\"],\n                \"bool\": [\"boolean\"],\n            }\n\n            for col_name, expected_type in schema_rules[\"types\"].items():\n                if col_name not in df.columns:\n                    failures.append(f\"Column '{col_name}' not found for type validation\")\n                    continue\n\n                actual_type = dict(df.dtypes)[col_name]\n                expected_dtypes = type_map.get(expected_type, [expected_type])\n\n                if actual_type not in expected_dtypes:\n                    failures.append(\n                        f\"Column '{col_name}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def validate_data(self, df, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate DataFrame against rules.\"\"\"\n        from pyspark.sql.functions import col\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        failures = []\n\n        if validation_config.not_empty:\n            if df.isEmpty():\n                failures.append(\"DataFrame is empty\")\n\n        if validation_config.no_nulls:\n            null_counts = self.count_nulls(df, validation_config.no_nulls)\n            for col_name, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col_name}' has {count} null values\")\n\n        if validation_config.schema_validation:\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        if validation_config.ranges:\n            for col_name, bounds in validation_config.ranges.items():\n                if col_name in df.columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    if min_val is not None:\n                        count = df.filter(col(col_name) &lt; min_val).count()\n                        if count &gt; 0:\n                            failures.append(f\"Column '{col_name}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        count = df.filter(col(col_name) &gt; max_val).count()\n                        if count &gt; 0:\n                            failures.append(f\"Column '{col_name}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col_name}' not found for range validation\")\n\n        if validation_config.allowed_values:\n            for col_name, allowed in validation_config.allowed_values.items():\n                if col_name in df.columns:\n                    count = df.filter(~col(col_name).isin(allowed)).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col_name}' not found for allowed values validation\")\n\n        ctx.log_validation_result(\n            passed=len(failures) == 0,\n            rule_name=\"data_validation\",\n            failures=failures if failures else None,\n        )\n\n        return failures\n\n    def get_sample(self, df, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\"\"\"\n        return [row.asDict() for row in df.limit(n).collect()]\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Handles orphan catalog entries where the table is registered but\n        the underlying Delta path no longer exists.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        if table:\n            try:\n                if not self.spark.catalog.tableExists(table):\n                    ctx.debug(f\"Table does not exist: {table}\")\n                    return False\n                # Table exists in catalog - verify it's actually readable\n                # This catches orphan entries where path was deleted\n                self.spark.table(table).limit(0).collect()\n                ctx.debug(f\"Table existence check: {table}\", exists=True)\n                return True\n            except Exception as e:\n                # Table exists in catalog but underlying data is gone (orphan entry)\n                # This is expected during first-run detection - log at debug level\n                ctx.debug(\n                    f\"Table {table} exists in catalog but is not accessible (treating as first run)\",\n                    error_message=str(e),\n                )\n                return False\n        elif path:\n            try:\n                from delta.tables import DeltaTable\n\n                full_path = connection.get_path(path)\n                exists = DeltaTable.isDeltaTable(self.spark, full_path)\n                ctx.debug(f\"Delta table existence check: {path}\", exists=exists)\n                return exists\n            except ImportError:\n                try:\n                    full_path = connection.get_path(path)\n                    exists = (\n                        self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(\n                            self.spark.sparkContext._jsc.hadoopConfiguration()\n                        ).exists(\n                            self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path(\n                                full_path\n                            )\n                        )\n                    )\n                    ctx.debug(f\"Path existence check: {path}\", exists=exists)\n                    return exists\n                except Exception as e:\n                    ctx.warning(f\"Path existence check failed: {path}\", error_message=str(e))\n                    return False\n            except Exception as e:\n                ctx.warning(f\"Table existence check failed: {path}\", error_message=str(e))\n                return False\n        return False\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            if table:\n                if self.spark.catalog.tableExists(table):\n                    schema = self.get_schema(self.spark.table(table))\n                    ctx.debug(f\"Retrieved schema for table: {table}\", columns=len(schema))\n                    return schema\n            elif path:\n                full_path = connection.get_path(path)\n                if format == \"delta\":\n                    from delta.tables import DeltaTable\n\n                    if DeltaTable.isDeltaTable(self.spark, full_path):\n                        schema = self.get_schema(DeltaTable.forPath(self.spark, full_path).toDF())\n                        ctx.debug(f\"Retrieved Delta schema: {path}\", columns=len(schema))\n                        return schema\n                elif format == \"parquet\":\n                    schema = self.get_schema(self.spark.read.parquet(full_path))\n                    ctx.debug(f\"Retrieved Parquet schema: {path}\", columns=len(schema))\n                    return schema\n                elif format:\n                    schema = self.get_schema(self.spark.read.format(format).load(full_path))\n                    ctx.debug(f\"Retrieved schema: {path}\", format=format, columns=len(schema))\n                    return schema\n        except Exception as e:\n            ctx.warning(\n                \"Failed to get schema\",\n                table=table,\n                path=path,\n                error_message=str(e),\n            )\n        return None\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n    ) -&gt; None:\n        \"\"\"VACUUM a Delta table to remove old files.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n        )\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            delta_table.vacuum(retention_hours / 24.0)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta VACUUM completed\",\n                path=path,\n                retention_hours=retention_hours,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Delta VACUUM failed\",\n                path=path,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Fetching Delta history\", path=path, limit=limit)\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            history_df = delta_table.history(limit) if limit else delta_table.history()\n            history = [row.asDict() for row in history_df.collect()]\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta history retrieved\",\n                path=path,\n                versions_returned=len(history),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n            return history\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Failed to get Delta history\",\n                path=path,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n        \"\"\"Restore Delta table to a specific version.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Restoring Delta table\", path=path, version=version)\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            delta_table.restoreToVersion(version)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta table restored\",\n                path=path,\n                version=version,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Delta restore failed\",\n                path=path,\n                version=version,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n        if format != \"delta\" or not config or not config.enabled:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        if table:\n            target = table\n        elif path:\n            full_path = connection.get_path(path)\n            target = f\"delta.`{full_path}`\"\n        else:\n            return\n\n        ctx.debug(\"Starting table maintenance\", target=target)\n\n        try:\n            ctx.debug(f\"Running OPTIMIZE on {target}\")\n            self.spark.sql(f\"OPTIMIZE {target}\")\n\n            retention = config.vacuum_retention_hours\n            if retention is not None and retention &gt; 0:\n                ctx.debug(f\"Running VACUUM on {target}\", retention_hours=retention)\n                self.spark.sql(f\"VACUUM {target} RETAIN {retention} HOURS\")\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                target=target,\n                vacuum_retention_hours=retention,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.warning(\n                f\"Auto-optimize failed for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n    def get_source_files(self, df) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\"\"\"\n        try:\n            return df.inputFiles()\n        except Exception:\n            return []\n\n    def profile_nulls(self, df) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\"\"\"\n        from pyspark.sql.functions import col, mean, when\n\n        aggs = []\n        for c in df.columns:\n            aggs.append(mean(when(col(c).isNull(), 1).otherwise(0)).alias(c))\n\n        if not aggs:\n            return {}\n\n        try:\n            result = df.select(*aggs).collect()[0].asDict()\n            return result\n        except Exception:\n            return {}\n\n    def filter_greater_than(self, df, column: str, value: Any) -&gt; Any:\n        \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n        return df.filter(f\"{column} &gt; '{value}'\")\n\n    def filter_coalesce(self, df, col1: str, col2: str, op: str, value: Any) -&gt; Any:\n        \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n        return df.filter(f\"COALESCE({col1}, {col2}) {op} '{value}'\")\n\n    def add_write_metadata(\n        self,\n        df,\n        metadata_config,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ):\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: Spark DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added\n        \"\"\"\n        from pyspark.sql import functions as F\n\n        from odibi.config import WriteMetadataConfig\n\n        if metadata_config is True:\n            config = WriteMetadataConfig()\n        elif isinstance(metadata_config, WriteMetadataConfig):\n            config = metadata_config\n        else:\n            return df\n\n        if config.extracted_at:\n            df = df.withColumn(\"_extracted_at\", F.current_timestamp())\n\n        if config.source_file and is_file_source and source_path:\n            df = df.withColumn(\"_source_file\", F.lit(source_path))\n\n        if config.source_connection and source_connection:\n            df = df.withColumn(\"_source_connection\", F.lit(source_connection))\n\n        if config.source_table and source_table:\n            df = df.withColumn(\"_source_table\", F.lit(source_table))\n\n        return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.__init__","title":"<code>__init__(connections=None, spark_session=None, config=None)</code>","text":"<p>Initialize Spark engine with import guard.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects (for multi-account config)</p> <code>None</code> <code>spark_session</code> <p>Existing SparkSession (optional, creates new if None)</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pyspark not installed</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    spark_session=None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Spark engine with import guard.\n\n    Args:\n        connections: Dictionary of connection objects (for multi-account config)\n        spark_session: Existing SparkSession (optional, creates new if None)\n        config: Engine configuration (optional)\n\n    Raises:\n        ImportError: If pyspark not installed\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    ctx.debug(\"Initializing SparkEngine\", connections_count=len(connections or {}))\n\n    try:\n        from pyspark.sql import SparkSession\n    except ImportError as e:\n        ctx.error(\n            \"PySpark not installed\",\n            error_type=\"ImportError\",\n            suggestion=\"pip install odibi[spark]\",\n        )\n        raise ImportError(\n            \"Spark support requires 'pip install odibi[spark]'. \"\n            \"See docs/setup_databricks.md for setup instructions.\"\n        ) from e\n\n    start_time = time.time()\n\n    # Configure Delta Lake support\n    try:\n        from delta import configure_spark_with_delta_pip\n\n        builder = SparkSession.builder.appName(\"odibi\").config(\n            \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n        )\n\n        # Performance Optimizations\n        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n        # Reduce Verbosity\n        builder = builder.config(\n            \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n        builder = builder.config(\n            \"spark.executor.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n\n        self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n        ctx.debug(\"Delta Lake support enabled\")\n\n    except ImportError:\n        ctx.debug(\"Delta Lake not available, using standard Spark\")\n        builder = SparkSession.builder.appName(\"odibi\").config(\n            \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n        )\n\n        # Performance Optimizations\n        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n        # Reduce Verbosity\n        builder = builder.config(\n            \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n\n        self.spark = spark_session or builder.getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n    self.config = config or {}\n    self.connections = connections or {}\n\n    # Configure all ADLS connections upfront\n    self._configure_all_connections()\n\n    # Apply user-defined Spark configs from performance settings\n    self._apply_spark_config()\n\n    elapsed = (time.time() - start_time) * 1000\n    ctx.info(\n        \"SparkEngine initialized\",\n        elapsed_ms=round(elapsed, 2),\n        app_name=self.spark.sparkContext.appName,\n        spark_version=self.spark.version,\n        connections_configured=len(self.connections),\n        using_existing_session=spark_session is not None,\n    )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Spark DataFrame</p> required <code>metadata_config</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <p>DataFrame with metadata columns added</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def add_write_metadata(\n    self,\n    df,\n    metadata_config,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n):\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: Spark DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added\n    \"\"\"\n    from pyspark.sql import functions as F\n\n    from odibi.config import WriteMetadataConfig\n\n    if metadata_config is True:\n        config = WriteMetadataConfig()\n    elif isinstance(metadata_config, WriteMetadataConfig):\n        config = metadata_config\n    else:\n        return df\n\n    if config.extracted_at:\n        df = df.withColumn(\"_extracted_at\", F.current_timestamp())\n\n    if config.source_file and is_file_source and source_path:\n        df = df.withColumn(\"_source_file\", F.lit(source_path))\n\n    if config.source_connection and source_connection:\n        df = df.withColumn(\"_source_connection\", F.lit(source_connection))\n\n    if config.source_table and source_table:\n        df = df.withColumn(\"_source_table\", F.lit(source_table))\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize columns using Spark functions.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def anonymize(self, df, columns: List[str], method: str, salt: Optional[str] = None):\n    \"\"\"Anonymize columns using Spark functions.\"\"\"\n    from pyspark.sql.functions import col, concat, lit, regexp_replace, sha2\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    ctx.debug(\n        \"Anonymizing columns\",\n        columns=columns,\n        method=method,\n        has_salt=salt is not None,\n    )\n\n    res = df\n    for c in columns:\n        if c not in df.columns:\n            ctx.warning(f\"Column '{c}' not found for anonymization, skipping\", column=c)\n            continue\n\n        if method == \"hash\":\n            if salt:\n                res = res.withColumn(c, sha2(concat(col(c), lit(salt)), 256))\n            else:\n                res = res.withColumn(c, sha2(col(c), 256))\n\n        elif method == \"mask\":\n            res = res.withColumn(c, regexp_replace(col(c), \".(?=.{4})\", \"*\"))\n\n        elif method == \"redact\":\n            res = res.withColumn(c, lit(\"[REDACTED]\"))\n\n    ctx.debug(f\"Anonymization completed using {method}\")\n    return res\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def count_nulls(self, df, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\"\"\"\n    from pyspark.sql.functions import col, count, when\n\n    missing = set(columns) - set(df.columns)\n    if missing:\n        raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing)}\")\n\n    aggs = [count(when(col(c).isNull(), c)).alias(c) for c in columns]\n    result = df.select(*aggs).collect()[0].asDict()\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def count_rows(self, df) -&gt; int:\n    \"\"\"Count rows in DataFrame.\"\"\"\n    return df.count()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation on Spark DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def execute_operation(self, operation: str, params: Dict[str, Any], df) -&gt; Any:\n    \"\"\"Execute built-in operation on Spark DataFrame.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    params = params or {}\n\n    ctx.debug(f\"Executing operation: {operation}\", params=list(params.keys()))\n\n    if operation == \"pivot\":\n        group_by = params.get(\"group_by\", [])\n        pivot_column = params.get(\"pivot_column\")\n        value_column = params.get(\"value_column\")\n        agg_func = params.get(\"agg_func\", \"first\")\n\n        if not pivot_column or not value_column:\n            ctx.error(\"Pivot requires 'pivot_column' and 'value_column'\")\n            raise ValueError(\"Pivot requires 'pivot_column' and 'value_column'\")\n\n        if isinstance(group_by, str):\n            group_by = [group_by]\n\n        agg_expr = {value_column: agg_func}\n        return df.groupBy(*group_by).pivot(pivot_column).agg(agg_expr)\n\n    elif operation == \"drop_duplicates\":\n        subset = params.get(\"subset\")\n        if subset:\n            if isinstance(subset, str):\n                subset = [subset]\n            return df.dropDuplicates(subset=subset)\n        return df.dropDuplicates()\n\n    elif operation == \"fillna\":\n        value = params.get(\"value\")\n        subset = params.get(\"subset\")\n        return df.fillna(value, subset=subset)\n\n    elif operation == \"drop\":\n        columns = params.get(\"columns\")\n        if not columns:\n            return df\n        if isinstance(columns, str):\n            columns = [columns]\n        return df.drop(*columns)\n\n    elif operation == \"rename\":\n        columns = params.get(\"columns\")\n        if not columns:\n            return df\n\n        res = df\n        for old_name, new_name in columns.items():\n            res = res.withColumnRenamed(old_name, new_name)\n        return res\n\n    elif operation == \"sort\":\n        by = params.get(\"by\")\n        ascending = params.get(\"ascending\", True)\n\n        if not by:\n            return df\n\n        if isinstance(by, str):\n            by = [by]\n\n        if not ascending:\n            from pyspark.sql.functions import desc\n\n            sort_cols = [desc(c) for c in by]\n            return df.orderBy(*sort_cols)\n\n        return df.orderBy(*by)\n\n    elif operation == \"sample\":\n        fraction = params.get(\"frac\", 0.1)\n        seed = params.get(\"random_state\")\n        with_replacement = params.get(\"replace\", False)\n        return df.sample(withReplacement=with_replacement, fraction=fraction, seed=seed)\n\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext\n        from odibi.registry import FunctionRegistry\n\n        ctx.debug(\n            f\"Checking registry for operation: {operation}\",\n            registered_functions=list(FunctionRegistry._functions.keys())[:10],\n            has_function=FunctionRegistry.has_function(operation),\n        )\n\n        if FunctionRegistry.has_function(operation):\n            ctx.debug(f\"Executing registered transformer as operation: {operation}\")\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df\n            from odibi.context import SparkContext\n\n            engine_ctx = EngineContext(\n                context=SparkContext(self.spark),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n        ctx.error(f\"Unsupported operation for Spark engine: {operation}\")\n        raise ValueError(f\"Unsupported operation for Spark engine: {operation}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.execute_sql","title":"<code>execute_sql(sql, context=None)</code>","text":"<p>Execute SQL query in Spark.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Any</code> <p>Execution context (optional, not used for Spark)</p> <code>None</code> <p>Returns:</p> Type Description <p>Spark DataFrame with query results</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Any = None):\n    \"\"\"Execute SQL query in Spark.\n\n    Args:\n        sql: SQL query string\n        context: Execution context (optional, not used for Spark)\n\n    Returns:\n        Spark DataFrame with query results\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Executing Spark SQL\", query_preview=sql[:200] if len(sql) &gt; 200 else sql)\n\n    try:\n        result = self.spark.sql(sql)\n        elapsed = (time.time() - start_time) * 1000\n        partition_count = result.rdd.getNumPartitions()\n\n        ctx.log_spark_metrics(partition_count=partition_count)\n        ctx.info(\n            \"Spark SQL executed\",\n            elapsed_ms=round(elapsed, 2),\n            partitions=partition_count,\n        )\n\n        return result\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        error_type = type(e).__name__\n        clean_message = _extract_spark_error_message(e)\n\n        if \"AnalysisException\" in error_type:\n            ctx.error(\n                \"Spark SQL Analysis Error\",\n                error_type=error_type,\n                error_message=clean_message,\n                query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Analysis Error: {clean_message}\")\n\n        if \"ParseException\" in error_type:\n            ctx.error(\n                \"Spark SQL Parse Error\",\n                error_type=error_type,\n                error_message=clean_message,\n                query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Parse Error: {clean_message}\")\n\n        ctx.error(\n            \"Spark SQL execution failed\",\n            error_type=error_type,\n            error_message=clean_message,\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise TransformError(f\"Spark SQL Error: {clean_message}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.filter_coalesce","title":"<code>filter_coalesce(df, col1, col2, op, value)</code>","text":"<p>Filter using COALESCE(col1, col2) op value.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def filter_coalesce(self, df, col1: str, col2: str, op: str, value: Any) -&gt; Any:\n    \"\"\"Filter using COALESCE(col1, col2) op value.\"\"\"\n    return df.filter(f\"COALESCE({col1}, {col2}) {op} '{value}'\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.filter_greater_than","title":"<code>filter_greater_than(df, column, value)</code>","text":"<p>Filter DataFrame where column &gt; value.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def filter_greater_than(self, df, column: str, value: Any) -&gt; Any:\n    \"\"\"Filter DataFrame where column &gt; value.\"\"\"\n    return df.filter(f\"{column} &gt; '{value}'\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Fetching Delta history\", path=path, limit=limit)\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        history_df = delta_table.history(limit) if limit else delta_table.history()\n        history = [row.asDict() for row in history_df.collect()]\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=path,\n            versions_returned=len(history),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Failed to get Delta history\",\n            path=path,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_sample(self, df, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\"\"\"\n    return [row.asDict() for row in df.limit(n).collect()]\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema with types.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_schema(self, df) -&gt; Dict[str, str]:\n    \"\"\"Get DataFrame schema with types.\"\"\"\n    return {f.name: f.dataType.simpleString() for f in df.schema}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape as (rows, columns).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_shape(self, df) -&gt; Tuple[int, int]:\n    \"\"\"Get DataFrame shape as (rows, columns).\"\"\"\n    return (df.count(), len(df.columns))\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_source_files(self, df) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\"\"\"\n    try:\n        return df.inputFiles()\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    try:\n        if table:\n            if self.spark.catalog.tableExists(table):\n                schema = self.get_schema(self.spark.table(table))\n                ctx.debug(f\"Retrieved schema for table: {table}\", columns=len(schema))\n                return schema\n        elif path:\n            full_path = connection.get_path(path)\n            if format == \"delta\":\n                from delta.tables import DeltaTable\n\n                if DeltaTable.isDeltaTable(self.spark, full_path):\n                    schema = self.get_schema(DeltaTable.forPath(self.spark, full_path).toDF())\n                    ctx.debug(f\"Retrieved Delta schema: {path}\", columns=len(schema))\n                    return schema\n            elif format == \"parquet\":\n                schema = self.get_schema(self.spark.read.parquet(full_path))\n                ctx.debug(f\"Retrieved Parquet schema: {path}\", columns=len(schema))\n                return schema\n            elif format:\n                schema = self.get_schema(self.spark.read.format(format).load(full_path))\n                ctx.debug(f\"Retrieved schema: {path}\", format=format, columns=len(schema))\n                return schema\n    except Exception as e:\n        ctx.warning(\n            \"Failed to get schema\",\n            table=table,\n            path=path,\n            error_message=str(e),\n        )\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def harmonize_schema(self, df, target_schema: Dict[str, str], policy: Any):\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n    from pyspark.sql.functions import col, lit\n\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    target_cols = list(target_schema.keys())\n    current_cols = df.columns\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    ctx.debug(\n        \"Schema harmonization\",\n        target_columns=len(target_cols),\n        current_columns=len(current_cols),\n        missing_columns=list(missing) if missing else None,\n        new_columns=list(new_cols) if new_cols else None,\n        policy_mode=policy.mode.value if hasattr(policy.mode, \"value\") else str(policy.mode),\n    )\n\n    # Check Validations\n    if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n        ctx.error(\n            f\"Schema Policy Violation: Missing columns {missing}\",\n            missing_columns=list(missing),\n        )\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n        ctx.error(\n            f\"Schema Policy Violation: New columns {new_cols}\",\n            new_columns=list(new_cols),\n        )\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # Apply Transformations\n    if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n        res = df\n        for c in missing:\n            res = res.withColumn(c, lit(None))\n        ctx.debug(\"Schema evolved: added missing columns as null\")\n        return res\n    else:\n        select_exprs = []\n        for c in target_cols:\n            if c in current_cols:\n                select_exprs.append(col(c))\n            else:\n                select_exprs.append(lit(None).alias(c))\n\n        ctx.debug(\"Schema enforced: projected to target schema\")\n        return df.select(*select_exprs)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n    if format != \"delta\" or not config or not config.enabled:\n        return\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    if table:\n        target = table\n    elif path:\n        full_path = connection.get_path(path)\n        target = f\"delta.`{full_path}`\"\n    else:\n        return\n\n    ctx.debug(\"Starting table maintenance\", target=target)\n\n    try:\n        ctx.debug(f\"Running OPTIMIZE on {target}\")\n        self.spark.sql(f\"OPTIMIZE {target}\")\n\n        retention = config.vacuum_retention_hours\n        if retention is not None and retention &gt; 0:\n            ctx.debug(f\"Running VACUUM on {target}\", retention_hours=retention)\n            self.spark.sql(f\"VACUUM {target} RETAIN {retention} HOURS\")\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            target=target,\n            vacuum_retention_hours=retention,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.warning(\n            f\"Auto-optimize failed for {target}\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def profile_nulls(self, df) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\"\"\"\n    from pyspark.sql.functions import col, mean, when\n\n    aggs = []\n    for c in df.columns:\n        aggs.append(mean(when(col(c).isNull(), 1).otherwise(0)).alias(c))\n\n    if not aggs:\n        return {}\n\n    try:\n        result = df.select(*aggs).collect()[0].asDict()\n        return result\n    except Exception:\n        return {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, as_of_version=None, as_of_timestamp=None)</code>","text":"<p>Read data using Spark.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object (with get_path method)</p> required <code>format</code> <code>str</code> <p>Data format (csv, parquet, json, delta, sql_server)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Whether to read as a stream (readStream)</p> <code>False</code> <code>schema</code> <code>Optional[str]</code> <p>Schema string in DDL format (required for streaming file sources)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options (including versionAsOf for Delta time travel)</p> <code>None</code> <code>as_of_version</code> <code>Optional[int]</code> <p>Time travel version</p> <code>None</code> <code>as_of_timestamp</code> <code>Optional[str]</code> <p>Time travel timestamp</p> <code>None</code> <p>Returns:</p> Type Description <p>Spark DataFrame (or Streaming DataFrame)</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    as_of_version: Optional[int] = None,\n    as_of_timestamp: Optional[str] = None,\n):\n    \"\"\"Read data using Spark.\n\n    Args:\n        connection: Connection object (with get_path method)\n        format: Data format (csv, parquet, json, delta, sql_server)\n        table: Table name\n        path: File path\n        streaming: Whether to read as a stream (readStream)\n        schema: Schema string in DDL format (required for streaming file sources)\n        options: Format-specific options (including versionAsOf for Delta time travel)\n        as_of_version: Time travel version\n        as_of_timestamp: Time travel timestamp\n\n    Returns:\n        Spark DataFrame (or Streaming DataFrame)\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n    options = options or {}\n\n    source_identifier = table or path or \"unknown\"\n    ctx.debug(\n        \"Starting Spark read\",\n        format=format,\n        source=source_identifier,\n        streaming=streaming,\n        as_of_version=as_of_version,\n        as_of_timestamp=as_of_timestamp,\n    )\n\n    # Handle Time Travel options\n    if as_of_version is not None:\n        options[\"versionAsOf\"] = as_of_version\n        ctx.debug(f\"Time travel enabled: version {as_of_version}\")\n    if as_of_timestamp is not None:\n        options[\"timestampAsOf\"] = as_of_timestamp\n        ctx.debug(f\"Time travel enabled: timestamp {as_of_timestamp}\")\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if streaming:\n            ctx.error(\"Streaming not supported for SQL Server / Azure SQL\")\n            raise ValueError(\"Streaming not supported for SQL Server / Azure SQL yet.\")\n\n        if not hasattr(connection, \"get_spark_options\"):\n            conn_type = type(connection).__name__\n            msg = f\"Connection type '{conn_type}' does not support Spark SQL read\"\n            ctx.error(msg, connection_type=conn_type)\n            raise ValueError(msg)\n\n        jdbc_options = connection.get_spark_options()\n        merged_options = {**jdbc_options, **options}\n\n        # Extract filter for SQL pushdown\n        sql_filter = merged_options.pop(\"filter\", None)\n\n        if \"query\" in merged_options:\n            merged_options.pop(\"dbtable\", None)\n            # If filter provided with query, append to WHERE clause\n            if sql_filter:\n                existing_query = merged_options[\"query\"]\n                # Wrap existing query and add filter\n                if \"WHERE\" in existing_query.upper():\n                    merged_options[\"query\"] = f\"({existing_query}) AND ({sql_filter})\"\n                else:\n                    subquery = f\"SELECT * FROM ({existing_query}) AS _subq WHERE {sql_filter}\"\n                    merged_options[\"query\"] = subquery\n                ctx.debug(f\"Applied SQL pushdown filter to query: {sql_filter}\")\n        elif table:\n            # Build query with filter pushdown instead of using dbtable\n            if sql_filter:\n                merged_options.pop(\"dbtable\", None)\n                merged_options[\"query\"] = f\"SELECT * FROM {table} WHERE {sql_filter}\"\n                ctx.debug(f\"Applied SQL pushdown filter: {sql_filter}\")\n            else:\n                merged_options[\"dbtable\"] = table\n        elif \"dbtable\" not in merged_options:\n            ctx.error(\"SQL format requires 'table' config or 'query' option\")\n            raise ValueError(\"SQL format requires 'table' config or 'query' option\")\n\n        ctx.debug(\"Executing JDBC read\", has_query=\"query\" in merged_options)\n\n        try:\n            df = self.spark.read.format(\"jdbc\").options(**merged_options).load()\n            elapsed = (time.time() - start_time) * 1000\n            partition_count = df.rdd.getNumPartitions()\n\n            ctx.log_file_io(path=source_identifier, format=format, mode=\"read\")\n            ctx.log_spark_metrics(partition_count=partition_count)\n            ctx.info(\n                \"JDBC read completed\",\n                source=source_identifier,\n                elapsed_ms=round(elapsed, 2),\n                partitions=partition_count,\n            )\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"JDBC read failed\",\n                source=source_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    # Read based on format\n    if table:\n        # Managed/External Table (Catalog)\n        ctx.debug(f\"Reading from catalog table: {table}\")\n\n        if streaming:\n            reader = self.spark.readStream.format(format)\n        else:\n            reader = self.spark.read.format(format)\n\n        for key, value in options.items():\n            reader = reader.option(key, value)\n\n        try:\n            df = reader.table(table)\n\n            if \"filter\" in options:\n                df = df.filter(options[\"filter\"])\n                ctx.debug(f\"Applied filter: {options['filter']}\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            if not streaming:\n                partition_count = df.rdd.getNumPartitions()\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.log_file_io(path=table, format=format, mode=\"read\")\n                ctx.info(\n                    f\"Table read completed: {table}\",\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                )\n            else:\n                ctx.info(f\"Streaming read started: {table}\", elapsed_ms=round(elapsed, 2))\n\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Table read failed: {table}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    elif path:\n        # File Path\n        full_path = connection.get_path(path)\n        ctx.debug(f\"Reading from path: {full_path}\")\n\n        # Auto-detect encoding for CSV (Batch only)\n        if not streaming and format == \"csv\" and options.get(\"auto_encoding\"):\n            options = options.copy()\n            options.pop(\"auto_encoding\")\n\n            if \"encoding\" not in options:\n                try:\n                    from odibi.utils.encoding import detect_encoding\n\n                    detected = detect_encoding(connection, path)\n                    if detected:\n                        options[\"encoding\"] = detected\n                        ctx.debug(f\"Detected encoding: {detected}\", path=path)\n                except ImportError:\n                    pass\n                except Exception as e:\n                    ctx.warning(\n                        f\"Encoding detection failed for {path}\",\n                        error_message=str(e),\n                    )\n\n        if streaming:\n            reader = self.spark.readStream.format(format)\n            if schema:\n                reader = reader.schema(schema)\n                ctx.debug(f\"Applied schema for streaming read: {schema[:100]}...\")\n            else:\n                # Determine if we should warn about missing schema\n                # Formats that can infer schema: delta, parquet, avro (embedded schema)\n                # cloudFiles with schemaLocation or self-describing formats (avro, parquet) are fine\n                should_warn = True\n\n                if format in [\"delta\", \"parquet\"]:\n                    should_warn = False\n                elif format == \"cloudFiles\":\n                    cloud_format = options.get(\"cloudFiles.format\", \"\")\n                    has_schema_location = \"cloudFiles.schemaLocation\" in options\n                    # avro and parquet have embedded schemas\n                    if cloud_format in [\"avro\", \"parquet\"] or has_schema_location:\n                        should_warn = False\n\n                if should_warn:\n                    ctx.warning(\n                        f\"Streaming read from '{format}' format without schema. \"\n                        \"Schema inference is not supported for streaming sources. \"\n                        \"Consider adding 'schema' to your read config.\"\n                    )\n        else:\n            reader = self.spark.read.format(format)\n            if schema:\n                reader = reader.schema(schema)\n\n        for key, value in options.items():\n            if key == \"header\" and isinstance(value, bool):\n                value = str(value).lower()\n            reader = reader.option(key, value)\n\n        try:\n            df = reader.load(full_path)\n\n            if \"filter\" in options:\n                df = df.filter(options[\"filter\"])\n                ctx.debug(f\"Applied filter: {options['filter']}\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            if not streaming:\n                partition_count = df.rdd.getNumPartitions()\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.log_file_io(path=path, format=format, mode=\"read\")\n                ctx.info(\n                    f\"File read completed: {path}\",\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                    format=format,\n                )\n            else:\n                ctx.info(f\"Streaming read started: {path}\", elapsed_ms=round(elapsed, 2))\n\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"File read failed: {path}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n                format=format,\n            )\n            raise\n    else:\n        ctx.error(\"Either path or table must be provided\")\n        raise ValueError(\"Either path or table must be provided\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.restore_delta","title":"<code>restore_delta(connection, path, version)</code>","text":"<p>Restore Delta table to a specific version.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n    \"\"\"Restore Delta table to a specific version.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Restoring Delta table\", path=path, version=version)\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        delta_table.restoreToVersion(version)\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta table restored\",\n            path=path,\n            version=version,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Delta restore failed\",\n            path=path,\n            version=version,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> <p>Handles orphan catalog entries where the table is registered but the underlying Delta path no longer exists.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Handles orphan catalog entries where the table is registered but\n    the underlying Delta path no longer exists.\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    if table:\n        try:\n            if not self.spark.catalog.tableExists(table):\n                ctx.debug(f\"Table does not exist: {table}\")\n                return False\n            # Table exists in catalog - verify it's actually readable\n            # This catches orphan entries where path was deleted\n            self.spark.table(table).limit(0).collect()\n            ctx.debug(f\"Table existence check: {table}\", exists=True)\n            return True\n        except Exception as e:\n            # Table exists in catalog but underlying data is gone (orphan entry)\n            # This is expected during first-run detection - log at debug level\n            ctx.debug(\n                f\"Table {table} exists in catalog but is not accessible (treating as first run)\",\n                error_message=str(e),\n            )\n            return False\n    elif path:\n        try:\n            from delta.tables import DeltaTable\n\n            full_path = connection.get_path(path)\n            exists = DeltaTable.isDeltaTable(self.spark, full_path)\n            ctx.debug(f\"Delta table existence check: {path}\", exists=exists)\n            return exists\n        except ImportError:\n            try:\n                full_path = connection.get_path(path)\n                exists = (\n                    self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(\n                        self.spark.sparkContext._jsc.hadoopConfiguration()\n                    ).exists(\n                        self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path(\n                            full_path\n                        )\n                    )\n                )\n                ctx.debug(f\"Path existence check: {path}\", exists=exists)\n                return exists\n            except Exception as e:\n                ctx.warning(f\"Path existence check failed: {path}\", error_message=str(e))\n                return False\n        except Exception as e:\n            ctx.warning(f\"Table existence check failed: {path}\", error_message=str(e))\n            return False\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n) -&gt; None:\n    \"\"\"VACUUM a Delta table to remove old files.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n    )\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        delta_table.vacuum(retention_hours / 24.0)\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=path,\n            retention_hours=retention_hours,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Delta VACUUM failed\",\n            path=path,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate DataFrame against rules.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def validate_data(self, df, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate DataFrame against rules.\"\"\"\n    from pyspark.sql.functions import col\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    failures = []\n\n    if validation_config.not_empty:\n        if df.isEmpty():\n            failures.append(\"DataFrame is empty\")\n\n    if validation_config.no_nulls:\n        null_counts = self.count_nulls(df, validation_config.no_nulls)\n        for col_name, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col_name}' has {count} null values\")\n\n    if validation_config.schema_validation:\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    if validation_config.ranges:\n        for col_name, bounds in validation_config.ranges.items():\n            if col_name in df.columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                if min_val is not None:\n                    count = df.filter(col(col_name) &lt; min_val).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    count = df.filter(col(col_name) &gt; max_val).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col_name}' not found for range validation\")\n\n    if validation_config.allowed_values:\n        for col_name, allowed in validation_config.allowed_values.items():\n            if col_name in df.columns:\n                count = df.filter(~col(col_name).isin(allowed)).count()\n                if count &gt; 0:\n                    failures.append(f\"Column '{col_name}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col_name}' not found for allowed values validation\")\n\n    ctx.log_validation_result(\n        passed=len(failures) == 0,\n        rule_name=\"data_validation\",\n        failures=failures if failures else None,\n    )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def validate_schema(self, df, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\"\"\"\n    failures = []\n\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(df.columns)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    if \"types\" in schema_rules:\n        type_map = {\n            \"int\": [\"integer\", \"long\", \"short\", \"byte\", \"bigint\"],\n            \"float\": [\"double\", \"float\"],\n            \"str\": [\"string\"],\n            \"bool\": [\"boolean\"],\n        }\n\n        for col_name, expected_type in schema_rules[\"types\"].items():\n            if col_name not in df.columns:\n                failures.append(f\"Column '{col_name}' not found for type validation\")\n                continue\n\n            actual_type = dict(df.dtypes)[col_name]\n            expected_dtypes = type_map.get(expected_type, [expected_type])\n\n            if actual_type not in expected_dtypes:\n                failures.append(\n                    f\"Column '{col_name}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.write","title":"<code>write(df, connection, format, table=None, path=None, register_table=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Spark.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Spark DataFrame to write</p> required <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Output format (csv, parquet, json, delta)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>register_table</code> <code>Optional[str]</code> <p>Name to register as external table (if path is used)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Write mode (overwrite, append, error, ignore, upsert, append_once)</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options (including partition_by for partitioning)</p> <code>None</code> <code>streaming_config</code> <code>Optional[Any]</code> <p>StreamingWriteConfig for streaming DataFrames</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary containing Delta commit metadata (if format=delta),</p> <code>Optional[Dict[str, Any]]</code> <p>or streaming query info (if streaming)</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def write(\n    self,\n    df,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    register_table: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Spark.\n\n    Args:\n        df: Spark DataFrame to write\n        connection: Connection object\n        format: Output format (csv, parquet, json, delta)\n        table: Table name\n        path: File path\n        register_table: Name to register as external table (if path is used)\n        mode: Write mode (overwrite, append, error, ignore, upsert, append_once)\n        options: Format-specific options (including partition_by for partitioning)\n        streaming_config: StreamingWriteConfig for streaming DataFrames\n\n    Returns:\n        Optional dictionary containing Delta commit metadata (if format=delta),\n        or streaming query info (if streaming)\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n    options = options or {}\n\n    if getattr(df, \"isStreaming\", False) is True:\n        return self._write_streaming(\n            df=df,\n            connection=connection,\n            format=format,\n            table=table,\n            path=path,\n            register_table=register_table,\n            options=options,\n            streaming_config=streaming_config,\n        )\n\n    target_identifier = table or path or \"unknown\"\n    try:\n        partition_count = df.rdd.getNumPartitions()\n    except Exception:\n        partition_count = 1  # Fallback for mocks or unsupported DataFrames\n\n    # Auto-coalesce DataFrames for Delta writes to reduce file overhead\n    # Use coalesce_partitions option to explicitly set target partitions\n    # NOTE: We avoid df.count() here as it would trigger double-evaluation of lazy DataFrames\n    coalesce_partitions = options.pop(\"coalesce_partitions\", None)\n    if (\n        coalesce_partitions\n        and isinstance(partition_count, int)\n        and partition_count &gt; coalesce_partitions\n    ):\n        df = df.coalesce(coalesce_partitions)\n        ctx.debug(\n            f\"Coalesced DataFrame to {coalesce_partitions} partition(s)\",\n            original_partitions=partition_count,\n        )\n        partition_count = coalesce_partitions\n\n    ctx.debug(\n        \"Starting Spark write\",\n        format=format,\n        target=target_identifier,\n        mode=mode,\n        partitions=partition_count,\n    )\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if not hasattr(connection, \"get_spark_options\"):\n            conn_type = type(connection).__name__\n            msg = f\"Connection type '{conn_type}' does not support Spark SQL write\"\n            ctx.error(msg, connection_type=conn_type)\n            raise ValueError(msg)\n\n        jdbc_options = connection.get_spark_options()\n        merged_options = {**jdbc_options, **options}\n\n        if table:\n            merged_options[\"dbtable\"] = table\n        elif \"dbtable\" not in merged_options:\n            ctx.error(\"SQL format requires 'table' config or 'dbtable' option\")\n            raise ValueError(\"SQL format requires 'table' config or 'dbtable' option\")\n\n        if mode not in [\"overwrite\", \"append\", \"ignore\", \"error\"]:\n            if mode == \"fail\":\n                mode = \"error\"\n            else:\n                ctx.error(f\"Write mode '{mode}' not supported for Spark SQL write\")\n                raise ValueError(f\"Write mode '{mode}' not supported for Spark SQL write\")\n\n        ctx.debug(\"Executing JDBC write\", target=table or merged_options.get(\"dbtable\"))\n\n        try:\n            df.write.format(\"jdbc\").options(**merged_options).mode(mode).save()\n            elapsed = (time.time() - start_time) * 1000\n            ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n            ctx.info(\n                \"JDBC write completed\",\n                target=target_identifier,\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n            return None\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"JDBC write failed\",\n                target=target_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    # Handle Upsert/AppendOnce (Delta Only)\n    if mode in [\"upsert\", \"append_once\"]:\n        if format != \"delta\":\n            ctx.error(f\"Mode '{mode}' only supported for Delta format\")\n            raise NotImplementedError(\n                f\"Mode '{mode}' only supported for Delta format in Spark engine.\"\n            )\n\n        keys = options.get(\"keys\")\n        if not keys:\n            ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        exists = self.table_exists(connection, table, path)\n        ctx.debug(\"Table existence check for merge\", target=target_identifier, exists=exists)\n\n        if not exists:\n            mode = \"overwrite\"\n            ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n        else:\n            from delta.tables import DeltaTable\n\n            target_dt = None\n            target_name = \"\"\n            is_table_target = False\n\n            if table:\n                target_dt = DeltaTable.forName(self.spark, table)\n                target_name = table\n                is_table_target = True\n            elif path:\n                full_path = connection.get_path(path)\n                target_dt = DeltaTable.forPath(self.spark, full_path)\n                target_name = full_path\n                is_table_target = False\n\n            condition = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in keys])\n            ctx.debug(\"Executing Delta merge\", merge_mode=mode, keys=keys, condition=condition)\n\n            merge_builder = target_dt.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n            try:\n                if mode == \"upsert\":\n                    merge_builder.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                elif mode == \"append_once\":\n                    merge_builder.whenNotMatchedInsertAll().execute()\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Delta merge completed\",\n                    target=target_name,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                self._optimize_delta_write(target_name, options, is_table=is_table_target)\n                commit_info = self._get_last_delta_commit_info(\n                    target_name, is_table=is_table_target\n                )\n\n                if commit_info:\n                    ctx.debug(\n                        \"Delta commit info\",\n                        version=commit_info.get(\"version\"),\n                        operation=commit_info.get(\"operation\"),\n                    )\n\n                return commit_info\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Delta merge failed\",\n                    target=target_name,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n    # Get output location\n    if table:\n        # Managed/External Table (Catalog)\n        ctx.debug(f\"Writing to catalog table: {table}\")\n        writer = df.write.format(format).mode(mode)\n\n        partition_by = options.get(\"partition_by\")\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            writer.saveAsTable(table)\n            elapsed = (time.time() - start_time) * 1000\n\n            ctx.log_file_io(\n                path=table,\n                format=format,\n                mode=mode,\n                partitions=partition_by,\n            )\n            ctx.info(\n                f\"Table write completed: {table}\",\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n            if format == \"delta\":\n                self._optimize_delta_write(table, options, is_table=True)\n                return self._get_last_delta_commit_info(table, is_table=True)\n            return None\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Table write failed: {table}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    elif path:\n        full_path = connection.get_path(path)\n    else:\n        ctx.error(\"Either path or table must be provided\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Extract partition_by option\n    partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n\n    # Extract cluster_by option (Liquid Clustering)\n    cluster_by = options.pop(\"cluster_by\", None)\n\n    # Warn about partitioning anti-patterns\n    if partition_by and cluster_by:\n        import warnings\n\n        ctx.warning(\n            \"Conflict: Both 'partition_by' and 'cluster_by' are set\",\n            partition_by=partition_by,\n            cluster_by=cluster_by,\n        )\n        warnings.warn(\n            \"\u26a0\ufe0f  Conflict: Both 'partition_by' and 'cluster_by' (Liquid Clustering) are set. \"\n            \"Liquid Clustering supersedes partitioning. 'partition_by' will be ignored \"\n            \"if the table is being created now.\",\n            UserWarning,\n        )\n\n    elif partition_by:\n        import warnings\n\n        ctx.warning(\n            \"Partitioning warning: ensure low-cardinality columns\",\n            partition_by=partition_by,\n        )\n        warnings.warn(\n            \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n            \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n            \"and ensure each partition has &gt; 1000 rows.\",\n            UserWarning,\n        )\n\n    # Handle Upsert/Append-Once for Delta Lake (Path-based only for now)\n    if format == \"delta\" and mode in [\"upsert\", \"append_once\"]:\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\"Delta Lake support requires 'delta-spark'\")\n\n        if \"keys\" not in options:\n            ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        if DeltaTable.isDeltaTable(self.spark, full_path):\n            ctx.debug(f\"Performing Delta merge at path: {full_path}\")\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            keys = options[\"keys\"]\n            if isinstance(keys, str):\n                keys = [keys]\n\n            condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in keys])\n            merger = delta_table.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n            try:\n                if mode == \"upsert\":\n                    merger.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                else:\n                    merger.whenNotMatchedInsertAll().execute()\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Delta merge completed at path\",\n                    path=path,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table:\n                    try:\n                        table_in_catalog = self.spark.catalog.tableExists(register_table)\n                        needs_registration = not table_in_catalog\n\n                        # Handle orphan catalog entries (only for path-not-found errors)\n                        if table_in_catalog:\n                            try:\n                                self.spark.table(register_table).limit(0).collect()\n                                ctx.debug(\n                                    f\"Table '{register_table}' already registered and valid\"\n                                )\n                            except Exception as verify_err:\n                                error_str = str(verify_err)\n                                is_orphan = (\n                                    \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                                    or \"Path does not exist\" in error_str\n                                    or \"FileNotFoundException\" in error_str\n                                )\n                                if is_orphan:\n                                    ctx.warning(\n                                        f\"Table '{register_table}' is orphan, re-registering\"\n                                    )\n                                    try:\n                                        self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                                    except Exception:\n                                        pass\n                                    needs_registration = True\n                                else:\n                                    ctx.debug(\n                                        f\"Table '{register_table}' verify failed, \"\n                                        \"skipping registration\"\n                                    )\n\n                        if needs_registration:\n                            create_sql = (\n                                f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                f\"USING DELTA LOCATION '{full_path}'\"\n                            )\n                            self.spark.sql(create_sql)\n                            ctx.info(f\"Registered table: {register_table}\", path=full_path)\n                    except Exception as e:\n                        ctx.error(\n                            f\"Failed to register external table '{register_table}'\",\n                            error_message=str(e),\n                        )\n\n                self._optimize_delta_write(full_path, options, is_table=False)\n                return self._get_last_delta_commit_info(full_path, is_table=False)\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Delta merge failed at path\",\n                    path=path,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n        else:\n            mode = \"overwrite\"\n            ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n\n    # Write based on format (Path-based)\n    ctx.debug(f\"Writing to path: {full_path}\")\n\n    # Handle Liquid Clustering (New Table Creation via SQL)\n    if format == \"delta\" and cluster_by:\n        should_create = False\n        target_name = None\n\n        if table:\n            target_name = table\n            if mode == \"overwrite\":\n                should_create = True\n            elif mode == \"append\":\n                if not self.spark.catalog.tableExists(table):\n                    should_create = True\n        elif path:\n            full_path = connection.get_path(path)\n            target_name = f\"delta.`{full_path}`\"\n            if mode == \"overwrite\":\n                should_create = True\n            elif mode == \"append\":\n                try:\n                    from delta.tables import DeltaTable\n\n                    if not DeltaTable.isDeltaTable(self.spark, full_path):\n                        should_create = True\n                except ImportError:\n                    pass\n\n        if should_create:\n            if isinstance(cluster_by, str):\n                cluster_by = [cluster_by]\n\n            cols = \", \".join(cluster_by)\n            temp_view = f\"odibi_temp_writer_{abs(hash(str(target_name)))}\"\n            df.createOrReplaceTempView(temp_view)\n\n            create_cmd = (\n                \"CREATE OR REPLACE TABLE\"\n                if mode == \"overwrite\"\n                else \"CREATE TABLE IF NOT EXISTS\"\n            )\n\n            sql = (\n                f\"{create_cmd} {target_name} USING DELTA CLUSTER BY ({cols}) \"\n                f\"AS SELECT * FROM {temp_view}\"\n            )\n\n            ctx.debug(\"Creating clustered Delta table\", sql=sql, cluster_by=cluster_by)\n\n            try:\n                self.spark.sql(sql)\n                self.spark.catalog.dropTempView(temp_view)\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Clustered Delta table created\",\n                    target=target_name,\n                    cluster_by=cluster_by,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table and path:\n                    try:\n                        reg_sql = (\n                            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                            f\"USING DELTA LOCATION '{full_path}'\"\n                        )\n                        self.spark.sql(reg_sql)\n                        ctx.info(f\"Registered table: {register_table}\")\n                    except Exception:\n                        pass\n\n                if format == \"delta\":\n                    self._optimize_delta_write(\n                        target_name if table else full_path, options, is_table=bool(table)\n                    )\n                    return self._get_last_delta_commit_info(\n                        target_name if table else full_path, is_table=bool(table)\n                    )\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Failed to create clustered Delta table\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n    # Extract table_properties from options\n    table_properties = options.pop(\"table_properties\", None)\n\n    # For column mapping and other properties that must be set BEFORE write\n    original_configs = {}\n    if table_properties and format == \"delta\":\n        for prop_name, prop_value in table_properties.items():\n            spark_conf_key = (\n                f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n            )\n            try:\n                original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n            except Exception:\n                original_configs[spark_conf_key] = None\n            self.spark.conf.set(spark_conf_key, prop_value)\n        ctx.debug(\n            \"Applied table properties as session defaults\",\n            properties=list(table_properties.keys()),\n        )\n\n    writer = df.write.format(format).mode(mode)\n\n    if partition_by:\n        if isinstance(partition_by, str):\n            partition_by = [partition_by]\n        writer = writer.partitionBy(*partition_by)\n        ctx.debug(f\"Partitioning by: {partition_by}\")\n\n    for key, value in options.items():\n        writer = writer.option(key, value)\n\n    try:\n        writer.save(full_path)\n        elapsed = (time.time() - start_time) * 1000\n\n        ctx.log_file_io(\n            path=path,\n            format=format,\n            mode=mode,\n            partitions=partition_by,\n        )\n        ctx.info(\n            f\"File write completed: {path}\",\n            format=format,\n            mode=mode,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            f\"File write failed: {path}\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n    finally:\n        for conf_key, original_value in original_configs.items():\n            if original_value is None:\n                self.spark.conf.unset(conf_key)\n            else:\n                self.spark.conf.set(conf_key, original_value)\n\n    if format == \"delta\":\n        self._optimize_delta_write(full_path, options, is_table=False)\n\n    if register_table and format == \"delta\":\n        try:\n            table_in_catalog = self.spark.catalog.tableExists(register_table)\n            needs_registration = not table_in_catalog\n\n            # Handle orphan catalog entries: table exists but points to deleted path\n            # Only treat as orphan if it's specifically a DELTA_PATH_DOES_NOT_EXIST error\n            if table_in_catalog:\n                try:\n                    self.spark.table(register_table).limit(0).collect()\n                    ctx.debug(\n                        f\"Table '{register_table}' already registered and valid, \"\n                        \"skipping registration\"\n                    )\n                except Exception as verify_err:\n                    error_str = str(verify_err)\n                    is_orphan = (\n                        \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                        or \"Path does not exist\" in error_str\n                        or \"FileNotFoundException\" in error_str\n                    )\n\n                    if is_orphan:\n                        # Orphan entry - table in catalog but path was deleted\n                        ctx.warning(\n                            f\"Table '{register_table}' is orphan (path deleted), \"\n                            \"dropping and re-registering\",\n                            error_message=error_str[:200],\n                        )\n                        try:\n                            self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                        except Exception:\n                            pass  # Best effort cleanup\n                        needs_registration = True\n                    else:\n                        # Other error (auth, network, etc.) - don't drop, just log\n                        ctx.debug(\n                            f\"Table '{register_table}' exists but verify failed \"\n                            \"(not orphan), skipping registration\",\n                            error_message=error_str[:200],\n                        )\n\n            if needs_registration:\n                ctx.debug(f\"Registering table '{register_table}' at '{full_path}'\")\n                reg_sql = (\n                    f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                    f\"USING DELTA LOCATION '{full_path}'\"\n                )\n                self.spark.sql(reg_sql)\n                ctx.info(f\"Registered table: {register_table}\", path=full_path)\n        except Exception as e:\n            ctx.error(\n                f\"Failed to register table '{register_table}'\",\n                error_message=str(e),\n            )\n            raise RuntimeError(\n                f\"Failed to register external table '{register_table}': {e}\"\n            ) from e\n\n    if format == \"delta\":\n        return self._get_last_delta_commit_info(full_path, is_table=False)\n\n    return None\n</code></pre>"},{"location":"reference/api/pipeline/","title":"Pipeline API","text":""},{"location":"reference/api/pipeline/#odibi.pipeline","title":"<code>odibi.pipeline</code>","text":"<p>Pipeline executor and orchestration.</p>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager","title":"<code>PipelineManager</code>","text":"<p>Manages multiple pipelines from a YAML configuration.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>class PipelineManager:\n    \"\"\"Manages multiple pipelines from a YAML configuration.\"\"\"\n\n    def __init__(\n        self,\n        project_config: ProjectConfig,\n        connections: Dict[str, Any],\n    ):\n        \"\"\"Initialize pipeline manager.\n\n        Args:\n            project_config: Validated project configuration\n            connections: Connection objects (already instantiated)\n        \"\"\"\n        self.project_config = project_config\n        self.connections = connections\n        self._pipelines: Dict[str, Pipeline] = {}\n        self.catalog_manager = None\n        self.lineage_adapter = None\n\n        # Configure logging\n        configure_logging(\n            structured=project_config.logging.structured, level=project_config.logging.level.value\n        )\n\n        # Create manager-level logging context\n        self._ctx = create_logging_context(engine=project_config.engine)\n\n        self._ctx.info(\n            \"Initializing PipelineManager\",\n            project=project_config.project,\n            engine=project_config.engine,\n            pipeline_count=len(project_config.pipelines),\n            connection_count=len(connections),\n        )\n\n        # Initialize Lineage Adapter\n        self.lineage_adapter = OpenLineageAdapter(project_config.lineage)\n\n        # Initialize CatalogManager if configured\n        if project_config.system:\n            from odibi.catalog import CatalogManager\n\n            spark = None\n            engine_instance = None\n\n            if project_config.engine == \"spark\":\n                try:\n                    from odibi.engine.spark_engine import SparkEngine\n\n                    temp_engine = SparkEngine(connections=connections, config={})\n                    spark = temp_engine.spark\n                    self._ctx.debug(\"Spark session initialized for System Catalog\")\n                except Exception as e:\n                    self._ctx.warning(\n                        f\"Failed to initialize Spark for System Catalog: {e}\",\n                        suggestion=\"Check Spark configuration\",\n                    )\n\n            sys_conn = connections.get(project_config.system.connection)\n            if sys_conn:\n                base_path = sys_conn.get_path(project_config.system.path)\n\n                if not spark:\n                    try:\n                        from odibi.engine.pandas_engine import PandasEngine\n\n                        engine_instance = PandasEngine(config={})\n                        self._ctx.debug(\"PandasEngine initialized for System Catalog\")\n                    except Exception as e:\n                        self._ctx.warning(\n                            f\"Failed to initialize PandasEngine for System Catalog: {e}\"\n                        )\n\n                if spark or engine_instance:\n                    self.catalog_manager = CatalogManager(\n                        spark=spark,\n                        config=project_config.system,\n                        base_path=base_path,\n                        engine=engine_instance,\n                        connection=sys_conn,\n                    )\n                    self.catalog_manager.bootstrap()\n                    self._ctx.info(\"System Catalog initialized\", path=base_path)\n            else:\n                self._ctx.warning(\n                    f\"System connection '{project_config.system.connection}' not found\",\n                    suggestion=\"Configure the system connection in your config\",\n                )\n\n        # Get story configuration\n        story_config = self._get_story_config()\n\n        # Create all pipeline instances\n        self._ctx.debug(\n            \"Creating pipeline instances\",\n            pipelines=[p.pipeline for p in project_config.pipelines],\n        )\n        for pipeline_config in project_config.pipelines:\n            pipeline_name = pipeline_config.pipeline\n\n            self._pipelines[pipeline_name] = Pipeline(\n                pipeline_config=pipeline_config,\n                engine=project_config.engine,\n                connections=connections,\n                generate_story=story_config.get(\"auto_generate\", True),\n                story_config=story_config,\n                retry_config=project_config.retry,\n                alerts=project_config.alerts,\n                performance_config=project_config.performance,\n                catalog_manager=self.catalog_manager,\n                lineage_adapter=self.lineage_adapter,\n            )\n            self._pipelines[pipeline_name].project_config = project_config\n\n        self._ctx.info(\n            \"PipelineManager ready\",\n            pipelines=list(self._pipelines.keys()),\n        )\n\n    def _get_story_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Build story config from project_config.story.\n\n        Resolves story output path using connection.\n\n        Returns:\n            Dictionary for StoryGenerator initialization\n        \"\"\"\n        story_cfg = self.project_config.story\n\n        # Resolve story path using connection\n        story_conn = self.connections[story_cfg.connection]\n        output_path = story_conn.get_path(story_cfg.path)\n\n        # Get storage options (e.g., credentials) from connection if available\n        storage_options = {}\n        if hasattr(story_conn, \"pandas_storage_options\"):\n            storage_options = story_conn.pandas_storage_options()\n\n        return {\n            \"auto_generate\": story_cfg.auto_generate,\n            \"max_sample_rows\": story_cfg.max_sample_rows,\n            \"output_path\": output_path,\n            \"storage_options\": storage_options,\n            \"async_generation\": story_cfg.async_generation,\n        }\n\n    @classmethod\n    def from_yaml(cls, yaml_path: str, env: str = None) -&gt; \"PipelineManager\":\n        \"\"\"Create PipelineManager from YAML file.\n\n        Args:\n            yaml_path: Path to YAML configuration file\n            env: Environment name to apply overrides (e.g. 'prod')\n\n        Returns:\n            PipelineManager instance ready to run pipelines\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n            &gt;&gt;&gt; results = manager.run()  # Run all pipelines\n        \"\"\"\n        logger.info(f\"Loading configuration from: {yaml_path}\")\n\n        register_standard_library()\n\n        yaml_path_obj = Path(yaml_path)\n        config_dir = yaml_path_obj.parent.absolute()\n\n        import importlib.util\n        import os\n        import sys\n\n        def load_transforms_module(path):\n            if os.path.exists(path):\n                try:\n                    spec = importlib.util.spec_from_file_location(\"transforms_autodiscovered\", path)\n                    if spec and spec.loader:\n                        module = importlib.util.module_from_spec(spec)\n                        sys.modules[\"transforms_autodiscovered\"] = module\n                        spec.loader.exec_module(module)\n                        logger.info(f\"Auto-loaded transforms from: {path}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to auto-load transforms from {path}: {e}\")\n\n        load_transforms_module(os.path.join(config_dir, \"transforms.py\"))\n\n        cwd = os.getcwd()\n        if os.path.abspath(cwd) != str(config_dir):\n            load_transforms_module(os.path.join(cwd, \"transforms.py\"))\n\n        try:\n            config = load_yaml_with_env(str(yaml_path_obj), env=env)\n            logger.debug(\"Configuration loaded successfully\")\n        except FileNotFoundError:\n            logger.error(f\"YAML file not found: {yaml_path}\")\n            raise FileNotFoundError(f\"YAML file not found: {yaml_path}\")\n\n        project_config = ProjectConfig(**config)\n        logger.debug(\n            \"Project config validated\",\n            project=project_config.project,\n            pipelines=len(project_config.pipelines),\n        )\n\n        connections = cls._build_connections(project_config.connections)\n\n        return cls(\n            project_config=project_config,\n            connections=connections,\n        )\n\n    @staticmethod\n    def _build_connections(conn_configs: Dict[str, Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Convert connection configs to connection objects.\n\n        Args:\n            conn_configs: Connection configurations from ProjectConfig\n\n        Returns:\n            Dictionary of connection name -&gt; connection object\n\n        Raises:\n            ValueError: If connection type is not supported\n        \"\"\"\n        from odibi.connections.factory import register_builtins\n\n        logger.debug(f\"Building {len(conn_configs)} connections\")\n\n        connections = {}\n\n        register_builtins()\n        load_plugins()\n\n        for conn_name, conn_config in conn_configs.items():\n            if hasattr(conn_config, \"model_dump\"):\n                conn_config = conn_config.model_dump()\n            elif hasattr(conn_config, \"dict\"):\n                conn_config = conn_config.dict()\n\n            conn_type = conn_config.get(\"type\", \"local\")\n\n            factory = get_connection_factory(conn_type)\n            if factory:\n                try:\n                    connections[conn_name] = factory(conn_name, conn_config)\n                    logger.debug(\n                        f\"Connection created: {conn_name}\",\n                        type=conn_type,\n                    )\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to create connection '{conn_name}'\",\n                        type=conn_type,\n                        error=str(e),\n                    )\n                    raise ValueError(\n                        f\"Failed to create connection '{conn_name}' (type={conn_type}): {e}\"\n                    ) from e\n            else:\n                logger.error(\n                    f\"Unsupported connection type: {conn_type}\",\n                    connection=conn_name,\n                    suggestion=\"Check supported connection types in docs\",\n                )\n                raise ValueError(\n                    f\"Unsupported connection type: {conn_type}. \"\n                    f\"Supported types: local, azure_adls, azure_sql, delta, etc. \"\n                    f\"See docs for connection setup.\"\n                )\n\n        try:\n            from odibi.utils import configure_connections_parallel\n\n            connections, errors = configure_connections_parallel(connections, verbose=False)\n            if errors:\n                for error in errors:\n                    logger.warning(error)\n        except ImportError:\n            pass\n\n        logger.info(f\"Built {len(connections)} connections successfully\")\n\n        return connections\n\n    def run(\n        self,\n        pipelines: Optional[Union[str, List[str]]] = None,\n        dry_run: bool = False,\n        resume_from_failure: bool = False,\n        parallel: bool = False,\n        max_workers: int = 4,\n        on_error: Optional[str] = None,\n        tag: Optional[str] = None,\n        node: Optional[str] = None,\n        console: bool = False,\n    ) -&gt; Union[PipelineResults, Dict[str, PipelineResults]]:\n        \"\"\"Run one, multiple, or all pipelines.\n\n        Args:\n            pipelines: Pipeline name(s) to run.\n            dry_run: Whether to simulate execution.\n            resume_from_failure: Whether to skip successfully completed nodes from last run.\n            parallel: Whether to run nodes in parallel.\n            max_workers: Maximum number of worker threads for parallel execution.\n            on_error: Override error handling strategy (fail_fast, fail_later, ignore).\n            tag: Filter nodes by tag (only nodes with this tag will run).\n            node: Run only the specific node by name.\n            console: Whether to show rich console output with progress.\n\n        Returns:\n            PipelineResults or Dict of results\n        \"\"\"\n        if pipelines is None:\n            pipeline_names = list(self._pipelines.keys())\n        elif isinstance(pipelines, str):\n            pipeline_names = [pipelines]\n        else:\n            pipeline_names = pipelines\n\n        for name in pipeline_names:\n            if name not in self._pipelines:\n                available = \", \".join(self._pipelines.keys())\n                self._ctx.error(\n                    f\"Pipeline not found: {name}\",\n                    available=list(self._pipelines.keys()),\n                )\n                raise ValueError(f\"Pipeline '{name}' not found. Available pipelines: {available}\")\n\n        # Phase 2: Auto-register pipelines and nodes before execution\n        if self.catalog_manager:\n            self._auto_register_pipelines(pipeline_names)\n\n        self._ctx.info(\n            f\"Running {len(pipeline_names)} pipeline(s)\",\n            pipelines=pipeline_names,\n            dry_run=dry_run,\n            parallel=parallel,\n        )\n\n        results = {}\n        for idx, name in enumerate(pipeline_names):\n            # Invalidate cache before each pipeline so it sees latest outputs\n            if self.catalog_manager:\n                self.catalog_manager.invalidate_cache()\n\n            self._ctx.info(\n                f\"Executing pipeline {idx + 1}/{len(pipeline_names)}: {name}\",\n                pipeline=name,\n                order=idx + 1,\n            )\n\n            results[name] = self._pipelines[name].run(\n                dry_run=dry_run,\n                resume_from_failure=resume_from_failure,\n                parallel=parallel,\n                max_workers=max_workers,\n                on_error=on_error,\n                tag=tag,\n                node=node,\n                console=console,\n            )\n\n            result = results[name]\n            status = \"SUCCESS\" if not result.failed else \"FAILED\"\n            self._ctx.info(\n                f\"Pipeline {status}: {name}\",\n                status=status,\n                duration_s=round(result.duration, 2),\n                completed=len(result.completed),\n                failed=len(result.failed),\n            )\n\n            if result.story_path:\n                self._ctx.debug(f\"Story generated: {result.story_path}\")\n\n        if len(pipeline_names) == 1:\n            return results[pipeline_names[0]]\n        else:\n            return results\n\n    def list_pipelines(self) -&gt; List[str]:\n        \"\"\"Get list of available pipeline names.\n\n        Returns:\n            List of pipeline names\n        \"\"\"\n        return list(self._pipelines.keys())\n\n    def get_pipeline(self, name: str) -&gt; Pipeline:\n        \"\"\"Get a specific pipeline instance.\n\n        Args:\n            name: Pipeline name\n\n        Returns:\n            Pipeline instance\n\n        Raises:\n            ValueError: If pipeline not found\n        \"\"\"\n        if name not in self._pipelines:\n            available = \", \".join(self._pipelines.keys())\n            raise ValueError(f\"Pipeline '{name}' not found. Available: {available}\")\n        return self._pipelines[name]\n\n    def deploy(self, pipelines: Optional[Union[str, List[str]]] = None) -&gt; bool:\n        \"\"\"Deploy pipeline definitions to the System Catalog.\n\n        This registers pipeline and node configurations in the catalog,\n        enabling drift detection and governance features.\n\n        Args:\n            pipelines: Optional pipeline name(s) to deploy. If None, deploys all.\n\n        Returns:\n            True if deployment succeeded, False otherwise.\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"odibi.yaml\")\n            &gt;&gt;&gt; manager.deploy()  # Deploy all pipelines\n            &gt;&gt;&gt; manager.deploy(\"sales_daily\")  # Deploy specific pipeline\n        \"\"\"\n        if not self.catalog_manager:\n            self._ctx.warning(\n                \"System Catalog not configured. Cannot deploy.\",\n                suggestion=\"Configure system catalog in your YAML config\",\n            )\n            return False\n\n        if pipelines is None:\n            to_deploy = self.project_config.pipelines\n        elif isinstance(pipelines, str):\n            to_deploy = [p for p in self.project_config.pipelines if p.pipeline == pipelines]\n        else:\n            to_deploy = [p for p in self.project_config.pipelines if p.pipeline in pipelines]\n\n        if not to_deploy:\n            self._ctx.warning(\"No matching pipelines found to deploy.\")\n            return False\n\n        self._ctx.info(\n            f\"Deploying {len(to_deploy)} pipeline(s) to System Catalog\",\n            pipelines=[p.pipeline for p in to_deploy],\n        )\n\n        try:\n            self.catalog_manager.bootstrap()\n\n            for pipeline_config in to_deploy:\n                self._ctx.debug(\n                    f\"Deploying pipeline: {pipeline_config.pipeline}\",\n                    node_count=len(pipeline_config.nodes),\n                )\n                self.catalog_manager.register_pipeline(pipeline_config, self.project_config)\n\n                for node in pipeline_config.nodes:\n                    self.catalog_manager.register_node(pipeline_config.pipeline, node)\n\n            self._ctx.info(\n                f\"Deployment complete: {len(to_deploy)} pipeline(s)\",\n                deployed=[p.pipeline for p in to_deploy],\n            )\n            return True\n\n        except Exception as e:\n            self._ctx.error(\n                f\"Deployment failed: {e}\",\n                error_type=type(e).__name__,\n                suggestion=\"Check catalog configuration and permissions\",\n            )\n            return False\n\n    def _auto_register_pipelines(self, pipeline_names: List[str]) -&gt; None:\n        \"\"\"Auto-register pipelines and nodes before execution.\n\n        This ensures meta_pipelines and meta_nodes are populated automatically\n        when running pipelines, without requiring explicit deploy() calls.\n\n        Uses \"check-before-write\" pattern with batch writes for performance:\n        - Reads existing hashes in one read\n        - Compares version_hash to skip unchanged records\n        - Batch writes only changed/new records\n\n        Args:\n            pipeline_names: List of pipeline names to register\n        \"\"\"\n        if not self.catalog_manager:\n            return\n\n        try:\n            import hashlib\n            import json\n\n            existing_pipelines = self.catalog_manager.get_all_registered_pipelines()\n            existing_nodes = self.catalog_manager.get_all_registered_nodes(pipeline_names)\n\n            pipeline_records = []\n            node_records = []\n\n            for name in pipeline_names:\n                pipeline = self._pipelines[name]\n                config = pipeline.config\n\n                if hasattr(config, \"model_dump\"):\n                    dump = config.model_dump(mode=\"json\")\n                else:\n                    dump = config.dict()\n                dump_str = json.dumps(dump, sort_keys=True)\n                pipeline_hash = hashlib.md5(dump_str.encode(\"utf-8\")).hexdigest()\n\n                if existing_pipelines.get(name) != pipeline_hash:\n                    all_tags = set()\n                    for node in config.nodes:\n                        if node.tags:\n                            all_tags.update(node.tags)\n\n                    pipeline_records.append(\n                        {\n                            \"pipeline_name\": name,\n                            \"version_hash\": pipeline_hash,\n                            \"description\": config.description or \"\",\n                            \"layer\": config.layer or \"\",\n                            \"schedule\": \"\",\n                            \"tags_json\": json.dumps(list(all_tags)),\n                        }\n                    )\n\n                pipeline_existing_nodes = existing_nodes.get(name, {})\n                for node in config.nodes:\n                    if hasattr(node, \"model_dump\"):\n                        node_dump = node.model_dump(\n                            mode=\"json\", exclude={\"description\", \"tags\", \"log_level\"}\n                        )\n                    else:\n                        node_dump = node.dict(exclude={\"description\", \"tags\", \"log_level\"})\n                    node_dump_str = json.dumps(node_dump, sort_keys=True)\n                    node_hash = hashlib.md5(node_dump_str.encode(\"utf-8\")).hexdigest()\n\n                    if pipeline_existing_nodes.get(node.name) != node_hash:\n                        node_type = \"transform\"\n                        if node.read:\n                            node_type = \"read\"\n                        if node.write:\n                            node_type = \"write\"\n\n                        node_records.append(\n                            {\n                                \"pipeline_name\": name,\n                                \"node_name\": node.name,\n                                \"version_hash\": node_hash,\n                                \"type\": node_type,\n                                \"config_json\": json.dumps(node_dump),\n                            }\n                        )\n\n            if pipeline_records:\n                self.catalog_manager.register_pipelines_batch(pipeline_records)\n                self._ctx.debug(\n                    f\"Batch registered {len(pipeline_records)} changed pipeline(s)\",\n                    pipelines=[r[\"pipeline_name\"] for r in pipeline_records],\n                )\n            else:\n                self._ctx.debug(\"All pipelines unchanged - skipping registration\")\n\n            if node_records:\n                self.catalog_manager.register_nodes_batch(node_records)\n                self._ctx.debug(\n                    f\"Batch registered {len(node_records)} changed node(s)\",\n                    nodes=[r[\"node_name\"] for r in node_records],\n                )\n            else:\n                self._ctx.debug(\"All nodes unchanged - skipping registration\")\n\n        except Exception as e:\n            self._ctx.warning(\n                f\"Auto-registration failed (non-fatal): {e}\",\n                error_type=type(e).__name__,\n            )\n\n    # -------------------------------------------------------------------------\n    # Phase 5: List/Query Methods\n    # -------------------------------------------------------------------------\n\n    def list_registered_pipelines(self) -&gt; \"pd.DataFrame\":\n        \"\"\"List all registered pipelines from the system catalog.\n\n        Returns:\n            DataFrame with pipeline metadata from meta_pipelines\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(\n                self.catalog_manager.tables[\"meta_pipelines\"]\n            )\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list pipelines: {e}\")\n            return pd.DataFrame()\n\n    def list_registered_nodes(self, pipeline: Optional[str] = None) -&gt; \"pd.DataFrame\":\n        \"\"\"List nodes from the system catalog.\n\n        Args:\n            pipeline: Optional pipeline name to filter by\n\n        Returns:\n            DataFrame with node metadata from meta_nodes\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_nodes\"])\n            if not df.empty and pipeline:\n                df = df[df[\"pipeline_name\"] == pipeline]\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list nodes: {e}\")\n            return pd.DataFrame()\n\n    def list_runs(\n        self,\n        pipeline: Optional[str] = None,\n        node: Optional[str] = None,\n        status: Optional[str] = None,\n        limit: int = 10,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"List recent runs with optional filters.\n\n        Args:\n            pipeline: Optional pipeline name to filter by\n            node: Optional node name to filter by\n            status: Optional status to filter by (SUCCESS, FAILURE)\n            limit: Maximum number of runs to return\n\n        Returns:\n            DataFrame with run history from meta_runs\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n            if df.empty:\n                return df\n\n            if pipeline:\n                df = df[df[\"pipeline_name\"] == pipeline]\n            if node:\n                df = df[df[\"node_name\"] == node]\n            if status:\n                df = df[df[\"status\"] == status]\n\n            if \"timestamp\" in df.columns:\n                df = df.sort_values(\"timestamp\", ascending=False)\n\n            return df.head(limit)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list runs: {e}\")\n            return pd.DataFrame()\n\n    def list_tables(self) -&gt; \"pd.DataFrame\":\n        \"\"\"List registered assets from meta_tables.\n\n        Returns:\n            DataFrame with table/asset metadata\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list tables: {e}\")\n            return pd.DataFrame()\n\n    # -------------------------------------------------------------------------\n    # Phase 5.2: State Methods\n    # -------------------------------------------------------------------------\n\n    def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get a specific state entry (HWM, content hash, etc.).\n\n        Args:\n            key: The state key to look up\n\n        Returns:\n            Dictionary with state data or None if not found\n        \"\"\"\n\n        if not self.catalog_manager:\n            return None\n\n        try:\n            df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n            if df.empty or \"key\" not in df.columns:\n                return None\n\n            row = df[df[\"key\"] == key]\n            if row.empty:\n                return None\n\n            return row.iloc[0].to_dict()\n        except Exception:\n            return None\n\n    def get_all_state(self, prefix: Optional[str] = None) -&gt; \"pd.DataFrame\":\n        \"\"\"Get all state entries, optionally filtered by key prefix.\n\n        Args:\n            prefix: Optional key prefix to filter by\n\n        Returns:\n            DataFrame with state entries\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n            if not df.empty and prefix and \"key\" in df.columns:\n                df = df[df[\"key\"].str.startswith(prefix)]\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get state: {e}\")\n            return pd.DataFrame()\n\n    def clear_state(self, key: str) -&gt; bool:\n        \"\"\"Remove a state entry.\n\n        Args:\n            key: The state key to remove\n\n        Returns:\n            True if deleted, False otherwise\n        \"\"\"\n        if not self.catalog_manager:\n            return False\n\n        try:\n            return self.catalog_manager.clear_state_key(key)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to clear state: {e}\")\n            return False\n\n    # -------------------------------------------------------------------------\n    # Phase 5.3-5.4: Schema/Lineage and Stats Methods\n    # -------------------------------------------------------------------------\n\n    def get_schema_history(\n        self,\n        table: str,\n        limit: int = 5,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Get schema version history for a table.\n\n        Args:\n            table: Table identifier (supports smart path resolution)\n            limit: Maximum number of versions to return\n\n        Returns:\n            DataFrame with schema history\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            resolved_path = self._resolve_table_path(table)\n            history = self.catalog_manager.get_schema_history(resolved_path, limit)\n            return pd.DataFrame(history)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get schema history: {e}\")\n            return pd.DataFrame()\n\n    def get_lineage(\n        self,\n        table: str,\n        direction: str = \"both\",\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Get lineage for a table.\n\n        Args:\n            table: Table identifier (supports smart path resolution)\n            direction: \"upstream\", \"downstream\", or \"both\"\n\n        Returns:\n            DataFrame with lineage relationships\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            resolved_path = self._resolve_table_path(table)\n\n            results = []\n            if direction in (\"upstream\", \"both\"):\n                upstream = self.catalog_manager.get_upstream(resolved_path)\n                for r in upstream:\n                    r[\"direction\"] = \"upstream\"\n                results.extend(upstream)\n\n            if direction in (\"downstream\", \"both\"):\n                downstream = self.catalog_manager.get_downstream(resolved_path)\n                for r in downstream:\n                    r[\"direction\"] = \"downstream\"\n                results.extend(downstream)\n\n            return pd.DataFrame(results)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get lineage: {e}\")\n            return pd.DataFrame()\n\n    def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n        \"\"\"Get last run status, duration, timestamp for a pipeline.\n\n        Args:\n            pipeline: Pipeline name\n\n        Returns:\n            Dict with status info\n        \"\"\"\n        if not self.catalog_manager:\n            return {}\n\n        try:\n            runs = self.list_runs(pipeline=pipeline, limit=1)\n            if runs.empty:\n                return {\"status\": \"never_run\", \"pipeline\": pipeline}\n\n            last_run = runs.iloc[0].to_dict()\n            return {\n                \"pipeline\": pipeline,\n                \"last_status\": last_run.get(\"status\"),\n                \"last_run_at\": last_run.get(\"timestamp\"),\n                \"last_duration_ms\": last_run.get(\"duration_ms\"),\n                \"last_node\": last_run.get(\"node_name\"),\n            }\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get pipeline status: {e}\")\n            return {}\n\n    def get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n        \"\"\"Get average duration, row counts, success rate over period.\n\n        Args:\n            node: Node name\n            days: Number of days to look back\n\n        Returns:\n            Dict with node statistics\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return {}\n\n        try:\n            avg_duration = self.catalog_manager.get_average_duration(node, days)\n\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n            if df.empty:\n                return {\"node\": node, \"runs\": 0}\n\n            if \"timestamp\" in df.columns:\n                cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n                if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n                    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n                if df[\"timestamp\"].dt.tz is None:\n                    df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"UTC\")\n                df = df[df[\"timestamp\"] &gt;= cutoff]\n\n            node_runs = df[df[\"node_name\"] == node]\n            if node_runs.empty:\n                return {\"node\": node, \"runs\": 0}\n\n            total = len(node_runs)\n            success = len(node_runs[node_runs[\"status\"] == \"SUCCESS\"])\n            avg_rows = node_runs[\"rows_processed\"].mean() if \"rows_processed\" in node_runs else None\n\n            return {\n                \"node\": node,\n                \"runs\": total,\n                \"success_rate\": success / total if total &gt; 0 else 0,\n                \"avg_duration_s\": avg_duration,\n                \"avg_rows\": avg_rows,\n                \"period_days\": days,\n            }\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get node stats: {e}\")\n            return {}\n\n    # -------------------------------------------------------------------------\n    # Phase 6: Smart Path Resolution\n    # -------------------------------------------------------------------------\n\n    def _resolve_table_path(self, identifier: str) -&gt; str:\n        \"\"\"Resolve a user-friendly identifier to a full table path.\n\n        Accepts:\n        - Relative path: \"bronze/OEE/vw_OSMPerformanceOEE\"\n        - Registered table: \"test.vw_OSMPerformanceOEE\"\n        - Node name: \"opsvisdata_vw_OSMPerformanceOEE\"\n        - Full path: \"abfss://...\" (used as-is)\n\n        Args:\n            identifier: User-friendly table identifier\n\n        Returns:\n            Full table path\n        \"\"\"\n        if self._is_full_path(identifier):\n            return identifier\n\n        if self.catalog_manager:\n            resolved = self._lookup_in_catalog(identifier)\n            if resolved:\n                return resolved\n\n        for pipeline in self._pipelines.values():\n            for node in pipeline.config.nodes:\n                if node.name == identifier and node.write:\n                    conn = self.connections.get(node.write.connection)\n                    if conn:\n                        return conn.get_path(node.write.path or node.write.table)\n\n        sys_conn_name = (\n            self.project_config.system.connection if self.project_config.system else None\n        )\n        if sys_conn_name:\n            sys_conn = self.connections.get(sys_conn_name)\n            if sys_conn:\n                return sys_conn.get_path(identifier)\n\n        return identifier\n\n    def _is_full_path(self, identifier: str) -&gt; bool:\n        \"\"\"Check if identifier is already a full path.\"\"\"\n        full_path_prefixes = (\"abfss://\", \"s3://\", \"gs://\", \"hdfs://\", \"/\", \"C:\", \"D:\")\n        return identifier.startswith(full_path_prefixes)\n\n    def _lookup_in_catalog(self, identifier: str) -&gt; Optional[str]:\n        \"\"\"Look up identifier in meta_tables catalog.\"\"\"\n        if not self.catalog_manager:\n            return None\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n            if df.empty or \"table_name\" not in df.columns:\n                return None\n\n            match = df[df[\"table_name\"] == identifier]\n            if not match.empty and \"path\" in match.columns:\n                return match.iloc[0][\"path\"]\n\n            if \".\" in identifier:\n                parts = identifier.split(\".\", 1)\n                if len(parts) == 2:\n                    match = df[df[\"table_name\"] == parts[1]]\n                    if not match.empty and \"path\" in match.columns:\n                        return match.iloc[0][\"path\"]\n\n        except Exception:\n            pass\n\n        return None\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.__init__","title":"<code>__init__(project_config, connections)</code>","text":"<p>Initialize pipeline manager.</p> <p>Parameters:</p> Name Type Description Default <code>project_config</code> <code>ProjectConfig</code> <p>Validated project configuration</p> required <code>connections</code> <code>Dict[str, Any]</code> <p>Connection objects (already instantiated)</p> required Source code in <code>odibi\\pipeline.py</code> <pre><code>def __init__(\n    self,\n    project_config: ProjectConfig,\n    connections: Dict[str, Any],\n):\n    \"\"\"Initialize pipeline manager.\n\n    Args:\n        project_config: Validated project configuration\n        connections: Connection objects (already instantiated)\n    \"\"\"\n    self.project_config = project_config\n    self.connections = connections\n    self._pipelines: Dict[str, Pipeline] = {}\n    self.catalog_manager = None\n    self.lineage_adapter = None\n\n    # Configure logging\n    configure_logging(\n        structured=project_config.logging.structured, level=project_config.logging.level.value\n    )\n\n    # Create manager-level logging context\n    self._ctx = create_logging_context(engine=project_config.engine)\n\n    self._ctx.info(\n        \"Initializing PipelineManager\",\n        project=project_config.project,\n        engine=project_config.engine,\n        pipeline_count=len(project_config.pipelines),\n        connection_count=len(connections),\n    )\n\n    # Initialize Lineage Adapter\n    self.lineage_adapter = OpenLineageAdapter(project_config.lineage)\n\n    # Initialize CatalogManager if configured\n    if project_config.system:\n        from odibi.catalog import CatalogManager\n\n        spark = None\n        engine_instance = None\n\n        if project_config.engine == \"spark\":\n            try:\n                from odibi.engine.spark_engine import SparkEngine\n\n                temp_engine = SparkEngine(connections=connections, config={})\n                spark = temp_engine.spark\n                self._ctx.debug(\"Spark session initialized for System Catalog\")\n            except Exception as e:\n                self._ctx.warning(\n                    f\"Failed to initialize Spark for System Catalog: {e}\",\n                    suggestion=\"Check Spark configuration\",\n                )\n\n        sys_conn = connections.get(project_config.system.connection)\n        if sys_conn:\n            base_path = sys_conn.get_path(project_config.system.path)\n\n            if not spark:\n                try:\n                    from odibi.engine.pandas_engine import PandasEngine\n\n                    engine_instance = PandasEngine(config={})\n                    self._ctx.debug(\"PandasEngine initialized for System Catalog\")\n                except Exception as e:\n                    self._ctx.warning(\n                        f\"Failed to initialize PandasEngine for System Catalog: {e}\"\n                    )\n\n            if spark or engine_instance:\n                self.catalog_manager = CatalogManager(\n                    spark=spark,\n                    config=project_config.system,\n                    base_path=base_path,\n                    engine=engine_instance,\n                    connection=sys_conn,\n                )\n                self.catalog_manager.bootstrap()\n                self._ctx.info(\"System Catalog initialized\", path=base_path)\n        else:\n            self._ctx.warning(\n                f\"System connection '{project_config.system.connection}' not found\",\n                suggestion=\"Configure the system connection in your config\",\n            )\n\n    # Get story configuration\n    story_config = self._get_story_config()\n\n    # Create all pipeline instances\n    self._ctx.debug(\n        \"Creating pipeline instances\",\n        pipelines=[p.pipeline for p in project_config.pipelines],\n    )\n    for pipeline_config in project_config.pipelines:\n        pipeline_name = pipeline_config.pipeline\n\n        self._pipelines[pipeline_name] = Pipeline(\n            pipeline_config=pipeline_config,\n            engine=project_config.engine,\n            connections=connections,\n            generate_story=story_config.get(\"auto_generate\", True),\n            story_config=story_config,\n            retry_config=project_config.retry,\n            alerts=project_config.alerts,\n            performance_config=project_config.performance,\n            catalog_manager=self.catalog_manager,\n            lineage_adapter=self.lineage_adapter,\n        )\n        self._pipelines[pipeline_name].project_config = project_config\n\n    self._ctx.info(\n        \"PipelineManager ready\",\n        pipelines=list(self._pipelines.keys()),\n    )\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.clear_state","title":"<code>clear_state(key)</code>","text":"<p>Remove a state entry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The state key to remove</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False otherwise</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def clear_state(self, key: str) -&gt; bool:\n    \"\"\"Remove a state entry.\n\n    Args:\n        key: The state key to remove\n\n    Returns:\n        True if deleted, False otherwise\n    \"\"\"\n    if not self.catalog_manager:\n        return False\n\n    try:\n        return self.catalog_manager.clear_state_key(key)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to clear state: {e}\")\n        return False\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.deploy","title":"<code>deploy(pipelines=None)</code>","text":"<p>Deploy pipeline definitions to the System Catalog.</p> <p>This registers pipeline and node configurations in the catalog, enabling drift detection and governance features.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional pipeline name(s) to deploy. If None, deploys all.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if deployment succeeded, False otherwise.</p> Example <p>manager = PipelineManager.from_yaml(\"odibi.yaml\") manager.deploy()  # Deploy all pipelines manager.deploy(\"sales_daily\")  # Deploy specific pipeline</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def deploy(self, pipelines: Optional[Union[str, List[str]]] = None) -&gt; bool:\n    \"\"\"Deploy pipeline definitions to the System Catalog.\n\n    This registers pipeline and node configurations in the catalog,\n    enabling drift detection and governance features.\n\n    Args:\n        pipelines: Optional pipeline name(s) to deploy. If None, deploys all.\n\n    Returns:\n        True if deployment succeeded, False otherwise.\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"odibi.yaml\")\n        &gt;&gt;&gt; manager.deploy()  # Deploy all pipelines\n        &gt;&gt;&gt; manager.deploy(\"sales_daily\")  # Deploy specific pipeline\n    \"\"\"\n    if not self.catalog_manager:\n        self._ctx.warning(\n            \"System Catalog not configured. Cannot deploy.\",\n            suggestion=\"Configure system catalog in your YAML config\",\n        )\n        return False\n\n    if pipelines is None:\n        to_deploy = self.project_config.pipelines\n    elif isinstance(pipelines, str):\n        to_deploy = [p for p in self.project_config.pipelines if p.pipeline == pipelines]\n    else:\n        to_deploy = [p for p in self.project_config.pipelines if p.pipeline in pipelines]\n\n    if not to_deploy:\n        self._ctx.warning(\"No matching pipelines found to deploy.\")\n        return False\n\n    self._ctx.info(\n        f\"Deploying {len(to_deploy)} pipeline(s) to System Catalog\",\n        pipelines=[p.pipeline for p in to_deploy],\n    )\n\n    try:\n        self.catalog_manager.bootstrap()\n\n        for pipeline_config in to_deploy:\n            self._ctx.debug(\n                f\"Deploying pipeline: {pipeline_config.pipeline}\",\n                node_count=len(pipeline_config.nodes),\n            )\n            self.catalog_manager.register_pipeline(pipeline_config, self.project_config)\n\n            for node in pipeline_config.nodes:\n                self.catalog_manager.register_node(pipeline_config.pipeline, node)\n\n        self._ctx.info(\n            f\"Deployment complete: {len(to_deploy)} pipeline(s)\",\n            deployed=[p.pipeline for p in to_deploy],\n        )\n        return True\n\n    except Exception as e:\n        self._ctx.error(\n            f\"Deployment failed: {e}\",\n            error_type=type(e).__name__,\n            suggestion=\"Check catalog configuration and permissions\",\n        )\n        return False\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.from_yaml","title":"<code>from_yaml(yaml_path, env=None)</code>  <code>classmethod</code>","text":"<p>Create PipelineManager from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to YAML configuration file</p> required <code>env</code> <code>str</code> <p>Environment name to apply overrides (e.g. 'prod')</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineManager</code> <p>PipelineManager instance ready to run pipelines</p> Example <p>manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\") results = manager.run()  # Run all pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: str, env: str = None) -&gt; \"PipelineManager\":\n    \"\"\"Create PipelineManager from YAML file.\n\n    Args:\n        yaml_path: Path to YAML configuration file\n        env: Environment name to apply overrides (e.g. 'prod')\n\n    Returns:\n        PipelineManager instance ready to run pipelines\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n        &gt;&gt;&gt; results = manager.run()  # Run all pipelines\n    \"\"\"\n    logger.info(f\"Loading configuration from: {yaml_path}\")\n\n    register_standard_library()\n\n    yaml_path_obj = Path(yaml_path)\n    config_dir = yaml_path_obj.parent.absolute()\n\n    import importlib.util\n    import os\n    import sys\n\n    def load_transforms_module(path):\n        if os.path.exists(path):\n            try:\n                spec = importlib.util.spec_from_file_location(\"transforms_autodiscovered\", path)\n                if spec and spec.loader:\n                    module = importlib.util.module_from_spec(spec)\n                    sys.modules[\"transforms_autodiscovered\"] = module\n                    spec.loader.exec_module(module)\n                    logger.info(f\"Auto-loaded transforms from: {path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to auto-load transforms from {path}: {e}\")\n\n    load_transforms_module(os.path.join(config_dir, \"transforms.py\"))\n\n    cwd = os.getcwd()\n    if os.path.abspath(cwd) != str(config_dir):\n        load_transforms_module(os.path.join(cwd, \"transforms.py\"))\n\n    try:\n        config = load_yaml_with_env(str(yaml_path_obj), env=env)\n        logger.debug(\"Configuration loaded successfully\")\n    except FileNotFoundError:\n        logger.error(f\"YAML file not found: {yaml_path}\")\n        raise FileNotFoundError(f\"YAML file not found: {yaml_path}\")\n\n    project_config = ProjectConfig(**config)\n    logger.debug(\n        \"Project config validated\",\n        project=project_config.project,\n        pipelines=len(project_config.pipelines),\n    )\n\n    connections = cls._build_connections(project_config.connections)\n\n    return cls(\n        project_config=project_config,\n        connections=connections,\n    )\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_all_state","title":"<code>get_all_state(prefix=None)</code>","text":"<p>Get all state entries, optionally filtered by key prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Optional[str]</code> <p>Optional key prefix to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with state entries</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_all_state(self, prefix: Optional[str] = None) -&gt; \"pd.DataFrame\":\n    \"\"\"Get all state entries, optionally filtered by key prefix.\n\n    Args:\n        prefix: Optional key prefix to filter by\n\n    Returns:\n        DataFrame with state entries\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n        if not df.empty and prefix and \"key\" in df.columns:\n            df = df[df[\"key\"].str.startswith(prefix)]\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get state: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_lineage","title":"<code>get_lineage(table, direction='both')</code>","text":"<p>Get lineage for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table identifier (supports smart path resolution)</p> required <code>direction</code> <code>str</code> <p>\"upstream\", \"downstream\", or \"both\"</p> <code>'both'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with lineage relationships</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_lineage(\n    self,\n    table: str,\n    direction: str = \"both\",\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Get lineage for a table.\n\n    Args:\n        table: Table identifier (supports smart path resolution)\n        direction: \"upstream\", \"downstream\", or \"both\"\n\n    Returns:\n        DataFrame with lineage relationships\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        resolved_path = self._resolve_table_path(table)\n\n        results = []\n        if direction in (\"upstream\", \"both\"):\n            upstream = self.catalog_manager.get_upstream(resolved_path)\n            for r in upstream:\n                r[\"direction\"] = \"upstream\"\n            results.extend(upstream)\n\n        if direction in (\"downstream\", \"both\"):\n            downstream = self.catalog_manager.get_downstream(resolved_path)\n            for r in downstream:\n                r[\"direction\"] = \"downstream\"\n            results.extend(downstream)\n\n        return pd.DataFrame(results)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get lineage: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_node_stats","title":"<code>get_node_stats(node, days=7)</code>","text":"<p>Get average duration, row counts, success rate over period.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Node name</p> required <code>days</code> <code>int</code> <p>Number of days to look back</p> <code>7</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with node statistics</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n    \"\"\"Get average duration, row counts, success rate over period.\n\n    Args:\n        node: Node name\n        days: Number of days to look back\n\n    Returns:\n        Dict with node statistics\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return {}\n\n    try:\n        avg_duration = self.catalog_manager.get_average_duration(node, days)\n\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n        if df.empty:\n            return {\"node\": node, \"runs\": 0}\n\n        if \"timestamp\" in df.columns:\n            cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n            if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n            if df[\"timestamp\"].dt.tz is None:\n                df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"UTC\")\n            df = df[df[\"timestamp\"] &gt;= cutoff]\n\n        node_runs = df[df[\"node_name\"] == node]\n        if node_runs.empty:\n            return {\"node\": node, \"runs\": 0}\n\n        total = len(node_runs)\n        success = len(node_runs[node_runs[\"status\"] == \"SUCCESS\"])\n        avg_rows = node_runs[\"rows_processed\"].mean() if \"rows_processed\" in node_runs else None\n\n        return {\n            \"node\": node,\n            \"runs\": total,\n            \"success_rate\": success / total if total &gt; 0 else 0,\n            \"avg_duration_s\": avg_duration,\n            \"avg_rows\": avg_rows,\n            \"period_days\": days,\n        }\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get node stats: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_pipeline","title":"<code>get_pipeline(name)</code>","text":"<p>Get a specific pipeline instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Pipeline instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pipeline not found</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_pipeline(self, name: str) -&gt; Pipeline:\n    \"\"\"Get a specific pipeline instance.\n\n    Args:\n        name: Pipeline name\n\n    Returns:\n        Pipeline instance\n\n    Raises:\n        ValueError: If pipeline not found\n    \"\"\"\n    if name not in self._pipelines:\n        available = \", \".join(self._pipelines.keys())\n        raise ValueError(f\"Pipeline '{name}' not found. Available: {available}\")\n    return self._pipelines[name]\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_pipeline_status","title":"<code>get_pipeline_status(pipeline)</code>","text":"<p>Get last run status, duration, timestamp for a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>Pipeline name</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with status info</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n    \"\"\"Get last run status, duration, timestamp for a pipeline.\n\n    Args:\n        pipeline: Pipeline name\n\n    Returns:\n        Dict with status info\n    \"\"\"\n    if not self.catalog_manager:\n        return {}\n\n    try:\n        runs = self.list_runs(pipeline=pipeline, limit=1)\n        if runs.empty:\n            return {\"status\": \"never_run\", \"pipeline\": pipeline}\n\n        last_run = runs.iloc[0].to_dict()\n        return {\n            \"pipeline\": pipeline,\n            \"last_status\": last_run.get(\"status\"),\n            \"last_run_at\": last_run.get(\"timestamp\"),\n            \"last_duration_ms\": last_run.get(\"duration_ms\"),\n            \"last_node\": last_run.get(\"node_name\"),\n        }\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get pipeline status: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_schema_history","title":"<code>get_schema_history(table, limit=5)</code>","text":"<p>Get schema version history for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table identifier (supports smart path resolution)</p> required <code>limit</code> <code>int</code> <p>Maximum number of versions to return</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with schema history</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_schema_history(\n    self,\n    table: str,\n    limit: int = 5,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Get schema version history for a table.\n\n    Args:\n        table: Table identifier (supports smart path resolution)\n        limit: Maximum number of versions to return\n\n    Returns:\n        DataFrame with schema history\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        resolved_path = self._resolve_table_path(table)\n        history = self.catalog_manager.get_schema_history(resolved_path, limit)\n        return pd.DataFrame(history)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get schema history: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_state","title":"<code>get_state(key)</code>","text":"<p>Get a specific state entry (HWM, content hash, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The state key to look up</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary with state data or None if not found</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get a specific state entry (HWM, content hash, etc.).\n\n    Args:\n        key: The state key to look up\n\n    Returns:\n        Dictionary with state data or None if not found\n    \"\"\"\n\n    if not self.catalog_manager:\n        return None\n\n    try:\n        df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n        if df.empty or \"key\" not in df.columns:\n            return None\n\n        row = df[df[\"key\"] == key]\n        if row.empty:\n            return None\n\n        return row.iloc[0].to_dict()\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_pipelines","title":"<code>list_pipelines()</code>","text":"<p>Get list of available pipeline names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of pipeline names</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_pipelines(self) -&gt; List[str]:\n    \"\"\"Get list of available pipeline names.\n\n    Returns:\n        List of pipeline names\n    \"\"\"\n    return list(self._pipelines.keys())\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_registered_nodes","title":"<code>list_registered_nodes(pipeline=None)</code>","text":"<p>List nodes from the system catalog.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Optional[str]</code> <p>Optional pipeline name to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with node metadata from meta_nodes</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_registered_nodes(self, pipeline: Optional[str] = None) -&gt; \"pd.DataFrame\":\n    \"\"\"List nodes from the system catalog.\n\n    Args:\n        pipeline: Optional pipeline name to filter by\n\n    Returns:\n        DataFrame with node metadata from meta_nodes\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_nodes\"])\n        if not df.empty and pipeline:\n            df = df[df[\"pipeline_name\"] == pipeline]\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list nodes: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_registered_pipelines","title":"<code>list_registered_pipelines()</code>","text":"<p>List all registered pipelines from the system catalog.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with pipeline metadata from meta_pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_registered_pipelines(self) -&gt; \"pd.DataFrame\":\n    \"\"\"List all registered pipelines from the system catalog.\n\n    Returns:\n        DataFrame with pipeline metadata from meta_pipelines\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(\n            self.catalog_manager.tables[\"meta_pipelines\"]\n        )\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list pipelines: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_runs","title":"<code>list_runs(pipeline=None, node=None, status=None, limit=10)</code>","text":"<p>List recent runs with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Optional[str]</code> <p>Optional pipeline name to filter by</p> <code>None</code> <code>node</code> <code>Optional[str]</code> <p>Optional node name to filter by</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Optional status to filter by (SUCCESS, FAILURE)</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with run history from meta_runs</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_runs(\n    self,\n    pipeline: Optional[str] = None,\n    node: Optional[str] = None,\n    status: Optional[str] = None,\n    limit: int = 10,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"List recent runs with optional filters.\n\n    Args:\n        pipeline: Optional pipeline name to filter by\n        node: Optional node name to filter by\n        status: Optional status to filter by (SUCCESS, FAILURE)\n        limit: Maximum number of runs to return\n\n    Returns:\n        DataFrame with run history from meta_runs\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n        if df.empty:\n            return df\n\n        if pipeline:\n            df = df[df[\"pipeline_name\"] == pipeline]\n        if node:\n            df = df[df[\"node_name\"] == node]\n        if status:\n            df = df[df[\"status\"] == status]\n\n        if \"timestamp\" in df.columns:\n            df = df.sort_values(\"timestamp\", ascending=False)\n\n        return df.head(limit)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list runs: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_tables","title":"<code>list_tables()</code>","text":"<p>List registered assets from meta_tables.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with table/asset metadata</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_tables(self) -&gt; \"pd.DataFrame\":\n    \"\"\"List registered assets from meta_tables.\n\n    Returns:\n        DataFrame with table/asset metadata\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list tables: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.run","title":"<code>run(pipelines=None, dry_run=False, resume_from_failure=False, parallel=False, max_workers=4, on_error=None, tag=None, node=None, console=False)</code>","text":"<p>Run one, multiple, or all pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Pipeline name(s) to run.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>Whether to simulate execution.</p> <code>False</code> <code>resume_from_failure</code> <code>bool</code> <p>Whether to skip successfully completed nodes from last run.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run nodes in parallel.</p> <code>False</code> <code>max_workers</code> <code>int</code> <p>Maximum number of worker threads for parallel execution.</p> <code>4</code> <code>on_error</code> <code>Optional[str]</code> <p>Override error handling strategy (fail_fast, fail_later, ignore).</p> <code>None</code> <code>tag</code> <code>Optional[str]</code> <p>Filter nodes by tag (only nodes with this tag will run).</p> <code>None</code> <code>node</code> <code>Optional[str]</code> <p>Run only the specific node by name.</p> <code>None</code> <code>console</code> <code>bool</code> <p>Whether to show rich console output with progress.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[PipelineResults, Dict[str, PipelineResults]]</code> <p>PipelineResults or Dict of results</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def run(\n    self,\n    pipelines: Optional[Union[str, List[str]]] = None,\n    dry_run: bool = False,\n    resume_from_failure: bool = False,\n    parallel: bool = False,\n    max_workers: int = 4,\n    on_error: Optional[str] = None,\n    tag: Optional[str] = None,\n    node: Optional[str] = None,\n    console: bool = False,\n) -&gt; Union[PipelineResults, Dict[str, PipelineResults]]:\n    \"\"\"Run one, multiple, or all pipelines.\n\n    Args:\n        pipelines: Pipeline name(s) to run.\n        dry_run: Whether to simulate execution.\n        resume_from_failure: Whether to skip successfully completed nodes from last run.\n        parallel: Whether to run nodes in parallel.\n        max_workers: Maximum number of worker threads for parallel execution.\n        on_error: Override error handling strategy (fail_fast, fail_later, ignore).\n        tag: Filter nodes by tag (only nodes with this tag will run).\n        node: Run only the specific node by name.\n        console: Whether to show rich console output with progress.\n\n    Returns:\n        PipelineResults or Dict of results\n    \"\"\"\n    if pipelines is None:\n        pipeline_names = list(self._pipelines.keys())\n    elif isinstance(pipelines, str):\n        pipeline_names = [pipelines]\n    else:\n        pipeline_names = pipelines\n\n    for name in pipeline_names:\n        if name not in self._pipelines:\n            available = \", \".join(self._pipelines.keys())\n            self._ctx.error(\n                f\"Pipeline not found: {name}\",\n                available=list(self._pipelines.keys()),\n            )\n            raise ValueError(f\"Pipeline '{name}' not found. Available pipelines: {available}\")\n\n    # Phase 2: Auto-register pipelines and nodes before execution\n    if self.catalog_manager:\n        self._auto_register_pipelines(pipeline_names)\n\n    self._ctx.info(\n        f\"Running {len(pipeline_names)} pipeline(s)\",\n        pipelines=pipeline_names,\n        dry_run=dry_run,\n        parallel=parallel,\n    )\n\n    results = {}\n    for idx, name in enumerate(pipeline_names):\n        # Invalidate cache before each pipeline so it sees latest outputs\n        if self.catalog_manager:\n            self.catalog_manager.invalidate_cache()\n\n        self._ctx.info(\n            f\"Executing pipeline {idx + 1}/{len(pipeline_names)}: {name}\",\n            pipeline=name,\n            order=idx + 1,\n        )\n\n        results[name] = self._pipelines[name].run(\n            dry_run=dry_run,\n            resume_from_failure=resume_from_failure,\n            parallel=parallel,\n            max_workers=max_workers,\n            on_error=on_error,\n            tag=tag,\n            node=node,\n            console=console,\n        )\n\n        result = results[name]\n        status = \"SUCCESS\" if not result.failed else \"FAILED\"\n        self._ctx.info(\n            f\"Pipeline {status}: {name}\",\n            status=status,\n            duration_s=round(result.duration, 2),\n            completed=len(result.completed),\n            failed=len(result.failed),\n        )\n\n        if result.story_path:\n            self._ctx.debug(f\"Story generated: {result.story_path}\")\n\n    if len(pipeline_names) == 1:\n        return results[pipeline_names[0]]\n    else:\n        return results\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults","title":"<code>PipelineResults</code>  <code>dataclass</code>","text":"<p>Results from pipeline execution.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>@dataclass\nclass PipelineResults:\n    \"\"\"Results from pipeline execution.\"\"\"\n\n    pipeline_name: str\n    completed: List[str] = field(default_factory=list)\n    failed: List[str] = field(default_factory=list)\n    skipped: List[str] = field(default_factory=list)\n    node_results: Dict[str, NodeResult] = field(default_factory=dict)\n    duration: float = 0.0\n    start_time: Optional[str] = None\n    end_time: Optional[str] = None\n    story_path: Optional[str] = None\n\n    def get_node_result(self, name: str) -&gt; Optional[NodeResult]:\n        \"\"\"Get result for specific node.\n\n        Args:\n            name: Node name\n\n        Returns:\n            NodeResult if available, None otherwise\n        \"\"\"\n        return self.node_results.get(name)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary.\n\n        Returns:\n            Dictionary representation\n        \"\"\"\n        return {\n            \"pipeline_name\": self.pipeline_name,\n            \"completed\": self.completed,\n            \"failed\": self.failed,\n            \"skipped\": self.skipped,\n            \"duration\": self.duration,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"node_count\": len(self.node_results),\n        }\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.get_node_result","title":"<code>get_node_result(name)</code>","text":"<p>Get result for specific node.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Node name</p> required <p>Returns:</p> Type Description <code>Optional[NodeResult]</code> <p>NodeResult if available, None otherwise</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_node_result(self, name: str) -&gt; Optional[NodeResult]:\n    \"\"\"Get result for specific node.\n\n    Args:\n        name: Node name\n\n    Returns:\n        NodeResult if available, None otherwise\n    \"\"\"\n    return self.node_results.get(name)\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"pipeline_name\": self.pipeline_name,\n        \"completed\": self.completed,\n        \"failed\": self.failed,\n        \"skipped\": self.skipped,\n        \"duration\": self.duration,\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"node_count\": len(self.node_results),\n    }\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.create_context","title":"<code>create_context(engine, spark_session=None)</code>","text":"<p>Factory function to create appropriate context.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>str</code> <p>Engine type ('pandas' or 'spark')</p> required <code>spark_session</code> <code>Optional[Any]</code> <p>SparkSession (required if engine='spark')</p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context instance for the specified engine</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If engine is invalid or SparkSession missing for Spark</p> Source code in <code>odibi\\context.py</code> <pre><code>def create_context(engine: str, spark_session: Optional[Any] = None) -&gt; Context:\n    \"\"\"Factory function to create appropriate context.\n\n    Args:\n        engine: Engine type ('pandas' or 'spark')\n        spark_session: SparkSession (required if engine='spark')\n\n    Returns:\n        Context instance for the specified engine\n\n    Raises:\n        ValueError: If engine is invalid or SparkSession missing for Spark\n    \"\"\"\n    if engine == \"pandas\":\n        return PandasContext()\n    elif engine == \"spark\":\n        if spark_session is None:\n            raise ValueError(\"SparkSession required for Spark engine\")\n        return SparkContext(spark_session)\n    elif engine == \"polars\":\n        return PolarsContext()\n    else:\n        raise ValueError(f\"Unsupported engine: {engine}. Use 'pandas' or 'spark'\")\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/","title":"Medallion Architecture Enhancements Plan","text":"<p>Status: Draft Created: 2024-01-30 Total Effort: ~6-7 days Priority: High - Production readiness for medallion pipelines</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#executive-summary","title":"Executive Summary","text":"<p>This plan addresses 5 key gaps in Odibi's medallion architecture support:</p> # Enhancement Effort Priority 1 Quarantine Tables 1.5 days \ud83d\udd34 Critical 2 Quality Gates 1 day \ud83d\udd34 Critical 3 Alerting Enhancements 1 day \ud83d\udfe1 High 4 Schema Version Tracking 0.5 days \ud83d\udfe2 Medium 5 Cross-Pipeline Lineage 2-3 days \ud83d\udfe2 Medium"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#1-quarantine-tables","title":"1. Quarantine Tables","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem","title":"Problem","text":"<p>When validation tests fail, bad rows are either: - Logged and written anyway (<code>on_fail: warn</code>) - Cause the entire pipeline to fail (<code>on_fail: fail</code>)</p> <p>Neither option preserves bad data for later analysis/reprocessing.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution","title":"Solution","text":"<p>Route failed rows to a dedicated quarantine table with rejection metadata.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#yaml-schema","title":"YAML Schema","text":"<pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id, email]\n          on_fail: quarantine  # NEW: Route to quarantine instead of fail/warn\n        - type: regex_match\n          column: email\n          pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n          on_fail: quarantine\n\n      # NEW: Quarantine configuration\n      quarantine:\n        connection: silver\n        path: customers_quarantine  # Or table: customers_quarantine\n        add_columns:\n          _rejection_reason: true    # Which test(s) failed\n          _rejected_at: true         # Timestamp\n          _source_batch_id: true     # Link back to source batch\n          _failed_tests: true        # List of failed test names\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#config-model-changes","title":"Config Model Changes","text":"<pre><code># odibi/config.py\n\nclass QuarantineConfig(BaseModel):\n    \"\"\"Configuration for quarantine table routing.\"\"\"\n\n    connection: str = Field(description=\"Connection for quarantine writes\")\n    path: Optional[str] = Field(default=None, description=\"Path for quarantine data\")\n    table: Optional[str] = Field(default=None, description=\"Table name for quarantine\")\n\n    # Metadata columns to add\n    add_columns: QuarantineColumnsConfig = Field(\n        default_factory=QuarantineColumnsConfig,\n        description=\"Metadata columns to add to quarantined rows\"\n    )\n\n    # Retention\n    retention_days: Optional[int] = Field(\n        default=90,\n        ge=1,\n        description=\"Days to retain quarantined data (auto-cleanup)\"\n    )\n\nclass QuarantineColumnsConfig(BaseModel):\n    \"\"\"Columns added to quarantined rows.\"\"\"\n    rejection_reason: bool = Field(default=True, alias=\"_rejection_reason\")\n    rejected_at: bool = Field(default=True, alias=\"_rejected_at\")\n    source_batch_id: bool = Field(default=True, alias=\"_source_batch_id\")\n    failed_tests: bool = Field(default=True, alias=\"_failed_tests\")\n    original_node: bool = Field(default=False, alias=\"_original_node\")\n\n\nclass ValidationConfig(BaseModel):\n    \"\"\"Updated validation config with quarantine support.\"\"\"\n    tests: List[TestConfig] = Field(default_factory=list)\n    quarantine: Optional[QuarantineConfig] = Field(\n        default=None,\n        description=\"Quarantine configuration for failed rows\"\n    )\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation","title":"Implementation","text":"<p>File: <code>odibi/validation/quarantine.py</code> (new)</p> <pre><code>from typing import Any, Dict, List, Tuple\nfrom datetime import datetime\nimport hashlib\n\ndef split_valid_invalid(\n    df: Any,\n    tests: List[TestConfig],\n    engine: Engine,\n    context: EngineContext\n) -&gt; Tuple[Any, Any, List[Dict]]:\n    \"\"\"\n    Split DataFrame into valid and invalid portions.\n\n    Returns:\n        Tuple of (valid_df, invalid_df, rejection_details)\n    \"\"\"\n    # Build combined validity mask\n    validity_masks = []\n    test_results = {}  # row_index -&gt; [failed_test_names]\n\n    for test in tests:\n        if test.on_fail == ContractSeverity.QUARANTINE:\n            mask = _evaluate_test(df, test, engine)\n            validity_masks.append(mask)\n            test_results[test.name or test.type] = ~mask\n\n    # Combine masks (row is valid if passes ALL quarantine tests)\n    combined_valid = engine.all_true(validity_masks)\n\n    valid_df = engine.filter(df, combined_valid)\n    invalid_df = engine.filter(df, ~combined_valid)\n\n    return valid_df, invalid_df, test_results\n\n\ndef add_quarantine_metadata(\n    invalid_df: Any,\n    test_results: Dict,\n    config: QuarantineConfig,\n    engine: Engine,\n    node_name: str,\n    run_id: str\n) -&gt; Any:\n    \"\"\"Add metadata columns to quarantined rows.\"\"\"\n\n    if config.add_columns.rejection_reason:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_rejection_reason\",\n            _get_rejection_reasons(invalid_df, test_results, engine)\n        )\n\n    if config.add_columns.rejected_at:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_rejected_at\",\n            datetime.utcnow().isoformat()\n        )\n\n    if config.add_columns.source_batch_id:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_source_batch_id\",\n            run_id\n        )\n\n    if config.add_columns.failed_tests:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_failed_tests\",\n            _get_failed_test_list(invalid_df, test_results, engine)\n        )\n\n    if config.add_columns.original_node:\n        invalid_df = engine.add_column(\n            invalid_df,\n            \"_original_node\",\n            node_name\n        )\n\n    return invalid_df\n\n\ndef write_quarantine(\n    invalid_df: Any,\n    config: QuarantineConfig,\n    engine: Engine,\n    connections: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"Write quarantined rows to destination.\"\"\"\n\n    connection = connections[config.connection]\n\n    result = engine.write(\n        invalid_df,\n        connection=connection,\n        format=\"delta\",\n        path=config.path,\n        table=config.table,\n        mode=\"append\"  # Always append\n    )\n\n    return {\n        \"rows_quarantined\": engine.count(invalid_df),\n        \"quarantine_path\": config.path or config.table,\n        \"write_info\": result\n    }\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#node-execution-changes","title":"Node Execution Changes","text":"<p>File: <code>odibi/node.py</code> - Update validation flow</p> <pre><code># In _execute_validation method:\n\ndef _execute_validation(self, df, config, context):\n    \"\"\"Execute validation with quarantine support.\"\"\"\n\n    validator = Validator()\n\n    # Check if any tests use quarantine\n    has_quarantine_tests = any(\n        t.on_fail == ContractSeverity.QUARANTINE\n        for t in config.validation.tests\n    )\n\n    if has_quarantine_tests and config.validation.quarantine:\n        # Split valid/invalid\n        valid_df, invalid_df, test_results = split_valid_invalid(\n            df, config.validation.tests, self.engine, context\n        )\n\n        invalid_count = self.engine.count(invalid_df)\n\n        if invalid_count &gt; 0:\n            # Add metadata\n            invalid_df = add_quarantine_metadata(\n                invalid_df,\n                test_results,\n                config.validation.quarantine,\n                self.engine,\n                config.name,\n                context.run_id\n            )\n\n            # Write to quarantine\n            quarantine_result = write_quarantine(\n                invalid_df,\n                config.validation.quarantine,\n                self.engine,\n                context.connections\n            )\n\n            # Emit alert if configured\n            self._emit_quarantine_alert(config, quarantine_result, context)\n\n            logger.info(\n                f\"Quarantined {invalid_count} rows to \"\n                f\"{config.validation.quarantine.path or config.validation.quarantine.table}\"\n            )\n\n        # Continue with valid rows only\n        return valid_df, []\n\n    else:\n        # Original validation logic (no quarantine)\n        failures = validator.validate(df, config.validation, context)\n        return df, failures\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#tests","title":"Tests","text":"<pre><code># tests/test_quarantine.py\n\ndef test_quarantine_splits_valid_invalid():\n    \"\"\"Valid rows continue, invalid rows go to quarantine.\"\"\"\n\ndef test_quarantine_adds_metadata_columns():\n    \"\"\"Quarantined rows have _rejection_reason, _rejected_at, etc.\"\"\"\n\ndef test_quarantine_appends_not_overwrites():\n    \"\"\"Multiple runs append to same quarantine table.\"\"\"\n\ndef test_quarantine_triggers_alert():\n    \"\"\"Alert sent when rows are quarantined.\"\"\"\n\ndef test_multiple_failed_tests_captured():\n    \"\"\"Row failing 3 tests has all 3 in _failed_tests column.\"\"\"\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#2-quality-gates","title":"2. Quality Gates","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem_1","title":"Problem","text":"<p>Validation tests run per-row, but there's no batch-level check that says \"abort if too many rows fail.\"</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution_1","title":"Solution","text":"<p>Add a <code>gate</code> configuration that evaluates the batch as a whole before writing.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#yaml-schema_1","title":"YAML Schema","text":"<pre><code>nodes:\n  - name: load_silver_customers\n    read:\n      connection: bronze\n      path: customers\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n        - type: unique\n          columns: [customer_id]\n\n    # NEW: Quality Gate\n    gate:\n      # Minimum percentage of rows that must pass ALL tests\n      require_pass_rate: 0.95  # 95%\n\n      # What to do if gate fails\n      on_fail: abort  # abort | warn_and_write | write_valid_only\n\n      # Optional: Specific thresholds per test type\n      thresholds:\n        - test: not_null\n          min_pass_rate: 0.99  # 99% must have non-null customer_id\n        - test: unique\n          min_pass_rate: 1.0   # 100% must be unique (no duplicates)\n\n      # Row count anomaly detection\n      row_count:\n        min: 100              # At least 100 rows expected\n        max: 1000000          # At most 1M rows\n        change_threshold: 0.5 # Fail if row count changes &gt;50% vs last run\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#config-model","title":"Config Model","text":"<pre><code># odibi/config.py\n\nclass GateThreshold(BaseModel):\n    \"\"\"Threshold for a specific test.\"\"\"\n    test: str = Field(description=\"Test name or type\")\n    min_pass_rate: float = Field(ge=0.0, le=1.0, description=\"Minimum pass rate\")\n\n\nclass RowCountGate(BaseModel):\n    \"\"\"Row count anomaly detection.\"\"\"\n    min: Optional[int] = Field(default=None, ge=0)\n    max: Optional[int] = Field(default=None, ge=0)\n    change_threshold: Optional[float] = Field(\n        default=None,\n        ge=0.0,\n        le=1.0,\n        description=\"Max allowed change vs previous run (e.g., 0.5 = 50%)\"\n    )\n\n\nclass GateOnFail(str, Enum):\n    \"\"\"Action when gate fails.\"\"\"\n    ABORT = \"abort\"                    # Stop, write nothing\n    WARN_AND_WRITE = \"warn_and_write\"  # Log warning, write all rows\n    WRITE_VALID_ONLY = \"write_valid_only\"  # Write only passing rows\n\n\nclass GateConfig(BaseModel):\n    \"\"\"Quality gate configuration.\"\"\"\n\n    require_pass_rate: float = Field(\n        default=0.95,\n        ge=0.0,\n        le=1.0,\n        description=\"Minimum percentage of rows passing ALL tests\"\n    )\n\n    on_fail: GateOnFail = Field(\n        default=GateOnFail.ABORT,\n        description=\"Action when gate fails\"\n    )\n\n    thresholds: List[GateThreshold] = Field(\n        default_factory=list,\n        description=\"Per-test thresholds (overrides global require_pass_rate)\"\n    )\n\n    row_count: Optional[RowCountGate] = Field(\n        default=None,\n        description=\"Row count anomaly detection\"\n    )\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation_1","title":"Implementation","text":"<p>File: <code>odibi/validation/gate.py</code> (new)</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclass\nclass GateResult:\n    \"\"\"Result of gate evaluation.\"\"\"\n    passed: bool\n    pass_rate: float\n    total_rows: int\n    passed_rows: int\n    failed_rows: int\n    details: Dict[str, Any]\n    action: GateOnFail\n\n\ndef evaluate_gate(\n    df: Any,\n    validation_results: Dict[str, List[bool]],  # test_name -&gt; per-row results\n    gate_config: GateConfig,\n    engine: Engine,\n    catalog: Optional[CatalogManager] = None,\n    node_name: Optional[str] = None\n) -&gt; GateResult:\n    \"\"\"\n    Evaluate quality gate on validation results.\n\n    Returns GateResult with pass/fail status and action to take.\n    \"\"\"\n    total_rows = engine.count(df)\n\n    # Calculate overall pass rate (row passes if ALL tests pass)\n    all_pass_mask = _combine_test_results(validation_results, engine)\n    passed_rows = engine.count_true(all_pass_mask)\n    pass_rate = passed_rows / total_rows if total_rows &gt; 0 else 1.0\n\n    details = {\n        \"overall_pass_rate\": pass_rate,\n        \"per_test_rates\": {},\n        \"row_count_check\": None\n    }\n\n    gate_passed = True\n    failure_reasons = []\n\n    # Check global threshold\n    if pass_rate &lt; gate_config.require_pass_rate:\n        gate_passed = False\n        failure_reasons.append(\n            f\"Overall pass rate {pass_rate:.1%} &lt; required {gate_config.require_pass_rate:.1%}\"\n        )\n\n    # Check per-test thresholds\n    for threshold in gate_config.thresholds:\n        test_results = validation_results.get(threshold.test)\n        if test_results:\n            test_pass_rate = sum(test_results) / len(test_results)\n            details[\"per_test_rates\"][threshold.test] = test_pass_rate\n\n            if test_pass_rate &lt; threshold.min_pass_rate:\n                gate_passed = False\n                failure_reasons.append(\n                    f\"Test '{threshold.test}' pass rate {test_pass_rate:.1%} \"\n                    f\"&lt; required {threshold.min_pass_rate:.1%}\"\n                )\n\n    # Check row count anomalies\n    if gate_config.row_count:\n        row_check = _check_row_count(\n            total_rows,\n            gate_config.row_count,\n            catalog,\n            node_name\n        )\n        details[\"row_count_check\"] = row_check\n\n        if not row_check[\"passed\"]:\n            gate_passed = False\n            failure_reasons.append(row_check[\"reason\"])\n\n    details[\"failure_reasons\"] = failure_reasons\n\n    return GateResult(\n        passed=gate_passed,\n        pass_rate=pass_rate,\n        total_rows=total_rows,\n        passed_rows=passed_rows,\n        failed_rows=total_rows - passed_rows,\n        details=details,\n        action=gate_config.on_fail if not gate_passed else None\n    )\n\n\ndef _check_row_count(\n    current_count: int,\n    config: RowCountGate,\n    catalog: Optional[CatalogManager],\n    node_name: Optional[str]\n) -&gt; Dict[str, Any]:\n    \"\"\"Check row count against thresholds and history.\"\"\"\n\n    result = {\"passed\": True, \"reason\": None, \"current\": current_count}\n\n    if config.min is not None and current_count &lt; config.min:\n        result[\"passed\"] = False\n        result[\"reason\"] = f\"Row count {current_count} &lt; minimum {config.min}\"\n        return result\n\n    if config.max is not None and current_count &gt; config.max:\n        result[\"passed\"] = False\n        result[\"reason\"] = f\"Row count {current_count} &gt; maximum {config.max}\"\n        return result\n\n    # Historical comparison\n    if config.change_threshold is not None and catalog and node_name:\n        previous_count = catalog.get_last_row_count(node_name)\n        if previous_count:\n            change = abs(current_count - previous_count) / previous_count\n            result[\"previous\"] = previous_count\n            result[\"change\"] = change\n\n            if change &gt; config.change_threshold:\n                result[\"passed\"] = False\n                result[\"reason\"] = (\n                    f\"Row count changed {change:.1%} \"\n                    f\"({previous_count} \u2192 {current_count}), \"\n                    f\"exceeds threshold {config.change_threshold:.1%}\"\n                )\n\n    return result\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#node-integration","title":"Node Integration","text":"<pre><code># In node.py - after validation, before write:\n\ndef _check_gate(self, df, validation_results, config, context):\n    \"\"\"Check quality gate before proceeding to write.\"\"\"\n\n    if not config.gate:\n        return df, True  # No gate configured\n\n    gate_result = evaluate_gate(\n        df,\n        validation_results,\n        config.gate,\n        self.engine,\n        context.catalog,\n        config.name\n    )\n\n    if gate_result.passed:\n        logger.info(f\"Gate passed: {gate_result.pass_rate:.1%} pass rate\")\n        return df, True\n\n    # Gate failed - take action\n    logger.warning(\n        f\"Gate FAILED: {gate_result.pass_rate:.1%} pass rate. \"\n        f\"Reasons: {gate_result.details['failure_reasons']}\"\n    )\n\n    # Emit alert\n    self._emit_gate_alert(config, gate_result, context)\n\n    if gate_result.action == GateOnFail.ABORT:\n        raise GateFailedError(\n            f\"Quality gate blocked write for node '{config.name}'. \"\n            f\"Pass rate: {gate_result.pass_rate:.1%}\"\n        )\n\n    elif gate_result.action == GateOnFail.WARN_AND_WRITE:\n        return df, True  # Write all, warning already logged\n\n    elif gate_result.action == GateOnFail.WRITE_VALID_ONLY:\n        valid_df = self.engine.filter(df, validation_results[\"_all_passed\"])\n        return valid_df, True\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#3-alerting-enhancements","title":"3. Alerting Enhancements","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#current-gaps","title":"Current Gaps","text":"Feature Teams Slack Gap Timestamp \u2705 \u274c Slack missing Status-based styling \u2705 (Adaptive Card styles) \u26a0\ufe0f (emoji only) Slack needs color Structured facts \u2705 (FactSet) \u26a0\ufe0f (inline fields) Minor Action buttons \u274c \u274c Both missing Validation events \u274c \u274c Both missing Throttling \u274c \u274c Both missing"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#new-alert-events","title":"New Alert Events","text":"<pre><code># odibi/config.py\n\nclass AlertEvent(str, Enum):\n    \"\"\"Events that trigger alerts.\"\"\"\n\n    # Existing\n    ON_START = \"on_start\"\n    ON_SUCCESS = \"on_success\"\n    ON_FAILURE = \"on_failure\"\n\n    # NEW: Validation &amp; Quality\n    ON_VALIDATION_FAIL = \"on_validation_fail\"    # Any validation test failed\n    ON_GATE_BLOCK = \"on_gate_block\"              # Quality gate blocked write\n    ON_QUARANTINE = \"on_quarantine\"              # Rows sent to quarantine\n    ON_THRESHOLD_BREACH = \"on_threshold_breach\"  # Row count anomaly, etc.\n\n    # NEW: Data Quality\n    ON_SCHEMA_DRIFT = \"on_schema_drift\"          # Schema changed vs previous\n    ON_FRESHNESS_FAIL = \"on_freshness_fail\"      # Data too old\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#enhanced-slack-payload","title":"Enhanced Slack Payload","text":"<pre><code># odibi/utils/alerting.py\n\ndef _build_slack_payload(config: AlertConfig, message: str, context: Dict[str, Any]) -&gt; Dict:\n    \"\"\"Enhanced Slack Block Kit payload with parity to Teams.\"\"\"\n\n    pipeline = context.get(\"pipeline\", \"Unknown\")\n    status = context.get(\"status\", \"UNKNOWN\")\n    event_type = context.get(\"event_type\", \"on_failure\")\n    duration = context.get(\"duration\", 0.0)\n    timestamp = context.get(\"timestamp\", datetime.utcnow().isoformat())\n    project_config = context.get(\"project_config\")\n\n    # Status styling\n    status_config = {\n        \"SUCCESS\": {\"icon\": \"\u2705\", \"color\": \"#36a64f\"},\n        \"STARTED\": {\"icon\": \"\ud83d\ude80\", \"color\": \"#1e90ff\"},\n        \"FAILED\": {\"icon\": \"\u274c\", \"color\": \"#dc3545\"},\n        \"GATE_BLOCKED\": {\"icon\": \"\ud83d\udeab\", \"color\": \"#ff6b35\"},\n        \"QUARANTINED\": {\"icon\": \"\ud83d\udd12\", \"color\": \"#ffc107\"},\n        \"WARNING\": {\"icon\": \"\u26a0\ufe0f\", \"color\": \"#ffc107\"},\n    }.get(status, {\"icon\": \"\u2753\", \"color\": \"#6c757d\"})\n\n    # Build blocks\n    blocks = [\n        {\n            \"type\": \"header\",\n            \"text\": {\n                \"type\": \"plain_text\",\n                \"text\": f\"{status_config['icon']} ODIBI: {pipeline} - {status}\"\n            }\n        },\n        {\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"*Project:*\\n{project_config.project if project_config else 'Unknown'}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Status:*\\n{status}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Duration:*\\n{duration:.2f}s\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Timestamp:*\\n{timestamp}\"},  # NEW\n            ]\n        },\n        {\n            \"type\": \"section\",\n            \"text\": {\"type\": \"mrkdwn\", \"text\": f\"*Message:*\\n{message}\"}\n        }\n    ]\n\n    # Add owner if present\n    if project_config and project_config.owner:\n        blocks[1][\"fields\"].append(\n            {\"type\": \"mrkdwn\", \"text\": f\"*Owner:*\\n{project_config.owner}\"}\n        )\n\n    # NEW: Add event-specific details\n    if event_type == \"on_quarantine\":\n        quarantine_details = context.get(\"quarantine_details\", {})\n        blocks.append({\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"*Rows Quarantined:*\\n{quarantine_details.get('rows_quarantined', 0):,}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Quarantine Table:*\\n`{quarantine_details.get('quarantine_path', 'N/A')}`\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Failed Tests:*\\n{', '.join(quarantine_details.get('failed_tests', []))}\"},\n            ]\n        })\n\n    elif event_type == \"on_gate_block\":\n        gate_details = context.get(\"gate_details\", {})\n        blocks.append({\n            \"type\": \"section\",\n            \"fields\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"*Pass Rate:*\\n{gate_details.get('pass_rate', 0):.1%}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Required:*\\n{gate_details.get('required_rate', 0.95):.1%}\"},\n                {\"type\": \"mrkdwn\", \"text\": f\"*Rows Failed:*\\n{gate_details.get('failed_rows', 0):,}\"},\n            ]\n        })\n        if gate_details.get(\"failure_reasons\"):\n            blocks.append({\n                \"type\": \"section\",\n                \"text\": {\"type\": \"mrkdwn\", \"text\": f\"*Failure Reasons:*\\n\u2022 \" + \"\\n\u2022 \".join(gate_details[\"failure_reasons\"])}\n            })\n\n    # NEW: Action buttons (link to story)\n    story_url = context.get(\"story_url\")\n    if story_url:\n        blocks.append({\n            \"type\": \"actions\",\n            \"elements\": [\n                {\n                    \"type\": \"button\",\n                    \"text\": {\"type\": \"plain_text\", \"text\": \"\ud83d\udcca View Story\"},\n                    \"url\": story_url,\n                    \"style\": \"primary\"\n                }\n            ]\n        })\n\n    # Add divider and context footer\n    blocks.extend([\n        {\"type\": \"divider\"},\n        {\n            \"type\": \"context\",\n            \"elements\": [\n                {\"type\": \"mrkdwn\", \"text\": f\"Odibi v{__version__} | {event_type}\"}\n            ]\n        }\n    ])\n\n    # Use attachment for color sidebar\n    return {\n        \"attachments\": [{\n            \"color\": status_config[\"color\"],\n            \"blocks\": blocks\n        }]\n    }\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#enhanced-teams-payload","title":"Enhanced Teams Payload","text":"<pre><code>def _build_teams_payload(config: AlertConfig, message: str, context: Dict[str, Any]) -&gt; Dict:\n    \"\"\"Enhanced Teams Adaptive Card with event-specific details.\"\"\"\n\n    # ... existing header/status logic ...\n\n    # NEW: Event-specific sections\n    event_type = context.get(\"event_type\", \"on_failure\")\n\n    body_items = [\n        # ... existing header container ...\n    ]\n\n    # Standard facts\n    facts = [\n        {\"title\": \"\u23f1 Duration\", \"value\": f\"{duration:.2f}s\"},\n        {\"title\": \"\ud83d\udcc5 Time\", \"value\": context.get(\"timestamp\", \"\")},\n        {\"title\": \"\ud83d\udcdd Message\", \"value\": message},\n    ]\n\n    # NEW: Event-specific facts\n    if event_type == \"on_quarantine\":\n        qd = context.get(\"quarantine_details\", {})\n        facts.extend([\n            {\"title\": \"\ud83d\udd12 Rows Quarantined\", \"value\": f\"{qd.get('rows_quarantined', 0):,}\"},\n            {\"title\": \"\ud83d\udccd Quarantine Table\", \"value\": qd.get(\"quarantine_path\", \"N/A\")},\n            {\"title\": \"\u274c Failed Tests\", \"value\": \", \".join(qd.get(\"failed_tests\", []))},\n        ])\n\n    elif event_type == \"on_gate_block\":\n        gd = context.get(\"gate_details\", {})\n        facts.extend([\n            {\"title\": \"\ud83d\udcca Pass Rate\", \"value\": f\"{gd.get('pass_rate', 0):.1%}\"},\n            {\"title\": \"\ud83c\udfaf Required\", \"value\": f\"{gd.get('required_rate', 0.95):.1%}\"},\n            {\"title\": \"\u274c Rows Failed\", \"value\": f\"{gd.get('failed_rows', 0):,}\"},\n        ])\n\n    body_items.append({\"type\": \"Container\", \"items\": [{\"type\": \"FactSet\", \"facts\": facts}]})\n\n    # NEW: Action button for story\n    story_url = context.get(\"story_url\")\n    if story_url:\n        body_items.append({\n            \"type\": \"ActionSet\",\n            \"actions\": [\n                {\n                    \"type\": \"Action.OpenUrl\",\n                    \"title\": \"\ud83d\udcca View Story\",\n                    \"url\": story_url,\n                    \"style\": \"positive\"\n                }\n            ]\n        })\n\n    # ... rest of card construction ...\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#alert-throttling","title":"Alert Throttling","text":"<pre><code># odibi/utils/alerting.py\n\nclass AlertThrottler:\n    \"\"\"Prevent alert spam by throttling repeated alerts.\"\"\"\n\n    def __init__(self):\n        self._last_alerts: Dict[str, datetime] = {}\n        self._alert_counts: Dict[str, int] = {}\n\n    def should_send(\n        self,\n        alert_key: str,\n        throttle_minutes: int = 15,\n        max_per_hour: int = 10\n    ) -&gt; bool:\n        \"\"\"Check if alert should be sent based on throttling rules.\"\"\"\n\n        now = datetime.utcnow()\n        last = self._last_alerts.get(alert_key)\n\n        # Check time-based throttle\n        if last and (now - last).total_seconds() &lt; throttle_minutes * 60:\n            logger.debug(f\"Alert throttled: {alert_key} (within {throttle_minutes}m)\")\n            return False\n\n        # Check rate limit\n        hour_key = f\"{alert_key}:{now.strftime('%Y%m%d%H')}\"\n        count = self._alert_counts.get(hour_key, 0)\n        if count &gt;= max_per_hour:\n            logger.debug(f\"Alert rate-limited: {alert_key} ({count}/{max_per_hour} per hour)\")\n            return False\n\n        # Update tracking\n        self._last_alerts[alert_key] = now\n        self._alert_counts[hour_key] = count + 1\n\n        return True\n\n\n# Global throttler instance\n_throttler = AlertThrottler()\n\n\ndef send_alert(config: AlertConfig, message: str, context: Dict[str, Any]) -&gt; None:\n    \"\"\"Send alert with throttling.\"\"\"\n\n    # Build throttle key\n    pipeline = context.get(\"pipeline\", \"unknown\")\n    event = context.get(\"event_type\", \"unknown\")\n    throttle_key = f\"{pipeline}:{event}\"\n\n    # Check throttling (configurable per alert)\n    throttle_minutes = config.metadata.get(\"throttle_minutes\", 15)\n    if not _throttler.should_send(throttle_key, throttle_minutes):\n        return\n\n    # ... existing send logic ...\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#yaml-config-for-enhanced-alerting","title":"YAML Config for Enhanced Alerting","text":"<pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15  # Don't repeat same alert within 15 min\n      channel: \"#data-alerts\"\n\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n      - on_threshold_breach\n    metadata:\n      throttle_minutes: 30\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#4-schema-version-tracking","title":"4. Schema Version Tracking","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem_2","title":"Problem","text":"<p>Schema changes between runs are shown in stories but not tracked historically in the catalog.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution_2","title":"Solution","text":"<p>Store schema snapshots in <code>meta_schemas</code> table for audit trail.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#catalog-schema","title":"Catalog Schema","text":"<pre><code># odibi/catalog.py\n\nSCHEMA_TABLE_SCHEMA = StructType([\n    StructField(\"table_path\", StringType(), False),      # e.g., \"silver/customers\"\n    StructField(\"schema_version\", LongType(), False),    # Auto-incrementing\n    StructField(\"schema_hash\", StringType(), False),     # MD5 of column definitions\n    StructField(\"columns\", StringType(), False),         # JSON: {\"col\": \"type\", ...}\n    StructField(\"captured_at\", TimestampType(), False),\n    StructField(\"pipeline\", StringType(), True),\n    StructField(\"node\", StringType(), True),\n    StructField(\"run_id\", StringType(), True),\n    StructField(\"columns_added\", ArrayType(StringType()), True),\n    StructField(\"columns_removed\", ArrayType(StringType()), True),\n    StructField(\"columns_type_changed\", ArrayType(StringType()), True),\n])\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation_2","title":"Implementation","text":"<pre><code># odibi/catalog.py\n\ndef track_schema(\n    self,\n    table_path: str,\n    schema: Dict[str, str],\n    pipeline: str,\n    node: str,\n    run_id: str\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Track schema version for a table.\n\n    Returns:\n        Dict with version info and detected changes\n    \"\"\"\n    schema_hash = self._hash_schema(schema)\n\n    # Get previous version\n    previous = self._get_latest_schema(table_path)\n\n    if previous and previous[\"schema_hash\"] == schema_hash:\n        # No change\n        return {\"changed\": False, \"version\": previous[\"schema_version\"]}\n\n    # Detect changes\n    changes = {}\n    if previous:\n        prev_cols = json.loads(previous[\"columns\"])\n        changes = {\n            \"columns_added\": list(set(schema.keys()) - set(prev_cols.keys())),\n            \"columns_removed\": list(set(prev_cols.keys()) - set(schema.keys())),\n            \"columns_type_changed\": [\n                col for col in schema\n                if col in prev_cols and schema[col] != prev_cols[col]\n            ]\n        }\n        new_version = previous[\"schema_version\"] + 1\n    else:\n        new_version = 1\n\n    # Insert new version\n    record = {\n        \"table_path\": table_path,\n        \"schema_version\": new_version,\n        \"schema_hash\": schema_hash,\n        \"columns\": json.dumps(schema),\n        \"captured_at\": datetime.utcnow(),\n        \"pipeline\": pipeline,\n        \"node\": node,\n        \"run_id\": run_id,\n        **changes\n    }\n\n    self._append_to_table(\"meta_schemas\", record)\n\n    return {\n        \"changed\": True,\n        \"version\": new_version,\n        \"previous_version\": previous[\"schema_version\"] if previous else None,\n        **changes\n    }\n\n\ndef get_schema_history(\n    self,\n    table_path: str,\n    limit: int = 10\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get schema version history for a table.\"\"\"\n\n    return self._query_table(\n        \"meta_schemas\",\n        filter=f\"table_path = '{table_path}'\",\n        order_by=\"schema_version DESC\",\n        limit=limit\n    )\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#cli-command","title":"CLI Command","text":"<pre><code># Show schema history\n$ odibi schema history silver/customers\n\nVersion  Captured At          Changes\n-------  -------------------  ----------------------------------------\nv5       2024-01-30 10:15:00  +loyalty_tier (new column)\nv4       2024-01-15 08:30:00  email: VARCHAR\u2192STRING (type change)\nv3       2024-01-01 12:00:00  -legacy_id (removed)\nv2       2023-12-15 09:00:00  +created_at, +updated_at\nv1       2023-12-01 10:00:00  Initial schema (12 columns)\n\n# Compare two versions\n$ odibi schema diff silver/customers --from v3 --to v5\n\n  customer_id    STRING     (unchanged)\n  email          STRING     (unchanged)\n- legacy_id      STRING     (removed in v3)\n+ loyalty_tier   STRING     (added in v5)\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#5-cross-pipeline-lineage","title":"5. Cross-Pipeline Lineage","text":""},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#problem_3","title":"Problem","text":"<p>Lineage is tracked within a single pipeline. No visibility into how pipelines connect (e.g., Bronze pipeline feeds Silver pipeline).</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#solution_3","title":"Solution","text":"<p>Track table-level lineage in the catalog, linking pipelines through shared tables.</p>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#catalog-schema_1","title":"Catalog Schema","text":"<pre><code># odibi/catalog.py\n\nLINEAGE_TABLE_SCHEMA = StructType([\n    StructField(\"source_table\", StringType(), False),    # Full path\n    StructField(\"target_table\", StringType(), False),\n    StructField(\"source_pipeline\", StringType(), True),\n    StructField(\"source_node\", StringType(), True),\n    StructField(\"target_pipeline\", StringType(), True),\n    StructField(\"target_node\", StringType(), True),\n    StructField(\"relationship\", StringType(), False),    # \"feeds\" | \"derived_from\"\n    StructField(\"last_observed\", TimestampType(), False),\n    StructField(\"run_id\", StringType(), True),\n])\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation_3","title":"Implementation","text":"<pre><code># odibi/lineage.py (extend existing)\n\nclass LineageTracker:\n    \"\"\"Track cross-pipeline lineage relationships.\"\"\"\n\n    def __init__(self, catalog: CatalogManager):\n        self.catalog = catalog\n\n    def record_lineage(\n        self,\n        read_config: Optional[ReadConfig],\n        write_config: Optional[WriteConfig],\n        pipeline: str,\n        node: str,\n        run_id: str,\n        connections: Dict[str, Any]\n    ):\n        \"\"\"Record lineage from node's read/write config.\"\"\"\n\n        if not write_config:\n            return  # No output = no lineage to record\n\n        target_table = self._resolve_table_path(write_config, connections)\n\n        # Record read source -&gt; write target\n        if read_config:\n            source_table = self._resolve_table_path(read_config, connections)\n            self._upsert_lineage(\n                source_table=source_table,\n                target_table=target_table,\n                target_pipeline=pipeline,\n                target_node=node,\n                run_id=run_id\n            )\n\n        # Record depends_on -&gt; write target (if depends_on reads from another pipeline's output)\n        # This is discovered by matching table paths\n\n    def get_upstream(self, table_path: str, depth: int = 3) -&gt; List[Dict]:\n        \"\"\"Get all upstream sources for a table.\"\"\"\n\n        upstream = []\n        visited = set()\n        queue = [(table_path, 0)]\n\n        while queue:\n            current, level = queue.pop(0)\n            if current in visited or level &gt; depth:\n                continue\n            visited.add(current)\n\n            sources = self.catalog.query(\n                \"meta_lineage\",\n                filter=f\"target_table = '{current}'\"\n            )\n\n            for source in sources:\n                upstream.append({**source, \"depth\": level})\n                queue.append((source[\"source_table\"], level + 1))\n\n        return upstream\n\n    def get_downstream(self, table_path: str, depth: int = 3) -&gt; List[Dict]:\n        \"\"\"Get all downstream consumers of a table.\"\"\"\n\n        downstream = []\n        visited = set()\n        queue = [(table_path, 0)]\n\n        while queue:\n            current, level = queue.pop(0)\n            if current in visited or level &gt; depth:\n                continue\n            visited.add(current)\n\n            targets = self.catalog.query(\n                \"meta_lineage\",\n                filter=f\"source_table = '{current}'\"\n            )\n\n            for target in targets:\n                downstream.append({**target, \"depth\": level})\n                queue.append((target[\"target_table\"], level + 1))\n\n        return downstream\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#cli-commands","title":"CLI Commands","text":"<pre><code># Trace upstream lineage\n$ odibi lineage upstream gold.customer_360\n\ngold.customer_360\n\u2514\u2500\u2500 silver.dim_customers (silver_pipeline.dedupe_customers)\n    \u2514\u2500\u2500 bronze.customers_raw (bronze_pipeline.ingest_customers)\n        \u2514\u2500\u2500 [external] azure_sql.dbo.Customers\n\n# Trace downstream lineage\n$ odibi lineage downstream bronze.customers_raw\n\nbronze.customers_raw\n\u251c\u2500\u2500 silver.dim_customers (silver_pipeline.dedupe_customers)\n\u2502   \u251c\u2500\u2500 gold.customer_360 (gold_pipeline.build_360)\n\u2502   \u2514\u2500\u2500 gold.churn_features (gold_pipeline.churn_model)\n\u2514\u2500\u2500 silver.customer_events (silver_pipeline.process_events)\n\n# Impact analysis\n$ odibi lineage impact bronze.customers_raw --if-schema-changes\n\n\u26a0\ufe0f  Schema change to bronze.customers_raw would affect:\n  - silver.dim_customers (2 pipelines depend on this)\n  - gold.customer_360 (production dashboard)\n  - gold.churn_features (ML model input)\n\n  Total: 3 downstream tables in 2 pipelines\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#story-integration","title":"Story Integration","text":"<pre><code>&lt;!-- In run_story.html, add cross-pipeline lineage section --&gt;\n\n{% if metadata.cross_pipeline_lineage %}\n&lt;div class=\"node-card\" style=\"padding: 20px;\"&gt;\n    &lt;h3&gt;\ud83d\udd17 Cross-Pipeline Lineage&lt;/h3&gt;\n    &lt;div class=\"mermaid\"&gt;\n    graph LR\n        subgraph \"This Pipeline\"\n            {% for node in metadata.nodes %}\n            {{ node.node_name }}\n            {% endfor %}\n        end\n\n        subgraph \"Upstream Pipelines\"\n            {% for source in metadata.upstream_sources %}\n            {{ source.pipeline }}.{{ source.node }}\n            {% endfor %}\n        end\n\n        subgraph \"Downstream Pipelines\"\n            {% for target in metadata.downstream_targets %}\n            {{ target.pipeline }}.{{ target.node }}\n            {% endfor %}\n        end\n\n        {% for link in metadata.cross_pipeline_links %}\n        {{ link.source }} --&gt; {{ link.target }}\n        {% endfor %}\n    &lt;/div&gt;\n&lt;/div&gt;\n{% endif %}\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#implementation-order","title":"Implementation Order","text":"<pre><code>Week 1:\n\u251c\u2500\u2500 Day 1-2: Quarantine Tables\n\u2502   \u251c\u2500\u2500 Config models\n\u2502   \u251c\u2500\u2500 split_valid_invalid()\n\u2502   \u251c\u2500\u2500 Quarantine metadata columns\n\u2502   \u251c\u2500\u2500 Integration with node.py\n\u2502   \u2514\u2500\u2500 Tests\n\u2502\n\u251c\u2500\u2500 Day 3: Quality Gates\n\u2502   \u251c\u2500\u2500 Gate config model\n\u2502   \u251c\u2500\u2500 evaluate_gate()\n\u2502   \u251c\u2500\u2500 Row count anomaly detection\n\u2502   \u251c\u2500\u2500 Integration with node.py\n\u2502   \u2514\u2500\u2500 Tests\n\nWeek 2:\n\u251c\u2500\u2500 Day 4: Alerting Enhancements\n\u2502   \u251c\u2500\u2500 Slack parity (timestamp, colors, action buttons)\n\u2502   \u251c\u2500\u2500 New alert events (quarantine, gate_block)\n\u2502   \u251c\u2500\u2500 Event-specific payloads\n\u2502   \u251c\u2500\u2500 Throttling\n\u2502   \u2514\u2500\u2500 Tests\n\u2502\n\u251c\u2500\u2500 Day 5 (half): Schema Version Tracking\n\u2502   \u251c\u2500\u2500 meta_schemas table\n\u2502   \u251c\u2500\u2500 track_schema() method\n\u2502   \u251c\u2500\u2500 CLI: odibi schema history\n\u2502   \u2514\u2500\u2500 Tests\n\u2502\n\u251c\u2500\u2500 Day 5-7: Cross-Pipeline Lineage\n\u2502   \u251c\u2500\u2500 meta_lineage table\n\u2502   \u251c\u2500\u2500 LineageTracker class\n\u2502   \u251c\u2500\u2500 get_upstream() / get_downstream()\n\u2502   \u251c\u2500\u2500 CLI commands\n\u2502   \u251c\u2500\u2500 Story integration\n\u2502   \u2514\u2500\u2500 Tests\n</code></pre>"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#success-criteria","title":"Success Criteria","text":"Enhancement Success Criteria Quarantine Bad rows written to quarantine table with metadata; valid rows continue Gates Pipeline aborts when pass rate below threshold; alert sent Alerting Slack/Teams parity; event-specific details; throttling works Schema Tracking Schema history queryable; changes detected between runs Lineage Can trace 3 levels up/down across pipelines; visualized in stories"},{"location":"roadmap/MEDALLION_ENHANCEMENTS_PLAN/#dependencies","title":"Dependencies","text":"<ul> <li>No external dependencies required</li> <li>All features build on existing infrastructure</li> <li>Backward compatible (new config fields are optional)</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/","title":"Spark Structured Streaming Support Plan","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#overview","title":"Overview","text":"<p>This document outlines the plan to fully support Spark Structured Streaming in Odibi. Currently, the <code>streaming</code> config flag creates a streaming DataFrame but the rest of the pipeline isn't designed to handle it.</p>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#current-state","title":"Current State","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#what-works","title":"What Works","text":"<ul> <li><code>SparkEngine.read()</code> uses <code>spark.readStream</code> when <code>streaming=True</code></li> <li>Merge transformer has <code>foreachBatch</code> support for streaming sources</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#whats-broken","title":"What's Broken","text":"<ol> <li>Row counting fails - <code>df.count()</code> doesn't work on streaming DataFrames</li> <li>No streaming write - <code>engine.write()</code> doesn't handle streaming DataFrames</li> <li>No checkpoint configuration - Streaming requires checkpoint locations</li> <li>No trigger configuration - No way to specify processing intervals</li> <li>No output mode configuration - append/update/complete modes not exposed</li> <li>Validation/contracts fail - Can't run validations on streaming data</li> <li>Sample collection fails - Can't get samples from streaming DataFrames</li> </ol>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#11-add-streamingconfig-to-configpy","title":"1.1 Add StreamingConfig to config.py","text":"<pre><code>class StreamingConfig(BaseModel):\n    \"\"\"Configuration for Spark Structured Streaming.\"\"\"\n\n    enabled: bool = Field(\n        default=False,\n        description=\"Enable streaming mode for this node\"\n    )\n    checkpoint_location: Optional[str] = Field(\n        default=None,\n        description=\"Path for streaming checkpoints. Required for fault tolerance.\"\n    )\n    trigger: Optional[str] = Field(\n        default=None,\n        description=\"Trigger interval. Options: 'once', '10 seconds', '1 minute', 'availableNow'\"\n    )\n    output_mode: Literal[\"append\", \"update\", \"complete\"] = Field(\n        default=\"append\",\n        description=\"Output mode for streaming writes\"\n    )\n    watermark: Optional[str] = Field(\n        default=None,\n        description=\"Watermark column and delay, e.g., 'event_time, 10 minutes'\"\n    )\n    processing_time: Optional[str] = Field(\n        default=None,\n        description=\"Processing time trigger, e.g., '10 seconds'\"\n    )\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#12-move-streaming-from-readconfig-to-nodeconfig","title":"1.2 Move streaming from ReadConfig to NodeConfig","text":"<p>The <code>streaming</code> flag should be at the node level since it affects the entire execution flow:</p> <pre><code>nodes:\n  - name: ingest_events\n    streaming:\n      enabled: true\n      checkpoint_location: \"/checkpoints/ingest_events\"\n      trigger: \"10 seconds\"\n      output_mode: append\n    read:\n      connection: kafka\n      format: kafka\n      options:\n        subscribe: events_topic\n    write:\n      connection: silver\n      format: delta\n      table: events_stream\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#13-update-nodeexecutor-to-detect-streaming-mode","title":"1.3 Update NodeExecutor to detect streaming mode","text":"<pre><code>def execute(self, config: NodeConfig, ...):\n    is_streaming = config.streaming and config.streaming.enabled\n\n    if is_streaming:\n        return self._execute_streaming(config, ...)\n    else:\n        return self._execute_batch(config, ...)\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-2-streaming-aware-execution","title":"Phase 2: Streaming-Aware Execution","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#21-skip-incompatible-operations-for-streaming","title":"2.1 Skip incompatible operations for streaming","text":"Operation Streaming Behavior Row counting Skip (return None) Sample collection Skip Contracts/validation Skip or use foreachBatch Schema capture Use schema from DataFrame HWM updates Not applicable (Kafka offsets managed by checkpoints)"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#22-add-streaming-write-to-sparkengine","title":"2.2 Add streaming write to SparkEngine","text":"<pre><code>def write(self, df, connection, format, ..., streaming_config=None):\n    if df.isStreaming:\n        if not streaming_config:\n            raise ValueError(\"Streaming DataFrame requires streaming_config\")\n\n        writer = df.writeStream \\\n            .format(format) \\\n            .outputMode(streaming_config.output_mode) \\\n            .option(\"checkpointLocation\", streaming_config.checkpoint_location)\n\n        if streaming_config.trigger == \"once\":\n            writer = writer.trigger(once=True)\n        elif streaming_config.trigger == \"availableNow\":\n            writer = writer.trigger(availableNow=True)\n        elif streaming_config.trigger:\n            writer = writer.trigger(processingTime=streaming_config.trigger)\n\n        if table:\n            query = writer.toTable(table)\n        else:\n            query = writer.start(path)\n\n        return {\"streaming_query\": query}\n    else:\n        # existing batch logic\n        ...\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#23-handle-streaming-query-lifecycle","title":"2.3 Handle streaming query lifecycle","text":"<pre><code>def _execute_streaming(self, config, ...):\n    # Read (returns streaming DataFrame)\n    df = self._execute_read_phase(config, ...)\n\n    # Transform (must be streaming-compatible)\n    df = self._execute_transform_phase(config, df, ...)\n\n    # Write (starts streaming query)\n    query_info = self._execute_write_phase(config, df, ...)\n\n    # Return query handle for management\n    return NodeResult(\n        node_name=config.name,\n        success=True,\n        metadata={\n            \"streaming\": True,\n            \"query_id\": query_info[\"streaming_query\"].id,\n            \"query_name\": query_info[\"streaming_query\"].name,\n        }\n    )\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-3-streaming-sources-sinks","title":"Phase 3: Streaming Sources &amp; Sinks","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#31-kafka-connection-type","title":"3.1 Kafka Connection Type","text":"<pre><code>class KafkaConnection(BaseConnection):\n    \"\"\"Kafka connection for streaming.\"\"\"\n\n    bootstrap_servers: str\n    security_protocol: str = \"PLAINTEXT\"\n    sasl_mechanism: Optional[str] = None\n    sasl_username: Optional[str] = None\n    sasl_password: Optional[str] = None\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        return {\n            \"kafka.bootstrap.servers\": self.bootstrap_servers,\n            \"kafka.security.protocol\": self.security_protocol,\n            ...\n        }\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#32-event-hubs-connection-type","title":"3.2 Event Hubs Connection Type","text":"<pre><code>class EventHubsConnection(BaseConnection):\n    \"\"\"Azure Event Hubs connection for streaming.\"\"\"\n\n    connection_string: str\n    consumer_group: str = \"$Default\"\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        return {\n            \"eventhubs.connectionString\": self.connection_string,\n            \"eventhubs.consumerGroup\": self.consumer_group,\n        }\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#33-supported-streaming-formats","title":"3.3 Supported Streaming Formats","text":"Source Read Write Kafka \u2713 \u2713 Event Hubs \u2713 \u2713 Delta \u2713 \u2713 Rate (testing) \u2713 - File (continuous) \u2713 \u2713"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-4-streaming-transforms","title":"Phase 4: Streaming Transforms","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#41-watermarking-support","title":"4.1 Watermarking Support","text":"<pre><code>nodes:\n  - name: windowed_aggregation\n    streaming:\n      enabled: true\n      watermark: \"event_time, 10 minutes\"\n    transform:\n      steps:\n        - type: sql\n          query: |\n            SELECT\n              window(event_time, '5 minutes') as window,\n              COUNT(*) as event_count\n            FROM {input}\n            GROUP BY window(event_time, '5 minutes')\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#42-streaming-compatible-transforms","title":"4.2 Streaming-Compatible Transforms","text":"Transform Streaming Support SQL (stateless) \u2713 SQL (windowed aggregation) \u2713 with watermark filter_rows \u2713 derive_columns \u2713 join (stream-static) \u2713 join (stream-stream) \u2713 with watermark deduplicate \u2713 with watermark aggregate \u2713 with watermark"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#43-incompatible-transforms-fail-fast","title":"4.3 Incompatible Transforms (Fail Fast)","text":"<ul> <li><code>sort</code> (requires complete data)</li> <li><code>limit</code> (requires complete data)</li> <li><code>distinct</code> without watermark</li> <li>Any transform requiring <code>.collect()</code></li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-5-monitoring-management","title":"Phase 5: Monitoring &amp; Management","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#51-query-status-tracking","title":"5.1 Query Status Tracking","text":"<pre><code>class StreamingQueryManager:\n    \"\"\"Manage active streaming queries.\"\"\"\n\n    def __init__(self, spark):\n        self.spark = spark\n\n    def list_queries(self) -&gt; List[Dict]:\n        return [\n            {\n                \"id\": q.id,\n                \"name\": q.name,\n                \"status\": q.status,\n                \"recent_progress\": q.recentProgress,\n            }\n            for q in self.spark.streams.active\n        ]\n\n    def stop_query(self, query_id: str):\n        for q in self.spark.streams.active:\n            if q.id == query_id:\n                q.stop()\n                return True\n        return False\n\n    def await_termination(self, query_id: str, timeout: Optional[int] = None):\n        for q in self.spark.streams.active:\n            if q.id == query_id:\n                q.awaitTermination(timeout)\n                return\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#52-cli-commands","title":"5.2 CLI Commands","text":"<pre><code># List active streaming queries\nodibi streaming list\n\n# Stop a streaming query\nodibi streaming stop &lt;query-id&gt;\n\n# Show query progress\nodibi streaming status &lt;query-id&gt;\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#53-metrics-integration","title":"5.3 Metrics Integration","text":"<pre><code># OpenTelemetry metrics for streaming\nstreaming_records_processed = meter.create_counter(\n    \"odibi.streaming.records_processed\",\n    description=\"Records processed by streaming query\"\n)\n\nstreaming_batch_duration = meter.create_histogram(\n    \"odibi.streaming.batch_duration_ms\",\n    description=\"Duration of each micro-batch\"\n)\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#phase-6-testing","title":"Phase 6: Testing","text":""},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#61-unit-tests","title":"6.1 Unit Tests","text":"<ul> <li>Test streaming config validation</li> <li>Test checkpoint path generation</li> <li>Test trigger parsing</li> <li>Mock streaming DataFrame handling</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#62-integration-tests-spark-required","title":"6.2 Integration Tests (Spark Required)","text":"<ul> <li>Rate source \u2192 Delta sink</li> <li>File source \u2192 File sink (continuous)</li> <li>Watermark and windowed aggregation</li> <li>Stream-static join</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#63-e2e-tests-kafka-required","title":"6.3 E2E Tests (Kafka Required)","text":"<ul> <li>Kafka \u2192 Delta pipeline</li> <li>Event Hubs \u2192 Delta pipeline</li> <li>Exactly-once semantics verification</li> </ul>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#implementation-order","title":"Implementation Order","text":"Phase Effort Priority Dependencies 1.1 StreamingConfig 2h P0 None 1.2 Move to NodeConfig 1h P0 1.1 1.3 Detect streaming mode 2h P0 1.2 2.1 Skip incompatible ops 2h P0 1.3 2.2 Streaming write 4h P0 2.1 2.3 Query lifecycle 3h P0 2.2 3.1 Kafka connection 3h P1 2.3 3.2 Event Hubs connection 2h P1 2.3 4.1 Watermarking 2h P1 2.3 4.2 Transform compatibility 4h P1 4.1 5.1 Query manager 3h P2 2.3 5.2 CLI commands 2h P2 5.1 5.3 Metrics 2h P2 5.1 6.x Testing 8h P1 All <p>Total Estimate: ~40 hours</p>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#example-complete-streaming-pipeline","title":"Example: Complete Streaming Pipeline","text":"<pre><code># project.yaml\nconnections:\n  kafka:\n    type: kafka\n    bootstrap_servers: \"localhost:9092\"\n\n  silver:\n    type: databricks\n    catalog: main\n    schema: silver\n\n# pipeline.yaml\npipelines:\n  - pipeline: realtime_events\n    description: \"Process events in real-time from Kafka to Delta\"\n\n    nodes:\n      - name: ingest_events\n        description: \"Ingest raw events from Kafka\"\n        streaming:\n          enabled: true\n          checkpoint_location: \"abfss://checkpoints/ingest_events\"\n          trigger: \"10 seconds\"\n          output_mode: append\n          watermark: \"event_time, 5 minutes\"\n\n        read:\n          connection: kafka\n          format: kafka\n          options:\n            subscribe: raw_events\n            startingOffsets: earliest\n\n        transform:\n          steps:\n            - type: sql\n              query: |\n                SELECT\n                  CAST(key AS STRING) as event_key,\n                  CAST(value AS STRING) as event_json,\n                  timestamp as kafka_timestamp,\n                  from_json(CAST(value AS STRING), 'event_type STRING, event_time TIMESTAMP, payload STRING') as parsed\n                FROM {input}\n\n            - type: derive\n              columns:\n                event_type: \"parsed.event_type\"\n                event_time: \"parsed.event_time\"\n                payload: \"parsed.payload\"\n\n        write:\n          connection: silver\n          format: delta\n          table: events_stream\n          partition_by: [\"event_type\"]\n</code></pre>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#open-questions","title":"Open Questions","text":"<ol> <li>Should streaming nodes run indefinitely or support batch-like \"run once\"?</li> <li>Spark supports <code>trigger(once=True)</code> for processing all available data then stopping</li> <li> <p>Useful for testing and scheduled streaming jobs</p> </li> <li> <p>How to handle streaming in the Pipeline orchestrator?</p> </li> <li>Streaming nodes don't \"complete\" in the traditional sense</li> <li> <p>May need separate <code>run_streaming()</code> method</p> </li> <li> <p>Should we support foreachBatch for custom sinks?</p> </li> <li>Enables writing to non-streaming sinks (JDBC, etc.)</li> <li> <p>More complex but more flexible</p> </li> <li> <p>How to expose streaming metrics in Stories?</p> </li> <li>Streaming runs continuously, no single \"run\" to report on</li> <li>May need real-time dashboard integration</li> </ol>"},{"location":"roadmap/STREAMING_SUPPORT_PLAN/#references","title":"References","text":"<ul> <li>Spark Structured Streaming Guide</li> <li>Delta Lake Streaming</li> <li>Kafka + Spark Integration</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/","title":"System Catalog Integration Plan","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#overview","title":"Overview","text":"<p>This plan addresses gaps in the system catalog integration to achieve the goal: \"Declare YAML, run pipeline, everything works automatically.\"</p> <p>Currently only <code>meta_state</code> works correctly. All other system tables have integration gaps.</p>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-1-fix-critical-bugs-done","title":"Phase 1: Fix Critical Bugs (Done)","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#11-fix-state_manager-in-nodeexecutor","title":"1.1 Fix state_manager in NodeExecutor \u2705","text":"<ul> <li>Issue: <code>state_manager</code> wasn't passed to <code>NodeExecutor</code>, breaking <code>skip_if_unchanged</code></li> <li>Fix: Added <code>state_manager</code> parameter to <code>NodeExecutor.__init__</code> and pass it from <code>Node</code> class</li> <li>Status: Completed this session, needs commit</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-2-auto-registration-on-run","title":"Phase 2: Auto-Registration on Run","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#21-auto-register-pipelines-and-nodes","title":"2.1 Auto-register pipelines and nodes","text":"<ul> <li>Location: <code>PipelineManager.run()</code></li> <li>Action: Before executing, call <code>register_pipeline()</code> and <code>register_node()</code> for each pipeline/node being run</li> <li>Behavior: Idempotent upsert - safe to call on every run</li> <li>Result: <code>meta_pipelines</code> and <code>meta_nodes</code> populated automatically</li> </ul> <pre><code>def run(self, pipelines=None, ...):\n    # Auto-register before execution\n    if self.catalog_manager:\n        for name in pipeline_names:\n            config = self._pipelines[name].config\n            self.catalog_manager.register_pipeline(config, self.project_config)\n            for node in config.nodes:\n                self.catalog_manager.register_node(config.pipeline, node)\n\n    # ... rest of run logic\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-3-wire-catalog-logging-into-pipeline-flow","title":"Phase 3: Wire Catalog Logging into Pipeline Flow","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#31-log-runs-from-pipelinenodeexecutor","title":"3.1 Log runs from Pipeline/NodeExecutor","text":"<ul> <li>Location: <code>Pipeline._execute_node()</code> or <code>NodeExecutor</code> after node completion</li> <li>Action: Call <code>catalog_manager.log_run()</code> with execution results</li> <li>Result: <code>meta_runs</code> populated from YAML flow (not just <code>Node</code> class)</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#32-register-assets-on-write","title":"3.2 Register assets on write","text":"<ul> <li>Location: <code>NodeExecutor._execute_write_phase()</code> after successful write</li> <li>Action: Call <code>catalog_manager.register_asset()</code> with table details</li> <li>Result: <code>meta_tables</code> populated automatically</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#33-track-schema-changes","title":"3.3 Track schema changes","text":"<ul> <li>Location: <code>NodeExecutor._execute_write_phase()</code> after write</li> <li>Action:</li> <li>Get current schema from written data</li> <li>Call <code>catalog_manager.track_schema()</code> if schema differs from previous</li> <li>Result: <code>meta_schemas</code> tracks schema evolution</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#34-log-pattern-usage","title":"3.4 Log pattern usage","text":"<ul> <li>Location: <code>Pipeline._execute_node()</code> when pattern-based nodes execute</li> <li>Action: Call <code>catalog_manager.log_pattern()</code> with pattern config</li> <li>Result: <code>meta_patterns</code> populated</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#35-record-lineage","title":"3.5 Record lineage","text":"<ul> <li>Location: <code>Pipeline._execute_node()</code> after node completion</li> <li>Action: Extract source/target from node config, call <code>record_lineage()</code></li> <li>Result: <code>meta_lineage</code> populated automatically</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#36-create-and-log-metrics","title":"3.6 Create and log metrics","text":"<ul> <li>Location: <code>CatalogManager</code> + <code>Pipeline</code></li> <li>Action:</li> <li>Create <code>log_metrics()</code> method in <code>CatalogManager</code></li> <li>Call from Pipeline with execution stats</li> <li>Result: <code>meta_metrics</code> populated</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-4-cleanupremoval-methods","title":"Phase 4: Cleanup/Removal Methods","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#41-remove-pipeline","title":"4.1 Remove pipeline","text":"<pre><code>def remove_pipeline(self, pipeline_name: str) -&gt; int:\n    \"\"\"Remove pipeline and cascade to nodes, state entries.\n    Returns count of deleted entries.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#42-remove-node","title":"4.2 Remove node","text":"<pre><code>def remove_node(self, pipeline_name: str, node_name: str) -&gt; int:\n    \"\"\"Remove node and associated state entries.\n    Returns count of deleted entries.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#43-cleanup-orphans","title":"4.3 Cleanup orphans","text":"<pre><code>def cleanup_orphans(self, current_config: ProjectConfig) -&gt; Dict[str, int]:\n    \"\"\"Compare catalog against current config, remove stale entries.\n    Returns dict of {table: deleted_count}.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#44-clear-state","title":"4.4 Clear state","text":"<pre><code>def clear_state(self, key_pattern: str) -&gt; int:\n    \"\"\"Remove state entries matching pattern (supports wildcards).\n    Returns count of deleted entries.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-5-listquery-methods-on-pipelinemanager","title":"Phase 5: List/Query Methods on PipelineManager","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#51-list-methods","title":"5.1 List methods","text":"<pre><code>def list_pipelines(self) -&gt; pd.DataFrame:\n    \"\"\"List all registered pipelines.\"\"\"\n\ndef list_nodes(self, pipeline: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"List nodes, optionally filtered by pipeline.\"\"\"\n\ndef list_runs(\n    self,\n    pipeline: Optional[str] = None,\n    node: Optional[str] = None,\n    status: Optional[str] = None,\n    limit: int = 10\n) -&gt; pd.DataFrame:\n    \"\"\"List recent runs with optional filters.\"\"\"\n\ndef list_tables(self) -&gt; pd.DataFrame:\n    \"\"\"List registered assets from meta_tables.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#52-state-methods","title":"5.2 State methods","text":"<pre><code>def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get specific state entry (HWM, content hash, etc.).\"\"\"\n\ndef get_all_state(self, prefix: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"Get all state entries, optionally filtered by key prefix.\"\"\"\n\ndef clear_state(self, key: str) -&gt; bool:\n    \"\"\"Remove a state entry. Returns True if deleted.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#53-schemalineage-methods","title":"5.3 Schema/Lineage methods","text":"<pre><code>def get_schema_history(\n    self,\n    table: str,  # Accepts friendly name, resolved automatically\n    limit: int = 5\n) -&gt; pd.DataFrame:\n    \"\"\"Get schema version history for a table.\"\"\"\n\ndef get_lineage(\n    self,\n    table: str,\n    direction: str = \"both\"  # \"upstream\", \"downstream\", \"both\"\n) -&gt; pd.DataFrame:\n    \"\"\"Get lineage for a table.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#54-statshealth-methods","title":"5.4 Stats/Health methods","text":"<pre><code>def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n    \"\"\"Get last run status, duration, timestamp.\"\"\"\n\ndef get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n    \"\"\"Get avg duration, row counts, success rate over period.\"\"\"\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#phase-6-smart-path-resolution","title":"Phase 6: Smart Path Resolution","text":""},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#61-implement-_resolve_table_path","title":"6.1 Implement <code>_resolve_table_path()</code>","text":"<p>All query methods accept user-friendly identifiers: - Relative path: <code>\"bronze/OEE/vw_OSMPerformanceOEE\"</code> - Registered table: <code>\"test.vw_OSMPerformanceOEE\"</code> - Node name: <code>\"opsvisdata_vw_OSMPerformanceOEE\"</code> - Full path: <code>\"abfss://...\"</code> (used as-is)</p> <pre><code>def _resolve_table_path(self, identifier: str) -&gt; str:\n    # 1. Full path - use as-is\n    if self._is_full_path(identifier):\n        return identifier\n\n    # 2. Check meta_tables for registered name\n    if self.catalog_manager:\n        resolved = self.catalog_manager.resolve_table_path(identifier)\n        if resolved:\n            return resolved\n\n    # 3. Check if it's a node name\n    for pipeline in self._pipelines.values():\n        for node in pipeline.config.nodes:\n            if node.name == identifier and node.write:\n                conn = self.connections[node.write.connection]\n                return conn.get_path(node.write.path or node.write.table)\n\n    # 4. Resolve using system connection as fallback\n    sys_conn = self.connections.get(self.project_config.system.connection)\n    return sys_conn.get_path(identifier) if sys_conn else identifier\n</code></pre>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#implementation-order","title":"Implementation Order","text":"<ol> <li>Phase 2 - Auto-registration (quick win, enables other phases)</li> <li>Phase 3.1 - Log runs (most visible improvement)</li> <li>Phase 5.1-5.2 - List and state methods (immediate usability)</li> <li>Phase 6 - Path resolution (needed for other methods)</li> <li>Phase 3.2-3.5 - Remaining catalog logging</li> <li>Phase 4 - Cleanup methods</li> <li>Phase 5.3-5.4 - Advanced query methods</li> <li>Phase 3.6 - Metrics (lowest priority)</li> </ol>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#testing-requirements","title":"Testing Requirements","text":"<ul> <li>Unit tests for each new method</li> <li>Integration test: run pipeline, verify all 9 tables populated</li> <li>Test cleanup methods don't break active pipelines</li> <li>Test path resolution with all identifier formats</li> <li>Test with both Spark and Pandas engines</li> </ul>"},{"location":"roadmap/SYSTEM_CATALOG_INTEGRATION_PLAN/#files-to-modify","title":"Files to Modify","text":"File Changes <code>odibi/pipeline.py</code> Auto-register, log_run, list methods, path resolution <code>odibi/node.py</code> Already fixed (state_manager), add register_asset/track_schema calls <code>odibi/catalog.py</code> Add remove_*, cleanup_orphans, log_metrics methods <code>tests/unit/test_pipeline_manager.py</code> New tests for list/query methods <code>tests/integration/test_catalog_integration.py</code> End-to-end catalog tests"},{"location":"roadmap/ease_of_life_proposals/","title":"Odibi Feature Specification: The \"Orgasmic\" Upgrade","text":"<p>2: 3: This document details the specification and implementation plan for the top 3 \"Ease of Life\" features. It is the primary reference for the implementation phase. 4: 5: --- 6: 7: ## 1. Validation (The \"Quality Gate\") \ud83d\udee1\ufe0f 8: 9: ### \ud83d\udca1 Concept 10: A declarative \"Validation Block\" on the Node level. Instead of writing manual SQL queries for common checks, users declare their intent. Odibi handles the execution, reporting, and alerting. 11: 12: ### \ud83d\udcdd User Story 13: &gt; \"As a Data Engineer, I want to ensure my 'Silver' tables never contain duplicate IDs or null timestamps, so that downstream reports are accurate. I want to define this in one line of config, not 20 lines of SQL.\" 14: 15: ### \u2699\ufe0f YAML Specification 16: <code>yaml 17: - name: \"dim_customers\" 18:   depends_on: [\"clean_customers\"] 19: 20:   validation: 21:     mode: \"fail\"          # Options: fail (stop pipeline), warn (log only) 22:     on_fail: \"alert\"      # Options: alert (send slack), ignore 23:     tests: 24:       # Built-in Test Types 25:       - type: \"not_null\" 26:         columns: [\"customer_id\", \"email\"] 27:   28:       - type: \"unique\" 29:         columns: [\"customer_id\"] 30:   31:       - type: \"accepted_values\" 32:         column: \"status\" 33:         values: [\"ACTIVE\", \"INACTIVE\", \"CHURNED\"] 34:   35:       - type: \"row_count\" 36:         min: 100 37:   38:       # Custom Logic 39:       - type: \"custom_sql\" 40:         name: \"age_sanity_check\" 41:         condition: \"age &gt;= 18 AND age &lt; 120\" 42:         threshold: 0.01   # Allow 1% failure rate before breaking 43:</code> 44: 45: ### \ud83c\udfd7\ufe0f Technical Implementation Plan 46: 1.  [x] Pydantic Models (<code>odibi/config.py</code>): 47:     *   Update <code>ValidationConfig</code> to support the new structure (<code>mode</code>, <code>on_fail</code>, <code>tests</code> list). 48:     *   Create <code>TestConfig</code> polymorphic model (discriminator on <code>type</code>). 49: 2.  [x] Validation Logic (<code>odibi/validation/engine.py</code>): 50:     *   Create a new <code>Validator</code> class. 51:     *   Implement methods for each test type (<code>check_not_null</code>, <code>check_unique</code>, etc.) that generate SQL/Pandas logic. 52:     *   Spark Optimization: Use <code>count(CASE WHEN ...)</code> to batch multiple checks into a single pass if possible. 53: 3.  [x] Node Integration (<code>odibi/node.py</code>): 54:     *   Update <code>_execute_validation_phase</code> to use the new <code>Validator</code>. 55:     *   Handle <code>mode: fail</code> vs <code>warn</code> logic. 56: 4.  [x] Reporting (<code>odibi/story/</code>): 57:     *   Capture validation results (pass/fail counts) in <code>NodeResult</code>. 58:     *   Render a \"Data Quality\" section in the Story HTML. 59: 60: --- 61: 62: ## 2. Incremental Loading (The \"Auto-Pilot\") \u2728 63: 64: ### \ud83d\udca1 Concept 65: \"Stateful\" High-Water-Mark (HWM) management. Odibi tracks the last processed state (timestamp or ID) in a persistent backend and automatically filters input data. 66: 67: ### \ud83d\udcdd User Story 68: &gt; \"As a Data Engineer, I want to ingest only new orders from Postgres into Delta Lake. I don't want to manually query the target table or manage a state file. I just want to say 'keep it in sync'.\" 69: 70: ### \u2699\ufe0f YAML Specification 71: <code>yaml 72: - name: \"orders_incremental\" 73:   read: 74:     connection: \"postgres_prod\" 75:     format: \"sql\" 76:     # The base query (Odibi appends the WHERE clause) 77:     query: \"SELECT * FROM public.orders\" 78:   79:     incremental: 80:       mode: \"stateful\"                  # The new magic mode 81:       key_column: \"updated_at\" 82:       fallback_column: \"created_at\"     # If updated_at is null, check this 83:       watermark_lag: \"2h\"               # Safety buffer (overlaps window) 84:       state_key: \"orders_prod_ingest\"   # Unique ID for state tracking 85:</code> 86: 87: ### \ud83c\udfd7\ufe0f Technical Implementation Plan 88: 1.  [x] State Backend (<code>odibi/state.py</code>): 89:     *   Done: <code>DeltaStateBackend</code> is ready. 90: 2.  [x] Config (<code>odibi/config.py</code>): 91:     *   Update <code>IncrementalConfig</code> to add <code>mode</code>, <code>state_key</code>, <code>watermark_lag</code>. 92: 3.  [x] Read Logic (<code>odibi/node.py</code> - <code>_execute_read</code>): 93:     *   Inject <code>StateManager</code>. 94:     *   Retrieve last HWM from <code>state_manager.get_last_run_state(key)</code>. 95:     *   Construct <code>WHERE</code> clause: <code>column &gt; last_hwm - lag</code>. 96:     *   Critical: After successful execution, update the State with the new <code>max(key_column)</code> from the fetched data. 97: 98: --- 99: 100: ## 3. Schema Management (The \"Drift Handler\") \ud83d\udee1\ufe0f 101: 102: ### \ud83d\udca1 Concept 103: Declarative handling of schema evolution. Instead of crashing on unknown columns, users define a policy (<code>enforce</code> vs <code>evolve</code>). 104: 105: ### \ud83d\udcdd User Story 106: &gt; \"As a Data Engineer, I work with a JSON API that frequently adds new fields. I want my 'Bronze' layer to automatically add these new columns to the table without crashing the pipeline.\" 107: 108: ### \u2699\ufe0f YAML Specification 109: <code>yaml 110: - name: \"web_events\" 111:   depends_on: [\"raw_json_logs\"] 112:   113:   schema: 114:     mode: \"evolve\"                    # Options: enforce, evolve 115:   116:     # What to do when input has columns NOT in target 117:     on_new_columns: \"add_nullable\"    # ignore, fail, add_nullable 118:   119:     # What to do when input is MISSING columns expected by target 120:     on_missing_columns: \"fill_null\"   # fail, fill_null 121:   122:     # Explicit expectations (optional) 123:     expected: 124:       event_id: { type: \"string\", nullable: false } 125:</code> 126: 127: ### \ud83c\udfd7\ufe0f Technical Implementation Plan 128: 1.  [x] Config (<code>odibi/config.py</code>): 129:     *   Add <code>SchemaPolicyConfig</code> model. 130: 2.  [x] Schema Logic (<code>odibi/engine/base.py</code> &amp; subclasses): 131:     *   Add <code>harmonize_schema(df, target_schema, policy)</code> method. 132:     *   Spark: Use <code>df.withColumn</code> to add missing cols. Use <code>mergeSchema</code> option for Delta writes. 133:     *   Pandas: Use <code>reindex</code> or direct assignment. 134: 3.  [x] Node Integration (<code>odibi/node.py</code>): 135:     *   Before Write Phase: Fetch target table schema (if exists). 136:     *   Compare <code>current_df.schema</code> vs <code>target_schema</code>. 137:     *   Apply harmonization logic based on policy. 138: 139: --- 140: 141: ## \ud83d\udd2e Future Scope: Other \"Ease of Life\" Features 142: 143: These are prioritized for subsequent iterations. 144: 145: *   \"Auto-Optimize\": <code>write.auto_optimize: true</code>. Automatically runs <code>OPTIMIZE</code> and <code>VACUUM</code> on Delta tables based on a \"reporting\" or \"ingest\" profile. 146: *   Privacy Suite: <code>privacy.anonymize: true</code>. Config-driven PII hashing/masking based on <code>ColumnMetadata</code>. 147: *   New Transformers: 148:     *   <code>normalize_json</code>: Deep flattening. 149:     *   <code>sessionize</code>: Window-based session creation. 150:     *   <code>geocode</code>: IP-to-Geo enrichment. 151: 152: --- 153: 154: ## \u2705 Validation Strategy 155: 156: For every feature above, the definition of \"Done\" includes: 157: 1.  Unit Tests: Verify logic in isolation (e.g., <code>test_validation_engine.py</code>). 158: 2.  Integration Tests: End-to-end pipeline run with a local Delta/DuckDB setup. 159: 3.  Story Check: Verify that the feature's activity is correctly logged in the Odibi Story (HTML report). 160: 161: --- 162:</p>"},{"location":"semantics/","title":"Semantic Layer","text":"<p>The Odibi Semantic Layer provides a unified interface for defining and querying business metrics. Define metrics once, query them across any dimension combination.</p>"},{"location":"semantics/#how-it-fits-into-odibi","title":"How It Fits Into Odibi","text":"<p>The semantic layer is a separate module from the core pipeline YAML. It's designed for:</p> <ol> <li>Ad-hoc metric queries via Python API</li> <li>Scheduled metric materialization to pre-compute aggregates</li> <li>Self-service analytics where business users query by metric name</li> </ol> <p>It does NOT replace the pipeline YAML - instead, it works alongside it: - Pipelines build your fact and dimension tables - The semantic layer queries those tables using metric definitions</p> <pre><code>flowchart TB\n    subgraph Pipeline[\"Odibi Pipelines (YAML)\"]\n        R[Read Sources]\n        D[Build Dimensions]\n        F[Build Facts]\n        A[Build Aggregates]\n    end\n\n    subgraph Data[\"Data Layer\"]\n        DIM[(dim_customer&lt;br&gt;dim_product&lt;br&gt;dim_date)]\n        FACT[(fact_orders)]\n        AGG[(agg_daily_sales)]\n    end\n\n    subgraph Semantic[\"Semantic Layer (Python API)\"]\n        M[Metric Definitions]\n        Q[SemanticQuery]\n        MAT[Materializer]\n    end\n\n    subgraph Output[\"Output\"]\n        DF[DataFrame Results]\n        TABLE[Materialized Tables]\n    end\n\n    R --&gt; D --&gt; DIM\n    R --&gt; F --&gt; FACT\n    FACT --&gt; A --&gt; AGG\n\n    DIM --&gt; M\n    FACT --&gt; M\n    M --&gt; Q --&gt; DF\n    M --&gt; MAT --&gt; TABLE\n\n    style Pipeline fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Semantic fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"semantics/#when-to-use-what","title":"When to Use What","text":"Use Case Solution Build dimension tables Use <code>transformer: dimension</code> in pipeline YAML Build fact tables Use <code>transformer: fact</code> in pipeline YAML Build scheduled aggregates Use <code>transformer: aggregation</code> in pipeline YAML Ad-hoc metric queries Use Semantic Layer Python API Self-service BI metrics Use Semantic Layer with materialization"},{"location":"semantics/#configuration","title":"Configuration","text":"<p>The semantic layer is configured via Python, not YAML. You can load config from a YAML file if desired:</p> <pre><code>from odibi.semantics import SemanticQuery, Materializer, parse_semantic_config\nimport yaml\n\n# Load from YAML (optional - can also build programmatically)\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Query interface\nquery = SemanticQuery(config)\nresult = query.execute(\"revenue BY region\", context)\n\n# Materialization\nmaterializer = Materializer(config)\nmaterializer.execute(\"monthly_revenue\", context)\n</code></pre>"},{"location":"semantics/#example-semantic_configyaml","title":"Example semantic_config.yaml","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  - name: avg_order_value\n    expr: \"AVG(total_amount)\"\n    source: fact_orders\n\ndimensions:\n  - name: region\n    source: fact_orders\n    column: region\n\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy: [year, quarter, month_name]\n\n  - name: category\n    source: dim_product\n    column: category\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n</code></pre>"},{"location":"semantics/#core-concepts","title":"Core Concepts","text":""},{"location":"semantics/#metrics","title":"Metrics","text":"<p>Metrics are measurable values that can be aggregated across dimensions:</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"semantics/#dimensions","title":"Dimensions","text":"<p>Dimensions are attributes for grouping and filtering:</p> <pre><code>dimensions:\n  - name: order_date\n    source: dim_date\n    hierarchy: [year, quarter, month, full_date]\n</code></pre>"},{"location":"semantics/#queries","title":"Queries","text":"<p>Query the semantic layer with a simple string syntax:</p> <pre><code>result = query.execute(\"revenue, order_count BY region, month\", context)\n</code></pre>"},{"location":"semantics/#materializations","title":"Materializations","text":"<p>Pre-compute metrics at specific grain:</p> <pre><code>materializations:\n  - name: monthly_revenue\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre>"},{"location":"semantics/#quick-start","title":"Quick Start","text":""},{"location":"semantics/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is through the unified <code>Project</code> API, which automatically resolves table paths from your connections:</p> <pre><code>from odibi import Project\n\n# Load project - semantic layer inherits connections from odibi.yaml\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics - tables are auto-loaded from connections\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\n\n# With filters\nresult = project.query(\"revenue BY category WHERE region = 'North'\")\n</code></pre> <p>odibi.yaml with semantic layer:</p> <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n      - name: dim_customer\n        write:\n          connection: gold\n          table: dim_customer\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n</code></pre> <p>The <code>source: $build_warehouse.fact_orders</code> notation tells the semantic layer to: 1. Look up the <code>fact_orders</code> node in the <code>build_warehouse</code> pipeline 2. Read its <code>write.connection</code> and <code>write.table</code> config 3. Auto-load the Delta table when queried</p> <p>Alternative: connection.path notation</p> <p>For external tables not managed by pipelines, use <code>connection.path</code>:</p> <pre><code>source: gold.fact_orders              # \u2192 /mnt/data/gold/fact_orders\nsource: gold.oee/plant_a/metrics      # \u2192 /mnt/data/gold/oee/plant_a/metrics (nested paths work!)\n</code></pre> <p>The split happens on the first dot only, so subdirectories are supported.</p>"},{"location":"semantics/#option-b-manual-setup","title":"Option B: Manual Setup","text":""},{"location":"semantics/#1-build-your-data-with-pipelines","title":"1. Build Your Data with Pipelines","text":"<p>First, use standard Odibi pipelines to build your star schema:</p> <pre><code># odibi.yaml - Build the data layer\nproject: my_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_star_schema\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns: [name, region]\n        write:\n          connection: warehouse\n          path: dim_customer\n\n      - name: fact_orders\n        depends_on: [dim_customer]\n        read:\n          connection: staging\n          path: orders\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n        write:\n          connection: warehouse\n          path: fact_orders\n</code></pre>"},{"location":"semantics/#2-define-semantic-layer","title":"2. Define Semantic Layer","text":"<p>Create a semantic config (Python or YAML):</p> <pre><code>from odibi.semantics import SemanticLayerConfig, MetricDefinition, DimensionDefinition\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            expr=\"SUM(total_amount)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\"\n        )\n    ],\n    dimensions=[\n        DimensionDefinition(\n            name=\"region\",\n            source=\"dim_customer\",\n            column=\"region\"\n        )\n    ]\n)\n</code></pre>"},{"location":"semantics/#3-query-metrics","title":"3. Query Metrics","text":"<pre><code>from odibi.semantics import SemanticQuery\nfrom odibi.context import EngineContext\n\n# Setup context with your data\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", spark.table(\"warehouse.fact_orders\"))\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\n\n# Query\nquery = SemanticQuery(config)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.show())\n</code></pre>"},{"location":"semantics/#architecture","title":"Architecture","text":"<pre><code>classDiagram\n    class SemanticLayerConfig {\n        +metrics: List[MetricDefinition]\n        +dimensions: List[DimensionDefinition]\n        +materializations: List[MaterializationConfig]\n    }\n\n    class MetricDefinition {\n        +name: str\n        +expr: str\n        +source: str\n        +filters: List[str]\n    }\n\n    class DimensionDefinition {\n        +name: str\n        +source: str\n        +column: str\n        +hierarchy: List[str]\n    }\n\n    class SemanticQuery {\n        +execute(query_string, context)\n    }\n\n    class Materializer {\n        +execute(name, context)\n        +execute_all(context)\n    }\n\n    SemanticLayerConfig --&gt; MetricDefinition\n    SemanticLayerConfig --&gt; DimensionDefinition\n    SemanticQuery --&gt; SemanticLayerConfig\n    Materializer --&gt; SemanticLayerConfig\n</code></pre>"},{"location":"semantics/#next-steps","title":"Next Steps","text":"<ul> <li>Defining Metrics - Create metric and dimension definitions</li> <li>Querying - Query syntax and examples</li> <li>Materializing - Pre-compute and schedule metrics</li> <li>Pattern Docs - Build your data layer with patterns</li> </ul>"},{"location":"semantics/materialize/","title":"Materializing Metrics","text":"<p>This guide covers how to pre-compute and persist metrics using the <code>Materializer</code> class.</p> <p>Source Notation: Metrics and dimensions use <code>source</code> to reference tables. Supports <code>$pipeline.node</code> (recommended), <code>connection.path</code>, or bare names. See Source Notation for details.</p>"},{"location":"semantics/materialize/#overview","title":"Overview","text":"<p>Materialization pre-computes aggregated metrics at a specific grain and persists them to an output table. This enables:</p> <ul> <li>Faster query response: Pre-computed aggregates vs. real-time calculation</li> <li>Scheduled refresh: Cron-based updates for dashboards</li> <li>Incremental updates: Merge new data without full recalculation</li> </ul>"},{"location":"semantics/materialize/#materializationconfig","title":"MaterializationConfig","text":"<p>Define materializations in your semantic layer config:</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region     # Unique identifier\n    metrics: [revenue, order_count]     # Metrics to include\n    dimensions: [region, month]         # Grain (GROUP BY)\n    output: gold/agg_monthly_revenue    # Output table path\n    schedule: \"0 2 1 * *\"               # Optional: cron schedule\n    incremental:                        # Optional: incremental config\n      timestamp_column: order_date\n      merge_strategy: replace\n</code></pre>"},{"location":"semantics/materialize/#fields","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique materialization identifier <code>metrics</code> list Yes - Metrics to materialize <code>dimensions</code> list Yes - Dimensions for grouping <code>output</code> str Yes - Output table path <code>schedule</code> str No - Cron schedule for refresh <code>incremental</code> dict No - Incremental refresh config"},{"location":"semantics/materialize/#materializer-class","title":"Materializer Class","text":""},{"location":"semantics/materialize/#basic-usage","title":"Basic Usage","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\n\n# Load config\nconfig = parse_semantic_config(yaml.safe_load(open(\"semantic_layer.yaml\")))\n\n# Create materializer\nmaterializer = Materializer(config)\n\n# Execute single materialization\nresult = materializer.execute(\"monthly_revenue_by_region\", context)\n\nprint(result.name)        # 'monthly_revenue_by_region'\nprint(result.output)      # 'gold/agg_monthly_revenue'\nprint(result.row_count)   # Number of aggregated rows\nprint(result.elapsed_ms)  # Execution time\nprint(result.success)     # True if successful\nprint(result.error)       # Error message if failed\n</code></pre>"},{"location":"semantics/materialize/#execute-all-materializations","title":"Execute All Materializations","text":"<pre><code># Execute all configured materializations\nresults = materializer.execute_all(context)\n\nfor result in results:\n    status = \"OK\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status} ({result.row_count} rows)\")\n</code></pre>"},{"location":"semantics/materialize/#write-callback","title":"Write Callback","text":"<p>Provide a callback to write the output:</p> <pre><code>def write_delta(df, output_path):\n    \"\"\"Write DataFrame to Delta Lake.\"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n\n# Execute with write callback\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_delta\n)\n</code></pre>"},{"location":"semantics/materialize/#scheduling","title":"Scheduling","text":"<p>Materializations can have cron schedules for automated refresh:</p> <pre><code>materializations:\n  - name: daily_revenue\n    metrics: [revenue]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    schedule: \"0 2 * * *\"  # 2am daily\n\n  - name: monthly_summary\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_summary\n    schedule: \"0 3 1 * *\"  # 3am on 1st of month\n</code></pre>"},{"location":"semantics/materialize/#reading-schedules","title":"Reading Schedules","text":"<pre><code># Get schedule for a materialization\nschedule = materializer.get_schedule(\"daily_revenue\")\nprint(schedule)  # \"0 2 * * *\"\n\n# List all materializations with schedules\nfor mat in materializer.list_materializations():\n    print(f\"{mat['name']}: {mat['schedule']}\")\n</code></pre>"},{"location":"semantics/materialize/#integration-with-orchestrators","title":"Integration with Orchestrators","text":"<p>Use schedules with your orchestrator (Airflow, Dagster, etc.):</p> <pre><code># Airflow example\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndef run_materialization(name):\n    materializer.execute(name, context, write_callback=write_delta)\n\nfor mat in materializer.list_materializations():\n    if mat['schedule']:\n        PythonOperator(\n            task_id=f\"materialize_{mat['name']}\",\n            python_callable=run_materialization,\n            op_args=[mat['name']],\n            schedule_interval=mat['schedule']\n        )\n</code></pre>"},{"location":"semantics/materialize/#incremental-materialization","title":"Incremental Materialization","text":"<p>Use <code>IncrementalMaterializer</code> for efficient updates:</p> <pre><code>from odibi.semantics.materialize import IncrementalMaterializer\n\n# Create incremental materializer\ninc_materializer = IncrementalMaterializer(config)\n\n# Load existing materialized data\nexisting_df = spark.read.format(\"delta\").load(\"gold/agg_monthly_revenue\")\n\n# Get last processed timestamp\nlast_timestamp = existing_df.agg({\"load_timestamp\": \"max\"}).collect()[0][0]\n\n# Execute incremental update\nresult = inc_materializer.execute_incremental(\n    name=\"monthly_revenue_by_region\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_timestamp,\n    merge_strategy=\"replace\"\n)\n</code></pre>"},{"location":"semantics/materialize/#merge-strategies","title":"Merge Strategies","text":""},{"location":"semantics/materialize/#replace-strategy","title":"Replace Strategy","text":"<p>New aggregates overwrite existing for matching grain keys:</p> <pre><code>result = inc_materializer.execute_incremental(\n    name=\"monthly_revenue_by_region\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_processed,\n    merge_strategy=\"replace\"\n)\n</code></pre> <p>Behavior: 1. Filter source to <code>order_date &gt; since_timestamp</code> 2. Aggregate new data at grain 3. Remove matching grain keys from existing 4. Union remaining existing + new aggregates</p> <p>Use case: Late-arriving data, corrections, any non-additive metrics</p>"},{"location":"semantics/materialize/#sum-strategy","title":"Sum Strategy","text":"<p>Add new measure values to existing aggregates:</p> <pre><code>result = inc_materializer.execute_incremental(\n    name=\"daily_order_count\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"created_at\",\n    since_timestamp=last_processed,\n    merge_strategy=\"sum\"\n)\n</code></pre> <p>Behavior: 1. Filter source to <code>created_at &gt; since_timestamp</code> 2. Aggregate new data at grain 3. Full outer join with existing on grain 4. Sum measure values</p> <p>Use case: Purely additive metrics (counts, sums) where data is append-only</p> <p>Warning: Don't use for AVG, DISTINCT counts, or ratios.</p>"},{"location":"semantics/materialize/#full-example","title":"Full Example","text":"<p>Complete materialization pipeline:</p> <pre><code># semantic_layer.yaml\nsemantic_layer:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n      filters: [\"status = 'completed'\"]\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n      source: fact_orders\n\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n      source: fact_orders\n\n  dimensions:\n    - name: region\n      source: dim_customer\n      column: region\n\n    - name: month\n      source: dim_date\n      column: month_name\n\n    - name: date_sk\n      source: dim_date\n      column: date_sk\n\n  materializations:\n    - name: daily_revenue\n      metrics: [revenue, order_count]\n      dimensions: [date_sk, region]\n      output: gold/agg_daily_revenue\n      schedule: \"0 2 * * *\"\n\n    - name: monthly_summary\n      metrics: [revenue, order_count, unique_customers]\n      dimensions: [region, month]\n      output: gold/agg_monthly_summary\n      schedule: \"0 3 1 * *\"\n</code></pre> <pre><code>from odibi.semantics import Materializer, IncrementalMaterializer, parse_semantic_config\nfrom odibi.context import EngineContext\nimport yaml\n\n# Load config\nwith open(\"semantic_layer.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f)[\"semantic_layer\"])\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", spark.table(\"silver.fact_orders\"))\ncontext.register(\"dim_customer\", spark.table(\"gold.dim_customer\"))\ncontext.register(\"dim_date\", spark.table(\"gold.dim_date\"))\n\n# Write callback\ndef write_to_delta(df, output_path):\n    df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/warehouse/{output_path}\")\n\n# Full refresh all materializations\nmaterializer = Materializer(config)\nresults = materializer.execute_all(context, write_callback=write_to_delta)\n\n# Print summary\nfor r in results:\n    status = \"SUCCESS\" if r.success else f\"FAILED: {r.error}\"\n    print(f\"{r.name}: {status} - {r.row_count} rows in {r.elapsed_ms:.0f}ms\")\n\n# Incremental refresh for daily\ninc_materializer = IncrementalMaterializer(config)\nexisting_daily = spark.read.format(\"delta\").load(\"/mnt/warehouse/gold/agg_daily_revenue\")\nlast_date = existing_daily.agg({\"date_sk\": \"max\"}).collect()[0][0]\n\nresult = inc_materializer.execute_incremental(\n    name=\"daily_revenue\",\n    context=context,\n    existing_df=existing_daily,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_date,\n    merge_strategy=\"replace\"\n)\n\n# Write incremental result\nif result.success:\n    write_to_delta(result.df, \"gold/agg_daily_revenue\")\n    print(f\"Updated daily_revenue: {result.row_count} rows\")\n</code></pre>"},{"location":"semantics/materialize/#materializationresult","title":"MaterializationResult","text":"Field Type Description <code>name</code> str Materialization name <code>output</code> str Output table path <code>row_count</code> int Number of aggregated rows <code>elapsed_ms</code> float Execution time in milliseconds <code>success</code> bool Whether execution succeeded <code>error</code> str Error message if failed"},{"location":"semantics/materialize/#best-practices","title":"Best Practices","text":""},{"location":"semantics/materialize/#grain-selection","title":"Grain Selection","text":"<ul> <li>Choose grain based on query patterns</li> <li>Finer grain = more rows, but more flexibility</li> <li>Coarser grain = faster queries, less flexibility</li> </ul>"},{"location":"semantics/materialize/#scheduling_1","title":"Scheduling","text":"<ul> <li>Schedule based on source data freshness</li> <li>Daily aggregates: run after nightly ETL</li> <li>Monthly: run after month close</li> </ul>"},{"location":"semantics/materialize/#incremental-strategy","title":"Incremental Strategy","text":"<ul> <li>Use <code>replace</code> for late-arriving data tolerance</li> <li>Use <code>sum</code> only for append-only sources</li> <li>Track <code>since_timestamp</code> in state store</li> </ul>"},{"location":"semantics/materialize/#performance","title":"Performance","text":"<ul> <li>Partition output by time dimension</li> <li>Use Delta Lake for efficient updates</li> <li>Monitor execution times</li> </ul>"},{"location":"semantics/materialize/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Create metric definitions</li> <li>Querying - Interactive metric queries</li> <li>Aggregation Pattern - Pattern-based aggregation</li> </ul>"},{"location":"semantics/metrics/","title":"Defining Metrics","text":"<p>This guide covers how to define metrics and dimensions in the Odibi semantic layer.</p> <p>Source Notation: The <code>source</code> field supports three formats: <code>$pipeline.node</code> (recommended), <code>connection.path</code>, or bare table names. See Source Notation for details.</p>"},{"location":"semantics/metrics/#metricdefinition","title":"MetricDefinition","text":"<p>A metric represents a measurable value that can be aggregated across dimensions.</p>"},{"location":"semantics/metrics/#schema","title":"Schema","text":"<pre><code>metrics:\n  - name: revenue              # Required: unique identifier\n    description: \"...\"         # Optional: human-readable description\n    expr: \"SUM(total_amount)\"  # Required: SQL aggregation expression\n    source: fact_orders        # Required for simple metrics: source table\n    filters:                   # Optional: WHERE conditions\n      - \"status = 'completed'\"\n    type: simple               # Optional: \"simple\" or \"derived\"\n</code></pre>"},{"location":"semantics/metrics/#fields","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique metric identifier (lowercase, alphanumeric + underscore) <code>description</code> str No - Human-readable description <code>expr</code> str Yes - SQL aggregation expression <code>source</code> str For simple - Source table name <code>filters</code> list No [] WHERE conditions to apply <code>type</code> str No \"simple\" \"simple\" (direct) or \"derived\" (references other metrics)"},{"location":"semantics/metrics/#simple-metrics","title":"Simple Metrics","text":"<p>Simple metrics aggregate directly from source data:</p> <pre><code>metrics:\n  # Count\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  # Sum\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n\n  # Average\n  - name: avg_order_value\n    expr: \"AVG(total_amount)\"\n    source: fact_orders\n\n  # Distinct count\n  - name: unique_customers\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n\n  # Min/Max\n  - name: max_order\n    expr: \"MAX(total_amount)\"\n    source: fact_orders\n\n  # Complex expression\n  - name: total_margin\n    expr: \"SUM(revenue - cost)\"\n    source: fact_orders\n</code></pre>"},{"location":"semantics/metrics/#filtered-metrics","title":"Filtered Metrics","text":"<p>Apply filters to constrain the aggregation:</p> <pre><code>metrics:\n  # Only completed orders\n  - name: completed_revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Multiple filters (AND)\n  - name: domestic_completed_revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n      - \"country = 'USA'\"\n\n  # Time-filtered\n  - name: last_30_days_orders\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"order_date &gt;= CURRENT_DATE - INTERVAL 30 DAY\"\n</code></pre>"},{"location":"semantics/metrics/#derived-metrics","title":"Derived Metrics","text":"<p>Derived metrics reference other metrics (future enhancement):</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  # Derived: revenue / order_count\n  - name: avg_order_value_derived\n    expr: \"revenue / order_count\"\n    type: derived\n</code></pre>"},{"location":"semantics/metrics/#dimensiondefinition","title":"DimensionDefinition","text":"<p>A dimension represents an attribute for grouping and filtering metrics.</p>"},{"location":"semantics/metrics/#schema_1","title":"Schema","text":"<pre><code>dimensions:\n  - name: region               # Required: unique identifier\n    source: fact_orders        # Required: source table\n    column: region             # Optional: column name (defaults to name)\n    hierarchy:                 # Optional: drill-down hierarchy\n      - year\n      - quarter\n      - month\n    description: \"...\"         # Optional: human-readable description\n</code></pre>"},{"location":"semantics/metrics/#fields_1","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique dimension identifier <code>source</code> str Yes - Source table name <code>column</code> str No name Column name in source <code>hierarchy</code> list No [] Ordered drill-down columns <code>description</code> str No - Human-readable description"},{"location":"semantics/metrics/#dimension-examples","title":"Dimension Examples","text":"<pre><code>dimensions:\n  # Simple dimension\n  - name: region\n    source: fact_orders\n    column: region\n\n  # Dimension with different column name\n  - name: customer_region\n    source: dim_customer\n    column: billing_region\n\n  # Date dimension with hierarchy\n  - name: order_date\n    source: dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter\n      - month\n      - week_of_year\n      - full_date\n\n  # Product category hierarchy\n  - name: product\n    source: dim_product\n    column: product_name\n    hierarchy:\n      - department\n      - category\n      - subcategory\n      - product_name\n</code></pre>"},{"location":"semantics/metrics/#complete-yaml-example","title":"Complete YAML Example","text":"<p>Full semantic layer configuration:</p> <pre><code>semantic_layer:\n  metrics:\n    # Revenue metrics\n    - name: revenue\n      description: \"Total revenue from all orders\"\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n\n    - name: completed_revenue\n      description: \"Revenue from completed orders only\"\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n    # Volume metrics\n    - name: order_count\n      description: \"Number of orders\"\n      expr: \"COUNT(*)\"\n      source: fact_orders\n\n    - name: units_sold\n      description: \"Total units sold\"\n      expr: \"SUM(quantity)\"\n      source: fact_orders\n\n    # Customer metrics\n    - name: unique_customers\n      description: \"Distinct customer count\"\n      expr: \"COUNT(DISTINCT customer_sk)\"\n      source: fact_orders\n\n    # Calculated metrics\n    - name: avg_order_value\n      description: \"Average order value\"\n      expr: \"AVG(total_amount)\"\n      source: fact_orders\n\n    - name: avg_units_per_order\n      description: \"Average units per order\"\n      expr: \"AVG(quantity)\"\n      source: fact_orders\n\n  dimensions:\n    # Geographic dimensions\n    - name: region\n      source: dim_customer\n      column: region\n      description: \"Customer region\"\n\n    - name: country\n      source: dim_customer\n      column: country\n\n    - name: city\n      source: dim_customer\n      column: city\n\n    # Time dimensions\n    - name: order_date\n      source: dim_date\n      column: full_date\n      hierarchy: [year, quarter, month, full_date]\n\n    - name: year\n      source: dim_date\n      column: year\n\n    - name: month\n      source: dim_date\n      column: month_name\n\n    - name: quarter\n      source: dim_date\n      column: quarter_name\n\n    # Product dimensions\n    - name: category\n      source: dim_product\n      column: category\n\n    - name: product\n      source: dim_product\n      column: product_name\n      hierarchy: [category, subcategory, product_name]\n\n    # Order dimensions\n    - name: channel\n      source: fact_orders\n      column: sales_channel\n\n    - name: payment_method\n      source: fact_orders\n      column: payment_type\n\n  materializations:\n    - name: daily_revenue_by_region\n      metrics: [revenue, order_count, unique_customers]\n      dimensions: [region, order_date]\n      output: gold/agg_daily_revenue_region\n\n    - name: monthly_revenue_by_category\n      metrics: [revenue, units_sold]\n      dimensions: [category, month]\n      output: gold/agg_monthly_revenue_category\n      schedule: \"0 2 1 * *\"\n</code></pre>"},{"location":"semantics/metrics/#python-api","title":"Python API","text":"<pre><code>from odibi.semantics.metrics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig,\n    parse_semantic_config\n)\n\n# Create metrics programmatically\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue\",\n    expr=\"SUM(total_amount)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Create dimensions\nregion = DimensionDefinition(\n    name=\"region\",\n    source=\"dim_customer\",\n    column=\"region\"\n)\n\norder_date = DimensionDefinition(\n    name=\"order_date\",\n    source=\"dim_date\",\n    column=\"full_date\",\n    hierarchy=[\"year\", \"quarter\", \"month\", \"full_date\"]\n)\n\n# Create config\nconfig = SemanticLayerConfig(\n    metrics=[revenue],\n    dimensions=[region, order_date]\n)\n\n# Or parse from YAML\nconfig = parse_semantic_config({\n    \"metrics\": [...],\n    \"dimensions\": [...],\n    \"materializations\": [...]\n})\n\n# Validate references\nerrors = config.validate_references()\nif errors:\n    print(\"Validation errors:\", errors)\n\n# Lookup by name\nmetric = config.get_metric(\"revenue\")\ndimension = config.get_dimension(\"region\")\n</code></pre>"},{"location":"semantics/metrics/#validation","title":"Validation","text":"<p>The semantic layer validates:</p> <ol> <li>Metric names: Must be alphanumeric + underscore, lowercase</li> <li>Non-empty expressions: <code>expr</code> cannot be empty</li> <li>Materialization references: All referenced metrics/dimensions must exist</li> </ol> <pre><code># Validate the config\nerrors = config.validate_references()\n# Returns: [\"Materialization 'x' references unknown metric 'y'\"]\n</code></pre>"},{"location":"semantics/metrics/#best-practices","title":"Best Practices","text":""},{"location":"semantics/metrics/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use <code>snake_case</code> for metric and dimension names</li> <li>Be descriptive: <code>completed_order_revenue</code> over <code>rev1</code></li> <li>Prefix related metrics: <code>revenue</code>, <code>revenue_completed</code>, <code>revenue_refunded</code></li> </ul>"},{"location":"semantics/metrics/#filter-usage","title":"Filter Usage","text":"<ul> <li>Define filtered variants as separate metrics</li> <li>Makes queries cleaner and consistent</li> <li>Enables caching of common filter combinations</li> </ul>"},{"location":"semantics/metrics/#hierarchy-design","title":"Hierarchy Design","text":"<ul> <li>Order from coarsest to finest grain</li> <li>Match BI tool drill-down expectations</li> <li>Include intermediate levels for flexibility</li> </ul>"},{"location":"semantics/metrics/#see-also","title":"See Also","text":"<ul> <li>Querying - Query syntax and execution</li> <li>Materializing - Pre-compute metrics</li> <li>Semantic Layer Overview - Architecture and concepts</li> </ul>"},{"location":"semantics/query/","title":"Querying the Semantic Layer","text":"<p>This guide covers how to query the Odibi semantic layer using the <code>SemanticQuery</code> interface.</p>"},{"location":"semantics/query/#query-syntax","title":"Query Syntax","text":"<p>The semantic query syntax follows a simple pattern:</p> <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre>"},{"location":"semantics/query/#examples","title":"Examples","text":"<pre><code># Single metric, single dimension\n\"revenue BY region\"\n\n# Multiple metrics, single dimension\n\"revenue, order_count BY region\"\n\n# Multiple metrics, multiple dimensions\n\"revenue, order_count BY region, month\"\n\n# With WHERE filter\n\"revenue BY region WHERE year = 2024\"\n\n# Complex filter\n\"revenue BY category WHERE region = 'North' AND status = 'completed'\"\n</code></pre>"},{"location":"semantics/query/#semanticquery-class","title":"SemanticQuery Class","text":""},{"location":"semantics/query/#initialization","title":"Initialization","text":"<pre><code>from odibi.semantics import SemanticQuery, SemanticLayerConfig\n\n# From config object\nconfig = SemanticLayerConfig(\n    metrics=[...],\n    dimensions=[...],\n    materializations=[...]\n)\nquery = SemanticQuery(config)\n\n# From YAML\nimport yaml\nfrom odibi.semantics.metrics import parse_semantic_config\n\nwith open(\"semantic_layer.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\nquery = SemanticQuery(config)\n</code></pre>"},{"location":"semantics/query/#execute-query","title":"Execute Query","text":"<pre><code>from odibi.context import EngineContext\n\n# Create context with source data\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\n\n# Execute query\nresult = query.execute(\"revenue BY region\", context)\n\n# Access results\nprint(result.df)           # DataFrame with results\nprint(result.metrics)      # ['revenue']\nprint(result.dimensions)   # ['region']\nprint(result.row_count)    # Number of result rows\nprint(result.elapsed_ms)   # Execution time\nprint(result.sql_generated)  # Generated SQL (for debugging)\n</code></pre>"},{"location":"semantics/query/#queryresult","title":"QueryResult","text":"Field Type Description <code>df</code> DataFrame Result DataFrame (Spark or Pandas) <code>metrics</code> List[str] Metrics that were computed <code>dimensions</code> List[str] Dimensions used for grouping <code>row_count</code> int Number of result rows <code>elapsed_ms</code> float Execution time in milliseconds <code>sql_generated</code> str Generated SQL query (for debugging)"},{"location":"semantics/query/#query-examples","title":"Query Examples","text":""},{"location":"semantics/query/#basic-queries","title":"Basic Queries","text":"<pre><code># Total revenue\nresult = query.execute(\"revenue\", context)\n# Returns single row with total\n\n# Revenue by region\nresult = query.execute(\"revenue BY region\", context)\n# Returns one row per region\n\n# Multiple metrics\nresult = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\n</code></pre>"},{"location":"semantics/query/#multi-dimensional-queries","title":"Multi-Dimensional Queries","text":"<pre><code># Two dimensions\nresult = query.execute(\"revenue BY region, category\", context)\n\n# Three dimensions\nresult = query.execute(\"revenue BY region, category, month\", context)\n\n# Time series\nresult = query.execute(\"revenue, order_count BY year, month\", context)\n</code></pre>"},{"location":"semantics/query/#filtered-queries","title":"Filtered Queries","text":"<pre><code># Single filter\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North'\",\n    context\n)\n\n# Multiple filters (combined with AND)\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North' AND year = 2024\",\n    context\n)\n\n# Using metric filters + query filters\n# If metric has filters, they combine with query filters\nresult = query.execute(\"completed_revenue BY region\", context)\n# Metric filter: status = 'completed'\n# Combined: WHERE status = 'completed'\n</code></pre>"},{"location":"semantics/query/#parse-and-validate","title":"Parse and Validate","text":"<p>You can parse and validate queries before execution:</p> <pre><code># Parse query string\nparsed = query.parse(\"revenue, order_count BY region, month WHERE year = 2024\")\n\nprint(parsed.metrics)      # ['revenue', 'order_count']\nprint(parsed.dimensions)   # ['region', 'month']\nprint(parsed.filters)      # ['year = 2024']\nprint(parsed.raw_query)    # Original query string\n\n# Validate against config\nerrors = query.validate(parsed)\nif errors:\n    print(\"Validation errors:\", errors)\n    # [\"Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count']\"]\n</code></pre>"},{"location":"semantics/query/#generated-sql","title":"Generated SQL","text":"<p>View the SQL generated from a semantic query:</p> <pre><code>parsed = query.parse(\"revenue BY region\")\nsql, source = query.generate_sql(parsed)\n\nprint(sql)\n# SELECT region, SUM(total_amount) AS revenue \n# FROM fact_orders \n# GROUP BY region\n\nprint(source)\n# fact_orders\n</code></pre>"},{"location":"semantics/query/#full-python-example","title":"Full Python Example","text":"<pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load semantic layer config\nconfig_dict = {\n    \"metrics\": [\n        {\n            \"name\": \"revenue\",\n            \"expr\": \"SUM(total_amount)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"order_count\",\n            \"expr\": \"COUNT(*)\",\n            \"source\": \"fact_orders\"\n        },\n        {\n            \"name\": \"avg_order_value\",\n            \"expr\": \"AVG(total_amount)\",\n            \"source\": \"fact_orders\"\n        }\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"fact_orders\", \"column\": \"region\"},\n        {\"name\": \"month\", \"source\": \"dim_date\", \"column\": \"month_name\"}\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\nquery = SemanticQuery(config)\n\n# Create context with data\ncontext = EngineContext(\n    df=None, \n    engine_type=EngineType.PANDAS\n)\ncontext.register(\"fact_orders\", orders_df)\ncontext.register(\"dim_date\", dates_df)\n\n# Query 1: Total revenue\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\n\n# Query 2: Revenue by region\nresult = query.execute(\"revenue, order_count BY region\", context)\nprint(\"\\nRevenue by Region:\")\nprint(result.df.to_string(index=False))\n\n# Query 3: Monthly trend\nresult = query.execute(\"revenue BY month\", context)\nprint(\"\\nMonthly Revenue:\")\nfor _, row in result.df.iterrows():\n    print(f\"  {row['month']}: ${row['revenue']:,.2f}\")\n\n# Query 4: Filtered query\nresult = query.execute(\n    \"revenue, avg_order_value BY region WHERE region IN ('North', 'South')\",\n    context\n)\nprint(\"\\nNorth/South Regions:\")\nprint(result.df.to_string(index=False))\n\n# Check execution performance\nprint(f\"\\nQuery executed in {result.elapsed_ms:.2f}ms\")\nprint(f\"Generated SQL: {result.sql_generated}\")\n</code></pre>"},{"location":"semantics/query/#using-with-source-dataframe","title":"Using with Source DataFrame","text":"<p>Override the context lookup with a specific DataFrame:</p> <pre><code># Instead of using context.get(source_table)\n# Pass source_df directly\nresult = query.execute(\n    \"revenue BY region\",\n    context,\n    source_df=my_filtered_dataframe\n)\n</code></pre>"},{"location":"semantics/query/#error-handling","title":"Error Handling","text":"<pre><code>from odibi.semantics import SemanticQuery\n\ntry:\n    result = query.execute(\"invalid_metric BY region\", context)\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\n    # \"Invalid semantic query: Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count']\"\n\ntry:\n    result = query.execute(\"revenue BY invalid_dimension\", context)\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\n    # \"Invalid semantic query: Unknown dimension 'invalid_dimension'. Available: ['region', 'month']\"\n</code></pre>"},{"location":"semantics/query/#engine-support","title":"Engine Support","text":"<p>Queries work with both Spark and Pandas:</p>"},{"location":"semantics/query/#spark","title":"Spark","text":"<pre><code>context = EngineContext(\n    df=None,\n    engine_type=EngineType.SPARK,\n    spark=spark_session\n)\nresult = query.execute(\"revenue BY region\", context)\nresult.df.show()  # Spark DataFrame\n</code></pre>"},{"location":"semantics/query/#pandas","title":"Pandas","text":"<pre><code>context = EngineContext(\n    df=None,\n    engine_type=EngineType.PANDAS\n)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df)  # Pandas DataFrame\n</code></pre>"},{"location":"semantics/query/#performance-tips","title":"Performance Tips","text":"<ol> <li>Materialize frequent queries: Use <code>Materializer</code> for dashboards</li> <li>Pre-filter source data: Pass filtered <code>source_df</code> parameter</li> <li>Limit dimensions: More dimensions = larger result set</li> <li>Use indexed columns: Ensure dimension columns are indexed in source</li> </ol>"},{"location":"semantics/query/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Create metric and dimension definitions</li> <li>Materializing Metrics - Pre-compute for performance</li> <li>Semantic Layer Overview - Architecture and concepts</li> </ul>"},{"location":"tutorials/getting_started/","title":"\ud83c\udfc1 Getting Started with Odibi","text":"<p>This tutorial will guide you through creating your first data pipeline. By the end, you will have a running project that reads data, cleans it, and generates an audit report (\"Data Story\").</p> <p>Prerequisites: *   Python 3.9 or higher installed. *   Basic familiarity with terminal/command line.</p>"},{"location":"tutorials/getting_started/#1-installation","title":"1. Installation","text":"<p>First, install Odibi. We recommend creating a virtual environment to keep your system clean.</p> <pre><code># 1. Create a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# 2. Install Odibi\npip install odibi\n</code></pre> <p>Note: If you plan to use Spark or Azure later, you can install <code>pip install \"odibi[spark,azure]\"</code>, but for this tutorial, the base package is enough.</p>"},{"location":"tutorials/getting_started/#2-create-sample-data","title":"2. Create Sample Data","text":"<p>Odibi shines when working with messy real-world data. Let's create some \"bad\" data to clean.</p> <p>Create a folder named <code>raw_data</code> and a file inside it named <code>customers.csv</code>:</p> <p>raw_data/customers.csv</p> <pre><code>id, name,           email,              joined_at\n1,  Alice,          alice@example.com,  2023-01-01\n2,  Bob,            bob@example.com,    2023-02-15\n3,  Charlie,        NULL,               2023-03-10\n4,  Dave,           dave@example.com,   invalid-date\n</code></pre> <p>(Notice the extra spaces, the NULL value, and the invalid date string.)</p>"},{"location":"tutorials/getting_started/#3-generate-your-project","title":"3. Generate Your Project","text":"<p>Instead of writing configuration files from scratch, use the Odibi Initializer. It creates a project skeleton with best practices baked in.</p> <p>Run this command in your terminal:</p> <pre><code>odibi init-pipeline my_first_project --template local-medallion\n</code></pre> <p>This creates a new folder <code>my_first_project</code> with a standard structure: *   <code>odibi.yaml</code>: The pipeline configuration. *   <code>data/</code>: Folders for your data layers (landing, raw, silver, etc.). *   <code>README.md</code>: Instructions for your project.</p> <p>Move your sample data into the landing zone:</p> <pre><code># On Windows (PowerShell)\nmv raw_data/customers.csv my_first_project/data/landing/\n# On Mac/Linux\nmv raw_data/customers.csv my_first_project/data/landing/\n</code></pre> <p>Note: You can also generate a project from existing data using <code>odibi generate-project</code>, but <code>init-pipeline</code> is the recommended way to start fresh.</p>"},{"location":"tutorials/getting_started/#4-explore-the-project","title":"4. Explore the Project","text":"<p>Navigate into your new project:</p> <pre><code>cd my_first_project\n</code></pre> <p>You will see a file structure like this:</p> <ul> <li><code>odibi.yaml</code>: The brain of your project. It defines the pipeline.</li> <li><code>sql/</code>: Contains SQL transformation files.</li> <li><code>data/</code>: (Created automatically) Where data will be stored.</li> </ul> <p>Open <code>odibi.yaml</code> in your text editor. You will see two \"nodes\" (steps): 1.  Ingestion Node: Reads the <code>customers.csv</code> from <code>landing/</code>. 2.  Refinement Node: Merges the data into <code>silver/</code>.</p> <p>Since we used the template, the config is already set up to look for <code>landing/customers.csv</code>.</p>"},{"location":"tutorials/getting_started/#5-run-the-pipeline","title":"5. Run the Pipeline","text":"<p>Now, execute the pipeline:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Odibi will: 1.  Read <code>customers.csv</code> from <code>landing/</code>. 2.  Convert it to Parquet in <code>raw/</code>. 3.  Merge it into a Delta/Parquet table in <code>silver/</code>. 4.  Generate a \"Data Story\".</p>"},{"location":"tutorials/getting_started/#6-view-the-data-story","title":"6. View the Data Story","text":"<p>Data engineering is often invisible. Odibi makes it visible. Every run generates a report.</p> <p>List the generated stories:</p> <pre><code>odibi story list\n</code></pre> <p>You will see output like:</p> <pre><code>\ud83d\udcda Stories in .odibi/stories:\n================================================================================\n  \ud83d\udcc4 main_documentation.html\n     Modified: 2025-11-21 14:30:00\n     Size: 15.2KB\n     Path: .odibi/stories/main_documentation.html\n</code></pre> <p>Open the HTML file in your browser to view the report: - Windows: <code>start .odibi/stories/main_documentation.html</code> - Mac: <code>open .odibi/stories/main_documentation.html</code> - Linux: <code>xdg-open .odibi/stories/main_documentation.html</code></p> <p>What to look for in the report: *   Row Counts: Did we lose any rows? *   Schema: Did the column types change? *   Execution Time: How long did it take?</p>"},{"location":"tutorials/getting_started/#7-add-data-validation","title":"7. Add Data Validation","text":"<p>Data pipelines are only as good as their data quality checks. Let's add validation to catch bad data before it corrupts your warehouse.</p>"},{"location":"tutorials/getting_started/#inline-validation-in-yaml","title":"Inline Validation in YAML","text":"<p>Add validation rules directly to your node:</p> <pre><code>nodes:\n  - name: customers\n    read:\n      connection: landing\n      format: csv\n      path: customers.csv\n    validation:\n      tests:\n        - type: not_null\n          columns: [id, name]\n        - type: unique\n          columns: [id]\n        - type: row_count\n          min: 1\n      on_failure: warn  # or \"error\" to stop the pipeline\n    write:\n      connection: raw\n      format: parquet\n      path: customers\n</code></pre>"},{"location":"tutorials/getting_started/#using-contracts-for-input-validation","title":"Using Contracts for Input Validation","text":"<p>Contracts validate data before processing:</p> <pre><code>nodes:\n  - name: validate_orders\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id, amount]\n      - type: freshness\n        column: created_at\n        max_age: \"24h\"\n    read:\n      connection: landing\n      path: orders.csv\n    write:\n      connection: raw\n      path: orders\n</code></pre> <p>If contracts fail, the pipeline stops immediately with clear error messages.</p>"},{"location":"tutorials/getting_started/#running-validation","title":"Running Validation","text":"<p>Run the pipeline and watch for validation warnings:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Validation results appear in both the console output and the Data Story.</p>"},{"location":"tutorials/getting_started/#8-whats-next","title":"8. What's Next?","text":"<p>You have successfully built a data pipeline with data validation!</p> <ul> <li>Incremental Loading: Learn how to efficiently process only new data using State Tracking (\"Auto-Pilot\").</li> <li>Write Custom Transformations: Learn how to add Python logic (like advanced validation) to your pipeline.</li> <li>Data Validation Guide: Deep dive into all validation options.</li> <li>Master the CLI: Learn about <code>odibi stress</code> and <code>odibi doctor</code>.</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/","title":"Introduction to Dimensional Modeling","text":"<p>Welcome to the Odibi dimensional modeling tutorial series. This comprehensive guide will teach you how to build a complete data warehouse using dimensional modeling techniques, from scratch.</p> <p>Prerequisites: Basic SQL knowledge and familiarity with data concepts like tables and columns.</p> <p>What You'll Build: A complete star schema for a retail sales system, plus a semantic layer for business intelligence.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-is-dimensional-modeling","title":"What is Dimensional Modeling?","text":"<p>Dimensional modeling is a technique for organizing data to make it easy to query and understand. It was developed by Ralph Kimball and is the foundation of most data warehouses today.</p> <p>Think of it like organizing a library: - Facts are like the checkout receipts\u2014they record what happened (a book was borrowed) - Dimensions are like the card catalogs\u2014they describe the who, what, where, and when</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#the-star-schema","title":"The Star Schema","text":"<p>The most common dimensional model is the star schema, named because it looks like a star when diagrammed:</p> <pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        int order_sk PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        int quantity\n        decimal unit_price\n        decimal line_total\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string email\n        string region\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n        decimal price\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        string month_name\n        int year\n    }\n</code></pre> <p>The fact table sits in the center and contains measurements (quantities, amounts, counts). The dimension tables surround it and provide context (who bought it, what was purchased, when did it happen).</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#facts-vs-dimensions-the-grocery-receipt-analogy","title":"Facts vs Dimensions: The Grocery Receipt Analogy","text":"<p>Imagine you're at a grocery store and you get a receipt:</p> <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n   FRESH FOODS MARKET\n   Store #42 - Downtown\n   Date: Jan 15, 2024  Time: 2:34 PM\n   Cashier: Maria\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n   Organic Milk 1 gal       $4.99\n   Wheat Bread              $3.49\n   Bananas 2.5 lb           $1.87\n   Cheddar Cheese           $5.99\n\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   SUBTOTAL                $16.34\n   TAX                      $0.82\n   TOTAL                   $17.16\n\n   Paid: VISA ****1234\n\n   Thank you for shopping with us!\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre> <p>Facts (the measurements): - Quantity of each item - Price of each item - Total amount</p> <p>Dimensions (the context): - Who: The customer (you) - What: The products (milk, bread, bananas, cheese) - Where: The store (#42, Downtown) - When: The date and time - How: The payment method (VISA)</p> <p>In a data warehouse, we'd model this as:</p> Concept Fact or Dimension? Example Columns The line items Fact quantity, unit_price, line_total The customer Dimension name, email, loyalty_tier The product Dimension product_name, category, brand The store Dimension store_name, city, region The date Dimension day_of_week, month, year"},{"location":"tutorials/dimensional_modeling/01_introduction/#why-surrogate-keys","title":"Why Surrogate Keys?","text":"<p>You might wonder: \"If customers already have a customer_id, why do we need another key?\"</p> <p>Good question! Here's why we use surrogate keys (like <code>customer_sk</code>) instead of natural keys (like <code>customer_id</code>):</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-1-business-keys-change","title":"Problem 1: Business Keys Change","text":"<p>Imagine your source system uses email as the customer identifier. What happens when a customer changes their email?</p> <p>Using natural key (email):</p> <pre><code>-- Old orders are orphaned!\nSELECT * FROM orders WHERE customer_email = 'alice@oldmail.com';  -- No longer exists\nSELECT * FROM customers WHERE email = 'alice@oldmail.com';        -- Record was updated to new email\n</code></pre> <p>Using surrogate key:</p> <pre><code>-- Customer SK never changes, even if email does\nSELECT * FROM fact_orders WHERE customer_sk = 42;     -- Still works!\nSELECT * FROM dim_customer WHERE customer_sk = 42;    -- Returns current info\n</code></pre>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-2-tracking-history-scd-type-2","title":"Problem 2: Tracking History (SCD Type 2)","text":"<p>When you need to track historical changes, surrogate keys become essential:</p> <p>Customer Data Over Time:</p> customer_sk customer_id email valid_from valid_to is_current 42 C001 alice@oldmail.com 2023-01-01 2024-01-15 false 157 C001 alice@newmail.com 2024-01-15 NULL true <p>The same customer (C001) has two dimension rows with different surrogate keys. Orders placed before Jan 15 link to SK=42 (capturing the email at that time). Orders after link to SK=157.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-3-performance","title":"Problem 3: Performance","text":"<ul> <li>Surrogate keys are simple integers (4 bytes)</li> <li>Natural keys can be long strings (variable length)</li> <li>Integer joins are much faster than string joins</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-4-multi-source-integration","title":"Problem 4: Multi-Source Integration","text":"<p>When combining data from multiple systems:</p> Source System Customer ID CRM CUST-00042 E-commerce user_42 Support 42 <p>With surrogate keys, all three become customer_sk = 42, regardless of source format.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#the-unknown-member-problem","title":"The Unknown Member Problem","text":"<p>What happens when a fact record references a dimension that doesn't exist?</p> <p>Scenario: An order arrives with <code>customer_id = 'C999'</code>, but there's no customer with that ID in the dimension table.</p> <p>Without unknown member:</p> <pre><code>-- This join loses the order entirely!\nSELECT o.*, c.name\nFROM fact_orders o\nJOIN dim_customer c ON o.customer_sk = c.customer_sk\nWHERE o.order_id = 'ORD999';\n-- Returns: (no rows)\n</code></pre> <p>With unknown member (customer_sk = 0):</p> customer_sk customer_id name email 0 -1 Unknown Unknown 1 C001 Alice Johnson alice@example.com 2 C002 Bob Smith bob@example.com <p>Now orphan orders get assigned to customer_sk = 0:</p> <pre><code>SELECT o.*, c.name\nFROM fact_orders o\nJOIN dim_customer c ON o.customer_sk = c.customer_sk\nWHERE o.order_id = 'ORD999';\n-- Returns: order data with name = 'Unknown'\n</code></pre> <p>The order isn't lost\u2014it's explicitly marked as having an unknown customer, which you can investigate later.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-types-handling-changes","title":"SCD Types: Handling Changes","text":"<p>When dimension data changes, how should you handle it? There are three common strategies:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-0-never-update","title":"SCD Type 0: Never Update","text":"<p>The dimension never changes after initial load. Use for truly static data.</p> <p>Example: ISO country codes, fixed reference data</p> country_sk country_code country_name 1 USA United States 2 CAN Canada"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-1-overwrite","title":"SCD Type 1: Overwrite","text":"<p>Update the dimension in place. History is lost, but you always see current data.</p> <p>Example: Customer email (you only care about current contact info)</p> <p>Before:</p> customer_sk customer_id email 1 C001 alice@oldmail.com <p>After (email changed):</p> customer_sk customer_id email 1 C001 alice@newmail.com"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-2-track-history","title":"SCD Type 2: Track History","text":"<p>Create a new row for each version. Full audit trail preserved.</p> <p>Example: Customer address (for accurate point-in-time reporting)</p> <p>Before:</p> customer_sk customer_id city valid_from valid_to is_current 1 C001 Chicago 2020-01-01 NULL true <p>After (customer moved):</p> customer_sk customer_id city valid_from valid_to is_current 1 C001 Chicago 2020-01-01 2024-01-15 false 42 C001 Seattle 2024-01-15 NULL true"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-youll-build-in-this-tutorial-series","title":"What You'll Build in This Tutorial Series","text":"<p>By the end of this series, you'll have built:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-1-dimensional-modeling-foundations","title":"Part 1: Dimensional Modeling Foundations","text":"<ol> <li>Introduction (this tutorial) - Core concepts</li> <li>Dimension Pattern - Build customer dimension with SCD 0/1/2</li> <li>Date Dimension Pattern - Generate complete date dimension</li> <li>Fact Pattern - Build fact table with SK lookups</li> <li>Aggregation Pattern - Build pre-aggregated tables</li> <li>Full Star Schema - Complete working example</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-2-semantic-layer","title":"Part 2: Semantic Layer","text":"<ol> <li>Semantic Layer Intro - What and why</li> <li>Defining Metrics - Revenue, counts, averages</li> <li>Defining Dimensions - Regions, dates, hierarchies</li> <li>Querying Metrics - \"revenue BY region\" syntax</li> <li>Materializing Metrics - Pre-compute for dashboards</li> <li>Semantic Full Example - Complete semantic layer</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-3-data-quality","title":"Part 3: Data Quality","text":"<ol> <li>FK Validation - Ensure referential integrity</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#sample-data","title":"Sample Data","text":"<p>Throughout these tutorials, we'll use consistent sample data representing a retail business:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#customers-12-rows","title":"Customers (12 rows)","text":"customer_id name email region city C001 Alice Johnson alice@example.com North Chicago C002 Bob Smith bob@example.com South Houston C003 Carol White carol@example.com North Detroit C004 David Brown david@example.com East New York C005 Emma Davis emma@example.com West Seattle C006 Frank Miller frank@example.com South Miami C007 Grace Lee grace@example.com East Boston C008 Henry Wilson henry@example.com West Portland C009 Ivy Chen ivy@example.com North Minneapolis C010 Jack Taylor jack@example.com South Dallas C011 Karen Martinez karen@example.com East Philadelphia C012 Leo Anderson leo@example.com West Denver"},{"location":"tutorials/dimensional_modeling/01_introduction/#products-10-rows","title":"Products (10 rows)","text":"product_id name category price P001 Laptop Pro 15 Electronics $1,299.99 P002 Wireless Mouse Electronics $29.99 P003 Office Chair Furniture $249.99 P004 USB-C Hub Electronics $49.99 P005 Standing Desk Furniture $599.99 P006 Mechanical Keyboard Electronics $149.99 P007 Monitor 27\" Electronics $399.99 P008 Desk Lamp Furniture $45.99 P009 Webcam HD Electronics $79.99 P010 Filing Cabinet Furniture $189.99"},{"location":"tutorials/dimensional_modeling/01_introduction/#orders-30-rows-across-14-days","title":"Orders (30 rows across 14 days)","text":"<p>Sample orders spanning January 15-28, 2024, with various quantities and statuses.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-you-learned","title":"What You Learned","text":"<p>In this introduction, you learned:</p> <ul> <li>Dimensional modeling organizes data into facts and dimensions</li> <li>Star schemas put the fact table in the center, surrounded by dimensions</li> <li>Facts contain measurements (quantities, amounts)</li> <li>Dimensions provide context (who, what, where, when)</li> <li>Surrogate keys solve problems with changing business keys and enable history tracking</li> <li>Unknown members prevent orphan records from being lost</li> <li>SCD Types define how to handle dimension changes (0=static, 1=overwrite, 2=history)</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/#next-steps","title":"Next Steps","text":"<p>Ready to build your first dimension table?</p> <p>Next: Dimension Pattern Tutorial - Build a customer dimension with SCD support</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#navigation","title":"Navigation","text":"Previous Up Next - Tutorials Dimension Pattern"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/","title":"Dimension Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>dimension</code> pattern to build dimension tables with automatic surrogate key generation and SCD (Slowly Changing Dimension) support.</p> <p>What You'll Learn: - How surrogate keys are generated - SCD Type 0 (static) - never update - SCD Type 1 (overwrite) - update in place - SCD Type 2 (history) - track all changes - Unknown member handling</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#source-data","title":"Source Data","text":"<p>We'll start with this customer data (12 rows):</p> <p>Source Data (customers.csv) - 12 rows:</p> customer_id name email region city state C001 Alice Johnson alice@example.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david@example.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace@example.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-1-scd-type-0-static-dimensions","title":"Step 1: SCD Type 0 - Static Dimensions","text":"<p>When to use: Reference data that never changes (country codes, fixed lookups).</p> <p>SCD Type 0 creates surrogate keys but never updates existing records. New records are inserted, but changes to existing records are ignored.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd0\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 0\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#output-dim_customer-13-rows","title":"Output: dim_customer (13 rows)","text":"<p>After running with <code>scd_type: 0</code>, here's the dimension table with generated surrogate keys:</p> customer_sk customer_id name email region city state load_timestamp 0 -1 Unknown Unknown Unknown Unknown Unknown 1900-01-01 00:00:00 1 C001 Alice Johnson alice@example.com North Chicago IL 2024-01-15 10:00:00 2 C002 Bob Smith bob@example.com South Houston TX 2024-01-15 10:00:00 3 C003 Carol White carol@example.com North Detroit MI 2024-01-15 10:00:00 4 C004 David Brown david@example.com East New York NY 2024-01-15 10:00:00 5 C005 Emma Davis emma@example.com West Seattle WA 2024-01-15 10:00:00 6 C006 Frank Miller frank@example.com South Miami FL 2024-01-15 10:00:00 7 C007 Grace Lee grace@example.com East Boston MA 2024-01-15 10:00:00 8 C008 Henry Wilson henry@example.com West Portland OR 2024-01-15 10:00:00 9 C009 Ivy Chen ivy@example.com North Minneapolis MN 2024-01-15 10:00:00 10 C010 Jack Taylor jack@example.com South Dallas TX 2024-01-15 10:00:00 11 C011 Karen Martinez karen@example.com East Philadelphia PA 2024-01-15 10:00:00 12 C012 Leo Anderson leo@example.com West Denver CO 2024-01-15 10:00:00 <p>Key observations: - Row 0 is the unknown member (customer_sk = 0, customer_id = -1) - Surrogate keys are sequential integers starting at 1 - Each source row gets a unique SK</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-2-scd-type-1-overwrite-updates","title":"Step 2: SCD Type 1 - Overwrite Updates","text":"<p>When to use: Attributes where you only need the current value (email, phone, preferences).</p> <p>SCD Type 1 updates existing records in place when changes are detected. No history is preserved.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#the-update-scenario","title":"The Update Scenario","text":"<p>Three customers changed their email addresses:</p> <p>Updated Source Data (customers_updated.csv) - 3 changes highlighted:</p> customer_id name email region city state C001 Alice Johnson alice.johnson@newmail.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david.b@corporate.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace.lee@gmail.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration_1","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd1\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 1\n          track_columns:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          unknown_member: true\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#before-vs-after-comparison","title":"Before vs After Comparison","text":"<p>BEFORE (original load):</p> customer_sk customer_id email load_timestamp 0 -1 Unknown 1900-01-01 00:00:00 1 C001 alice@example.com 2024-01-15 10:00:00 4 C004 david@example.com 2024-01-15 10:00:00 7 C007 grace@example.com 2024-01-15 10:00:00 ... ... ... ... <p>AFTER (SCD1 update):</p> customer_sk customer_id email load_timestamp 0 -1 Unknown 1900-01-01 00:00:00 1 C001 alice.johnson@newmail.com 2024-01-20 14:30:00 4 C004 david.b@corporate.com 2024-01-20 14:30:00 7 C007 grace.lee@gmail.com 2024-01-20 14:30:00 ... ... ... ... <p>Key observations: - Same surrogate keys - C001 is still customer_sk = 1 - Values updated in place - old emails are gone - Timestamp updated - shows when the record was last modified - No history preserved - we can't see the old email addresses</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#complete-scd1-output-13-rows","title":"Complete SCD1 Output (13 rows)","text":"customer_sk customer_id name email region city state load_timestamp 0 -1 Unknown Unknown Unknown Unknown Unknown 1900-01-01 00:00:00 1 C001 Alice Johnson alice.johnson@newmail.com North Chicago IL 2024-01-20 14:30:00 2 C002 Bob Smith bob@example.com South Houston TX 2024-01-15 10:00:00 3 C003 Carol White carol@example.com North Detroit MI 2024-01-15 10:00:00 4 C004 David Brown david.b@corporate.com East New York NY 2024-01-20 14:30:00 5 C005 Emma Davis emma@example.com West Seattle WA 2024-01-15 10:00:00 6 C006 Frank Miller frank@example.com South Miami FL 2024-01-15 10:00:00 7 C007 Grace Lee grace.lee@gmail.com East Boston MA 2024-01-20 14:30:00 8 C008 Henry Wilson henry@example.com West Portland OR 2024-01-15 10:00:00 9 C009 Ivy Chen ivy@example.com North Minneapolis MN 2024-01-15 10:00:00 10 C010 Jack Taylor jack@example.com South Dallas TX 2024-01-15 10:00:00 11 C011 Karen Martinez karen@example.com East Philadelphia PA 2024-01-15 10:00:00 12 C012 Leo Anderson leo@example.com West Denver CO 2024-01-15 10:00:00"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-3-scd-type-2-full-history-tracking","title":"Step 3: SCD Type 2 - Full History Tracking","text":"<p>When to use: Attributes where historical accuracy matters (address for shipping analysis, tier for billing history).</p> <p>SCD Type 2 preserves full history by creating a new row for each change. Old versions are closed with a <code>valid_to</code> date.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#the-history-scenario","title":"The History Scenario","text":"<p>Same three customers changed their emails. With SCD2, we keep both versions:</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration_2","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd2\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#output-full-history-16-rows","title":"Output: Full History (16 rows)","text":"customer_sk customer_id name email region valid_from valid_to is_current 0 -1 Unknown Unknown Unknown 1900-01-01 NULL true 1 C001 Alice Johnson alice@example.com North 2024-01-15 2024-01-20 false 2 C002 Bob Smith bob@example.com South 2024-01-15 NULL true 3 C003 Carol White carol@example.com North 2024-01-15 NULL true 4 C004 David Brown david@example.com East 2024-01-15 2024-01-20 false 5 C005 Emma Davis emma@example.com West 2024-01-15 NULL true 6 C006 Frank Miller frank@example.com South 2024-01-15 NULL true 7 C007 Grace Lee grace@example.com East 2024-01-15 2024-01-20 false 8 C008 Henry Wilson henry@example.com West 2024-01-15 NULL true 9 C009 Ivy Chen ivy@example.com North 2024-01-15 NULL true 10 C010 Jack Taylor jack@example.com South 2024-01-15 NULL true 11 C011 Karen Martinez karen@example.com East 2024-01-15 NULL true 12 C012 Leo Anderson leo@example.com West 2024-01-15 NULL true 13 C001 Alice Johnson alice.johnson@newmail.com North 2024-01-20 NULL true 14 C004 David Brown david.b@corporate.com East 2024-01-20 NULL true 15 C007 Grace Lee grace.lee@gmail.com East 2024-01-20 NULL true <p>Key observations: - New surrogate keys for new versions (13, 14, 15) - Old versions marked closed (is_current = false, valid_to = 2024-01-20) - New versions marked current (is_current = true, valid_to = NULL) - Full history preserved - we can query data as of any point in time</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#how-to-query-scd2","title":"How to Query SCD2","text":"<p>Current view (most common):</p> <pre><code>SELECT * FROM dim_customer WHERE is_current = true;\n</code></pre> <p>Point-in-time query (as of January 17):</p> <pre><code>SELECT * FROM dim_customer \nWHERE '2024-01-17' &gt;= valid_from \n  AND ('2024-01-17' &lt; valid_to OR valid_to IS NULL);\n</code></pre> <p>Customer C001's email history:</p> <pre><code>SELECT customer_sk, email, valid_from, valid_to \nFROM dim_customer \nWHERE customer_id = 'C001'\nORDER BY valid_from;\n</code></pre> customer_sk email valid_from valid_to 1 alice@example.com 2024-01-15 2024-01-20 13 alice.johnson@newmail.com 2024-01-20 NULL"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#understanding-the-unknown-member","title":"Understanding the Unknown Member","text":"<p>The unknown member row (customer_sk = 0) is automatically created when <code>unknown_member: true</code>:</p> customer_sk customer_id name email all other columns 0 -1 Unknown Unknown Unknown <p>Why it matters:</p> <p>When building fact tables, orders might reference a customer_id that doesn't exist in the dimension (data quality issue, late-arriving data, etc.). Instead of: - Failing the pipeline (strict but inflexible) - Losing the order data (dangerous)</p> <p>We assign those orphan records to customer_sk = 0. This: - Preserves all fact data - Makes orphans easily identifiable - Allows later cleanup/investigation</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<p>Here's a complete YAML file you can run:</p> <pre><code># File: odibi_dimension_tutorial.yaml\nproject: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  # Initial load with SCD2\n  - pipeline: initial_load\n    description: \"First load of customer dimension\"\n    nodes:\n      - name: dim_customer\n        description: \"Customer dimension with SCD2 history tracking\"\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n\n  # Incremental update with changes\n  - pipeline: incremental_update\n    description: \"Process updates to customer dimension\"\n    nodes:\n      - name: dim_customer\n        description: \"Update customer dimension with new email addresses\"\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#python-api-alternative","title":"Python API Alternative","text":"<p>If you prefer Python over YAML:</p> <pre><code>from odibi.patterns.dimension import DimensionPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load source data\nsource_df = pd.read_csv(\"examples/tutorials/dimensional_modeling/data/customers.csv\")\n\n# Create pattern\npattern = DimensionPattern(params={\n    \"natural_key\": \"customer_id\",\n    \"surrogate_key\": \"customer_sk\",\n    \"scd_type\": 2,\n    \"track_columns\": [\"name\", \"email\", \"region\", \"city\", \"state\"],\n    \"unknown_member\": True,\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"crm\"\n    }\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern\ncontext = EngineContext(df=source_df, engine_type=EngineType.PANDAS)\nresult_df = pattern.execute(context)\n\n# View results\nprint(f\"Generated {len(result_df)} dimension rows\")\nprint(result_df.head(15))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>SCD Type 0 creates surrogate keys but never updates existing records</li> <li>SCD Type 1 updates records in place, losing history but keeping current data</li> <li>SCD Type 2 creates new rows for changes, preserving full history with valid_from/valid_to dates</li> <li>Surrogate keys are auto-generated integers, sequential starting from 1</li> <li>Unknown member (SK=0) provides a default for orphan FK handling</li> <li>track_columns defines which columns trigger a new version in SCD1/SCD2</li> <li>Audit columns (load_timestamp, source_system) track when/where data came from</li> </ul>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you can build customer dimensions, let's create a date dimension that's automatically generated.</p> <p>Next: Date Dimension Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#navigation","title":"Navigation","text":"Previous Up Next Introduction Tutorials Date Dimension"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Dimension Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/","title":"Date Dimension Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>date_dimension</code> pattern to generate a complete date dimension table with pre-calculated attributes for reporting and analytics.</p> <p>What You'll Learn: - Why you need a date dimension - How the pattern generates dates automatically - Understanding all 19 generated columns - Fiscal calendar configuration - Unknown date handling</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#why-do-you-need-a-date-dimension","title":"Why Do You Need a Date Dimension?","text":"<p>Consider this question: \"What were our sales on Tuesdays in January?\"</p> <p>Your raw order data looks like this:</p> order_id order_date amount ORD001 2024-01-15 1,299.99 ORD002 2024-01-16 249.99 ORD003 2024-01-23 599.99 <p>Problem: The date <code>2024-01-15</code> doesn't tell you it's a Tuesday. You'd need to calculate that in every query.</p> <p>Without a date dimension:</p> <pre><code>-- Complex, repeated logic in every query\nSELECT \n    DATENAME(weekday, order_date) AS day_of_week,\n    SUM(amount) AS total\nFROM orders\nWHERE MONTH(order_date) = 1\n  AND DATENAME(weekday, order_date) = 'Tuesday'\nGROUP BY DATENAME(weekday, order_date);\n</code></pre> <p>With a date dimension:</p> <pre><code>-- Simple join, pre-calculated attributes\nSELECT \n    d.day_of_week,\n    SUM(o.amount) AS total\nFROM fact_orders o\nJOIN dim_date d ON o.date_sk = d.date_sk\nWHERE d.month = 1\n  AND d.day_of_week = 'Tuesday'\nGROUP BY d.day_of_week;\n</code></pre> <p>A date dimension also provides: - Fiscal calendar attributes (fiscal year, fiscal quarter) - Holiday flags (is_holiday, holiday_name) - Business day calculations (is_weekend, is_month_start) - Consistent naming (\"January\" not \"1\", \"Q1\" not \"1\")</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#the-date-dimension-pattern","title":"The Date Dimension Pattern","text":"<p>Unlike other patterns that transform source data, the <code>date_dimension</code> pattern generates data. You don't need a <code>read:</code> block\u2014just configure the date range and options.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#basic-yaml-configuration","title":"Basic YAML Configuration","text":"<pre><code>project: date_dimension_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    nodes:\n      - name: dim_date\n        # No read block needed - pattern generates data\n        transformer: date_dimension\n        params:\n          start_date: \"2024-01-15\"\n          end_date: \"2024-01-28\"\n          fiscal_year_start_month: 7\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-1-generate-a-small-date-range","title":"Step 1: Generate a Small Date Range","text":"<p>Let's generate 14 days (January 15-28, 2024) to see exactly what columns are created.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#output-dim_date-15-rows-including-unknown","title":"Output: dim_date (15 rows including unknown)","text":"<p>Here are the first 10 rows showing all 19 columns:</p> date_sk full_date day_of_week day_of_week_num day_of_month day_of_year is_weekend week_of_year month month_name quarter quarter_name year fiscal_year fiscal_quarter is_month_start is_month_end is_year_start is_year_end 0 1900-01-01 Unknown 0 0 0 false 0 0 Unknown 0 Unknown 0 0 0 false false false false 20240115 2024-01-15 Monday 1 15 15 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240116 2024-01-16 Tuesday 2 16 16 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240117 2024-01-17 Wednesday 3 17 17 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240118 2024-01-18 Thursday 4 18 18 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240119 2024-01-19 Friday 5 19 19 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240120 2024-01-20 Saturday 6 20 20 true 3 1 January 1 Q1 2024 2024 3 false false false false 20240121 2024-01-21 Sunday 7 21 21 true 3 1 January 1 Q1 2024 2024 3 false false false false 20240122 2024-01-22 Monday 1 22 22 false 4 1 January 1 Q1 2024 2024 3 false false false false 20240123 2024-01-23 Tuesday 2 23 23 false 4 1 January 1 Q1 2024 2024 3 false false false false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#remaining-rows-24-28","title":"Remaining rows (24-28):","text":"date_sk full_date day_of_week day_of_week_num is_weekend week_of_year fiscal_year fiscal_quarter 20240124 2024-01-24 Wednesday 3 false 4 2024 3 20240125 2024-01-25 Thursday 4 false 4 2024 3 20240126 2024-01-26 Friday 5 false 4 2024 3 20240127 2024-01-27 Saturday 6 true 4 2024 3 20240128 2024-01-28 Sunday 7 true 4 2024 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-2-understanding-the-19-columns","title":"Step 2: Understanding the 19 Columns","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#surrogate-key","title":"Surrogate Key","text":"Column Type Description Example <code>date_sk</code> int Surrogate key in YYYYMMDD format 20240115 <p>The <code>date_sk</code> uses YYYYMMDD format, which: - Is human-readable (you can see the date in the key) - Sorts chronologically - Is efficient for range queries</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#date-columns","title":"Date Columns","text":"Column Type Description Example <code>full_date</code> date The actual date 2024-01-15"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#day-attributes","title":"Day Attributes","text":"Column Type Description Example <code>day_of_week</code> string Day name Monday <code>day_of_week_num</code> int Day number (1=Monday, 7=Sunday) 1 <code>day_of_month</code> int Day of month (1-31) 15 <code>day_of_year</code> int Day of year (1-366) 15 <code>is_weekend</code> bool Weekend flag false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#week-attributes","title":"Week Attributes","text":"Column Type Description Example <code>week_of_year</code> int ISO week number (1-53) 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#month-attributes","title":"Month Attributes","text":"Column Type Description Example <code>month</code> int Month number (1-12) 1 <code>month_name</code> string Month name January <code>is_month_start</code> bool First day of month false <code>is_month_end</code> bool Last day of month false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#quarter-attributes","title":"Quarter Attributes","text":"Column Type Description Example <code>quarter</code> int Calendar quarter (1-4) 1 <code>quarter_name</code> string Quarter name Q1"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#year-attributes","title":"Year Attributes","text":"Column Type Description Example <code>year</code> int Calendar year 2024 <code>is_year_start</code> bool First day of year false <code>is_year_end</code> bool Last day of year false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-calendar","title":"Fiscal Calendar","text":"Column Type Description Example <code>fiscal_year</code> int Fiscal year 2024 <code>fiscal_quarter</code> int Fiscal quarter (1-4) 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-3-understanding-fiscal-calendars","title":"Step 3: Understanding Fiscal Calendars","text":"<p>Many organizations don't use January-December as their fiscal year. Common alternatives:</p> Industry Fiscal Year Start Example US Government October 1 FY2024 = Oct 2023 - Sep 2024 Retail February 1 FY2024 = Feb 2024 - Jan 2025 Education July 1 FY2024 = Jul 2023 - Jun 2024"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#how-fiscal-year-calculation-works","title":"How Fiscal Year Calculation Works","text":"<p>With <code>fiscal_year_start_month: 7</code> (July):</p> Calendar Date Calendar Year Fiscal Year Why June 15, 2024 2024 2024 Before fiscal year start July 1, 2024 2024 2025 Fiscal year starts December 31, 2024 2024 2025 Still FY2025 <p>Formula: If current month &gt;= fiscal_year_start_month, add 1 to calendar year.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-quarters","title":"Fiscal Quarters","text":"<p>With <code>fiscal_year_start_month: 7</code>:</p> Fiscal Quarter Months Q1 July, August, September Q2 October, November, December Q3 January, February, March Q4 April, May, June"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#example-output-with-july-fiscal-year","title":"Example Output with July Fiscal Year","text":"<p>January 2024 dates would show:</p> full_date year fiscal_year quarter fiscal_quarter 2024-01-15 2024 2024 1 3 2024-01-16 2024 2024 1 3 <p>Note: January is calendar Q1 but fiscal Q3 (since the fiscal year started in July).</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-4-the-unknown-date-row","title":"Step 4: The Unknown Date Row","text":"<p>When <code>unknown_member: true</code>, a special row is added with <code>date_sk = 0</code>:</p> Column Value date_sk 0 full_date 1900-01-01 day_of_week Unknown day_of_week_num 0 day_of_month 0 day_of_year 0 is_weekend false week_of_year 0 month 0 month_name Unknown quarter 0 quarter_name Unknown year 0 fiscal_year 0 fiscal_quarter 0 is_month_start false is_month_end false is_year_start false is_year_end false <p>Use case: When fact records have NULL or invalid dates, they can be assigned to date_sk = 0 rather than being dropped.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<p>Here's a complete YAML file for a production-ready date dimension spanning 10 years:</p> <pre><code># File: odibi_date_dimension_tutorial.yaml\nproject: date_dimension_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    description: \"Generate date dimension for 10-year range\"\n    nodes:\n      - name: dim_date\n        description: \"Standard date dimension with fiscal calendar\"\n        transformer: date_dimension\n        params:\n          # 10-year range for typical warehouse\n          start_date: \"2020-01-01\"\n          end_date: \"2030-12-31\"\n\n          # July fiscal year (education/government style)\n          fiscal_year_start_month: 7\n\n          # Add unknown member for orphan handling\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n</code></pre> <p>This generates 4,018 rows (4,017 days + 1 unknown member).</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#common-fiscal-year-configurations","title":"Common Fiscal Year Configurations","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#standard-calendar-year-default","title":"Standard Calendar Year (Default)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 1  # Default\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#us-governmentretail-october","title":"US Government/Retail (October)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 10\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#education-july","title":"Education (July)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 7\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#uk-tax-year-april","title":"UK Tax Year (April)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 4\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#python-api-alternative","title":"Python API Alternative","text":"<pre><code>from odibi.patterns.date_dimension import DateDimensionPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\n\n# Create pattern\npattern = DateDimensionPattern(params={\n    \"start_date\": \"2024-01-15\",\n    \"end_date\": \"2024-01-28\",\n    \"fiscal_year_start_month\": 7,\n    \"unknown_member\": True\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern (no input df needed - generates data)\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\nresult_df = pattern.execute(context)\n\n# View results\nprint(f\"Generated {len(result_df)} date rows\")\nprint(\"\\nColumns:\", result_df.columns.tolist())\nprint(\"\\nSample data:\")\nprint(result_df.head(10).to_string())\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#querying-the-date-dimension","title":"Querying the Date Dimension","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#what-day-of-the-week-had-the-most-sales","title":"\"What day of the week had the most sales?\"","text":"<pre><code>SELECT \n    d.day_of_week,\n    SUM(f.line_total) AS total_sales,\n    COUNT(*) AS order_count\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.day_of_week\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#show-monthly-sales-trend","title":"\"Show monthly sales trend\"","text":"<pre><code>SELECT \n    d.year,\n    d.month_name,\n    SUM(f.line_total) AS monthly_sales\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.year, d.month, d.month_name\nORDER BY d.year, d.month;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#compare-weekday-vs-weekend-sales","title":"\"Compare weekday vs weekend sales\"","text":"<pre><code>SELECT \n    CASE WHEN d.is_weekend THEN 'Weekend' ELSE 'Weekday' END AS period,\n    SUM(f.line_total) AS total_sales,\n    AVG(f.line_total) AS avg_order\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.is_weekend;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-year-performance","title":"\"Fiscal year performance\"","text":"<pre><code>SELECT \n    d.fiscal_year,\n    d.fiscal_quarter,\n    SUM(f.line_total) AS quarterly_sales\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.fiscal_year, d.fiscal_quarter\nORDER BY d.fiscal_year, d.fiscal_quarter;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Date dimensions pre-calculate date attributes for easier querying</li> <li>The pattern generates data\u2014no source file needed</li> <li>19 columns are created automatically covering day, week, month, quarter, year, and fiscal calendar</li> <li>Surrogate keys use YYYYMMDD format (e.g., 20240115)</li> <li>Fiscal calendars can start on any month\u2014the pattern calculates fiscal year and quarter automatically</li> <li>Unknown member (date_sk = 0) handles NULL or invalid dates in fact tables</li> </ul>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you have customer and date dimensions, let's build a fact table that links them together.</p> <p>Next: Fact Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#navigation","title":"Navigation","text":"Previous Up Next Dimension Pattern Tutorials Fact Pattern"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Date Dimension Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/","title":"Fact Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>fact</code> pattern to build fact tables with automatic surrogate key lookups, orphan handling, grain validation, and measure calculations.</p> <p>What You'll Learn: - How surrogate key lookups work - Orphan handling strategies - Grain validation and deduplication - Measure calculations - Joining facts with dimensions</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#source-data","title":"Source Data","text":"<p>We'll use orders data that references customers and products by their natural keys:</p> <p>Source Data (orders.csv) - 30 rows:</p> order_id customer_id product_id order_date quantity unit_price status ORD001 C001 P001 2024-01-15 1 1299.99 completed ORD002 C001 P002 2024-01-15 2 29.99 completed ORD003 C002 P003 2024-01-16 1 249.99 completed ORD004 C003 P004 2024-01-16 3 49.99 completed ORD005 C004 P005 2024-01-17 1 599.99 completed ORD006 C005 P006 2024-01-17 1 149.99 completed ORD007 C006 P007 2024-01-18 2 399.99 completed ORD008 C007 P008 2024-01-18 4 45.99 completed ORD009 C008 P009 2024-01-19 1 79.99 completed ORD010 C009 P010 2024-01-19 1 189.99 completed ORD011 C010 P001 2024-01-20 1 1299.99 completed ORD012 C011 P002 2024-01-20 5 29.99 completed ORD013 C012 P003 2024-01-21 2 249.99 completed ORD014 C001 P004 2024-01-21 1 49.99 completed ORD015 C002 P005 2024-01-22 1 599.99 pending ORD016 C003 P006 2024-01-22 2 149.99 completed ORD017 C004 P007 2024-01-23 1 399.99 completed ORD018 C005 P008 2024-01-23 3 45.99 completed ORD019 C006 P009 2024-01-24 2 79.99 completed ORD020 C007 P010 2024-01-24 1 189.99 completed ORD021 C008 P001 2024-01-25 1 1299.99 completed ORD022 C009 P002 2024-01-25 3 29.99 completed ORD023 C010 P003 2024-01-26 1 249.99 cancelled ORD024 C011 P004 2024-01-26 2 49.99 completed ORD025 C012 P005 2024-01-27 1 599.99 completed ORD026 C001 P006 2024-01-27 1 149.99 completed ORD027 C002 P007 2024-01-28 1 399.99 completed ORD028 C003 P008 2024-01-28 2 45.99 completed ORD029 C004 P009 2024-01-15 1 79.99 completed ORD030 C005 P010 2024-01-16 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dimension-tables-pre-built","title":"Dimension Tables (Pre-built)","text":"<p>Before building fact tables, we need dimension tables. Here are the dimensions from previous tutorials:</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_customer-13-rows-including-unknown","title":"dim_customer (13 rows including unknown)","text":"customer_sk customer_id name region is_current 0 -1 Unknown Unknown true 1 C001 Alice Johnson North true 2 C002 Bob Smith South true 3 C003 Carol White North true 4 C004 David Brown East true 5 C005 Emma Davis West true 6 C006 Frank Miller South true 7 C007 Grace Lee East true 8 C008 Henry Wilson West true 9 C009 Ivy Chen North true 10 C010 Jack Taylor South true 11 C011 Karen Martinez East true 12 C012 Leo Anderson West true"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_product-11-rows-including-unknown","title":"dim_product (11 rows including unknown)","text":"product_sk product_id name category 0 -1 Unknown Unknown 1 P001 Laptop Pro 15 Electronics 2 P002 Wireless Mouse Electronics 3 P003 Office Chair Furniture 4 P004 USB-C Hub Electronics 5 P005 Standing Desk Furniture 6 P006 Mechanical Keyboard Electronics 7 P007 Monitor 27\" Electronics 8 P008 Desk Lamp Furniture 9 P009 Webcam HD Electronics 10 P010 Filing Cabinet Furniture"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_date-15-rows-for-jan-15-28-unknown","title":"dim_date (15 rows for Jan 15-28 + unknown)","text":"date_sk full_date day_of_week month_name 0 1900-01-01 Unknown Unknown 20240115 2024-01-15 Monday January 20240116 2024-01-16 Tuesday January 20240117 2024-01-17 Wednesday January 20240118 2024-01-18 Thursday January 20240119 2024-01-19 Friday January 20240120 2024-01-20 Saturday January 20240121 2024-01-21 Sunday January 20240122 2024-01-22 Monday January 20240123 2024-01-23 Tuesday January 20240124 2024-01-24 Wednesday January 20240125 2024-01-25 Thursday January 20240126 2024-01-26 Friday January 20240127 2024-01-27 Saturday January 20240128 2024-01-28 Sunday January"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-1-understanding-sk-lookups","title":"Step 1: Understanding SK Lookups","text":"<p>The fact pattern's main job is to replace natural keys with surrogate keys:</p> <pre><code>Source:     customer_id = \"C001\"\n                    \u2193\n            Look up in dim_customer where customer_id = \"C001\"\n                    \u2193\nFact:       customer_sk = 1\n</code></pre> <p>This transformation happens for every dimension referenced.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: fact_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_fact_orders\n    nodes:\n      # First, load the dimension tables into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # Then build the fact table\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#the-transformation","title":"The Transformation","text":"<p>Here's what happens to the first 10 rows:</p> <p>Before (Source Data):</p> order_id customer_id product_id order_date quantity unit_price ORD001 C001 P001 2024-01-15 1 1299.99 ORD002 C001 P002 2024-01-15 2 29.99 ORD003 C002 P003 2024-01-16 1 249.99 ORD004 C003 P004 2024-01-16 3 49.99 ORD005 C004 P005 2024-01-17 1 599.99 ORD006 C005 P006 2024-01-17 1 149.99 ORD007 C006 P007 2024-01-18 2 399.99 ORD008 C007 P008 2024-01-18 4 45.99 ORD009 C008 P009 2024-01-19 1 79.99 ORD010 C009 P010 2024-01-19 1 189.99 <p>After (Fact Table):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status load_timestamp ORD001 1 1 20240115 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD002 1 2 20240115 2 29.99 59.98 completed 2024-01-30 10:00:00 ORD003 2 3 20240116 1 249.99 249.99 completed 2024-01-30 10:00:00 ORD004 3 4 20240116 3 49.99 149.97 completed 2024-01-30 10:00:00 ORD005 4 5 20240117 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD006 5 6 20240117 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD007 6 7 20240118 2 399.99 799.98 completed 2024-01-30 10:00:00 ORD008 7 8 20240118 4 45.99 183.96 completed 2024-01-30 10:00:00 ORD009 8 9 20240119 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD010 9 10 20240119 1 189.99 189.99 completed 2024-01-30 10:00:00 <p>Key observations: - <code>customer_id = \"C001\"</code> \u2192 <code>customer_sk = 1</code> - <code>product_id = \"P001\"</code> \u2192 <code>product_sk = 1</code> - <code>order_date = \"2024-01-15\"</code> \u2192 <code>date_sk = 20240115</code> - New column <code>line_total</code> calculated as <code>quantity * unit_price</code> - Original natural keys can be dropped or kept (configurable)</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-2-orphan-handling","title":"Step 2: Orphan Handling","text":"<p>What happens when a source record references a dimension value that doesn't exist?</p> <p>Source with Orphans (orders_with_orphans.csv) - 5 orphan rows:</p> order_id customer_id product_id order_date quantity unit_price ... ... ... ... ... ... ORD031 C999 P001 2024-01-17 1 1299.99 ORD032 C888 P002 2024-01-18 2 29.99 ORD033 C777 P003 2024-01-19 1 249.99 ORD034 C666 P004 2024-01-20 1 49.99 ORD035 C555 P005 2024-01-21 1 599.99 <p>Customer IDs C999, C888, C777, C666, C555 don't exist in dim_customer.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-1-unknown-default","title":"Strategy 1: Unknown (Default)","text":"<pre><code>params:\n  orphan_handling: unknown\n</code></pre> <p>Orphans are assigned to the unknown member (SK = 0):</p> order_id customer_sk product_sk note ORD031 0 1 C999 not found \u2192 SK = 0 ORD032 0 2 C888 not found \u2192 SK = 0 ORD033 0 3 C777 not found \u2192 SK = 0 ORD034 0 4 C666 not found \u2192 SK = 0 ORD035 0 5 C555 not found \u2192 SK = 0 <p>Pros: Data isn't lost, can investigate later Cons: Need to ensure unknown member exists in dimension</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-2-reject","title":"Strategy 2: Reject","text":"<pre><code>params:\n  orphan_handling: reject\n</code></pre> <p>Pipeline fails with an error listing orphan values:</p> <pre><code>OrphanRecordError: Found 5 orphan records for dimension 'dim_customer':\n  - customer_id='C999' (1 record)\n  - customer_id='C888' (1 record)\n  - customer_id='C777' (1 record)\n  - customer_id='C666' (1 record)\n  - customer_id='C555' (1 record)\n</code></pre> <p>Pros: Strict data quality enforcement Cons: Entire load fails, need to fix source data</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-3-quarantine","title":"Strategy 3: Quarantine","text":"<pre><code>params:\n  orphan_handling: quarantine\n</code></pre> <p>Orphan records are routed to a separate quarantine table:</p> <p>fact_orders (valid records): 30 rows</p> <p>fact_orders_quarantine (orphans): 5 rows</p> order_id customer_id orphan_reason ORD031 C999 customer_id not found in dim_customer ORD032 C888 customer_id not found in dim_customer ORD033 C777 customer_id not found in dim_customer ORD034 C666 customer_id not found in dim_customer ORD035 C555 customer_id not found in dim_customer <p>Pros: Valid data loads, orphans are preserved for review Cons: Need to manage quarantine table</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-3-grain-validation","title":"Step 3: Grain Validation","text":"<p>The grain defines what makes a fact row unique. For orders, it's typically <code>order_id</code>.</p> <pre><code>params:\n  grain: [order_id]\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#what-happens-with-duplicate-grain","title":"What Happens with Duplicate Grain?","text":"<p>If your source has duplicate order IDs:</p> order_id customer_id quantity ORD001 C001 1 ORD001 C001 2 <p>The pattern detects this and raises an error:</p> <pre><code>GrainValidationError: Duplicate grain detected in fact_orders\n  Grain columns: ['order_id']\n  Duplicate count: 1\n  Sample duplicates:\n    - order_id='ORD001' (2 occurrences)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#enabling-deduplication","title":"Enabling Deduplication","text":"<p>If duplicates are expected and you want to keep the latest:</p> <pre><code>params:\n  grain: [order_id]\n  deduplicate: true\n  keys: [order_id]\n</code></pre> <p>This keeps only the last occurrence of each order_id.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-4-measure-calculations","title":"Step 4: Measure Calculations","text":"<p>Measures are the numeric values in your fact table. You can: - Pass through existing columns - Rename columns - Calculate derived values</p> <pre><code>params:\n  measures:\n    # Pass through\n    - quantity\n    - unit_price\n\n    # Rename (maps status to order_status)\n    - order_status: status\n\n    # Calculate\n    - line_total: \"quantity * unit_price\"\n    - discount_amount: \"unit_price * 0.1\"\n    - net_total: \"quantity * unit_price * 0.9\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#output-with-calculated-measures","title":"Output with Calculated Measures","text":"order_id quantity unit_price line_total discount_amount net_total ORD001 1 1299.99 1299.99 130.00 1169.99 ORD002 2 29.99 59.98 3.00 53.98 ORD003 1 249.99 249.99 25.00 224.99"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#complete-fact-table-output","title":"Complete Fact Table Output","text":"<p>Here's the complete fact_orders table (30 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status load_timestamp ORD001 1 1 20240115 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD002 1 2 20240115 2 29.99 59.98 completed 2024-01-30 10:00:00 ORD003 2 3 20240116 1 249.99 249.99 completed 2024-01-30 10:00:00 ORD004 3 4 20240116 3 49.99 149.97 completed 2024-01-30 10:00:00 ORD005 4 5 20240117 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD006 5 6 20240117 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD007 6 7 20240118 2 399.99 799.98 completed 2024-01-30 10:00:00 ORD008 7 8 20240118 4 45.99 183.96 completed 2024-01-30 10:00:00 ORD009 8 9 20240119 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD010 9 10 20240119 1 189.99 189.99 completed 2024-01-30 10:00:00 ORD011 10 1 20240120 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD012 11 2 20240120 5 29.99 149.95 completed 2024-01-30 10:00:00 ORD013 12 3 20240121 2 249.99 499.98 completed 2024-01-30 10:00:00 ORD014 1 4 20240121 1 49.99 49.99 completed 2024-01-30 10:00:00 ORD015 2 5 20240122 1 599.99 599.99 pending 2024-01-30 10:00:00 ORD016 3 6 20240122 2 149.99 299.98 completed 2024-01-30 10:00:00 ORD017 4 7 20240123 1 399.99 399.99 completed 2024-01-30 10:00:00 ORD018 5 8 20240123 3 45.99 137.97 completed 2024-01-30 10:00:00 ORD019 6 9 20240124 2 79.99 159.98 completed 2024-01-30 10:00:00 ORD020 7 10 20240124 1 189.99 189.99 completed 2024-01-30 10:00:00 ORD021 8 1 20240125 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD022 9 2 20240125 3 29.99 89.97 completed 2024-01-30 10:00:00 ORD023 10 3 20240126 1 249.99 249.99 cancelled 2024-01-30 10:00:00 ORD024 11 4 20240126 2 49.99 99.98 completed 2024-01-30 10:00:00 ORD025 12 5 20240127 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD026 1 6 20240127 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD027 2 7 20240128 1 399.99 399.99 completed 2024-01-30 10:00:00 ORD028 3 8 20240128 2 45.99 91.98 completed 2024-01-30 10:00:00 ORD029 4 9 20240115 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD030 5 10 20240116 1 189.99 189.99 completed 2024-01-30 10:00:00"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<pre><code># File: odibi_fact_tutorial.yaml\nproject: fact_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_fact_orders\n    description: \"Build fact table with SK lookups\"\n    nodes:\n      # Load dimension tables\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # Build fact table\n      - name: fact_orders\n        description: \"Orders fact table with surrogate key lookups\"\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        transformer: fact\n        params:\n          # Define the grain (uniqueness)\n          grain: [order_id]\n\n          # Define dimension lookups\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true  # Filter to is_current = true\n\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n\n          # Handle missing dimension values\n          orphan_handling: unknown\n\n          # Define measures\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n\n          # Add audit columns\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#python-api-alternative","title":"Python API Alternative","text":"<pre><code>from odibi.patterns.fact import FactPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load source data\norders_df = pd.read_csv(\"examples/tutorials/dimensional_modeling/data/orders.csv\")\n\n# Load dimensions\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\n# Create context and register dimensions\ncontext = EngineContext(df=orders_df, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# Create pattern\npattern = FactPattern(params={\n    \"grain\": [\"order_id\"],\n    \"dimensions\": [\n        {\n            \"source_column\": \"customer_id\",\n            \"dimension_table\": \"dim_customer\",\n            \"dimension_key\": \"customer_id\",\n            \"surrogate_key\": \"customer_sk\",\n            \"scd2\": True\n        },\n        {\n            \"source_column\": \"product_id\",\n            \"dimension_table\": \"dim_product\",\n            \"dimension_key\": \"product_id\",\n            \"surrogate_key\": \"product_sk\"\n        },\n        {\n            \"source_column\": \"order_date\",\n            \"dimension_table\": \"dim_date\",\n            \"dimension_key\": \"full_date\",\n            \"surrogate_key\": \"date_sk\"\n        }\n    ],\n    \"orphan_handling\": \"unknown\",\n    \"measures\": [\n        \"quantity\",\n        \"unit_price\",\n        {\"line_total\": \"quantity * unit_price\"}\n    ],\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"pos\"\n    }\n})\n\n# Execute pattern\nresult_df = pattern.execute(context)\n\nprint(f\"Generated {len(result_df)} fact rows\")\nprint(result_df.head(10))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#querying-the-star-schema","title":"Querying the Star Schema","text":"<p>Now you can run powerful analytical queries:</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#sales-by-region","title":"\"Sales by region\"","text":"<pre><code>SELECT \n    c.region,\n    COUNT(*) AS order_count,\n    SUM(f.line_total) AS total_revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.region\nORDER BY total_revenue DESC;\n</code></pre> region order_count total_revenue North 8 2,599.87 East 8 2,023.87 South 7 2,449.92 West 7 2,629.88"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#daily-sales-trend","title":"\"Daily sales trend\"","text":"<pre><code>SELECT \n    d.full_date,\n    d.day_of_week,\n    SUM(f.line_total) AS daily_revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.full_date, d.day_of_week\nORDER BY d.full_date;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#top-products-by-category","title":"\"Top products by category\"","text":"<pre><code>SELECT \n    p.category,\n    p.name,\n    SUM(f.quantity) AS units_sold,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.category, p.name\nORDER BY revenue DESC;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Surrogate key lookups automatically replace natural keys with dimension SKs</li> <li>scd2: true filters to current dimension rows when looking up SCD2 dimensions</li> <li>Orphan handling strategies: unknown (default), reject, or quarantine</li> <li>Grain validation detects duplicate records at the grain level</li> <li>Measures can be passed through, renamed, or calculated</li> <li>depends_on ensures dimension tables are loaded before the fact pattern runs</li> <li>Audit columns track when and where data was loaded</li> </ul>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you have a complete star schema, let's build aggregated tables for faster reporting.</p> <p>Next: Aggregation Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#navigation","title":"Navigation","text":"Previous Up Next Date Dimension Tutorials Aggregation Pattern"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Fact Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/","title":"Aggregation Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>aggregation</code> pattern to build pre-aggregated tables for faster reporting and dashboards.</p> <p>What You'll Learn: - Why pre-aggregate fact tables - Defining grain and measures - Using aggregate functions (SUM, COUNT, AVG) - Incremental merge strategies (replace vs sum)</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#why-pre-aggregate","title":"Why Pre-Aggregate?","text":"<p>Consider a dashboard showing \"Daily Revenue by Product Category\":</p> <p>Without pre-aggregation:</p> <pre><code>-- Runs against 10 million fact rows every time\nSELECT \n    d.full_date,\n    p.category,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY d.full_date, p.category;\n-- Takes 30 seconds\n</code></pre> <p>With pre-aggregation:</p> <pre><code>-- Runs against 5,000 aggregated rows\nSELECT full_date, category, total_revenue\nFROM agg_daily_category_sales;\n-- Takes 0.1 seconds\n</code></pre> <p>Pre-aggregated tables trade storage for speed. For frequently-queried metrics, this is almost always worth it.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#source-data-fact_orders","title":"Source Data: fact_orders","text":"<p>We'll aggregate the fact table from the previous tutorial (30 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD011 10 1 20240120 1 1299.99 1299.99 completed ORD012 11 2 20240120 5 29.99 149.95 completed ORD013 12 3 20240121 2 249.99 499.98 completed ORD014 1 4 20240121 1 49.99 49.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending ORD016 3 6 20240122 2 149.99 299.98 completed ORD017 4 7 20240123 1 399.99 399.99 completed ORD018 5 8 20240123 3 45.99 137.97 completed ORD019 6 9 20240124 2 79.99 159.98 completed ORD020 7 10 20240124 1 189.99 189.99 completed ORD021 8 1 20240125 1 1299.99 1299.99 completed ORD022 9 2 20240125 3 29.99 89.97 completed ORD023 10 3 20240126 1 249.99 249.99 cancelled ORD024 11 4 20240126 2 49.99 99.98 completed ORD025 12 5 20240127 1 599.99 599.99 completed ORD026 1 6 20240127 1 149.99 149.99 completed ORD027 2 7 20240128 1 399.99 399.99 completed ORD028 3 8 20240128 2 45.99 91.98 completed ORD029 4 9 20240115 1 79.99 79.99 completed ORD030 5 10 20240116 1 189.99 189.99 completed"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-1-basic-aggregation-by-date-and-product","title":"Step 1: Basic Aggregation by Date and Product","text":"<p>Let's aggregate orders by date and product to see daily product sales.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: aggregation_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    nodes:\n      - name: agg_daily_product_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [date_sk, product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: total_quantity\n              expr: \"SUM(quantity)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#understanding-grain-and-measures","title":"Understanding Grain and Measures","text":"<p>Grain: The columns that define uniqueness in the output. Here, each combination of <code>date_sk</code> + <code>product_sk</code> gets one row.</p> <p>Measures: The aggregations to compute. Each measure needs: - <code>name</code>: Output column name - <code>expr</code>: SQL aggregation expression</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#output-agg_daily_product_sales-26-rows","title":"Output: agg_daily_product_sales (26 rows)","text":"<p>Here are all 26 aggregated rows:</p> date_sk product_sk total_revenue order_count total_quantity load_timestamp 20240115 1 1299.99 1 1 2024-01-30 10:00:00 20240115 2 59.98 1 2 2024-01-30 10:00:00 20240115 9 79.99 1 1 2024-01-30 10:00:00 20240116 3 249.99 1 1 2024-01-30 10:00:00 20240116 4 149.97 1 3 2024-01-30 10:00:00 20240116 10 189.99 1 1 2024-01-30 10:00:00 20240117 5 599.99 1 1 2024-01-30 10:00:00 20240117 6 149.99 1 1 2024-01-30 10:00:00 20240118 7 799.98 1 2 2024-01-30 10:00:00 20240118 8 183.96 1 4 2024-01-30 10:00:00 20240119 9 79.99 1 1 2024-01-30 10:00:00 20240119 10 189.99 1 1 2024-01-30 10:00:00 20240120 1 1299.99 1 1 2024-01-30 10:00:00 20240120 2 149.95 1 5 2024-01-30 10:00:00 20240121 3 499.98 1 2 2024-01-30 10:00:00 20240121 4 49.99 1 1 2024-01-30 10:00:00 20240122 5 599.99 1 1 2024-01-30 10:00:00 20240122 6 299.98 1 2 2024-01-30 10:00:00 20240123 7 399.99 1 1 2024-01-30 10:00:00 20240123 8 137.97 1 3 2024-01-30 10:00:00 20240124 9 159.98 1 2 2024-01-30 10:00:00 20240124 10 189.99 1 1 2024-01-30 10:00:00 20240125 1 1299.99 1 1 2024-01-30 10:00:00 20240125 2 89.97 1 3 2024-01-30 10:00:00 20240126 3 249.99 1 1 2024-01-30 10:00:00 20240126 4 99.98 1 2 2024-01-30 10:00:00 20240127 5 599.99 1 1 2024-01-30 10:00:00 20240127 6 149.99 1 1 2024-01-30 10:00:00 20240128 7 399.99 1 1 2024-01-30 10:00:00 20240128 8 91.98 1 2 2024-01-30 10:00:00 <p>Result: 30 fact rows became 26 aggregate rows (some date+product combinations had multiple orders).</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-2-adding-more-measures","title":"Step 2: Adding More Measures","text":"<p>Let's add average and distinct count measures:</p> <pre><code>params:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: total_quantity\n      expr: \"SUM(quantity)\"\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n    - name: max_order_value\n      expr: \"MAX(line_total)\"\n    - name: min_order_value\n      expr: \"MIN(line_total)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>SUM(column)</code> Total of values <code>SUM(line_total)</code> <code>COUNT(*)</code> Number of rows <code>COUNT(*)</code> <code>COUNT(column)</code> Non-null count <code>COUNT(customer_sk)</code> <code>COUNT(DISTINCT column)</code> Unique values <code>COUNT(DISTINCT customer_sk)</code> <code>AVG(column)</code> Average value <code>AVG(line_total)</code> <code>MAX(column)</code> Maximum value <code>MAX(line_total)</code> <code>MIN(column)</code> Minimum value <code>MIN(line_total)</code>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#complex-expressions","title":"Complex Expressions","text":"<p>You can use expressions within aggregations:</p> <pre><code>measures:\n  # Total after 10% discount\n  - name: discounted_revenue\n    expr: \"SUM(line_total * 0.9)\"\n\n  # Margin (if cost column existed)\n  - name: total_margin\n    expr: \"SUM(line_total - cost)\"\n\n  # Discount rate\n  - name: avg_discount_rate\n    expr: \"AVG(discount_amount / line_total)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-3-aggregating-by-different-grains","title":"Step 3: Aggregating by Different Grains","text":""},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#daily-sales-grain-date-only","title":"Daily Sales (Grain: date only)","text":"<pre><code>params:\n  grain: [date_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: unique_products\n      expr: \"COUNT(DISTINCT product_sk)\"\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n</code></pre> <p>Output: agg_daily_sales (14 rows)</p> date_sk total_revenue order_count unique_products unique_customers 20240115 1439.96 3 3 2 20240116 589.95 3 3 3 20240117 749.98 2 2 2 20240118 983.94 2 2 2 20240119 269.98 2 2 2 20240120 1449.94 2 2 2 20240121 549.97 2 2 2 20240122 899.97 2 2 2 20240123 537.96 2 2 2 20240124 349.97 2 2 2 20240125 1389.96 2 2 2 20240126 349.97 2 2 2 20240127 749.98 2 2 2 20240128 491.97 2 2 2"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#product-sales-grain-product-only","title":"Product Sales (Grain: product only)","text":"<pre><code>params:\n  grain: [product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: total_quantity\n      expr: \"SUM(quantity)\"\n</code></pre> <p>Output: agg_product_sales (10 rows)</p> product_sk total_revenue order_count total_quantity 1 3899.97 3 3 2 299.90 3 10 3 999.96 3 4 4 299.94 3 6 5 1799.97 3 3 6 599.96 3 4 7 1599.96 3 4 8 413.91 3 9 9 319.96 3 4 10 569.97 3 3"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-4-incremental-merge-strategies","title":"Step 4: Incremental Merge Strategies","text":"<p>For ongoing pipelines, you don't want to rebuild the entire aggregate table. Incremental strategies allow you to update only affected rows.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#replace-strategy","title":"Replace Strategy","text":"<p>How it works: New aggregates completely replace existing rows for matching grain keys.</p> <pre><code>params:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n  incremental:\n    timestamp_column: load_timestamp\n    merge_strategy: replace\n  target: warehouse.agg_daily_product_sales\n</code></pre> <p>Example Scenario:</p> <p>Existing agg_daily_product_sales:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 20240115 2 59.98 1 20240116 3 249.99 1 <p>New orders arrive for 2024-01-15:</p> order_id product_sk date_sk line_total ORD100 1 20240115 1299.99 <p>New aggregate computed:</p> date_sk product_sk total_revenue order_count 20240115 1 2599.98 2 <p>After Replace Merge:</p> date_sk product_sk total_revenue order_count Note 20240115 1 2599.98 2 Replaced 20240115 2 59.98 1 Unchanged 20240116 3 249.99 1 Unchanged <p>Use case: Best for most scenarios. Handles late-arriving data, corrections, and restatements correctly.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#sum-strategy","title":"Sum Strategy","text":"<p>How it works: New measure values are added to existing values.</p> <pre><code>params:\n  incremental:\n    timestamp_column: load_timestamp\n    merge_strategy: sum\n</code></pre> <p>Example Scenario:</p> <p>Existing:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 <p>New aggregate:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 <p>After Sum Merge:</p> date_sk product_sk total_revenue order_count 20240115 1 2599.98 2 <p>Use case: Only for purely additive metrics (counts, sums) on append-only data.</p> <p>Warning: Do NOT use sum strategy for: - AVG (would become average of averages, incorrect) - COUNT DISTINCT (would overcount) - MIN/MAX (would be wrong) - Data with updates or late-arriving records</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#complete-yaml-example","title":"Complete YAML Example","text":"<p>Here's a complete example with multiple aggregate tables:</p> <pre><code># File: odibi_aggregation_tutorial.yaml\nproject: aggregation_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    description: \"Build all aggregate tables from fact_orders\"\n    nodes:\n      # Daily aggregate at product level\n      - name: agg_daily_product_sales\n        description: \"Daily sales by product\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain:\n            - date_sk\n            - product_sk\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: total_quantity\n              expr: \"SUM(quantity)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          having: \"SUM(line_total) &gt; 0\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: parquet\n          mode: overwrite\n\n      # Daily summary (no product breakdown)\n      - name: agg_daily_sales\n        description: \"Daily sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [date_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: unique_products\n              expr: \"COUNT(DISTINCT product_sk)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          format: parquet\n          mode: overwrite\n\n      # Product summary (no date breakdown)\n      - name: agg_product_sales\n        description: \"Product sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [product_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: total_quantity\n              expr: \"SUM(quantity)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_product_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#using-aggregates-in-queries","title":"Using Aggregates in Queries","text":""},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#dashboard-query-daily-revenue-trend","title":"Dashboard Query: Daily Revenue Trend","text":"<pre><code>SELECT \n    a.date_sk,\n    d.full_date,\n    d.day_of_week,\n    a.total_revenue,\n    a.order_count\nFROM agg_daily_sales a\nJOIN dim_date d ON a.date_sk = d.date_sk\nORDER BY a.date_sk;\n</code></pre> full_date day_of_week total_revenue order_count 2024-01-15 Monday 1439.96 3 2024-01-16 Tuesday 589.95 3 2024-01-17 Wednesday 749.98 2 2024-01-18 Thursday 983.94 2 2024-01-19 Friday 269.98 2 2024-01-20 Saturday 1449.94 2 2024-01-21 Sunday 549.97 2 2024-01-22 Monday 899.97 2 2024-01-23 Tuesday 537.96 2 2024-01-24 Wednesday 349.97 2 2024-01-25 Thursday 1389.96 2 2024-01-26 Friday 349.97 2 2024-01-27 Saturday 749.98 2 2024-01-28 Sunday 491.97 2"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#dashboard-query-top-products","title":"Dashboard Query: Top Products","text":"<pre><code>SELECT \n    p.name AS product_name,\n    p.category,\n    a.total_revenue,\n    a.order_count,\n    a.total_quantity\nFROM agg_product_sales a\nJOIN dim_product p ON a.product_sk = p.product_sk\nORDER BY a.total_revenue DESC\nLIMIT 5;\n</code></pre> product_name category total_revenue order_count total_quantity Laptop Pro 15 Electronics 3899.97 3 3 Standing Desk Furniture 1799.97 3 3 Monitor 27\" Electronics 1599.96 3 4 Office Chair Furniture 999.96 3 4 Mechanical Keyboard Electronics 599.96 3 4"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Pre-aggregation speeds up reporting by reducing data volume</li> <li>Grain defines the uniqueness (GROUP BY columns) of aggregate rows</li> <li>Measures define what to compute (SUM, COUNT, AVG, etc.)</li> <li>Complex expressions can be used within aggregations</li> <li>Replace strategy is best for most incremental scenarios</li> <li>Sum strategy works only for purely additive metrics on append-only data</li> <li>Aggregate tables are joined back to dimensions for rich reporting</li> </ul>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#next-steps","title":"Next Steps","text":"<p>Now let's put it all together with a complete star schema example.</p> <p>Next: Full Star Schema Tutorial</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#navigation","title":"Navigation","text":"Previous Up Next Fact Pattern Tutorials Full Star Schema"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Aggregation Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/","title":"Full Star Schema Tutorial","text":"<p>In this tutorial, you'll build a complete star schema from start to finish, combining all the patterns from previous tutorials into a single, cohesive data warehouse.</p> <p>What You'll Build: - <code>dim_customer</code> - Customer dimension (SCD2) - <code>dim_product</code> - Product dimension (SCD1) - <code>dim_date</code> - Date dimension (generated) - <code>fact_orders</code> - Orders fact table - <code>agg_daily_sales</code> - Daily sales aggregate</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#the-complete-star-schema","title":"The Complete Star Schema","text":"<pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n    AGG_DAILY_SALES ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        string order_id PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        int quantity\n        decimal unit_price\n        decimal line_total\n        string status\n        timestamp load_timestamp\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string email\n        string region\n        string city\n        string state\n        date valid_from\n        date valid_to\n        bool is_current\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n        string subcategory\n        decimal price\n        decimal cost\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        int month\n        string month_name\n        int quarter\n        int year\n        int fiscal_year\n        int fiscal_quarter\n    }\n\n    AGG_DAILY_SALES {\n        int date_sk PK\n        decimal total_revenue\n        int order_count\n        int unique_customers\n        int unique_products\n    }\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#source-data-files","title":"Source Data Files","text":"<p>All source data is in <code>examples/tutorials/dimensional_modeling/data/</code>:</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#customerscsv-12-rows","title":"customers.csv (12 rows)","text":"customer_id name email region city state C001 Alice Johnson alice@example.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david@example.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace@example.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#productscsv-10-rows","title":"products.csv (10 rows)","text":"product_id name category subcategory price cost P001 Laptop Pro 15 Electronics Computers 1299.99 850.00 P002 Wireless Mouse Electronics Accessories 29.99 12.00 P003 Office Chair Furniture Seating 249.99 120.00 P004 USB-C Hub Electronics Accessories 49.99 22.00 P005 Standing Desk Furniture Desks 599.99 320.00 P006 Mechanical Keyboard Electronics Accessories 149.99 65.00 P007 Monitor 27\" Electronics Displays 399.99 210.00 P008 Desk Lamp Furniture Lighting 45.99 18.00 P009 Webcam HD Electronics Accessories 79.99 35.00 P010 Filing Cabinet Furniture Storage 189.99 95.00"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#orderscsv-30-rows","title":"orders.csv (30 rows)","text":"order_id customer_id product_id order_date quantity unit_price status ORD001 C001 P001 2024-01-15 1 1299.99 completed ORD002 C001 P002 2024-01-15 2 29.99 completed ORD003 C002 P003 2024-01-16 1 249.99 completed ORD004 C003 P004 2024-01-16 3 49.99 completed ORD005 C004 P005 2024-01-17 1 599.99 completed ORD006 C005 P006 2024-01-17 1 149.99 completed ORD007 C006 P007 2024-01-18 2 399.99 completed ORD008 C007 P008 2024-01-18 4 45.99 completed ORD009 C008 P009 2024-01-19 1 79.99 completed ORD010 C009 P010 2024-01-19 1 189.99 completed ORD011 C010 P001 2024-01-20 1 1299.99 completed ORD012 C011 P002 2024-01-20 5 29.99 completed ORD013 C012 P003 2024-01-21 2 249.99 completed ORD014 C001 P004 2024-01-21 1 49.99 completed ORD015 C002 P005 2024-01-22 1 599.99 pending ORD016 C003 P006 2024-01-22 2 149.99 completed ORD017 C004 P007 2024-01-23 1 399.99 completed ORD018 C005 P008 2024-01-23 3 45.99 completed ORD019 C006 P009 2024-01-24 2 79.99 completed ORD020 C007 P010 2024-01-24 1 189.99 completed ORD021 C008 P001 2024-01-25 1 1299.99 completed ORD022 C009 P002 2024-01-25 3 29.99 completed ORD023 C010 P003 2024-01-26 1 249.99 cancelled ORD024 C011 P004 2024-01-26 2 49.99 completed ORD025 C012 P005 2024-01-27 1 599.99 completed ORD026 C001 P006 2024-01-27 1 149.99 completed ORD027 C002 P007 2024-01-28 1 399.99 completed ORD028 C003 P008 2024-01-28 2 45.99 completed ORD029 C004 P009 2024-01-15 1 79.99 completed ORD030 C005 P010 2024-01-16 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#complete-yaml-configuration","title":"Complete YAML Configuration","text":"<p>Here's the full pipeline that builds everything:</p> <pre><code># File: examples/tutorials/dimensional_modeling/star_schema.yaml\nproject: retail_star_schema\nengine: pandas\n\nconnections:\n  source:\n    type: file\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: file\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  # ==========================================\n  # PIPELINE 1: Build all dimensions\n  # ==========================================\n  - pipeline: build_dimensions\n    description: \"Build customer, product, and date dimensions\"\n    nodes:\n      # ------------------------------------------\n      # Customer Dimension (SCD Type 2)\n      # ------------------------------------------\n      - name: dim_customer\n        description: \"Customer dimension with full history tracking\"\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: customer_id\n          surrogate_key: customer_sk\n          scd_type: 2\n          track_columns:\n            - name\n            - email\n            - region\n            - city\n            - state\n          target: warehouse.dim_customer\n          valid_from_col: valid_from\n          valid_to_col: valid_to\n          is_current_col: is_current\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n\n      # ------------------------------------------\n      # Product Dimension (SCD Type 1)\n      # ------------------------------------------\n      - name: dim_product\n        description: \"Product dimension with overwrite updates\"\n        read:\n          connection: source\n          path: products.csv\n          format: csv\n\n        transformer: dimension\n        params:\n          natural_key: product_id\n          surrogate_key: product_sk\n          scd_type: 1\n          track_columns:\n            - name\n            - category\n            - subcategory\n            - price\n            - cost\n          target: warehouse.dim_product\n          unknown_member: true\n          audit:\n            load_timestamp: true\n            source_system: \"inventory\"\n\n        write:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n          mode: overwrite\n\n      # ------------------------------------------\n      # Date Dimension (Generated)\n      # ------------------------------------------\n      - name: dim_date\n        description: \"Date dimension covering Jan 2024\"\n        transformer: date_dimension\n        params:\n          start_date: \"2024-01-01\"\n          end_date: \"2024-01-31\"\n          fiscal_year_start_month: 7\n          unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n\n  # ==========================================\n  # PIPELINE 2: Build fact tables\n  # ==========================================\n  - pipeline: build_facts\n    description: \"Build fact_orders with SK lookups\"\n    nodes:\n      # Load dimensions into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # ------------------------------------------\n      # Orders Fact Table\n      # ------------------------------------------\n      - name: fact_orders\n        description: \"Orders fact with all dimension lookups\"\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n              scd2: true\n            - source_column: product_id\n              dimension_table: dim_product\n              dimension_key: product_id\n              surrogate_key: product_sk\n            - source_column: order_date\n              dimension_table: dim_date\n              dimension_key: full_date\n              surrogate_key: date_sk\n          orphan_handling: unknown\n          measures:\n            - quantity\n            - unit_price\n            - line_total: \"quantity * unit_price\"\n          audit:\n            load_timestamp: true\n            source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n\n  # ==========================================\n  # PIPELINE 3: Build aggregates\n  # ==========================================\n  - pipeline: build_aggregates\n    description: \"Build daily sales aggregate\"\n    nodes:\n      # ------------------------------------------\n      # Daily Sales Aggregate\n      # ------------------------------------------\n      - name: agg_daily_sales\n        description: \"Daily sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        transformer: aggregation\n        params:\n          grain: [date_sk]\n          measures:\n            - name: total_revenue\n              expr: \"SUM(line_total)\"\n            - name: order_count\n              expr: \"COUNT(*)\"\n            - name: unique_customers\n              expr: \"COUNT(DISTINCT customer_sk)\"\n            - name: unique_products\n              expr: \"COUNT(DISTINCT product_sk)\"\n            - name: avg_order_value\n              expr: \"AVG(line_total)\"\n          audit:\n            load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Run the pipelines in order:</p> <pre><code># Build dimensions first\nodibi run --config star_schema.yaml --pipeline build_dimensions\n\n# Then build facts (requires dimensions)\nodibi run --config star_schema.yaml --pipeline build_facts\n\n# Finally build aggregates (requires facts)\nodibi run --config star_schema.yaml --pipeline build_aggregates\n</code></pre> <p>Or run everything:</p> <pre><code>odibi run --config star_schema.yaml\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#final-table-schemas-and-sample-data","title":"Final Table Schemas and Sample Data","text":""},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_customer-13-rows","title":"dim_customer (13 rows)","text":"<p>Schema:</p> Column Type Description customer_sk int Surrogate key customer_id string Natural key name string Customer name email string Email address region string Geographic region city string City state string State code valid_from timestamp Version start date valid_to timestamp Version end date (NULL if current) is_current boolean Current version flag load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (13 rows):</p> customer_sk customer_id name email region city is_current 0 -1 Unknown Unknown Unknown Unknown true 1 C001 Alice Johnson alice@example.com North Chicago true 2 C002 Bob Smith bob@example.com South Houston true 3 C003 Carol White carol@example.com North Detroit true 4 C004 David Brown david@example.com East New York true 5 C005 Emma Davis emma@example.com West Seattle true 6 C006 Frank Miller frank@example.com South Miami true 7 C007 Grace Lee grace@example.com East Boston true 8 C008 Henry Wilson henry@example.com West Portland true 9 C009 Ivy Chen ivy@example.com North Minneapolis true 10 C010 Jack Taylor jack@example.com South Dallas true 11 C011 Karen Martinez karen@example.com East Philadelphia true 12 C012 Leo Anderson leo@example.com West Denver true"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_product-11-rows","title":"dim_product (11 rows)","text":"<p>Schema:</p> Column Type Description product_sk int Surrogate key product_id string Natural key name string Product name category string Product category subcategory string Product subcategory price decimal List price cost decimal Unit cost load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (11 rows):</p> product_sk product_id name category subcategory price cost 0 -1 Unknown Unknown Unknown 0.00 0.00 1 P001 Laptop Pro 15 Electronics Computers 1299.99 850.00 2 P002 Wireless Mouse Electronics Accessories 29.99 12.00 3 P003 Office Chair Furniture Seating 249.99 120.00 4 P004 USB-C Hub Electronics Accessories 49.99 22.00 5 P005 Standing Desk Furniture Desks 599.99 320.00 6 P006 Mechanical Keyboard Electronics Accessories 149.99 65.00 7 P007 Monitor 27\" Electronics Displays 399.99 210.00 8 P008 Desk Lamp Furniture Lighting 45.99 18.00 9 P009 Webcam HD Electronics Accessories 79.99 35.00 10 P010 Filing Cabinet Furniture Storage 189.99 95.00"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_date-32-rows-for-january-unknown","title":"dim_date (32 rows for January + unknown)","text":"<p>Sample Data (first 15 rows):</p> date_sk full_date day_of_week month_name quarter_name year fiscal_year fiscal_quarter 0 1900-01-01 Unknown Unknown Unknown 0 0 0 20240101 2024-01-01 Monday January Q1 2024 2024 3 20240102 2024-01-02 Tuesday January Q1 2024 2024 3 20240103 2024-01-03 Wednesday January Q1 2024 2024 3 20240104 2024-01-04 Thursday January Q1 2024 2024 3 20240105 2024-01-05 Friday January Q1 2024 2024 3 20240106 2024-01-06 Saturday January Q1 2024 2024 3 20240107 2024-01-07 Sunday January Q1 2024 2024 3 20240108 2024-01-08 Monday January Q1 2024 2024 3 20240109 2024-01-09 Tuesday January Q1 2024 2024 3 20240110 2024-01-10 Wednesday January Q1 2024 2024 3 20240111 2024-01-11 Thursday January Q1 2024 2024 3 20240112 2024-01-12 Friday January Q1 2024 2024 3 20240113 2024-01-13 Saturday January Q1 2024 2024 3 20240114 2024-01-14 Sunday January Q1 2024 2024 3"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#fact_orders-30-rows","title":"fact_orders (30 rows)","text":"<p>Schema:</p> Column Type Description order_id string Order identifier (grain) customer_sk int Customer surrogate key product_sk int Product surrogate key date_sk int Date surrogate key quantity int Quantity ordered unit_price decimal Price per unit line_total decimal Calculated: quantity \u00d7 unit_price status string Order status load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (first 15 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD011 10 1 20240120 1 1299.99 1299.99 completed ORD012 11 2 20240120 5 29.99 149.95 completed ORD013 12 3 20240121 2 249.99 499.98 completed ORD014 1 4 20240121 1 49.99 49.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#agg_daily_sales-14-rows","title":"agg_daily_sales (14 rows)","text":"<p>Sample Data (all 14 rows):</p> date_sk total_revenue order_count unique_customers unique_products avg_order_value 20240115 1439.96 3 2 3 479.99 20240116 589.95 3 3 3 196.65 20240117 749.98 2 2 2 374.99 20240118 983.94 2 2 2 491.97 20240119 269.98 2 2 2 134.99 20240120 1449.94 2 2 2 724.97 20240121 549.97 2 2 2 274.99 20240122 899.97 2 2 2 449.99 20240123 537.96 2 2 2 268.98 20240124 349.97 2 2 2 174.99 20240125 1389.96 2 2 2 694.98 20240126 349.97 2 2 2 174.99 20240127 749.98 2 2 2 374.99 20240128 491.97 2 2 2 245.99"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#sample-analytical-queries","title":"Sample Analytical Queries","text":""},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#revenue-by-region","title":"Revenue by Region","text":"<pre><code>SELECT \n    c.region,\n    COUNT(DISTINCT f.order_id) AS orders,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.region\nORDER BY revenue DESC;\n</code></pre> region orders revenue North 8 2,599.87 West 7 2,629.88 South 7 2,449.92 East 8 2,023.87"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#revenue-by-category","title":"Revenue by Category","text":"<pre><code>SELECT \n    p.category,\n    COUNT(*) AS orders,\n    SUM(f.line_total) AS revenue,\n    AVG(f.line_total) AS avg_order\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.category\nORDER BY revenue DESC;\n</code></pre> category orders revenue avg_order Electronics 18 6,619.63 367.76 Furniture 12 3,183.91 265.33"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#daily-trend-with-day-of-week","title":"Daily Trend with Day of Week","text":"<pre><code>SELECT \n    d.full_date,\n    d.day_of_week,\n    a.total_revenue,\n    a.order_count\nFROM agg_daily_sales a\nJOIN dim_date d ON a.date_sk = d.date_sk\nORDER BY a.date_sk;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#top-customers","title":"Top Customers","text":"<pre><code>SELECT \n    c.name,\n    c.region,\n    COUNT(*) AS orders,\n    SUM(f.line_total) AS total_spent\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.customer_sk, c.name, c.region\nORDER BY total_spent DESC\nLIMIT 5;\n</code></pre> name region orders total_spent Alice Johnson North 4 1,559.95 Bob Smith South 3 1,049.97 Carol White North 3 541.93 David Brown East 3 1,079.97 Emma Davis West 3 477.97"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#data-flow-diagram","title":"Data Flow Diagram","text":"<pre><code>flowchart TB\n    subgraph Sources[\"Source Data\"]\n        S1[customers.csv]\n        S2[products.csv]\n        S3[orders.csv]\n    end\n\n    subgraph Dimensions[\"Dimension Layer\"]\n        D1[dim_customer&lt;br/&gt;SCD Type 2]\n        D2[dim_product&lt;br/&gt;SCD Type 1]\n        D3[dim_date&lt;br/&gt;Generated]\n    end\n\n    subgraph Facts[\"Fact Layer\"]\n        F1[fact_orders&lt;br/&gt;30 rows]\n    end\n\n    subgraph Aggregates[\"Aggregate Layer\"]\n        A1[agg_daily_sales&lt;br/&gt;14 rows]\n    end\n\n    S1 --&gt; D1\n    S2 --&gt; D2\n    S3 --&gt; F1\n\n    D1 --&gt; F1\n    D2 --&gt; F1\n    D3 --&gt; F1\n\n    F1 --&gt; A1\n\n    style Sources fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Dimensions fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Facts fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Aggregates fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>How to combine all patterns into a complete star schema</li> <li>The order of operations: dimensions first, then facts, then aggregates</li> <li>How to organize pipelines for maintainability</li> <li>Pipeline dependencies using <code>depends_on</code></li> <li>How to query the completed star schema for analytics</li> </ul>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#congratulations","title":"Congratulations!","text":"<p>You've built a complete dimensional data warehouse! This foundation can scale to handle millions of rows and complex business requirements.</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#next-steps","title":"Next Steps","text":"<p>Now that you have a working star schema, let's add a semantic layer to make querying even easier.</p> <p>Next: Semantic Layer Introduction</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#navigation","title":"Navigation","text":"Previous Up Next Aggregation Pattern Tutorials Semantic Layer Intro"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/","title":"Semantic Layer Introduction","text":"<p>In this tutorial, you'll learn what a semantic layer is, why it's valuable, and how it sits on top of your star schema to provide a business-friendly query interface.</p> <p>What You'll Learn: - What is a semantic layer? - Why not just write SQL? - Metrics, dimensions, and materializations - When to use pipelines vs semantic layer</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#what-is-a-semantic-layer","title":"What is a Semantic Layer?","text":"<p>A semantic layer is a translation layer between your raw data and your business users. It defines business concepts (like \"revenue\" or \"active customers\") once, and lets everyone query them consistently.</p> <p>Think of it like a glossary for your data:</p> Business Term Technical Definition Revenue <code>SUM(line_total)</code> from <code>fact_orders</code> where <code>status = 'completed'</code> Order Count <code>COUNT(*)</code> from <code>fact_orders</code> Active Customer Customer with order in last 90 days North Region <code>region = 'North'</code> in <code>dim_customer</code> <p>Without a semantic layer, everyone writes their own SQL\u2014and everyone might calculate \"revenue\" differently.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#the-problem-inconsistent-definitions","title":"The Problem: Inconsistent Definitions","text":""},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#without-semantic-layer","title":"Without Semantic Layer","text":"<p>Marketing team:</p> <pre><code>-- \"Revenue\" = all orders\nSELECT SUM(line_total) AS revenue FROM fact_orders;\n-- Result: $9,803.54\n</code></pre> <p>Finance team:</p> <pre><code>-- \"Revenue\" = only completed orders\nSELECT SUM(line_total) AS revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Result: $8,953.56\n</code></pre> <p>Executive dashboard:</p> <pre><code>-- \"Revenue\" = completed orders, excluding discounts\nSELECT SUM(line_total * 0.95) AS revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Result: $8,505.88\n</code></pre> <p>Result: Three different \"revenue\" numbers in the same meeting. Chaos.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#with-semantic-layer","title":"With Semantic Layer","text":"<pre><code># Everyone uses the same metric definition\nresult = query.execute(\"revenue\", context)\n# Result: $8,953.56 (always)\n</code></pre> <p>The semantic layer enforces a single, governed definition of \"revenue.\"</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#why-not-just-write-sql","title":"Why Not Just Write SQL?","text":"<p>SQL is powerful, but it has limitations for business users:</p> Challenge Without Semantic Layer With Semantic Layer Complex joins Users must know table relationships Automatic join handling Consistent definitions Everyone writes their own Define once, use everywhere Filter logic Repeated in every query Embedded in metric definition Aggregation errors Easy to make mistakes Pre-validated expressions Self-service Requires SQL expertise Business-friendly syntax Governance No central control Single source of truth"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#how-the-semantic-layer-fits","title":"How the Semantic Layer Fits","text":"<p>The semantic layer sits on top of your star schema:</p> <pre><code>flowchart TB\n    subgraph Sources[\"Source Systems\"]\n        S1[CRM]\n        S2[ERP]\n        S3[POS]\n    end\n\n    subgraph Pipeline[\"Odibi Pipelines (YAML)\"]\n        P1[dimension pattern]\n        P2[fact pattern]\n        P3[aggregation pattern]\n    end\n\n    subgraph StarSchema[\"Star Schema\"]\n        D1[(dim_customer)]\n        D2[(dim_product)]\n        D3[(dim_date)]\n        F1[(fact_orders)]\n        A1[(agg_daily_sales)]\n    end\n\n    subgraph Semantic[\"Semantic Layer (Python API)\"]\n        M1[Metrics]\n        M2[Dimensions]\n        M3[Materializations]\n    end\n\n    subgraph Consumers[\"Consumers\"]\n        C1[Dashboards]\n        C2[Reports]\n        C3[Ad-hoc Queries]\n        C4[Data Apps]\n    end\n\n    S1 --&gt; P1\n    S2 --&gt; P2\n    S3 --&gt; P3\n\n    P1 --&gt; D1\n    P1 --&gt; D2\n    P2 --&gt; D3\n    P2 --&gt; F1\n    P3 --&gt; A1\n\n    D1 --&gt; M1\n    D2 --&gt; M1\n    D3 --&gt; M1\n    F1 --&gt; M1\n    A1 --&gt; M1\n\n    M1 --&gt; C1\n    M2 --&gt; C2\n    M3 --&gt; C3\n    M1 --&gt; C4\n\n    style Sources fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Pipeline fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style StarSchema fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Semantic fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Consumers fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre> <p>Odibi Pipelines build the data (dimensions, facts, aggregates). Semantic Layer defines how to query the data (metrics, dimensions, materializations).</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#core-concepts","title":"Core Concepts","text":""},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#metrics","title":"Metrics","text":"<p>A metric is a measurable value that can be aggregated. It answers \"how much?\" or \"how many?\"</p> <pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre> <p>Examples: - Revenue (<code>SUM(line_total)</code>) - Order Count (<code>COUNT(*)</code>) - Average Order Value (<code>AVG(line_total)</code>) - Unique Customers (<code>COUNT(DISTINCT customer_sk)</code>)</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#dimensions","title":"Dimensions","text":"<p>A dimension is an attribute for grouping and filtering. It answers \"by what?\" or \"for what?\"</p> <pre><code>dimensions:\n  - name: region\n    source: dim_customer\n    column: region\n</code></pre> <p>Examples: - Region (North, South, East, West) - Category (Electronics, Furniture) - Month (January, February, ...) - Day of Week (Monday, Tuesday, ...)</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#materializations","title":"Materializations","text":"<p>A materialization pre-computes metrics at a specific grain and saves them to a table. It answers \"what should be pre-calculated for dashboards?\"</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n    schedule: \"0 2 1 * *\"  # Monthly\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#when-to-use-what","title":"When to Use What","text":"Task Solution Build dimension tables from source Odibi Pipeline: <code>dimension</code> pattern Build fact tables from source Odibi Pipeline: <code>fact</code> pattern Build scheduled aggregates Odibi Pipeline: <code>aggregation</code> pattern Ad-hoc metric queries Semantic Layer: <code>SemanticQuery</code> Self-service analytics Semantic Layer with dimensions Dashboard metrics Semantic Layer: <code>Materializer</code>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#pipelines-vs-semantic-layer","title":"Pipelines vs Semantic Layer","text":"<p>Use Pipelines when: - Building the star schema from source data - Scheduled ETL/ELT processes - Transforming and cleaning data - Generating surrogate keys</p> <p>Use Semantic Layer when: - Defining business metrics consistently - Enabling self-service analytics - Pre-computing dashboard metrics - Creating a governed metric catalog</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#unified-project-api-recommended","title":"Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is through the unified <code>Project</code> API. This connects your pipelines and semantic layer seamlessly:</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#1-add-semantic-config-to-odibiyaml","title":"1. Add Semantic Config to odibi.yaml","text":"<pre><code># odibi.yaml\nproject: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#2-query-with-two-lines-of-code","title":"2. Query with Two Lines of Code","text":"<pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>That's it! The <code>Project</code> class: - Reads connections and pipelines from your YAML - Resolves <code>$build_warehouse.fact_orders</code> \u2192 node's write path - Auto-loads Delta tables when queried - No manual <code>context.register()</code> calls needed</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#manual-approach","title":"Manual Approach","text":"<p>If you prefer more control, you can use the semantic layer components directly.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#example-revenue-metric","title":"Example: Revenue Metric","text":"<p>Let's see how a simple metric works:</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#1-define-the-metric","title":"1. Define the Metric","text":"<pre><code>from odibi.semantics import MetricDefinition\n\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from completed orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#2-create-the-config","title":"2. Create the Config","text":"<pre><code>from odibi.semantics import SemanticLayerConfig, DimensionDefinition\n\nconfig = SemanticLayerConfig(\n    metrics=[revenue],\n    dimensions=[\n        DimensionDefinition(\n            name=\"region\",\n            source=\"dim_customer\",\n            column=\"region\"\n        )\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#3-query-the-metric","title":"3. Query the Metric","text":"<pre><code>from odibi.semantics import SemanticQuery\n\nquery = SemanticQuery(config)\n\n# Total revenue\nresult = query.execute(\"revenue\", context)\nprint(result.df)\n# | revenue    |\n# |------------|\n# | 8,953.56   |\n\n# Revenue by region\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df)\n# | region | revenue  |\n# |--------|----------|\n# | North  | 2,549.88 |\n# | South  | 2,349.93 |\n# | East   | 1,923.88 |\n# | West   | 2,129.87 |\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#the-query-syntax","title":"The Query Syntax","text":"<p>Semantic queries use a simple, business-friendly syntax:</p> <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre> <p>Examples:</p> Query Meaning <code>\"revenue\"</code> Total revenue <code>\"revenue BY region\"</code> Revenue grouped by region <code>\"revenue, order_count BY region\"</code> Multiple metrics by region <code>\"revenue BY region, month\"</code> Revenue by region and month <code>\"revenue BY region WHERE year = 2024\"</code> Filtered revenue by region"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#benefits-summary","title":"Benefits Summary","text":"Benefit Description Consistency One definition of \"revenue\" everywhere Governance Central control over metric logic Self-Service Business users query without SQL Performance Pre-computed materializations for dashboards Discoverability Metrics are documented and cataloged Maintainability Change definition once, updates everywhere"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#architecture-overview","title":"Architecture Overview","text":"<pre><code>classDiagram\n    class SemanticLayerConfig {\n        +metrics: List[MetricDefinition]\n        +dimensions: List[DimensionDefinition]\n        +materializations: List[MaterializationConfig]\n        +get_metric(name)\n        +get_dimension(name)\n    }\n\n    class MetricDefinition {\n        +name: str\n        +description: str\n        +expr: str\n        +source: str\n        +filters: List[str]\n    }\n\n    class DimensionDefinition {\n        +name: str\n        +source: str\n        +column: str\n        +hierarchy: List[str]\n    }\n\n    class SemanticQuery {\n        +config: SemanticLayerConfig\n        +execute(query_string, context)\n        +parse(query_string)\n        +validate(parsed_query)\n    }\n\n    class Materializer {\n        +config: SemanticLayerConfig\n        +execute(name, context)\n        +execute_all(context)\n    }\n\n    SemanticLayerConfig --&gt; MetricDefinition\n    SemanticLayerConfig --&gt; DimensionDefinition\n    SemanticQuery --&gt; SemanticLayerConfig\n    Materializer --&gt; SemanticLayerConfig\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>A semantic layer translates business concepts into technical queries</li> <li>It ensures consistent definitions across all users</li> <li>Metrics are measurable values (SUM, COUNT, AVG)</li> <li>Dimensions are grouping attributes (region, category, date)</li> <li>Materializations pre-compute metrics for performance</li> <li>Pipelines build the data; semantic layer queries it</li> <li>The query syntax is simple: <code>\"metric BY dimension WHERE filter\"</code></li> </ul>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to define metrics in detail.</p> <p>Next: Defining Metrics</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#navigation","title":"Navigation","text":"Previous Up Next Full Star Schema Tutorials Defining Metrics"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#reference","title":"Reference","text":"<p>For complete documentation, see: Semantic Layer Overview</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/","title":"Defining Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to define metrics in the Odibi semantic layer, from simple aggregations to complex filtered and derived metrics.</p> <p>What You'll Learn: - Simple metrics (SUM, COUNT, AVG) - Filtered metrics (with WHERE conditions) - Multiple metrics together - Derived metrics (calculated from other metrics)</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#source-data-fact_orders","title":"Source Data: fact_orders","text":"<p>We'll use the fact_orders table from Tutorial 06 (15 sample rows shown):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending ORD023 10 3 20240126 1 249.99 249.99 cancelled ... ... ... ... ... ... ... ... <p>Total: 30 rows - 27 completed - 1 pending - 2 cancelled</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-1-define-a-simple-metric","title":"Step 1: Define a Simple Metric","text":"<p>Let's start with the most basic metric: total revenue.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import MetricDefinition\n\n# Define a simple revenue metric\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from all orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from all orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"revenue\"</code> How you'll reference this metric in queries <code>description</code> <code>\"Total revenue...\"</code> Human-readable documentation <code>expr</code> <code>\"SUM(line_total)\"</code> SQL aggregation expression <code>source</code> <code>\"fact_orders\"</code> Table to aggregate from"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#source-notation","title":"Source Notation","text":"<p>The <code>source</code> field supports three formats:</p> Format Example When to Use $pipeline.node <code>$build_warehouse.fact_orders</code> With <code>Project</code> API (recommended) connection.path <code>gold.fact_orders</code> External tables not in pipelines bare name <code>fact_orders</code> Manual setup with <code>context.register()</code> <p>Note: This tutorial shows both the <code>Project</code> API (recommended) and manual setup approaches. The manual sections use bare names like <code>source: fact_orders</code> because they match what you register with <code>context.register(\"fact_orders\", df)</code>.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#option-1-node-reference-recommended","title":"Option 1: Node Reference (Recommended)","text":"<p>Reference the pipeline node that produces the table. The semantic layer automatically reads from wherever that node writes:</p> <pre><code># odibi.yaml\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References the node above\n</code></pre> <p>This approach: - DRY - No duplication; the node already knows its write location - Auto-synced - If you change the node's write config, the semantic layer follows - Uses the same <code>$pipeline.node</code> pattern as cross-pipeline <code>inputs</code></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#option-2-connectionpath-explicit","title":"Option 2: Connection.Path (Explicit)","text":"<p>For tables that exist outside pipelines or when you want explicit control:</p> <pre><code>semantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: gold.fact_orders    # Resolves to /mnt/data/gold/fact_orders\n</code></pre> <p>Nested paths are supported. The split happens on the first dot only, so everything after becomes the path:</p> <pre><code># Given connection:\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# These all work:\nsource: gold.fact_orders              # \u2192 /mnt/data/gold/fact_orders\nsource: gold.oee/plant_a/metrics      # \u2192 /mnt/data/gold/oee/plant_a/metrics\nsource: gold.domain/v2/fact_sales     # \u2192 /mnt/data/gold/domain/v2/fact_sales\n</code></pre> <p>For Unity Catalog connections with <code>catalog</code> + <code>schema_name</code>:</p> <pre><code>connections:\n  gold:\n    type: delta\n    catalog: main\n    schema_name: gold_db\n\nsource: gold.fact_orders    # \u2192 main.gold_db.fact_orders\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-result","title":"Query Result","text":"<pre><code>result = query.execute(\"revenue\", context)\n</code></pre> revenue 9,803.54 <p>This includes ALL orders (completed, pending, cancelled).</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-2-define-a-filtered-metric","title":"Step 2: Define a Filtered Metric","text":"<p>Usually, you only want \"revenue\" to include completed orders. Add a filter:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_1","title":"Python Code","text":"<pre><code>completed_revenue = MetricDefinition(\n    name=\"completed_revenue\",\n    description=\"Revenue from completed orders only\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_1","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: completed_revenue\n    description: \"Revenue from completed orders only\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#how-filters-work","title":"How Filters Work","text":"<p>Without filter (revenue):</p> <pre><code>SELECT SUM(line_total) AS revenue FROM fact_orders;\n-- Uses all 30 rows\n</code></pre> <p>With filter (completed_revenue):</p> <pre><code>SELECT SUM(line_total) AS completed_revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Uses only 27 completed rows\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#data-comparison","title":"Data Comparison","text":"<p>All orders (30 rows) - includes:</p> order_id line_total status ORD001 1299.99 completed ORD002 59.98 completed ... ... ... ORD015 599.99 pending (excluded) ORD023 249.99 cancelled (excluded) ... ... ... <p>Completed only (27 rows):</p> order_id line_total status ORD001 1299.99 completed ORD002 59.98 completed ... ... ..."},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-results","title":"Query Results","text":"Metric Value Rows Included revenue 9,803.54 30 (all) completed_revenue 8,953.56 27 (completed only)"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-3-define-multiple-metrics","title":"Step 3: Define Multiple Metrics","text":"<p>Let's define a complete set of metrics:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_2","title":"Python Code","text":"<pre><code>from odibi.semantics import MetricDefinition, SemanticLayerConfig\n\n# Revenue metrics\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from completed orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Count metrics\norder_count = MetricDefinition(\n    name=\"order_count\",\n    description=\"Number of orders\",\n    expr=\"COUNT(*)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\nunique_customers = MetricDefinition(\n    name=\"unique_customers\",\n    description=\"Number of unique customers\",\n    expr=\"COUNT(DISTINCT customer_sk)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Average metrics\navg_order_value = MetricDefinition(\n    name=\"avg_order_value\",\n    description=\"Average order value\",\n    expr=\"AVG(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Volume metrics\ntotal_quantity = MetricDefinition(\n    name=\"total_quantity\",\n    description=\"Total units sold\",\n    expr=\"SUM(quantity)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Combine into config\nconfig = SemanticLayerConfig(\n    metrics=[\n        revenue,\n        order_count,\n        unique_customers,\n        avg_order_value,\n        total_quantity\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_2","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: order_count\n    description: \"Number of orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-results_1","title":"Query Results","text":"<p>Single metric:</p> <pre><code>result = query.execute(\"revenue\", context)\n</code></pre> revenue 8,953.56 <p>Multiple metrics:</p> <pre><code>result = query.execute(\"revenue, order_count, avg_order_value\", context)\n</code></pre> revenue order_count avg_order_value 8,953.56 27 331.61"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-4-define-a-derived-metric","title":"Step 4: Define a Derived Metric","text":"<p>A derived metric is calculated from other metrics. It doesn't aggregate directly from the source.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#scenario-profit-margin","title":"Scenario: Profit Margin","text":"<p>We want to calculate profit margin, which requires cost data. Let's assume we've added a cost column:</p> <p>fact_orders with cost (sample):</p> order_id line_total cost_total ORD001 1299.99 850.00 ORD002 59.98 24.00 ORD003 249.99 120.00"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_3","title":"Python Code","text":"<pre><code># Base metrics\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\ntotal_cost = MetricDefinition(\n    name=\"total_cost\",\n    expr=\"SUM(cost_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\nprofit = MetricDefinition(\n    name=\"profit\",\n    expr=\"SUM(line_total) - SUM(cost_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Derived metric (calculated from other metrics)\nprofit_margin = MetricDefinition(\n    name=\"profit_margin\",\n    description=\"Profit as percentage of revenue\",\n    expr=\"(revenue - total_cost) / revenue\",\n    type=\"derived\"  # Indicates this references other metrics\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_3","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: total_cost\n    expr: \"SUM(cost_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit\n    expr: \"SUM(line_total) - SUM(cost_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit_margin\n    description: \"Profit as percentage of revenue\"\n    expr: \"(revenue - total_cost) / revenue\"\n    type: derived\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#complete-semanticlayerconfig","title":"Complete SemanticLayerConfig","text":"<p>Here's the complete configuration with all our metrics:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_4","title":"Python Code","text":"<pre><code>from odibi.semantics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig\n)\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        # Revenue metrics\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"pending_revenue\",\n            description=\"Revenue from pending orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'pending'\"]\n        ),\n\n        # Count metrics\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"unique_customers\",\n            description=\"Number of unique customers\",\n            expr=\"COUNT(DISTINCT customer_sk)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n\n        # Average metrics\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n\n        # Volume metrics\n        MetricDefinition(\n            name=\"total_quantity\",\n            description=\"Total units sold\",\n            expr=\"SUM(quantity)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[]  # We'll add these in the next tutorial\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_4","title":"YAML Alternative","text":"<pre><code># File: semantic_config.yaml\nmetrics:\n  # Revenue metrics\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: pending_revenue\n    description: \"Revenue from pending orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'pending'\"\n\n  # Count metrics\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Average metrics\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Volume metrics\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\ndimensions: []  # Added in next tutorial\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>SUM(column)</code> Sum of values <code>SUM(line_total)</code> <code>COUNT(*)</code> Row count <code>COUNT(*)</code> <code>COUNT(column)</code> Non-null count <code>COUNT(customer_sk)</code> <code>COUNT(DISTINCT column)</code> Unique count <code>COUNT(DISTINCT customer_sk)</code> <code>AVG(column)</code> Average <code>AVG(line_total)</code> <code>MIN(column)</code> Minimum <code>MIN(line_total)</code> <code>MAX(column)</code> Maximum <code>MAX(line_total)</code>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#complex-expressions","title":"Complex Expressions","text":"<pre><code># Percentage of total (within group)\n- name: revenue_share\n  expr: \"SUM(line_total) / SUM(SUM(line_total)) OVER ()\"\n\n# Conditional sum\n- name: high_value_revenue\n  expr: \"SUM(CASE WHEN line_total &gt; 500 THEN line_total ELSE 0 END)\"\n\n# Ratio\n- name: items_per_order\n  expr: \"SUM(quantity) / COUNT(*)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#metric-naming-best-practices","title":"Metric Naming Best Practices","text":"Do Don't <code>revenue</code> <code>rev</code> <code>order_count</code> <code>cnt</code> <code>completed_revenue</code> <code>rev_comp</code> <code>avg_order_value</code> <code>aov</code> (unless standard) <code>unique_customers</code> <code>cust_distinct</code> <p>Guidelines: - Use <code>snake_case</code> - Be descriptive: <code>completed_order_revenue</code> over <code>rev1</code> - Prefix related metrics: <code>revenue</code>, <code>revenue_completed</code>, <code>revenue_pending</code> - Include the filter in the name: <code>last_30_days_revenue</code></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple metrics aggregate directly from source: <code>SUM(line_total)</code></li> <li>Filters constrain which rows are included: <code>status = 'completed'</code></li> <li>Multiple metrics can be defined and queried together</li> <li>Derived metrics calculate from other metrics: <code>revenue - cost</code></li> <li>The expr field uses SQL aggregation syntax</li> <li>The source field specifies which table to query</li> <li>Naming conventions make metrics discoverable</li> </ul>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to define dimensions for grouping and filtering.</p> <p>Next: Defining Dimensions</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#navigation","title":"Navigation","text":"Previous Up Next Semantic Layer Intro Tutorials Defining Dimensions"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Defining Metrics Reference</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/","title":"Defining Dimensions Tutorial","text":"<p>In this tutorial, you'll learn how to define semantic layer dimensions for grouping and filtering metrics. Dimensions are the \"BY\" part of queries like <code>\"revenue BY region\"</code>.</p> <p>What You'll Learn: - Simple dimensions (single column) - Dimensions with different column names - Hierarchical dimensions (drill-down) - Complete config with metrics AND dimensions</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#star-schema-data","title":"Star Schema Data","text":"<p>We'll use the star schema from Tutorial 06:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_customer-sample","title":"dim_customer (sample)","text":"customer_sk customer_id name region city state 1 C001 Alice Johnson North Chicago IL 2 C002 Bob Smith South Houston TX 3 C003 Carol White North Detroit MI 4 C004 David Brown East New York NY 5 C005 Emma Davis West Seattle WA 6 C006 Frank Miller South Miami FL 7 C007 Grace Lee East Boston MA 8 C008 Henry Wilson West Portland OR 9 C009 Ivy Chen North Minneapolis MN 10 C010 Jack Taylor South Dallas TX 11 C011 Karen Martinez East Philadelphia PA 12 C012 Leo Anderson West Denver CO"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_product-sample","title":"dim_product (sample)","text":"product_sk product_id name category subcategory 1 P001 Laptop Pro 15 Electronics Computers 2 P002 Wireless Mouse Electronics Accessories 3 P003 Office Chair Furniture Seating 4 P004 USB-C Hub Electronics Accessories 5 P005 Standing Desk Furniture Desks 6 P006 Mechanical Keyboard Electronics Accessories 7 P007 Monitor 27\" Electronics Displays 8 P008 Desk Lamp Furniture Lighting 9 P009 Webcam HD Electronics Accessories 10 P010 Filing Cabinet Furniture Storage"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_date-sample","title":"dim_date (sample)","text":"date_sk full_date day_of_week month month_name quarter_name year 20240115 2024-01-15 Monday 1 January Q1 2024 20240116 2024-01-16 Tuesday 1 January Q1 2024 20240117 2024-01-17 Wednesday 1 January Q1 2024 20240118 2024-01-18 Thursday 1 January Q1 2024 20240119 2024-01-19 Friday 1 January Q1 2024 20240120 2024-01-20 Saturday 1 January Q1 2024 20240121 2024-01-21 Sunday 1 January Q1 2024 20240122 2024-01-22 Monday 1 January Q1 2024 20240123 2024-01-23 Tuesday 1 January Q1 2024 20240124 2024-01-24 Wednesday 1 January Q1 2024"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#unified-project-api-recommended","title":"Unified Project API (Recommended)","text":"<p>When using the unified <code>Project</code> API, dimensions can reference pipeline nodes directly using the <code>$pipeline.node</code> notation:</p> <pre><code># odibi.yaml\nproject: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: dim_customer\n        write:\n          connection: gold\n          table: dim_customer\n      - name: dim_product\n        write:\n          connection: gold\n          table: dim_product\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # References node's write target\n      column: region\n\n    - name: category\n      source: $build_warehouse.dim_product    # No path duplication!\n      column: category\n</code></pre> <p>The <code>source: $build_warehouse.dim_customer</code> notation: 1. Looks up the <code>dim_customer</code> node in the <code>build_warehouse</code> pipeline 2. Reads its <code>write.connection</code> and <code>write.table</code> config 3. Resolves the full path automatically</p> <p>Alternative: You can also use <code>source: gold.dim_customer</code> (connection.table) for tables not managed by pipelines.</p> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region, category\")  # Tables auto-loaded\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-1-define-a-simple-dimension","title":"Step 1: Define a Simple Dimension","text":"<p>The simplest dimension maps a column directly:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import DimensionDefinition\n\n# Simple dimension: region from dim_customer\nregion = DimensionDefinition(\n    name=\"region\",\n    source=\"dim_customer\",\n    column=\"region\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: region\n    source: dim_customer\n    column: region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"region\"</code> How you reference it in queries <code>source</code> <code>\"dim_customer\"</code> Table containing the dimension <code>column</code> <code>\"region\"</code> Column to GROUP BY"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#query-example","title":"Query Example","text":"<pre><code>result = query.execute(\"revenue BY region\", context)\n</code></pre> <p>Result (4 rows):</p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-2-dimension-with-different-column-name","title":"Step 2: Dimension with Different Column Name","text":"<p>Sometimes you want the dimension name to differ from the column name:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_1","title":"Python Code","text":"<pre><code># Dimension name differs from column name\ncustomer_city = DimensionDefinition(\n    name=\"customer_city\",      # Query uses \"customer_city\"\n    source=\"dim_customer\",\n    column=\"city\"              # Actual column is \"city\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_1","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: customer_city\n    source: dim_customer\n    column: city\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#query-example_1","title":"Query Example","text":"<pre><code>result = query.execute(\"revenue BY customer_city\", context)\n</code></pre> <p>Result (12 rows):</p> customer_city revenue Chicago 1,559.95 Houston 1,049.97 Detroit 541.93 New York 1,079.97 Seattle 477.97 Miami 959.95 Boston 573.94 Portland 1,379.98 Minneapolis 469.95 Dallas 1,549.98 Philadelphia 249.92 Denver 1,309.96"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-3-dimension-with-hierarchy","title":"Step 3: Dimension with Hierarchy","text":"<p>A hierarchy defines drill-down levels. Users can start at a high level (year) and drill into details (month, week, day).</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#the-drill-down-concept","title":"The Drill-Down Concept","text":"<pre><code>Year (2024)\n  \u2514\u2500\u2500 Quarter (Q1)\n        \u2514\u2500\u2500 Month (January)\n              \u2514\u2500\u2500 Week (Week 3)\n                    \u2514\u2500\u2500 Day (Jan 15)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_2","title":"Python Code","text":"<pre><code># Date dimension with hierarchy\norder_date = DimensionDefinition(\n    name=\"order_date\",\n    source=\"dim_date\",\n    column=\"full_date\",\n    hierarchy=[\"year\", \"quarter_name\", \"month_name\", \"full_date\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_2","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: order_date\n    source: dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n      - full_date\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#using-hierarchy-levels-in-queries","title":"Using Hierarchy Levels in Queries","text":"<p>Top level - Year:</p> <pre><code>result = query.execute(\"revenue BY year\", context)\n</code></pre> year revenue 2024 8,953.56 <p>Drill down - Quarter:</p> <pre><code>result = query.execute(\"revenue BY quarter_name\", context)\n</code></pre> quarter_name revenue Q1 8,953.56 <p>Drill down - Month:</p> <pre><code>result = query.execute(\"revenue BY month_name\", context)\n</code></pre> month_name revenue January 8,953.56 <p>Drill down - Day:</p> <pre><code>result = query.execute(\"revenue BY full_date\", context)\n</code></pre> full_date revenue 2024-01-15 1,439.96 2024-01-16 589.95 2024-01-17 749.98 ... ..."},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-4-define-all-dimensions-for-our-star-schema","title":"Step 4: Define All Dimensions for Our Star Schema","text":"<p>Let's define a complete set of dimensions:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_3","title":"Python Code","text":"<pre><code>from odibi.semantics import DimensionDefinition\n\ndimensions = [\n    # Geographic dimensions (from dim_customer)\n    DimensionDefinition(\n        name=\"region\",\n        source=\"dim_customer\",\n        column=\"region\",\n        description=\"Customer geographic region\"\n    ),\n    DimensionDefinition(\n        name=\"city\",\n        source=\"dim_customer\",\n        column=\"city\"\n    ),\n    DimensionDefinition(\n        name=\"state\",\n        source=\"dim_customer\",\n        column=\"state\"\n    ),\n\n    # Product dimensions (from dim_product)\n    DimensionDefinition(\n        name=\"category\",\n        source=\"dim_product\",\n        column=\"category\",\n        description=\"Product category\"\n    ),\n    DimensionDefinition(\n        name=\"subcategory\",\n        source=\"dim_product\",\n        column=\"subcategory\"\n    ),\n    DimensionDefinition(\n        name=\"product_name\",\n        source=\"dim_product\",\n        column=\"name\",\n        hierarchy=[\"category\", \"subcategory\", \"name\"]\n    ),\n\n    # Time dimensions (from dim_date)\n    DimensionDefinition(\n        name=\"year\",\n        source=\"dim_date\",\n        column=\"year\"\n    ),\n    DimensionDefinition(\n        name=\"quarter\",\n        source=\"dim_date\",\n        column=\"quarter_name\"\n    ),\n    DimensionDefinition(\n        name=\"month\",\n        source=\"dim_date\",\n        column=\"month_name\",\n        hierarchy=[\"year\", \"quarter_name\", \"month_name\"]\n    ),\n    DimensionDefinition(\n        name=\"day_of_week\",\n        source=\"dim_date\",\n        column=\"day_of_week\"\n    ),\n\n    # Order dimensions (from fact_orders)\n    DimensionDefinition(\n        name=\"status\",\n        source=\"fact_orders\",\n        column=\"status\"\n    )\n]\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_3","title":"YAML Alternative","text":"<pre><code>dimensions:\n  # Geographic dimensions\n  - name: region\n    source: dim_customer\n    column: region\n    description: \"Customer geographic region\"\n\n  - name: city\n    source: dim_customer\n    column: city\n\n  - name: state\n    source: dim_customer\n    column: state\n\n  # Product dimensions\n  - name: category\n    source: dim_product\n    column: category\n    description: \"Product category\"\n\n  - name: subcategory\n    source: dim_product\n    column: subcategory\n\n  - name: product_name\n    source: dim_product\n    column: name\n    hierarchy:\n      - category\n      - subcategory\n      - name\n\n  # Time dimensions\n  - name: year\n    source: dim_date\n    column: year\n\n  - name: quarter\n    source: dim_date\n    column: quarter_name\n\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n\n  - name: day_of_week\n    source: dim_date\n    column: day_of_week\n\n  # Order dimensions\n  - name: status\n    source: fact_orders\n    column: status\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#complete-config-with-metrics-and-dimensions","title":"Complete Config with Metrics AND Dimensions","text":"<p>Here's the full SemanticLayerConfig:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_4","title":"Python Code","text":"<pre><code>from odibi.semantics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig\n)\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"unique_customers\",\n            description=\"Number of unique customers\",\n            expr=\"COUNT(DISTINCT customer_sk)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"total_quantity\",\n            description=\"Total units sold\",\n            expr=\"SUM(quantity)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[\n        # Geographic\n        DimensionDefinition(name=\"region\", source=\"dim_customer\", column=\"region\"),\n        DimensionDefinition(name=\"city\", source=\"dim_customer\", column=\"city\"),\n        DimensionDefinition(name=\"state\", source=\"dim_customer\", column=\"state\"),\n\n        # Product\n        DimensionDefinition(name=\"category\", source=\"dim_product\", column=\"category\"),\n        DimensionDefinition(name=\"subcategory\", source=\"dim_product\", column=\"subcategory\"),\n        DimensionDefinition(\n            name=\"product_name\", \n            source=\"dim_product\", \n            column=\"name\",\n            hierarchy=[\"category\", \"subcategory\", \"name\"]\n        ),\n\n        # Time\n        DimensionDefinition(name=\"year\", source=\"dim_date\", column=\"year\"),\n        DimensionDefinition(name=\"quarter\", source=\"dim_date\", column=\"quarter_name\"),\n        DimensionDefinition(\n            name=\"month\", \n            source=\"dim_date\", \n            column=\"month_name\",\n            hierarchy=[\"year\", \"quarter_name\", \"month_name\"]\n        ),\n        DimensionDefinition(name=\"day_of_week\", source=\"dim_date\", column=\"day_of_week\"),\n\n        # Order\n        DimensionDefinition(name=\"status\", source=\"fact_orders\", column=\"status\")\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative-semantic_configyaml","title":"YAML Alternative (semantic_config.yaml)","text":"<pre><code># File: semantic_config.yaml\nmetrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\ndimensions:\n  # Geographic\n  - name: region\n    source: dim_customer\n    column: region\n  - name: city\n    source: dim_customer\n    column: city\n  - name: state\n    source: dim_customer\n    column: state\n\n  # Product\n  - name: category\n    source: dim_product\n    column: category\n  - name: subcategory\n    source: dim_product\n    column: subcategory\n  - name: product_name\n    source: dim_product\n    column: name\n    hierarchy: [category, subcategory, name]\n\n  # Time\n  - name: year\n    source: dim_date\n    column: year\n  - name: quarter\n    source: dim_date\n    column: quarter_name\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy: [year, quarter_name, month_name]\n  - name: day_of_week\n    source: dim_date\n    column: day_of_week\n\n  # Order\n  - name: status\n    source: fact_orders\n    column: status\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#example-queries-with-dimensions","title":"Example Queries with Dimensions","text":"<p>Now you can run rich queries:</p> <p>Revenue by region:</p> <pre><code>result = query.execute(\"revenue BY region\", context)\n</code></pre> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87 <p>Revenue by category and region:</p> <pre><code>result = query.execute(\"revenue BY category, region\", context)\n</code></pre> category region revenue Electronics North 1,549.94 Electronics South 1,449.95 Electronics East 1,323.91 Electronics West 1,079.93 Furniture North 999.94 Furniture South 899.98 Furniture East 599.97 Furniture West 1,049.94 <p>Multiple metrics by day of week:</p> <pre><code>result = query.execute(\"revenue, order_count, avg_order_value BY day_of_week\", context)\n</code></pre> day_of_week revenue order_count avg_order_value Monday 2,189.94 5 437.99 Tuesday 1,177.93 5 235.59 Wednesday 1,099.95 4 275.00 Thursday 2,373.90 4 593.48 Friday 619.96 4 155.00 Saturday 1,549.94 3 516.65 Sunday 941.94 2 470.97"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple dimensions map a column for grouping: <code>region</code>, <code>category</code></li> <li>Column renaming lets dimension names differ from columns: <code>customer_city</code> \u2192 <code>city</code></li> <li>Hierarchies define drill-down paths: <code>year &gt; quarter &gt; month</code></li> <li>Dimensions can come from dimension tables or fact tables</li> <li>Complete config includes both metrics and dimensions</li> <li>Queries use dimensions with <code>BY</code>: <code>\"revenue BY region, category\"</code></li> </ul>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to execute queries against our semantic layer.</p> <p>Next: Querying Metrics</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#navigation","title":"Navigation","text":"Previous Up Next Defining Metrics Tutorials Querying Metrics"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#reference","title":"Reference","text":"<p>For complete documentation, see: Defining Metrics Reference</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/","title":"Querying Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to execute queries against the semantic layer using both the unified <code>Project</code> API and the <code>SemanticQuery</code> interface.</p> <p>What You'll Learn: - Unified Project API (recommended) - simplest approach - Simple queries (total, no grouping) - Queries with one dimension - Queries with multiple dimensions - Filtered queries - Multiple metrics together</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to query metrics is through the <code>Project</code> class:</p> <pre><code>from odibi import Project\n\n# Load project - semantic layer inherits connections\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics - tables auto-loaded from connections\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>The Project API: - Reads connections from your <code>odibi.yaml</code> - Resolves <code>source: $pipeline.node</code> or <code>connection.path</code> to full paths - Auto-loads Delta tables when queried - No manual <code>context.register()</code> calls needed</p> <p>Example queries:</p> <pre><code># Total revenue\nresult = project.query(\"revenue\")\n\n# Revenue by region\nresult = project.query(\"revenue BY region\")\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\n\n# With filter\nresult = project.query(\"revenue BY category WHERE region = 'North'\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#option-b-manual-semanticquery-interface","title":"Option B: Manual SemanticQuery Interface","text":"<p>For more control, use the <code>SemanticQuery</code> class directly.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#source-data","title":"Source Data","text":"<p>We'll use the star schema and semantic config from previous tutorials:</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#fact_orders-20-sample-rows","title":"fact_orders (20 sample rows)","text":"order_id customer_sk product_sk date_sk quantity line_total status ORD001 1 1 20240115 1 1299.99 completed ORD002 1 2 20240115 2 59.98 completed ORD003 2 3 20240116 1 249.99 completed ORD004 3 4 20240116 3 149.97 completed ORD005 4 5 20240117 1 599.99 completed ORD006 5 6 20240117 1 149.99 completed ORD007 6 7 20240118 2 799.98 completed ORD008 7 8 20240118 4 183.96 completed ORD009 8 9 20240119 1 79.99 completed ORD010 9 10 20240119 1 189.99 completed ORD011 10 1 20240120 1 1299.99 completed ORD012 11 2 20240120 5 149.95 completed ORD013 12 3 20240121 2 499.98 completed ORD014 1 4 20240121 1 49.99 completed ORD015 2 5 20240122 1 599.99 pending ORD016 3 6 20240122 2 299.98 completed ORD017 4 7 20240123 1 399.99 completed ORD018 5 8 20240123 3 137.97 completed ORD019 6 9 20240124 2 159.98 completed ORD020 7 10 20240124 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#dim_customer-12-rows","title":"dim_customer (12 rows)","text":"customer_sk name region city 1 Alice Johnson North Chicago 2 Bob Smith South Houston 3 Carol White North Detroit 4 David Brown East New York 5 Emma Davis West Seattle 6 Frank Miller South Miami 7 Grace Lee East Boston 8 Henry Wilson West Portland 9 Ivy Chen North Minneapolis 10 Jack Taylor South Dallas 11 Karen Martinez East Philadelphia 12 Leo Anderson West Denver"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#dim_product-10-rows","title":"dim_product (10 rows)","text":"product_sk name category 1 Laptop Pro 15 Electronics 2 Wireless Mouse Electronics 3 Office Chair Furniture 4 USB-C Hub Electronics 5 Standing Desk Furniture 6 Mechanical Keyboard Electronics 7 Monitor 27\" Electronics 8 Desk Lamp Furniture 9 Webcam HD Electronics 10 Filing Cabinet Furniture"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-1-simple-query-total-no-grouping","title":"Step 1: Simple Query - Total (No Grouping)","text":"<p>The simplest query returns a single aggregated value.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query","title":"Query","text":"<pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load config\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Create query interface\nquery = SemanticQuery(config)\n\n# Setup context with data\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\ncontext.register(\"dim_product\", dim_product_df)\ncontext.register(\"dim_date\", dim_date_df)\n\n# Execute query\nresult = query.execute(\"revenue\", context)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string","title":"Query String","text":"<pre><code>\"revenue\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql","title":"Generated SQL","text":"<pre><code>SELECT SUM(line_total) AS revenue\nFROM fact_orders\nWHERE status = 'completed'\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-1-row","title":"Result (1 row)","text":"revenue 8,953.56"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#accessing-the-result","title":"Accessing the Result","text":"<pre><code>print(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\n# Output: Total Revenue: $8,953.56\n\nprint(f\"Row count: {result.row_count}\")\n# Output: Row count: 1\n\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n# Output: Execution time: 12.34ms\n\nprint(f\"Generated SQL: {result.sql_generated}\")\n# Output: SELECT SUM(line_total) AS revenue FROM fact_orders WHERE status = 'completed'\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-2-query-with-one-dimension","title":"Step 2: Query with One Dimension","text":"<p>Add a dimension to group the results.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_1","title":"Query String","text":"<pre><code>\"revenue BY region\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_1","title":"Generated SQL","text":"<pre><code>SELECT \n    c.region,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE f.status = 'completed'\nGROUP BY c.region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue BY region\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-4-rows","title":"Result (4 rows)","text":"region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-3-query-with-multiple-dimensions","title":"Step 3: Query with Multiple Dimensions","text":"<p>Add more dimensions for a cross-tabulation.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_2","title":"Query String","text":"<pre><code>\"revenue, order_count BY region, category\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_2","title":"Generated SQL","text":"<pre><code>SELECT \n    c.region,\n    p.category,\n    SUM(f.line_total) AS revenue,\n    COUNT(*) AS order_count\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nWHERE f.status = 'completed'\nGROUP BY c.region, p.category\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_1","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue, order_count BY region, category\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-8-rows-region-category","title":"Result (8 rows - region \u00d7 category)","text":"region category revenue order_count North Electronics 1,549.94 4 North Furniture 999.94 3 South Electronics 1,449.95 4 South Furniture 899.98 3 East Electronics 1,323.91 4 East Furniture 599.97 3 West Electronics 1,079.93 3 West Furniture 1,049.94 4"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-4-query-with-filter","title":"Step 4: Query with Filter","text":"<p>Add a WHERE clause to filter results.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_3","title":"Query String","text":"<pre><code>\"revenue BY category WHERE region = 'North'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_3","title":"Generated SQL","text":"<pre><code>SELECT \n    p.category,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nWHERE f.status = 'completed'\n  AND c.region = 'North'\nGROUP BY p.category\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_2","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue BY category WHERE region = 'North'\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-2-rows","title":"Result (2 rows)","text":"category revenue Electronics 1,549.94 Furniture 999.94"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#multiple-filters","title":"Multiple Filters","text":"<p>You can combine multiple filter conditions:</p> <pre><code># Multiple conditions with AND\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North' AND year = 2024\",\n    context\n)\n\n# IN clause\nresult = query.execute(\n    \"revenue BY region WHERE region IN ('North', 'South')\",\n    context\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-5-multiple-metrics","title":"Step 5: Multiple Metrics","text":"<p>Query multiple metrics in a single call.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_4","title":"Query String","text":"<pre><code>\"revenue, order_count, avg_order_value BY region\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_3","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-4-rows_1","title":"Result (4 rows)","text":"region revenue order_count avg_order_value North 2,549.88 7 364.27 South 2,349.93 7 335.70 East 1,923.88 7 274.84 West 2,129.87 7 304.27"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#complete-python-script","title":"Complete Python Script","text":"<p>Here's a complete, runnable example:</p> <pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\n\n# ===========================================\n# 1. Load the semantic config\n# ===========================================\nconfig_dict = {\n    \"metrics\": [\n        {\n            \"name\": \"revenue\",\n            \"description\": \"Total revenue from completed orders\",\n            \"expr\": \"SUM(line_total)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"order_count\",\n            \"expr\": \"COUNT(*)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"avg_order_value\",\n            \"expr\": \"AVG(line_total)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        }\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"dim_customer\", \"column\": \"region\"},\n        {\"name\": \"category\", \"source\": \"dim_product\", \"column\": \"category\"},\n        {\"name\": \"month\", \"source\": \"dim_date\", \"column\": \"month_name\"}\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\nquery = SemanticQuery(config)\n\n# ===========================================\n# 2. Load data and create context\n# ===========================================\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# ===========================================\n# 3. Execute queries\n# ===========================================\n\n# Query 1: Total revenue\nprint(\"=\" * 50)\nprint(\"Query 1: Total Revenue\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\nprint()\n\n# Query 2: Revenue by region\nprint(\"=\" * 50)\nprint(\"Query 2: Revenue by Region\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 3: Multiple metrics by region\nprint(\"=\" * 50)\nprint(\"Query 3: Multiple Metrics by Region\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 4: Revenue by region and category\nprint(\"=\" * 50)\nprint(\"Query 4: Revenue by Region and Category\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue BY region, category\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 5: Filtered query\nprint(\"=\" * 50)\nprint(\"Query 5: North Region Only\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue, order_count BY category WHERE region = 'North'\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# ===========================================\n# 4. Show execution details\n# ===========================================\nprint(\"=\" * 50)\nprint(\"Execution Details (last query)\")\nprint(\"=\" * 50)\nprint(f\"Metrics: {result.metrics}\")\nprint(f\"Dimensions: {result.dimensions}\")\nprint(f\"Row count: {result.row_count}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#output","title":"Output","text":"<pre><code>==================================================\nQuery 1: Total Revenue\n==================================================\nTotal Revenue: $8,953.56\n\n==================================================\nQuery 2: Revenue by Region\n==================================================\n region   revenue\n  North  2549.88\n  South  2349.93\n   East  1923.88\n   West  2129.87\n\n==================================================\nQuery 3: Multiple Metrics by Region\n==================================================\n region   revenue  order_count  avg_order_value\n  North  2549.88            7           364.27\n  South  2349.93            7           335.70\n   East  1923.88            7           274.84\n   West  2129.87            7           304.27\n\n==================================================\nQuery 4: Revenue by Region and Category\n==================================================\n region     category   revenue\n  North  Electronics  1549.94\n  North    Furniture   999.94\n  South  Electronics  1449.95\n  South    Furniture   899.98\n   East  Electronics  1323.91\n   East    Furniture   599.97\n   West  Electronics  1079.93\n   West    Furniture  1049.94\n\n==================================================\nQuery 5: North Region Only\n==================================================\n    category   revenue  order_count\n Electronics  1549.94            4\n   Furniture   999.94            3\n\n==================================================\nExecution Details (last query)\n==================================================\nMetrics: ['revenue', 'order_count']\nDimensions: ['category']\nRow count: 2\nExecution time: 8.45ms\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-syntax-reference","title":"Query Syntax Reference","text":"Pattern Example Description Single metric <code>\"revenue\"</code> Total, no grouping Metric + dimension <code>\"revenue BY region\"</code> Grouped by one dimension Multiple metrics <code>\"revenue, order_count\"</code> Multiple metrics together Multiple dimensions <code>\"revenue BY region, category\"</code> Cross-tabulation With filter <code>\"revenue BY region WHERE year = 2024\"</code> Filtered results Complex filter <code>\"revenue BY region WHERE region IN ('North', 'South')\"</code> IN clause"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#error-handling","title":"Error Handling","text":"<pre><code># Invalid metric\ntry:\n    result = query.execute(\"invalid_metric BY region\", context)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count', 'avg_order_value']\n\n# Invalid dimension\ntry:\n    result = query.execute(\"revenue BY invalid_dimension\", context)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Unknown dimension 'invalid_dimension'. Available: ['region', 'category', 'month']\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple queries return totals: <code>\"revenue\"</code></li> <li>One dimension groups results: <code>\"revenue BY region\"</code></li> <li>Multiple dimensions create cross-tabs: <code>\"revenue BY region, category\"</code></li> <li>Filters constrain results: <code>\"revenue BY region WHERE year = 2024\"</code></li> <li>Multiple metrics can be queried together: <code>\"revenue, order_count BY region\"</code></li> <li>QueryResult contains the DataFrame, metrics, dimensions, and execution info</li> </ul>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to pre-compute metrics for dashboard performance.</p> <p>Next: Materializing Metrics</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#navigation","title":"Navigation","text":"Previous Up Next Defining Dimensions Tutorials Materializing Metrics"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Querying Reference</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/","title":"Materializing Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to pre-compute and persist metrics using the <code>Materializer</code> class. Materialization creates pre-aggregated tables for fast dashboard performance.</p> <p>What You'll Learn: - Why materialize metrics - Defining materializations - Executing materializations - Scheduling with cron - Incremental strategies (replace vs sum)</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#unified-project-api-note","title":"Unified Project API Note","text":"<p>When using the unified <code>Project</code> API, materializations are defined in the <code>semantic</code> section of your <code>odibi.yaml</code>. Sources can reference pipeline nodes directly:</p> <pre><code># odibi.yaml\nproject: retail_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n      - name: dim_date\n        write: { connection: gold, table: dim_date }\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters: [\"status = 'completed'\"]\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n    - name: month\n      source: $build_warehouse.dim_date\n      column: month_name\n\n  materializations:\n    - name: monthly_revenue_by_region\n      metrics: [revenue]\n      dimensions: [region, month]\n      output: gold/agg_monthly_revenue\n      schedule: \"0 2 1 * *\"\n</code></pre> <p>The <code>$pipeline.node</code> notation automatically reads from wherever the node writes. For full control over materialization execution, continue with the <code>Materializer</code> class below.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#why-materialize","title":"Why Materialize?","text":"<p>Ad-hoc queries are powerful but slow for dashboards:</p> Approach Query Time Use Case Ad-hoc query 5-30 seconds Exploratory analysis Materialized table 0.1-0.5 seconds Production dashboards <p>Materialization pre-computes metrics at a specific grain and saves them to a table. Dashboards query the pre-computed table instead of raw data.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#semantic-config-with-materializations","title":"Semantic Config with Materializations","text":"<p>Let's extend our semantic config to include materializations:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># semantic_config.yaml\nmetrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: unique_customers\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: avg_order_value\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\ndimensions:\n  - name: region\n    source: dim_customer\n    column: region\n\n  - name: category\n    source: dim_product\n    column: category\n\n  - name: month\n    source: dim_date\n    column: month_name\n\n  - name: date_sk\n    source: dim_date\n    column: date_sk\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue_region\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n\n  - name: daily_revenue\n    metrics: [revenue, order_count, unique_customers]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    schedule: \"0 3 * * *\"  # 3am daily\n\n  - name: category_summary\n    metrics: [revenue, order_count, avg_order_value]\n    dimensions: [category]\n    output: gold/agg_category_summary\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-1-define-a-materialization","title":"Step 1: Define a Materialization","text":"<p>A materialization specifies which metrics and dimensions to pre-compute:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import MaterializationConfig\n\n# Define a materialization\nmonthly_revenue = MaterializationConfig(\n    name=\"monthly_revenue_by_region\",\n    metrics=[\"revenue\", \"order_count\"],\n    dimensions=[\"region\", \"month\"],\n    output=\"gold/agg_monthly_revenue_region\",\n    schedule=\"0 2 1 * *\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"monthly_revenue_by_region\"</code> Unique identifier <code>metrics</code> <code>[\"revenue\", \"order_count\"]</code> Which metrics to compute <code>dimensions</code> <code>[\"region\", \"month\"]</code> Grain (GROUP BY) <code>output</code> <code>\"gold/agg_monthly_revenue_region\"</code> Output table path <code>schedule</code> <code>\"0 2 1 * *\"</code> Cron schedule (optional)"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#what-gets-generated","title":"What Gets Generated","text":"<p>The materialization creates a table with: - One row per unique combination of <code>region</code> \u00d7 <code>month</code> - Columns for each metric (<code>revenue</code>, <code>order_count</code>)</p> <p>Output Table (12 rows - 4 regions \u00d7 3 months):</p> region month revenue order_count North January 2,549.88 7 North February 3,120.50 9 North March 2,890.25 8 South January 2,349.93 7 South February 2,780.40 8 South March 3,050.75 9 East January 1,923.88 7 East February 2,450.60 7 East March 2,180.35 6 West January 2,129.87 7 West February 2,890.45 8 West March 2,650.90 7"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-2-execute-a-materialization","title":"Step 2: Execute a Materialization","text":"<p>Use the <code>Materializer</code> class to execute:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_1","title":"Python Code","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load config\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Create context\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\ncontext.register(\"dim_product\", dim_product_df)\ncontext.register(\"dim_date\", dim_date_df)\n\n# Create materializer\nmaterializer = Materializer(config)\n\n# Execute single materialization\nresult = materializer.execute(\"monthly_revenue_by_region\", context)\n\n# Check result\nprint(f\"Name: {result.name}\")\nprint(f\"Output: {result.output}\")\nprint(f\"Row count: {result.row_count}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#materializationresult","title":"MaterializationResult","text":"Field Type Description <code>name</code> str Materialization name <code>output</code> str Output table path <code>row_count</code> int Number of rows generated <code>elapsed_ms</code> float Execution time in ms <code>success</code> bool Whether it succeeded <code>error</code> str Error message (if failed) <code>df</code> DataFrame The computed data"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-3-write-the-output","title":"Step 3: Write the Output","text":"<p>Use a callback to write the materialized data:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_2","title":"Python Code","text":"<pre><code># Define write callback\ndef write_to_parquet(df, output_path):\n    \"\"\"Write DataFrame to Parquet.\"\"\"\n    full_path = f\"warehouse/{output_path}.parquet\"\n    df.to_parquet(full_path, index=False)\n    print(f\"Wrote {len(df)} rows to {full_path}\")\n\n# Execute with write callback\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_to_parquet\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#spark-example","title":"Spark Example","text":"<pre><code>def write_to_delta(df, output_path):\n    \"\"\"Write Spark DataFrame to Delta Lake.\"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/warehouse/{output_path}\")\n\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_to_delta\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-4-understanding-schedules","title":"Step 4: Understanding Schedules","text":"<p>The <code>schedule</code> field uses cron syntax:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0-59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0-23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 day of month (1-31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500 month (1-12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500 day of week (0-6, Sun=0)\n\u2502 \u2502 \u2502 \u2502 \u2502\n* * * * *\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#common-schedules","title":"Common Schedules","text":"Schedule Cron When Daily at 2am <code>0 2 * * *</code> Every day at 2:00 AM Monthly at 2am <code>0 2 1 * *</code> 1st of month at 2:00 AM Weekly Sunday <code>0 3 * * 0</code> Sunday at 3:00 AM Hourly <code>0 * * * *</code> Every hour on the hour"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#reading-schedules","title":"Reading Schedules","text":"<pre><code># Get schedule for a materialization\nmat_config = materializer.get_materialization(\"monthly_revenue_by_region\")\nprint(f\"Schedule: {mat_config.schedule}\")\n# Output: Schedule: 0 2 1 * *\n\n# List all materializations with schedules\nfor mat in materializer.list_materializations():\n    print(f\"{mat['name']}: {mat['schedule'] or 'No schedule'}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-5-execute-all-materializations","title":"Step 5: Execute All Materializations","text":"<p>Execute all defined materializations at once:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_3","title":"Python Code","text":"<pre><code># Execute all materializations\nresults = materializer.execute_all(context, write_callback=write_to_parquet)\n\n# Print summary\nprint(\"=\" * 60)\nprint(\"Materialization Summary\")\nprint(\"=\" * 60)\nfor result in results:\n    status = \"SUCCESS\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status}\")\n    if result.success:\n        print(f\"    Rows: {result.row_count}, Time: {result.elapsed_ms:.0f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#output","title":"Output","text":"<pre><code>============================================================\nMaterialization Summary\n============================================================\n  monthly_revenue_by_region: SUCCESS\n    Rows: 12, Time: 45ms\n  daily_revenue: SUCCESS\n    Rows: 14, Time: 32ms\n  category_summary: SUCCESS\n    Rows: 2, Time: 18ms\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-6-incremental-materialization","title":"Step 6: Incremental Materialization","text":"<p>For large datasets, use incremental updates instead of full rebuilds.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#replace-strategy","title":"Replace Strategy","text":"<p>New data replaces existing rows for matching grain keys:</p> <pre><code>materializations:\n  - name: daily_revenue\n    metrics: [revenue, order_count]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    incremental:\n      timestamp_column: load_timestamp\n      merge_strategy: replace\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#how-replace-works","title":"How Replace Works","text":"<p>Existing Table:</p> date_sk revenue order_count 20240115 1,439.96 3 20240116 589.95 3 20240117 749.98 2 <p>New Data Arrives (late order for Jan 15):</p> date_sk revenue order_count 20240115 1,539.96 4 <p>After Replace Merge:</p> date_sk revenue order_count Note 20240115 1,539.96 4 Replaced 20240116 589.95 3 Unchanged 20240117 749.98 2 Unchanged"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#sum-strategy-use-with-caution","title":"Sum Strategy (Use with Caution)","text":"<p>New measure values add to existing values:</p> <pre><code>materializations:\n  - name: daily_order_count\n    metrics: [order_count]  # Only COUNT metrics!\n    dimensions: [date_sk]\n    output: gold/agg_daily_count\n    incremental:\n      timestamp_column: created_at\n      merge_strategy: sum\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#how-sum-works","title":"How Sum Works","text":"<p>Existing Table:</p> date_sk order_count 20240115 3 20240116 3 <p>New Orders (2 new orders on Jan 15):</p> date_sk order_count 20240115 2 <p>After Sum Merge:</p> date_sk order_count Note 20240115 5 3 + 2 = 5 20240116 3 Unchanged"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#when-not-to-use-sum","title":"When NOT to Use Sum","text":"<p>Never use sum for: - <code>AVG()</code> - Would become average of averages - <code>COUNT(DISTINCT)</code> - Would overcount - <code>MIN()</code> / <code>MAX()</code> - Would be wrong - Data with corrections/updates</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#complete-python-example","title":"Complete Python Example","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\n\n# ===========================================\n# 1. Load config with materializations\n# ===========================================\nconfig_dict = {\n    \"metrics\": [\n        {\"name\": \"revenue\", \"expr\": \"SUM(line_total)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]},\n        {\"name\": \"order_count\", \"expr\": \"COUNT(*)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]},\n        {\"name\": \"avg_order_value\", \"expr\": \"AVG(line_total)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]}\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"dim_customer\", \"column\": \"region\"},\n        {\"name\": \"category\", \"source\": \"dim_product\", \"column\": \"category\"},\n        {\"name\": \"date_sk\", \"source\": \"dim_date\", \"column\": \"date_sk\"}\n    ],\n    \"materializations\": [\n        {\n            \"name\": \"daily_summary\",\n            \"metrics\": [\"revenue\", \"order_count\"],\n            \"dimensions\": [\"date_sk\"],\n            \"output\": \"gold/agg_daily_summary\",\n            \"schedule\": \"0 3 * * *\"\n        },\n        {\n            \"name\": \"region_summary\",\n            \"metrics\": [\"revenue\", \"order_count\", \"avg_order_value\"],\n            \"dimensions\": [\"region\"],\n            \"output\": \"gold/agg_region_summary\"\n        },\n        {\n            \"name\": \"category_by_region\",\n            \"metrics\": [\"revenue\", \"order_count\"],\n            \"dimensions\": [\"category\", \"region\"],\n            \"output\": \"gold/agg_category_region\"\n        }\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\n\n# ===========================================\n# 2. Setup context with data\n# ===========================================\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", pd.read_parquet(\"warehouse/fact_orders\"))\ncontext.register(\"dim_customer\", pd.read_parquet(\"warehouse/dim_customer\"))\ncontext.register(\"dim_product\", pd.read_parquet(\"warehouse/dim_product\"))\ncontext.register(\"dim_date\", pd.read_parquet(\"warehouse/dim_date\"))\n\n# ===========================================\n# 3. Define write callback\n# ===========================================\ndef write_output(df, output_path):\n    full_path = f\"warehouse/{output_path}.parquet\"\n    df.to_parquet(full_path, index=False)\n    print(f\"  \u2192 Wrote {len(df)} rows to {full_path}\")\n\n# ===========================================\n# 4. Execute all materializations\n# ===========================================\nmaterializer = Materializer(config)\n\nprint(\"=\" * 60)\nprint(\"Executing Materializations\")\nprint(\"=\" * 60)\n\nresults = materializer.execute_all(context, write_callback=write_output)\n\n# ===========================================\n# 5. Show results\n# ===========================================\nprint()\nprint(\"=\" * 60)\nprint(\"Results Summary\")\nprint(\"=\" * 60)\n\nfor result in results:\n    status = \"\u2713 SUCCESS\" if result.success else f\"\u2717 FAILED: {result.error}\"\n    print(f\"\\n{result.name}:\")\n    print(f\"  Status: {status}\")\n    print(f\"  Output: {result.output}\")\n    print(f\"  Rows: {result.row_count}\")\n    print(f\"  Time: {result.elapsed_ms:.0f}ms\")\n\n    # Show sample data\n    if result.success and result.df is not None:\n        print(f\"  Sample data:\")\n        print(result.df.head(5).to_string(index=False))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#output_1","title":"Output","text":"<pre><code>============================================================\nExecuting Materializations\n============================================================\n  \u2192 Wrote 14 rows to warehouse/gold/agg_daily_summary.parquet\n  \u2192 Wrote 4 rows to warehouse/gold/agg_region_summary.parquet\n  \u2192 Wrote 8 rows to warehouse/gold/agg_category_region.parquet\n\n============================================================\nResults Summary\n============================================================\n\ndaily_summary:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_daily_summary\n  Rows: 14\n  Time: 42ms\n  Sample data:\n   date_sk   revenue  order_count\n  20240115  1439.96            3\n  20240116   589.95            3\n  20240117   749.98            2\n  20240118   983.94            2\n  20240119   269.98            2\n\nregion_summary:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_region_summary\n  Rows: 4\n  Time: 28ms\n  Sample data:\n  region   revenue  order_count  avg_order_value\n   North  2549.88            7           364.27\n   South  2349.93            7           335.70\n    East  1923.88            7           274.84\n    West  2129.87            7           304.27\n\ncategory_by_region:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_category_region\n  Rows: 8\n  Time: 35ms\n  Sample data:\n     category  region   revenue  order_count\n  Electronics   North  1549.94            4\n  Electronics   South  1449.95            4\n  Electronics    East  1323.91            4\n  Electronics    West  1079.93            3\n    Furniture   North   999.94            3\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Materialization pre-computes metrics for fast dashboard queries</li> <li>MaterializationConfig specifies metrics, dimensions, output, and schedule</li> <li>Materializer.execute() runs a single materialization</li> <li>Materializer.execute_all() runs all configured materializations</li> <li>Schedules use cron syntax: <code>\"0 2 * * *\"</code> (daily at 2am)</li> <li>Replace strategy overwrites matching grain keys (recommended)</li> <li>Sum strategy adds to existing values (use with caution)</li> </ul>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's put everything together in a complete semantic layer example.</p> <p>Next: Semantic Full Example</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#navigation","title":"Navigation","text":"Previous Up Next Querying Metrics Tutorials Semantic Full Example"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Materializing Reference</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/","title":"Semantic Layer Full Example","text":"<p>This tutorial brings together everything you've learned about the semantic layer into a complete, end-to-end example.</p> <p>What You'll See: - Complete semantic config (5 metrics, 5 dimensions, 3 materializations) - Unified Project API (simplest approach) - Full Python script that loads data, queries metrics, and materializes results - All output tables with sample data</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is with the unified <code>Project</code> API:</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#odibiyaml-with-semantic-layer","title":"odibi.yaml with Semantic Layer","text":"<pre><code># odibi.yaml\nproject: retail_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n      - name: dim_product\n        write: { connection: gold, table: dim_product }\n      - name: dim_date\n        write: { connection: gold, table: dim_date }\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      description: \"Total revenue from completed orders\"\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n      source: $build_warehouse.fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n      source: $build_warehouse.fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n\n    - name: category\n      source: $build_warehouse.dim_product\n      column: category\n\n    - name: month\n      source: $build_warehouse.dim_date\n      column: month_name\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#query-with-two-lines","title":"Query with Two Lines","text":"<pre><code>from odibi import Project\n\n# Load project - tables auto-resolved from connections\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count, avg_order_value BY region, category\")\nprint(result.df)\n</code></pre> <p>That's it! No manual table loading or context registration required.</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#option-b-manual-approach","title":"Option B: Manual Approach","text":"<p>For more control, use the semantic layer components directly.</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#the-complete-semantic-configuration","title":"The Complete Semantic Configuration","text":""},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#semantic_configyaml","title":"semantic_config.yaml","text":"<pre><code># File: semantic_config.yaml\n# Complete semantic layer configuration for retail star schema\n\nmetrics:\n  # Revenue metrics\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: cost\n    description: \"Total cost of goods sold\"\n    expr: \"SUM(quantity * 0.6 * unit_price)\"  # Assume 60% cost ratio\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit\n    description: \"Gross profit (revenue - cost)\"\n    expr: \"SUM(line_total * 0.4)\"  # 40% margin\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Volume metrics\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\ndimensions:\n  # Geographic dimensions\n  - name: region\n    description: \"Customer geographic region\"\n    source: dim_customer\n    column: region\n\n  - name: category\n    description: \"Product category\"\n    source: dim_product\n    column: category\n\n  # Time dimensions\n  - name: month\n    description: \"Month name\"\n    source: dim_date\n    column: month_name\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n\n  - name: quarter\n    description: \"Quarter name\"\n    source: dim_date\n    column: quarter_name\n\n  - name: year\n    description: \"Calendar year\"\n    source: dim_date\n    column: year\n\nmaterializations:\n  # Daily aggregate for trend analysis\n  - name: daily_metrics\n    description: \"Daily revenue and order metrics\"\n    metrics:\n      - revenue\n      - order_count\n      - avg_order_value\n    dimensions:\n      - year\n      - month\n    output: gold/agg_daily_metrics\n    schedule: \"0 3 * * *\"  # 3am daily\n\n  # Monthly by region for regional dashboards\n  - name: monthly_by_region\n    description: \"Monthly metrics by region\"\n    metrics:\n      - revenue\n      - profit\n      - order_count\n    dimensions:\n      - region\n      - month\n    output: gold/agg_monthly_region\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n\n  # Category performance summary\n  - name: category_performance\n    description: \"Category performance metrics\"\n    metrics:\n      - revenue\n      - profit\n      - order_count\n      - avg_order_value\n    dimensions:\n      - category\n    output: gold/agg_category_performance\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#complete-python-script","title":"Complete Python Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete Semantic Layer Example\n\nThis script demonstrates the full workflow:\n1. Load star schema data\n2. Configure semantic layer\n3. Run queries\n4. Materialize metrics\n\"\"\"\n\nfrom odibi.semantics import (\n    SemanticQuery,\n    Materializer,\n    SemanticLayerConfig,\n    MetricDefinition,\n    DimensionDefinition,\n    MaterializationConfig,\n    parse_semantic_config\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\nfrom pathlib import Path\n\n# =============================================================================\n# 1. LOAD THE STAR SCHEMA DATA\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 1: Loading Star Schema Data\")\nprint(\"=\" * 70)\n\n# Load dimension tables\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\n\nprint(f\"dim_customer: {len(dim_customer)} rows\")\nprint(f\"dim_product:  {len(dim_product)} rows\")\nprint(f\"dim_date:     {len(dim_date)} rows\")\nprint(f\"fact_orders:  {len(fact_orders)} rows\")\nprint()\n\n# =============================================================================\n# 2. CREATE SEMANTIC LAYER CONFIG\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 2: Creating Semantic Layer Configuration\")\nprint(\"=\" * 70)\n\n# Option A: Load from YAML file\n# with open(\"semantic_config.yaml\") as f:\n#     config = parse_semantic_config(yaml.safe_load(f))\n\n# Option B: Build programmatically\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"cost\",\n            description=\"Total cost of goods sold\",\n            expr=\"SUM(quantity * 0.6 * unit_price)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"profit\",\n            description=\"Gross profit (revenue - cost)\",\n            expr=\"SUM(line_total * 0.4)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[\n        DimensionDefinition(name=\"region\", source=\"dim_customer\", column=\"region\"),\n        DimensionDefinition(name=\"category\", source=\"dim_product\", column=\"category\"),\n        DimensionDefinition(name=\"month\", source=\"dim_date\", column=\"month_name\",\n                           hierarchy=[\"year\", \"quarter_name\", \"month_name\"]),\n        DimensionDefinition(name=\"quarter\", source=\"dim_date\", column=\"quarter_name\"),\n        DimensionDefinition(name=\"year\", source=\"dim_date\", column=\"year\")\n    ],\n    materializations=[\n        MaterializationConfig(\n            name=\"daily_metrics\",\n            metrics=[\"revenue\", \"order_count\", \"avg_order_value\"],\n            dimensions=[\"year\", \"month\"],\n            output=\"gold/agg_daily_metrics\",\n            schedule=\"0 3 * * *\"\n        ),\n        MaterializationConfig(\n            name=\"monthly_by_region\",\n            metrics=[\"revenue\", \"profit\", \"order_count\"],\n            dimensions=[\"region\", \"month\"],\n            output=\"gold/agg_monthly_region\",\n            schedule=\"0 2 1 * *\"\n        ),\n        MaterializationConfig(\n            name=\"category_performance\",\n            metrics=[\"revenue\", \"profit\", \"order_count\", \"avg_order_value\"],\n            dimensions=[\"category\"],\n            output=\"gold/agg_category_performance\"\n        )\n    ]\n)\n\nprint(f\"Metrics defined:          {len(config.metrics)}\")\nprint(f\"Dimensions defined:       {len(config.dimensions)}\")\nprint(f\"Materializations defined: {len(config.materializations)}\")\n\n# List them\nprint(\"\\nMetrics:\")\nfor m in config.metrics:\n    print(f\"  - {m.name}: {m.description}\")\n\nprint(\"\\nDimensions:\")\nfor d in config.dimensions:\n    print(f\"  - {d.name}: from {d.source}.{d.column}\")\n\nprint(\"\\nMaterializations:\")\nfor mat in config.materializations:\n    print(f\"  - {mat.name}: {mat.metrics} BY {mat.dimensions}\")\nprint()\n\n# =============================================================================\n# 3. SETUP CONTEXT\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 3: Setting Up Engine Context\")\nprint(\"=\" * 70)\n\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\nprint(\"Registered tables: fact_orders, dim_customer, dim_product, dim_date\")\nprint()\n\n# =============================================================================\n# 4. RUN QUERIES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 4: Running Semantic Queries\")\nprint(\"=\" * 70)\n\nquery = SemanticQuery(config)\n\n# Query 1: Total revenue (no grouping)\nprint(\"\\n--- Query 1: Total Revenue ---\")\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n\n# Query 2: Revenue by region\nprint(\"\\n--- Query 2: Revenue by Region ---\")\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 3: Multiple metrics by region\nprint(\"\\n--- Query 3: Revenue, Profit, Order Count by Region ---\")\nresult = query.execute(\"revenue, profit, order_count BY region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 4: Revenue by category and region\nprint(\"\\n--- Query 4: Revenue by Category and Region ---\")\nresult = query.execute(\"revenue BY category, region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 5: Filtered query - North region only\nprint(\"\\n--- Query 5: North Region Performance ---\")\nresult = query.execute(\"revenue, profit, avg_order_value BY category WHERE region = 'North'\", context)\nprint(result.df.to_string(index=False))\n\nprint()\n\n# =============================================================================\n# 5. MATERIALIZE METRICS\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 5: Materializing Metrics\")\nprint(\"=\" * 70)\n\nmaterializer = Materializer(config)\n\n# Track outputs for display\noutput_tables = {}\n\ndef write_and_store(df, output_path):\n    \"\"\"Write to disk and store for display.\"\"\"\n    output_tables[output_path] = df.copy()\n    full_path = Path(f\"warehouse/{output_path}.parquet\")\n    full_path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_parquet(full_path, index=False)\n    print(f\"  \u2192 Wrote {len(df)} rows to {full_path}\")\n\n# Execute all materializations\nresults = materializer.execute_all(context, write_callback=write_and_store)\n\nprint(\"\\nMaterialization Results:\")\nfor result in results:\n    status = \"SUCCESS\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status} ({result.row_count} rows, {result.elapsed_ms:.0f}ms)\")\n\nprint()\n\n# =============================================================================\n# 6. SHOW OUTPUT TABLES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 6: Output Tables\")\nprint(\"=\" * 70)\n\nfor output_path, df in output_tables.items():\n    print(f\"\\n--- {output_path} ({len(df)} rows) ---\")\n    print(df.to_string(index=False))\n\nprint()\nprint(\"=\" * 70)\nprint(\"COMPLETE!\")\nprint(\"=\" * 70)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#sample-output","title":"Sample Output","text":""},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-1-data-loading","title":"Step 1: Data Loading","text":"<pre><code>======================================================================\nSTEP 1: Loading Star Schema Data\n======================================================================\ndim_customer: 13 rows\ndim_product:  11 rows\ndim_date:     32 rows\nfact_orders:  30 rows\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-4-query-results","title":"Step 4: Query Results","text":"<p>Query 1: Total Revenue</p> revenue 8,953.56 <p>Query 2: Revenue by Region (4 rows)</p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87 <p>Query 3: Multiple Metrics by Region (4 rows)</p> region revenue profit order_count North 2,549.88 1,019.95 7 South 2,349.93 939.97 7 East 1,923.88 769.55 7 West 2,129.87 851.95 7 <p>Query 4: Revenue by Category and Region (8 rows)</p> category region revenue Electronics North 1,549.94 Electronics South 1,449.95 Electronics East 1,323.91 Electronics West 1,079.93 Furniture North 999.94 Furniture South 899.98 Furniture East 599.97 Furniture West 1,049.94 <p>Query 5: North Region Only (2 rows)</p> category revenue profit avg_order_value Electronics 1,549.94 619.98 387.49 Furniture 999.94 399.98 333.31"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-5-materialization-results","title":"Step 5: Materialization Results","text":"<pre><code>======================================================================\nSTEP 5: Materializing Metrics\n======================================================================\n  \u2192 Wrote 1 rows to warehouse/gold/agg_daily_metrics.parquet\n  \u2192 Wrote 4 rows to warehouse/gold/agg_monthly_region.parquet\n  \u2192 Wrote 2 rows to warehouse/gold/agg_category_performance.parquet\n\nMaterialization Results:\n  daily_metrics: SUCCESS (1 rows, 32ms)\n  monthly_by_region: SUCCESS (4 rows, 45ms)\n  category_performance: SUCCESS (2 rows, 28ms)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-6-output-tables","title":"Step 6: Output Tables","text":"<p>gold/agg_daily_metrics (1 row)</p> year month revenue order_count avg_order_value 2024 January 8,953.56 27 331.61 <p>gold/agg_monthly_region (4 rows)</p> region month revenue profit order_count North January 2,549.88 1,019.95 7 South January 2,349.93 939.97 7 East January 1,923.88 769.55 7 West January 2,129.87 851.95 7 <p>gold/agg_category_performance (2 rows)</p> category revenue profit order_count avg_order_value Electronics 5,403.73 2,161.49 15 360.25 Furniture 3,549.83 1,419.93 12 295.82"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>flowchart TB\n    subgraph StarSchema[\"Star Schema (Built by Pipelines)\"]\n        DC[(dim_customer&lt;br/&gt;13 rows)]\n        DP[(dim_product&lt;br/&gt;11 rows)]\n        DD[(dim_date&lt;br/&gt;32 rows)]\n        FO[(fact_orders&lt;br/&gt;30 rows)]\n    end\n\n    subgraph SemanticLayer[\"Semantic Layer\"]\n        subgraph Metrics[\"5 Metrics\"]\n            M1[revenue]\n            M2[cost]\n            M3[profit]\n            M4[order_count]\n            M5[avg_order_value]\n        end\n\n        subgraph Dims[\"5 Dimensions\"]\n            D1[region]\n            D2[category]\n            D3[month]\n            D4[quarter]\n            D5[year]\n        end\n\n        subgraph Mats[\"3 Materializations\"]\n            MAT1[daily_metrics]\n            MAT2[monthly_by_region]\n            MAT3[category_performance]\n        end\n    end\n\n    subgraph Output[\"Materialized Tables\"]\n        O1[(agg_daily_metrics&lt;br/&gt;1 row)]\n        O2[(agg_monthly_region&lt;br/&gt;4 rows)]\n        O3[(agg_category_performance&lt;br/&gt;2 rows)]\n    end\n\n    DC --&gt; D1\n    DP --&gt; D2\n    DD --&gt; D3\n    DD --&gt; D4\n    DD --&gt; D5\n    FO --&gt; M1\n    FO --&gt; M2\n    FO --&gt; M3\n    FO --&gt; M4\n    FO --&gt; M5\n\n    MAT1 --&gt; O1\n    MAT2 --&gt; O2\n    MAT3 --&gt; O3\n\n    style StarSchema fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style SemanticLayer fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Output fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#summary","title":"Summary","text":"<p>In this complete example, you saw:</p> <ol> <li>5 Metrics: revenue, cost, profit, order_count, avg_order_value</li> <li>5 Dimensions: region, category, month, quarter, year</li> <li>3 Materializations: daily_metrics, monthly_by_region, category_performance</li> <li>5 Queries: Total, by region, multiple metrics, cross-tab, filtered</li> </ol> <p>The semantic layer provides: - Consistent definitions across all queries - Business-friendly syntax: <code>\"revenue BY region\"</code> - Pre-computed aggregates for fast dashboards - Self-documenting config with descriptions</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial series (Part 2: Semantic Layer), you learned:</p> <ol> <li>What a semantic layer is and why it matters</li> <li>Defining metrics with expressions and filters</li> <li>Defining dimensions for grouping and drill-down</li> <li>Querying with the simple <code>\"metric BY dimension\"</code> syntax</li> <li>Materializing metrics for dashboard performance</li> <li>Putting it all together in a production-ready configuration</li> </ol>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#next-steps","title":"Next Steps","text":"<p>The final tutorial covers FK validation for data quality.</p> <p>Next: FK Validation</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#navigation","title":"Navigation","text":"Previous Up Next Materializing Metrics Tutorials FK Validation"},{"location":"tutorials/dimensional_modeling/13_fk_validation/","title":"FK Validation Tutorial","text":"<p>In this tutorial, you'll learn how to validate foreign key relationships between fact and dimension tables to ensure data quality.</p> <p>What You'll Learn: - Why validate foreign keys - Defining relationships - Detecting orphan records - Handling orphans with different strategies - Generating lineage diagrams</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#why-validate-foreign-keys","title":"Why Validate Foreign Keys?","text":"<p>In a star schema, fact tables reference dimension tables via foreign keys. But what happens when: - A customer places an order, but the customer isn't in <code>dim_customer</code>? - An order references a product that was never loaded? - A date value doesn't exist in the date dimension?</p> <p>These are called orphan records\u2014facts that reference non-existent dimensions.</p> <p>Problems with orphans: - Reports show \"Unknown\" values - Counts and sums may be incorrect - Data quality issues go undetected - Downstream analytics are unreliable</p> <p>FK validation helps you detect, report, and handle these issues.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#source-data","title":"Source Data","text":"<p>We'll use the fact_orders table with some intentional orphan records:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#fact_orders-35-rows-including-5-orphans","title":"fact_orders (35 rows including 5 orphans)","text":"order_id customer_sk product_sk date_sk line_total status ORD001 1 1 20240115 1299.99 completed ORD002 1 2 20240115 59.98 completed ... ... ... ... ... ... ORD030 5 10 20240116 189.99 completed ORD031 99 1 20240117 1299.99 completed ORD032 88 2 20240118 29.99 completed ORD033 77 3 20240119 249.99 completed ORD034 66 4 20240120 49.99 completed ORD035 55 5 20240121 599.99 completed <p>Orphan records: ORD031-ORD035 have customer_sk values (99, 88, 77, 66, 55) that don't exist in dim_customer.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#dim_customer-13-rows","title":"dim_customer (13 rows)","text":"customer_sk customer_id name 0 -1 Unknown 1 C001 Alice Johnson 2 C002 Bob Smith ... ... ... 12 C012 Leo Anderson <p>Note: customer_sk values 55, 66, 77, 88, 99 do NOT exist.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-1-define-relationships","title":"Step 1: Define Relationships","text":"<p>First, declare the FK relationships in your star schema:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code","title":"Python Code","text":"<pre><code>from odibi.validation.fk import RelationshipConfig, RelationshipRegistry\n\n# Define all FK relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_dates\",\n        fact=\"fact_orders\",\n        dimension=\"dim_date\",\n        fact_key=\"date_sk\",\n        dimension_key=\"date_sk\",\n        nullable=True,  # Pending orders may not have dates\n        on_violation=\"warn\"\n    )\n]\n\n# Create registry\nregistry = RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#yaml-alternative","title":"YAML Alternative","text":"<pre><code># relationships.yaml\nrelationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_dates\n    fact: fact_orders\n    dimension: dim_date\n    fact_key: date_sk\n    dimension_key: date_sk\n    nullable: true\n    on_violation: warn\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#relationshipconfig-fields","title":"RelationshipConfig Fields","text":"Field Type Required Description <code>name</code> str Yes Unique identifier <code>fact</code> str Yes Fact table name <code>dimension</code> str Yes Dimension table name <code>fact_key</code> str Yes FK column in fact <code>dimension_key</code> str Yes PK/SK column in dimension <code>nullable</code> bool No Whether nulls are allowed (default: false) <code>on_violation</code> str No Action: \"error\", \"warn\", \"filter\" (default: \"error\")"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-2-validate-a-clean-fact-table","title":"Step 2: Validate a Clean Fact Table","text":"<p>Let's first validate a fact table with NO orphans:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_1","title":"Python Code","text":"<pre><code>from odibi.validation.fk import FKValidator\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load data\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")  # Clean data - 30 rows\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# Create validator\nvalidator = FKValidator(registry)\n\n# Validate\nreport = validator.validate_fact(fact_orders, \"fact_orders\", context)\n\n# Check results\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Total relationships: {report.total_relationships}\")\nprint(f\"Valid relationships: {report.valid_relationships}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#validation-report-clean-data","title":"Validation Report (Clean Data)","text":"<pre><code>All valid: True\nTotal relationships: 3\nValid relationships: 3\n\nRelationship: orders_to_customers\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n\nRelationship: orders_to_products\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n\nRelationship: orders_to_dates\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-3-validate-with-orphan-records","title":"Step 3: Validate with Orphan Records","text":"<p>Now let's validate the data with 5 orphan records:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_2","title":"Python Code","text":"<pre><code># Load data with orphans\nfact_orders_dirty = pd.read_csv(\"data/orders_with_orphans.csv\")\n\n# This has 35 rows - 5 with invalid customer_sk values\n\n# Validate\nreport = validator.validate_fact(fact_orders_dirty, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Orphan records found: {len(report.orphan_records)}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#validation-report-with-orphans","title":"Validation Report (With Orphans)","text":"<pre><code>All valid: False\nTotal relationships: 3\nValid relationships: 2\nOrphan records found: 5\n\nRelationship: orders_to_customers\n  Status: INVALID\n  Total rows: 35\n  Orphan count: 5\n  Orphan values (sample):\n    - customer_sk = 99 (1 occurrence)\n    - customer_sk = 88 (1 occurrence)\n    - customer_sk = 77 (1 occurrence)\n    - customer_sk = 66 (1 occurrence)\n    - customer_sk = 55 (1 occurrence)\n\nRelationship: orders_to_products\n  Status: VALID\n  Total rows: 35\n  Orphan count: 0\n\nRelationship: orders_to_dates\n  Status: VALID\n  Total rows: 35\n  Orphan count: 0\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#detailed-orphan-records","title":"Detailed Orphan Records","text":"<pre><code># Get the orphan records\nfor orphan in report.orphan_records:\n    print(f\"Order {orphan.order_id}: customer_sk={orphan.customer_sk} not found\")\n</code></pre> order_id customer_sk reason ORD031 99 Not found in dim_customer ORD032 88 Not found in dim_customer ORD033 77 Not found in dim_customer ORD034 66 Not found in dim_customer ORD035 55 Not found in dim_customer"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-4-handle-orphans-with-different-strategies","title":"Step 4: Handle Orphans with Different Strategies","text":""},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-1-error-default","title":"Strategy 1: Error (Default)","text":"<p>Raise an exception when orphans are found:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load\n\ntry:\n    validated_df = validate_fk_on_load(\n        fact_df=fact_orders_dirty,\n        relationships=relationships,\n        context=context,\n        on_failure=\"error\"\n    )\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre> <p>Output:</p> <pre><code>Validation failed: FK validation found 5 orphan records:\n  - orders_to_customers: 5 orphans (customer_sk: 99, 88, 77, 66, 55)\n</code></pre> <p>Use case: Strict data quality\u2014fail the pipeline if any orphans exist.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-2-warn","title":"Strategy 2: Warn","text":"<p>Log a warning but continue processing:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.WARNING)\n\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=relationships,\n    context=context,\n    on_failure=\"warn\"\n)\n\nprint(f\"Returned {len(validated_df)} rows (including orphans)\")\n</code></pre> <p>Output:</p> <pre><code>WARNING:odibi.validation.fk:FK validation found 5 orphan records for orders_to_customers\n  Orphan values: customer_sk in [99, 88, 77, 66, 55]\nReturned 35 rows (including orphans)\n</code></pre> <p>Use case: Log issues for investigation but don't block processing.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-3-filter","title":"Strategy 3: Filter","text":"<p>Remove orphan records from the result:</p> <pre><code>validated_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=relationships,\n    context=context,\n    on_failure=\"filter\"\n)\n\nprint(f\"Before: {len(fact_orders_dirty)} rows\")\nprint(f\"After:  {len(validated_df)} rows\")\nprint(f\"Filtered: {len(fact_orders_dirty) - len(validated_df)} orphan rows\")\n</code></pre> <p>Output:</p> <pre><code>Before: 35 rows\nAfter:  30 rows\nFiltered: 5 orphan rows\n</code></pre> <p>Before (35 rows):</p> order_id customer_sk line_total ORD001 1 1299.99 ORD002 1 59.98 ... ... ... ORD030 5 189.99 ORD031 99 1299.99 ORD032 88 29.99 ORD033 77 249.99 ORD034 66 49.99 ORD035 55 599.99 <p>After (30 rows - orphans removed):</p> order_id customer_sk line_total ORD001 1 1299.99 ORD002 1 59.98 ... ... ... ORD030 5 189.99 <p>Use case: Silently exclude bad data (use with caution\u2014you may lose legitimate records).</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-5-generate-lineage-from-relationships","title":"Step 5: Generate Lineage from Relationships","text":"<p>The relationship registry can generate a lineage graph:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_3","title":"Python Code","text":"<pre><code>lineage = registry.generate_lineage()\nprint(lineage)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#output","title":"Output","text":"<pre><code>{\n    'fact_orders': ['dim_customer', 'dim_product', 'dim_date']\n}\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#mermaid-diagram","title":"Mermaid Diagram","text":"<pre><code># Generate mermaid diagram code\nmermaid_code = registry.to_mermaid()\nprint(mermaid_code)\n</code></pre> <pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        string order_id PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        decimal line_total\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string region\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        string month_name\n    }\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#complete-example","title":"Complete Example","text":"<p>Here's a complete script for FK validation:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nFK Validation Example\n\nThis script demonstrates:\n1. Defining FK relationships\n2. Validating a fact table\n3. Handling orphan records\n4. Generating lineage\n\"\"\"\n\nfrom odibi.validation.fk import (\n    RelationshipConfig,\n    RelationshipRegistry,\n    FKValidator,\n    validate_fk_on_load\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# =============================================================================\n# 1. LOAD DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 1: Load Data\")\nprint(\"=\" * 60)\n\n# Clean fact data\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\nprint(f\"fact_orders: {len(fact_orders)} rows\")\n\n# Fact data with orphans\nfact_orders_dirty = pd.read_csv(\n    \"examples/tutorials/dimensional_modeling/data/orders_with_orphans.csv\"\n)\nprint(f\"fact_orders_dirty: {len(fact_orders_dirty)} rows (5 orphans)\")\n\n# Dimensions\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\nprint()\n\n# =============================================================================\n# 2. DEFINE RELATIONSHIPS\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 2: Define Relationships\")\nprint(\"=\" * 60)\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_dates\",\n        fact=\"fact_orders\",\n        dimension=\"dim_date\",\n        fact_key=\"date_sk\",\n        dimension_key=\"date_sk\",\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\nprint(f\"Defined {len(relationships)} relationships\")\nprint()\n\n# =============================================================================\n# 3. SETUP CONTEXT\n# =============================================================================\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# =============================================================================\n# 4. VALIDATE CLEAN DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 3: Validate Clean Data\")\nprint(\"=\" * 60)\n\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(fact_orders, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Relationships checked: {report.total_relationships}\")\nprint(f\"Valid: {report.valid_relationships}\")\nprint()\n\n# =============================================================================\n# 5. VALIDATE DIRTY DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 4: Validate Data with Orphans\")\nprint(\"=\" * 60)\n\nreport = validator.validate_fact(fact_orders_dirty, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Orphan records: {len(report.orphan_records)}\")\n\nfor result in report.results:\n    status = \"VALID\" if result.valid else \"INVALID\"\n    print(f\"\\n  {result.relationship_name}: {status}\")\n    print(f\"    Total rows: {result.total_rows}\")\n    print(f\"    Orphans: {result.orphan_count}\")\n    if not result.valid:\n        print(f\"    Orphan values: {result.orphan_values[:5]}\")\nprint()\n\n# =============================================================================\n# 6. DEMONSTRATE HANDLING STRATEGIES\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 5: Orphan Handling Strategies\")\nprint(\"=\" * 60)\n\n# Filter strategy\nfiltered_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=[relationships[0]],  # Just customer relationship\n    context=context,\n    on_failure=\"filter\"\n)\n\nprint(f\"\\nFilter Strategy:\")\nprint(f\"  Before: {len(fact_orders_dirty)} rows\")\nprint(f\"  After:  {len(filtered_df)} rows\")\nprint(f\"  Removed: {len(fact_orders_dirty) - len(filtered_df)} orphan rows\")\nprint()\n\n# =============================================================================\n# 7. GENERATE LINEAGE\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 6: Generate Lineage\")\nprint(\"=\" * 60)\n\nlineage = registry.generate_lineage()\nprint(f\"Lineage: {lineage}\")\n\nprint(\"\\nDimensions referenced by fact_orders:\")\nfor dim in lineage.get(\"fact_orders\", []):\n    print(f\"  \u2192 {dim}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"COMPLETE!\")\nprint(\"=\" * 60)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Why FK validation matters: Orphan records cause data quality issues</li> <li>Defining relationships: Specify fact, dimension, and key columns</li> <li>Validating facts: Use FKValidator to detect orphans</li> <li>Handling strategies:</li> <li><code>error</code>: Fail the pipeline (strict)</li> <li><code>warn</code>: Log and continue (monitoring)</li> <li><code>filter</code>: Remove orphans (permissive)</li> <li>Generating lineage: Visualize table relationships</li> </ul>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#congratulations","title":"Congratulations!","text":"<p>You've completed the entire dimensional modeling tutorial series!</p> <p>What you built: - Part 1: Complete star schema (dimensions, facts, aggregates) - Part 2: Semantic layer (metrics, dimensions, materializations) - Part 3: FK validation (data quality)</p> <p>You now have all the tools to build production-ready dimensional data warehouses with Odibi.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#navigation","title":"Navigation","text":"Previous Up Next Semantic Full Example Tutorials -"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#reference","title":"Reference","text":"<p>For complete documentation, see: FK Validation Reference</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/","title":"Pipeline Run Story: schema_evolution_demo","text":"<p>Executed: 2025-11-19T16:56:10.535762 Completed: 2025-11-19T16:56:10.611748 Duration: 0.08s Status: \u2705 Success</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#summary","title":"Summary","text":"<ul> <li>\u2705 Completed: 3 nodes</li> <li>\u274c Failed: 0 nodes</li> <li>\u23ed\ufe0f Skipped: 0 nodes</li> <li>\u23f1\ufe0f Duration: 0.08s</li> </ul> <p>Completed nodes: create_source, enrich_data, cleanup_data</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-create_source","title":"Node: create_source","text":"<p>Status: \u2705 Success Duration: 0.0336s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Output schema: - Columns (3): product, region, sales - Rows: 3</p> <p>Sample output (first 3 rows):</p> product region sales A North 100 B North 150 A South 200"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-enrich_data","title":"Node: enrich_data","text":"<p>Status: \u2705 Success Duration: 0.0264s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Input schema: - Columns (3): product, region, sales</p> <p>Sample input (first 3 rows):</p> product region sales A North 100 B North 150 A South 200 <p>Output schema: - Columns (4): product, region, sales, tax</p> <p>Schema Changes: - \ud83d\udfe2 Added: tax - Rows: 3</p> <p>Sample output (first 3 rows):</p> product region sales tax A North 100 10.0 B North 150 15.0 A South 200 20.0"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-cleanup_data","title":"Node: cleanup_data","text":"<p>Status: \u2705 Success Duration: 0.0160s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Input schema: - Columns (4): product, region, sales, tax</p> <p>Sample input (first 3 rows):</p> product region sales tax A North 100 10.0 B North 150 15.0 A South 200 20.0 <p>Output schema: - Columns (3): product, sales, tax</p> <p>Schema Changes: - \ud83d\udd34 Removed: region - Rows: 3</p> <p>Sample output (first 3 rows):</p> product sales tax A 100 10.0 B 150 15.0 A 200 20.0"},{"location":"validation/fk/","title":"FK Validation","text":"<p>The FK Validation module declares and validates referential integrity between fact and dimension tables.</p>"},{"location":"validation/fk/#how-it-fits-into-odibi","title":"How It Fits Into Odibi","text":"<p>FK Validation is a Python API module that complements the <code>fact</code> pattern. While the <code>fact</code> pattern handles basic orphan handling (unknown member assignment), the FK validation module provides:</p> <ul> <li>Detailed orphan reporting with sample values</li> <li>Multiple validation strategies (error, warn, filter)</li> <li>Relationship registry for documenting your data model</li> <li>Lineage generation from relationships</li> </ul> <p>It's typically used for: 1. Post-pipeline validation/auditing 2. Custom fact loading with advanced orphan handling 3. Documenting relationships for data governance</p>"},{"location":"validation/fk/#quick-start","title":"Quick Start","text":""},{"location":"validation/fk/#1-define-relationships","title":"1. Define Relationships","text":"<pre><code>from odibi.validation.fk import RelationshipConfig, RelationshipRegistry\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"validation/fk/#2-validate-a-fact-table","title":"2. Validate a Fact Table","text":"<pre><code>from odibi.validation.fk import FKValidator\nfrom odibi.context import EngineContext\n\n# Setup context with dimension tables\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\ncontext.register(\"dim_product\", spark.table(\"warehouse.dim_product\"))\n\n# Load fact table\nfact_df = spark.table(\"warehouse.fact_orders\")\n\n# Validate\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(fact_df, \"fact_orders\", context)\n\n# Check results\nif report.all_valid:\n    print(\"All FK relationships valid!\")\nelse:\n    print(f\"Found {len(report.orphan_records)} orphan records\")\n    for result in report.results:\n        if not result.valid:\n            print(f\"  {result.relationship_name}: {result.orphan_count} orphans\")\n</code></pre>"},{"location":"validation/fk/#yaml-configuration-optional","title":"YAML Configuration (Optional)","text":"<p>You can define relationships in YAML and load them:</p> <pre><code># relationships.yaml\nrelationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_dates\n    fact: fact_orders\n    dimension: dim_date\n    fact_key: order_date_sk\n    dimension_key: date_sk\n    nullable: true  # Pending orders may not have date\n    on_violation: warn\n</code></pre> <pre><code>from odibi.validation.fk import parse_relationships_config\nimport yaml\n\nwith open(\"relationships.yaml\") as f:\n    config = yaml.safe_load(f)\n\nregistry = parse_relationships_config(config)\n</code></pre>"},{"location":"validation/fk/#relationshipconfig","title":"RelationshipConfig","text":"Field Type Required Default Description <code>name</code> str Yes - Unique relationship identifier <code>fact</code> str Yes - Fact table name <code>dimension</code> str Yes - Dimension table name <code>fact_key</code> str Yes - FK column in fact table <code>dimension_key</code> str Yes - PK/SK column in dimension <code>nullable</code> bool No false Whether nulls are allowed in fact_key <code>on_violation</code> str No \"error\" Action on violation: \"error\", \"warn\", \"quarantine\""},{"location":"validation/fk/#validation-results","title":"Validation Results","text":""},{"location":"validation/fk/#fkvalidationresult-per-relationship","title":"FKValidationResult (per relationship)","text":"Field Type Description <code>relationship_name</code> str Relationship identifier <code>valid</code> bool Whether validation passed <code>total_rows</code> int Total rows in fact table <code>orphan_count</code> int Number of orphan records <code>null_count</code> int Number of null FK values <code>orphan_values</code> list Sample orphan values (up to 100) <code>elapsed_ms</code> float Validation time"},{"location":"validation/fk/#fkvalidationreport-for-entire-fact-table","title":"FKValidationReport (for entire fact table)","text":"Field Type Description <code>fact_table</code> str Fact table name <code>all_valid</code> bool True if all relationships valid <code>total_relationships</code> int Number of relationships checked <code>valid_relationships</code> int Number that passed <code>results</code> List[FKValidationResult] Individual results <code>orphan_records</code> List[OrphanRecord] All orphan records"},{"location":"validation/fk/#validate_fk_on_load","title":"validate_fk_on_load","text":"<p>Convenience function for pipeline integration:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load, RelationshipConfig\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    )\n]\n\n# Error on violation (default) - raises ValueError\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"error\"\n)\n\n# Warn on violation - logs warning, returns original\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"warn\"\n)\n\n# Filter orphans - removes orphan rows\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"filter\"\n)\n</code></pre>"},{"location":"validation/fk/#integration-with-fact-pattern","title":"Integration with Fact Pattern","text":"<p>The <code>fact</code> pattern already handles basic orphan handling. Use FK validation for additional auditing:</p> <pre><code># odibi.yaml - Build fact with orphan handling\npipelines:\n  - pipeline: build_facts\n    nodes:\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n\n      - name: fact_orders\n        depends_on: [dim_customer]\n        read:\n          connection: staging\n          path: orders\n        transformer: fact\n        params:\n          grain: [order_id]\n          dimensions:\n            - source_column: customer_id\n              dimension_table: dim_customer\n              dimension_key: customer_id\n              surrogate_key: customer_sk\n          orphan_handling: unknown  # Assigns SK=0 to orphans\n        write:\n          connection: warehouse\n          path: fact_orders\n</code></pre> <p>Then run FK validation as a post-pipeline check:</p> <pre><code># Post-pipeline audit\nfrom odibi.validation.fk import FKValidator, RelationshipRegistry, RelationshipConfig\n\n# Define expected relationships\nregistry = RelationshipRegistry(relationships=[\n    RelationshipConfig(\n        name=\"verify_customer_fk\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    )\n])\n\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(\n    spark.table(\"warehouse.fact_orders\"),\n    \"fact_orders\",\n    context\n)\n\n# Report findings\nif not report.all_valid:\n    print(f\"WARNING: {report.orphan_records} orphan records found\")\n    # Log to monitoring, send alert, etc.\n</code></pre>"},{"location":"validation/fk/#lineage-generation","title":"Lineage Generation","text":"<p>Generate lineage graph from relationships:</p> <pre><code>registry = RelationshipRegistry(relationships=[\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\"\n    ),\n    RelationshipConfig(\n        name=\"line_items_to_orders\",\n        fact=\"fact_line_items\",\n        dimension=\"fact_orders\",\n        fact_key=\"order_sk\",\n        dimension_key=\"order_sk\"\n    )\n])\n\nlineage = registry.generate_lineage()\n# {\n#     'fact_orders': ['dim_customer', 'dim_product'],\n#     'fact_line_items': ['fact_orders']\n# }\n</code></pre>"},{"location":"validation/fk/#full-example","title":"Full Example","text":"<p>Complete FK validation workflow:</p> <pre><code>from odibi.validation.fk import (\n    RelationshipConfig,\n    RelationshipRegistry,\n    FKValidator,\n    get_orphan_records\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\n\n# Define relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        nullable=False,\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\nvalidator = FKValidator(registry)\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\ncontext.register(\"dim_product\", spark.table(\"warehouse.dim_product\"))\n\n# Load and validate\nfact_df = spark.table(\"warehouse.fact_orders\")\nreport = validator.validate_fact(fact_df, \"fact_orders\", context)\n\n# Report\nprint(f\"Validation {'PASSED' if report.all_valid else 'FAILED'}\")\nprint(f\"Checked {report.total_relationships} relationships\")\n\nfor result in report.results:\n    status = \"PASS\" if result.valid else \"FAIL\"\n    print(f\"  {result.relationship_name}: {status}\")\n    if not result.valid:\n        print(f\"    Orphans: {result.orphan_count}\")\n        print(f\"    Sample values: {result.orphan_values[:5]}\")\n\n# Generate lineage\nlineage = registry.generate_lineage()\nprint(f\"Lineage: {lineage}\")\n</code></pre>"},{"location":"validation/fk/#pipeline-integration-patterns","title":"Pipeline Integration Patterns","text":""},{"location":"validation/fk/#pattern-1-pre-load-validation","title":"Pattern 1: Pre-Load Validation","text":"<p>Validate FK relationships before writing to the warehouse:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load, RelationshipConfig\n\n@transform\ndef validate_and_load_orders(context, current):\n    \"\"\"Validate FKs before writing to warehouse.\"\"\"\n    relationships = [\n        RelationshipConfig(\n            name=\"orders_to_customers\",\n            fact=\"orders\",\n            dimension=\"dim_customer\",\n            fact_key=\"customer_id\",\n            dimension_key=\"customer_id\"\n        )\n    ]\n\n    validated_df = validate_fk_on_load(\n        fact_df=current,\n        relationships=relationships,\n        context=context,\n        on_failure=\"filter\"  # Remove orphans\n    )\n    return validated_df\n</code></pre> <p>YAML configuration:</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: warehouse\n      path: dim_customer\n\n  - name: validated_orders\n    depends_on: [dim_customer]\n    read:\n      connection: staging\n      path: orders\n    transform:\n      steps:\n        - function: validate_and_load_orders\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/fk/#pattern-2-post-pipeline-audit-job","title":"Pattern 2: Post-Pipeline Audit Job","text":"<p>Run FK validation as a separate audit pipeline:</p> <pre><code>pipelines:\n  - pipeline: audit_referential_integrity\n    description: \"Nightly FK validation audit\"\n    nodes:\n      - name: load_dimensions\n        read:\n          connection: warehouse\n          tables:\n            - dim_customer\n            - dim_product\n            - dim_date\n\n      - name: validate_fact_orders\n        depends_on: [load_dimensions]\n        read:\n          connection: warehouse\n          path: fact_orders\n        transform:\n          steps:\n            - function: run_fk_audit\n              params:\n                relationships_file: \"config/fk_relationships.yaml\"\n</code></pre>"},{"location":"validation/fk/#pattern-3-integrated-with-data-quality-gate","title":"Pattern 3: Integrated with Data Quality Gate","text":"<p>Use FK validation as a quality gate:</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: staging\n      path: orders\n    transformer: fact\n    params:\n      grain: [order_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          dimension_key: customer_id\n          surrogate_key: customer_sk\n    gate:\n      - type: custom\n        function: fk_validation_gate\n        params:\n          max_orphan_percent: 0.1  # Fail if &gt; 0.1% orphans\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/fk/#see-also","title":"See Also","text":"<ul> <li>Fact Pattern - Build fact tables with orphan handling</li> <li>Dimension Pattern - Build dimensions with unknown member</li> <li>Patterns Overview - All available patterns</li> </ul>"}]}