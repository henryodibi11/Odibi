{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Odibi Documentation","text":"<p>Declarative data engineering. YAML in, pipelines out.</p>"},{"location":"#quick-start","title":"\u26a1 Quick Start","text":"<pre><code>pip install odibi\n\n# Create a project from template\nodibi init my_project --template star-schema\ncd my_project\n\n# Run it\nodibi run odibi.yaml\n\n# View the audit report\nodibi story last\n</code></pre> <p>That's it. You now have a working star schema pipeline.</p>"},{"location":"#start-here","title":"\ud83c\udfaf Start Here","text":"Goal Go to Get running in 10 minutes Golden Path Copy THE working config THE_REFERENCE.md \u2b50 Solve a specific problem Playbook"},{"location":"#what-is-odibi","title":"What is Odibi?","text":"<p>Odibi is a framework for building data pipelines. You describe what you want in YAML; Odibi handles how.</p> <ul> <li>Declarative: YAML over imperative Python</li> <li>Auditable: Every run generates a \"Data Story\" (HTML report)</li> <li>Dual-engine: Pandas (local) \u2192 Spark (production) with zero config changes</li> </ul>"},{"location":"#cli-cheat-sheet","title":"CLI Cheat Sheet","text":"<pre><code># Create &amp; Run\nodibi init my_project          # Scaffold from template\nodibi run odibi.yaml           # Execute pipeline\nodibi validate odibi.yaml      # Check config without running\n\n# Debug\nodibi story last               # View most recent story\nodibi story last --node X      # Inspect a specific node\nodibi doctor                   # Check environment health\nodibi graph odibi.yaml         # Visualize dependencies\n</code></pre>"},{"location":"#the-canonical-example","title":"The Canonical Example","text":"<p>Every new user should run this first:</p> <pre><code>cd docs/examples/canonical/runnable\nodibi run 04_fact_table.yaml\n</code></pre> <p>This builds a complete star schema with dimensions, facts, FK lookups, and orphan handling. See the full breakdown \u2192</p>"},{"location":"#documentation-map","title":"Documentation Map","text":""},{"location":"#new-to-odibi","title":"New to Odibi?","text":"<ol> <li>Golden Path \u2014 Zero to running in 10 minutes</li> <li>THE_REFERENCE.md \u2014 The one example to copy</li> </ol>"},{"location":"#building-pipelines","title":"Building Pipelines?","text":"<ul> <li>Patterns \u2014 SCD2, Merge, Aggregation, etc.</li> <li>YAML Schema \u2014 Complete configuration reference</li> <li>Canonical Examples \u2014 More runnable configs</li> </ul>"},{"location":"#going-to-production","title":"Going to Production?","text":"<ul> <li>Decision Guide \u2014 When to use what</li> <li>Production Deployment \u2014 Cloud setup</li> <li>Alerting \u2014 Notifications on failure</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Repository: GitHub</li> <li>Issues: Report a Bug</li> <li>PyPI: View Package</li> </ul> <p>Last updated: January 2025</p>"},{"location":"ODIBI_DEEP_CONTEXT/","title":"Odibi Deep Context","text":"<p>What is Odibi? A Python data pipeline framework for building enterprise data warehouses. It orchestrates nodes (read \u2192 transform \u2192 validate \u2192 write) with dependency resolution, supports Pandas/Spark/Polars engines, and provides patterns for common DWH tasks (SCD2, dimension tables, fact tables, merges).</p> <p>Target User: Solo data engineer or small team building data pipelines without dedicated infrastructure support.</p> <p>Core Philosophy: YAML-first configuration, engine parity (same YAML works on Pandas/Spark), patterns for DWH best practices.</p> <p>\ud83d\udca1 AI/LLM Tip: Use CLI introspection to discover features programmatically: <pre><code>odibi list transformers --format json   # All 52+ transformers\nodibi list patterns --format json       # All 6 patterns\nodibi explain &lt;name&gt;                    # Detailed docs + example YAML\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Critical Runtime Behavior</li> <li>Core Execution Model</li> <li>Patterns Reference</li> <li>Transformers Reference</li> <li>Validation &amp; Quarantine</li> <li>Connections Reference</li> <li>Write Configuration (Delta Lake Features)</li> <li>Alerts &amp; Notifications</li> <li>System Catalog (The Brain)</li> <li>OpenLineage Integration</li> <li>Foreign Key Validation</li> <li>Orchestration Export</li> <li>Manufacturing Transformers</li> <li>Semantic Layer</li> <li>Story Generation</li> <li>Common Workflows</li> <li>CLI Reference</li> <li>Anti-Patterns and Gotchas</li> <li>SQL Server Writer</li> <li>Incremental Loading (Advanced)</li> <li>Diagnostics &amp; Diff Tools</li> <li>Cross-Check Transformer</li> <li>Testing Utilities</li> <li>Derived Updater (Internal)</li> <li>Extension Points</li> <li>Quick Reference Cheat Sheet</li> <li>Documentation Map</li> </ol>"},{"location":"ODIBI_DEEP_CONTEXT/#1-critical-runtime-behavior","title":"1. Critical Runtime Behavior","text":""},{"location":"ODIBI_DEEP_CONTEXT/#11-spark-temp-view-registration-most-important","title":"1.1 Spark Temp View Registration (MOST IMPORTANT)","text":"<p>When using PipelineManager with Spark engine, each node's output DataFrame is registered as a Spark temp view:</p> <pre><code># In odibi/context.py, SparkContext.register() at line 432:\ndf.createOrReplaceTempView(name)\n</code></pre> <p>You can query any node's output with: <pre><code>spark.sql(\"SELECT * FROM node_name\")\n</code></pre></p> <p>Constraints: - Node names must be alphanumeric + underscore only (no spaces, hyphens, dots) - Validated by <code>SparkContext._validate_name()</code> with regex <code>^[a-zA-Z0-9_]+$</code> - Invalid names raise <code>ValueError</code>: \"Invalid node name 'X' for Spark engine. Names must contain only alphanumeric characters and underscores\"</p> <p>Lifecycle: - Views persist for duration of pipeline execution - Tracked in <code>SparkContext._registered_views: set[str]</code> - Cleaned up via <code>context.clear()</code> which calls <code>spark.catalog.dropTempView(name)</code> - Thread-safe: Uses <code>threading.RLock()</code> for concurrent access</p> <p>How to list registered views: <pre><code># From context\ncontext.list_names()  # Returns list of registered DataFrame names\n\n# From SparkSession\nspark.catalog.listTables()\n</code></pre></p> <p>How to get a DataFrame back: <pre><code># Via context\ndf = context.get(\"node_name\")\n\n# Via Spark (equivalent)\ndf = spark.table(\"node_name\")\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#12-pandas-context-behavior","title":"1.2 Pandas Context Behavior","text":"<p>With Pandas engine, DataFrames are stored in an in-memory dictionary: <pre><code># odibi/context.py, PandasContext at line 209:\nself._data: Dict[str, pd.DataFrame] = {}\n</code></pre></p> <ul> <li>No temp views - data accessed via <code>context.get(name)</code></li> <li>SQL operations use DuckDB under the hood (not Spark SQL)</li> <li>The <code>context.sql(query)</code> method:</li> <li>Registers current DataFrame as a unique temp view</li> <li>Replaces <code>df</code> references in query with that view name</li> <li>Executes via DuckDB</li> <li>Cleans up temp view</li> </ul> <p>DuckDB vs Spark SQL differences: | Feature | Pandas (DuckDB) | Spark | |---------|-----------------|-------| | Exclude columns | <code>SELECT * EXCLUDE (col)</code> | <code>SELECT * EXCEPT (col)</code> | | Row number | <code>ROW_NUMBER() OVER (...)</code> | Same | | Array explode | <code>UNNEST(arr)</code> | <code>explode(arr)</code> |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#13-polars-context-behavior","title":"1.3 Polars Context Behavior","text":"<p>Similar to Pandas - in-memory dictionary storage: <pre><code># odibi/context.py, PolarsContext:\nself._data: Dict[str, Any] = {}  # Stores pl.DataFrame or pl.LazyFrame\n</code></pre></p> <ul> <li>Supports both eager (<code>DataFrame</code>) and lazy (<code>LazyFrame</code>) evaluation</li> <li>Limited SQL support compared to Pandas/Spark</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#14-data-flow-between-nodes","title":"1.4 Data Flow Between Nodes","text":"<p>When Node B depends on Node A:</p> <ol> <li>Node A completes \u2192 output DataFrame registered via <code>context.register(name, df)</code></li> <li>Node B starts \u2192 gets input via <code>context.get(\"node_a\")</code></li> <li>For Spark: Node B can also use <code>spark.sql(\"SELECT * FROM node_a\")</code></li> </ol> <pre><code># YAML declaration\nnodes:\n  - name: node_b\n    depends_on: [node_a]  # Explicit dependency (required for execution order)\n    inputs:\n      source: node_a      # How to reference the data\n</code></pre> <p>Cross-Pipeline References: <pre><code>inputs:\n  customers: $other_pipeline.customer_node  # Reference node from another pipeline\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#2-core-execution-model","title":"2. Core Execution Model","text":""},{"location":"ODIBI_DEEP_CONTEXT/#21-pipeline-execution-flow","title":"2.1 Pipeline Execution Flow","text":"<pre><code>PipelineManager.run()\n    \u2502\n    \u251c\u2500\u2500 1. Load project.yaml + pipeline.yaml\n    \u251c\u2500\u2500 2. Build DependencyGraph (topological sort)\n    \u251c\u2500\u2500 3. Create Context (Pandas/Spark/Polars)\n    \u251c\u2500\u2500 4. Register standard transformers\n    \u2502\n    \u2514\u2500\u2500 5. For each node in execution order:\n            \u2502\n            \u251c\u2500\u2500 PRE-SQL: Execute pre_sql statements\n            \u251c\u2500\u2500 READ: Load data from source/connection\n            \u2502   \u2514\u2500\u2500 Apply incremental filter if configured\n            \u251c\u2500\u2500 TRANSFORM: Apply transformers/patterns in order\n            \u251c\u2500\u2500 VALIDATE: Run quality checks\n            \u2502   \u251c\u2500\u2500 Tests with on_fail=FAIL \u2192 stop pipeline\n            \u2502   \u251c\u2500\u2500 Tests with on_fail=WARN \u2192 log warning\n            \u2502   \u2514\u2500\u2500 Tests with on_fail=QUARANTINE \u2192 route to quarantine table\n            \u251c\u2500\u2500 WRITE: Persist to target\n            \u2502   \u2514\u2500\u2500 Add metadata columns if configured\n            \u2514\u2500\u2500 REGISTER: context.register(node_name, df)\n</code></pre> <p>Key Classes: | Class | File | Role | |-------|------|------| | <code>Pipeline</code> | <code>odibi/pipeline.py</code> | Orchestrates execution, manages state | | <code>PipelineManager</code> | <code>odibi/pipeline.py</code> | Entry point, loads config, creates Pipeline | | <code>Node</code> | <code>odibi/node.py</code> | Single unit of work | | <code>NodeExecutor</code> | <code>odibi/node.py</code> | Executes read/transform/validate/write | | <code>DependencyGraph</code> | <code>odibi/graph.py</code> | Resolves execution order | | <code>Context</code> | <code>odibi/context.py</code> | Stores DataFrames between nodes |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#22-node-execution-phases","title":"2.2 Node Execution Phases","text":"<p>Each node goes through these phases (tracked by <code>PhaseTimer</code>):</p> Phase What Happens <code>pre_sql</code> Execute SQL statements before read <code>read</code> Load data from connection/input <code>incremental_filter</code> Apply HWM or rolling window filter <code>transform</code> Apply each transformer in order <code>validate</code> Run validation tests <code>quarantine</code> Route failed rows if configured <code>gate</code> Evaluate quality gates <code>write</code> Persist to destination <code>post_sql</code> Execute SQL statements after write"},{"location":"ODIBI_DEEP_CONTEXT/#23-error-handling-strategies","title":"2.3 Error Handling Strategies","text":"<pre><code># In pipeline.yaml or CLI\nerror_strategy: fail_fast  # Options: fail_fast, fail_later, ignore\n</code></pre> Strategy Behavior Use Case <code>fail_fast</code> Stop immediately on first error Development, critical pipelines <code>fail_later</code> Continue (dependents skipped), fail at end Batch processing, partial success OK <code>ignore</code> Log errors, continue (dependents run) Non-critical, alerting-only"},{"location":"ODIBI_DEEP_CONTEXT/#24-retry-configuration","title":"2.4 Retry Configuration","text":"<pre><code>retry:\n  max_attempts: 3\n  delay_seconds: 10\n  backoff_multiplier: 2.0  # Exponential backoff\n  retry_on:\n    - ConnectionError\n    - TimeoutError\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#25-context-factory","title":"2.5 Context Factory","text":"<pre><code>from odibi.context import create_context\n\n# Create appropriate context\ncontext = create_context(\"pandas\")  # Returns PandasContext\ncontext = create_context(\"spark\", spark_session=spark)  # Returns SparkContext\ncontext = create_context(\"polars\")  # Returns PolarsContext\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#3-patterns-reference","title":"3. Patterns Reference","text":"<p>Patterns are high-level abstractions for common DWH operations. They encapsulate complex logic into declarative configuration.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#31-dimension-pattern","title":"3.1 Dimension Pattern","text":"<p>Purpose: Create dimension tables with surrogate keys for star schema.</p> <p>Class: <code>DimensionPattern</code> in <code>odibi/patterns/dimension.py</code></p> <p>Features: - Auto-generate integer surrogate keys (MAX + ROW_NUMBER) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> Parameter Type Required Default Description <code>natural_key</code> str/list Yes - Business key column(s) <code>surrogate_key</code> str Yes - Name for generated SK column <code>scd_type</code> int No 1 0=static, 1=overwrite, 2=history <code>track_cols</code> list For SCD1/2 - Columns to monitor for changes <code>target</code> str For SCD2 - Path to existing dimension <code>unknown_member</code> bool No False Add SK=0 unknown row <code>audit.load_timestamp</code> bool No True Add load timestamp <code>audit.source_system</code> str No None Source system name <p>YAML Example: <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 1\n    track_cols: [name, email, tier]\n    unknown_member: true\n    audit:\n      load_timestamp: true\n      source_system: crm\n</code></pre></p> <p>Output Schema Changes: - Adds: <code>{surrogate_key}</code> column (integer, starts at 1) - Adds: <code>_load_timestamp</code> (if audit.load_timestamp=true) - Adds: <code>_source_system</code> (if audit.source_system set) - SCD2 adds: <code>valid_from</code>, <code>valid_to</code>, <code>is_current</code> - Unknown member: Row with SK=0, natural_key=-1 or \"UNKNOWN\"</p>"},{"location":"ODIBI_DEEP_CONTEXT/#32-fact-pattern","title":"3.2 Fact Pattern","text":"<p>Purpose: Build fact tables with foreign key lookups to dimensions.</p> <p>Class: <code>FactPattern</code> in <code>odibi/patterns/fact.py</code></p> <p>Features: - Automatic surrogate key lookups from dimension tables - Orphan handling (unknown member, reject, quarantine) - Grain validation (detect duplicates) - Measure calculations - Deduplication</p> Parameter Type Required Description <code>grain</code> list No Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No Dimension lookup configurations <code>orphan_handling</code> str No <code>unknown</code>, <code>reject</code>, <code>quarantine</code> (default: unknown) <code>quarantine</code> dict If orphan=quarantine Quarantine destination config <code>measures</code> list No Measure column definitions <code>deduplicate</code> bool No Remove duplicates (requires <code>keys</code>) <code>keys</code> list If deduplicate Deduplication keys <code>audit</code> dict No Audit column configuration <p>Dimension Lookup Structure: <pre><code>dimensions:\n  - source_column: customer_id      # FK column in fact\n    dimension_table: dim_customer   # Dimension node name in context\n    dimension_key: customer_id      # NK column in dimension\n    surrogate_key: customer_sk      # SK to retrieve\n    scd2: true                      # Filter is_current=true (optional)\n</code></pre></p> <p>YAML Example: <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n        scd2: true\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n    orphan_handling: unknown  # Orphans get SK=0\n    measures:\n      - quantity\n      - total_amount\n      - profit: \"total_amount - cost\"  # Calculated measure\n    audit:\n      load_timestamp: true\n      source_system: pos\n</code></pre></p> <p>Orphan Handling Options: | Mode | Behavior | |------|----------| | <code>unknown</code> | Assign SK=0 (requires unknown member in dimension) | | <code>reject</code> | Filter out orphan rows | | <code>quarantine</code> | Route orphans to separate table |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#33-scd2-pattern-transformer","title":"3.3 SCD2 Pattern (Transformer)","text":"<p>Purpose: Maintain full history of changes with effective date ranges.</p> <p>Class: <code>SCD2Params</code> in <code>odibi/transformers/scd.py</code></p> Parameter Type Required Description <code>target</code> str Yes* Table name or path <code>connection</code> str Yes* Connection name (with <code>path</code>) <code>path</code> str Yes* Relative path within connection <code>keys</code> list Yes Natural key columns <code>track_cols</code> list Yes Columns to monitor for changes <code>effective_time_col</code> str Yes Source column with change timestamp <code>end_time_col</code> str No Default: <code>valid_to</code> <code>current_flag_col</code> str No Default: <code>is_current</code> <code>delete_col</code> str No Column indicating soft deletion <p>*Either <code>target</code> OR <code>connection</code>+<code>path</code> required, not both.</p> <p>YAML Example (Table Name): <pre><code>transformer: scd2\nparams:\n  target: silver.dim_customers\n  keys: [customer_id]\n  track_cols: [address, tier, email]\n  effective_time_col: updated_at\n</code></pre></p> <p>YAML Example (Connection + Path): <pre><code>transformer: scd2\nparams:\n  connection: adls_prod\n  path: OEE/silver/dim_customers\n  keys: [customer_id]\n  track_cols: [address, tier]\n  effective_time_col: txn_date\n</code></pre></p> <p>How It Works: 1. Match: Find existing records using <code>keys</code> 2. Compare: Check <code>track_cols</code> for changes (uses <code>IS DISTINCT FROM</code> for null-safe comparison) 3. Close: Update old record's <code>end_time_col</code> to <code>effective_time_col</code>, set <code>is_current=False</code> 4. Insert: Add new record with <code>is_current=True</code>, open-ended <code>end_time_col</code></p> <p>CRITICAL: SCD2 returns the FULL history dataset. You MUST use <code>mode: overwrite</code>: <pre><code>write:\n  connection: silver\n  path: dim_customers\n  format: delta\n  mode: overwrite  # NOT append!\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#34-merge-pattern-transformer","title":"3.4 Merge Pattern (Transformer)","text":"<p>Purpose: Upsert, append, or delete records in target table.</p> <p>Class: <code>MergeTransformer</code> in <code>odibi/transformers/merge_transformer.py</code></p> Parameter Type Required Description <code>target</code> str Yes Target table/path <code>keys</code> list Yes Match keys for merge <code>strategy</code> str No <code>upsert</code>, <code>append_only</code>, <code>delete_match</code> <code>created_col</code> str No Audit column for inserts <code>updated_col</code> str No Audit column for updates <code>soft_delete_col</code> str No Column for soft delete flag <p>Strategies: | Strategy | Behavior | |----------|----------| | <code>upsert</code> | Insert new, update existing | | <code>append_only</code> | Insert new only, ignore existing | | <code>delete_match</code> | Delete target rows matching source keys |</p> <p>YAML Example: <pre><code>transformer: merge\nparams:\n  target: silver.products\n  keys: [product_id]\n  strategy: upsert\n  updated_col: _updated_at\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#35-aggregation-pattern","title":"3.5 Aggregation Pattern","text":"<p>Purpose: Group and aggregate data with optional time rollups and incremental merging.</p> <p>Class: <code>AggregationPattern</code> in <code>odibi/patterns/aggregation.py</code></p> Parameter Type Required Description <code>grain</code> list Yes Group by columns <code>measures</code> list Yes SQL aggregation expressions <code>time_rollup</code> str No <code>daily</code>, <code>weekly</code>, <code>monthly</code> <code>merge_strategy</code> str No <code>replace</code>, <code>sum</code>, <code>min</code>, <code>max</code> <code>having</code> str No HAVING clause filter <code>audit_config</code> dict No Add <code>load_timestamp</code>, <code>source_system</code> <p>YAML Example: <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk]\n    measures:\n      - \"SUM(amount) as total_amount\"\n      - \"COUNT(*) as transaction_count\"\n      - \"AVG(unit_price) as avg_price\"\n    having: \"COUNT(*) &gt; 10\"\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#36-date-dimension-pattern","title":"3.6 Date Dimension Pattern","text":"<p>Purpose: Generate a complete date dimension table.</p> <p>Class: <code>DateDimensionPattern</code> in <code>odibi/patterns/date_dimension.py</code></p> Parameter Type Required Description <code>start_date</code> str Yes Start date (YYYY-MM-DD) <code>end_date</code> str Yes End date (YYYY-MM-DD) <code>fiscal_year_start_month</code> int No Fiscal year start (1-12) <code>week_start_day</code> int No Week start (0=Mon, 6=Sun) <code>holidays</code> list No Holiday dates or calendar name <p>YAML Example: <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    fiscal_year_start_month: 7\n    week_start_day: 0\n</code></pre></p> <p>Output Columns: - <code>date_sk</code> (int): Surrogate key (YYYYMMDD format) - <code>date_key</code> (date): Date value - <code>full_date</code> (str): Full date string - <code>year</code>, <code>quarter</code>, <code>month</code>, <code>day</code> - <code>day_of_week</code>, <code>day_of_week_name</code> - <code>week_of_year</code>, <code>month_name</code> - <code>is_weekend</code>, <code>is_holiday</code> - <code>fiscal_year</code>, <code>fiscal_quarter</code>, <code>fiscal_month</code></p>"},{"location":"ODIBI_DEEP_CONTEXT/#4-transformers-reference","title":"4. Transformers Reference","text":"<p>Transformers are atomic operations applied to DataFrames. All transformers work on Pandas (via DuckDB) and Spark with engine parity.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#41-sql-core-transformers","title":"4.1 SQL Core Transformers","text":"Transformer Purpose Key Parameters <code>filter_rows</code> WHERE clause <code>condition: str</code> <code>derive_columns</code> Add computed columns <code>derivations: {col: expr}</code> <code>cast_columns</code> Change column types <code>casts: {col: type}</code> <code>select_columns</code> Keep specific columns <code>columns: list</code> <code>drop_columns</code> Remove columns <code>columns: list</code> <code>rename_columns</code> Rename columns <code>mapping: {old: new}</code> <code>sort</code> Order rows <code>by: list</code>, <code>ascending: bool</code> <code>limit</code> Limit rows <code>n: int</code> <code>sample</code> Random sample <code>fraction: float</code> or <code>n: int</code> <code>distinct</code> Remove duplicates <code>columns: list</code> (optional) <code>fill_nulls</code> Replace nulls <code>columns: list</code>, <code>value: any</code> <code>clean_text</code> Trim/case transform <code>columns</code>, <code>trim</code>, <code>case</code> <code>case_when</code> Conditional logic <code>conditions: list</code>, <code>output_col</code> <code>coalesce_columns</code> First non-null <code>columns</code>, <code>output_col</code> <code>concat_columns</code> String concatenation <code>columns</code>, <code>output_col</code>, <code>separator</code> <code>normalize_column_names</code> Clean column names <code>style</code>, <code>lowercase</code>, <code>remove_special</code> <code>replace_values</code> Bulk value replacement <code>columns</code>, <code>mapping</code> <code>trim_whitespace</code> Trim all string columns <code>columns</code> (optional) <code>add_prefix</code> Add column prefix <code>prefix</code>, <code>columns</code> <code>add_suffix</code> Add column suffix <code>suffix</code>, <code>columns</code> <code>split_part</code> Split string <code>column</code>, <code>delimiter</code>, <code>part</code> <p>Example: <pre><code>transform:\n  - filter_rows:\n      condition: \"status = 'active' AND amount &gt; 0\"\n  - derive_columns:\n      derivations:\n        total_price: \"quantity * unit_price\"\n        full_name: \"concat(first_name, ' ', last_name)\"\n  - cast_columns:\n      casts:\n        created_at: timestamp\n        amount: double\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#42-date-transformers","title":"4.2 Date Transformers","text":"Transformer Purpose Key Parameters <code>extract_date_parts</code> Extract year/month/day <code>column</code>, <code>parts: list</code> <code>date_add</code> Add interval <code>column</code>, <code>days</code>/<code>months</code>/<code>years</code>, <code>output_col</code> <code>date_diff</code> Difference between dates <code>start_col</code>, <code>end_col</code>, <code>unit</code>, <code>output_col</code> <code>date_trunc</code> Truncate to unit <code>column</code>, <code>unit</code> <code>convert_timezone</code> TZ conversion <code>column</code>, <code>from_tz</code>, <code>to_tz</code> <p>Example: <pre><code>transform:\n  - extract_date_parts:\n      column: order_date\n      parts: [year, month, day_of_week]\n  - date_diff:\n      start_col: start_date\n      end_col: end_date\n      unit: days\n      output_col: duration_days\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#43-relational-transformers","title":"4.3 Relational Transformers","text":"Transformer Purpose Key Parameters <code>join</code> Join datasets <code>right_dataset</code>, <code>on</code>, <code>how</code>, <code>prefix</code> <code>union</code> Stack datasets <code>datasets: list</code>, <code>by_name: bool</code> <code>aggregate</code> Group by <code>group_by: list</code>, <code>aggregations: dict</code> <code>pivot</code> Rows to columns <code>group_by</code>, <code>pivot_col</code>, <code>agg_col</code>, <code>agg_func</code> <code>unpivot</code> Columns to rows <code>id_cols</code>, <code>value_vars</code>, <code>var_name</code>, <code>value_name</code> <p>Join Types: <code>inner</code>, <code>left</code>, <code>right</code>, <code>full</code>, <code>cross</code>, <code>anti</code>, <code>semi</code></p> <p>Join Example: <pre><code>transform:\n  - join:\n      right_dataset: dim_customer  # Must be in depends_on!\n      on: [customer_id]\n      how: left\n      prefix: cust  # Avoid column collisions \u2192 cust_name, cust_email\n</code></pre></p> <p>Aggregate Example: <pre><code>transform:\n  - aggregate:\n      group_by: [department, region]\n      aggregations:\n        salary: sum\n        employee_id: count\n        age: avg\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#44-advanced-transformers","title":"4.4 Advanced Transformers","text":"Transformer Purpose Key Parameters <code>deduplicate</code> Keep first/last per key <code>keys</code>, <code>order_by</code> <code>explode_list_column</code> Flatten arrays <code>column</code>, <code>outer: bool</code> <code>dict_based_mapping</code> Value replacement <code>column</code>, <code>mapping</code>, <code>default</code>, <code>output_column</code> <code>hash_columns</code> Generate hash <code>columns</code>, <code>output_col</code>, <code>algorithm</code> <code>generate_surrogate_key</code> UUID/hash key <code>columns</code>, <code>output_col</code> <code>generate_numeric_key</code> Integer sequence <code>output_col</code>, <code>start</code> <code>window_calculation</code> Window functions <code>partition_by</code>, <code>order_by</code>, <code>calculations</code> <code>parse_json</code> Extract from JSON <code>column</code>, <code>schema</code> <code>normalize_json</code> Flatten nested JSON <code>column</code>, <code>max_level</code> <code>regex_replace</code> Regex substitution <code>column</code>, <code>pattern</code>, <code>replacement</code> <code>unpack_struct</code> Flatten struct columns <code>column</code>, <code>prefix</code> <code>sessionize</code> Session detection <code>user_col</code>, <code>timestamp_col</code>, <code>timeout</code> <code>geocode</code> Geocode addresses <code>address_col</code>, <code>lat_col</code>, <code>lon_col</code> <code>validate_and_flag</code> Flag invalid rows <code>validations</code>, <code>flag_col</code> <code>split_events_by_period</code> Split time-spanning events <code>start_col</code>, <code>end_col</code>, <code>period</code>, <code>shifts</code> <p>Deduplicate Example: <pre><code>transform:\n  - deduplicate:\n      keys: [customer_id]\n      order_by: \"updated_at DESC\"  # Keep latest\n</code></pre></p> <p>Window Calculation Example: <pre><code>transform:\n  - window_calculation:\n      partition_by: [customer_id]\n      order_by: [order_date]\n      calculations:\n        - function: row_number\n          output_col: order_seq\n        - function: sum\n          column: amount\n          output_col: running_total\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#45-delete-detection-transformer","title":"4.5 Delete Detection Transformer","text":"<p>Purpose: CDC-like behavior for sources without native change capture.</p> <p>Class: <code>DeleteDetectionConfig</code> in <code>odibi/config.py</code></p> Parameter Type Required Description <code>mode</code> str Yes <code>none</code>, <code>snapshot_diff</code>, <code>sql_compare</code> <code>keys</code> list Yes Primary key columns <code>soft_delete_col</code> str No Add boolean flag (default: <code>_is_deleted</code>) <code>max_delete_percent</code> float No Safety threshold (default: 50.0) <code>on_threshold_breach</code> str No <code>warn</code>, <code>error</code>, <code>skip</code> <code>on_first_run</code> str No <code>skip</code>, <code>error</code> <p>Modes: | Mode | When to Use | |------|-------------| | <code>none</code> | Append-only facts, no delete tracking needed | | <code>snapshot_diff</code> | Full snapshot sources (compares Delta version N vs N-1) | | <code>sql_compare</code> | HWM incremental ingestion (queries live source) |</p> <p>Example (SQL Compare - Recommended): <pre><code>transformer: detect_deletes\nparams:\n  mode: sql_compare\n  keys: [customer_id]\n  source_connection: azure_sql\n  source_table: dbo.Customers\n  soft_delete_col: _is_deleted\n  max_delete_percent: 10.0\n  on_threshold_breach: error\n</code></pre></p> <p>Example (Snapshot Diff): <pre><code>transformer: detect_deletes\nparams:\n  mode: snapshot_diff\n  keys: [product_id]\n  connection: silver_conn\n  path: silver/products\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#5-validation-quarantine","title":"5. Validation &amp; Quarantine","text":""},{"location":"ODIBI_DEEP_CONTEXT/#51-validation-tests","title":"5.1 Validation Tests","text":"<p>Define data quality tests in the <code>validate:</code> block:</p> <pre><code>validate:\n  tests:\n    - type: not_null\n      columns: [customer_id, order_id]\n      on_fail: fail  # fail, warn, quarantine\n\n    - type: unique\n      columns: [order_id]\n      on_fail: fail\n\n    - type: accepted_values\n      column: status\n      values: [pending, shipped, delivered, cancelled]\n      on_fail: warn\n\n    - type: range\n      column: quantity\n      min: 1\n      max: 10000\n      on_fail: quarantine\n\n    - type: regex_match\n      column: email\n      pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n      on_fail: warn\n\n    - type: freshness\n      column: updated_at\n      max_age: \"24h\"  # or \"7d\", \"30m\"\n      on_fail: fail\n\n    - type: row_count\n      min: 1\n      max: 10000000\n      on_fail: fail\n\n    - type: custom_sql\n      name: positive_amounts\n      condition: \"amount &gt; 0\"\n      on_fail: quarantine\n</code></pre> <p>Test Types: | Type | Purpose | Parameters | |------|---------|------------| | <code>not_null</code> | Check for nulls | <code>columns: list</code> | | <code>unique</code> | Check uniqueness | <code>columns: list</code> | | <code>accepted_values</code> | Check allowed values | <code>column</code>, <code>values: list</code> | | <code>range</code> | Check numeric range | <code>column</code>, <code>min</code>, <code>max</code> | | <code>regex_match</code> | Check pattern | <code>column</code>, <code>pattern</code> | | <code>freshness</code> | Check data age | <code>column</code>, <code>max_age</code> | | <code>row_count</code> | Check row count | <code>min</code>, <code>max</code> | | <code>schema</code> | Check columns exist | <code>strict: bool</code> | | <code>custom_sql</code> | Custom condition | <code>condition</code>, <code>name</code> |</p> <p>on_fail Options: | Value | Behavior | |-------|----------| | <code>fail</code> | Stop pipeline with error | | <code>warn</code> | Log warning, continue | | <code>quarantine</code> | Route failing rows to quarantine table |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#52-quarantine-configuration","title":"5.2 Quarantine Configuration","text":"<p>Route failing rows to a separate table:</p> <pre><code>validate:\n  tests:\n    - type: range\n      column: quantity\n      min: 1\n      on_fail: quarantine\n\n  quarantine:\n    connection: silver\n    path: quarantine/orders\n    # OR: table: silver.orders_quarantine\n    add_columns:\n      rejection_reason: true      # \"_rejection_reason\" column\n      rejected_at: true           # \"_rejected_at\" timestamp\n      source_batch_id: true       # \"_source_batch_id\" run ID\n      failed_tests: true          # \"_failed_tests\" list\n      original_node: true         # \"_original_node\" source node\n    max_rows: 10000              # Limit quarantine rows (optional)\n    sample_fraction: 0.1         # Sample quarantine rows (optional)\n</code></pre> <p>Quarantine Output Schema: Original columns plus: - <code>_rejection_reason</code>: Text description of failure - <code>_rejected_at</code>: ISO timestamp - <code>_source_batch_id</code>: Pipeline run ID - <code>_failed_tests</code>: Comma-separated test names - <code>_original_node</code>: Node name that failed validation</p>"},{"location":"ODIBI_DEEP_CONTEXT/#53-quality-gates","title":"5.3 Quality Gates","text":"<p>Batch-level validation that can block the entire write:</p> <pre><code>validate:\n  gate:\n    require_pass_rate: 0.95  # 95% of rows must pass\n    thresholds:\n      - test: not_null\n        min_pass_rate: 0.99\n      - test: accepted_values\n        min_pass_rate: 0.90\n    row_count:\n      min: 100\n      max: 1000000\n      change_threshold: 0.5  # Alert if &gt;50% change from previous run\n    on_fail: abort  # abort, warn, quarantine_all\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#6-connections-reference","title":"6. Connections Reference","text":""},{"location":"ODIBI_DEEP_CONTEXT/#61-local-connection","title":"6.1 Local Connection","text":"<p>Class: <code>LocalConnection</code> in <code>odibi/connections/local.py</code></p> <pre><code>connections:\n  local_data:\n    type: local\n    base_path: ./data/bronze\n</code></pre> Parameter Type Required Default Description <code>base_path</code> str No <code>./data</code> Root directory <p>Supports: Local paths, <code>file://</code>, <code>dbfs:/</code> URIs</p> <p>Path Resolution: <pre><code>conn.get_path(\"silver/customers\")\n# Local: /absolute/path/to/data/bronze/silver/customers\n# URI: dbfs:/mnt/data/silver/customers\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#62-azure-adls-connection","title":"6.2 Azure ADLS Connection","text":"<p>Class: <code>AzureADLS</code> in <code>odibi/connections/azure_adls.py</code></p> <pre><code>connections:\n  adls_prod:\n    type: azure_adls\n    account: mystorageaccount\n    container: datalake\n    path_prefix: oee/prod\n    auth_mode: key_vault\n    key_vault_name: my-keyvault\n    secret_name: storage-key\n</code></pre> Parameter Type Required Description <code>account</code> str Yes Storage account name <code>container</code> str Yes Container/filesystem name <code>path_prefix</code> str No Prefix for all paths <code>auth_mode</code> str Yes Authentication mode (see below) <p>Auth Modes:</p> Mode Required Parameters Use Case <code>key_vault</code> <code>key_vault_name</code>, <code>secret_name</code> Production (recommended) <code>direct_key</code> <code>account_key</code> Development only <code>sas_token</code> <code>sas_token</code> Limited access scenarios <code>service_principal</code> <code>tenant_id</code>, <code>client_id</code>, <code>client_secret</code> Automation/CI <code>managed_identity</code> (none) Databricks/Azure VMs <p>Path Resolution: <pre><code>conn.get_path(\"silver/customers\")\n# Returns: abfss://datalake@mystorageaccount.dfs.core.windows.net/oee/prod/silver/customers\n</code></pre></p> <p>Spark Configuration: <pre><code>conn.configure_spark(spark)  # Sets fs.azure.* configs automatically\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#63-azure-sql-connection","title":"6.3 Azure SQL Connection","text":"<p>Class: <code>AzureSQL</code> in <code>odibi/connections/azure_sql.py</code></p> <pre><code>connections:\n  sql_prod:\n    type: azure_sql\n    server: myserver.database.windows.net\n    database: analytics\n    auth_mode: aad_msi\n</code></pre> Parameter Type Required Description <code>server</code> str Yes SQL server hostname <code>database</code> str Yes Database name <code>driver</code> str No Default: <code>ODBC Driver 18 for SQL Server</code> <code>auth_mode</code> str Yes <code>aad_msi</code>, <code>sql</code>, <code>key_vault</code> <code>username</code> str For <code>sql</code> SQL username <code>password</code> str For <code>sql</code> SQL password <code>port</code> int No Default: 1433 <p>Methods: <pre><code># Read data\ndf = conn.read_query(\"SELECT * FROM dbo.Customers\")\ndf = conn.read_table(\"Customers\", schema=\"dbo\")\n\n# Write data\nconn.write_table(df, \"Customers\", schema=\"dbo\", if_exists=\"replace\")\n\n# Execute statements\nconn.execute(\"TRUNCATE TABLE dbo.Staging\")\n\n# Spark JDBC options\noptions = conn.get_spark_options()\nspark.read.format(\"jdbc\").options(**options).option(\"dbtable\", \"dbo.Customers\").load()\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#64-http-connection","title":"6.4 HTTP Connection","text":"<p>Class: <code>HTTPConnection</code> in <code>odibi/connections/http.py</code></p> <pre><code>connections:\n  api:\n    type: http\n    base_url: https://api.example.com\n    headers:\n      User-Agent: odibi-pipeline\n    auth:\n      mode: bearer\n      token: ${API_TOKEN}\n</code></pre> <p>Auth Modes: <code>none</code>, <code>basic</code>, <code>bearer</code>, <code>api_key</code></p>"},{"location":"ODIBI_DEEP_CONTEXT/#65-local-dbfs-connection","title":"6.5 Local DBFS Connection","text":"<p>For Databricks DBFS paths:</p> <pre><code>connections:\n  dbfs:\n    type: local_dbfs\n    base_path: /dbfs/mnt/datalake\n</code></pre> <p>Supports: <code>dbfs:/</code>, <code>/dbfs/</code>, and mounted paths.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#7-write-configuration-delta-lake-features","title":"7. Write Configuration (Delta Lake Features)","text":""},{"location":"ODIBI_DEEP_CONTEXT/#71-partitioning-z-ordering","title":"7.1 Partitioning &amp; Z-Ordering","text":"<p>For large tables, use partitioning and Z-ordering to optimize query performance:</p> <pre><code>write:\n  connection: gold_lake\n  format: delta\n  table: fact_sales\n  mode: append\n\n  # Partitioning: Physical folders (low-cardinality columns)\n  partition_by: [country_code, txn_year_month]\n\n  # Z-Ordering: Data clustering (high-cardinality columns)\n  zorder_by: [customer_id, product_id]\n\n  # Delta table properties\n  table_properties:\n    \"delta.autoOptimize.optimizeWrite\": \"true\"\n    \"delta.autoOptimize.autoCompact\": \"true\"\n</code></pre> Feature When to Use Cardinality <code>partition_by</code> Columns in WHERE clauses Low (country, year_month) <code>zorder_by</code> Columns in JOINs/filters High (customer_id, product_id)"},{"location":"ODIBI_DEEP_CONTEXT/#72-schema-evolution","title":"7.2 Schema Evolution","text":"<p>Allow adding new columns without breaking existing pipelines:</p> <pre><code>write:\n  connection: silver\n  path: customers\n  format: delta\n  mode: overwrite\n  merge_schema: true  # Enable schema evolution\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#73-auto-optimize-vacuum-optimize","title":"7.3 Auto-Optimize (VACUUM &amp; OPTIMIZE)","text":"<p>Automatically run Delta Lake maintenance after writes:</p> <pre><code>write:\n  connection: silver\n  path: customers\n  format: delta\n  auto_optimize: true  # Use defaults (168 hours retention)\n\n# OR with custom config:\nwrite:\n  connection: silver\n  path: customers\n  format: delta\n  auto_optimize:\n    enabled: true\n    vacuum_retention_hours: 168  # 7 days (set to 0 to disable VACUUM)\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#74-bronze-metadata-columns","title":"7.4 Bronze Metadata Columns","text":"<p>Add lineage tracking columns during ingestion:</p> <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # Adds all applicable columns\n\n# OR selective:\nwrite:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true        # _extracted_at: pipeline timestamp\n    source_file: true         # _source_file: filename (file sources)\n    source_connection: false  # _source_connection: connection name\n    source_table: false       # _source_table: table name (SQL sources)\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#75-skip-if-unchanged","title":"7.5 Skip If Unchanged","text":"<p>Avoid redundant writes for snapshot tables:</p> <pre><code>write:\n  connection: silver\n  path: reference_data\n  format: delta\n  mode: overwrite\n  skip_if_unchanged: true           # Compares SHA256 hash of content\n  skip_hash_columns: [id, name]     # Only hash these columns\n  skip_hash_sort_columns: [id]      # Sort before hashing for determinism\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#76-time-travel-read","title":"7.6 Time Travel (Read)","text":"<p>Read historical versions of Delta tables:</p> <pre><code>read:\n  connection: silver\n  path: customers\n  format: delta\n  time_travel:\n    as_of_version: 10\n    # OR\n    as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#77-streaming-writes-spark-structured-streaming","title":"7.7 Streaming Writes (Spark Structured Streaming)","text":"<p>Process data continuously with fault tolerance:</p> <pre><code>write:\n  connection: silver_lake\n  format: delta\n  table: events_stream\n  streaming:\n    output_mode: append          # append, update, complete\n    checkpoint_location: /checkpoints/events_stream\n    trigger:\n      processing_time: \"10 seconds\"  # Micro-batch interval\n      # OR: available_now: true      # Process all, then stop\n    query_name: events_ingestion     # For monitoring\n    await_termination: false\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#8-alerts-notifications","title":"8. Alerts &amp; Notifications","text":""},{"location":"ODIBI_DEEP_CONTEXT/#81-alert-configuration","title":"8.1 Alert Configuration","text":"<p>Send notifications to Slack, Teams, or webhooks on pipeline events:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15  # Don't spam\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#82-available-events","title":"8.2 Available Events","text":"Event Trigger <code>on_start</code> Pipeline started <code>on_success</code> Pipeline completed successfully <code>on_failure</code> Pipeline failed <code>on_quarantine</code> Rows were quarantined <code>on_gate_block</code> Quality gate blocked pipeline <code>on_threshold_breach</code> A threshold was exceeded"},{"location":"ODIBI_DEEP_CONTEXT/#83-alert-types","title":"8.3 Alert Types","text":"Type Use Case <code>slack</code> Slack webhook <code>teams</code> Microsoft Teams webhook <code>webhook</code> Generic HTTP POST"},{"location":"ODIBI_DEEP_CONTEXT/#9-system-catalog-the-brain","title":"9. System Catalog (The Brain)","text":"<p>The System Catalog stores metadata about pipelines, runs, schemas, and lineage in Delta tables.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#91-configuration","title":"9.1 Configuration","text":"<pre><code>system:\n  connection: adls_bronze        # Must be blob storage (supports Delta)\n  path: _odibi_system\n  environment: dev               # Tag for multi-environment queries\n\n  # Optional: Sync to SQL Server for dashboards\n  sync_to:\n    connection: sql_server_prod\n    schema_name: odibi_system\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#92-meta-tables","title":"9.2 Meta Tables","text":"Table Purpose <code>meta_tables</code> Table schemas and column info <code>meta_runs</code> Pipeline execution history <code>meta_patterns</code> Pattern configurations used <code>meta_metrics</code> Metric definitions and values <code>meta_state</code> High Water Mark and incremental state <code>meta_pipelines</code> Pipeline definitions <code>meta_nodes</code> Node configurations <code>meta_schemas</code> Schema versions <code>meta_lineage</code> Data lineage relationships <code>meta_outputs</code> Output table metadata <code>meta_pipeline_runs</code> Summary of pipeline runs <code>meta_node_runs</code> Summary of node runs <code>meta_failures</code> Failed runs with error details <code>meta_daily_stats</code> Daily execution statistics <code>meta_pipeline_health</code> Pipeline health scores <code>meta_sla_status</code> SLA compliance tracking"},{"location":"ODIBI_DEEP_CONTEXT/#93-cost-tracking","title":"9.3 Cost Tracking","text":"<pre><code>system:\n  connection: adls_prod\n  cost_per_compute_hour: 0.15    # Estimated $/hour\n  databricks_billing_enabled: true  # Query actual DBR costs\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#10-openlineage-integration","title":"10. OpenLineage Integration","text":"<p>Track data lineage to external systems (Marquez, Atlan, DataHub):</p> <pre><code>lineage:\n  url: \"http://localhost:5000\"    # OpenLineage API endpoint\n  namespace: \"my_project\"\n  api_key: \"${OPENLINEAGE_API_KEY}\"\n</code></pre> <p>Events emitted: - Pipeline start/complete/fail - Node inputs/outputs with schemas - Parent-child run relationships</p>"},{"location":"ODIBI_DEEP_CONTEXT/#11-foreign-key-validation","title":"11. Foreign Key Validation","text":"<p>Validate referential integrity between fact and dimension tables:</p> <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error  # warn, error, quarantine\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n</code></pre> <p>Behavior: - Validates FK columns exist in dimension tables - Detects orphan records (FK not in dimension) - Integrates with FactPattern automatically</p>"},{"location":"ODIBI_DEEP_CONTEXT/#12-orchestration-export","title":"12. Orchestration Export","text":"<p>Generate Airflow or Dagster DAGs from odibi pipelines:</p>"},{"location":"ODIBI_DEEP_CONTEXT/#121-airflow-export","title":"12.1 Airflow Export","text":"<pre><code>odibi export airflow --pipeline my_pipeline --output dags/\n</code></pre> <p>Generates a Python DAG file with: - BashOperator tasks for each node - Task dependencies from <code>depends_on</code> - Retry configuration</p>"},{"location":"ODIBI_DEEP_CONTEXT/#122-dagster-export","title":"12.2 Dagster Export","text":"<pre><code>odibi export dagster --pipeline my_pipeline --output ops/\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#13-manufacturing-transformers","title":"13. Manufacturing Transformers","text":"<p>Specialized transformers for process/batch manufacturing data:</p>"},{"location":"ODIBI_DEEP_CONTEXT/#131-detect-sequential-phases","title":"13.1 Detect Sequential Phases","text":"<p>Analyze batch process phases from PLC timer columns:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by: BatchID           # Or [BatchID, AssetID]\n      timestamp_col: ts\n      phases:\n        - timer_col: LoadTime\n        - timer_col: CookTime\n        - timer_col: CoolTime\n        - timer_col: UnloadTime\n      start_threshold: 240        # Max seconds to consider valid start\n      status_col: Status\n      status_mapping:\n        1: idle\n        2: active\n        3: hold\n        4: faulted\n      phase_metrics:\n        Level: max               # Aggregate column during phase\n      metadata:\n        ProductCode: first_after_start\n        Weight: max\n</code></pre> <p>Use cases: - Batch reactor cycle analysis - CIP (Clean-in-Place) phase timing - Food processing cycle tracking - Any multi-step batch process with PLC timers</p> <p>Output columns: - <code>phase_name</code>, <code>phase_start</code>, <code>phase_end</code>, <code>phase_duration_seconds</code> - Status time breakdown: <code>time_in_idle</code>, <code>time_in_active</code>, etc. - Aggregated metrics per phase</p>"},{"location":"ODIBI_DEEP_CONTEXT/#14-semantic-layer","title":"14. Semantic Layer","text":"<p>The semantic layer provides a metrics abstraction for BI consumption.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#141-metric-definitions","title":"14.1 Metric Definitions","text":"<pre><code># metrics.yaml\nmetrics:\n  - name: total_revenue\n    description: Total order revenue\n    expr: \"SUM(total_amount)\"\n    source: $build_warehouse.fact_orders\n\n  - name: order_count\n    description: Number of orders\n    expr: \"COUNT(*)\"\n    source: $build_warehouse.fact_orders\n\n  - name: avg_order_value\n    description: Average order value\n    type: derived\n    components: [total_revenue, order_count]\n    formula: \"total_revenue / order_count\"\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#142-dimension-definitions","title":"14.2 Dimension Definitions","text":"<pre><code>dimensions:\n  - name: order_date\n    source: $build_warehouse.fact_orders\n    column: order_date\n    grain: month  # day, week, month, quarter, year\n\n  - name: customer_region\n    source: $build_warehouse.dim_customer\n    column: region\n    hierarchy: [country, region, city]  # Drill-down path\n\n  - name: fiscal_year\n    source: $build_warehouse.dim_date\n    expr: \"YEAR(DATEADD(month, 6, date_key))\"  # Custom expression\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#143-semantic-queries","title":"14.3 Semantic Queries","text":"<p>Query metrics using natural syntax:</p> <pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\n\nconfig = parse_semantic_config(yaml.safe_load(open(\"metrics.yaml\")))\nquery = SemanticQuery(config)\n\n# Execute query\nresult = query.execute(\n    \"total_revenue, order_count BY customer_region, order_date WHERE status = 'completed'\",\n    context\n)\n\nprint(result.df)  # DataFrame with aggregated metrics\n</code></pre> <p>Query Syntax: <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#144-materialization","title":"14.4 Materialization","text":"<p>Pre-compute metrics for performance:</p> <pre><code>materializations:\n  - name: daily_sales_by_region\n    metrics: [total_revenue, order_count]\n    dimensions: [order_date, customer_region]\n    output: gold.daily_sales_by_region\n    schedule: \"0 5 * * *\"  # Daily at 5am\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#145-sql-server-views","title":"14.5 SQL Server Views","text":"<p>Generate views for analyst consumption:</p> <pre><code>views:\n  - name: vw_daily_sales\n    description: Daily sales metrics\n    metrics: [total_revenue, order_count, avg_order_value]\n    dimensions: [order_date]\n    db_schema: semantic\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#15-story-generation","title":"15. Story Generation","text":"<p>Stories are execution reports generated after each pipeline run.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#151-configuration","title":"15.1 Configuration","text":"<pre><code>story:\n  enabled: true\n  output_path: stories/  # Local or abfss:// path\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#152-story-contents","title":"15.2 Story Contents","text":"<p>Each story includes: - Pipeline metadata: Name, start/end time, duration - Node details: Status, row counts, duration, schema - Validation results: Tests passed/failed - Data samples: First N rows of output - Lineage: Input/output relationships - DAG visualization: Interactive dependency graph</p>"},{"location":"ODIBI_DEEP_CONTEXT/#153-viewing-stories","title":"15.3 Viewing Stories","text":"<pre><code># View last story\nodibi story last\n\n# View specific story\nodibi story show stories/my_pipeline_2024-01-15_10-30-00.html\n\n# View specific node\nodibi story last --node dim_customers\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#154-story-metadata","title":"15.4 Story Metadata","text":"<p>Available in <code>PipelineResults.story_path</code>:</p> <pre><code>results = pm.run(\"my_pipeline\")\nprint(f\"Story: {results.story_path}\")\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#155-documentation-generation","title":"15.5 Documentation Generation","text":"<p>Generate structured markdown documentation from Story artifacts:</p> <pre><code>story:\n  connection: local_data\n  path: stories/\n\n  docs:\n    enabled: true\n    output_path: docs/generated/   # Project-level docs location\n\n    outputs:\n      readme: true              # README.md - stakeholder-facing\n      technical_details: true   # TECHNICAL_DETAILS.md - engineer-facing\n      node_cards: true          # NODE_CARDS/*.md - per-node details\n      run_memo: true            # Per-run summary (always generated)\n\n    include:\n      sql: true                 # Include executed SQL in node cards\n      config_snapshot: true     # Include YAML config snapshots\n      schema: true              # Include schema tables\n</code></pre> <p>Note: When using remote storage (ADLS, S3, etc.), documentation files are written to the same storage location as stories. The <code>output_path</code> is relative to the story connection's base path. Use Azure Storage Explorer, AWS Console, or the appropriate tool to browse generated docs.</p> <p>Generated Artifacts:</p> Artifact Scope Update Policy README.md Project On successful runs only TECHNICAL_DETAILS.md Project On successful runs only NODE_CARDS/*.md Project On successful runs only run_*_memo.md Per-run Every run (including failures) <p>File Structure: <pre><code>project/\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 generated/\n\u2502       \u251c\u2500\u2500 README.md              # Pipeline overview\n\u2502       \u251c\u2500\u2500 TECHNICAL_DETAILS.md   # Full execution details\n\u2502       \u2514\u2500\u2500 node_cards/\n\u2502           \u251c\u2500\u2500 load_customers.md  # Per-node documentation\n\u2502           \u2514\u2500\u2500 dim_customer.md\n\u2514\u2500\u2500 stories/\n    \u2514\u2500\u2500 my_pipeline/\n        \u2514\u2500\u2500 2026-01-21/\n            \u251c\u2500\u2500 run_14-30-00.html   # Interactive story\n            \u251c\u2500\u2500 run_14-30-00.json   # Machine-readable\n            \u2514\u2500\u2500 run_14-30-00_memo.md  # Run summary\n</code></pre></p> <p>README.md includes: - Pipeline name and layer badge - Last run status and metrics - Project context (project, plant, asset) - Node summary table with links to NODE_CARDS - Links to TECHNICAL_DETAILS and Story HTML</p> <p>NODE_CARDS include: - Node description (from config) - Operation type and duration - Schema in/out with changes highlighted - Executed SQL (syntax highlighted) - Transformation stack - Validation warnings - Config snapshot (YAML)</p> <p>RUN_MEMO includes: - Run status and timestamp - What changed from last successful run - Anomalies detected - Failed node details with error messages - Data quality issues</p>"},{"location":"ODIBI_DEEP_CONTEXT/#16-common-workflows","title":"16. Common Workflows","text":""},{"location":"ODIBI_DEEP_CONTEXT/#161-run-a-pipeline-and-query-intermediate-results-spark","title":"16.1 Run a Pipeline and Query Intermediate Results (Spark)","text":"<pre><code>from odibi import PipelineManager\n\n# Initialize\npm = PipelineManager(\"project.yaml\")\n\n# Run pipeline\nresults = pm.run(\"silver_pipeline\")\n\n# Query any node's output via Spark SQL\nspark = pm.engine.spark\ncustomers_df = spark.sql(\"SELECT * FROM dim_customers WHERE tier = 'Gold'\")\ncustomers_df.show()\n\n# Or via context\ndf = pm.context.get(\"dim_customers\")\n\n# List all available nodes\nprint(pm.context.list_names())\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#162-debug-a-failing-node","title":"16.2 Debug a Failing Node","text":"<pre><code># 1. Run with verbose logging\nodibi run pipeline.yaml --log-level DEBUG\n\n# 2. View the execution story\nodibi story last\n\n# 3. Inspect specific node\nodibi story last --node failing_node_name\n\n# 4. Run single node in isolation\nodibi run pipeline.yaml --node failing_node_name --dry-run\n</code></pre> <p>In Python: <pre><code>pm = PipelineManager(\"project.yaml\")\nresult = pm.run(\"pipeline\")\n\n# Get specific node result\nnode_result = result.get_node_result(\"failing_node\")\nprint(node_result.error)  # Full error message\nprint(node_result.metadata.get(\"input_schema\"))  # Schema at input\nprint(node_result.metadata.get(\"rows_read\"))  # Row count at input\n\n# Debug summary with next steps\nprint(result.debug_summary())\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#163-add-a-new-scd2-dimension","title":"16.3 Add a New SCD2 Dimension","text":"<p>Step 1: Create bronze node (raw data) <pre><code>nodes:\n  - name: bronze_customers\n    read:\n      connection: source_db\n      table: customers\n</code></pre></p> <p>Step 2: Create silver node (SCD2) <pre><code>  - name: dim_customers\n    depends_on: [bronze_customers]\n    inputs:\n      source: bronze_customers\n    transform:\n      - scd2:\n          connection: adls_prod\n          path: silver/dim_customers\n          keys: [customer_id]\n          track_cols: [name, email, address, tier]\n          effective_time_col: updated_at\n    write:\n      connection: adls_prod\n      path: silver/dim_customers\n      format: delta\n      mode: overwrite  # SCD2 returns full history!\n</code></pre></p> <p>Step 3: Verify <pre><code>spark.sql(\"\"\"\n  SELECT customer_id, name, valid_to, is_current\n  FROM dim_customers\n  WHERE customer_id = 123\n  ORDER BY valid_to\n\"\"\").show()\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#164-create-a-custom-transformer","title":"16.4 Create a Custom Transformer","text":"<p>Step 1: Create the function <pre><code># my_transformers.py\nfrom pydantic import BaseModel, Field\nfrom odibi.context import EngineContext\n\nclass MyTransformParams(BaseModel):\n    input_col: str = Field(..., description=\"Column to transform\")\n    output_col: str = Field(..., description=\"Output column name\")\n\ndef my_custom_transform(context: EngineContext, params: MyTransformParams) -&gt; EngineContext:\n    sql = f\"SELECT *, UPPER({params.input_col}) AS {params.output_col} FROM df\"\n    return context.sql(sql)\n</code></pre></p> <p>Step 2: Register it <pre><code>from odibi.registry import FunctionRegistry\nfrom my_transformers import my_custom_transform, MyTransformParams\n\nFunctionRegistry.register(my_custom_transform, \"my_custom_transform\", MyTransformParams)\n</code></pre></p> <p>Step 3: Use in YAML <pre><code>transform:\n  - my_custom_transform:\n      input_col: name\n      output_col: name_upper\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#165-run-in-databricks","title":"16.5 Run in Databricks","text":"<pre><code># Databricks notebook cell\n\n# Install odibi\n%pip install odibi[azure]\n\n# Get SparkSession\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Configure pipeline with Spark engine\nfrom odibi import PipelineManager\n\npm = PipelineManager(\n    \"project.yaml\",\n    engine=\"spark\",\n    spark_session=spark\n)\n\n# Run\nresults = pm.run(\"my_pipeline\")\n\n# Query results as temp views\nspark.sql(\"SELECT * FROM node_name\").display()\n\n# Or write to Unity Catalog\nspark.sql(\"CREATE TABLE catalog.schema.table AS SELECT * FROM node_name\")\n</code></pre> <p>Key Differences from Local: - Use <code>engine=\"spark\"</code> - Pass existing <code>spark_session</code> - ADLS connections use <code>managed_identity</code> auth - Node outputs are temp views queryable via <code>spark.sql()</code></p>"},{"location":"ODIBI_DEEP_CONTEXT/#166-incremental-ingestion-with-high-water-mark","title":"16.6 Incremental Ingestion with High Water Mark","text":"<pre><code>nodes:\n  - name: bronze_orders\n    read:\n      connection: source_db\n      table: orders\n    incremental:\n      mode: hwm              # High Water Mark\n      column: updated_at\n      state_key: orders_hwm  # Persisted in state store\n    write:\n      connection: bronze\n      path: orders\n      mode: append\n</code></pre> <p>First Run: Loads all data, stores max(updated_at) as HWM Subsequent Runs: Filters <code>WHERE updated_at &gt; {hwm}</code>, appends new rows</p>"},{"location":"ODIBI_DEEP_CONTEXT/#17-cli-reference","title":"17. CLI Reference","text":""},{"location":"ODIBI_DEEP_CONTEXT/#171-core-commands","title":"17.1 Core Commands","text":"<pre><code># Run a pipeline\nodibi run pipeline.yaml\n\n# Run specific nodes\nodibi run pipeline.yaml --node node_a --node node_b\n\n# Filter by tag\nodibi run pipeline.yaml --tag silver\n\n# Dry run (validate without execution)\nodibi run pipeline.yaml --dry-run\n\n# Resume from failure\nodibi run pipeline.yaml --resume\n\n# Parallel execution\nodibi run pipeline.yaml --parallel --workers 4\n\n# Error handling\nodibi run pipeline.yaml --on-error fail_later\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#172-story-commands","title":"17.2 Story Commands","text":"<pre><code>odibi story last                    # View last execution\nodibi story show path/to/story.html # View specific story\nodibi story last --node node_name   # View specific node\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#173-validation-debug","title":"17.3 Validation &amp; Debug","text":"<pre><code>odibi validate pipeline.yaml  # Validate configuration\nodibi graph pipeline.yaml     # Show dependency graph\nodibi graph pipeline.yaml --format ascii\nodibi doctor                  # Check environment\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#174-introspection-commands-ai-friendly","title":"17.4 Introspection Commands (AI-Friendly)","text":"<p>These commands help AI tools discover available features programmatically:</p> <pre><code># List all available transformers\nodibi list transformers\nodibi list transformers --format json   # JSON output for parsing\n\n# List all available patterns\nodibi list patterns\nodibi list patterns --format json\n\n# List all connection types\nodibi list connections\nodibi list connections --format json\n\n# Get detailed documentation for any feature\nodibi explain fill_nulls      # Explain a transformer\nodibi explain dimension       # Explain a pattern\nodibi explain azure_sql       # Explain a connection type\n</code></pre> <p>AI Workflow Example: <pre><code># AI checks what's available before generating config\nodibi list transformers --format json | jq '.[] | .name'\n\n# AI looks up specific transformer usage\nodibi explain derive_columns\n\n# AI validates generated config\nodibi validate generated_pipeline.yaml\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#175-semantic-layer","title":"17.5 Semantic Layer","text":"<pre><code>odibi semantic run metrics.yaml     # Execute semantic layer\nodibi semantic materialize metrics.yaml\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#18-anti-patterns-and-gotchas","title":"18. Anti-Patterns and Gotchas","text":""},{"location":"ODIBI_DEEP_CONTEXT/#181-node-naming","title":"18.1 Node Naming","text":"<p>\u274c Wrong: <pre><code>nodes:\n  - name: dim-customers      # Hyphens break Spark SQL\n  - name: fact sales         # Spaces break Spark SQL\n  - name: customers.silver   # Dots break Spark SQL\n</code></pre></p> <p>\u2705 Correct: <pre><code>nodes:\n  - name: dim_customers\n  - name: fact_sales\n  - name: customers_silver\n</code></pre></p> <p>Error Message: \"Invalid node name 'X' for Spark engine. Names must contain only alphanumeric characters and underscores\"</p>"},{"location":"ODIBI_DEEP_CONTEXT/#182-scd2-mode-must-be-overwrite","title":"18.2 SCD2 Mode Must Be Overwrite","text":"<p>\u274c Wrong: <pre><code>transformer: scd2\nparams:\n  target: dim_customers\n  ...\nwrite:\n  mode: append  # WRONG - duplicates history!\n</code></pre></p> <p>\u2705 Correct: <pre><code>transformer: scd2\nparams:\n  target: dim_customers\n  ...\nwrite:\n  mode: overwrite  # SCD2 returns FULL history\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#183-missing-depends_on-for-joins","title":"18.3 Missing depends_on for Joins","text":"<p>\u274c Wrong: <pre><code>- name: fact_sales\n  transform:\n    - join:\n        right_dataset: dim_customer  # Not in depends_on!\n</code></pre></p> <p>\u2705 Correct: <pre><code>- name: fact_sales\n  depends_on: [dim_customer]  # Required!\n  transform:\n    - join:\n        right_dataset: dim_customer\n</code></pre></p> <p>Error Message: \"Join failed: dataset 'dim_customer' not found in context\"</p>"},{"location":"ODIBI_DEEP_CONTEXT/#184-engine-specific-sql-syntax","title":"18.4 Engine-Specific SQL Syntax","text":"Feature Pandas (DuckDB) Spark Exclude columns <code>SELECT * EXCLUDE (col)</code> <code>SELECT * EXCEPT (col)</code> String contains <code>ILIKE '%pattern%'</code> <code>LIKE '%pattern%'</code> Array explode <code>UNNEST(arr)</code> <code>explode(arr)</code> Null-safe compare <code>IS DISTINCT FROM</code> <code>IS DISTINCT FROM</code> (both support) <p>Transformers handle this automatically - prefer using transformers over raw SQL.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#185-performance-gotchas","title":"18.5 Performance Gotchas","text":"Issue Solution Pandas OOM on large data Use Spark engine or chunked processing Slow Spark startup Reuse SparkSession, use <code>--parallel</code> Repeated Delta reads Cache with <code>df.cache()</code> or persist Large quarantine tables Use <code>max_rows</code> and <code>sample_fraction</code>"},{"location":"ODIBI_DEEP_CONTEXT/#186-common-error-messages","title":"18.6 Common Error Messages","text":"Error Cause Fix \"DataFrame 'X' not found in context\" Missing <code>depends_on</code> Add to <code>depends_on</code> list \"Column 'X' not found\" Typo or missing column Check schema, run <code>--dry-run</code> \"SCD2: provide either 'target' OR both 'connection' and 'path'\" Invalid SCD2 config Use one approach, not both \"Key Vault fetch timed out\" Network/auth issue Check VPN, credentials"},{"location":"ODIBI_DEEP_CONTEXT/#19-sql-server-writer","title":"19. SQL Server Writer","text":""},{"location":"ODIBI_DEEP_CONTEXT/#191-merge-operations","title":"19.1 Merge Operations","text":"<p>Write to SQL Server with MERGE (upsert) semantics:</p> <pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: sales.fact_orders\n  mode: merge\n  merge_keys: [DateId, store_id]\n  merge_options:\n    update_condition: \"source._hash_diff != target._hash_diff\"\n    delete_condition: \"source._is_deleted = 1\"\n    insert_condition: \"source.is_valid = 1\"\n    exclude_columns: [_hash_diff]\n    staging_schema: staging\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n    auto_create_table: true\n    primary_key_on_merge_keys: true\n</code></pre> <p>Merge Options: | Option | Description | |--------|-------------| | <code>update_condition</code> | Only update rows matching this SQL condition | | <code>delete_condition</code> | Delete rows matching this condition (soft delete) | | <code>insert_condition</code> | Only insert rows matching this condition | | <code>exclude_columns</code> | Columns to exclude from target table | | <code>staging_schema</code> | Schema for staging table (default: <code>staging</code>) | | <code>auto_create_table</code> | Auto-create target if not exists | | <code>auto_create_schema</code> | Auto-create schema if not exists | | <code>batch_size</code> | Chunk large DataFrames for memory efficiency |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#192-overwrite-strategies","title":"19.2 Overwrite Strategies","text":"<pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: fact.combined_downtime\n  mode: overwrite\n  overwrite_options:\n    strategy: truncate_insert  # or: drop_create, delete_insert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre> Strategy Behavior Use Case <code>truncate_insert</code> TRUNCATE then INSERT Fastest, requires TRUNCATE permission <code>drop_create</code> DROP, CREATE, INSERT Refreshes schema <code>delete_insert</code> DELETE then INSERT Works with limited permissions"},{"location":"ODIBI_DEEP_CONTEXT/#20-incremental-loading-advanced","title":"20. Incremental Loading (Advanced)","text":""},{"location":"ODIBI_DEEP_CONTEXT/#201-stateful-high-water-mark","title":"20.1 Stateful High Water Mark","text":"<p>Track the last processed value for exact incremental loading:</p> <pre><code>read:\n  connection: source_db\n  table: orders\nincremental:\n  mode: stateful           # Track HWM in system catalog\n  column: updated_at\n  state_key: orders_hwm    # Unique ID for state tracking\n  watermark_lag: \"2h\"      # Safety buffer for late-arriving data\n</code></pre> <p>Watermark Lag: Subtracts this duration from stored HWM when filtering. Use when source has replication lag or eventual consistency.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#202-rolling-window","title":"20.2 Rolling Window","text":"<p>Load recent data without state tracking:</p> <pre><code>incremental:\n  mode: rolling_window\n  column: updated_at\n  lookback: 3\n  unit: day  # hour, day, month, year\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#203-date-format-handling","title":"20.3 Date Format Handling","text":"<p>For string columns with specific date formats:</p> <pre><code>incremental:\n  mode: rolling_window\n  column: EVENT_START\n  lookback: 3\n  unit: day\n  date_format: oracle  # DD-MON-YY format\n</code></pre> <p>Supported formats: | Format | Pattern | Use Case | |--------|---------|----------| | <code>oracle</code> | DD-MON-YY | Oracle databases | | <code>oracle_sqlserver</code> | DD-MON-YY in SQL Server | Legacy migrations | | <code>sql_server</code> | CONVERT style 120 | SQL Server | | <code>us</code> | MM/DD/YYYY | US format | | <code>eu</code> | DD/MM/YYYY | European format | | <code>iso</code> | YYYY-MM-DDTHH:MM:SS | ISO 8601 |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#21-diagnostics-diff-tools","title":"21. Diagnostics &amp; Diff Tools","text":""},{"location":"ODIBI_DEEP_CONTEXT/#211-delta-table-diff","title":"21.1 Delta Table Diff","text":"<p>Compare two versions of a Delta table:</p> <pre><code>from odibi.diagnostics.delta import get_delta_diff\n\nresult = get_delta_diff(\n    table_path=\"abfss://lake/silver/customers\",\n    version_a=10,\n    version_b=15,\n    spark=spark,\n    deep=True,           # Row-by-row comparison\n    keys=[\"customer_id\"] # For detecting updates\n)\n\nprint(f\"Rows changed: {result.rows_change}\")\nprint(f\"Schema added: {result.schema_added}\")\nprint(f\"Schema removed: {result.schema_removed}\")\nprint(f\"Operations: {result.operations}\")\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#212-run-diff","title":"21.2 Run Diff","text":"<p>Compare two pipeline runs to identify drift:</p> <pre><code>from odibi.diagnostics.diff import diff_nodes\n\nresult = diff_nodes(node_a_metadata, node_b_metadata)\n\nif result.has_drift:\n    print(f\"Status change: {result.status_change}\")\n    print(f\"Rows diff: {result.rows_diff}\")\n    print(f\"Columns added: {result.columns_added}\")\n    print(f\"Columns removed: {result.columns_removed}\")\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#22-cross-check-transformer","title":"22. Cross-Check Transformer","text":"<p>Validate data across multiple nodes:</p> <pre><code>transform:\n  - cross_check:\n      type: row_count_diff\n      inputs: [node_a, node_b]\n      threshold: 0.05  # Allow 5% difference\n\n  - cross_check:\n      type: schema_match\n      inputs: [staging_orders, prod_orders]\n</code></pre> <p>Check Types: | Type | Behavior | |------|----------| | <code>row_count_diff</code> | Compare row counts, fail if diff exceeds threshold | | <code>schema_match</code> | Ensure schemas are identical |</p>"},{"location":"ODIBI_DEEP_CONTEXT/#23-testing-utilities","title":"23. Testing Utilities","text":""},{"location":"ODIBI_DEEP_CONTEXT/#231-test-fixtures","title":"23.1 Test Fixtures","text":"<p>Generate sample data for tests:</p> <pre><code>from odibi.testing.fixtures import generate_sample_data, temp_directory\n\n# Generate sample DataFrame\ndf = generate_sample_data(\n    rows=100,\n    engine_type=\"pandas\",  # or \"spark\"\n    schema={\"id\": \"int\", \"value\": \"float\", \"category\": \"str\", \"date\": \"date\"}\n)\n\n# Temporary directory for test artifacts\nwith temp_directory() as temp_dir:\n    df.to_parquet(f\"{temp_dir}/test.parquet\")\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#232-assertions","title":"23.2 Assertions","text":"<p>Assert DataFrame equality:</p> <pre><code>from odibi.testing.assertions import assert_frame_equal, assert_schema_equal\n\n# Compare DataFrames (works with Pandas and Spark)\nassert_frame_equal(actual_df, expected_df, check_dtype=True)\n\n# Compare schemas only\nassert_schema_equal(actual_df, expected_df)\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#24-derived-updater-internal","title":"24. Derived Updater (Internal)","text":"<p>The DerivedUpdater provides exactly-once semantics for system catalog updates:</p> <ul> <li>Guard table: <code>meta_derived_applied_runs</code> prevents duplicate processing</li> <li>Retry with backoff: Handles Delta concurrency conflicts</li> <li>Stale claim recovery: Reclaims stuck jobs after timeout</li> </ul> <p>Valid derived tables: - <code>meta_daily_stats</code> - Daily execution statistics - <code>meta_pipeline_health</code> - Pipeline health scores - <code>meta_sla_status</code> - SLA compliance tracking</p>"},{"location":"ODIBI_DEEP_CONTEXT/#25-extension-points","title":"25. Extension Points","text":""},{"location":"ODIBI_DEEP_CONTEXT/#251-custom-patterns","title":"25.1 Custom Patterns","text":"<pre><code>from odibi.patterns.base import Pattern\nfrom odibi.context import EngineContext\n\nclass MyPattern(Pattern):\n    def validate(self) -&gt; None:\n        if not self.params.get(\"required_param\"):\n            raise ValueError(\"MyPattern: 'required_param' is required\")\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        # Transform and return DataFrame\n        return transformed_df\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#252-custom-connections","title":"25.2 Custom Connections","text":"<pre><code>from odibi.connections.base import BaseConnection\n\nclass MyConnection(BaseConnection):\n    def __init__(self, base_url: str, api_key: str):\n        self.base_url = base_url\n        self.api_key = api_key\n\n    def get_path(self, relative_path: str) -&gt; str:\n        return f\"{self.base_url}/{relative_path}\"\n\n    def validate(self) -&gt; None:\n        if not self.api_key:\n            raise ValueError(\"API key required\")\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#253-plugin-system","title":"25.3 Plugin System","text":"<p>Register via entry points in <code>pyproject.toml</code>: <pre><code>[project.entry-points.\"odibi.connections\"]\nmy_connector = \"my_package.connections:MyConnectionFactory\"\n</code></pre></p> <p>Plugins discovered via: <pre><code>from odibi.plugins import load_plugins\nload_plugins()  # Called automatically on startup\n</code></pre></p>"},{"location":"ODIBI_DEEP_CONTEXT/#26-quick-reference-cheat-sheet","title":"26. Quick Reference Cheat Sheet","text":""},{"location":"ODIBI_DEEP_CONTEXT/#yaml-node-structure","title":"YAML Node Structure","text":"<pre><code>nodes:\n  - name: node_name              # Required, alphanumeric + underscore\n    depends_on: [other_node]     # Dependencies for execution order\n    tags: [silver, daily]        # For filtering with --tag\n    inputs:\n      source: other_node         # Input from another node\n    read:                        # OR read from connection\n      connection: conn_name\n      path: relative/path\n      format: parquet            # parquet, delta, csv, json\n    transform:                   # List of transformations\n      - filter_rows:\n          condition: \"col &gt; 0\"\n      - derive_columns:\n          derivations:\n            new_col: \"expr\"\n    pattern:                     # OR use a pattern\n      type: dimension\n      params: {...}\n    validate:\n      tests:\n        - type: not_null\n          columns: [id]\n    write:                       # Persist output\n      connection: conn_name\n      path: output/path\n      format: delta\n      mode: overwrite            # overwrite, append, merge\n      partition_by: [year, month] # Physical partitioning\n      zorder_by: [customer_id]   # Z-ordering (Delta only)\n      merge_schema: true         # Allow schema evolution\n      auto_optimize: true        # Run VACUUM/OPTIMIZE after write\n      add_metadata: true         # Add _extracted_at, _source_file\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#common-transform-operations","title":"Common Transform Operations","text":"Task Transformer Filter rows <code>filter_rows: {condition: \"...\"}</code> Add column <code>derive_columns: {derivations: {col: expr}}</code> Remove duplicates <code>deduplicate: {keys: [...], order_by: \"...\"}</code> Join tables <code>join: {right_dataset: x, on: [...], how: left}</code> Aggregate <code>aggregate: {group_by: [...], aggregations: {...}}</code> Type casting <code>cast_columns: {casts: {col: type}}</code> Rename columns <code>rename_columns: {mapping: {old: new}}</code> SCD2 history <code>scd2: {target: ..., keys: [...], track_cols: [...]}</code> Detect deletes <code>detect_deletes: {mode: sql_compare, keys: [...]}</code>"},{"location":"ODIBI_DEEP_CONTEXT/#query-node-results-spark","title":"Query Node Results (Spark)","text":"<pre><code># After pipeline.run()\nspark.sql(\"SELECT * FROM node_name\")\nspark.sql(\"SELECT * FROM node_name WHERE condition\")\n\n# Get DataFrame directly\ndf = context.get(\"node_name\")\n\n# List all available nodes\ncontext.list_names()\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#debug-commands","title":"Debug Commands","text":"<pre><code>odibi run x.yaml --dry-run      # Validate only\nodibi run x.yaml --log-level DEBUG\nodibi story last                # View execution story\nodibi graph x.yaml              # View dependencies\nodibi doctor                    # Check environment\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#validation-quick-reference","title":"Validation Quick Reference","text":"<pre><code>validate:\n  tests:\n    - type: not_null\n      columns: [id]\n      on_fail: fail          # fail, warn, quarantine\n    - type: unique\n      columns: [id]\n    - type: accepted_values\n      column: status\n      values: [A, B, C]\n    - type: range\n      column: amount\n      min: 0\n      max: 1000000\n    - type: freshness\n      column: updated_at\n      max_age: \"24h\"\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#delta-lake-quick-reference","title":"Delta Lake Quick Reference","text":"<pre><code>write:\n  # Performance optimization\n  partition_by: [country, year_month]  # Low cardinality columns\n  zorder_by: [customer_id, product_id] # High cardinality columns\n\n  # Schema evolution\n  merge_schema: true\n\n  # Auto maintenance (VACUUM/OPTIMIZE)\n  auto_optimize: true\n  # OR with custom retention:\n  auto_optimize:\n    enabled: true\n    vacuum_retention_hours: 168  # 7 days\n\n  # Skip redundant writes\n  skip_if_unchanged: true\n  skip_hash_columns: [id, name]\n\n  # Bronze metadata\n  add_metadata: true  # Adds _extracted_at, _source_file\n\nread:\n  # Time travel\n  time_travel:\n    as_of_version: 10\n    # OR: as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#alerts-quick-reference","title":"Alerts Quick Reference","text":"<pre><code>alerts:\n  - type: slack              # slack, teams, webhook\n    url: \"${SLACK_WEBHOOK}\"\n    on_events: [on_failure, on_quarantine, on_gate_block]\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#connection-quick-reference","title":"Connection Quick Reference","text":"<pre><code>connections:\n  local:\n    type: local\n    base_path: ./data\n\n  adls:\n    type: azure_adls\n    account: myaccount\n    container: datalake\n    auth_mode: managed_identity\n\n  sql:\n    type: azure_sql\n    server: server.database.windows.net\n    database: db\n    auth_mode: aad_msi\n</code></pre>"},{"location":"ODIBI_DEEP_CONTEXT/#27-documentation-map","title":"27. Documentation Map","text":"<p>This section lists all detailed documentation files. Use <code>get_doc(\"path\")</code> to retrieve any doc.</p>"},{"location":"ODIBI_DEEP_CONTEXT/#core-reference","title":"Core Reference","text":"<ul> <li><code>docs/reference/yaml_schema.md</code> - Complete YAML configuration reference</li> <li><code>docs/reference/cheatsheet.md</code> - Quick reference card</li> <li><code>docs/reference/configuration.md</code> - Configuration options</li> <li><code>docs/reference/glossary.md</code> - Term definitions</li> <li><code>docs/troubleshooting.md</code> - Common issues and solutions</li> <li><code>docs/golden_path.md</code> - Recommended workflow</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#patterns-16-docs","title":"Patterns (16 docs)","text":"<ul> <li><code>docs/patterns/dimension.md</code> - Dimension pattern details</li> <li><code>docs/patterns/fact.md</code> - Fact table pattern</li> <li><code>docs/patterns/scd2.md</code> - SCD Type 2 implementation</li> <li><code>docs/patterns/aggregation.md</code> - Aggregation pattern</li> <li><code>docs/patterns/date_dimension.md</code> - Date dimension generation</li> <li><code>docs/patterns/merge_upsert.md</code> - Merge/upsert pattern</li> <li><code>docs/patterns/incremental_stateful.md</code> - Stateful incremental loads</li> <li><code>docs/patterns/smart_read.md</code> - Smart read with HWM</li> <li><code>docs/patterns/sql_server_merge.md</code> - SQL Server MERGE statement</li> <li><code>docs/patterns/anti_patterns.md</code> - What NOT to do</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#guides-21-docs","title":"Guides (21 docs)","text":"<ul> <li><code>docs/guides/best_practices.md</code> - Best practices</li> <li><code>docs/guides/writing_transformations.md</code> - How to write transforms</li> <li><code>docs/guides/dimensional_modeling_guide.md</code> - Star schema design</li> <li><code>docs/guides/python_api_guide.md</code> - Python API usage</li> <li><code>docs/guides/cli_master_guide.md</code> - CLI deep dive</li> <li><code>docs/guides/testing.md</code> - Testing strategies</li> <li><code>docs/guides/performance_tuning.md</code> - Performance optimization</li> <li><code>docs/guides/production_deployment.md</code> - Production setup</li> <li><code>docs/guides/secrets.md</code> - Secret management</li> <li><code>docs/guides/environments.md</code> - Environment configuration</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#features-22-docs","title":"Features (22 docs)","text":"<ul> <li><code>docs/features/transformers.md</code> - Transformer system</li> <li><code>docs/features/validation.md</code> - Data validation</li> <li><code>docs/features/quarantine.md</code> - Quarantine system</li> <li><code>docs/features/quality_gates.md</code> - Quality gates</li> <li><code>docs/features/connections.md</code> - Connection types</li> <li><code>docs/features/engines.md</code> - Pandas/Spark/Polars engines</li> <li><code>docs/features/patterns.md</code> - Pattern overview</li> <li><code>docs/features/lineage.md</code> - Data lineage</li> <li><code>docs/features/stories.md</code> - Execution stories</li> <li><code>docs/features/catalog.md</code> - System catalog</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#tutorials-19-docs","title":"Tutorials (19 docs)","text":"<ul> <li><code>docs/tutorials/getting_started.md</code> - Getting started</li> <li><code>docs/tutorials/bronze_layer.md</code> - Bronze layer tutorial</li> <li><code>docs/tutorials/silver_layer.md</code> - Silver layer tutorial</li> <li><code>docs/tutorials/gold_layer.md</code> - Gold layer tutorial</li> <li><code>docs/tutorials/spark_engine.md</code> - Using Spark</li> <li><code>docs/tutorials/dimensional_modeling/</code> - Dimensional modeling series (12 parts)</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#context-phases-14-docs","title":"Context Phases (14 docs)","text":"<p>Deep-dive implementation context for each system component: - <code>docs/context/PHASE_1_CORE_EXECUTION.md</code> - Core execution - <code>docs/context/PHASE_2_PATTERNS.md</code> - Pattern system - <code>docs/context/PHASE_3_TRANSFORMERS.md</code> - Transformer implementation - <code>docs/context/PHASE_4_CONNECTIONS.md</code> - Connection system - <code>docs/context/PHASE_5_RUNTIME.md</code> - Runtime behavior - <code>docs/context/PHASE_6_WORKFLOWS.md</code> - Common workflows - <code>docs/context/PHASE_7_GOTCHAS.md</code> - Gotchas and edge cases - <code>docs/context/PHASE_8_CLI.md</code> - CLI implementation - <code>docs/context/PHASE_9_EXTENSIONS.md</code> - Extension points</p>"},{"location":"ODIBI_DEEP_CONTEXT/#examples-10-docs","title":"Examples (10 docs)","text":"<ul> <li><code>docs/examples/canonical/01_hello_world.md</code> - Hello world</li> <li><code>docs/examples/canonical/02_incremental_sql.md</code> - Incremental loading</li> <li><code>docs/examples/canonical/03_scd2_dimension.md</code> - SCD2 example</li> <li><code>docs/examples/canonical/04_fact_table.md</code> - Fact table example</li> <li><code>docs/examples/canonical/05_full_pipeline.md</code> - Full pipeline</li> <li><code>docs/examples/canonical/THE_REFERENCE.md</code> - Reference implementation</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#validation-4-docs","title":"Validation (4 docs)","text":"<ul> <li><code>docs/validation/README.md</code> - Validation overview</li> <li><code>docs/validation/tests.md</code> - Validation tests</li> <li><code>docs/validation/contracts.md</code> - Data contracts</li> <li><code>docs/validation/fk.md</code> - Foreign key validation</li> </ul>"},{"location":"ODIBI_DEEP_CONTEXT/#semantic-layer-6-docs","title":"Semantic Layer (6 docs)","text":"<ul> <li><code>docs/semantics/index.md</code> - Semantic layer overview</li> <li><code>docs/semantics/metrics.md</code> - Metric definitions</li> <li><code>docs/semantics/query.md</code> - Querying metrics</li> <li><code>docs/semantics/materialize.md</code> - Materialization</li> <li><code>docs/semantics/runner.md</code> - Semantic runner</li> </ul> <p>MCP Usage: Call <code>list_docs(category=\"patterns\")</code> to list all pattern docs, then <code>get_doc(\"docs/patterns/scd2.md\")</code> to get specific content.</p>"},{"location":"ROADMAP/","title":"Odibi Roadmap","text":""},{"location":"ROADMAP/#current-state-v280","title":"Current State (v2.8.0)","text":"Metric Value Tests 2,118 collected (1,627 unit tests) Test Coverage 46% (target: 80%) Transformers 52 Patterns 6 (Dimension, Fact, SCD2, Merge, Aggregation, Date Dimension) Engines 3 (Pandas/DuckDB, Spark, Polars) MCP Tools 21 Python Support 3.9-3.12"},{"location":"ROADMAP/#coverage-by-module-low-priority-80-medium-50-80-high-50","title":"Coverage by Module (Low Priority = &gt;80%, Medium = 50-80%, High = &lt;50%)","text":"<p>Note on Engine Coverage: The low coverage for Spark (4%) and Polars (7%) is misleading. These engines are tested via: - Mock-based tests that validate logic without executing engine code - Production validation (Spark runs in Databricks) - Skip markers when dependencies aren't installed</p> <p>The coverage numbers reflect CI environment limitations, not actual test gaps.</p> Module Coverage Priority Notes <code>engine/spark_engine.py</code> 4% OK Mock-tested + Databricks validated <code>engine/polars_engine.py</code> 7% MEDIUM Needs more mock tests <code>engine/pandas_engine.py</code> 25% HIGH Primary engine, needs coverage <code>connections/azure_adls.py</code> 10% HIGH <code>diagnostics/delta.py</code> 13% HIGH <code>state/__init__.py</code> 33% HIGH <code>story/generator.py</code> 25% HIGH <code>transformers/merge_transformer.py</code> 36% HIGH <code>transformers/advanced.py</code> 44% MEDIUM <code>node.py</code> 44% MEDIUM <code>derived_updater.py</code> 49% MEDIUM <code>context.py</code> 62% MEDIUM <code>validation/engine.py</code> 68% LOW <code>validation/gate.py</code> 96% LOW <code>graph.py</code> 96% LOW"},{"location":"ROADMAP/#priority-1-stability-defect-triage","title":"Priority 1: Stability &amp; Defect Triage","text":"<p>Goal: Zero critical bugs, reliable cross-engine behavior</p>"},{"location":"ROADMAP/#11-known-issues-to-investigate","title":"1.1 Known Issues to Investigate","text":"<ul> <li>[ ] Audit edge cases in SCD2 pattern (null handling, late-arriving records)</li> <li>[ ] Validate complex pattern interactions (e.g., SCD2 + FK validation)</li> <li>[ ] Test YAML validation for all error paths</li> <li>[ ] Review engine-specific SQL syntax divergences</li> </ul>"},{"location":"ROADMAP/#12-test-infrastructure","title":"1.2 Test Infrastructure","text":"<ul> <li>[ ] Run <code>pytest --cov=odibi --cov-report=html</code> and identify modules &lt; 80% coverage</li> <li>[ ] Add parametrized tests for transformers across all 3 engines</li> <li>[ ] Add edge-case tests: empty DataFrames, null-only columns, Unicode data</li> </ul>"},{"location":"ROADMAP/#priority-2-engine-parity","title":"Priority 2: Engine Parity","text":"<p>Goal: Every feature works identically on Pandas, Spark, and Polars</p>"},{"location":"ROADMAP/#21-parity-audit","title":"2.1 Parity Audit","text":"<p>Run <code>tests/engine/test_parity.py</code> and expand coverage:</p> Transformer Pandas Spark Polars Notes <code>scd2</code> \u2713 \u2713 ? Verify Polars impl <code>pivot</code> \u2713 \u2713 ? Polars syntax differs <code>window_calculation</code> \u2713 \u2713 ? Frame spec differences <code>normalize_json</code> \u2713 \u2713 ? Struct handling"},{"location":"ROADMAP/#22-sql-abstraction","title":"2.2 SQL Abstraction","text":"<ul> <li>[ ] Document Spark SQL vs DuckDB SQL differences in <code>get_engine_differences</code> MCP tool</li> <li>[ ] Add compatibility layer for common divergences (e.g., <code>REGEXP_REPLACE</code> syntax)</li> <li>[ ] Test all 52 transformers on Polars engine</li> </ul>"},{"location":"ROADMAP/#priority-3-error-diagnostics-cli","title":"Priority 3: Error Diagnostics &amp; CLI","text":"<p>Goal: Misconfiguration is obvious, actionable errors</p>"},{"location":"ROADMAP/#31-yaml-validation","title":"3.1 YAML Validation","text":"<ul> <li>[ ] Ensure <code>odibi validate config.yaml</code> catches ALL common mistakes</li> <li>[ ] Add validation for node name format (alphanumeric + underscore only)</li> <li>[ ] Add validation for missing <code>format:</code> in inputs/outputs</li> <li>[ ] Improve error messages with line numbers and suggestions</li> </ul>"},{"location":"ROADMAP/#32-runtime-diagnostics","title":"3.2 Runtime Diagnostics","text":"<ul> <li>[ ] Expand <code>diagnose_error</code> MCP tool with more error patterns</li> <li>[ ] Add <code>odibi doctor</code> checks for environment issues</li> <li>[ ] Improve traceback cleaning for node execution errors</li> </ul>"},{"location":"ROADMAP/#priority-4-documentation-examples","title":"Priority 4: Documentation &amp; Examples","text":"<p>Goal: Golden path is clear, pitfalls are documented</p>"},{"location":"ROADMAP/#41-examples","title":"4.1 Examples","text":"<ul> <li>[ ] Verify all examples in <code>examples/</code> run successfully</li> <li>[ ] Add end-to-end example for each pattern</li> <li>[ ] Add \"migration from raw SQL\" example</li> </ul>"},{"location":"ROADMAP/#42-pitfall-documentation","title":"4.2 Pitfall Documentation","text":"<ul> <li>[ ] Document anti-patterns (e.g., using hyphens in node names)</li> <li>[ ] Document engine-specific gotchas</li> <li>[ ] Add troubleshooting section to AI assistant setup guide</li> </ul>"},{"location":"ROADMAP/#priority-5-feature-gaps","title":"Priority 5: Feature Gaps","text":"<p>Goal: Cover common use cases, stay focused</p>"},{"location":"ROADMAP/#51-potential-additions-evaluate-need-first","title":"5.1 Potential Additions (Evaluate Need First)","text":"<ul> <li>[ ] <code>apply_mapping</code> transformer for lookup-based value replacement</li> <li>[ ] <code>flatten_struct</code> for deeply nested JSON (beyond single level)</li> <li>[ ] <code>row_number</code> as standalone transformer (simpler than <code>window_calculation</code>)</li> <li>[ ] CDC (Change Data Capture) pattern variant</li> </ul>"},{"location":"ROADMAP/#52-not-adding","title":"5.2 NOT Adding","text":"<ul> <li>Complex orchestration (use Dagster/Airflow)</li> <li>Agent/chat infrastructure (use Amp/Cursor/Cline)</li> <li>GUI/web interface</li> </ul>"},{"location":"ROADMAP/#immediate-actions-next-sprint","title":"Immediate Actions (Next Sprint)","text":""},{"location":"ROADMAP/#week-1-core-coverage-gaps","title":"Week 1: Core Coverage Gaps","text":"<ol> <li>Pandas engine tests (<code>engine/pandas_engine.py</code> at 25%)</li> <li>Cover DuckDB SQL execution paths</li> <li>Test chunked reading/writing</li> <li> <p>This is the primary development engine</p> </li> <li> <p>Polars engine tests (<code>engine/polars_engine.py</code> at 7%)</p> </li> <li>Add mock-based tests for SQL operations</li> <li> <p>Verify lazy vs eager execution</p> </li> <li> <p>~~Spark engine tests~~ - Already covered via mocks + Databricks validation</p> </li> </ol>"},{"location":"ROADMAP/#week-2-integration-transformers","title":"Week 2: Integration &amp; Transformers","text":"<ol> <li>Merge transformer (<code>transformers/merge_transformer.py</code> at 36%)</li> <li>Test all merge modes (insert, update, upsert)</li> <li> <p>Test edge cases (empty source, all updates)</p> </li> <li> <p>Advanced transformers (44% coverage)</p> </li> <li>Parameterized tests across engines</li> </ol>"},{"location":"ROADMAP/#week-3-state-node","title":"Week 3: State &amp; Node","text":"<ol> <li>State management (<code>state/__init__.py</code> at 33%)</li> <li> <p>HWM persistence, checkpointing</p> </li> <li> <p>Node execution (<code>node.py</code> at 44%)</p> </li> <li>Error handling, retry logic, traceback cleaning</li> </ol>"},{"location":"ROADMAP/#week-4-polish","title":"Week 4: Polish","text":"<ol> <li>Expand <code>diagnose_error</code> MCP tool with 10+ error patterns</li> <li>Review GitHub issues and close stale ones</li> <li>Validate all examples in <code>examples/</code> directory</li> </ol>"},{"location":"ROADMAP/#existing-test-infrastructure-not-reflected-in-coverage","title":"Existing Test Infrastructure (Not Reflected in Coverage)","text":"<p>The <code>scripts/run_test_campaign.py</code> runs end-to-end validation that pytest coverage doesn't capture:</p> Phase What It Tests Engine Phase 1 CSV read, Parquet write, schema validation Pandas Phase 3 State/HWM persistence Pandas Phase 4 Merge pattern (upsert) Pandas Phase 5 SCD2 pattern Pandas Phase 6 Logical path resolution Pandas Phase 11 10k row scaling Pandas <p>Production validation: Spark engine runs in Databricks - not tested in CI but validated in production.</p>"},{"location":"ROADMAP/#documentation-gaps-to-address","title":"Documentation Gaps to Address","text":"<ul> <li>[x] Add <code>scripts/run_test_campaign.py</code> to <code>docs/guides/testing.md</code> \u2713</li> <li>[ ] Document Spark/Databricks testing approach in <code>docs/tutorials/spark_engine.md</code></li> <li>[x] Update <code>docs/features/engines.md</code> with engine-specific testing notes \u2713</li> <li>[ ] Add \"How to run the test campaign\" section to AGENTS.md or CONTRIBUTING.md</li> </ul>"},{"location":"ROADMAP/#success-metrics","title":"Success Metrics","text":"Goal Target Measurement Test coverage 80%+ <code>pytest --cov</code> CI pass rate 100% GitHub Actions Polars parity 100% Engine parity tests Example validation 100% All examples run Issue backlog &lt; 10 open GitHub Issues"},{"location":"ROADMAP/#long-term-vision","title":"Long-term Vision","text":"<p>Odibi should be: - Declarative: YAML-first, SQL-based transformations - Portable: Works on laptop (Pandas), cluster (Spark), or serverless (Polars) - Stable: Comprehensive tests, predictable behavior - Documented: AI assistants can generate correct configs without trial and error</p> <p>The ultimate test: Can you hand a business analyst the docs and have them build a working pipeline without your help?</p>"},{"location":"golden_path/","title":"Golden Path: From Zero to Production","text":"<p>Get a working Odibi pipeline in under 10 minutes. One path. No choices.</p>"},{"location":"golden_path/#step-1-install","title":"Step 1: Install","text":"<pre><code>pip install odibi\n</code></pre>"},{"location":"golden_path/#step-2-create-your-project","title":"Step 2: Create Your Project","text":"<p>Option A: Use the CLI (recommended) <pre><code>odibi init my_project --template star-schema\ncd my_project\n</code></pre></p> <p>Option B: Clone the reference <pre><code>git clone https://github.com/henryodibi11/Odibi.git\ncd Odibi/docs/examples/canonical/runnable\n</code></pre></p>"},{"location":"golden_path/#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline","text":"<pre><code>odibi run odibi.yaml          # Option A\nodibi run 04_fact_table.yaml  # Option B\n</code></pre> <p>This builds a complete star schema:</p> Output What It Is <code>dim_customer</code> Customer dimension with surrogate keys <code>dim_product</code> Product dimension with surrogate keys <code>dim_date</code> Generated date dimension (366 rows) <code>fact_sales</code> Sales fact with FK lookups to all dimensions"},{"location":"golden_path/#step-4-see-what-happened","title":"Step 4: See What Happened","text":"<pre><code># View the audit report (opens in browser)\nodibi story last\n\n# Or manually browse the output\nls data/gold/\n</code></pre>"},{"location":"golden_path/#step-5-copy-and-modify","title":"Step 5: Copy and Modify","text":"<p>The canonical pipeline is your template. Copy it:</p> <pre><code>cp 04_fact_table.yaml my_pipeline.yaml\n</code></pre> <p>Then modify: 1. Change <code>connections.source.base_path</code> to your data location 2. Update node names and paths to match your tables 3. Adjust patterns to fit your schema</p> <p>See the full breakdown: THE_REFERENCE.md</p>"},{"location":"golden_path/#the-config-you-just-ran","title":"The Config You Just Ran","text":"<pre><code>project: sales_star_schema\n\nconnections:\n  source:\n    type: local\n    base_path: ../sample_data\n  gold:\n    type: local\n    base_path: ./data/gold\n\nstory:\n  connection: gold\n  path: stories\n\nsystem:\n  connection: gold\n  path: _system\n\npipelines:\n  - pipeline: build_dimensions\n    layer: gold\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          format: csv\n          path: customers.csv\n          options:\n            header: true\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 1\n            track_cols: [name, email, tier, city]\n            unknown_member: true\n        write:\n          connection: gold\n          format: parquet\n          path: dim_customer\n          mode: overwrite\n\n      - name: dim_product\n        # ... (similar pattern)\n\n      - name: dim_date\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2025-01-01\"\n            end_date: \"2025-12-31\"\n            unknown_member: true\n        write:\n          connection: gold\n          format: parquet\n          path: dim_date\n          mode: overwrite\n\n  - pipeline: build_facts\n    layer: gold\n    nodes:\n      - name: fact_sales\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          format: csv\n          path: orders.csv\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, line_item_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n            orphan_handling: unknown\n            measures: [quantity, amount]\n        write:\n          connection: gold\n          format: parquet\n          path: fact_sales\n          mode: overwrite\n</code></pre> <p>Full version: 04_fact_table.yaml</p>"},{"location":"golden_path/#whats-next","title":"What's Next?","text":"Goal Go To Understand every line THE_REFERENCE.md Add validation/contracts YAML Schema Scale to production (Spark) Decision Guide Solve a specific problem Playbook"},{"location":"golden_path/#production-upgrade","title":"Production Upgrade","text":"<p>When you outgrow Pandas (files &gt; 1GB), switch to Spark:</p> <pre><code>engine: spark  # Add this at root level\n</code></pre> <p>That's it. Same config works on Databricks.</p> <p>Questions? Open an issue.</p>"},{"location":"philosophy/","title":"Odibi Philosophy","text":"<p>The core principles that guide Odibi's design and evolution.</p>"},{"location":"philosophy/#why-odibi-exists","title":"Why Odibi Exists","text":"<p>Odibi was built for data engineers who work alone or in small teams\u2014people who need to move fast without sacrificing quality. It encodes best practices into reusable patterns so you can focus on solving business problems, not reinventing infrastructure.</p> <p>Core Belief: Data pipelines should be declared, not coded. You describe what you want; the framework handles how.</p>"},{"location":"philosophy/#non-negotiable-principles","title":"Non-Negotiable Principles","text":"<ol> <li>Declarative over imperative \u2014 YAML is the source of truth</li> <li>Pydantic models, not dicts \u2014 Type safety at the boundary</li> <li>Explicit dependencies over implicit behavior \u2014 No magic</li> <li>Composition over inheritance \u2014 Build from proven patterns</li> <li>Fail-fast validation \u2014 Catch errors early, with context</li> <li>Performance awareness over cleverness \u2014 Simple scales</li> </ol>"},{"location":"philosophy/#the-five-laws","title":"The Five Laws","text":"<p>These laws guide every design decision:</p> <ol> <li> <p>Robots Remember, Humans Forget    \u2192 Checkpoint bookkeeping, not manual tracking. State is managed automatically.</p> </li> <li> <p>Raw is Sacred    \u2192 Append-only, immutable. Never destroy original data.</p> </li> <li> <p>Rebuild the Bucket, Don't Patch the Hole    \u2192 Reprocess windows, don't patch aggregates. When something breaks, rebuild cleanly.</p> </li> <li> <p>One-off logic is a smell    \u2192 If you do it twice, canonize it. Patterns emerge from repetition.</p> </li> <li> <p>Proven patterns should be reused forever    \u2192 Don't invent without evidence. Build on what works.</p> </li> </ol>"},{"location":"philosophy/#the-chimera-mindset","title":"The Chimera Mindset","text":"<p>Odibi evolves through a continuous cycle:</p> <pre><code>Observe real usage \u2192 Absorb what works \u2192 Discard what doesn't \u2192 Evolve the system\n</code></pre> <p>We don't invent abstractions without proof. Features graduate to the framework only when real pipelines demonstrate the need.</p>"},{"location":"philosophy/#how-features-get-added","title":"How Features Get Added","text":"<ul> <li>Build it only if 2+ real use cases show the need</li> <li>Reuse it if a pattern already exists</li> <li>Discard it if no one has used it in 3+ months</li> <li>Propose it with evidence (real examples), not speculation</li> </ul>"},{"location":"philosophy/#engine-parity-rule","title":"Engine Parity Rule","text":"<p>If Pandas has it, Spark and Polars must too. No exceptions.</p> <p>All three engines must produce identical results for the same input. You can develop locally with Pandas and deploy to production with Spark\u2014same YAML config, same behavior.</p>"},{"location":"philosophy/#what-odibi-is","title":"What Odibi Is","text":"\u2705 Odibi Is Description A pipeline framework Declarative YAML \u2192 executed pipelines Multi-engine Pandas, Spark, Polars with identical behavior Pattern-driven Dimension, Fact, SCD2, Aggregation built-in Self-documenting Every run generates a \"Data Story\" audit report Quality-focused Validation, contracts, quarantine, gates"},{"location":"philosophy/#what-odibi-is-not","title":"What Odibi Is NOT","text":"\u274c Odibi Is Not Use Instead A scheduler Airflow, Databricks Workflows, Prefect A BI tool PowerBI, Tableau, Looker A data catalog Unity Catalog, DataHub, Amundsen An agent/chat framework LangChain, CrewAI, custom agents"},{"location":"philosophy/#contributing","title":"Contributing","text":"<p>Want to contribute? Read CONTRIBUTING.md first.</p> <p>Remember: Propose features with evidence from real use cases, not speculation. The Chimera Mindset applies to contributions too.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>Quick reference for diagnosing and fixing common Odibi issues.</p> <p>For beginners: Each error section includes: - \ud83d\udccb Exact error message - Copy-paste to search - \ud83d\udca1 What it means - Plain English explanation - \ud83d\udd0d Why it happened - Root cause - \u2705 Step-by-step fix - How to resolve it - \ud83d\udee1\ufe0f How to prevent it - Stop it from happening again - \ud83d\udcdd YAML before/after - Broken vs fixed config</p>"},{"location":"troubleshooting/#quick-diagnostic-steps","title":"Quick Diagnostic Steps","text":"<p>My pipeline failed, now what?</p> <ol> <li>Check the error message - Look for the specific error type (validation, engine, pattern)</li> <li>Check logs for context - Use <code>get_logging_context()</code> for structured logs</li> <li>Run with verbose logging: <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></li> <li>Check data quality issues - Look for null keys, schema mismatches, FK violations</li> </ol>"},{"location":"troubleshooting/#common-errors-and-fixes","title":"Common Errors and Fixes","text":""},{"location":"troubleshooting/#import-and-installation-issues","title":"Import and Installation Issues","text":""},{"location":"troubleshooting/#module-not-found-errors","title":"Module Not Found Errors","text":"<pre><code>ModuleNotFoundError: No module named 'odibi'\n</code></pre> <p>Fix: Install odibi in your environment: <pre><code>pip install -e .  # Development install\n# or\npip install odibi\n</code></pre></p>"},{"location":"troubleshooting/#python-39-type-hint-compatibility","title":"Python 3.9 Type Hint Compatibility","text":"<pre><code>TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\n</code></pre> <p>Cause: Code uses Python 3.10+ union syntax (<code>str | None</code>) on Python 3.9.</p> <p>Fix: Use typing module syntax: <pre><code># Python 3.10+ (will fail on 3.9)\ndef func(param: str | None = None) -&gt; list[str]:\n    ...\n\n# Python 3.9 compatible\nfrom typing import Optional, List\ndef func(param: Optional[str] = None) -&gt; List[str]:\n    ...\n</code></pre></p>"},{"location":"troubleshooting/#engine-errors","title":"Engine Errors","text":""},{"location":"troubleshooting/#spark-python-version-mismatch","title":"Spark Python Version Mismatch","text":"<pre><code>PYTHON_VERSION_MISMATCH: Python in worker has different version (3, 8) than that in driver 3.9\n</code></pre> <p>Cause: Spark workers and driver use different Python versions.</p> <p>Fix: Set environment variables before starting Spark: <pre><code>export PYSPARK_PYTHON=python3.9\nexport PYSPARK_DRIVER_PYTHON=python3.9\n</code></pre></p> <p>Or in Python: <pre><code>import os\nos.environ['PYSPARK_PYTHON'] = 'python3.9'\nos.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n</code></pre></p>"},{"location":"troubleshooting/#pandaspolars-compatibility","title":"Pandas/Polars Compatibility","text":"<p>Pandas FutureWarning (fillna downcasting): <pre><code>FutureWarning: Downcasting object dtype arrays on .fillna is deprecated\n</code></pre></p> <p>Fix: Chain <code>.infer_objects(copy=False)</code> after fillna: <pre><code>df['column'].fillna(value).infer_objects(copy=False)\n</code></pre></p> <p>Polars API Changes: <pre><code>DeprecationWarning: `columns` argument renamed to `on`\n</code></pre></p> <p>Fix: Use the new parameter name: <pre><code>df.pivot(on=\"column\", ...)  # Not columns=\"column\"\n</code></pre></p>"},{"location":"troubleshooting/#delta-lake-issues","title":"Delta Lake Issues","text":""},{"location":"troubleshooting/#schema-mismatch-errors","title":"Schema Mismatch Errors","text":"<pre><code>Schema of data does not match table schema\n</code></pre> <p>Cause: DataFrame columns don't match the Delta table schema.</p> <p>Fix: Ensure column types match exactly: <pre><code># Check schemas before writing\nprint(df.dtypes)\n# Cast columns if needed\ndf['column'] = df['column'].astype('string')\n</code></pre></p>"},{"location":"troubleshooting/#pyarrow-engine-limitations","title":"PyArrow Engine Limitations","text":"<pre><code>schema_mode 'merge' is not supported in pyarrow engine. Use engine=rust\n</code></pre> <p>Cause: The PyArrow engine doesn't support schema evolution with <code>schema_mode='merge'</code>.</p> <p>Fix: Either: 1. Use the Rust engine: <code>engine='rust'</code> 2. Remove <code>schema_mode='merge'</code> for append-only operations (schema is fixed at bootstrap)</p>"},{"location":"troubleshooting/#catalog-log_run-failures","title":"Catalog log_run Failures","text":"<p>If <code>log_run</code> fails with schema errors: 1. Schema is fixed at bootstrap time 2. Use exact column types that match the run log schema 3. Serialize complex types (like lists) to JSON strings</p>"},{"location":"troubleshooting/#validation-errors","title":"Validation Errors","text":""},{"location":"troubleshooting/#fk-validation-failures","title":"FK Validation Failures","text":"<pre><code>Foreign key validation failed: 3 orphan records found\n</code></pre> <p>Diagnosis: <pre><code>result = validator.validate_foreign_key(df, 'fk_column', ref_df, 'pk_column')\nprint(result.orphan_records)  # See which records failed\n</code></pre></p> <p>Common causes: - Null FK values (decide: allow nulls or require matches) - Stale reference data - Case sensitivity mismatches</p>"},{"location":"troubleshooting/#quality-gate-blocks","title":"Quality Gate Blocks","text":"<pre><code>Quality gate blocked execution: data_quality_score &lt; 0.95\n</code></pre> <p>Diagnosis: Check which rules failed: <pre><code>result = validator.run_all()\nfor check in result.failed_checks:\n    print(f\"{check.rule}: {check.message}\")\n</code></pre></p>"},{"location":"troubleshooting/#quarantine-issues","title":"Quarantine Issues","text":"<p>If records are unexpectedly quarantined: 1. Check quarantine rules configuration 2. Review quarantine output for specific failures 3. Verify data types match expected patterns</p>"},{"location":"troubleshooting/#pattern-specific-issues","title":"Pattern-Specific Issues","text":""},{"location":"troubleshooting/#dimension-pattern-unknown-member-concat-failures","title":"Dimension Pattern: Unknown Member Concat Failures","text":"<pre><code>ValueError: all the input array dimensions except for the concatenation axis must match exactly\n</code></pre> <p>Cause: Datetime columns have mismatched types when concatenating unknown member row with data.</p> <p>Fix (already applied in framework): Unknown member row columns are cast to match DataFrame dtypes. If you see this on an older version, upgrade.</p>"},{"location":"troubleshooting/#scd2-merge-key-issues","title":"SCD2: Merge Key Issues","text":"<pre><code>KeyError: 'merge_key' not found\n</code></pre> <p>Checklist: 1. Verify merge key column exists in source data 2. Check column name spelling/case sensitivity 3. Ensure key isn't being dropped by upstream transforms</p>"},{"location":"troubleshooting/#aggregation-null-handling","title":"Aggregation: Null Handling","text":"<pre><code>Cannot aggregate on null values\n</code></pre> <p>Fix: Handle nulls before aggregation: <pre><code># Option 1: Filter nulls\ndf = df.dropna(subset=['group_column'])\n\n# Option 2: Replace nulls with placeholder\ndf['group_column'] = df['group_column'].fillna('UNKNOWN')\n</code></pre></p>"},{"location":"troubleshooting/#pipeline-configuration-errors","title":"Pipeline &amp; Configuration Errors","text":""},{"location":"troubleshooting/#cannot-resolve-column-name","title":"Cannot Resolve Column Name","text":"<p>\ud83d\udccb Error Message: <pre><code>AnalysisException: Cannot resolve column name 'customer_id' among (CustomerID, Name, Email, Address)\n</code></pre></p> <p>\ud83d\udca1 What It Means: You're trying to use a column that doesn't exist in your DataFrame. The column names you specified don't match the actual column names in your data.</p> <p>\ud83d\udd0d Why It Happens: - Column names are case-sensitive (e.g., <code>customer_id</code> \u2260 <code>CustomerID</code>) - The source system changed column names - An upstream transformation renamed columns - You have a typo in your YAML</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Print actual column names: <pre><code>df = spark.read.format(\"delta\").load(\"your/path\")\nprint(df.columns)\n# Output: ['CustomerID', 'Name', 'Email', 'Address']\n</code></pre></p> </li> <li> <p>Update YAML to match exactly: <pre><code># BEFORE (broken)\nparams:\n  keys: [\"customer_id\"]  # \u274c Wrong case\n\n# AFTER (fixed)\nparams:\n  keys: [\"CustomerID\"]   # \u2705 Matches actual column\n</code></pre></p> </li> </ol> <p>\ud83d\udee1\ufe0f Prevention: Normalize column names to lowercase in Bronze/Silver: <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        lowercase: true\n</code></pre></p>"},{"location":"troubleshooting/#column-not-found","title":"Column Not Found","text":"<p>\ud83d\udccb Error Message: <pre><code>KeyError: 'effective_date'\n# or\nColumn 'effective_date' does not exist\n</code></pre></p> <p>\ud83d\udca1 What It Means: A column you referenced in your config doesn't exist in the DataFrame at that point in the pipeline.</p> <p>\ud83d\udd0d Why It Happens: - Column was dropped by an earlier transformation - Column was renamed upstream - Column is in the wrong format (e.g., expecting <code>effective_date</code> but source has <code>EffectiveDate</code>) - You're referencing a column before it's created</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Add a debug step to see columns at each stage: <pre><code>nodes:\n  - name: \"debug_columns\"\n    depends_on: [\"previous_node\"]\n    transform:\n      steps:\n        - sql: \"SELECT *, 'columns:' as debug FROM df LIMIT 1\"\n    # Check logs for actual columns\n</code></pre></p> </li> <li> <p>Track column renames through your pipeline: <pre><code># Node 1: Rename columns\n- name: \"clean_data\"\n  transform:\n    steps:\n      - function: \"rename_columns\"\n        params:\n          columns:\n            EffectiveDate: \"effective_date\"  # Now it's lowercase\n\n# Node 2: Use the NEW name\n- name: \"process_data\"\n  depends_on: [\"clean_data\"]\n  params:\n    effective_time_col: \"effective_date\"  # \u2705 Use renamed column\n</code></pre></p> </li> </ol> <p>\ud83d\udcdd YAML Before/After: <pre><code># BEFORE (broken) - Column renamed but old name used\nnodes:\n  - name: \"prep\"\n    transform:\n      steps:\n        - function: \"rename_columns\"\n          params: { columns: { OldName: \"new_name\" } }\n\n  - name: \"process\"\n    depends_on: [\"prep\"]\n    params:\n      key_col: \"OldName\"  # \u274c Still using old name!\n\n# AFTER (fixed) - Use the new column name\nnodes:\n  - name: \"prep\"\n    transform:\n      steps:\n        - function: \"rename_columns\"\n          params: { columns: { OldName: \"new_name\" } }\n\n  - name: \"process\"\n    depends_on: [\"prep\"]\n    params:\n      key_col: \"new_name\"  # \u2705 Use the renamed column\n</code></pre></p>"},{"location":"troubleshooting/#unionbyname-failures","title":"unionByName Failures","text":"<p>\ud83d\udccb Error Message: <pre><code>AnalysisException: Union can only be performed on tables with compatible column types.\nColumn customer_id is of type StringType in first table and IntegerType in second.\n# or\nCannot resolve column 'new_column' in the right table\n</code></pre></p> <p>\ud83d\udca1 What It Means: You're trying to combine (union) two DataFrames that have different schemas\u2014either column types don't match, or columns are missing.</p> <p>\ud83d\udd0d Why It Happens: - Source schema changed but target table has old schema - SCD2 target has different columns than current source - Appending data with different types (e.g., source sends <code>\"123\"</code> as string, target expects integer)</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Compare schemas: <pre><code>source_df = spark.read.format(\"delta\").load(\"source/path\")\ntarget_df = spark.read.format(\"delta\").load(\"target/path\")\n\nprint(\"Source columns:\", source_df.dtypes)\nprint(\"Target columns:\", target_df.dtypes)\n</code></pre></p> </li> <li> <p>Cast columns to match: <pre><code>transform:\n  steps:\n    - function: \"cast_columns\"\n      params:\n        columns:\n          customer_id: \"integer\"  # Match target type\n          amount: \"double\"\n</code></pre></p> </li> <li> <p>Or enable schema merging: <pre><code>write:\n  format: delta\n  delta_options:\n    mergeSchema: true\n</code></pre></p> </li> </ol> <p>\ud83d\udcdd YAML Before/After: <pre><code># BEFORE (broken) - Mismatched types\nnodes:\n  - name: \"load_new_data\"\n    read:\n      connection: landing\n      path: new_customers.csv  # customer_id is STRING in CSV\n\n    write:\n      connection: silver\n      table: dim_customers  # customer_id is INTEGER in target\n      mode: append  # \u274c Fails due to type mismatch\n\n# AFTER (fixed) - Cast types before writing\nnodes:\n  - name: \"load_new_data\"\n    read:\n      connection: landing\n      path: new_customers.csv\n\n    transform:\n      steps:\n        - function: \"cast_columns\"\n          params:\n            columns:\n              customer_id: \"integer\"  # \u2705 Match target type\n\n    write:\n      connection: silver\n      table: dim_customers\n      mode: append\n</code></pre></p>"},{"location":"troubleshooting/#spaces-in-column-names","title":"Spaces in Column Names","text":"<p>\ud83d\udccb Error Message: <pre><code>AnalysisException: Syntax error in SQL: unexpected token 'Date'\n# or\nParseException: mismatched input 'Name' expecting &lt;EOF&gt;\n</code></pre></p> <p>\ud83d\udca1 What It Means: Your column names have spaces (e.g., <code>Customer Name</code>) which breaks SQL parsing.</p> <p>\ud83d\udd0d Why It Happens: - Source data came from Excel with friendly column headers - API returned columns with spaces - Someone created columns with spaces in the source system</p> <p>\u2705 Step-by-Step Fix:</p> <p>Option 1: Use backticks in SQL: <pre><code>transform:\n  steps:\n    - sql: \"SELECT `Customer Name`, `Order Date`, `Total Amount` FROM df\"\n</code></pre></p> <p>Option 2: Rename columns (recommended): <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        columns:\n          \"Customer Name\": \"customer_name\"\n          \"Order Date\": \"order_date\"\n          \"Total Amount\": \"total_amount\"\n</code></pre></p> <p>Option 3: Auto-normalize all columns: <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        snake_case: true  # Converts \"Customer Name\" \u2192 \"customer_name\"\n</code></pre></p> <p>\ud83d\udcdd YAML Before/After: <pre><code># BEFORE (broken) - Spaces in column names\nnodes:\n  - name: \"process_data\"\n    transform:\n      steps:\n        - sql: \"SELECT Customer Name, Order Date FROM df\"  # \u274c Syntax error!\n\n# AFTER (fixed) - Rename first\nnodes:\n  - name: \"process_data\"\n    transform:\n      steps:\n        - function: \"rename_columns\"\n          params:\n            snake_case: true\n        - sql: \"SELECT customer_name, order_date FROM df\"  # \u2705 Works now\n</code></pre></p>"},{"location":"troubleshooting/#connection-not-found","title":"Connection Not Found","text":"<p>\ud83d\udccb Error Message: <pre><code>KeyError: Connection 'prod_warehouse' not found\n# or\nConnectionError: No connection named 'gold' is defined\n</code></pre></p> <p>\ud83d\udca1 What It Means: Your pipeline references a connection name that isn't defined in your project config.</p> <p>\ud83d\udd0d Why It Happens: - Connection name is misspelled - Connection is defined in a different config file - Environment-specific connection isn't loaded - Connection was renamed but references weren't updated</p> <p>\u2705 Step-by-Step Fix:</p> <ol> <li> <p>Check available connections: <pre><code># In your odibi.yaml, list all connections:\nconnections:\n  bronze_storage:  # \u2190 These are your available names\n    type: azure_blob\n    ...\n  silver_storage:\n    type: azure_blob\n    ...\n</code></pre></p> </li> <li> <p>Fix the reference: <pre><code># BEFORE (broken)\nnodes:\n  - name: \"load_data\"\n    write:\n      connection: gold  # \u274c Not defined in connections!\n\n# AFTER (fixed)\nnodes:\n  - name: \"load_data\"\n    write:\n      connection: silver_storage  # \u2705 Matches defined connection\n</code></pre></p> </li> </ol> <p>\ud83d\udee1\ufe0f Prevention: Use a consistent naming convention: <pre><code>connections:\n  landing:   # Source data\n    ...\n  bronze:    # Raw layer\n    ...\n  silver:    # Cleaned layer\n    ...\n  gold:      # Business layer\n    ...\n</code></pre></p>"},{"location":"troubleshooting/#delta-table-version-conflicts","title":"Delta Table Version Conflicts","text":"<p>\ud83d\udccb Error Message: <pre><code>ConcurrentAppendException: Files were added to the root of the table by a concurrent update.\n# or\nConcurrentDeleteReadException: This transaction attempted to read files that were deleted by a concurrent commit.\n</code></pre></p> <p>\ud83d\udca1 What It Means: Multiple processes tried to write to the same Delta table at the same time, causing a conflict.</p> <p>\ud83d\udd0d Why It Happens: - Two pipeline runs overlap (same table, same time) - Parallel nodes trying to write to the same table - Streaming job and batch job writing to same table - Previous job didn't complete before retry started</p> <p>\u2705 Step-by-Step Fix:</p> <p>Option 1: Add retry with backoff: <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n  initial_delay: 5  # seconds\n</code></pre></p> <p>Option 2: Use merge instead of append (for some cases): <pre><code># Merge is idempotent - safe to retry\ntransformer: \"merge\"\nparams:\n  target: \"silver.dim_customers\"\n  keys: [\"customer_id\"]\n</code></pre></p> <p>Option 3: Ensure serial execution: <pre><code># In your orchestrator (Airflow, Databricks Workflows):\n# - Don't allow concurrent runs of same pipeline\n# - Or partition writes by date\n</code></pre></p> <p>\ud83d\udee1\ufe0f Prevention: - Use unique write paths for parallel jobs - Configure orchestrator to prevent overlapping runs - Use Delta Lake isolation levels appropriately</p>"},{"location":"troubleshooting/#common-odibi-configuration-errors","title":"Common Odibi Configuration Errors","text":"<p>These are the most frequent errors beginners encounter when configuring Odibi pipelines. Don't panic\u2014each one has a straightforward fix.</p>"},{"location":"troubleshooting/#schema-mismatch-column-not-found-in-dataframe","title":"Schema Mismatch: Column Not Found in DataFrame","text":"<p>Error message: <pre><code>AnalysisException: Cannot resolve column name 'customer_id' among [CustomerID, order_date, amount]\n</code></pre></p> <p>What it means: You referenced a column name in your YAML config that doesn't exist in the actual DataFrame.</p> <p>Why it happened: - Typo in the column name - Wrong case (column names are case-sensitive) - Column was renamed or dropped in an upstream step</p> <p>Step-by-step fix:</p> <ol> <li>Check the exact column names in your DataFrame:    <pre><code>print(df.columns)  # Pandas/Polars\ndf.printSchema()   # Spark\n</code></pre></li> <li>Compare with what you have in your YAML</li> <li>Update the YAML to match the exact column name (including case)</li> </ol> <p>YAML before (broken): <pre><code>pattern: dimension\nconfig:\n  natural_key: customer_id  # Wrong case!\n  columns:\n    - customer_id\n    - name\n</code></pre></p> <p>YAML after (fixed): <pre><code>pattern: dimension\nconfig:\n  natural_key: CustomerID  # Matches DataFrame exactly\n  columns:\n    - CustomerID\n    - name\n</code></pre></p> <p>How to prevent it next time: - Always print <code>df.columns</code> before writing your YAML - Use consistent naming conventions (snake_case recommended) - Add a schema validation step at pipeline start</p>"},{"location":"troubleshooting/#column-not-found-in-pattern-config","title":"Column Not Found in Pattern Config","text":"<p>Error message: <pre><code>KeyError: 'customer_id'\n</code></pre> or <pre><code>Column 'customer_id' not found in DataFrame\n</code></pre></p> <p>What it means: A column specified in your pattern configuration doesn't exist in the data.</p> <p>Why it happened: - Column name mismatch (typo or case difference) - Column was renamed in a previous transform - Column exists in source but not in transformed data</p> <p>Step-by-step fix:</p> <ol> <li>Identify which config field is causing the error (the traceback usually shows this)</li> <li>Print your DataFrame columns at the point of failure</li> <li>Match your config to the actual column names</li> </ol> <p>YAML before (broken): <pre><code>pattern: fact\nconfig:\n  grain:\n    - order_id\n    - line_item\n  measures:\n    - qty      # Wrong! Column is actually 'quantity'\n    - amount\n</code></pre></p> <p>YAML after (fixed): <pre><code>pattern: fact\nconfig:\n  grain:\n    - order_id\n    - line_item\n  measures:\n    - quantity  # Matches DataFrame column\n    - amount\n</code></pre></p> <p>How to prevent it next time: - Document expected column names in comments - Use a pre-flight check that validates all columns exist before processing</p>"},{"location":"troubleshooting/#unionbyname-failures-scd2-targetsource-mismatch","title":"UnionByName Failures (SCD2 Target/Source Mismatch)","text":"<p>Error message: <pre><code>AnalysisException: Cannot resolve column name 'is_current' among [customer_id, name, effective_date]\n</code></pre> or <pre><code>ValueError: Cannot union DataFrames with different columns\n</code></pre></p> <p>What it means: When merging source data with an existing target table (common in SCD2), the schemas don't match. The target has columns the source doesn't have.</p> <p>Why it happened: - Target table has SCD2-specific columns (<code>is_current</code>, <code>effective_from</code>, <code>effective_to</code>, <code>row_hash</code>) - Source data doesn't include these columns (and shouldn't\u2014the pattern adds them) - Previous schema changes weren't migrated properly</p> <p>Step-by-step fix:</p> <ol> <li>Let the SCD2 pattern add the tracking columns\u2014don't add them to source</li> <li>If manually fixing, ensure both schemas match:    <pre><code># Check target schema\ntarget_df.printSchema()\n\n# Check what SCD2 expects to add\n# is_current, effective_from, effective_to, row_hash\n</code></pre></li> <li>If target has extra columns, either:</li> <li>Add them to source with null values</li> <li>Rebuild target with correct schema</li> </ol> <p>YAML before (broken): <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: load_date\n  # Source has: customer_id, name, load_date\n  # Target has: customer_id, name, load_date, is_current, effective_from, effective_to, row_hash\n  # MISMATCH! But this is expected - SCD2 adds those columns\n</code></pre></p> <p>YAML after (fixed): <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: load_date\n  # Let the pattern handle the SCD2 columns automatically\n  # Don't pre-add them to your source data\n</code></pre></p> <p>How to prevent it next time: - Never manually add SCD2 tracking columns to source data - When bootstrapping, let the pattern create the initial schema - Document which columns are managed by the pattern</p>"},{"location":"troubleshooting/#spaces-in-column-names_1","title":"Spaces in Column Names","text":"<p>Error message: <pre><code>AnalysisException: Column name 'Customer Name' cannot be resolved\n</code></pre> or <pre><code>KeyError: 'Customer Name'\n</code></pre></p> <p>What it means: Your column names contain spaces, which cause parsing issues.</p> <p>Why it happened: - Data imported from Excel with human-readable headers - Source system uses spaces in column names - CSV headers weren't cleaned before loading</p> <p>Step-by-step fix:</p> <ol> <li> <p>Option A: Rename columns in source (recommended):    <pre><code># Pandas\ndf.columns = df.columns.str.replace(' ', '_')\n\n# Spark\nfor col in df.columns:\n    df = df.withColumnRenamed(col, col.replace(' ', '_'))\n\n# Polars\ndf = df.rename({col: col.replace(' ', '_') for col in df.columns})\n</code></pre></p> </li> <li> <p>Option B: Use backticks in YAML (Spark only):    <pre><code>columns:\n  - \"`Customer Name`\"\n</code></pre></p> </li> </ol> <p>YAML before (broken): <pre><code>pattern: dimension\nconfig:\n  natural_key: Customer ID  # Space causes issues!\n  columns:\n    - Customer ID\n    - Customer Name\n    - Email Address\n</code></pre></p> <p>YAML after (fixed): <pre><code>pattern: dimension\nconfig:\n  natural_key: customer_id  # Clean snake_case\n  columns:\n    - customer_id\n    - customer_name\n    - email_address\n</code></pre></p> <p>Pre-processing step to add: <pre><code># Add this before any pattern processing\ndf.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n</code></pre></p> <p>How to prevent it next time: - Always clean column names at the start of your pipeline - Establish a naming convention (snake_case is standard) - Add a column name validator to your ingestion layer</p>"},{"location":"troubleshooting/#scd2-effective_time_col-errors","title":"SCD2 effective_time_col Errors","text":"<p>Error message: <pre><code>KeyError: 'effective_time_col'\n</code></pre> or <pre><code>Column 'txn_date' not found in DataFrame\n</code></pre></p> <p>What it means: The column you specified as <code>effective_time_col</code> doesn't exist in your source data at the point where SCD2 needs it.</p> <p>Why it happened: - Column was renamed in an earlier transform step - Column name has a typo - Column was dropped before reaching SCD2 pattern - You're referencing a derived column that doesn't exist yet</p> <p>Step-by-step fix:</p> <ol> <li>Verify the column exists in your source data:    <pre><code>print('txn_date' in df.columns)  # Should be True\n</code></pre></li> <li>Check if any transform renamed or dropped it</li> <li>Update the config to use the correct column name</li> </ol> <p>YAML before (broken): <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: txn_date  # Oops! Column was renamed to 'transaction_date'\n</code></pre></p> <p>YAML after (fixed): <pre><code>pattern: scd2\nconfig:\n  natural_key: customer_id\n  effective_time_col: transaction_date  # Matches actual column name\n</code></pre></p> <p>Common gotcha: The <code>effective_time_col</code> must exist in your source DataFrame. It gets used to set <code>effective_from</code> dates and may be dropped after processing.</p> <p>How to prevent it next time: - Print <code>df.columns</code> right before the SCD2 pattern runs - Keep your transformation pipeline documented - Use meaningful, consistent column names throughout</p>"},{"location":"troubleshooting/#connection-not-found_1","title":"Connection Not Found","text":"<p>Error message: <pre><code>ConnectionError: Connection 'warehouse' not found in project config\n</code></pre> or <pre><code>KeyError: 'warehouse'\n</code></pre></p> <p>What it means: You referenced a connection name that isn't defined in your project configuration.</p> <p>Why it happened: - Connection not defined in project config - Typo in connection name - Connection section missing entirely - Environment-specific config not loaded</p> <p>Step-by-step fix:</p> <ol> <li>Check your project config file structure</li> <li>Add or fix the connections section</li> <li>Ensure connection name matches exactly (case-sensitive)</li> </ol> <p>YAML before (broken): <pre><code># pipeline.yaml\nsources:\n  - name: customers\n    connection: Warehouse  # Wrong case!\n    table: dim_customer\n</code></pre></p> <p>YAML after (fixed): <pre><code># project_config.yaml - Must have connections defined\nconnections:\n  warehouse:  # lowercase to match\n    type: databricks\n    catalog: main\n    schema: gold\n\n# pipeline.yaml\nsources:\n  - name: customers\n    connection: warehouse  # Matches connection name exactly\n    table: dim_customer\n</code></pre></p> <p>How to prevent it next time: - Use lowercase connection names consistently - Keep a template project config with all required sections - Validate config on pipeline startup</p>"},{"location":"troubleshooting/#delta-table-version-conflicts_1","title":"Delta Table Version Conflicts","text":"<p>Error message: <pre><code>ConcurrentModificationException: Conflicting commits\n</code></pre> or <pre><code>DeltaTableVersionMismatch: Expected version X but found version Y\n</code></pre> or <pre><code>ConcurrentAppendException: Files were added by a concurrent update\n</code></pre></p> <p>What it means: Multiple processes tried to write to the same Delta table simultaneously, or your reference to the table is stale.</p> <p>Why it happened: - Two pipelines writing to the same table at the same time - Long-running transaction conflicted with another write - Cached table reference is outdated - Overwrite operation conflicted with append</p> <p>Step-by-step fix:</p> <ol> <li> <p>Identify the conflict source: <pre><code># Check Delta table history\nfrom delta.tables import DeltaTable\ndt = DeltaTable.forPath(spark, \"path/to/table\")\ndt.history().show()\n</code></pre></p> </li> <li> <p>Use merge instead of overwrite: <pre><code># Instead of overwrite (can conflict)\ndf.write.format(\"delta\").mode(\"overwrite\").save(path)\n\n# Use merge (handles concurrency better)\ndelta_table.alias(\"target\").merge(\n    df.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n</code></pre></p> </li> <li> <p>Add retry logic: <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))\ndef safe_write(df, path):\n    df.write.format(\"delta\").mode(\"append\").save(path)\n</code></pre></p> </li> </ol> <p>YAML before (problematic): <pre><code>pattern: merge\nconfig:\n  write_mode: overwrite  # Can cause conflicts with concurrent writes\n</code></pre></p> <p>YAML after (safer): <pre><code>pattern: merge\nconfig:\n  write_mode: merge  # Handles concurrent operations better\n  merge_keys:\n    - customer_id\n</code></pre></p> <p>How to prevent it next time: - Avoid concurrent writes to the same table - Use merge patterns instead of overwrite when possible - Implement job orchestration to serialize conflicting writes - Enable Delta Lake optimistic concurrency settings</p>"},{"location":"troubleshooting/#azure-specific-troubleshooting","title":"Azure-Specific Troubleshooting","text":""},{"location":"troubleshooting/#adls-authentication","title":"ADLS Authentication","text":""},{"location":"troubleshooting/#credential-issues","title":"Credential Issues","text":"<pre><code>AuthenticationError: Invalid credentials\n</code></pre> <p>Checklist: 1. Verify service principal credentials are correct 2. Check tenant ID, client ID, client secret 3. Ensure service principal has Storage Blob Data Contributor role</p> <pre><code>from azure.identity import DefaultAzureCredential\ncredential = DefaultAzureCredential()\n# If using service principal:\nfrom azure.identity import ClientSecretCredential\ncredential = ClientSecretCredential(tenant_id, client_id, client_secret)\n</code></pre>"},{"location":"troubleshooting/#access-token-expiry","title":"Access Token Expiry","text":"<pre><code>TokenExpiredError: Token has expired\n</code></pre> <p>Fix: Use <code>DefaultAzureCredential</code> which handles token refresh automatically.</p>"},{"location":"troubleshooting/#delta-table-errors","title":"Delta Table Errors","text":""},{"location":"troubleshooting/#storage-throttling-429-errors","title":"Storage Throttling (429 Errors)","text":"<pre><code>TooManyRequests: Rate limit exceeded\n</code></pre> <p>Fixes: 1. Implement retry logic with exponential backoff 2. Reduce concurrent operations 3. Batch smaller writes</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, max=60))\ndef write_with_retry(df, path):\n    df.write.format(\"delta\").save(path)\n</code></pre>"},{"location":"troubleshooting/#concurrent-write-conflicts","title":"Concurrent Write Conflicts","text":"<pre><code>ConcurrentAppendException: Files were added by a concurrent update\n</code></pre> <p>Fixes: 1. Enable optimistic concurrency: Set <code>delta.enableChangeDataFeed = true</code> 2. Use merge instead of overwrite when possible 3. Coordinate write operations to avoid conflicts</p>"},{"location":"troubleshooting/#file-locking-issues","title":"File Locking Issues","text":"<p>If writes hang or fail with lock errors: 1. Check for stale lock files in <code>_delta_log/</code> 2. Wait for other operations to complete 3. Consider using a single writer pattern</p>"},{"location":"troubleshooting/#azure-sql-issues","title":"Azure SQL Issues","text":""},{"location":"troubleshooting/#odbc-driver-setup","title":"ODBC Driver Setup","text":"<pre><code>Error: [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 17 for SQL Server'\n</code></pre> <p>Fix (Ubuntu/WSL): <pre><code>curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/20.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql17\n</code></pre></p>"},{"location":"troubleshooting/#connection-timeout","title":"Connection Timeout","text":"<pre><code>OperationalError: Connection timed out\n</code></pre> <p>Fix: Increase connection timeout: <pre><code>from sqlalchemy import create_engine\nengine = create_engine(\n    connection_string,\n    connect_args={\"timeout\": 60}\n)\n</code></pre></p>"},{"location":"troubleshooting/#sqlalchemy-configuration","title":"SQLAlchemy Configuration","text":"<pre><code># Full connection string example\nconnection_string = (\n    \"mssql+pyodbc://user:password@server.database.windows.net:1433/\"\n    \"database?driver=ODBC+Driver+17+for+SQL+Server&amp;Encrypt=yes&amp;TrustServerCertificate=no\"\n)\n</code></pre>"},{"location":"troubleshooting/#wsllinux-setup-issues","title":"WSL/Linux Setup Issues","text":""},{"location":"troubleshooting/#python-command-not-found","title":"Python Command Not Found","text":"<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'python'\n</code></pre> <p>Fix: Either create a symlink or install the package: <pre><code># Option 1: Install symlink package\nsudo apt install python-is-python3\n\n# Option 2: Use python3.9 explicitly\npython3.9 -m pytest tests/\n</code></pre></p>"},{"location":"troubleshooting/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code>ModuleNotFoundError: No module named 'sqlalchemy'\n</code></pre> <p>Fix: <pre><code>pip3.9 install sqlalchemy pyodbc\n</code></pre></p>"},{"location":"troubleshooting/#spark-workers-python-version-mismatch","title":"Spark Workers Python Version Mismatch","text":"<p>Ensure all workers use the same Python: <pre><code># Add to ~/.bashrc or set before running Spark\nexport PYSPARK_PYTHON=/usr/bin/python3.9\nexport PYSPARK_DRIVER_PYTHON=/usr/bin/python3.9\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#how-to-report-bugs","title":"How to Report Bugs","text":"<ol> <li>Check GitHub Issues for existing reports</li> <li>Create a new issue with:</li> <li>Odibi version</li> <li>Python version</li> <li>Engine (Pandas/Spark/Polars)</li> <li>Full error message and traceback</li> <li>Minimal reproducible example</li> </ol>"},{"location":"troubleshooting/#required-info-for-bug-reports","title":"Required Info for Bug Reports","text":"<pre><code>**Environment:**\n- Odibi version: X.X.X\n- Python version: 3.X\n- OS: Windows/Linux/WSL\n- Engine: Pandas/Spark/Polars\n\n**Error:**\n[Paste full traceback]\n\n**Reproduction:**\n[Minimal code to reproduce]\n\n**Expected behavior:**\n[What you expected to happen]\n</code></pre>"},{"location":"troubleshooting/#links","title":"Links","text":"<ul> <li>GitHub Issues - Report bugs and request features</li> <li>Discussions - Ask questions and share ideas</li> <li>Roadmap - See what's coming next</li> </ul>"},{"location":"troubleshooting/#learning-resources","title":"Learning Resources","text":"<p>New to Odibi or data engineering? Start here:</p> <ul> <li>Data Engineering 101 - Complete beginner's guide</li> <li>Glossary - Every term explained simply</li> <li>4-Week Curriculum - Structured learning path</li> <li>Anti-Patterns Guide - What NOT to do</li> <li>SCD2 Pattern - History tracking with troubleshooting</li> </ul>"},{"location":"api/context/","title":"Context API","text":"<p>The Context API provides access to DataFrames, engine-specific features, and execution state within transforms.</p>"},{"location":"api/context/#overview","title":"Overview","text":"<p>Every transform function receives a <code>context</code> parameter:</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef my_transform(context, current, param1: str):\n    # context: ExecutionContext with DataFrame access\n    # current: The input DataFrame for this node\n    return current\n</code></pre>"},{"location":"api/context/#context-classes","title":"Context Classes","text":"<p>Odibi has engine-specific context implementations:</p> Engine Context Class Key Features Pandas <code>PandasContext</code> In-memory DataFrames Polars <code>PolarsContext</code> Lazy/eager DataFrames Spark <code>SparkContext</code> Distributed DataFrames, SQL <p>All contexts implement the base <code>Context</code> interface.</p>"},{"location":"api/context/#core-methods","title":"Core Methods","text":""},{"location":"api/context/#getname","title":"get(name)","text":"<p>Retrieve a DataFrame by node name:</p> <pre><code>@transform\ndef join_data(context, current):\n    # Get another node's output\n    customers = context.get(\"load_customers\")\n    return current.join(customers, on=\"customer_id\")\n</code></pre>"},{"location":"api/context/#setname-df","title":"set(name, df)","text":"<p>Register a DataFrame for downstream nodes:</p> <pre><code>@transform\ndef split_data(context, current):\n    valid = current.filter(\"is_valid = true\")\n    invalid = current.filter(\"is_valid = false\")\n\n    # Register additional output\n    context.set(\"invalid_records\", invalid)\n\n    return valid  # Primary output\n</code></pre>"},{"location":"api/context/#sqlquery","title":"sql(query)","text":"<p>Execute SQL against registered DataFrames (Spark only):</p> <pre><code>@transform\ndef sql_transform(context, current):\n    # Register current DataFrame as a view\n    context.set(\"input_data\", current)\n\n    # Execute SQL\n    result = context.sql(\"\"\"\n        SELECT customer_id, SUM(amount) as total\n        FROM input_data\n        GROUP BY customer_id\n    \"\"\")\n    return result\n</code></pre>"},{"location":"api/context/#engine-context","title":"Engine Context","text":"<p>Access engine-specific features via <code>engine_context</code>:</p>"},{"location":"api/context/#spark","title":"Spark","text":"<pre><code>@transform\ndef spark_specific(context, current):\n    spark = context.engine_context.spark\n\n    # Use Spark session directly\n    df = spark.read.parquet(\"/path/to/data\")\n\n    # Access catalog\n    spark.catalog.listTables()\n\n    return current\n</code></pre>"},{"location":"api/context/#pandas","title":"Pandas","text":"<pre><code>@transform\ndef pandas_specific(context, current):\n    # current is already a pd.DataFrame\n    # No special engine context needed\n    return current.groupby(\"category\").sum()\n</code></pre>"},{"location":"api/context/#polars","title":"Polars","text":"<pre><code>@transform\ndef polars_specific(context, current):\n    # current is a polars DataFrame\n    import polars as pl\n\n    return current.with_columns(\n        pl.col(\"amount\").sum().over(\"category\").alias(\"category_total\")\n    )\n</code></pre>"},{"location":"api/context/#enginecontext-class","title":"EngineContext Class","text":"<p>The <code>EngineContext</code> provides engine metadata:</p> <pre><code>class EngineContext:\n    engine_type: str          # \"pandas\", \"polars\", \"spark\"\n    spark: SparkSession       # Only for Spark engine\n\n    @property\n    def is_spark(self) -&gt; bool: ...\n\n    @property\n    def is_pandas(self) -&gt; bool: ...\n\n    @property\n    def is_polars(self) -&gt; bool: ...\n</code></pre>"},{"location":"api/context/#example-engine-agnostic-transform","title":"Example: Engine-Agnostic Transform","text":"<p>Write transforms that work on all engines:</p> <pre><code>@transform\ndef engine_agnostic(context, current, threshold: float = 100):\n    engine = context.engine_context\n\n    if engine.is_spark:\n        from pyspark.sql import functions as F\n        return current.filter(F.col(\"amount\") &gt; threshold)\n\n    elif engine.is_polars:\n        import polars as pl\n        return current.filter(pl.col(\"amount\") &gt; threshold)\n\n    else:  # pandas\n        return current[current[\"amount\"] &gt; threshold]\n</code></pre>"},{"location":"api/context/#available-in-context","title":"Available in Context","text":"Property Description <code>engine_context</code> Engine-specific context with <code>spark</code>, <code>engine_type</code> <code>config</code> Current node configuration <code>connections</code> Connection registry <code>state_manager</code> Access to HWM and run state"},{"location":"api/context/#related","title":"Related","text":"<ul> <li>Writing Transformations \u2014 Transform authoring guide</li> </ul>"},{"location":"context/PHASE_1_CORE_EXECUTION/","title":"PHASE 1: CORE EXECUTION ANALYSIS","text":""},{"location":"context/PHASE_1_CORE_EXECUTION/#overview","title":"Overview","text":"<p>This phase focuses on understanding the core execution paths in the odibi codebase, covering the interactions between key components: <code>Pipeline</code>, <code>Node/NodeExecutor</code>, and <code>DependencyGraph</code>.</p>"},{"location":"context/PHASE_1_CORE_EXECUTION/#key-components-and-their-execution-roles","title":"Key Components and Their Execution Roles","text":""},{"location":"context/PHASE_1_CORE_EXECUTION/#pipeline","title":"<code>Pipeline</code>","text":"<p>The <code>Pipeline</code> class acts as an orchestrator and context manager for executing nodes defined in a pipeline configuration. Key responsibilities include: - Initialization: Sets up logging context, initializes the dependency graph via <code>DependencyGraph</code>, and prepares the engine. - Execution: The <code>run()</code> method drives node execution in either serial or parallel mode using a topological sort of the nodes (produced by the dependency graph). - Artifacts: Generates stories, alerts, intermediate results (<code>story_path</code>), and lineage data. - State Management: Implements functions to resume from failure, manage catalog entries, and track metadata across runs. - Error Handling: Adopts configurable error strategies like <code>FAIL_FAST</code> or <code>FAIL_LATER</code>. - Key Methods:   - <code>run()</code>: The main execution method that invokes nodes using either a serial or parallel strategy.   - <code>flush_stories(timeout)</code>: Handles async story generation for added context visualization.   - <code>_send_alerts()</code>: Sends alerts at the start, success, or failure of the pipeline.</p>"},{"location":"context/PHASE_1_CORE_EXECUTION/#node-and-nodeexecutor","title":"<code>Node</code> and <code>NodeExecutor</code>","text":""},{"location":"context/PHASE_1_CORE_EXECUTION/#node","title":"Node","text":"<ul> <li>Role: High-level container handling the orchestration of <code>NodeExecutor</code> and state management.</li> <li>Restore: Attempts to restore a node's state from previous runs using <code>restore()</code>, relying on catalogs and configurations.</li> <li>Caching: Utilizes results caching for optimized execution.</li> </ul>"},{"location":"context/PHASE_1_CORE_EXECUTION/#nodeexecutor","title":"NodeExecutor","text":"<ul> <li>Role: Executes individual node logic, breaking down execution into well-defined phases (<code>read</code>, <code>transform</code>, <code>validate</code>, etc.).</li> <li>Execution Steps:</li> <li>Pre-SQL phase:<ul> <li>Executes pre-defined SQL queries on the engine tied to the pipeline context (if any).</li> </ul> </li> <li>Read phase:<ul> <li>Retrieves data either from the current pipeline or from other pipelines (via catalog references and dependency resolution).</li> <li>Handles partitioned datasets, Delta files, and incremental filtering.</li> </ul> </li> <li>Transformation phase:<ul> <li>Applies transformation logic, including Python functions and patterns (e.g., <code>merge</code>, <code>scd2</code>).</li> <li>Patterns are implemented using <code>EngineContext</code> and <code>Pattern</code> classes.</li> </ul> </li> <li>Validation phase:<ul> <li>Runs tests to ensure schema compliance and data validity.</li> </ul> </li> <li>Write phase:<ul> <li>Writes results to specified storage and registers them in catalogs for cross-pipeline dependencies.</li> </ul> </li> <li>Feature Highlights:</li> <li>Incremental Execution: Filters data based on stateful or rolling-window strategies.</li> <li>Metrics Logging: Tracks execution times, schema changes, and row count deltas for insights during debugging.</li> <li>Retry Logic: Executes operations with configurable retry mechanisms and backoff strategies.</li> <li>Quarantine Mechanism: Isolates invalid data during validation and logs warnings or errors.</li> </ul>"},{"location":"context/PHASE_1_CORE_EXECUTION/#dependencygraph","title":"DependencyGraph","text":""},{"location":"context/PHASE_1_CORE_EXECUTION/#role","title":"Role:","text":"<p><code>DependencyGraph</code> ensures that pipeline nodes are executed in the correct order efficiently while supporting parallelism where possible.</p>"},{"location":"context/PHASE_1_CORE_EXECUTION/#key-features","title":"Key Features:","text":"<ol> <li>Validation:</li> <li>Detects missing dependencies and ensures all referenced nodes exist in the graph.</li> <li> <p>Identifies and prevents circular dependencies.</p> </li> <li> <p>Execution Planning:</p> </li> <li>Topological Sort:      Ensures that all dependencies of a node are executed before the node itself is processed.</li> <li> <p>Execution Layers:      Groups nodes into parallelizable layers based on their dependencies.</p> </li> <li> <p>Analysis Helpers:</p> </li> <li><code>get_dependencies()</code>: Lists all direct and transitive dependencies for a node.</li> <li><code>get_dependents()</code>: Lists all direct and transitive dependent nodes.</li> <li><code>get_independent_nodes()</code>: Returns nodes with no dependencies.</li> <li><code>visualize()</code>: Generates a human-readable representation of the graph, helpful for debugging the execution plan.</li> </ol>"},{"location":"context/PHASE_1_CORE_EXECUTION/#non-obvious-behaviors-identified","title":"Non-Obvious Behaviors Identified","text":""},{"location":"context/PHASE_1_CORE_EXECUTION/#pipeline_1","title":"<code>Pipeline</code>:","text":"<ol> <li><code>resume_from_failure</code> relies on both context and the state manager. Missing configurations or corrupted states could cause silent failures or skipped operations.</li> <li>Complex error strategies (<code>FAIL_FAST</code>) with potential edge cases when running in parallel mode.</li> </ol>"},{"location":"context/PHASE_1_CORE_EXECUTION/#node_1","title":"<code>Node</code>:","text":"<ol> <li>Input data resolution hybridizes current pipeline results and catalog-manager lookups for cross-pipeline inputs, supporting scenarios like result pre-registration or unallocated memory restoration.</li> <li><code>dry_run=True</code> is feature-rich for debugging purposes but does not validate cross-layer dependencies (e.g., incremental filters rely on actual runtime queries).</li> </ol>"},{"location":"context/PHASE_1_CORE_EXECUTION/#dependencygraph_1","title":"<code>DependencyGraph</code>:","text":"<ol> <li>Cross-pipeline dependencies are supported using <code>$pipeline.node</code> references in a node's <code>inputs</code> block. These are visualized as external references but require catalog data.</li> </ol>"},{"location":"context/PHASE_1_CORE_EXECUTION/#observations","title":"Observations","text":"<p>This phase reinforces the tightly coupled system design: 1. Strong guarantees against dependency violations (e.g., cycles, missing nodes). 2. Considerable investment in observability (through alerts, lineage registration, and logging). 3. Configurable failover mechanisms in both <code>Pipeline</code> and <code>NodeExecutor</code> increase reliability but require precise configurations.</p>"},{"location":"context/PHASE_1_CORE_EXECUTION/#next-steps","title":"Next Steps","text":"<p>Transition to Phase 2 and analyze the behavioral intricacies within <code>patterns</code> to ensure parity between Panda, Spark, and Polars engines.</p>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/","title":"PHASE 2B: DIMENSION PATTERN ANALYSIS","text":""},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#overview","title":"Overview","text":"<p>The <code>DimensionPattern</code> is a fundamental pattern in Odibi used for building and managing dimension tables in data warehouses. It supports various Slowly Changing Dimension (SCD) types, surrogate key management, and audit column tracking. This document provides an in-depth analysis of its configuration, methods, and runtime behaviors.</p>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#key-parameters","title":"Key Parameters:","text":"<ol> <li><code>natural_key</code> (str)</li> <li>The natural/business key column name. This uniquely identifies records in the source system.</li> <li> <p>Required: Yes.</p> </li> <li> <p><code>surrogate_key</code> (str)</p> </li> <li>Surrogate key column name, auto-generated as the primary key for the dimension table.</li> <li> <p>Required: Yes.</p> </li> <li> <p><code>scd_type</code> (int)</p> </li> <li>Supported values:<ul> <li><code>0</code>: Static dimension (no updates).</li> <li><code>1</code>: Overwrite changes (no history retention).</li> <li><code>2</code>: Maintain full history (SCD2).</li> </ul> </li> <li> <p>Default: 1.</p> </li> <li> <p><code>track_cols</code> (list)</p> </li> <li>Specifies which columns to monitor for changes (applies to SCD1 and SCD2).</li> <li> <p>Required for SCD1, SCD2: Yes.</p> </li> <li> <p><code>target</code> (str)</p> </li> <li>Path to the target table, used mainly for SCD2 to detect changes in historical data.</li> <li> <p>Required for SCD2: Yes.</p> </li> <li> <p><code>unknown_member</code> (bool)</p> </li> <li>If True, inserts a special row (<code>SK=0</code>) to handle orphan Foreign Keys.</li> <li> <p>Default: False.</p> </li> <li> <p><code>audit</code> (dict)</p> </li> <li>Configures audit columns:<ul> <li><code>load_timestamp</code>: Adds time of data load.</li> <li><code>source_system</code>: Source system identifier.</li> </ul> </li> </ol>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#supported-target-formats","title":"Supported Target Formats","text":""},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#spark","title":"Spark:","text":"<ul> <li>Catalog Tables: <code>catalog.schema.table</code></li> <li>Delta, CSV, JSON, ORC, and Parquet file formats.</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#pandas","title":"Pandas:","text":"<ul> <li>Supports an extended range:</li> <li><code>Parquet</code>, <code>CSV</code>, <code>JSON</code>, <code>Excel</code>, <code>Feather</code>, <code>Pickle</code>, and connection paths.</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#key-methods","title":"Key Methods","text":""},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#1-validateself-none","title":"1. <code>validate(self) -&gt; None</code>","text":"<ul> <li>Performs parameter validation before execution.</li> <li>Validates the following critical errors:</li> <li>Missing required configurations such as <code>natural_key</code> and <code>surrogate_key</code>.</li> <li>Invalid SCD type values (<code>scd_type</code>).</li> <li>Missing <code>target</code> (for SCD2).</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#2-executeself-context-enginecontext-any","title":"2. <code>execute(self, context: EngineContext) -&gt; Any</code>","text":"<ul> <li>Central method for executing the dimension pattern.</li> <li>Implements the following:</li> <li>Differentiates among SCD0, SCD1, and SCD2.</li> <li>Introduces audit columns (if enabled).</li> <li>Handles unknown member row creation (<code>unknown_member=True</code>).</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#3-_execute_scd0","title":"3. <code>_execute_scd0</code>","text":"<ul> <li>Implements Static Dimension (No updates\u2014only adds new rows).</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#4-_execute_scd1","title":"4. <code>_execute_scd1</code>","text":"<ul> <li>Applies SCD Type 1 by overwriting updated records.</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#5-_execute_scd2","title":"5. <code>_execute_scd2</code>","text":"<ul> <li>Leverages the <code>scd2</code> transformer to ensure change tracking with history maintenance.</li> </ul>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#non-obvious-behaviors-gotchas","title":"Non-Obvious Behaviors / Gotchas","text":"<ol> <li>Audit Columns:</li> <li> <p>The inclusion of <code>load_timestamp</code> and <code>source_system</code> directly depends on the <code>audit</code> dictionary. A missing <code>load_timestamp</code> key in <code>audit</code> defaults it to <code>True</code>.</p> </li> <li> <p>Target Loading Failures:</p> </li> <li> <p>Filesystem paths that are missing or have unsupported formats result in a silent fallback, where the pattern assumes no historical data exists.</p> </li> <li> <p>Special Handling for Pandas vs. Spark:</p> </li> <li>Numerous conditional logic checks are included to support both processing engines. Users must ensure suitable data formats based on the target engine.</li> </ol>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#yaml-example-configuration","title":"YAML Example Configuration","text":"<pre><code>dimension_pattern:\n  natural_key: \"customer_id\"\n  surrogate_key: \"customer_sk\"\n  scd_type: 2\n  track_cols:\n    - \"email\"\n    - \"address\"\n  target: \"warehouse.dim_customer\"\n  unknown_member: true\n  audit:\n    load_timestamp: true\n    source_system: \"CRM\"\n</code></pre>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#summary","title":"Summary","text":"<p>The <code>DimensionPattern</code> offers a robust and flexible way to manage dimension tables. By supporting SCD methodologies, audit tracking, and versatile data sources, it aligns with the needs of modern data warehousing efforts. Its design highlights ease of configuration and multi-engine compatibility but requires careful attention to configuration to avoid common pitfalls.</p>"},{"location":"context/PHASE_2B_DIMENSION_PATTERN/#next-steps","title":"Next Steps","text":"<p>Continue to analyze and document <code>fact.py</code> corresponding to Phase 2C: FACT PATTERN.</p>"},{"location":"context/PHASE_2C_FACT_PATTERN/","title":"PHASE 2C: FACT PATTERN ANALYSIS","text":""},{"location":"context/PHASE_2C_FACT_PATTERN/#overview","title":"Overview","text":"<p>The <code>FactPattern</code> is a critical component in the Odibi framework, designed to build and manage fact tables in data warehousing. Its advanced functionality offers features such as surrogate key lookups, orphan handling, grain validation, and audit column tracking. This document provides an in-depth analysis of its configuration, methods, and runtime behaviors.</p>"},{"location":"context/PHASE_2C_FACT_PATTERN/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"context/PHASE_2C_FACT_PATTERN/#basic-parameters","title":"Basic Parameters:","text":"<ol> <li><code>deduplicate</code> (bool)</li> <li>Removes duplicates before data insertion.</li> <li>Default: False.</li> <li> <p>Required: If true, <code>keys</code> parameter must be provided.</p> </li> <li> <p><code>keys</code> (list)</p> </li> <li>Columns used for deduplication (used only if <code>deduplicate = true</code>).</li> <li> <p>Example: <code>[\"order_id\", \"product_id\"]</code>.</p> </li> <li> <p><code>grain</code> (list)</p> </li> <li>Defines uniqueness. Used for grain validation to detect duplicate rows.</li> <li> <p>Required: No, but critical for validating primary key integrity.</p> </li> <li> <p><code>dimensions</code> (list of dict)</p> </li> <li> <p>Defines dimension lookup configurations for surrogate key retrieval:</p> <ul> <li><code>source_column</code>:</li> <li>The column name in the source fact to look up.</li> <li><code>dimension_table</code>:</li> <li>The name of the dimension table to perform lookups.</li> <li><code>dimension_key</code>:</li> <li>The column holding the natural key in the dimension.</li> <li><code>surrogate_key</code>:</li> <li>The surrogate key to retrieve upon a match.</li> <li><code>scd2</code> (bool):</li> <li>Option to filter rows with <code>is_current = true</code> for Slowly Changing Dimension Type 2.</li> </ul> </li> <li> <p><code>orphan_handling</code> (str)</p> </li> <li> <p>Behavior for handling unmatched dimension lookups:</p> <ul> <li><code>\"unknown\"</code>: Maps unknown value to <code>SK = 0</code>.</li> <li><code>\"reject\"</code>: Fails with an error for unmatched rows.</li> <li><code>\"quarantine\"</code>: Moves unmatched rows to a quarantine location (requires <code>quarantine</code> configuration).</li> </ul> </li> <li> <p><code>quarantine</code> (dict)</p> </li> <li> <p>Configuration for writing quarantined records:</p> <ul> <li><code>connection</code>: Target connection for quarantine writes.</li> <li><code>path</code>: Path for quarantine files (or overwrites table with desired target).</li> <li><code>add_columns</code> (dict):<ul> <li><code>_rejection_reason</code>: Add rejection reasons as metadata (<code>True</code> or <code>False</code>).</li> <li><code>_rejected_at</code>: Include a timestamp recording rejection time.</li> <li><code>_source_dimension</code>: Add the name of the dimension source.</li> </ul> </li> </ul> </li> <li> <p><code>measures</code> (list)</p> </li> <li>Data computation and transformations after dimension lookups.</li> <li> <p>Supports three types:</p> <ul> <li>As-is Pass-through Columns: <code>[\"col1\", \"col2\", ...]</code>.</li> <li>Rename Columns: <code>{\"new_column_name\": \"original_column_name\"}</code>.</li> <li>Custom Calculations: <code>{\"new_measure_name\": \"expression e.g quantity*factor...\"}</code>.</li> </ul> </li> <li> <p><code>audit</code> (dict)</p> </li> <li>Controls audit metadata:<ul> <li><code>load_timestamp</code>: Timestamp for data load.</li> <li><code>source_system</code>: Identifies the source of the data.</li> </ul> </li> </ol>"},{"location":"context/PHASE_2C_FACT_PATTERN/#example-yaml","title":"Example YAML","text":"<p>Detailed Complex Measures / Lookup Customizing outlined key-setup inclusion.</p>"},{"location":"context/PHASE_2C_FACT_PATTERN/#path","title":"Path:","text":"<p>dimensional YAML FINAL Combined.  &lt;|vq_4406|&gt;</p>"},{"location":"context/PHASE_2D_SCD2_PATTERN/","title":"PHASE 2D: SCD2 PATTERN ANALYSIS","text":""},{"location":"context/PHASE_2D_SCD2_PATTERN/#overview","title":"Overview","text":"<p>The <code>SCD2Pattern</code> within the Odibi framework is a dedicated implementation of Slowly Changing Dimension Type 2 (SCD2). This pattern ensures historical tracking for dimensional data and maintains changes as new versions of rows with <code>valid_from</code> and <code>valid_to</code> dates, and an <code>is_current</code> flag.</p>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"context/PHASE_2D_SCD2_PATTERN/#required-parameters","title":"Required Parameters:","text":"<ol> <li><code>keys</code> (list)</li> <li>Business key columns used to uniquely identify records and detect changes.</li> <li> <p>Required: Yes.</p> </li> <li> <p><code>target</code> (str)</p> </li> <li>Target table or path (Spark/Pandas supported).</li> <li>Required: Yes.</li> </ol>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#additional-parameters","title":"Additional Parameters:","text":"<ol> <li><code>time_col</code> (str)</li> <li>Timestamp column for incoming records used for versioning.</li> <li> <p>Default: Current time.</p> </li> <li> <p><code>valid_from_col</code> (str)</p> </li> <li>Column for start date (when the record became valid).</li> <li> <p>Default: <code>valid_from</code>.</p> </li> <li> <p><code>valid_to_col</code> (str)</p> </li> <li>Column for end date (when the record was invalidated or replaced).</li> <li> <p>Default: <code>valid_to</code>.</p> </li> <li> <p><code>is_current_col</code> (str)</p> </li> <li>Column marking whether a record is the latest version (<code>True</code> or <code>False</code>).</li> <li> <p>Default: <code>is_current</code>.</p> </li> <li> <p><code>track_cols</code> (Optional)</p> </li> <li>Identifies the columns being tracked. Changes in these columns mark a new version.</li> </ol>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#key-methods","title":"Key Methods","text":""},{"location":"context/PHASE_2D_SCD2_PATTERN/#1-validateself-none","title":"1. <code>validate(self) -&gt; None</code>","text":"<ul> <li>Validates the required and optional parameters.</li> <li>Validates the following critical points:</li> <li>Presence of business <code>keys</code>.</li> <li><code>target</code> parameter (must specify an existing path or table).</li> <li>Correct field keys that correspond to expected configurations, as defined by <code>SCD2Params</code>.</li> </ul>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#2-executeself-context-enginecontext-any","title":"2. <code>execute(self, context: EngineContext) -&gt; Any</code>","text":"<ul> <li>The main execution method for implementing the SCD2 processing.</li> <li>Handles:</li> <li>Parameter extraction and validation.</li> <li>Row versioning.</li> <li>Exact tracking based on <code>track_cols</code>.</li> <li>Result preparation with updated surrogate keys.</li> <li>Returns the transformed DataFrame.</li> </ul>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#workflow","title":"Workflow","text":"<ol> <li>SCD2 Logic Pipeline:</li> <li>The <code>SCD2Params</code> configuration integrates closely with the <code>scd2</code> transformer to enforce history-tracking.</li> <li> <p>Records with the same business key (<code>keys</code>) and changes to <code>track_cols</code> are invalidated and new rows are generated.</p> </li> <li> <p>Integration with Engines:</p> </li> <li> <p>The pattern distinguishes between Spark and Pandas DataFrame operations, ensuring functionality across both engines.</p> </li> <li> <p>Performance and Optimization:</p> </li> <li>Enables manual filtering for current valid rows using <code>is_current</code> and related fields.</li> <li>Utilizes efficient data filtering functions specific to Spark or Pandas (<code>count</code> or <code>len</code>).</li> </ol>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#example-yaml-configuration","title":"Example YAML Configuration","text":"<pre><code>pattern:\n  type: scd2\n  params:\n    keys: [\"customer_id\"]\n    target: \"dim_customer\"\n    track_cols: [\"email\", \"address\"]\n    valid_from_col: \"valid_from\"\n    valid_to_col: \"valid_to\"\n    is_current_col: \"is_current\"\n    time_col: \"update_time\"\n</code></pre>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#observations-and-gotchas","title":"Observations and Gotchas","text":""},{"location":"context/PHASE_2D_SCD2_PATTERN/#1-target-definition","title":"1. Target Definition:","text":"<ul> <li>The <code>target</code> parameter must be correctly set to a valid table or path. If the target does not exist, the system raises runtime errors.</li> </ul>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#2-key-constraints","title":"2. Key Constraints:","text":"<ul> <li>The presence of <code>keys</code> is critical, as these determine the uniqueness and change tracking across records.</li> </ul>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#3-auto-filled-columns","title":"3. Auto-Filled Columns:","text":"<ul> <li>The configuration allows defaults for <code>valid_from_col</code>, <code>valid_to_col</code>, and <code>is_current_col</code>, reducing manual overhead when aligning column names across datasets.</li> </ul>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#summary","title":"Summary","text":"<p>The <code>SCD2Pattern</code> is an essential part of the Odibi framework, providing robust support for Slowly Changing Dimension Type 2 implementations. Its ability to handle complex historical data tracking with minimal configuration makes it a critical choice for data warehouse scenarios where data change over time needs to be preserved.</p>"},{"location":"context/PHASE_2D_SCD2_PATTERN/#next-steps","title":"Next Steps","text":"<p>Proceed to analyze and document the <code>merge.py</code> file corresponding to Phase 2E: MERGE PATTERN.</p>"},{"location":"context/PHASE_2E_MERGE_PATTERN/","title":"PHASE 2E: MERGE PATTERN ANALYSIS","text":""},{"location":"context/PHASE_2E_MERGE_PATTERN/#overview","title":"Overview","text":"<p>The <code>MergePattern</code> in the Odibi framework enables streamlined upsert and merge operations for integrating changes into existing datasets. It supports flexible strategies for upserts, append-only operations, and matching row deletions, ensuring seamless dataset updates across both batch and streaming data sources.</p>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"context/PHASE_2E_MERGE_PATTERN/#required-parameters","title":"Required Parameters:","text":"<ol> <li><code>keys</code> (list):</li> <li>List of columns to match between source and target datasets for merge operations.</li> <li>Required: Yes.</li> <li> <p>Example: <code>[\"id\", \"customer_id\"]</code>.</p> </li> <li> <p><code>target</code> (str or <code>path</code>):</p> </li> <li>Specifies the target dataset (table name or file path).</li> <li>Required: Yes.</li> </ol>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#optional-parameters","title":"Optional Parameters:","text":"<ol> <li><code>strategy</code> (str):</li> <li>Defines the merge strategy.<ul> <li>Available Strategies:</li> <li><code>upsert</code>: Updates existing records and inserts new ones.</li> <li><code>append_only</code>: Only appends new records (ignores existing ones).</li> <li><code>delete_match</code>: Removes rows present both in the source and target.</li> </ul> </li> <li>Default: <code>upsert</code>.</li> </ol>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#supported-engines","title":"Supported Engines","text":"<p>The <code>MergePattern</code> works with both Spark and Pandas engines, adapting behavior accordingly to make use of respective utilities such as <code>count</code> or <code>len</code> for source and target data operations.</p>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#key-methods","title":"Key Methods","text":""},{"location":"context/PHASE_2E_MERGE_PATTERN/#1-validateself-none","title":"1. <code>validate(self) -&gt; None</code>","text":"<ul> <li>Verifies that the required <code>keys</code> and <code>target</code> parameters are supplied.</li> <li>Validates configuration compatibility with the Merge transformer (<code>merge_transformer.MergeParams</code>).</li> </ul>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#2-executeself-context-enginecontext-any","title":"2. <code>execute(self, context: EngineContext) -&gt; Any</code>","text":"<ul> <li>Implements the merge pattern by invoking the <code>merge</code> transformer with validated parameters:</li> <li>Extracts relevant parameters from <code>params</code>.</li> <li>Applies the Merge logic based on the provided strategy (<code>upsert</code>, <code>append_only</code>, etc.).</li> <li>Logs detailed execution metrics and errors.</li> <li>Returns the resulting DataFrame.</li> </ul>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#workflow","title":"Workflow","text":"<ol> <li>Validation:</li> <li>Parameters such as <code>target</code> and <code>keys</code> are mandatory and validated before execution.</li> <li> <p>Ensures that configuration adheres to the rules established by the <code>merge</code> transformer.</p> </li> <li> <p>Strategy-Driven Execution:</p> </li> <li> <p>Depending on the <code>strategy</code>, the pattern adapts its behavior:</p> <ul> <li><code>upsert</code>: Combines updates and inserts.</li> <li><code>append_only</code>: Appends new records while leaving existing records untouched.</li> <li><code>delete_match</code>: Deletes rows that exist both in the source and target datasets.</li> </ul> </li> <li> <p>Compatibility:</p> </li> <li>Accommodates both <code>target</code> and <code>path</code> fields to ensure broader compatibility across different usage scenarios.</li> </ol>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#example-yaml-configuration","title":"Example YAML Configuration","text":"<pre><code>pattern:\n  type: merge\n  params:\n    keys: [\"id\"]\n    target: \"user_data\"\n    strategy: \"upsert\"\n</code></pre>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#observations-and-gotchas","title":"Observations and Gotchas","text":"<ol> <li>Target Configuration Compatibility:</li> <li> <p>The presence of <code>target</code> or <code>path</code> is essential. Omitting this parameter results in a validation error.</p> </li> <li> <p>Invalid Keys:</p> </li> <li> <p>If the <code>keys</code> parameter is missing, validation will fail. Ensure that keys correctly map between source and target schema.</p> </li> <li> <p>Strategy Specifics:</p> </li> <li>Each merge strategy must be clearly specified. The default behavior (<code>upsert</code>) may create unintended results if the desired strategy is not explicitly defined.</li> </ol>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#summary","title":"Summary","text":"<p>The <code>MergePattern</code> provides a robust mechanism for performing upsert and merge operations in the Odibi framework. Its flexible configuration options and support for multiple engines make it an essential tool for managing dynamic datasets, especially in systems requiring regular updates or synchronization between source and target datasets.</p>"},{"location":"context/PHASE_2E_MERGE_PATTERN/#next-steps","title":"Next Steps","text":"<p>Continue to analyze and document the <code>date_dimension.py</code> file as part of Phase 2F: DATE DIMENSION PATTERN.</p>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/","title":"PHASE 2F: DATE DIMENSION PATTERN ANALYSIS","text":""},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#overview","title":"Overview","text":"<p>The <code>DateDimensionPattern</code> in the Odibi framework is designed to generate a comprehensive date dimension table with pre-calculated attributes. These attributes are invaluable for Business Intelligence (BI) and reporting, supporting a wide range of analytical queries.</p>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#required-parameters","title":"Required Parameters:","text":"<ol> <li><code>start_date</code> (str)</li> <li>The start date for the date dimension in <code>YYYY-MM-DD</code> format.</li> <li>Required: Yes.</li> <li> <p>Example: <code>\"2024-01-01\"</code>.</p> </li> <li> <p><code>end_date</code> (str)</p> </li> <li>The end date for the date dimension in <code>YYYY-MM-DD</code> format.</li> <li>Required: Yes.</li> <li>Example: <code>\"2024-12-31\"</code>.</li> </ol>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#optional-parameters","title":"Optional Parameters:","text":"<ol> <li><code>date_key_format</code> (str)</li> <li>Format for the surrogate key (<code>date_sk</code>).</li> <li> <p>Default: <code>\"yyyyMMdd\"</code>.</p> </li> <li> <p><code>fiscal_year_start_month</code> (int)</p> </li> <li>Specifies the starting month of the fiscal year.</li> <li> <p>Default: <code>1</code> (January). Range: 1\u201312.</p> </li> <li> <p><code>unknown_member</code> (bool)</p> </li> <li>Adds an unknown date row (<code>date_sk=0</code>) to handle null or missing dates.</li> <li>Default: False.</li> </ol>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#generated-columns","title":"Generated Columns","text":"<ol> <li>Surrogate Key</li> <li> <p><code>date_sk</code>: Integer key (default is in <code>YYYYMMDD</code> format).</p> </li> <li> <p>Date Attributes</p> </li> <li><code>full_date</code>: Actual date.</li> <li><code>day_of_week</code>: Day name.</li> <li><code>day_of_week_num</code>: Day number (Monday = 1, Sunday = 7).</li> <li><code>day_of_month</code>: Day of the month (1\u201331).</li> <li><code>day_of_year</code>: Day of the year (1\u2013366).</li> <li> <p><code>is_weekend</code>: Boolean indicator for weekends.</p> </li> <li> <p>Calendar Periods</p> </li> <li><code>week_of_year</code>: Week of the year (ISO standard, 1\u201353).</li> <li><code>month</code>: Month number (1\u201312).</li> <li><code>month_name</code>: Month name.</li> <li><code>quarter</code>: Quarter of the year (1\u20134).</li> <li><code>quarter_name</code>: Quarter designation (<code>Q1</code>, <code>Q2</code>, etc.).</li> <li><code>year</code>: Calendar year.</li> <li><code>fiscal_year</code>: Fiscal year based on the specified fiscal start month.</li> <li> <p><code>fiscal_quarter</code>: Fiscal quarter (1\u20134).</p> </li> <li> <p>Boundary Indicators</p> </li> <li><code>is_month_start</code>: First day of the month.</li> <li><code>is_month_end</code>: Last day of the month.</li> <li><code>is_year_start</code>: First day of the year.</li> <li><code>is_year_end</code>: Last day of the year.</li> </ol>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#key-methods","title":"Key Methods","text":""},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#1-validateself-none","title":"1. <code>validate(self) -&gt; None</code>","text":"<ul> <li>Ensures the presence of valid <code>start_date</code> and <code>end_date</code>.</li> <li>Verifies that:</li> <li>Dates are in <code>YYYY-MM-DD</code> format.</li> <li><code>start_date</code> is before or equal to <code>end_date</code>.</li> <li><code>fiscal_year_start_month</code> is an integer between 1 and 12.</li> </ul>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#2-executeself-context-enginecontext-any","title":"2. <code>execute(self, context: EngineContext) -&gt; Any</code>","text":"<ul> <li>Generates the Date Dimension table using either Spark or Pandas:</li> <li>Pandas:<ul> <li>Uses <code>pandas.date_range</code> to generate rows between <code>start_date</code> and <code>end_date</code>.</li> <li>Populates all supported date attributes.</li> </ul> </li> <li>Spark:<ul> <li>Uses distributed processing to efficiently generate the date dimension.</li> </ul> </li> </ul>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#workflow","title":"Workflow","text":"<ol> <li>Validation:</li> <li> <p>Validates <code>start_date</code>, <code>end_date</code>, and <code>fiscal_year_start_month</code> for integrity.</p> </li> <li> <p>Data Generation:</p> </li> <li> <p>Creates records for each date between <code>start_date</code> and <code>end_date</code>, pre-calculating attribute fields.</p> </li> <li> <p>Integration:</p> </li> <li>Supports generating the dimension using Pandas or Spark. It chooses the appropriate implementation based on the <code>engine_type</code> in the <code>EngineContext</code>.</li> </ol>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#example-yaml-configuration","title":"Example YAML Configuration","text":"<pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2024-01-01\"\n    end_date: \"2024-12-31\"\n    date_key_format: \"yyyyMMdd\"\n    fiscal_year_start_month: 4\n    unknown_member: true\n</code></pre>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#observations-and-gotchas","title":"Observations and Gotchas","text":"<ol> <li>Date Range Validations:</li> <li>If <code>start_date</code> is after <code>end_date</code>, validation fails immediately.</li> <li> <p>Dates must follow the <code>YYYY-MM-DD</code> format.</p> </li> <li> <p>Unknown Date Member:</p> </li> <li> <p>Setting <code>unknown_member=True</code> generates a placeholder row with <code>date_sk=0</code> for missing dates.</p> </li> <li> <p>Fiscal Year Handling:</p> </li> <li>The fiscal year calculations adjust based on the starting month. For example:<ul> <li>Given a fiscal start month of <code>4</code> (April):</li> <li>Fiscal Quarters:<ul> <li>Jan\u2013Mar = Q4 of the previous year.</li> <li>Apr\u2013Jun = Q1 of the current year.</li> </ul> </li> </ul> </li> </ol>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#summary","title":"Summary","text":"<p>The <code>DateDimensionPattern</code> provides a simple yet powerful solution for generating date dimensions essential for BI and analytics. Its flexibility in configuration and comprehensive attribute generation make it a valuable asset in any data warehouse or ETL process.</p>"},{"location":"context/PHASE_2F_DATE_DIMENSION_PATTERN/#next-steps","title":"Next Steps","text":"<p>The analysis and documentation for the Patterns phase (PHASE 2) have now been completed as planned. The next step is to proceed with documenting Transformers.</p>"},{"location":"context/PHASE_2_PATTERNS/","title":"PHASE 2: PATTERNS ANALYSIS","text":""},{"location":"context/PHASE_2_PATTERNS/#overview","title":"Overview","text":"<p>This phase explores the <code>patterns</code> module in the odibi framework. It focuses on the functionality offered by the module, particularly the <code>AggregationPattern</code> defined in <code>aggregation.py</code>. This pattern provides standardized and efficient aggregation capabilities, including time-grain rollups, incremental merges, and audit column addition, across multiple execution engines (Spark and Pandas).</p>"},{"location":"context/PHASE_2_PATTERNS/#key-component-and-behavior-observations","title":"Key Component and Behavior Observations","text":""},{"location":"context/PHASE_2_PATTERNS/#aggregationpattern-class","title":"AggregationPattern Class","text":""},{"location":"context/PHASE_2_PATTERNS/#key-features","title":"Key Features:","text":"<ol> <li> <p>Declarative Aggregation:</p> <ul> <li>Users define <code>grain</code> (grouping columns) and <code>measures</code> (aggregations).</li> <li>SQL-like expressions (e.g., \"SUM(amount)\") facilitate seamless aggregation.</li> </ul> </li> <li> <p>Incremental Aggregation:</p> <ul> <li>Handles the merging of new data with existing aggregations, customizable via <code>merge_strategy</code> (<code>replace</code>, <code>sum</code>, <code>min</code>, or <code>max</code>).</li> </ul> </li> <li> <p>Time Rollups:</p> <ul> <li>Adds support for hierarchical grain levels (e.g., daily, weekly, or monthly summaries derived from timestamp grains).</li> </ul> </li> <li> <p>Audit Columns:</p> <ul> <li>Supports additional metadata columns (e.g., <code>load_timestamp</code>, <code>source_system</code>).</li> </ul> </li> </ol>"},{"location":"context/PHASE_2_PATTERNS/#high-level-workflow","title":"High-Level Workflow","text":"<ol> <li> <p>Validation:</p> <ul> <li>Validates grain, measures, and optional incremental configuration.</li> <li>Ensures required parameters exist and have valid data types.</li> <li>Issues warnings such as undefined merge strategies or missing grain/measure definitions.</li> </ul> </li> <li> <p>Execution:</p> <ul> <li>Steps:</li> <li>Retrieve source DataFrame.</li> <li>Perform aggregation using engine-specific logic:<ul> <li>Spark: Executes aggregation using PySpark with <code>groupBy</code> and aggregation expressions.</li> <li>Pandas: Executes aggregation via DuckDB SQL queries on in-memory DataFrames.</li> </ul> </li> <li>Apply incremental logic (if configured), merging with existing aggregated results.</li> <li>Add audit columns like <code>load_timestamp</code> or <code>source_system</code>.</li> </ul> </li> </ol>"},{"location":"context/PHASE_2_PATTERNS/#engine-specific-implementations","title":"Engine-Specific Implementations","text":""},{"location":"context/PHASE_2_PATTERNS/#spark-aggregation","title":"Spark Aggregation:","text":"<ul> <li>Uses <code>PySpark</code> APIs for loading, grouping, and aggregating data.</li> <li>Example:   <pre><code>result = df.groupBy(*grain_cols).agg(*agg_exprs)\n</code></pre></li> <li><code>HAVING</code> clauses are applied using Spark's <code>filter</code> API.</li> </ul>"},{"location":"context/PHASE_2_PATTERNS/#pandas-aggregation","title":"Pandas Aggregation:","text":"<ul> <li>Utilizes DuckDB integration through <code>context.sql()</code> to run fast in-process SQL queries.</li> <li>Example SQL:   <pre><code>SELECT date_sk, product_sk, SUM(total_amount) AS total_revenue\nFROM df\nGROUP BY date_sk, product_sk\n</code></pre></li> </ul>"},{"location":"context/PHASE_2_PATTERNS/#non-obvious-behaviors","title":"Non-Obvious Behaviors","text":"<ol> <li>Incremental Aggregation Import Paths:</li> <li> <p>Handles cross-file-target resolution through context connections (<code>engine.connections</code>) and supports path resolution for various storage types (e.g., Delta, Parquet, CSV).</p> </li> <li> <p>Exception-Friendly Row Count:</p> </li> <li> <p>Fails non-critically when determining row counts for unsupported engines (<code>try/except</code> block ensures fallback execution).</p> </li> <li> <p>Grain-Aware Merge Strategies:</p> </li> <li> <p>Merge behaviors (replace/sum/min/max) are abstracted with separate implementations for Spark and Pandas.</p> </li> <li> <p>Dynamic SQL-HAVING Clauses:</p> </li> <li>Expressions in <code>having</code> clauses are crafted dynamically using user configurations.</li> <li>Pandas implementation fallback relies on DuckDB.</li> </ol>"},{"location":"context/PHASE_2_PATTERNS/#observations","title":"Observations","text":"<ol> <li>Engine Parity and Time Optimization:</li> <li>Strong adherence to the \"Engine Parity\" principle across Pandas/DuckDB and PySpark.</li> <li> <p>Efficient time optimizations include:</p> <ul> <li>Hierarchical roll-ups for time-series data.</li> <li>Incremental merges to reuse prior results as base tables.</li> </ul> </li> <li> <p>Complexity in Validation:</p> </li> <li> <p>Multiple validation rules exist for grain/measure combinations that require careful configuration to avoid runtime failures during incremental merge logic.</p> </li> <li> <p>Flexible Audit Support:</p> </li> <li><code>audit_config</code> allows for operational observability with minimal metadata cost (only when needed).</li> </ol>"},{"location":"context/PHASE_2_PATTERNS/#next-steps","title":"Next Steps","text":"<p>Transition to Phase 3 and analyze the transformations within the <code>transformers</code> module to understand intermediate and advanced data manipulation patterns.</p>"},{"location":"context/PHASE_3_TRANSFORMERS/","title":"PHASE 3: TRANSFORMERS ANALYSIS","text":""},{"location":"context/PHASE_3_TRANSFORMERS/#overview","title":"Overview","text":"<p>This phase examines the <code>transformers</code> module, focusing on the <code>MergeTransformer</code> system. This transformer is integral to dynamic ETL pipelines with robust configurations supporting complex data management tasks such as GDPR compliance, SCD Type 1 implementations, and multi-strategy merging across Spark and Pandas engine contexts.</p>"},{"location":"context/PHASE_3_TRANSFORMERS/#key-component-and-behavior-observations","title":"Key Component and Behavior Observations","text":""},{"location":"context/PHASE_3_TRANSFORMERS/#mergetransformer-class","title":"<code>MergeTransformer</code> Class","text":""},{"location":"context/PHASE_3_TRANSFORMERS/#key-features","title":"Key Features:","text":"<ol> <li>Context Compatibility:</li> <li>Works seamlessly with Spark (Delta Lake) and Pandas (fallback to DuckDB for scalability).</li> <li> <p>Automatically adapts to the available execution engine.</p> </li> <li> <p>Flexible Merge Strategies:</p> </li> <li>UPSERT: Inserts new records and updates existing ones.</li> <li>APPEND_ONLY: Adds new records without modifying existing entries.</li> <li> <p>DELETE_MATCH: Removes records in the target that match keys in the source.</p> </li> <li> <p>Audit Columns:</p> </li> <li> <p>Tracks data changes using customizable <code>created_col</code> and <code>updated_col</code> parameters for traceability.</p> </li> <li> <p>Advanced Optimization:</p> </li> <li>Delta-specific features: Liquid Clustering, Z-Order indexing, and automatic schema evolution.</li> <li> <p>Post-merge optimizations ensure high query performance for large-scale data.</p> </li> <li> <p>Target Path Resolution:</p> </li> <li>Supports connection and path-based configurations for storage systems like ADLS or local Delta tables.</li> <li>Dynamic validation against project-defined connections.</li> </ol>"},{"location":"context/PHASE_3_TRANSFORMERS/#high-level-workflow","title":"High-Level Workflow","text":"<ol> <li> <p>Validation and Configuration:</p> <ul> <li>Parameter validation ensures correct configurations, including:</li> <li>Mandatory target keys.</li> <li>Conflicts between multiple definitions (e.g., <code>connection</code> and <code>path</code> vs. <code>target</code>).</li> <li>Logical constraints (e.g., <code>strategy=delete_match</code> shouldn't include additional audit columns).</li> </ul> </li> <li> <p>Source Loading:</p> <ul> <li>Dynamically unwraps runtime context to extract the source DataFrame.</li> <li>Calculates initial row counts for debugging and performance telemetry.</li> </ul> </li> <li> <p>Conditional Merge Execution:</p> <ul> <li>Executes merge operations based on context:</li> <li>Spark:<ul> <li>Uses Delta Lake for ACID guarantees.</li> <li>Supports advanced SQL-like conditional clauses for fine-grained control.</li> </ul> </li> <li>Pandas:<ul> <li>Leverages DuckDB for SQL fusion where possible or defaults to efficient Pandas routines.</li> </ul> </li> </ul> </li> <li> <p>Result Optimization:</p> <ul> <li>Automatically adjusts execution plans:</li> <li>Schema evolution using Spark's <code>autoMerge.enabled</code> configuration.</li> <li>Post-write optimizations like Delta's <code>ZORDER BY</code> and clustering.</li> </ul> </li> </ol>"},{"location":"context/PHASE_3_TRANSFORMERS/#non-obvious-behaviors","title":"Non-Obvious Behaviors","text":"<ol> <li>Strategy Complexity:</li> <li>Combining audit column logic with incremental strategies (e.g., append-only or upsert) requires careful handling to avoid overwriting historical records inappropriately.</li> <li> <p>While feature-rich, complex configurations may lead to operational overhead.</p> </li> <li> <p>DuckDB Fallback:</p> </li> <li> <p>Falls back gracefully to DuckDB for non-Spark environments but executes logic natively using Parquet paths (<code>delete_match</code> skips writes on non-existent targets).</p> </li> <li> <p>Advanced Table Variants:</p> </li> <li> <p>Audit column logic dynamically creates or ignores columns depending on the initial schema to ensure compliance with SCD principles.</p> </li> <li> <p>Unsupported Contexts:</p> </li> <li>Throws detailed errors when runtime context lacks required functions or structure (real-world example: legacy Pandas setups with missing connections).</li> </ol>"},{"location":"context/PHASE_3_TRANSFORMERS/#observations","title":"Observations","text":"<ol> <li>Engine Parity for Lifecycle Guarantees:</li> <li>High adherence to parity, enabling robust lifecycle execution regardless of the backend engine.</li> <li> <p>Detailed failure context passed through structured logging simplifies triage during pipeline errors.</p> </li> <li> <p>SCD and GDPR Readiness:</p> </li> <li> <p>Predefined workflows address common compliance scenarios (e.g., \"Right to be Forgotten\").</p> </li> <li> <p>Clustering Ambiguity:</p> </li> <li>Significant emphasis on Delta clustering likely over-complicates Pandas fallbacks.</li> <li>Documentation suggests carefully validating context-specific logic when switching between engines.</li> </ol>"},{"location":"context/PHASE_3_TRANSFORMERS/#next-steps","title":"Next Steps","text":"<p>Transition to Phase 4 for inspecting <code>Connections</code>, focusing on runtime behavior and identifying non-obvious connection management features and behaviors.</p>"},{"location":"context/PHASE_4_CONNECTIONS/","title":"PHASE 4: CONNECTIONS ANALYSIS","text":""},{"location":"context/PHASE_4_CONNECTIONS/#overview","title":"Overview","text":"<p>This phase focuses on the <code>connections</code> module, which provides interfaces and implementations for managing connections to various data sources (e.g., local storage, Azure Data Lake Storage, Databricks, and HTTP endpoints). The primary goal is to encapsulate the complexity of connecting to backends in a consistent and modular manner.</p>"},{"location":"context/PHASE_4_CONNECTIONS/#key-component-and-behavior-observations","title":"Key Component and Behavior Observations","text":""},{"location":"context/PHASE_4_CONNECTIONS/#baseconnection","title":"<code>BaseConnection</code>","text":"<p>The <code>BaseConnection</code> abstract class serves as the foundation for all connection types within the Odibi framework. Subclasses must implement two primary methods: 1. <code>get_path(relative_path: str)</code>:     - Returns the absolute path of a resource based on a given relative path.     - Ensures that the path resolution logic is consistent across all connection types.</p> <ol> <li><code>validate()</code>:<ul> <li>Validates connection-specific configuration details.</li> <li>Raises <code>ConnectionError</code> if the configuration is invalid, offering a structured way to debug issues.</li> </ul> </li> </ol>"},{"location":"context/PHASE_4_CONNECTIONS/#behavior","title":"Behavior:","text":"<ul> <li><code>BaseConnection</code> enforces a common contract ensuring all derived connection implementations support path resolution and validation properties.</li> <li>It is an abstract class, preventing direct instantiation, which enforces adherence to the framework's design principles.</li> </ul>"},{"location":"context/PHASE_4_CONNECTIONS/#non-obvious-behaviors","title":"Non-Obvious Behaviors:","text":"<ul> <li>The <code>get_path</code> method abstracts not only local path resolution for connection-based configurations but facilitates remote resource handling through varying connection services (e.g., MLOps data catalogs, cloud buckets).</li> </ul>"},{"location":"context/PHASE_4_CONNECTIONS/#observations","title":"Observations","text":"<ol> <li>Decoupled Core Logic:</li> <li> <p>By defining an abstract interface, Odibi allows downstream implementations (e.g., <code>AzureConnection</code>, <code>Local</code>) to tailor behavior without modifying the core architecture.</p> </li> <li> <p>Runtime Integrity:</p> </li> <li>Implementations must validate themselves during runtime; this ensures the correctness of connection configurations before their use.</li> </ol>"},{"location":"context/PHASE_4_CONNECTIONS/#next-analysis-step","title":"Next Analysis Step:","text":"<p>I will analyze a few specific connection implementations (e.g., <code>azure_adls.py</code> and <code>http.py</code>) to understand how this base interface is extended to cater to diverse data connections. By doing so, I aim to identify shared patterns and unique behaviors supporting connection operations.</p>"},{"location":"context/PHASE_5_RUNTIME/","title":"PHASE 5: RUNTIME ANALYSIS","text":""},{"location":"context/PHASE_5_RUNTIME/#overview","title":"Overview","text":"<p>This phase delves into how runtime processes are managed by the Odibi framework. Central to the Odibi design are the <code>Context</code> objects which encapsulate execution environments for different data engines like Spark, Pandas, and Polars. These contexts allow seamless integration, ensuring runtime portability and efficient execution across diverse data processing backends.</p>"},{"location":"context/PHASE_5_RUNTIME/#key-components-and-behavior-observations","title":"Key Components and Behavior Observations","text":""},{"location":"context/PHASE_5_RUNTIME/#enginecontext","title":"<code>EngineContext</code>","text":"<ul> <li>Role:</li> <li>Serves as an intermediary between the user\u2019s logic and specific execution engines, such as Spark or Pandas.</li> <li>Maintains local states like the current DataFrame and SQL history while integrating with the global execution context.</li> <li>Key Features:</li> <li>SQL Interface: Executes SQL queries on a DataFrame by registering it as a temporary view.<ul> <li>Spark compatibility enables distributed computation.</li> <li>Thread-safe implementation ensures proper execution in parallel processing scenarios.</li> </ul> </li> <li>Schema Introspection:<ul> <li>Provides schema insights via global context.</li> <li>Supports issues like PII metadata tracking during runtime.</li> </ul> </li> <li>Flexibility:<ul> <li>Supports various execution engines as backends (Spark, Pandas, Polars).</li> <li>Permits chaining transformations by returning new contexts, complete with SQL history sharing.</li> </ul> </li> </ul>"},{"location":"context/PHASE_5_RUNTIME/#non-obvious-behaviors","title":"Non-Obvious Behaviors:","text":"<ol> <li>Temporary View Naming:<ul> <li>A unique naming convention for temporary views is generated using thread and counter information to avoid collisions during parallelism.</li> </ul> </li> <li>Flexible History Management:<ul> <li>While users execute and chain multiple SQL queries (<code>sql()</code>), the queried history accumulates in the context instance.</li> </ul> </li> </ol>"},{"location":"context/PHASE_5_RUNTIME/#context-abstract-class","title":"<code>Context</code> (Abstract Class)","text":"<ul> <li>Provides a unified interface for managing datasets across transformations.</li> <li>Abstracts operations like dataset registration, metadata retrieval, and resource cleanup across diverse engines.</li> <li>Important Methods:</li> <li>register: Adds a dataset and optional metadata.</li> <li>get: Fetches a registered dataset or raises a <code>KeyError</code> if the dataset does not exist.</li> <li>unregister: Removes datasets and optionally performs cleanup.</li> <li>Implementations must define all abstract methods like <code>list_names</code>, <code>clear</code>, etc.</li> </ul>"},{"location":"context/PHASE_5_RUNTIME/#observations","title":"Observations:","text":"<ol> <li>By abstracting common, repetitive runtime tasks, <code>Context</code> helps reduce duplication across backend-specific implementations.</li> <li>A base validation structure (e.g., <code>unregister</code>) improves resource management, avoiding accidental memory overhead.</li> </ol>"},{"location":"context/PHASE_5_RUNTIME/#pandascontext-polarscontext-and-sparkcontext","title":"<code>PandasContext</code>, <code>PolarsContext</code>, and <code>SparkContext</code>","text":"<p>Handles runtime execution for their respective engines: 1. <code>PandasContext</code>:     - Designed for lightweight, in-memory processing.     - Supports native Pandas DataFrames and chunked computation using iterators.     - Leverages DuckDB where needed for SQL operations. 2. <code>PolarsContext</code>:     - Provides similar functionality tailored to Polars DataFrames.     - Currently lacks extensive parallelism optimizations but maintains parity with PandasContext for functional compatibility. 3. <code>SparkContext</code>:     - Built around Spark\u2019s DataFrame API and Delta Lake architecture.     - Includes strict validation for view names to maintain SQL compliance.     - Implements thread-safe operations using locks.     - Offers automatic handling of temporary views with explicit cleanup to avoid memory bloat.     - Advanced SQL compatibility means more dynamic runtime capabilities compared to Pandas.</p>"},{"location":"context/PHASE_5_RUNTIME/#critical-runtime-insights","title":"Critical Runtime Insights","text":""},{"location":"context/PHASE_5_RUNTIME/#challenges-and-considerations","title":"Challenges and Considerations:","text":"<ol> <li> <p>Engine-Specific Designs:</p> <ul> <li>Different backends have their unique structures (Spark, Pandas, Polars).</li> <li>A transparent <code>create_context()</code> ensures the correct context instantiates, but switching engines mid-run requires control to be refined further in client scripts.</li> </ul> </li> <li> <p>State Management:</p> <ul> <li>Purposive separation of runtime state for DataFrames enhances thread safety while maintaining lineage records and debugging transparency.</li> </ul> </li> </ol>"},{"location":"context/PHASE_5_RUNTIME/#observations_1","title":"Observations:","text":"<ol> <li> <p>Shared Abstractions, Divergent Implementations:</p> <ul> <li>The Panda and Polars contexts largely mirror each other, highlighting the community's shift towards supporting Apache Arrow-compatible tech. Polars may become more relevant for users as it scales better in single-node setups.</li> </ul> </li> <li> <p>Advanced Parallelism:</p> <ul> <li>Threading awareness in Spark/Pandas execution patterns ensures that workflows remain performant while maintaining data integrity.</li> </ul> </li> </ol>"},{"location":"context/PHASE_5_RUNTIME/#next-steps","title":"Next Steps","text":"<p>Proceed to Phase 6 to investigate workflows, which govern the orchestration of multiple nodes and how tasks are executed in parallel while ensuring lineage, validation, and error recovery features.</p>"},{"location":"context/PHASE_6_WORKFLOWS/","title":"PHASE 6: WORKFLOWS ANALYSIS","text":""},{"location":"context/PHASE_6_WORKFLOWS/#overview","title":"Overview","text":"<p>Workflows define the orchestration of multiple tasks and how these steps interconnect. The primary objective of this phase is to explore how workflow orchestration is represented and implemented in the Odibi framework. This document specifically focuses on the integration with orchestration frameworks like Apache Airflow.</p>"},{"location":"context/PHASE_6_WORKFLOWS/#key-components-and-behavior-observations","title":"Key Components and Behavior Observations","text":""},{"location":"context/PHASE_6_WORKFLOWS/#airflowexporter","title":"<code>AirflowExporter</code>","text":"<p>The <code>AirflowExporter</code> class is responsible for dynamically generating DAG (Directed Acyclic Graph) code for Apache Airflow from Odibi pipeline configurations.</p>"},{"location":"context/PHASE_6_WORKFLOWS/#core-features","title":"Core Features:","text":"<ol> <li>DAG Code Generation:</li> <li>Dynamically generates an Airflow DAG Python script for a specific pipeline.</li> <li> <p>Utilizes the templating system <code>jinja2</code> to generate the DAG structure using a pre-defined template.</p> </li> <li> <p>Process Workflow Nodes:</p> </li> <li>Parses nodes defined in a pipeline and converts them into Airflow tasks (<code>BashOperators</code>).</li> <li> <p>Establishes upstream/downstream dependencies between tasks.</p> </li> <li> <p>Dynamic Templating:</p> </li> <li> <p>Uses pipeline task details, such as a task's name, upstream dependencies, and layer, to create a fully functional Airflow DAG.</p> </li> <li> <p>Sanitization of Task Names:</p> </li> <li>Node/task names are sanitized to ensure they conform to naming conventions used in Airflow, e.g., replacing invalid characters with underscores.</li> </ol>"},{"location":"context/PHASE_6_WORKFLOWS/#non-obvious-behaviors","title":"Non-Obvious Behaviors:","text":"<ol> <li>Automatic Retry Handling:</li> <li>Incorporates retry logic directly from the Odibi project configuration (<code>Config.retry.max_attempts</code>).</li> <li> <p>Ensures alignment between Odibi pipelines and Airflow DAGs in terms of retry behavior.</p> </li> <li> <p>Pipeline-Metadata-Driven Context:</p> </li> <li>DAGs inherit metadata and descriptions directly from the pipeline configuration, embedding operational details such as owner information, node dependencies, and layers into generated DAGs.</li> </ol>"},{"location":"context/PHASE_6_WORKFLOWS/#observations","title":"Observations","text":"<ol> <li>Role of Templates:</li> <li>The <code>AIRFLOW_DAG_TEMPLATE</code> defines a highly structured way of turning interpreted data-processing steps into Airflow-compatible DAG files.</li> <li> <p>Nodes are rendered with explicit references to their upstream dependencies, reducing the need for manual edits to integrate Odibi pipelines with Airflow.</p> </li> <li> <p>Ease of Automation:</p> </li> <li>The <code>generate_code</code> function centralizes the generation process and reduces manual DAG creation, especially for complex pipelines with numerous tasks.</li> </ol>"},{"location":"context/PHASE_6_WORKFLOWS/#next-analysis-step","title":"Next Analysis Step:","text":"<p>The next exploration will focus on <code>dagster.py</code>, which likely contains orchestration for Dagster\u2014a modern orchestration tool. This will provide complementary insights into orchestration workflows designed with alternative execution frameworks.</p>"},{"location":"context/PHASE_7_GOTCHAS/","title":"PHASE 7: GOTCHAS ANALYSIS","text":""},{"location":"context/PHASE_7_GOTCHAS/#overview","title":"Overview","text":"<p>This phase identifies potential \"gotchas\" or pitfalls within the Odibi framework. These can be error-prone areas, unconventional behaviors, or design decisions that require careful attention at runtime. The analysis centers around the custom exceptions and error-handling mechanisms defined in the <code>exceptions.py</code> file.</p>"},{"location":"context/PHASE_7_GOTCHAS/#key-components-and-observations","title":"Key Components and Observations","text":""},{"location":"context/PHASE_7_GOTCHAS/#odibiexception","title":"<code>OdibiException</code>","text":"<ul> <li>Purpose: Serves as the base class for all exceptions in the framework, providing a consistent structure for error types.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#non-obvious-behaviors","title":"Non-Obvious Behaviors:","text":"<ol> <li>Error Formatting:</li> <li>Many derived exceptions include <code>_format_error</code> methods to customize the error message format, ensuring that runtime exceptions are verbose and user-friendly.</li> <li>Error messages often include actionable suggestions which simplify debugging.</li> </ol>"},{"location":"context/PHASE_7_GOTCHAS/#specific-exceptions","title":"Specific Exceptions","text":""},{"location":"context/PHASE_7_GOTCHAS/#1-configvalidationerror","title":"1. <code>ConfigValidationError</code>","text":"<ul> <li>Purpose: Raised when configuration validation fails.</li> <li>Features:</li> <li>Includes file and line information for precise debugging.</li> <li>Outputs errors in a structured format.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#2-connectionerror","title":"2. <code>ConnectionError</code>","text":"<ul> <li>Purpose: Raised when a connection fails or is invalid.</li> <li>Features:</li> <li>Provides a detailed reason for the failure.</li> <li>Suggests potential resolutions.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#gotchas","title":"Gotchas:","text":"<ul> <li>Misconfiguration of connections is a common source of runtime errors, with user oversight being the usual cause.</li> <li>Misleading or incomplete suggestions may exacerbate, rather than resolve, debugging efforts.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#graph-validation-and-execution-exceptions","title":"Graph-Validation and Execution Exceptions","text":""},{"location":"context/PHASE_7_GOTCHAS/#1-dependencyerror","title":"1. <code>DependencyError</code>","text":"<ul> <li>Purpose: Captures issues in dependency graphs, such as cycles or missing nodes.</li> <li>Features:</li> <li>Highlights dependency cycles for faster resolution.</li> <li>Gotchas:</li> <li>Can obscure the root cause of missing dependencies (e.g., upstream pipeline reconfiguration).</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#2-nodeexecutionerror","title":"2. <code>NodeExecutionError</code>","text":"<ul> <li>Purpose: Raised when a node in the pipeline encounters a failure during runtime.</li> <li>Features:</li> <li>Supports rich error context including <code>node_name</code>, <code>input_schema</code>, <code>input_shape</code>, and execution order.</li> <li>Attempts to clean complex error messages from frameworks like Spark (using <code>_clean_spark_error</code>).</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#gotchas_1","title":"Gotchas:","text":"<ul> <li>Spark-specific error cleaning might miss less common patterns.</li> <li>Error messages could still contain extraneous verbose stack traces, increasing triage time.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#validation-related-exceptions","title":"Validation-Related Exceptions","text":""},{"location":"context/PHASE_7_GOTCHAS/#1-validationerror","title":"1. <code>ValidationError</code>","text":"<ul> <li>Purpose: Raised when data validation fails.</li> <li>Features:</li> <li>Lists each validation failure separately with explicit details per node.</li> <li>Integrates directly with quality gate and PII validation workflows.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#2-gatefailederror","title":"2. <code>GateFailedError</code>","text":"<ul> <li>Purpose: Raised when a quality gate check fails.</li> <li>Features:</li> <li>Tracks pass rate, required rate, failed rows, and failure reasons.</li> <li>Outputs detailed statistics, helping users understand the scope and nature of failures.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#gotchas_2","title":"Gotchas:","text":"<ul> <li>Precision challenges in calculated pass/failure rates can lead to confusion during large data processing tasks.</li> <li>Divergences between validation logic for different backends (e.g., Spark, Pandas) may cause false positives or negatives.</li> </ul>"},{"location":"context/PHASE_7_GOTCHAS/#observations","title":"Observations","text":"<ol> <li>Verbose Exception Reporting:</li> <li>The level of error detail is commendable and aids debugging.</li> <li> <p>However, excessive verbosity in larger pipelines might make logs harder to parse, delaying resolution.</p> </li> <li> <p>Spark-Compatibility Challenges:</p> </li> <li> <p>The <code>NodeExecutionError</code> class attempts to clean Spark error messages using regular expressions. While this improves understanding, inaccuracies in the cleaning process can leave misleading error types or incomplete messages.</p> </li> <li> <p>Actionable Suggestions:</p> </li> <li>Many exceptions include explicit suggestions, which lower the barrier to entry for inexperienced users. However, these hints rely heavily on robust project metadata and may not work correctly if configurations are incomplete.</li> </ol>"},{"location":"context/PHASE_7_GOTCHAS/#next-steps","title":"Next Steps","text":"<p>Proceed to analyze the <code>cli/</code> directory in Phase 8 to explore how command-line interface behavior is defined and identify any potential pitfalls or usability challenges.</p>"},{"location":"context/PHASE_8_CLI/","title":"PHASE 8: CLI ANALYSIS","text":""},{"location":"context/PHASE_8_CLI/#overview","title":"Overview","text":"<p>This phase explores the Command-Line Interface (CLI) of the Odibi framework, focusing on the design, usability, and potential challenges. The <code>main.py</code> file in the <code>cli</code> module serves as the entry point for all CLI-related functionalities and integrates various commands for interacting with the framework.</p>"},{"location":"context/PHASE_8_CLI/#key-components-and-behavior-observations","title":"Key Components and Behavior Observations","text":""},{"location":"context/PHASE_8_CLI/#mainpy","title":"<code>main.py</code>","text":"<ul> <li>Core Role:</li> <li>Acts as the entry point for the Odibi CLI, invoking corresponding commands based on user input.</li> <li>Implements functionality for pipeline execution (<code>run</code>), dependency visualization (<code>graph</code>), configuration validation (<code>validate</code>), and much more.</li> </ul>"},{"location":"context/PHASE_8_CLI/#key-features","title":"Key Features:","text":"<ol> <li>Subcommand Handling:</li> <li>Utilizes Python's <code>argparse</code> module to manage CLI arguments and subcommands.</li> <li> <p>Commands like <code>run</code>, <code>deploy</code>, <code>validate</code>, and <code>graph</code> are individually parsed and forwarded to corresponding function handlers.</p> </li> <li> <p>Telemetry Configuration:</p> </li> <li> <p>Calls <code>setup_telemetry</code> at the start of execution to enable monitoring and logging of CLI usage.</p> </li> <li> <p>Golden Path Quick Start:</p> </li> <li> <p>The CLI epilog provides helpful usage examples for first-time users, introducing core commands like initializing a pipeline, validating configurations, and running a pipeline.</p> </li> <li> <p>Pipeline Execution Flexibility:</p> </li> <li>The <code>run</code> subcommand includes robust options such as:<ul> <li>Resume from failure (<code>--resume</code>).</li> <li>Filter nodes (<code>--tag</code>, <code>--node</code>).</li> <li>Error handling strategies (<code>--on-error</code> with options <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code>).</li> <li>Parallel execution (<code>--parallel</code>) with configurable worker thread counts (<code>--workers</code>).</li> </ul> </li> </ol>"},{"location":"context/PHASE_8_CLI/#non-obvious-behaviors","title":"Non-Obvious Behaviors:","text":"<ol> <li>Dynamic Command Discovery:</li> <li> <p>Dynamically integrates commands from multiple modules (e.g., <code>export.py</code>, <code>run.py</code>, <code>graph.py</code>), reducing the need for monolithic CLI logic within <code>main.py</code>.</p> </li> <li> <p>Error-Prone Options:</p> </li> <li>A wide range of options introduces potential for misuse. For example:<ul> <li>Incorrect worker count settings (<code>--workers</code>) might lead to performance bottlenecks.</li> <li>Combining incompatible flags (e.g., <code>--dry-run</code> with <code>--resume</code>) could create confusion.</li> </ul> </li> </ol>"},{"location":"context/PHASE_8_CLI/#usability-observations","title":"Usability Observations","text":"<ol> <li>Modular and Extendable Design:</li> <li> <p>The structure of <code>main.py</code> makes it easy to add new CLI commands via individual modules. Each subcommand adds its parser and functionality, ensuring separation of concerns.</p> </li> <li> <p>User-Centric Defaults:</p> </li> <li> <p>Defaults like <code>INFO</code> log level and <code>ascii</code> graph format ensure the CLI works out-of-the-box for common scenarios.</p> </li> <li> <p>Rich Debugging and Help Features:</p> </li> <li>Comprehensive <code>--log-level</code> options and epilog examples make debugging intuitive.</li> <li>Error messages are likely verbose, guiding users to correct usage.</li> </ol>"},{"location":"context/PHASE_8_CLI/#potential-gotchas","title":"Potential Gotchas","text":"<ol> <li>Verbose Help Output:</li> <li> <p>With numerous subcommands and options, the help output may overwhelm first-time users. Segmenting by command groups in quick start examples alleviates this to an extent.</p> </li> <li> <p>Error Handling Confusion:</p> </li> <li> <p>The <code>--on-error</code> flag offers flexibility but requires users to understand error propagation implications (<code>fail_fast</code> vs. <code>fail_later</code>).</p> </li> <li> <p>Telemetry Overhead:</p> </li> <li>While <code>setup_telemetry</code> is useful for monitoring, it might create privacy concerns for certain users. Clearly documenting telemetry behavior is recommended.</li> </ol>"},{"location":"context/PHASE_8_CLI/#conclusions","title":"Conclusions","text":"<p>The Odibi CLI is well-designed to handle a wide range of user interactions. Its modular, extendable approach ensures future scalability, while its user-centric defaults and debugging aid accessibility. However, the complexity of options and potential telemetry concerns are areas to monitor for usability improvements.</p>"},{"location":"context/PHASE_8_CLI/#next-steps","title":"Next Steps","text":"<p>For Phase 9, explore potential extensions to the framework, including plugin mechanisms or customization options that developers can leverage to extend Odibi's core capabilities.</p>"},{"location":"context/PHASE_9_EXTENSIONS/","title":"PHASE 9: EXTENSIONS ANALYSIS","text":""},{"location":"context/PHASE_9_EXTENSIONS/#overview","title":"Overview","text":"<p>This phase investigates the extension mechanisms within the Odibi framework. The <code>plugins.py</code> module exemplifies how Odibi enables modularity and extensibility, allowing developers to integrate custom functionality, particularly for managing connections.</p>"},{"location":"context/PHASE_9_EXTENSIONS/#key-components-and-behavior-observations","title":"Key Components and Behavior Observations","text":""},{"location":"context/PHASE_9_EXTENSIONS/#plugin-registration-mechanism","title":"Plugin Registration Mechanism","text":""},{"location":"context/PHASE_9_EXTENSIONS/#1-register_connection_factorytype_name-factory","title":"1. <code>register_connection_factory(type_name, factory)</code>","text":"<ul> <li>Registers a connection factory (a callable) under a specific type name.</li> <li>Factory functions must accept two arguments (<code>name</code> and <code>config</code>) and return a <code>Connection</code> instance.</li> <li>Registered factories are stored in the <code>_CONNECTION_FACTORIES</code> dictionary.</li> </ul>"},{"location":"context/PHASE_9_EXTENSIONS/#2-get_connection_factorytype_name","title":"2. <code>get_connection_factory(type_name)</code>","text":"<ul> <li>Retrieves a previously registered connection factory by type name.</li> <li>Returns <code>None</code> if no factory is registered for the given type.</li> </ul>"},{"location":"context/PHASE_9_EXTENSIONS/#plugin-discovery","title":"Plugin Discovery","text":""},{"location":"context/PHASE_9_EXTENSIONS/#1-load_plugins","title":"1. <code>load_plugins()</code>","text":"<ul> <li>Dynamically discerns and loads plugins using Python's <code>entry_points</code> from the <code>importlib.metadata</code> module.</li> <li>Supports multiple Python versions with tailored methods for accessing entry points:</li> <li>Python 3.10+: Uses <code>entry_points(group=...)</code>.</li> <li>Python 3.9: Uses <code>.select()</code> to filter groups where applicable.</li> <li> <p>Earlier versions: Uses <code>.get()</code> or equivalent fallback mechanisms.</p> </li> <li> <p>The entry point group, <code>odibi.connections</code>, is scanned for plugins:</p> </li> <li>Each entry point must define a callable factory.</li> <li>Successfully loaded factories are registered via <code>register_connection_factory</code>.</li> </ul>"},{"location":"context/PHASE_9_EXTENSIONS/#edge-case-management","title":"Edge Case Management:","text":"<ul> <li>Errors during plugin loading (e.g., malformed entry points or runtime exceptions) are logged with clear error messages.</li> </ul>"},{"location":"context/PHASE_9_EXTENSIONS/#observations","title":"Observations","text":"<ol> <li>Modular Architecture:</li> <li> <p>By using Python\u2019s plugin loading API (<code>importlib.metadata</code>), Odibi supports a modular ecosystem where third-party extensions can be seamlessly added without modifying core functionality.</p> </li> <li> <p>Backward Compatibility:</p> </li> <li> <p>The meticulous handling of differences in <code>entry_points</code> APIs across Python 3.8\u20133.10 ensures compatibility for a wider range of environments.</p> </li> <li> <p>Error Resilience:</p> </li> <li> <p>Comprehensive error handling during plugin discovery prevents runtime interruptions, logging failures instead for later inspection.</p> </li> <li> <p>Centralized Registry:</p> </li> <li>The <code>_CONNECTION_FACTORIES</code> dictionary serves as a centralized registry for connection-specific plugins, simplifying extension management.</li> </ol>"},{"location":"context/PHASE_9_EXTENSIONS/#gotchas-and-potential-challenges","title":"Gotchas and Potential Challenges","text":"<ol> <li>Plugin Compatibility:</li> <li>Developers must adhere to the expected signature for connection factories.</li> <li> <p>Incorrect factory function implementations may only surface runtime errors during execution.</p> </li> <li> <p>Discovery Limitations:</p> </li> <li> <p>Entry points must be correctly defined in the package\u2019s metadata for discovery to succeed. Misconfiguration could lead to silent failure or incomplete plugin registration.</p> </li> <li> <p>Scalability:</p> </li> <li>As the number of plugins grows, potential name conflicts in the <code>odibi.connections</code> group may arise.</li> </ol>"},{"location":"context/PHASE_9_EXTENSIONS/#recommendations","title":"Recommendations","text":"<ol> <li>Enhanced Debugging:</li> <li> <p>When a plugin fails to load, provide additional context about why it failed (e.g., expected vs. actual callable signature).</p> </li> <li> <p>Conflict Resolution:</p> </li> <li>Support a mechanism for resolving name conflicts, such as namespace segregation for plugin types or overwriting options.</li> </ol>"},{"location":"context/PHASE_9_EXTENSIONS/#conclusions","title":"Conclusions","text":"<p>The Plugin system in Odibi provides an elegant, modular mechanism to extend the framework's functionality without modifying its core. Its use of Python's <code>entry_points</code> API encourages community-driven ecosystems and ensures that the framework can evolve to meet diverse use cases.</p>"},{"location":"context/PHASE_9_EXTENSIONS/#next-steps","title":"Next Steps","text":"<p>Combine all analyzed phases into a single comprehensive document, <code>ODIBI_DEEP_CONTEXT.md</code>, summarizing the findings and insights from each phase of the investigation.</p>"},{"location":"design/SYNC_TO_DESIGN/","title":"System Catalog Sync Design Spec","text":""},{"location":"design/SYNC_TO_DESIGN/#overview","title":"Overview","text":"<p>Enable automatic replication of the system catalog (Delta tables) to a secondary destination for cross-environment visibility, dashboards, and SQL-based querying.</p>"},{"location":"design/SYNC_TO_DESIGN/#architecture","title":"Architecture","text":"<pre><code>Pipeline Execution\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PRIMARY: Delta Tables (Blob/Local Storage)               \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  \u2022 Source of truth for all operations                     \u2502\n\u2502  \u2022 ACID transactions, schema evolution, time travel       \u2502\n\u2502  \u2022 Merge/upsert operations                                \u2502\n\u2502  \u2022 17 meta tables                                         \u2502\n\u2502  Connection: system.connection                            \u2502\n\u2502  Path: system.path (e.g., _odibi_system)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n        \u2502  Auto-sync after pipeline run (if configured)\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SECONDARY: Sync Target                                   \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  Option A: SQL Server                                     \u2502\n\u2502    \u2022 Flat tables for Power BI / SQL queries               \u2502\n\u2502    \u2022 Cross-environment visibility                         \u2502\n\u2502    \u2022 No Spark required to query                           \u2502\n\u2502                                                           \u2502\n\u2502  Option B: Another Blob/ADLS                              \u2502\n\u2502    \u2022 Delta-to-Delta replication                           \u2502\n\u2502    \u2022 Cross-region backup                                  \u2502\n\u2502    \u2022 Shared catalog across projects                       \u2502\n\u2502                                                           \u2502\n\u2502  Connection: system.sync_to.connection                    \u2502\n\u2502  Path/Schema: system.sync_to.path or .schema_name         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#config-schema","title":"Config Schema","text":""},{"location":"design/SYNC_TO_DESIGN/#full-example","title":"Full Example","text":"<pre><code>system:\n  # PRIMARY - Required (Delta tables)\n  connection: goat_prod_dm          # Blob storage connection\n  path: _odibi_system               # Path within connection\n  environment: prod                 # Environment tag for records\n\n  # SECONDARY - Optional (Sync target)\n  sync_to:\n    connection: goat_qat            # SQL Server OR another blob\n    schema_name: odibi_system       # For SQL Server targets\n    path: _odibi_system_replica     # For blob targets\n    mode: incremental               # full | incremental\n    on: after_run                   # after_run | manual | schedule\n    tables:                         # Optional: subset of tables\n      - meta_runs\n      - meta_pipeline_runs\n      - meta_node_runs\n    retry:\n      enabled: true\n      max_attempts: 3\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#synctoconfig-model","title":"SyncToConfig Model","text":"<pre><code>class SyncToConfig(BaseModel):\n    \"\"\"Configuration for syncing system catalog to secondary destination.\"\"\"\n\n    connection: str = Field(\n        description=\"Target connection name (SQL Server or blob storage)\"\n    )\n    schema_name: Optional[str] = Field(\n        default=\"odibi_system\",\n        description=\"Schema name for SQL Server targets\"\n    )\n    path: Optional[str] = Field(\n        default=\"_odibi_system\",\n        description=\"Path for blob storage targets\"\n    )\n    mode: Literal[\"full\", \"incremental\"] = Field(\n        default=\"incremental\",\n        description=\"Sync mode: 'full' replaces all data, 'incremental' only new/changed\"\n    )\n    on: Literal[\"after_run\", \"manual\", \"schedule\"] = Field(\n        default=\"after_run\",\n        description=\"When to trigger sync\"\n    )\n    tables: Optional[List[str]] = Field(\n        default=None,\n        description=\"Subset of tables to sync. None = all tables\"\n    )\n    retry: Optional[RetryConfig] = Field(\n        default=None,\n        description=\"Retry configuration for sync failures\"\n    )\n    async_sync: bool = Field(\n        default=True,\n        description=\"Run sync asynchronously (don't block pipeline completion)\"\n    )\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#tables-to-sync","title":"Tables to Sync","text":""},{"location":"design/SYNC_TO_DESIGN/#all-17-meta-tables","title":"All 17 Meta Tables","text":"Table Priority Description <code>meta_runs</code> High Node-level run history <code>meta_pipeline_runs</code> High Pipeline-level run summaries <code>meta_node_runs</code> High Detailed node run records <code>meta_tables</code> High Asset registry <code>meta_lineage</code> Medium Data lineage records <code>meta_outputs</code> Medium Output registrations <code>meta_patterns</code> Medium Pattern execution logs <code>meta_schemas</code> Medium Schema tracking <code>meta_pipelines</code> Low Pipeline definitions <code>meta_nodes</code> Low Node definitions <code>meta_state</code> Low HWM and state values <code>meta_metrics</code> Low Custom metrics <code>meta_failures</code> High Failure records <code>meta_observability_errors</code> Medium Error details <code>meta_derived_applied_runs</code> Low Derived run tracking <code>meta_daily_stats</code> Medium Daily aggregated stats <code>meta_pipeline_health</code> Medium Health metrics <code>meta_sla_status</code> Medium SLA tracking"},{"location":"design/SYNC_TO_DESIGN/#default-tables-if-not-specified","title":"Default Tables (if not specified)","text":"<p>High priority tables synced by default: - <code>meta_runs</code> - <code>meta_pipeline_runs</code> - <code>meta_node_runs</code> - <code>meta_tables</code> - <code>meta_failures</code></p>"},{"location":"design/SYNC_TO_DESIGN/#sync-behavior","title":"Sync Behavior","text":""},{"location":"design/SYNC_TO_DESIGN/#trigger-after_run-default","title":"Trigger: <code>after_run</code> (Default)","text":"<pre><code># In Pipeline.run() completion\nif self.project_config.system.sync_to:\n    if sync_config.async_sync:\n        # Fire and forget - don't block pipeline\n        threading.Thread(target=self._sync_catalog).start()\n    else:\n        self._sync_catalog()\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#trigger-manual","title":"Trigger: <code>manual</code>","text":"<pre><code># CLI command\nodibi catalog sync\n\n# With options\nodibi catalog sync --tables meta_runs,meta_pipeline_runs\nodibi catalog sync --mode full\nodibi catalog sync --dry-run\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#trigger-schedule","title":"Trigger: <code>schedule</code>","text":"<p>User configures external scheduler (Databricks job, cron, etc.) to call: <pre><code>odibi catalog sync\n</code></pre></p>"},{"location":"design/SYNC_TO_DESIGN/#sync-strategies-by-target-type","title":"Sync Strategies by Target Type","text":""},{"location":"design/SYNC_TO_DESIGN/#delta-sql-server","title":"Delta \u2192 SQL Server","text":"<pre><code>def sync_to_sql_server(source_table: str, target_conn: SqlConnection):\n    # Read from Delta\n    df = spark.read.format(\"delta\").load(source_path)\n\n    # For incremental: filter by timestamp\n    if mode == \"incremental\":\n        last_sync = get_last_sync_timestamp(target_conn, table)\n        df = df.filter(col(\"timestamp\") &gt; last_sync)\n\n    # Write to SQL Server\n    # Option 1: JDBC bulk insert\n    df.write.format(\"jdbc\") \\\n        .option(\"url\", jdbc_url) \\\n        .option(\"dbtable\", f\"{schema}.{table}\") \\\n        .mode(\"append\" if incremental else \"overwrite\") \\\n        .save()\n\n    # Option 2: Use connection.execute() for smaller datasets\n    records = df.toPandas().to_dict('records')\n    for batch in chunked(records, 1000):\n        connection.execute_batch(insert_sql, batch)\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#delta-delta-another-blob","title":"Delta \u2192 Delta (Another Blob)","text":"<pre><code>def sync_to_delta(source_path: str, target_path: str):\n    # Simple Delta-to-Delta copy\n    df = spark.read.format(\"delta\").load(source_path)\n\n    if mode == \"incremental\":\n        # Use Delta merge for efficiency\n        target_table = DeltaTable.forPath(spark, target_path)\n        target_table.alias(\"target\").merge(\n            df.alias(\"source\"),\n            \"target.id = source.id\"\n        ).whenMatchedUpdateAll() \\\n         .whenNotMatchedInsertAll() \\\n         .execute()\n    else:\n        df.write.format(\"delta\") \\\n          .mode(\"overwrite\") \\\n          .save(target_path)\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#sql-server-table-ddl","title":"SQL Server Table DDL","text":"<p>Auto-generated from Delta schema:</p> <pre><code>-- Example: meta_runs\nCREATE TABLE [odibi_system].[meta_runs] (\n    run_id NVARCHAR(100),\n    pipeline_name NVARCHAR(255),\n    node_name NVARCHAR(255),\n    status NVARCHAR(50),\n    rows_processed BIGINT,\n    duration_ms BIGINT,\n    metrics_json NVARCHAR(MAX),\n    environment NVARCHAR(50),\n    timestamp DATETIME2,\n    date DATE,\n    -- Sync metadata\n    _synced_at DATETIME2 DEFAULT GETUTCDATE()\n);\n\n-- Index for queries\nCREATE INDEX IX_meta_runs_pipeline_date \nON [odibi_system].[meta_runs] (pipeline_name, date);\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#cli-commands","title":"CLI Commands","text":"<pre><code># Sync all configured tables\nodibi catalog sync\n\n# Sync specific tables\nodibi catalog sync --tables meta_runs,meta_failures\n\n# Full sync (replace all data)\nodibi catalog sync --mode full\n\n# Dry run - show what would be synced\nodibi catalog sync --dry-run\n\n# Show sync status\nodibi catalog sync-status\n\n# Manual trigger even if on=manual not set\nodibi catalog sync --force\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#error-handling","title":"Error Handling","text":""},{"location":"design/SYNC_TO_DESIGN/#non-blocking-failures","title":"Non-blocking Failures","text":"<p>Sync failures should never fail the pipeline. Log warning and continue.</p> <pre><code>try:\n    self._sync_catalog()\nexcept Exception as e:\n    self._ctx.warning(\n        f\"Catalog sync failed: {e}\",\n        suggestion=\"Run 'odibi catalog sync' manually to retry\"\n    )\n    # Pipeline still succeeds\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#retry-logic","title":"Retry Logic","text":"<pre><code>@retry(\n    max_attempts=3,\n    backoff=exponential,\n    exceptions=[ConnectionError, TimeoutError]\n)\ndef _sync_table(self, table: str):\n    ...\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#sync-state-tracking","title":"Sync State Tracking","text":"<p>Track last successful sync in <code>meta_state</code>:</p> <pre><code># Key: sync_to:{connection}:{table}:last_timestamp\n# Value: \"2026-01-11T16:59:50.000Z\"\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#implementation-plan","title":"Implementation Plan","text":""},{"location":"design/SYNC_TO_DESIGN/#phase-1-config-basic-sync","title":"Phase 1: Config &amp; Basic Sync","text":"<ol> <li>Add <code>SyncToConfig</code> to config.py</li> <li>Add validation (sync_to connection must exist)</li> <li>Implement basic full sync for SQL Server</li> <li>Add CLI <code>odibi catalog sync</code></li> </ol>"},{"location":"design/SYNC_TO_DESIGN/#phase-2-incremental-delta-to-delta","title":"Phase 2: Incremental &amp; Delta-to-Delta","text":"<ol> <li>Implement incremental sync with timestamp tracking</li> <li>Implement Delta-to-Delta sync for blob targets</li> <li>Add <code>--dry-run</code> support</li> </ol>"},{"location":"design/SYNC_TO_DESIGN/#phase-3-auto-sync-integration","title":"Phase 3: Auto-sync Integration","text":"<ol> <li>Integrate into Pipeline.run() completion</li> <li>Add async support</li> <li>Add retry logic</li> </ol>"},{"location":"design/SYNC_TO_DESIGN/#phase-4-observability","title":"Phase 4: Observability","text":"<ol> <li>Add sync status tracking</li> <li>Add <code>odibi catalog sync-status</code> command</li> <li>Log sync metrics to meta_runs</li> </ol>"},{"location":"design/SYNC_TO_DESIGN/#migration-from-current-broken-state","title":"Migration from Current Broken State","text":"<p>For users who currently have SQL Server as their <code>system.connection</code>:</p> <pre><code># BEFORE (broken)\nsystem:\n  connection: goat_qat              # SQL Server - causes path error\n  schema_name: odibi_system\n\n# AFTER (works)\nsystem:\n  connection: goat_prod_dm          # Blob storage - primary\n  path: _odibi_system\n  environment: prod\n  sync_to:\n    connection: goat_qat            # SQL Server - secondary\n    schema_name: odibi_system\n</code></pre> <p>Add deprecation warning if SQL Server is used as primary connection:</p> <pre><code>if conn_type in (\"sql_server\", \"azure_sql\"):\n    self._ctx.error(\n        \"SQL Server cannot be used as primary system.connection\",\n        suggestion=(\n            \"Use blob storage for system.connection and add \"\n            \"sync_to for SQL Server visibility. See docs/design/SYNC_TO_DESIGN.md\"\n        )\n    )\n</code></pre>"},{"location":"design/SYNC_TO_DESIGN/#questions-to-resolve","title":"Questions to Resolve","text":"<ol> <li>Sync granularity: Sync entire tables or partition by date/environment?</li> <li>Schema drift: How to handle when Delta schema evolves? Auto-migrate SQL tables?</li> <li>Large tables: For tables like meta_runs with millions of rows, should we only sync last N days?</li> <li>Permissions: Document required SQL Server permissions for sync</li> </ol>"},{"location":"examples/canonical/","title":"Canonical YAML Examples","text":"<p>Copy-paste ready configs for the 5 most common use cases.</p> <p>Each example is a complete, runnable config\u2014not a fragment.</p>"},{"location":"examples/canonical/#sample-data","title":"Sample Data","text":"<p>Download sample datasets from sample_data/ or copy them:</p> <pre><code>mkdir -p data/landing\ncp docs/examples/canonical/sample_data/*.csv data/landing/\n</code></pre> Example Use Case Key Features 1. Hello World Local CSV \u2192 Parquet Minimal viable config 2. Incremental SQL Database \u2192 Raw (HWM) Stateful high-water mark 3. SCD2 Dimension Track history Surrogate keys, versioning 4. Fact Table Star schema fact SK lookups, grain validation 5. Full Pipeline Validation + Quarantine + Alerting Production-ready"},{"location":"examples/canonical/#when-to-use-each","title":"When to Use Each","text":"<pre><code>I need to...\n\u2502\n\u251c\u2500\u25ba Get started fast \u2192 Example 1 (Hello World)\n\u251c\u2500\u25ba Load from a database \u2192 Example 2 (Incremental SQL)\n\u251c\u2500\u25ba Build dimension with history \u2192 Example 3 (SCD2)\n\u251c\u2500\u25ba Build fact table for BI \u2192 Example 4 (Fact Table)\n\u2514\u2500\u25ba Production pipeline with alerts \u2192 Example 5 (Full Pipeline)\n</code></pre>"},{"location":"examples/canonical/01_hello_world/","title":"Example 1: Hello World (CSV \u2192 Parquet)","text":"<p>The simplest possible Odibi pipeline. Read a CSV, write Parquet.</p>"},{"location":"examples/canonical/01_hello_world/#when-to-use","title":"When to Use","text":"<ul> <li>First-time users learning Odibi</li> <li>Quick local data conversions</li> <li>Prototyping before production setup</li> </ul>"},{"location":"examples/canonical/01_hello_world/#expected-output","title":"Expected Output","text":"<ul> <li><code>data/bronze/customers/*.parquet</code> \u2014 Your data in columnar format</li> <li><code>data/bronze/stories/*.html</code> \u2014 Audit report with row counts and schema</li> </ul>"},{"location":"examples/canonical/01_hello_world/#full-config","title":"Full Config","text":"<pre><code># odibi.yaml\nproject: hello_world\n\nconnections:\n  landing:\n    type: local\n    base_path: ./data/landing\n  bronze:\n    type: local\n    base_path: ./data/bronze\n\nstory:\n  connection: bronze\n  path: stories\n\nsystem:\n  connection: bronze\n  path: _system\n\npipelines:\n  - pipeline: ingest\n    layer: bronze\n    nodes:\n      - name: ingest_customers\n        read:\n          connection: landing\n          format: csv\n          path: customers.csv\n          options:\n            header: true\n        write:\n          connection: bronze\n          format: parquet\n          path: customers\n          mode: overwrite\n</code></pre>"},{"location":"examples/canonical/01_hello_world/#sample-data","title":"Sample Data","text":"<p>Copy from <code>docs/examples/canonical/sample_data/customers.csv</code> or create <code>data/landing/customers.csv</code>:</p> <pre><code>customer_id,name,email,tier,city,updated_at\n1,Alice,alice@example.com,Gold,NYC,2025-01-01\n2,Bob,bob@example.com,Silver,LA,2025-01-01\n3,Charlie,charlie@example.com,Bronze,Chicago,2025-01-01\n</code></pre>"},{"location":"examples/canonical/01_hello_world/#run","title":"Run","text":"<pre><code>odibi run odibi.yaml\n</code></pre>"},{"location":"examples/canonical/01_hello_world/#schema-reference","title":"Schema Reference","text":"Key Docs <code>connections[].type: local</code> LocalConnectionConfig <code>read.format: csv</code> ReadConfig <code>write.format: parquet</code> WriteConfig <code>story</code> StoryConfig"},{"location":"examples/canonical/02_incremental_sql/","title":"Example 2: Incremental SQL Ingestion (Database \u2192 Raw with HWM)","text":"<p>Load data from a SQL Server database incrementally using high-water mark (HWM).</p>"},{"location":"examples/canonical/02_incremental_sql/#when-to-use","title":"When to Use","text":"<ul> <li>Source table has millions of rows</li> <li>Source has an <code>updated_at</code> or <code>created_at</code> timestamp column</li> <li>You want to load only new/changed rows after the first full load</li> </ul>"},{"location":"examples/canonical/02_incremental_sql/#how-it-works","title":"How It Works","text":"<ol> <li>First run: No state exists \u2192 Full load (all rows)</li> <li>Subsequent runs: State exists \u2192 Only rows where <code>updated_at &gt; last_hwm</code></li> <li>State is persisted to <code>_system</code> catalog automatically</li> </ol>"},{"location":"examples/canonical/02_incremental_sql/#full-config","title":"Full Config","text":"<pre><code># odibi.yaml\nproject: incremental_orders\n\nengine: spark  # Use Spark for JDBC\n\nconnections:\n  source_db:\n    type: sql_server\n    host: ${SQL_SERVER_HOST}\n    database: production\n    username: ${SQL_USER}\n    password: ${SQL_PASSWORD}\n\n  lake:\n    type: local\n    base_path: ./data/lake\n\nstory:\n  connection: lake\n  path: stories\n\nsystem:\n  connection: lake\n  path: _system\n\npipelines:\n  - pipeline: bronze_orders\n    layer: bronze\n    nodes:\n      - name: ingest_orders\n        read:\n          connection: source_db\n          format: jdbc\n          table: dbo.orders\n        incremental:\n          mode: stateful\n          column: updated_at\n          # Optional: limit first load to recent data\n          # first_run_lookback: \"365d\"\n        write:\n          connection: lake\n          format: delta\n          path: bronze/orders\n          mode: append\n</code></pre>"},{"location":"examples/canonical/02_incremental_sql/#environment-variables","title":"Environment Variables","text":"<p>Set these before running:</p> <pre><code>export SQL_SERVER_HOST=your-server.database.windows.net\nexport SQL_USER=reader\nexport SQL_PASSWORD=your-password\n</code></pre>"},{"location":"examples/canonical/02_incremental_sql/#run","title":"Run","text":"<pre><code># First run: Full load\nodibi run odibi.yaml\n\n# Second run: Incremental (only new rows)\nodibi run odibi.yaml\n</code></pre>"},{"location":"examples/canonical/02_incremental_sql/#check-state","title":"Check State","text":"<pre><code>odibi catalog state odibi.yaml\n</code></pre> <p>Output: <pre><code>bronze_orders.ingest_orders:\n  hwm: 2025-01-03T12:00:00\n  last_run: 2025-01-03T12:05:32\n</code></pre></p>"},{"location":"examples/canonical/02_incremental_sql/#schema-reference","title":"Schema Reference","text":"Key Docs <code>connections[].type: sql_server</code> SQLServerConnectionConfig <code>incremental.mode: stateful</code> IncrementalConfig <code>write.mode: append</code> WriteConfig"},{"location":"examples/canonical/02_incremental_sql/#common-patterns","title":"Common Patterns","text":"<p>Choose <code>rolling_window</code> instead if: - You don't need exact row-level tracking - Source rows can be updated without changing <code>updated_at</code> - You want simpler state management</p> <pre><code>incremental:\n  mode: rolling_window\n  column: created_at\n  lookback: \"7d\"  # Always load last 7 days\n</code></pre> <p>\u2192 Pattern: Incremental Stateful</p>"},{"location":"examples/canonical/03_scd2_dimension/","title":"Example 3: SCD2 Dimension (Track History)","text":"<p>Build a customer dimension with full history tracking (Slowly Changing Dimension Type 2).</p>"},{"location":"examples/canonical/03_scd2_dimension/#when-to-use","title":"When to Use","text":"<ul> <li>You need to answer: \"What was the customer's tier last month?\"</li> <li>Dimension attributes change slowly (name, address, tier)</li> <li>BI reports need point-in-time accuracy</li> </ul>"},{"location":"examples/canonical/03_scd2_dimension/#how-it-works","title":"How It Works","text":"<p>When a tracked column changes: 1. Old row gets <code>valid_to</code> = change date, <code>is_current</code> = false 2. New row gets <code>valid_to</code> = NULL, <code>is_current</code> = true 3. All history is preserved</p>"},{"location":"examples/canonical/03_scd2_dimension/#full-config","title":"Full Config","text":"<pre><code># odibi.yaml\nproject: customer_dimension\n\nconnections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n  silver:\n    type: local\n    base_path: ./data/silver\n\nstory:\n  connection: silver\n  path: stories\n\nsystem:\n  connection: silver\n  path: _system\n\npipelines:\n  - pipeline: dimensions\n    layer: silver\n    nodes:\n      - name: dim_customer\n        read:\n          connection: bronze\n          format: parquet\n          path: customers\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols:\n              - name\n              - email\n              - tier\n              - city\n            unknown_member: true  # Creates SK=0 for orphan handling\n\n        write:\n          connection: silver\n          format: delta\n          path: dim_customer\n          mode: overwrite  # Required for SCD2\n</code></pre>"},{"location":"examples/canonical/03_scd2_dimension/#sample-input-bronze","title":"Sample Input (Bronze)","text":"<p>Copy from <code>docs/examples/canonical/sample_data/customers.csv</code> to <code>data/bronze/customers/</code>:</p> customer_id name email tier city updated_at 1 Alice alice@example.com Gold NYC 2025-01-01 2 Bob bob@example.com Silver LA 2025-01-01"},{"location":"examples/canonical/03_scd2_dimension/#expected-output-silver","title":"Expected Output (Silver)","text":"<p>After first run:</p> customer_sk customer_id name tier city valid_from valid_to is_current 0 _UNKNOWN Unknown Unknown Unknown 1900-01-01 NULL true 1 1 Alice Gold NYC 2025-01-01 NULL true 2 2 Bob Silver LA 2025-01-01 NULL true <p>After Alice's tier changes to Platinum:</p> customer_sk customer_id name tier city valid_from valid_to is_current 0 _UNKNOWN Unknown Unknown Unknown 1900-01-01 NULL true 1 1 Alice Gold NYC 2025-01-01 2025-01-15 false 2 2 Bob Silver LA 2025-01-01 NULL true 3 1 Alice Platinum NYC 2025-01-15 NULL true"},{"location":"examples/canonical/03_scd2_dimension/#run","title":"Run","text":"<pre><code>odibi run odibi.yaml\n</code></pre>"},{"location":"examples/canonical/03_scd2_dimension/#query-current-state","title":"Query Current State","text":"<pre><code>SELECT * FROM dim_customer WHERE is_current = true\n</code></pre>"},{"location":"examples/canonical/03_scd2_dimension/#query-historical-state","title":"Query Historical State","text":"<pre><code>-- What was Alice's tier on Jan 10?\nSELECT * FROM dim_customer \nWHERE customer_id = 'C001' \n  AND '2025-01-10' BETWEEN valid_from AND COALESCE(valid_to, '9999-12-31')\n</code></pre>"},{"location":"examples/canonical/03_scd2_dimension/#schema-reference","title":"Schema Reference","text":"Key Docs <code>pattern.type: dimension</code> Dimension Pattern <code>params.scd_type: 2</code> SCD2 Pattern <code>params.unknown_member</code> Dimension Pattern"},{"location":"examples/canonical/03_scd2_dimension/#decision-scd-type-1-vs-2","title":"Decision: SCD Type 1 vs 2","text":"Choose SCD1 if... Choose SCD2 if... History not needed \"What was the value last month?\" Storage is limited BI needs point-in-time accuracy Simpler queries Audit requirements <pre><code># SCD1: Overwrites, no history\nparams:\n  scd_type: 1\n</code></pre> <p>\u2192 Pattern: Dimension</p>"},{"location":"examples/canonical/04_fact_table/","title":"Example 4: Fact Table (Star Schema)","text":"<p>Build a fact table with automatic surrogate key lookups from dimensions.</p>"},{"location":"examples/canonical/04_fact_table/#when-to-use","title":"When to Use","text":"<ul> <li>Building a star schema for BI/reporting</li> <li>Need to join business keys to surrogate keys from dimensions</li> <li>Want grain validation and orphan handling</li> </ul>"},{"location":"examples/canonical/04_fact_table/#how-it-works","title":"How It Works","text":"<ol> <li>Reads staging data with business keys (<code>customer_id</code>, <code>product_id</code>)</li> <li>Looks up surrogate keys from dimension tables (<code>customer_sk</code>, <code>product_sk</code>)</li> <li>Handles orphan records (missing dimension entries)</li> <li>Validates grain (no duplicate combinations)</li> </ol>"},{"location":"examples/canonical/04_fact_table/#full-config","title":"Full Config","text":"<pre><code># odibi.yaml\nproject: sales_warehouse\n\nconnections:\n  staging:\n    type: local\n    base_path: ./data/staging\n  gold:\n    type: local\n    base_path: ./data/gold\n\nstory:\n  connection: gold\n  path: stories\n\nsystem:\n  connection: gold\n  path: _system\n\npipelines:\n  - pipeline: facts\n    layer: gold\n    nodes:\n      - name: fact_sales\n        read:\n          connection: staging\n          format: parquet\n          path: sales_events\n\n        pattern:\n          type: fact\n          params:\n            grain:\n              - order_id\n              - line_item_id\n\n            dimensions:\n              - source_column: customer_id\n                dimension_table: gold.dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n\n              - source_column: product_id\n                dimension_table: gold.dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n\n              - source_column: order_date\n                dimension_table: gold.dim_date\n                dimension_key: date_key\n                surrogate_key: date_sk\n\n            orphan_handling: unknown  # Map to SK=0\n            # Options: 'unknown' | 'quarantine' | 'fail'\n\n        write:\n          connection: gold\n          format: delta\n          path: fact_sales\n          mode: overwrite\n</code></pre>"},{"location":"examples/canonical/04_fact_table/#sample-input-staging","title":"Sample Input (Staging)","text":"<p>Copy from <code>docs/examples/canonical/sample_data/orders.csv</code> to <code>data/staging/sales_events/</code>:</p> order_id line_item_id customer_id product_id order_date quantity amount O001 1 1 P100 2025-01-15 2 49.99 O001 2 1 P200 2025-01-15 1 29.99 O005 1 999 P100 2025-01-19 1 24.99 <p>Note: <code>999</code> doesn't exist in <code>dim_customer</code> (orphan record).</p>"},{"location":"examples/canonical/04_fact_table/#expected-output","title":"Expected Output","text":"order_id line_item_id customer_sk product_sk date_sk quantity amount O001 1 1 1 20250115 2 49.99 O001 2 1 2 20250115 1 29.99 O005 1 0 1 20250119 1 24.99 <p><code>999</code> \u2192 <code>customer_sk = 0</code> (unknown member row)</p>"},{"location":"examples/canonical/04_fact_table/#orphan-handling-options","title":"Orphan Handling Options","text":"Option Behavior When to Use <code>unknown</code> Map to SK=0 BI can filter unknowns, no data loss <code>quarantine</code> Route to quarantine table Review orphans later <code>fail</code> Stop pipeline Strict referential integrity required <pre><code># Quarantine option\norphan_handling: quarantine\norphan_quarantine:\n  connection: gold\n  path: quarantine/fact_sales_orphans\n</code></pre>"},{"location":"examples/canonical/04_fact_table/#run","title":"Run","text":"<pre><code>odibi run odibi.yaml\n</code></pre>"},{"location":"examples/canonical/04_fact_table/#validate-grain","title":"Validate Grain","text":"<p>The fact pattern automatically checks for grain violations:</p> <pre><code>\u274c Grain violation: 3 duplicate combinations found for [order_id, line_item_id]\n</code></pre> <p>This prevents the silent corruption of having duplicate fact rows.</p>"},{"location":"examples/canonical/04_fact_table/#schema-reference","title":"Schema Reference","text":"Key Docs <code>pattern.type: fact</code> Fact Pattern <code>params.grain</code> Fact Pattern: Grain Validation <code>params.orphan_handling</code> Fact Pattern: Orphan Handling"},{"location":"examples/canonical/04_fact_table/#see-also","title":"See Also","text":"<ul> <li>Example 3: SCD2 Dimension \u2014 Build dimensions first</li> <li>Pattern: Aggregation \u2014 Pre-aggregate facts for BI</li> </ul>"},{"location":"examples/canonical/05_full_pipeline/","title":"Example 5: Full Pipeline (Validation + Quarantine + Alerting)","text":"<p>A production-ready pipeline with data contracts, quality gates, quarantine routing, and Slack alerts.</p>"},{"location":"examples/canonical/05_full_pipeline/#when-to-use","title":"When to Use","text":"<ul> <li>Production workloads</li> <li>Data quality is critical</li> <li>Need observability and alerting</li> </ul>"},{"location":"examples/canonical/05_full_pipeline/#full-config","title":"Full Config","text":"<pre><code># odibi.yaml\nproject: production_orders\n\nengine: spark\n\n# Resilience\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\n# Alerting\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    on_events: [on_failure, on_quality_gate_fail]\n\nconnections:\n  source_db:\n    type: sql_server\n    host: ${SQL_SERVER_HOST}\n    database: production\n    username: ${SQL_USER}\n    password: ${SQL_PASSWORD}\n\n  lake:\n    type: local\n    base_path: ./data/lake\n\nstory:\n  connection: lake\n  path: stories\n\nsystem:\n  connection: lake\n  path: _system\n\npipelines:\n  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  # BRONZE: Ingest with contracts\n  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  - pipeline: bronze\n    layer: bronze\n    nodes:\n      - name: ingest_orders\n        read:\n          connection: source_db\n          format: jdbc\n          table: dbo.orders\n\n        # Fail fast if source is broken\n        contracts:\n          - type: row_count\n            min: 1\n          - type: not_null\n            columns: [order_id, customer_id, amount]\n          - type: freshness\n            column: created_at\n            max_age: \"24h\"\n\n        incremental:\n          mode: stateful\n          column: updated_at\n\n        write:\n          connection: lake\n          format: delta\n          path: bronze/orders\n          mode: append\n\n  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  # SILVER: Clean with validation gates\n  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  - pipeline: silver\n    layer: silver\n    nodes:\n      - name: clean_orders\n        read:\n          connection: lake\n          format: delta\n          path: bronze/orders\n\n        transformer: deduplicate\n        params:\n          keys: [order_id]\n          order_by: \"updated_at DESC\"\n\n        transform:\n          steps:\n            - function: filter_rows\n              params:\n                condition: \"amount &gt; 0\"\n            - function: derive_columns\n              params:\n                derivations:\n                  amount_usd: \"amount * 1.0\"\n                  order_year: \"YEAR(order_date)\"\n\n        validation:\n          tests:\n            - type: unique\n              columns: [order_id]\n            - type: not_null\n              columns: [customer_id, amount_usd]\n            - type: range\n              column: amount_usd\n              min: 0\n              max: 1000000\n\n          gate:\n            require_pass_rate: 0.95\n            on_failure: warn  # 'fail' to stop pipeline\n\n          quarantine:\n            connection: lake\n            path: quarantine/orders\n\n        write:\n          connection: lake\n          format: delta\n          path: silver/orders\n          mode: overwrite\n\n  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  # GOLD: Aggregate for BI\n  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  - pipeline: gold\n    layer: gold\n    nodes:\n      - name: agg_daily_sales\n        depends_on: [clean_orders]\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [order_date, customer_id]\n            measures:\n              - name: total_amount\n                expr: \"SUM(amount_usd)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: avg_order_value\n                expr: \"AVG(amount_usd)\"\n\n        write:\n          connection: lake\n          format: delta\n          path: gold/daily_sales\n          mode: overwrite\n</code></pre>"},{"location":"examples/canonical/05_full_pipeline/#what-this-config-does","title":"What This Config Does","text":"Stage Action Bronze Ingest from SQL with contracts (fail if empty/stale) Silver Dedupe, filter, validate, quarantine bad rows Gold Aggregate for BI consumption Alerts Slack notification on failure"},{"location":"examples/canonical/05_full_pipeline/#run","title":"Run","text":"<pre><code># Set environment variables\nexport SQL_SERVER_HOST=your-server.database.windows.net\nexport SQL_USER=reader\nexport SQL_PASSWORD=your-password\nexport SLACK_WEBHOOK_URL=https://hooks.slack.com/...\n\n# Run\nodibi run odibi.yaml\n</code></pre>"},{"location":"examples/canonical/05_full_pipeline/#inspect-quarantine","title":"Inspect Quarantine","text":"<pre><code># Check quarantined rows\nodibi run odibi.yaml --node quarantine_report\n</code></pre> <p>Or query directly: <pre><code>spark.read.format(\"delta\").load(\"data/lake/quarantine/orders\").show()\n</code></pre></p>"},{"location":"examples/canonical/05_full_pipeline/#schema-reference","title":"Schema Reference","text":"Key Docs <code>contracts</code> ContractConfig <code>validation.tests</code> ValidationConfig <code>validation.gate</code> Quality Gates <code>validation.quarantine</code> Quarantine <code>alerts</code> Alerting <code>retry</code> RetryConfig"},{"location":"examples/canonical/05_full_pipeline/#decision-contracts-vs-validation-tests","title":"Decision: Contracts vs Validation Tests","text":"Use Contracts when... Use Validation when... Checking source data before processing Checking output after transformation Fail-fast is required Soft warnings are acceptable Freshness, schema, volume checks Row-level quality (nulls, ranges)"},{"location":"examples/canonical/05_full_pipeline/#see-also","title":"See Also","text":"<ul> <li>Validation Overview</li> <li>Alerting Guide</li> <li>Production Deployment</li> </ul>"},{"location":"examples/canonical/THE_REFERENCE/","title":"THE Reference Implementation","text":"<p>This is the canonical Odibi pipeline. Copy this. Learn from this. Start here.</p>"},{"location":"examples/canonical/THE_REFERENCE/#the-pipeline","title":"The Pipeline","text":"<p>04_fact_table.yaml is the complete reference implementation that demonstrates everything Odibi can do in a single, runnable example.</p>"},{"location":"examples/canonical/THE_REFERENCE/#what-it-builds","title":"What It Builds","text":"<p>A complete star schema with:</p> Table Pattern Key Features <code>dim_customer</code> Dimension (SCD1) Natural\u2192surrogate key, unknown member <code>dim_product</code> Dimension (SCD1) Track columns, auto SK generation <code>dim_date</code> Date Dimension Generated 366 rows, fiscal calendar <code>fact_sales</code> Fact FK lookups, orphan handling, grain validation"},{"location":"examples/canonical/THE_REFERENCE/#run-it","title":"Run It","text":"<pre><code>cd docs/examples/canonical/runnable\nodibi run 04_fact_table.yaml\n</code></pre>"},{"location":"examples/canonical/THE_REFERENCE/#why-this-example","title":"Why This Example?","text":"<ol> <li>Star Schema \u2014 The most common real-world pattern</li> <li>Multiple Pipelines \u2014 Shows dependency ordering (dimensions before facts)</li> <li>FK Lookups \u2014 Automatic surrogate key resolution</li> <li>Orphan Handling \u2014 <code>customer_id=999</code> maps to <code>customer_sk=0</code> (unknown member)</li> <li>Production Ready \u2014 Parquet output, proper layering, auditable</li> </ol>"},{"location":"examples/canonical/THE_REFERENCE/#the-full-config","title":"The Full Config","text":"<pre><code>project: sales_star_schema\n\nconnections:\n  source:\n    type: local\n    base_path: ../sample_data\n  gold:\n    type: local\n    base_path: ./data/gold\n\nstory:\n  connection: gold\n  path: stories\n\nsystem:\n  connection: gold\n  path: _system\n\npipelines:\n  # Pipeline 1: Build all dimensions\n  - pipeline: build_dimensions\n    layer: gold\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          format: csv\n          path: customers.csv\n          options:\n            header: true\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 1\n            track_cols: [name, email, tier, city]\n            unknown_member: true\n        write:\n          connection: gold\n          format: parquet\n          path: dim_customer\n          mode: overwrite\n\n      - name: dim_product\n        read:\n          connection: source\n          format: csv\n          path: products.csv\n          options:\n            header: true\n        pattern:\n          type: dimension\n          params:\n            natural_key: product_id\n            surrogate_key: product_sk\n            scd_type: 1\n            track_cols: [name, category, price]\n            unknown_member: true\n        write:\n          connection: gold\n          format: parquet\n          path: dim_product\n          mode: overwrite\n\n      - name: dim_date\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2025-01-01\"\n            end_date: \"2025-12-31\"\n            fiscal_year_start_month: 1\n            unknown_member: true\n        write:\n          connection: gold\n          format: parquet\n          path: dim_date\n          mode: overwrite\n\n  # Pipeline 2: Build fact table\n  - pipeline: build_facts\n    layer: gold\n    nodes:\n      - name: dim_customer\n        read:\n          connection: gold\n          format: parquet\n          path: dim_customer\n\n      - name: dim_product\n        read:\n          connection: gold\n          format: parquet\n          path: dim_product\n\n      - name: dim_date\n        read:\n          connection: gold\n          format: parquet\n          path: dim_date\n\n      - name: fact_sales\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          format: csv\n          path: orders.csv\n          options:\n            header: true\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, line_item_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures: [quantity, amount]\n            audit:\n              load_timestamp: true\n              source_system: \"orders_csv\"\n        write:\n          connection: gold\n          format: parquet\n          path: fact_sales\n          mode: overwrite\n</code></pre>"},{"location":"examples/canonical/THE_REFERENCE/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"examples/canonical/THE_REFERENCE/#1-dimension-pattern","title":"1. Dimension Pattern","text":"<pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id      # Your business key\n    surrogate_key: customer_sk    # Auto-generated SK\n    scd_type: 1                   # Overwrite (use 2 for history)\n    unknown_member: true          # Creates SK=0 row for orphans\n</code></pre>"},{"location":"examples/canonical/THE_REFERENCE/#2-fact-pattern-with-fk-lookups","title":"2. Fact Pattern with FK Lookups","text":"<pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, line_item_id]  # Uniqueness check\n    dimensions:\n      - source_column: customer_id   # What's in your source\n        dimension_table: dim_customer\n        dimension_key: customer_id   # Match on this\n        surrogate_key: customer_sk   # Return this\n    orphan_handling: unknown         # Orphans \u2192 SK=0\n</code></pre>"},{"location":"examples/canonical/THE_REFERENCE/#3-pipeline-ordering","title":"3. Pipeline Ordering","text":"<pre><code>pipelines:\n  - pipeline: build_dimensions    # Runs first\n  - pipeline: build_facts         # Runs second (depends on dims)\n</code></pre>"},{"location":"examples/canonical/THE_REFERENCE/#next-steps","title":"Next Steps","text":"Goal Link Add validation Contracts Reference Add SCD2 history SCD2 Pattern Production deployment Decision Guide All configuration options YAML Schema <p>This is the one example to rule them all. When in doubt, copy this.</p>"},{"location":"examples/canonical/runnable/","title":"Runnable Canonical Examples","text":"<p>Ready-to-run versions of the canonical examples. Each config uses sample data from <code>../sample_data/</code>.</p>"},{"location":"examples/canonical/runnable/#quick-start","title":"Quick Start","text":"<pre><code>cd docs/examples/canonical/runnable\n\n# Run any example\nodibi run 01_hello_world.yaml\nodibi run 03_scd2_dimension.yaml\nodibi run 04_fact_table.yaml\n</code></pre>"},{"location":"examples/canonical/runnable/#examples","title":"Examples","text":"Example Description Patterns Used 01_hello_world.yaml CSV \u2192 Parquet ingestion Basic read/write 03_scd2_dimension.yaml Customer dimension with history tracking <code>dimension</code> (SCD2) 04_fact_table.yaml Complete star schema with FK lookups <code>dimension</code>, <code>date_dimension</code>, <code>fact</code>"},{"location":"examples/canonical/runnable/#sample-data","title":"Sample Data","text":"<p>Located in <code>../sample_data/</code>:</p> File Rows Description <code>customers.csv</code> 5 Customer records (customer_id 1-5) <code>products.csv</code> 5 Product catalog (P100-P500) <code>orders.csv</code> 6 Order transactions (includes orphan customer_id=999)"},{"location":"examples/canonical/runnable/#expected-outputs","title":"Expected Outputs","text":""},{"location":"examples/canonical/runnable/#01_hello_worldyaml","title":"01_hello_world.yaml","text":"<p>Creates <code>./data/bronze/customers/</code> with 5 customer records in Parquet format.</p>"},{"location":"examples/canonical/runnable/#03_scd2_dimensionyaml","title":"03_scd2_dimension.yaml","text":"<p>Creates <code>./data/silver/dim_customer/</code> with: - 6 rows (5 customers + 1 unknown member row with customer_sk=0) - SCD2 columns: <code>valid_from</code>, <code>valid_to</code>, <code>is_current</code></p>"},{"location":"examples/canonical/runnable/#04_fact_tableyaml","title":"04_fact_table.yaml","text":"<p>Creates star schema in <code>./data/gold/</code>:</p> <pre><code>./data/gold/\n\u251c\u2500\u2500 dim_customer/     # 6 rows (5 customers + unknown member)\n\u251c\u2500\u2500 dim_product/      # 6 rows (5 products + unknown member)\n\u251c\u2500\u2500 dim_date/         # 366 rows (2025 dates + unknown member)\n\u251c\u2500\u2500 fact_sales/       # 6 rows with surrogate keys\n\u251c\u2500\u2500 stories/          # Pipeline metadata\n\u2514\u2500\u2500 _system/          # System tables\n</code></pre> <p>Key Demonstrations:</p> Feature Example Dimension pattern <code>dim_customer</code>, <code>dim_product</code> with surrogate keys Date dimension Generated <code>dim_date</code> with 19 pre-calculated columns Fact pattern FK lookups from dimensions Orphan handling <code>customer_id=999</code> \u2192 <code>customer_sk=0</code> Unknown member SK=0 rows created in all dimensions <p>Orphan Handling: Order O005 has <code>customer_id=999</code> (doesn't exist in customers.csv). The fact pattern maps it to <code>customer_sk=0</code> (unknown member).</p> <p>Note on Date Lookups: The date dimension <code>full_date</code> column uses Python <code>date</code> objects. If your source data has date strings, add a transform to convert them before the fact pattern, or use a string-based dimension key.</p>"},{"location":"examples/canonical/runnable/#validation","title":"Validation","text":"<pre><code># Validate config syntax before running\nodibi validate 04_fact_table.yaml\n\n# Run with verbose output\nodibi run 04_fact_table.yaml --verbose\n</code></pre>"},{"location":"examples/canonical/runnable/#cleanup","title":"Cleanup","text":"<pre><code># Remove generated data\nrm -rf ./data\n</code></pre>"},{"location":"examples/canonical/runnable/#troubleshooting","title":"Troubleshooting","text":"<p>\"Dimension table not found in context\" - Ensure dimension nodes run before the fact node - Check <code>depends_on</code> is set correctly</p> <p>\"No module named 'deltalake'\" - Install delta support: <code>pip install deltalake</code> - Or change <code>format: delta</code> to <code>format: parquet</code></p>"},{"location":"examples/canonical/runnable/03_scd2_demo/","title":"SCD2 History Tracking Demo","text":"<p>This example demonstrates Slowly Changing Dimension Type 2 (SCD2) history tracking.</p> <p>SCD2 maintains a complete history of changes by: - Closing the old row (setting <code>valid_to</code> and <code>is_current=false</code>) - Creating a new row with the updated values (setting <code>is_current=true</code>)</p>"},{"location":"examples/canonical/runnable/03_scd2_demo/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install odibi\n</code></pre>"},{"location":"examples/canonical/runnable/03_scd2_demo/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":""},{"location":"examples/canonical/runnable/03_scd2_demo/#step-1-initial-load","title":"Step 1: Initial Load","text":"<p>Run the initial load with original customer data:</p> <pre><code>cd docs/examples/canonical/runnable/03_scd2_demo\nodibi run step1_initial_load.yaml\n</code></pre> <p>Source Data (customers_v1.csv):</p> customer_id name tier updated_at 1 Alice Gold 2025-01-01 2 Bob Silver 2025-01-01 <p>Expected Output (dim_customer):</p> customer_sk customer_id name tier valid_from valid_to is_current 0 _UNKNOWN Unknown Unknown 1900-01-01 NULL true 1 1 Alice Gold 2025-01-01 NULL true 2 2 Bob Silver 2025-01-01 NULL true <p>All rows have <code>is_current=true</code> and <code>valid_to=NULL</code> (open-ended).</p>"},{"location":"examples/canonical/runnable/03_scd2_demo/#step-2-process-changes","title":"Step 2: Process Changes","text":"<p>Alice's tier changed from Gold to Platinum. Run the second config:</p> <pre><code>odibi run step2_with_changes.yaml\n</code></pre> <p>Source Data (customers_v2.csv):</p> customer_id name tier updated_at 1 Alice Platinum 2025-01-15 2 Bob Silver 2025-01-01 <p>Expected Output (dim_customer):</p> customer_sk customer_id name tier valid_from valid_to is_current 0 _UNKNOWN Unknown Unknown 1900-01-01 NULL true 1 1 Alice Gold 2025-01-01 2025-01-15 false 2 2 Bob Silver 2025-01-01 NULL true 3 1 Alice Platinum 2025-01-15 NULL true <p>Notice: - Row 1 (Alice - Gold): <code>valid_to</code> set to 2025-01-15, <code>is_current=false</code> - Row 3 (Alice - Platinum): New row created with new surrogate key, <code>is_current=true</code> - Row 2 (Bob): Unchanged, still <code>is_current=true</code></p>"},{"location":"examples/canonical/runnable/03_scd2_demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/canonical/runnable/03_scd2_demo/#tracked-columns","title":"Tracked Columns","text":"<p>The <code>track_cols</code> parameter specifies which columns trigger a new version:</p> <pre><code>track_cols:\n  - name\n  - email\n  - tier\n  - city\n</code></pre> <p>If any of these columns change, a new SCD2 version is created.</p>"},{"location":"examples/canonical/runnable/03_scd2_demo/#natural-key-vs-surrogate-key","title":"Natural Key vs Surrogate Key","text":"<ul> <li>Natural Key (<code>customer_id</code>): Business identifier that stays constant</li> <li>Surrogate Key (<code>customer_sk</code>): System-generated key, unique per version</li> </ul>"},{"location":"examples/canonical/runnable/03_scd2_demo/#query-patterns","title":"Query Patterns","text":"<p>Current state only: <pre><code>SELECT * FROM dim_customer WHERE is_current = true\n</code></pre></p> <p>Point-in-time lookup: <pre><code>SELECT * FROM dim_customer \nWHERE customer_id = 1 \n  AND '2025-01-10' BETWEEN valid_from AND COALESCE(valid_to, '9999-12-31')\n</code></pre></p> <p>Full history: <pre><code>SELECT * FROM dim_customer WHERE customer_id = 1 ORDER BY valid_from\n</code></pre></p>"},{"location":"examples/canonical/runnable/03_scd2_demo/#files-in-this-demo","title":"Files in This Demo","text":"File Description <code>customers_v1.csv</code> Initial customer data <code>customers_v2.csv</code> Changed data (Alice: Gold \u2192 Platinum) <code>step1_initial_load.yaml</code> Creates initial dimension <code>step2_with_changes.yaml</code> Processes changes, creates history"},{"location":"examples/canonical/sample_data/","title":"Sample Data for Canonical Examples","text":"<p>Copy these files to your project's <code>data/landing/</code> folder to run the examples.</p> File Used By <code>customers.csv</code> Examples 1, 3 (Hello World, SCD2) <code>orders.csv</code> Examples 4, 5 (Fact Table, Full Pipeline) <code>products.csv</code> Example 4 (Fact Table)"},{"location":"examples/canonical/sample_data/#setup","title":"Setup","text":"<pre><code># From your project root\nmkdir -p data/landing\ncp docs/examples/canonical/sample_data/*.csv data/landing/\n</code></pre>"},{"location":"explanation/architecture/","title":"Architecture Guide - Odibi System Design","text":"<p>Visual guide to how Odibi works. See the big picture!</p>"},{"location":"explanation/architecture/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        USER                                  \u2502\n\u2502                          \u2502                                   \u2502\n\u2502                          \u25bc                                   \u2502\n\u2502                   config.yaml                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   CONFIG LAYER                               \u2502\n\u2502                                                               \u2502\n\u2502  config.yaml \u2192 Pydantic Models \u2192 ProjectConfig               \u2502\n\u2502                     \u2193                                         \u2502\n\u2502              Validation happens here                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  PIPELINE LAYER                              \u2502\n\u2502                                                               \u2502\n\u2502  Pipeline \u2192 DependencyGraph \u2192 Execution Order                \u2502\n\u2502                     \u2193                                         \u2502\n\u2502              [Node A, Node B, Node C]                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   NODE LAYER                                 \u2502\n\u2502                                                               \u2502\n\u2502  Node \u2192 Read/Transform/Write \u2192 Engine                        \u2502\n\u2502           \u2193                      \u2193                            \u2502\n\u2502    Transformation           PandasEngine                      \u2502\n\u2502      Registry               PolarsEngine                      \u2502\n\u2502                             SparkEngine                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STATE &amp; METADATA LAYER                     \u2502\n\u2502                                                               \u2502\n\u2502  System Catalog (Delta Tables) \u2190\u2192 OpenLineage Emitter        \u2502\n\u2502           \u2193                           \u2193                       \u2502\n\u2502    _odibi_system/state          DataHub / Marquez             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STORAGE LAYER                              \u2502\n\u2502                                                               \u2502\n\u2502  Connections \u2192 Local / Azure / SQL                           \u2502\n\u2502                     \u2193                                         \u2502\n\u2502               Actual Data                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STORY LAYER                                \u2502\n\u2502                                                               \u2502\n\u2502  Metadata \u2192 Renderers \u2192 HTML/MD/JSON                         \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Automatic audit trail                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"explanation/architecture/#pipeline-execution-flow","title":"Pipeline Execution Flow","text":""},{"location":"explanation/architecture/#step-by-step-what-happens-when-you-run-odibi-run-configyaml","title":"Step-by-Step: What Happens When You Run <code>odibi run config.yaml</code>","text":"<pre><code>1. CLI Entry Point (cli/main.py)\n   \u2502\n   \u251c\u2500\u2192 Parse arguments\n   \u2514\u2500\u2192 Call run_command(args)\n\n2. Load Configuration (cli/run.py)\n   \u2502\n   \u251c\u2500\u2192 Read YAML file\n   \u251c\u2500\u2192 Parse to ProjectConfig (Pydantic validation)\n   \u2514\u2500\u2192 Create PipelineManager\n\n3. Build Dependency Graph (graph.py)\n   \u2502\n   \u251c\u2500\u2192 Extract all nodes\n   \u251c\u2500\u2192 Build dependency edges\n   \u251c\u2500\u2192 Check for cycles\n   \u2514\u2500\u2192 Topological sort \u2192 execution order\n\n4. Execute Nodes (pipeline.py)\n   \u2502\n   \u251c\u2500\u2192 For each node in order:\n   \u2502   \u2502\n   \u2502   \u251c\u2500\u2192 Create Node instance (node.py)\n   \u2502   \u251c\u2500\u2192 Execute read/transform/write\n   \u2502   \u2502   \u2502\n   \u2502   \u2502   \u251c\u2500\u2192 Read: Engine.read() \u2192 DataFrame\n   \u2502   \u2502   \u251c\u2500\u2192 Transform: Registry.get(operation) \u2192 transformed DataFrame  \n   \u2502   \u2502   \u2514\u2500\u2192 Write: Engine.write(DataFrame)\n   \u2502   \u2502\n   \u2502   \u251c\u2500\u2192 Store result in Context\n   \u2502   \u2514\u2500\u2192 Track metadata (timing, rows, schema)\n   \u2502\n   \u2514\u2500\u2192 All nodes complete\n\n5. Generate Story (story/generator.py)\n   \u2502\n   \u251c\u2500\u2192 Collect all node metadata\n   \u251c\u2500\u2192 Calculate aggregates (success rate, total rows)\n   \u251c\u2500\u2192 Render to HTML/MD/JSON\n   \u2514\u2500\u2192 Save to stories/runs/\n\n6. Return to User\n   \u2502\n   \u2514\u2500\u2192 \"Pipeline completed successfully\" \u2705\n</code></pre>"},{"location":"explanation/architecture/#module-dependencies","title":"Module Dependencies","text":""},{"location":"explanation/architecture/#core-dependencies","title":"Core Dependencies","text":"<pre><code>config.py (no dependencies - pure Pydantic models)\n    \u2193\ncontext.py (stores DataFrames)\n    \u2193\ntransformations/ (registry + decorators)\n    \u2193\noperations/ (uses transformations)\n    \u2193\nengine/ (executes operations)\n    \u2193\nnode.py (uses engine + context)\n    \u2193\ngraph.py (orders nodes)\n    \u2193\npipeline.py (orchestrates everything)\n    \u2193\nstory/ (documents execution)\n    \u2193\ncli/ (user interface)\n</code></pre>"},{"location":"explanation/architecture/#module-relationships","title":"Module Relationships","text":"<pre><code>transformations/\n    \u251c\u2500\u2192 Used by: node.py, story/doc_story.py\n    \u2514\u2500\u2192 Uses: registry.py (core)\n\nregistry.py\n    \u251c\u2500\u2192 Used by: transformations/, engine/\n    \u2514\u2500\u2192 Uses: Nothing (singleton)\n\nstate/\n    \u251c\u2500\u2192 Used by: node.py, pipeline.py\n    \u2514\u2500\u2192 Uses: deltalake (local), spark (distributed)\n\nlineage/\n    \u251c\u2500\u2192 Used by: node.py, pipeline.py\n    \u2514\u2500\u2192 Uses: openlineage-python (optional)\n\nconnections/\n    \u251c\u2500\u2192 Used by: engine/\n    \u2514\u2500\u2192 Uses: Nothing (independent connectors)\n\nengine/\n    \u251c\u2500\u2192 Used by: node.py\n    \u2514\u2500\u2192 Uses: connections/, transformations/\n\ncli/\n    \u251c\u2500\u2192 Used by: Users!\n    \u2514\u2500\u2192 Uses: Everything\n</code></pre> <p>Key insight: <code>transformations/</code> provides the logic, <code>engine/</code> provides the horsepower, and <code>state/</code> provides the memory.</p>"},{"location":"explanation/architecture/#data-flow","title":"Data Flow","text":""},{"location":"explanation/architecture/#how-data-moves-through-a-pipeline","title":"How Data Moves Through a Pipeline","text":"<pre><code>1. User YAML Config\n   \u2193\n2. Parsed to ProjectConfig (in-memory objects)\n   \u2193\n3. Pipeline.run() starts execution\n   \u2193\n4. For each node:\n\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Node Execution                           \u2502\n   \u2502                                          \u2502\n   \u2502  1. Read Phase (if configured)           \u2502\n   \u2502     \u2514\u2500\u2192 Engine.read() \u2192 DataFrame        \u2502\n   \u2502           \u2514\u2500\u2192 Connection.get_path()      \u2502\n   \u2502                 \u2514\u2500\u2192 Actual file/DB       \u2502\n   \u2502                                          \u2502\n   \u2502  2. Transform Phase (if configured)      \u2502\n   \u2502     \u251c\u2500\u2192 Get DataFrame from context       \u2502\n   \u2502     \u251c\u2500\u2192 Registry.get(operation)          \u2502\n   \u2502     \u2514\u2500\u2192 func(df, **params) \u2192 DataFrame   \u2502\n   \u2502                                          \u2502\n   \u2502  3. Write Phase (if configured)          \u2502\n   \u2502     \u2514\u2500\u2192 Engine.write(DataFrame)          \u2502\n   \u2502           \u2514\u2500\u2192 Connection + format        \u2502\n   \u2502                                          \u2502\n   \u2502  4. Store Result                         \u2502\n   \u2502     \u2514\u2500\u2192 Context.set(node_name, df)       \u2502\n   \u2502                                          \u2502\n   \u2502  5. Track Metadata                       \u2502\n   \u2502     \u2514\u2500\u2192 NodeExecutionMetadata            \u2502\n   \u2502           \u251c\u2500\u2192 Row counts                 \u2502\n   \u2502           \u251c\u2500\u2192 Schema                     \u2502\n   \u2502           \u2514\u2500\u2192 Timing                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   \u2193\n5. All nodes complete\n   \u2193\n6. Generate Story\n   \u2514\u2500\u2192 PipelineStoryMetadata\n       \u251c\u2500\u2192 All node metadata\n       \u2514\u2500\u2192 Rendered to HTML/MD/JSON\n</code></pre>"},{"location":"explanation/architecture/#transformation-lifecycle","title":"Transformation Lifecycle","text":""},{"location":"explanation/architecture/#registration-import-time","title":"Registration (Import Time)","text":"<pre><code># When Python imports odibi/operations/unpivot.py:\n\n@transformation(\"unpivot\", category=\"reshaping\")  # \u2190 This runs immediately!\ndef unpivot(df, ...):\n    ...\n\n# What happens:\n# 1. transformation(\"unpivot\", ...) returns a decorator\n# 2. Decorator wraps unpivot function\n# 3. Decorator calls registry.register(\"unpivot\", wrapped_unpivot)\n# 4. Registry stores it globally\n# 5. Function is now available to all pipelines\n</code></pre>"},{"location":"explanation/architecture/#lookup-runtime","title":"Lookup (Runtime)","text":"<pre><code># During pipeline execution:\n\n# 1. Node config says: operation=\"unpivot\"\n# 2. Node calls: registry.get(\"unpivot\")\n# 3. Registry returns the function\n# 4. Node calls: func(df, id_vars=\"ID\", ...)\n# 5. Result returned\n</code></pre>"},{"location":"explanation/architecture/#explanation-story-generation","title":"Explanation (Story Generation)","text":"<pre><code># During story generation:\n\n# 1. Story generator calls: func.get_explanation(**params, **context)\n# 2. ExplainableFunction looks for attached explain_func\n# 3. If found: calls explain_func(**params, **context)\n# 4. Returns formatted markdown\n# 5. Included in HTML story\n</code></pre>"},{"location":"explanation/architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"explanation/architecture/#connection-abstraction","title":"Connection Abstraction","text":"<pre><code>BaseConnection (interface)\n    \u2193\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        \u2502          \u2502             \u2502\nLocal   ADLS    AzureSQL      (more...)\n\u2502        \u2502          \u2502\n\u2193        \u2193          \u2193\n./data  Azure Blob  SQL Database\n</code></pre> <p>All connections implement: - <code>get_path(relative_path)</code> - Resolve full path - <code>validate()</code> - Check configuration</p> <p>Storage-specific methods: - ADLS: <code>pandas_storage_options()</code>, <code>configure_spark()</code> - AzureSQL: <code>read_sql()</code>, <code>write_table()</code>, <code>get_engine()</code> - Local: (just path manipulation)</p>"},{"location":"explanation/architecture/#engine-abstraction","title":"Engine Abstraction","text":"<pre><code>Engine (interface)\n    \u2193\n\u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                \u2502\nPandasEngine  SparkEngine\n\u2502                \u2502\n\u2193                \u2193\nDataFrame    pyspark.DataFrame\n</code></pre> <p>All engines implement: - <code>read(connection, path, format, options)</code> - <code>write(df, connection, path, format, mode, options)</code> - <code>execute_sql(df, query)</code></p> <p>Why? Swap Pandas \u2194 Spark without changing config!</p>"},{"location":"explanation/architecture/#story-generation-architecture","title":"Story Generation Architecture","text":""},{"location":"explanation/architecture/#three-types-of-stories","title":"Three Types of Stories","text":"<pre><code>1. RUN STORIES (automatic)\n   \u2502\n   \u2514\u2500\u2192 Generated during pipeline.run()\n       \u251c\u2500\u2192 Captures actual execution\n       \u251c\u2500\u2192 Saved to stories/runs/\n       \u2514\u2500\u2192 For audit/debugging\n\n2. DOC STORIES (on-demand)\n   \u2502\n   \u2514\u2500\u2192 Generated via CLI: odibi story generate\n       \u251c\u2500\u2192 Pulls operation explanations\n       \u251c\u2500\u2192 For stakeholder communication\n       \u2514\u2500\u2192 Saved to docs/\n\n3. DIFF STORIES (comparison)\n   \u2502\n   \u2514\u2500\u2192 Generated via CLI: odibi story diff\n       \u251c\u2500\u2192 Compares two run stories\n       \u251c\u2500\u2192 Shows what changed\n       \u2514\u2500\u2192 For troubleshooting\n</code></pre>"},{"location":"explanation/architecture/#story-generation-pipeline","title":"Story Generation Pipeline","text":"<pre><code>Execution \u2192 Metadata Collection \u2192 Rendering \u2192 Output\n   \u2193              \u2193                   \u2193          \u2193\nNodes run    NodeExecution      Renderer    HTML file\n             Metadata          (HTML/MD/JSON)\n             tracked\n</code></pre>"},{"location":"explanation/architecture/#the-registry-pattern-deep-dive","title":"The Registry Pattern (Deep Dive)","text":""},{"location":"explanation/architecture/#why-this-pattern","title":"Why This Pattern?","text":"<p>Problem: How do we make operations available globally?</p> <p>Bad Solution 1: Import everything</p> <pre><code>from odibi.operations import pivot, unpivot, join, sql, ...\n# Breaks as we add more operations\n</code></pre> <p>Bad Solution 2: String-based imports</p> <pre><code>op_module = __import__(f\"odibi.operations.{operation_name}\")\n# Fragile, hard to debug\n</code></pre> <p>Good Solution: Registry</p> <pre><code># Operations register themselves:\n@transformation(\"pivot\")\ndef pivot(...): ...\n\n# Look up by name:\nfunc = registry.get(\"pivot\")\n\n# Easy! Scalable! Type-safe!\n</code></pre>"},{"location":"explanation/architecture/#registry-singleton-pattern","title":"Registry Singleton Pattern","text":"<p>One registry for entire process:</p> <pre><code># odibi/transformations/registry.py\n\n# Create once at module level\n_global_registry = TransformationRegistry()\n\ndef get_registry():\n    return _global_registry  # Always same instance\n</code></pre> <p>Benefits: - \u2705 Single source of truth - \u2705 Operations registered once - \u2705 Available everywhere - \u2705 Easy to test (registry.clear() in tests)</p>"},{"location":"explanation/architecture/#error-handling-strategy","title":"Error Handling Strategy","text":""},{"location":"explanation/architecture/#validation-layers","title":"Validation Layers","text":"<pre><code>Layer 1: Pydantic (config validation)\n   \u2193\nLayer 2: Connection.validate() (connection validation)\n   \u2193\nLayer 3: Graph.validate() (dependency validation)\n   \u2193\nLayer 4: Runtime (execution errors)\n</code></pre> <p>Fail fast: Catch errors before execution starts!</p>"},{"location":"explanation/architecture/#error-propagation","title":"Error Propagation","text":"<pre><code>try:\n    # Node execution\n    result = node.execute()\nexcept Exception as e:\n    # Caught by node.py\n    node_result = NodeResult(\n        success=False,\n        error=e\n    )\n    # Stored in metadata\n    # Shown in story\n    # Pipeline continues (or stops, depending on config)\n</code></pre> <p>Stories capture all errors - makes debugging easy!</p>"},{"location":"explanation/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"explanation/architecture/#time-complexity","title":"Time Complexity","text":"<ul> <li>Config loading: O(1) - just YAML parse</li> <li>Dependency graph: O(n + e) - n nodes, e edges</li> <li>Node execution: O(n) - linear in number of nodes</li> <li>Story generation: O(n) - linear in number of nodes</li> </ul>"},{"location":"explanation/architecture/#space-complexity","title":"Space Complexity","text":"<ul> <li>Context storage: O(n \u00d7 m) - n nodes, m average DataFrame size</li> <li>Metadata: O(n) - one metadata object per node</li> <li>Stories: O(n) - proportional to nodes</li> </ul>"},{"location":"explanation/architecture/#optimization-points","title":"Optimization Points","text":"<p>1. Parallel Execution (future) <pre><code>Current: A \u2192 B \u2192 C \u2192 D (sequential)\nFuture:  A \u2192 B \u2510\n         A \u2192 C \u2534\u2192 D (parallel)\n</code></pre></p> <p>2. Lazy Evaluation (future) <pre><code>Current: Execute all nodes\nFuture:  Only execute nodes needed for requested output\n</code></pre></p> <p>3. Incremental Processing (future) <pre><code>Current: Reprocess all data every time\nFuture:  Only process new/changed data\n</code></pre></p>"},{"location":"explanation/architecture/#testing-architecture","title":"Testing Architecture","text":""},{"location":"explanation/architecture/#test-pyramid","title":"Test Pyramid","text":"<pre><code>        /\\\n       /E2E\\          \u2190 10 tests (slow, comprehensive)\n      /\u2500\u2500\u2500\u2500\u2500\u2500\\\n     /Integration\\    \u2190 30 tests (medium speed)\n    /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n   /   Unit Tests  \\  \u2190 380+ tests (fast, focused)\n  /\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\\n</code></pre> <p>Unit Tests (380+) - Test individual functions/classes - Use mocks for external dependencies - Run in &lt;5 seconds - Cover edge cases</p> <p>Integration Tests (30) - Test components working together - Use real files (temp directories) - Run in &lt;10 seconds - Cover common scenarios</p> <p>E2E Tests (10) - Test complete pipelines - Real configs, real data - Run in &lt;30 seconds - Cover critical paths</p>"},{"location":"explanation/architecture/#mocking-strategy","title":"Mocking Strategy","text":"<p>Mock external dependencies, not internal ones:</p> <pre><code># \u2705 Good: Mock external SQLAlchemy\n@patch('sqlalchemy.create_engine')\ndef test_azure_sql(mock_engine):\n    conn = AzureSQL(...)\n    # Test connection logic without real DB\n\n# \u274c Bad: Mock internal functions\n@patch('odibi.operations.pivot')\ndef test_something(mock_pivot):\n    # Doesn't test real code!\n</code></pre>"},{"location":"explanation/architecture/#design-patterns-used","title":"Design Patterns Used","text":""},{"location":"explanation/architecture/#1-registry-pattern","title":"1. Registry Pattern","text":"<p>Where: <code>odibi/registry.py</code></p> <p>Purpose: Centralized operation lookup</p> <p>Example: All operations register themselves globally</p> <pre><code># In odibi/transformers/math.py\n@transform(\"calculate_sum\")\ndef calculate_sum(df, ...): ...\n</code></pre>"},{"location":"explanation/architecture/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>Where: <code>odibi/connections/factory.py</code></p> <p>Purpose: Create connections by type name</p> <pre><code>conn = create_connection(config)  # Returns AzureBlobConnection, LocalConnection, etc.\n</code></pre>"},{"location":"explanation/architecture/#3-adapter-pattern-state","title":"3. Adapter Pattern (State)","text":"<p>Where: <code>odibi/state/__init__.py</code></p> <p>Purpose: Uniform interface for state management</p> <pre><code># Unified CatalogStateBackend handles both local and distributed modes:\nbackend = CatalogStateBackend(...)\n# Local: uses delta-rs (writes to local delta tables)\n# Spark: uses Spark SQL (writes to delta tables on ADLS/S3)\n</code></pre>"},{"location":"explanation/architecture/#4-observer-pattern-lineage","title":"4. Observer Pattern (Lineage)","text":"<p>Where: <code>odibi/lineage/</code></p> <p>Purpose: Emit events without coupling execution logic</p> <pre><code># Node execution emits events:\nlineage.emit_start(node)\n# ... execution ...\nlineage.emit_complete(node)\n</code></pre>"},{"location":"explanation/architecture/#5-strategy-pattern","title":"5. Strategy Pattern","text":"<p>Where: <code>engine/</code> (PandasEngine vs SparkEngine vs PolarsEngine)</p> <p>Purpose: Swap execution strategies</p> <pre><code># Same interface, different implementation:\nengine = PandasEngine()  # or PolarsEngine()\ndf = engine.read(...)    # Works with either!\n</code></pre>"},{"location":"explanation/architecture/#5-builder-pattern","title":"5. Builder Pattern","text":"<p>Where: <code>story/doc_story.py</code></p> <p>Purpose: Construct complex documentation</p> <pre><code>generator = DocStoryGenerator(config)\ngenerator.generate(\n    output_path=\"doc.html\",\n    format=\"html\",\n    theme=CORPORATE_THEME\n)\n</code></pre>"},{"location":"explanation/architecture/#6-template-method-pattern","title":"6. Template Method Pattern","text":"<p>Where: <code>story/renderers.py</code></p> <p>Purpose: Define rendering algorithm skeleton</p> <pre><code>class BaseRenderer:\n    def render_to_file(self, metadata, path):\n        content = self.render(metadata)  # Subclass implements\n        self._save(content, path)        # Common logic\n</code></pre>"},{"location":"explanation/architecture/#key-abstractions","title":"Key Abstractions","text":""},{"location":"explanation/architecture/#1-engine-abstraction","title":"1. Engine Abstraction","text":"<p>Why? Support multiple execution backends</p> <pre><code># User doesn't care if Pandas or Spark:\ndf = engine.read(connection, \"data.parquet\", \"parquet\")\n\n# PandasEngine: uses pd.read_parquet()\n# SparkEngine: uses spark.read.parquet()\n# Same interface!\n</code></pre>"},{"location":"explanation/architecture/#2-connection-abstraction","title":"2. Connection Abstraction","text":"<p>Why? Support multiple storage systems</p> <pre><code># User writes: path: \"data.csv\"\n# Connection resolves to:\n# - Local: ./data/data.csv\n# - ADLS: abfss://container@account.dfs.core.windows.net/data.csv\n# - SQL: Table reference\n\n# Same code, different storage!\n</code></pre>"},{"location":"explanation/architecture/#3-transformation-abstraction","title":"3. Transformation Abstraction","text":"<p>Why? User-defined operations work same as built-in</p> <pre><code># Built-in:\n@transformation(\"pivot\")\ndef pivot(...): ...\n\n# User-defined:\n@transformation(\"my_custom_op\")\ndef my_custom_op(...): ...\n\n# Both registered the same way!\n# Both available in YAML!\n</code></pre>"},{"location":"explanation/architecture/#extensibility-points","title":"Extensibility Points","text":""},{"location":"explanation/architecture/#where-you-can-extend-odibi","title":"Where You Can Extend Odibi","text":"<p>1. Add New Operations <pre><code>Location: odibi/operations/\nPattern: Use @transformation decorator\nImpact: Available in all pipelines\n</code></pre></p> <p>2. Add New Connections <pre><code>Location: odibi/connections/\nPattern: Extend BaseConnection\nImpact: New storage backends\n</code></pre></p> <p>3. Add New Engines <pre><code>Location: odibi/engine/\nPattern: Implement Engine interface\nImpact: New execution backends\n</code></pre></p> <p>4. Add New Renderers <pre><code>Location: odibi/story/renderers.py\nPattern: Implement .render() method\nImpact: New story output formats\n</code></pre></p> <p>5. Add New Themes <pre><code>Location: odibi/story/themes.py\nPattern: Create StoryTheme instance\nImpact: Custom branding\n</code></pre></p> <p>6. Add New Validators <pre><code>Location: odibi/validation/\nPattern: Create validator class\nImpact: Quality enforcement\n</code></pre></p>"},{"location":"explanation/architecture/#configuration-model","title":"Configuration Model","text":""},{"location":"explanation/architecture/#pydantic-model-hierarchy","title":"Pydantic Model Hierarchy","text":"<pre><code>ProjectConfig (root)\n    \u251c\u2500\u2500 connections: Dict[str, ConnectionConfig]\n    \u251c\u2500\u2500 story: StoryConfig\n    \u2514\u2500\u2500 pipelines: List[PipelineConfig]\n            \u2514\u2500\u2500 nodes: List[NodeConfig]\n                    \u251c\u2500\u2500 read: ReadConfig (optional)\n                    \u251c\u2500\u2500 transform: TransformConfig (optional)\n                    \u2514\u2500\u2500 write: WriteConfig (optional)\n</code></pre>"},{"location":"explanation/architecture/#validation-flow","title":"Validation Flow","text":"<pre><code>YAML file\n    \u2193\nyaml.safe_load() \u2192 dict\n    \u2193\nProjectConfig(**dict)  \u2190 Pydantic validation happens here!\n    \u2193\nIf valid: ProjectConfig instance\nIf invalid: ValidationError with helpful message\n</code></pre> <p>Example error: <pre><code>ValidationError: 1 validation error for NodeConfig\nname\n  Field required [type=missing]\n</code></pre></p>"},{"location":"explanation/architecture/#thread-safety","title":"Thread Safety","text":""},{"location":"explanation/architecture/#current-state-single-threaded","title":"Current State: Single-Threaded","text":"<p>Registry: Thread-safe (read-only after startup) Context: NOT thread-safe (single pipeline execution) Pipeline: NOT thread-safe (sequential execution)</p>"},{"location":"explanation/architecture/#future-parallel-execution","title":"Future: Parallel Execution","text":"<p>Possible: <pre><code># Execute independent nodes in parallel\nLayer 0: [A]\nLayer 1: [B, C]  \u2190 Can run in parallel!\nLayer 2: [D]\n</code></pre></p> <p>Required changes: - Thread-safe Context - Parallel node execution - Coordinated metadata collection</p>"},{"location":"explanation/architecture/#memory-management","title":"Memory Management","text":""},{"location":"explanation/architecture/#dataframe-lifecycle","title":"DataFrame Lifecycle","text":"<pre><code>1. Read \u2192 DataFrame created (stored in memory)\n   \u2193\n2. Transform \u2192 New DataFrame (old one can be GC'd if not reused)\n   \u2193\n3. Write \u2192 DataFrame written to disk\n   \u2193\n4. Context stores DataFrame for downstream nodes\n   \u2193\n5. Pipeline completes \u2192 Context cleared \u2192 memory freed\n</code></pre>"},{"location":"explanation/architecture/#large-dataset-strategies","title":"Large Dataset Strategies","text":"<p>Option 1: Don't store in context <pre><code># Future: Streaming mode\n# Don't keep DataFrames in memory\n# Process and write immediately\n</code></pre></p> <p>Option 2: Use Spark <pre><code>engine: spark\n# Spark handles large data with partitioning\n</code></pre></p> <p>Option 3: Chunk processing <pre><code># Write in chunks\nchunksize: 10000  # Process 10K rows at a time\n</code></pre></p>"},{"location":"explanation/architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"explanation/architecture/#credential-handling","title":"Credential Handling","text":"<p>\u2705 Good: <pre><code>connections:\n  azure:\n    auth_mode: key_vault  # Credentials in Key Vault\n    key_vault_name: myvault\n    secret_name: storage-key\n</code></pre></p> <p>\u274c Bad: <pre><code>connections:\n  azure:\n    account_key: \"hardcoded_key_here\"  # DON'T!\n</code></pre></p>"},{"location":"explanation/architecture/#sql-injection-protection","title":"SQL Injection Protection","text":"<p>Odibi uses DuckDB which executes on DataFrames (not databases): - No SQL injection risk - DataFrames are local - Safe execution</p> <p>For Azure SQL, use parameterized queries:</p> <pre><code># \u2705 Safe\nconn.read_sql(\n    \"SELECT * FROM users WHERE id = :user_id\",\n    params={\"user_id\": 123}\n)\n\n# \u274c Unsafe\nconn.read_sql(f\"SELECT * FROM users WHERE id = {user_id}\")\n</code></pre>"},{"location":"explanation/architecture/#next-steps","title":"Next Steps","text":"<p>You now understand the architecture!</p> <p>Learn how to build on it: - Transformation Guide - Create custom operations - Troubleshooting - Debug issues - Read the code! Start with <code>operations/</code> directory</p> <p>Remember: The tests are comprehensive examples. Use them! \ud83e\uddea</p>"},{"location":"features/alerting/","title":"Enhanced Alerting","text":"<p>Real-time notifications for pipeline events with throttling, event-specific payloads, @mentions, and support for Slack, Teams, and generic webhooks.</p>"},{"location":"features/alerting/#overview","title":"Overview","text":"<p>Odibi's alerting system provides:</p> <ul> <li>Multiple channels: Slack, Teams (Power Automate), generic webhooks</li> <li>Event-specific payloads: Contextual information for each event type</li> <li>Throttling: Prevent alert spam with rate limiting</li> <li>@Mentions: Tag users on alerts (Teams)</li> <li>Rich formatting: Adaptive Cards for Teams, Block Kit for Slack</li> </ul>"},{"location":"features/alerting/#quick-start","title":"Quick Start","text":"<pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n    metadata:\n      mention_on_failure: \"data-team@company.com\"\n</code></pre>"},{"location":"features/alerting/#configuration","title":"Configuration","text":""},{"location":"features/alerting/#alert-config-options","title":"Alert Config Options","text":"Field Type Required Description <code>type</code> string Yes Alert type: <code>slack</code>, <code>teams</code>, <code>webhook</code> <code>url</code> string Yes Webhook URL <code>on_events</code> list No Events to trigger on (default: <code>on_failure</code>) <code>metadata</code> object No Extra settings (throttling, mentions, etc.)"},{"location":"features/alerting/#event-types","title":"Event Types","text":"Event Description Triggered When <code>on_start</code> Pipeline started Pipeline execution begins <code>on_success</code> Pipeline completed Pipeline finishes without errors <code>on_failure</code> Pipeline failed Pipeline encounters an error <code>on_quarantine</code> Rows quarantined Validation routes bad rows to quarantine <code>on_gate_block</code> Gate blocked Quality gate prevents pipeline from continuing <code>on_threshold_breach</code> Threshold exceeded A metric exceeds its configured threshold"},{"location":"features/alerting/#metadata-reference","title":"Metadata Reference","text":"<p>The <code>metadata</code> field controls throttling, mentions, and channel routing.</p> Key Type Default Description <code>throttle_minutes</code> int <code>15</code> Minimum minutes between repeated alerts for the same pipeline+event <code>max_per_hour</code> int <code>10</code> Maximum alerts of the same type per hour <code>channel</code> string \u2014 Target channel override (Slack only) <code>mention</code> string or list \u2014 User email(s) to @mention on all events (Teams only) <code>mention_on_failure</code> string or list \u2014 User email(s) to @mention on failure events only (Teams only) <p>All metadata keys are optional</p> <p>If not specified, defaults are applied automatically.</p>"},{"location":"features/alerting/#mentions-teams","title":"@Mentions (Teams)","text":"<p>Tag users directly in Teams alerts to ensure they see critical notifications.</p> <p>Single user:</p> <pre><code>metadata:\n  mention_on_failure: \"alice@company.com\"\n</code></pre> <p>Multiple users:</p> <pre><code>metadata:\n  mention_on_failure:\n    - \"alice@company.com\"\n    - \"bob@company.com\"\n    - \"charlie@company.com\"\n</code></pre> <p>Mention on all events vs failures only:</p> <pre><code>metadata:\n  mention: \"ops-team@company.com\"              # @mention on ALL events\n  mention_on_failure: \"on-call@company.com\"    # @mention ONLY on failures\n</code></pre> <p>When a failure event occurs, both <code>mention</code> and <code>mention_on_failure</code> users are tagged.</p>"},{"location":"features/alerting/#throttling","title":"Throttling","text":"<p>Prevent alert spam with time-based and rate-based throttling:</p> <pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15   # Min 15 minutes between same alerts\n      max_per_hour: 10       # Max 10 alerts per hour\n</code></pre>"},{"location":"features/alerting/#throttle-key","title":"Throttle Key","text":"<p>Throttling is applied per unique combination of:</p> <ul> <li>Pipeline name</li> <li>Event type</li> </ul> <p>So a <code>process_orders</code> pipeline failing twice in 5 minutes sends one alert, but a different pipeline can still alert.</p>"},{"location":"features/alerting/#event-specific-payloads","title":"Event-Specific Payloads","text":""},{"location":"features/alerting/#quarantine-events","title":"Quarantine Events","text":"<p>Triggered when validation routes bad rows to a quarantine table.</p> <pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n</code></pre> <p>Payload includes:</p> <ul> <li>Rows quarantined count</li> <li>Quarantine table path</li> <li>Failed test names</li> <li>Node name</li> </ul>"},{"location":"features/alerting/#gate-block-events","title":"Gate Block Events","text":"<p>Triggered when a quality gate prevents the pipeline from continuing.</p> <pre><code>alerts:\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL}\"\n    on_events:\n      - on_gate_block\n</code></pre> <p>Payload includes:</p> <ul> <li>Pass rate achieved</li> <li>Required pass rate</li> <li>Number of failed rows</li> <li>Failure reasons</li> </ul>"},{"location":"features/alerting/#threshold-breach-events","title":"Threshold Breach Events","text":"<p>Triggered when a metric exceeds its configured threshold.</p> <pre><code>alerts:\n  - type: webhook\n    url: \"https://api.example.com/alerts\"\n    on_events:\n      - on_threshold_breach\n</code></pre> <p>Payload includes:</p> <ul> <li>Metric name</li> <li>Threshold value</li> <li>Actual value</li> <li>Node name</li> </ul>"},{"location":"features/alerting/#complete-example","title":"Complete Example","text":"<p>A comprehensive alerting setup with separate channels for different severity levels:</p> <pre><code>project: SalesAnalytics\nengine: spark\n\nalerts:\n  # Critical alerts - immediate notification with @mentions\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL_CRITICAL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 5\n      max_per_hour: 20\n      mention_on_failure:\n        - \"data-lead@company.com\"\n        - \"on-call@company.com\"\n\n  # Informational alerts - pipeline lifecycle\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL_INFO}\"\n    on_events:\n      - on_start\n      - on_success\n    metadata:\n      throttle_minutes: 1\n      max_per_hour: 50\n      mention: \"data-team@company.com\"\n\n  # Data quality alerts - quarantine notifications\n  - type: teams\n    url: \"${TEAMS_WEBHOOK_URL_QUALITY}\"\n    on_events:\n      - on_quarantine\n      - on_threshold_breach\n    metadata:\n      throttle_minutes: 30\n      max_per_hour: 10\n\n  # External integration - send to monitoring system\n  - type: webhook\n    url: \"${MONITORING_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_success\n    metadata:\n      throttle_minutes: 0  # No throttling for monitoring\n\nconnections:\n  # ...\n\npipelines:\n  - pipeline: daily_sales\n    nodes:\n      - name: validate_transactions\n        validation:\n          tests:\n            - type: not_null\n              columns: [transaction_id, amount]\n              on_fail: quarantine\n          quarantine:\n            connection: silver\n            path: quarantine/transactions\n          gate:\n            require_pass_rate: 0.95\n</code></pre>"},{"location":"features/alerting/#teams-configuration","title":"Teams Configuration","text":""},{"location":"features/alerting/#power-automate-workflows","title":"Power Automate Workflows","text":"<p>Teams alerts use Power Automate workflows (classic Incoming Webhooks were retired December 2025).</p> <p>Setup:</p> <ol> <li>In Teams, go to your channel</li> <li>Click <code>...</code> \u2192 Workflows \u2192 Post to a channel when a webhook request is received</li> <li>Configure the workflow and copy the webhook URL</li> <li>Use the URL in your alert config</li> </ol> <p>Private Channels</p> <p>Power Automate bots may not have access to private channels by default. Use a standard (public) channel or explicitly add the Workflows app to your private channel.</p>"},{"location":"features/alerting/#adaptive-card-format","title":"Adaptive Card Format","text":"<p>Teams alerts use Adaptive Cards with:</p> <ul> <li>Color-coded header (red for failures, green for success, orange for warnings)</li> <li>Fact set with key metrics (duration, rows processed, etc.)</li> <li>@mentions for tagged users</li> <li>Story path for debugging</li> </ul> <p>Example card contents:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udeab Pipeline: daily_sales - FAILED        \u2502\n\u2502 Project: SalesAnalytics | Status: FAILED \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u23f1 Duration:      45.23s                  \u2502\n\u2502 \ud83d\udcc5 Time:         2026-01-02T15:30:00Z    \u2502\n\u2502 \ud83d\udcca Rows:         125,000                 \u2502\n\u2502 \ud83d\udcc2 Story:        stories/daily_sales/... \u2502\n\u2502                                          \u2502\n\u2502 \ud83d\udd14 @alice @bob                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/alerting/#slack-configuration","title":"Slack Configuration","text":""},{"location":"features/alerting/#block-kit-format","title":"Block Kit Format","text":"<p>Slack alerts use Block Kit for rich formatting:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n    metadata:\n      channel: \"#data-alerts\"  # Override default channel\n</code></pre> <p>Example card:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udeab ODIBI: daily_sales - GATE_BLOCKED     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Project:    SalesAnalytics               \u2502\n\u2502 Status:     GATE_BLOCKED                 \u2502\n\u2502 Duration:   45.23s                       \u2502\n\u2502 Pass Rate:  92.3%                        \u2502\n\u2502 Required:   95.0%                        \u2502\n\u2502 Rows Failed: 1,542                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/alerting/#generic-webhooks","title":"Generic Webhooks","text":"<p>For custom integrations with monitoring systems, ticketing tools, or other services:</p> <pre><code>alerts:\n  - type: webhook\n    url: \"https://api.example.com/webhooks/odibi\"\n    on_events:\n      - on_failure\n      - on_quarantine\n</code></pre>"},{"location":"features/alerting/#webhook-payload","title":"Webhook Payload","text":"<pre><code>{\n  \"pipeline\": \"daily_sales\",\n  \"status\": \"QUARANTINE\",\n  \"duration\": 45.23,\n  \"message\": \"150 rows quarantined in validate_transactions\",\n  \"timestamp\": \"2026-01-02T10:15:00Z\",\n  \"event_type\": \"on_quarantine\",\n  \"quarantine_details\": {\n    \"rows_quarantined\": 150,\n    \"quarantine_path\": \"silver/quarantine/transactions\",\n    \"failed_tests\": [\"not_null\", \"valid_amount\"],\n    \"node_name\": \"validate_transactions\"\n  }\n}\n</code></pre>"},{"location":"features/alerting/#programmatic-alerts","title":"Programmatic Alerts","text":"<p>Send alerts programmatically from Python code:</p> <pre><code>from odibi.utils.alerting import (\n    send_alert,\n    send_quarantine_alert,\n    send_gate_block_alert,\n)\nfrom odibi.config import AlertConfig, AlertType\n\nconfig = AlertConfig(\n    type=AlertType.TEAMS,\n    url=\"https://your-webhook-url...\",\n    metadata={\"mention_on_failure\": [\"alice@company.com\"]},\n)\n\n# Generic alert\nsend_alert(\n    config=config,\n    message=\"Custom alert message\",\n    context={\n        \"pipeline\": \"my_pipeline\",\n        \"status\": \"WARNING\",\n        \"event_type\": \"on_failure\",\n    },\n)\n\n# Quarantine alert\nsend_quarantine_alert(\n    config=config,\n    pipeline=\"daily_sales\",\n    node_name=\"validate_transactions\",\n    rows_quarantined=150,\n    quarantine_path=\"silver/quarantine/transactions\",\n    failed_tests=[\"not_null\", \"valid_amount\"],\n)\n\n# Gate block alert\nsend_gate_block_alert(\n    config=config,\n    pipeline=\"daily_sales\",\n    node_name=\"validate_transactions\",\n    pass_rate=0.92,\n    required_rate=0.95,\n    failed_rows=1542,\n    total_rows=20000,\n    failure_reasons=[\"Pass rate 92.0% &lt; required 95.0%\"],\n)\n</code></pre>"},{"location":"features/alerting/#best-practices","title":"Best Practices","text":"<ol> <li>Use throttling - Prevent alert fatigue during cascading failures</li> <li>Separate channels - Critical alerts vs informational notifications</li> <li>Use @mentions sparingly - Reserve for truly critical events</li> <li>Test webhooks - Verify connectivity before production deployment</li> <li>Monitor alert volume - High volumes often indicate systemic issues</li> <li>Include context - Story paths help with debugging</li> </ol>"},{"location":"features/alerting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/alerting/#alerts-not-being-sent","title":"Alerts not being sent","text":"<ol> <li>Check that the webhook URL is correct and accessible</li> <li>Verify the event type matches your <code>on_events</code> configuration</li> <li>Check throttling settings - you may be rate-limited</li> </ol>"},{"location":"features/alerting/#teams-mentions-not-working","title":"Teams @mentions not working","text":"<ol> <li>Ensure you're using the correct email address format</li> <li>The user must be a member of the Teams channel</li> <li>Power Automate workflow must have permission to mention users</li> </ol>"},{"location":"features/alerting/#power-automate-bot-not-in-roster-error","title":"Power Automate \"Bot not in roster\" error","text":"<p>The Workflows app doesn't have access to the channel. Solutions:</p> <ol> <li>Use a standard (public) channel instead of private</li> <li>Add the Workflows app to your private channel manually</li> </ol>"},{"location":"features/alerting/#related","title":"Related","text":"<ul> <li>Quarantine Tables - Quarantine event source</li> <li>Quality Gates - Gate block event source</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/catalog/","title":"System Catalog","text":"<p>Centralized governance and metadata management for pipelines, execution history, schema evolution, and lineage tracking.</p>"},{"location":"features/catalog/#overview","title":"Overview","text":"<p>Odibi's System Catalog (\"The Brain\") provides: - Pipeline Registry: Track pipeline and node definitions with version hashing - Execution History: Complete run history with metrics and duration - State Management: High-water marks (HWM) for incremental processing - Schema Evolution: Automatic tracking of schema changes over time - Lineage Tracking: Table-level upstream/downstream relationships - Pattern Compliance: Track medallion architecture adherence</p>"},{"location":"features/catalog/#configuration","title":"Configuration","text":""},{"location":"features/catalog/#basic-catalog-setup","title":"Basic Catalog Setup","text":"<pre><code>system:\n  connection: system_storage\n  path: _odibi_system\n\nconnections:\n  system_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n</code></pre>"},{"location":"features/catalog/#system-config-options","title":"System Config Options","text":"Field Type Required Description <code>connection</code> string Yes Connection name for catalog storage <code>path</code> string No Subdirectory for catalog tables (default: <code>_odibi_system</code>) <code>environment</code> string No Environment tag (e.g., <code>dev</code>, <code>qat</code>, <code>prod</code>) written to all records <code>schema_name</code> string No SQL Server schema name (default: <code>odibi_system</code>). Used when connection is SQL Server <code>sync_from</code> object No Source configuration for syncing data from another backend"},{"location":"features/catalog/#sql-server-backend","title":"SQL Server Backend","text":"<p>For centralized observability across environments, you can store system tables in SQL Server instead of Delta:</p> <pre><code>system:\n  connection: sql_server\n  schema_name: odibi_system\n  environment: prod\n\nconnections:\n  sql_server:\n    type: sql_server\n    server: myserver.database.windows.net\n    database: odibi_metadata\n    auth:\n      mode: service_principal\n</code></pre> <p>The SQL Server backend: - Auto-creates schema and tables on first use - Stores <code>meta_runs</code> and <code>meta_state</code> tables - Enables cross-environment querying from a single location - Useful when multiple dev/qat/prod environments need unified observability</p>"},{"location":"features/catalog/#environment-tagging","title":"Environment Tagging","text":"<p>Tag all system records with an environment identifier for cross-environment analysis:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n  environment: prod  # All records tagged with 'prod'\n</code></pre> <p>This enables queries like: <pre><code>SELECT * FROM meta_runs WHERE environment = 'prod' AND status = 'FAILED'\n</code></pre></p>"},{"location":"features/catalog/#system-sync","title":"System Sync","text":"<p>Sync local development data to a centralized SQL Server:</p> <pre><code>system:\n  connection: sql_server\n  schema_name: odibi_system\n  environment: prod\n  sync_from:\n    connection: local_dev\n    path: .odibi/system/\n\nconnections:\n  sql_server:\n    type: sql_server\n    server: central-server.database.windows.net\n    database: odibi_metadata\n  local_dev:\n    type: local\n    base_path: ./\n</code></pre> <p>Then run: <pre><code>odibi system sync project.yaml\n</code></pre></p> <p>This pushes local <code>meta_runs</code> and <code>meta_state</code> to the central SQL Server, re-tagging records with the target environment.</p>"},{"location":"features/catalog/#catalog-tables","title":"Catalog Tables","text":"<p>The System Catalog consists of Delta tables that automatically bootstrap on first run:</p>"},{"location":"features/catalog/#meta_pipelines","title":"meta_pipelines","text":"<p>Tracks pipeline definitions and deployment versions.</p> Column Type Description <code>pipeline_name</code> string Unique pipeline identifier <code>version_hash</code> string MD5 hash of pipeline configuration <code>description</code> string Pipeline description <code>layer</code> string Medallion layer (bronze/silver/gold) <code>schedule</code> string Cron schedule (if defined) <code>tags_json</code> string JSON array of aggregated tags <code>updated_at</code> timestamp Last deployment timestamp"},{"location":"features/catalog/#meta_nodes","title":"meta_nodes","text":"<p>Tracks node configurations within pipelines.</p> Column Type Description <code>pipeline_name</code> string Parent pipeline name <code>node_name</code> string Unique node identifier <code>version_hash</code> string MD5 hash of node configuration <code>type</code> string Node type: read/transform/write <code>config_json</code> string Full node configuration as JSON <code>updated_at</code> timestamp Last deployment timestamp"},{"location":"features/catalog/#meta_runs","title":"meta_runs","text":"<p>Execution history with metrics. Partitioned by <code>pipeline_name</code> and <code>date</code>.</p> Column Type Description <code>run_id</code> string Unique execution identifier <code>pipeline_name</code> string Pipeline name <code>node_name</code> string Node name <code>status</code> string SUCCESS, FAILED, RUNNING <code>rows_processed</code> long Number of rows processed <code>duration_ms</code> long Execution time in milliseconds <code>metrics_json</code> string Additional metrics as JSON <code>environment</code> string Environment tag (dev, qat, prod) <code>timestamp</code> timestamp Execution timestamp <code>date</code> date Partition date"},{"location":"features/catalog/#meta_state","title":"meta_state","text":"<p>High-water mark (HWM) storage for incremental processing.</p> Column Type Description <code>key</code> string HWM key (e.g., <code>pipeline.node.hwm</code>) <code>value</code> string Serialized high-water mark value (JSON) <code>environment</code> string Environment tag (dev, qat, prod) <code>updated_at</code> timestamp Last update timestamp"},{"location":"features/catalog/#meta_patterns","title":"meta_patterns","text":"<p>Tracks pattern compliance for governance.</p> Column Type Description <code>table_name</code> string Table identifier <code>pattern_type</code> string Pattern type (SCD2, append, etc.) <code>configuration</code> string Pattern configuration as JSON <code>compliance_score</code> double Compliance score (0.0 - 1.0)"},{"location":"features/catalog/#meta_schemas","title":"meta_schemas","text":"<p>Schema version history for drift detection.</p> Column Type Description <code>table_path</code> string Full table path <code>schema_version</code> long Incrementing version number <code>schema_hash</code> string MD5 hash of column definitions <code>columns</code> string JSON: <code>captured_at</code> timestamp When schema was captured <code>pipeline</code> string Pipeline that wrote the schema <code>node</code> string Node that wrote the schema <code>run_id</code> string Execution run ID <code>columns_added</code> array New columns in this version <code>columns_removed</code> array Removed columns <code>columns_type_changed</code> array Columns with type changes"},{"location":"features/catalog/#meta_lineage","title":"meta_lineage","text":"<p>Cross-pipeline table lineage relationships.</p> Column Type Description <code>source_table</code> string Source table path <code>target_table</code> string Target table path <code>source_pipeline</code> string Source pipeline (if known) <code>source_node</code> string Source node (if known) <code>target_pipeline</code> string Target pipeline <code>target_node</code> string Target node <code>relationship</code> string \"feeds\" or \"derived_from\" <code>last_observed</code> timestamp Last time relationship was seen <code>run_id</code> string Execution run ID"},{"location":"features/catalog/#meta_tables","title":"meta_tables","text":"<p>Registry of all written tables/assets for discovery.</p> Column Type Description <code>table_path</code> string Full path to the table <code>table_name</code> string Table name <code>pipeline</code> string Pipeline that owns the table <code>node</code> string Node that writes the table <code>format</code> string Storage format (delta, parquet, etc.) <code>connection</code> string Connection name <code>last_updated</code> timestamp Last write timestamp"},{"location":"features/catalog/#meta_metrics","title":"meta_metrics","text":"<p>Business metric definitions for governance and documentation.</p> Column Type Description <code>metric_name</code> string Unique metric identifier <code>definition_sql</code> string SQL definition of the metric <code>dimensions</code> array List of dimension columns <code>source_table</code> string Source table for the metric"},{"location":"features/catalog/#features","title":"Features","text":""},{"location":"features/catalog/#auto-registration","title":"Auto-Registration","text":"<p>Pipelines and nodes are automatically registered when you run them\u2014no explicit <code>deploy()</code> calls required:</p> <pre><code>from odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# Auto-registers pipeline and nodes before execution\nmanager.run(\"my_pipeline\")\n</code></pre> <p>This ensures <code>meta_pipelines</code> and <code>meta_nodes</code> are always populated. Version hashes detect configuration drift automatically.</p>"},{"location":"features/catalog/#pipeline-registration","title":"Pipeline Registration","text":"<p>For explicit registration (e.g., CI/CD pipelines), use:</p> <pre><code>from odibi.catalog import CatalogManager\n\n# Explicit registration\ncatalog.register_pipeline(pipeline_config)\n</code></pre> <p>When a pipeline's configuration changes, the <code>version_hash</code> updates, providing: - Configuration drift detection - Deployment history tracking - Audit trail for changes</p>"},{"location":"features/catalog/#schema-tracking","title":"Schema Tracking","text":"<p>Schema evolution is tracked automatically after every successful write. No manual calls required:</p> <ul> <li><code>meta_schemas</code> is updated with column changes (added, removed, type changes)</li> <li>Version numbers increment on each schema change</li> <li>Change detection compares against the previous version</li> </ul> <p>Querying schema history:</p> <pre><code># Get schema history for a table\nhistory = manager.get_schema_history(\"silver/customers\", limit=10)\n\n# Returns DataFrame with columns_added, columns_removed, columns_type_changed\n</code></pre>"},{"location":"features/catalog/#lineage-tracking","title":"Lineage Tracking","text":"<p>Lineage is tracked automatically based on node dependencies and read/write operations:</p> <ul> <li>Source tables (from <code>read</code> config) are recorded as upstream</li> <li>Target tables (from <code>write</code> config) are recorded as downstream  </li> <li>Cross-pipeline relationships are captured via <code>meta_lineage</code></li> </ul> <p>Querying lineage:</p> <pre><code># Get upstream and downstream lineage\nlineage_df = manager.get_lineage(\"silver/orders\", direction=\"both\")\n\n# Or use CatalogManager directly\nupstream = catalog.get_upstream(\"gold/order_summary\", depth=3)\ndownstream = catalog.get_downstream(\"bronze/raw_orders\", depth=3)\n</code></pre>"},{"location":"features/catalog/#run-history-and-metrics","title":"Run History and Metrics","text":"<p>Execution runs are logged automatically after each node completes:</p> <ul> <li>Status (SUCCESS/FAILURE), duration, rows processed</li> <li>Metrics stored in <code>meta_runs</code>, partitioned by pipeline and date</li> </ul> <p>Querying run history:</p> <pre><code># Get recent runs\nruns_df = manager.list_runs(pipeline=\"orders_pipeline\", limit=20)\n\n# Get average duration for a node\navg_seconds = catalog.get_average_duration(\"transform_orders\", days=7)\n</code></pre>"},{"location":"features/catalog/#asset-registration","title":"Asset Registration","text":"<p>Tables are registered automatically in <code>meta_tables</code> after writes, enabling discovery across the catalog.</p>"},{"location":"features/catalog/#catalog-optimization","title":"Catalog Optimization","text":"<p>Maintenance operations for Spark deployments:</p> <pre><code># Run VACUUM and OPTIMIZE on meta_runs\ncatalog.optimize()\n</code></pre>"},{"location":"features/catalog/#cleanup-and-removal","title":"Cleanup and Removal","text":"<p>Remove stale pipelines, nodes, or orphaned entries:</p> <pre><code># Remove a pipeline and cascade to associated nodes\ndeleted = catalog.remove_pipeline(\"old_pipeline\")\n\n# Remove a specific node\ndeleted = catalog.remove_node(\"my_pipeline\", \"deprecated_node\")\n\n# Cleanup orphans: remove entries not in current config\nresults = catalog.cleanup_orphans(project_config)\n# Returns: {\"meta_pipelines\": 2, \"meta_nodes\": 5}\n\n# Clear state entries\ncatalog.clear_state_key(\"my_pipeline::my_node::hwm\")\ncatalog.clear_state_pattern(\"my_pipeline::*\")  # Wildcards supported\n</code></pre>"},{"location":"features/catalog/#catalogmanager-api","title":"CatalogManager API","text":""},{"location":"features/catalog/#initialization","title":"Initialization","text":"<pre><code>from odibi.catalog import CatalogManager\nfrom odibi.config import SystemConfig\n\ncatalog = CatalogManager(\n    spark=spark_session,           # SparkSession (or None for Pandas)\n    config=system_config,          # SystemConfig object\n    base_path=\"abfss://...\",       # Resolved catalog path\n    engine=pandas_engine           # Optional: for Pandas mode\n)\n</code></pre>"},{"location":"features/catalog/#key-methods","title":"Key Methods","text":"Method Description <code>bootstrap()</code> Create all system tables if missing <code>register_pipeline(config)</code> Register/update pipeline definition <code>register_nodes(config)</code> Register/update node definitions <code>log_run(...)</code> Record execution run <code>track_schema(...)</code> Track schema version <code>get_schema_history(table, limit)</code> Get schema version history <code>record_lineage(...)</code> Record table lineage relationship <code>get_upstream(table, depth)</code> Get upstream dependencies <code>get_downstream(table, depth)</code> Get downstream consumers <code>get_average_duration(node, days)</code> Get average node duration <code>log_metrics(...)</code> Log business metric definitions <code>remove_pipeline(name)</code> Remove pipeline and cascade to nodes <code>remove_node(pipeline, node)</code> Remove a specific node <code>cleanup_orphans(config)</code> Remove entries not in current config <code>clear_state_key(key)</code> Remove a state entry by key <code>clear_state_pattern(pattern)</code> Remove state entries matching pattern <code>optimize()</code> Run VACUUM and OPTIMIZE (Spark only)"},{"location":"features/catalog/#pipelinemanager-query-api","title":"PipelineManager Query API","text":"<p>The <code>PipelineManager</code> provides convenient query methods that wrap catalog operations with smart path resolution:</p>"},{"location":"features/catalog/#smart-path-resolution","title":"Smart Path Resolution","text":"<p>Query methods accept user-friendly identifiers that are automatically resolved:</p> <pre><code># All these work:\nmanager.get_schema_history(\"silver/orders\")           # Relative path\nmanager.get_lineage(\"test.vw_customers\")              # Registered table\nmanager.get_lineage(\"transform_orders\")               # Node name\nmanager.get_schema_history(\"abfss://container/...\")   # Full path (as-is)\n</code></pre>"},{"location":"features/catalog/#query-methods","title":"Query Methods","text":"Method Description <code>list_registered_pipelines()</code> DataFrame of all pipelines from <code>meta_pipelines</code> <code>list_registered_nodes(pipeline=None)</code> DataFrame of nodes, optionally filtered by pipeline <code>list_runs(pipeline, node, status, limit)</code> DataFrame of recent runs with filters <code>list_tables()</code> DataFrame of registered assets from <code>meta_tables</code> <code>get_state(key)</code> Get specific state entry (HWM, etc.) as dict <code>get_all_state(prefix=None)</code> DataFrame of state entries, optionally filtered <code>clear_state(key)</code> Remove a state entry <code>get_schema_history(table, limit)</code> DataFrame of schema versions <code>get_lineage(table, direction)</code> DataFrame of upstream/downstream lineage <code>get_pipeline_status(pipeline)</code> Dict with last run status, duration, timestamp <code>get_node_stats(node, days)</code> Dict with success rate, avg duration, avg rows"},{"location":"features/catalog/#usage-examples","title":"Usage Examples","text":"<pre><code>from odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# List all registered pipelines\npipelines_df = manager.list_registered_pipelines()\n\n# List nodes in a specific pipeline\nnodes_df = manager.list_registered_nodes(pipeline=\"orders_pipeline\")\n\n# Get recent failed runs\nfailed_runs = manager.list_runs(status=\"FAILURE\", limit=20)\n\n# Get HWM state for a node\nhwm = manager.get_state(\"orders_pipeline::load_orders::hwm\")\n\n# Get lineage for a table (both directions)\nlineage_df = manager.get_lineage(\"silver/orders\", direction=\"both\")\n\n# Get node statistics\nstats = manager.get_node_stats(\"transform_orders\", days=7)\n# Returns: {\"node\": \"...\", \"runs\": 42, \"success_rate\": 0.95, \"avg_duration_s\": 12.5, ...}\n\n# Get pipeline status\nstatus = manager.get_pipeline_status(\"orders_pipeline\")\n# Returns: {\"pipeline\": \"...\", \"last_status\": \"SUCCESS\", \"last_run_at\": \"...\", ...}\n</code></pre>"},{"location":"features/catalog/#cli-integration","title":"CLI Integration","text":"<p>Query the catalog from the command line:</p>"},{"location":"features/catalog/#list-execution-runs","title":"List Execution Runs","text":"<pre><code># Recent runs\nodibi catalog runs config.yaml\n\n# Filter by pipeline, status, and time range\nodibi catalog runs config.yaml --pipeline orders_pipeline --status FAILED --days 3\n\n# JSON output\nodibi catalog runs config.yaml --format json --limit 50\n</code></pre>"},{"location":"features/catalog/#list-registered-pipelines","title":"List Registered Pipelines","text":"<pre><code>odibi catalog pipelines config.yaml\nodibi catalog pipelines config.yaml --format json\n</code></pre>"},{"location":"features/catalog/#list-registered-nodes","title":"List Registered Nodes","text":"<pre><code>odibi catalog nodes config.yaml\nodibi catalog nodes config.yaml --pipeline orders_pipeline\n</code></pre>"},{"location":"features/catalog/#view-hwm-state","title":"View HWM State","text":"<pre><code>odibi catalog state config.yaml\nodibi catalog state config.yaml --pipeline orders_pipeline\n</code></pre>"},{"location":"features/catalog/#list-registered-assets","title":"List Registered Assets","text":"<pre><code>odibi catalog tables config.yaml\nodibi catalog tables config.yaml --project MyProject\n</code></pre>"},{"location":"features/catalog/#view-execution-statistics","title":"View Execution Statistics","text":"<pre><code># Statistics for last 7 days\nodibi catalog stats config.yaml\n\n# Filter by pipeline and time range\nodibi catalog stats config.yaml --pipeline orders_pipeline --days 30\n</code></pre> <p>Output includes: - Total runs, success/failure counts - Success rate percentage - Total and average rows processed - Average and total runtime - Runs by pipeline - Most failed nodes</p>"},{"location":"features/catalog/#cli-options","title":"CLI Options","text":"Command Options <code>runs</code> <code>--pipeline</code>, <code>--node</code>, <code>--status</code>, <code>--days</code>, <code>--limit</code>, <code>--format</code> <code>pipelines</code> <code>--format</code> <code>nodes</code> <code>--pipeline</code>, <code>--format</code> <code>state</code> <code>--pipeline</code>, <code>--format</code> <code>tables</code> <code>--project</code>, <code>--format</code> <code>metrics</code> <code>--format</code> <code>patterns</code> <code>--format</code> <code>stats</code> <code>--pipeline</code>, <code>--days</code>"},{"location":"features/catalog/#system-sync-cli","title":"System Sync CLI","text":"<p>Sync system data between backends:</p> <pre><code># Sync all tables (runs + state)\nodibi system sync project.yaml\n\n# Sync with environment override\nodibi system sync project.yaml --env prod\n\n# Sync only specific tables\nodibi system sync project.yaml --tables runs\nodibi system sync project.yaml --tables state\n\n# Preview without making changes\nodibi system sync project.yaml --dry-run\n</code></pre> Command Options <code>system sync</code> <code>--env</code>, <code>--tables</code>, <code>--dry-run</code>"},{"location":"features/catalog/#complete-example","title":"Complete Example","text":""},{"location":"features/catalog/#project-configuration","title":"Project Configuration","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nsystem:\n  connection: catalog_storage\n  path: _odibi_catalog\n\nconnections:\n  catalog_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n\n  bronze:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\n  silver:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: silver\n\npipelines:\n  - pipeline: orders_bronze_to_silver\n    description: \"Transform raw orders to silver layer\"\n    layer: silver\n    nodes:\n      - name: read_raw_orders\n        type: read\n        connection: bronze\n        path: raw/orders\n        format: delta\n\n      - name: transform_orders\n        type: transform\n        input: read_raw_orders\n        transform: |\n          SELECT\n            order_id,\n            customer_id,\n            order_date,\n            total_amount\n          FROM {input}\n          WHERE order_date &gt;= '2024-01-01'\n\n      - name: write_orders\n        type: write\n        input: transform_orders\n        connection: silver\n        path: orders\n        format: delta\n        mode: merge\n        merge_keys: [order_id]\n</code></pre>"},{"location":"features/catalog/#querying-the-catalog","title":"Querying the Catalog","text":"<pre><code># Check registered pipelines\nodibi catalog pipelines config.yaml\n\n# Output:\n# pipeline_name            | layer  | description                          | version_hash | updated_at\n# -------------------------+--------+--------------------------------------+--------------+--------------------\n# orders_bronze_to_silver  | silver | Transform raw orders to silver layer | a1b2c3d4...  | 2024-01-30 10:15:00\n\n# View execution history\nodibi catalog runs config.yaml --pipeline orders_bronze_to_silver --days 7\n\n# Get statistics\nodibi catalog stats config.yaml --pipeline orders_bronze_to_silver\n\n# Output:\n# === Execution Statistics (Last 7 Days) ===\n#\n# Total Runs:     42\n# Successful:     40\n# Failed:         2\n# Success Rate:   95.2%\n#\n# Total Rows:     1,250,000\n# Avg Rows/Run:   29,762\n#\n# Avg Duration:   12.45s\n# Total Runtime:  522.90s\n</code></pre>"},{"location":"features/catalog/#programmatic-access","title":"Programmatic Access","text":"<pre><code>from odibi.pipeline import PipelineManager\n\n# Load configuration\nmanager = PipelineManager.from_yaml(\"config.yaml\")\ncatalog = manager.catalog_manager\n\n# Query schema history\nhistory = catalog.get_schema_history(\"silver/orders\")\nfor version in history:\n    print(f\"v{version['schema_version']}: {version['columns_added']} added\")\n\n# Trace lineage\nupstream = catalog.get_upstream(\"gold/order_summary\")\nfor source in upstream:\n    print(f\"  {'  ' * source['depth']}{source['source_table']}\")\n</code></pre>"},{"location":"features/catalog/#best-practices","title":"Best Practices","text":"<ol> <li>Enable catalog early - Configure the system catalog from project start</li> <li>Use descriptive names - Pipeline and node names become permanent identifiers</li> <li>Monitor statistics - Regular <code>odibi catalog stats</code> reveals performance trends</li> <li>Review schema changes - Track breaking changes before they impact downstream</li> <li>Query lineage - Understand impact before modifying source tables</li> <li>Run optimization - Periodically run <code>catalog.optimize()</code> for Spark deployments</li> </ol>"},{"location":"features/catalog/#observability-tables","title":"Observability Tables","text":"<p>In addition to the core catalog tables above, Odibi provides 8 observability tables that auto-populate on every pipeline run for leadership dashboards:</p> Table Purpose See <code>meta_pipeline_runs</code> Pipeline execution log Observability <code>meta_node_runs</code> Node execution log Observability <code>meta_failures</code> Failure details Observability <code>meta_observability_errors</code> Observability system failures Observability <code>meta_derived_applied_runs</code> Idempotency guard Observability <code>meta_daily_stats</code> Daily aggregates Observability <code>meta_pipeline_health</code> Current health snapshot Observability <code>meta_sla_status</code> Freshness compliance Observability <p>For full documentation on observability tables, see Observability Tables.</p>"},{"location":"features/catalog/#related","title":"Related","text":"<ul> <li>Observability Tables - Auto-populating tables for leadership dashboards</li> <li>Pipeline Configuration - YAML schema reference</li> <li>State Management - HWM-based incremental loads</li> <li>Alerting - Notifications for pipeline events</li> </ul>"},{"location":"features/cli/","title":"Command-Line Interface","text":"<p>The Odibi CLI provides a comprehensive set of commands for running pipelines, managing configurations, exploring lineage, and querying the System Catalog.</p>"},{"location":"features/cli/#overview","title":"Overview","text":"<p>The Odibi CLI is your primary tool for: - Pipeline execution: Run, validate, and monitor data pipelines - Configuration management: Validate and scaffold YAML configs - Catalog queries: Explore runs, nodes, and execution metadata - Lineage exploration: Trace upstream/downstream dependencies - Schema tracking: View schema history and compare versions</p>"},{"location":"features/cli/#commands","title":"Commands","text":""},{"location":"features/cli/#odibi-run","title":"odibi run","text":"<p>Execute a pipeline from a YAML configuration file.</p> <pre><code>odibi run config.yaml\n</code></pre>"},{"location":"features/cli/#options","title":"Options","text":"Flag Description <code>--env</code> Environment to use (default: <code>development</code>) <code>--dry-run</code> Simulate execution without writing data <code>--resume</code> Resume from last failure (skip successful nodes) <code>--parallel</code> Run independent nodes in parallel <code>--workers</code> Number of worker threads for parallel execution (default: 4) <code>--on-error</code> Override error handling: <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code>"},{"location":"features/cli/#examples","title":"Examples","text":"<pre><code># Basic execution\nodibi run my_pipeline.yaml\n\n# Production run with parallel execution\nodibi run my_pipeline.yaml --env production --parallel --workers 8\n\n# Test without writing data\nodibi run my_pipeline.yaml --dry-run\n\n# Resume a failed run\nodibi run my_pipeline.yaml --resume\n</code></pre>"},{"location":"features/cli/#odibi-validate","title":"odibi validate","text":"<p>Validate a YAML configuration file for syntax and logical errors.</p> <pre><code>odibi validate config.yaml\n</code></pre> <p>Validation checks include: - YAML syntax - Required fields - Connection references - Transform function existence - Node dependency cycles</p>"},{"location":"features/cli/#example-output","title":"Example Output","text":"<pre><code>[OK] Config is valid\n</code></pre> <p>Or with errors:</p> <pre><code>[!] Pipeline 'process_orders' Errors:\n  - Node 'transform_orders' references unknown connection: missing_db\n  - Circular dependency detected: nodeA -&gt; nodeB -&gt; nodeA\n\n[X] Validation failed\n</code></pre>"},{"location":"features/cli/#odibi-catalog","title":"odibi catalog","text":"<p>Query the System Catalog for execution metadata, registered pipelines, and statistics.</p> <pre><code>odibi catalog &lt;command&gt; config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands","title":"Subcommands","text":"Subcommand Description <code>runs</code> List execution runs from <code>meta_runs</code> <code>pipelines</code> List registered pipelines from <code>meta_pipelines</code> <code>nodes</code> List registered nodes from <code>meta_nodes</code> <code>state</code> List HWM state checkpoints from <code>meta_state</code> <code>tables</code> List registered assets from <code>meta_tables</code> <code>metrics</code> List metrics definitions from <code>meta_metrics</code> <code>patterns</code> List pattern compliance from <code>meta_patterns</code> <code>stats</code> Show execution statistics"},{"location":"features/cli/#common-options","title":"Common Options","text":"Flag Description <code>--format</code>, <code>-f</code> Output format: <code>table</code> (default) or <code>json</code> <code>--pipeline</code>, <code>-p</code> Filter by pipeline name <code>--days</code>, <code>-d</code> Show data from last N days (default: 7) <code>--limit</code>, <code>-l</code> Maximum number of results (default: 20) <code>--status</code>, <code>-s</code> Filter by status: <code>SUCCESS</code>, <code>FAILED</code>, <code>RUNNING</code>"},{"location":"features/cli/#examples_1","title":"Examples","text":"<pre><code># List recent runs\nodibi catalog runs config.yaml\n\n# Filter runs by pipeline and status\nodibi catalog runs config.yaml --pipeline my_etl --status FAILED --days 14\n\n# List all registered pipelines\nodibi catalog pipelines config.yaml\n\n# View nodes for a specific pipeline\nodibi catalog nodes config.yaml --pipeline silver_pipeline\n\n# Check HWM state checkpoints\nodibi catalog state config.yaml\n\n# Get execution statistics\nodibi catalog stats config.yaml --days 30\n\n# Output as JSON\nodibi catalog runs config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output_1","title":"Example Output","text":"<pre><code>run_id     | pipeline_name | node_name      | status  | rows  | duration_ms | timestamp\n-----------+---------------+----------------+---------+-------+-------------+---------------------\nabc123     | bronze_etl    | ingest_orders  | SUCCESS | 15420 | 3250        | 2024-01-30 10:15:00\ndef456     | bronze_etl    | ingest_custo...| SUCCESS | 8932  | 2100        | 2024-01-30 10:14:00\n\nShowing 2 runs from the last 7 days.\n</code></pre>"},{"location":"features/cli/#odibi-lineage","title":"odibi lineage","text":"<p>Explore cross-pipeline data lineage and perform impact analysis.</p> <pre><code>odibi lineage &lt;command&gt; &lt;table&gt; --config config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands_1","title":"Subcommands","text":"Subcommand Description <code>upstream</code> Trace upstream sources of a table <code>downstream</code> Trace downstream consumers of a table <code>impact</code> Impact analysis for schema changes"},{"location":"features/cli/#options_1","title":"Options","text":"Flag Description <code>--config</code> Path to YAML config file (required) <code>--depth</code> Maximum depth to traverse (default: 3) <code>--format</code> Output format: <code>tree</code> (default) or <code>json</code>"},{"location":"features/cli/#examples_2","title":"Examples","text":"<pre><code># Trace upstream sources\nodibi lineage upstream gold/customer_360 --config config.yaml\n\n# Trace downstream consumers\nodibi lineage downstream bronze/customers_raw --config config.yaml\n\n# Impact analysis\nodibi lineage impact bronze/customers_raw --config config.yaml --depth 5\n\n# Output as JSON\nodibi lineage upstream gold/customer_360 --config config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output-upstream","title":"Example Output (upstream)","text":"<pre><code>Upstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre>"},{"location":"features/cli/#example-output-impact","title":"Example Output (impact)","text":"<pre><code>\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 3 downstream table(s) in 2 pipeline(s)\n</code></pre>"},{"location":"features/cli/#odibi-schema","title":"odibi schema","text":"<p>Track schema version history and compare schema changes over time.</p> <pre><code>odibi schema &lt;command&gt; &lt;table&gt; --config config.yaml [options]\n</code></pre>"},{"location":"features/cli/#subcommands_2","title":"Subcommands","text":"Subcommand Description <code>history</code> Show schema version history for a table <code>diff</code> Compare two schema versions"},{"location":"features/cli/#options_2","title":"Options","text":"Flag Description <code>--config</code> Path to YAML config file (required) <code>--limit</code> Maximum versions to show (default: 10) <code>--format</code> Output format: <code>table</code> (default) or <code>json</code> <code>--from-version</code> Source version number (for diff) <code>--to-version</code> Target version number (for diff)"},{"location":"features/cli/#examples_3","title":"Examples","text":"<pre><code># View schema history\nodibi schema history silver/customers --config config.yaml\n\n# Compare specific versions\nodibi schema diff silver/customers --config config.yaml --from-version 3 --to-version 5\n\n# Output history as JSON\nodibi schema history silver/customers --config config.yaml --format json\n</code></pre>"},{"location":"features/cli/#example-output-history","title":"Example Output (history)","text":"<pre><code>Schema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre>"},{"location":"features/cli/#example-output-diff","title":"Example Output (diff)","text":"<pre><code>Schema Diff: silver/customers\nFrom v3 \u2192 v5\n============================================================\n+ loyalty_tier                   STRING               (added in v5)\n~ email                          VARCHAR \u2192 STRING\n- legacy_id                      INTEGER              (removed in v5)\n  customer_id                    INTEGER              (unchanged)\n  name                           STRING               (unchanged)\n</code></pre>"},{"location":"features/cli/#global-options","title":"Global Options","text":"<p>These options are available for all commands:</p> Flag Description <code>--log-level</code> Set logging verbosity: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> (default: <code>INFO</code>) <pre><code># Enable debug logging\nodibi run config.yaml --log-level DEBUG\n</code></pre>"},{"location":"features/cli/#examples_4","title":"Examples","text":""},{"location":"features/cli/#complete-workflow","title":"Complete Workflow","text":"<pre><code># 1. Validate configuration\nodibi validate my_pipeline.yaml\n\n# 2. Dry run to test logic\nodibi run my_pipeline.yaml --dry-run\n\n# 3. Execute pipeline\nodibi run my_pipeline.yaml --env production --parallel\n\n# 4. Check execution results\nodibi catalog runs my_pipeline.yaml --days 1\n\n# 5. View statistics\nodibi catalog stats my_pipeline.yaml --pipeline bronze_etl\n</code></pre>"},{"location":"features/cli/#debugging-a-failed-pipeline","title":"Debugging a Failed Pipeline","text":"<pre><code># Check recent failures\nodibi catalog runs config.yaml --status FAILED --limit 10\n\n# Resume from failure\nodibi run config.yaml --resume\n\n# Enable verbose logging\nodibi run config.yaml --log-level DEBUG\n</code></pre>"},{"location":"features/cli/#schema-change-impact-assessment","title":"Schema Change Impact Assessment","text":"<pre><code># Check schema history before making changes\nodibi schema history bronze/customers_raw --config config.yaml\n\n# Assess downstream impact\nodibi lineage impact bronze/customers_raw --config config.yaml\n\n# After changes, verify schema was captured\nodibi schema history bronze/customers_raw --config config.yaml --limit 1\n</code></pre>"},{"location":"features/cli/#monitoring-pipeline-health","title":"Monitoring Pipeline Health","text":"<pre><code># Daily stats check\nodibi catalog stats config.yaml --days 7\n\n# Find problematic nodes\nodibi catalog runs config.yaml --status FAILED --days 30\n\n# Check state for incremental loads\nodibi catalog state config.yaml --pipeline my_incremental_etl\n</code></pre>"},{"location":"features/cli/#introspection-commands","title":"Introspection Commands","text":"<p>These commands help AI tools and developers discover available features without reading source code.</p>"},{"location":"features/cli/#odibi-list","title":"odibi list","text":"<p>List available transformers, patterns, or connections.</p> <pre><code>odibi list &lt;transformers|patterns|connections&gt; [--format table|json]\n</code></pre>"},{"location":"features/cli/#examples_5","title":"Examples","text":"<pre><code># List all registered transformers\nodibi list transformers\n\n# List patterns as JSON (for AI tools)\nodibi list patterns --format json\n\n# List available connection types\nodibi list connections\n</code></pre>"},{"location":"features/cli/#example-output-transformers","title":"Example Output (transformers)","text":"<pre><code>Available Transformers (52):\n============================================================\n  add_prefix                     Adds a prefix to column names.\n  aggregate                      Performs grouping and aggregation via SQL.\n  fill_nulls                     Replaces null values with specified defaults.\n  ...\n</code></pre>"},{"location":"features/cli/#odibi-explain","title":"odibi explain","text":"<p>Get detailed documentation for a transformer, pattern, or connection.</p> <pre><code>odibi explain &lt;name&gt;\n</code></pre>"},{"location":"features/cli/#examples_6","title":"Examples","text":"<pre><code># Explain a transformer\nodibi explain fill_nulls\n\n# Explain a pattern\nodibi explain dimension\n\n# Explain a connection type\nodibi explain azure_sql\n</code></pre>"},{"location":"features/cli/#example-output_2","title":"Example Output","text":"<pre><code>Pattern: dimension\n============================================================\n\nDimension Pattern: Builds complete dimension tables with surrogate keys and SCD support.\n\nFeatures:\n- Auto-generate integer surrogate keys\n- SCD Type 0 (static), 1 (overwrite), 2 (history tracking)\n- Optional unknown member row (SK=0) for orphan FK handling\n- Audit columns (load_timestamp, source_system)\n\nConfiguration Options:\n    - natural_key (str): Natural/business key column name\n    - surrogate_key (str): Surrogate key column name to generate\n    - scd_type (int): 0=static, 1=overwrite, 2=history (default: 1)\n    ...\n\nExample YAML:\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n</code></pre>"},{"location":"features/cli/#related","title":"Related","text":"<ul> <li>Getting Started - Getting started with Odibi</li> <li>CLI Master Guide - Comprehensive CLI reference</li> <li>System Catalog - Catalog metadata details</li> <li>YAML Schema Reference - Configuration reference</li> </ul>"},{"location":"features/configuration/","title":"Configuration System","text":"<p>YAML-based configuration for defining projects, pipelines, and nodes with built-in validation, environment variable support, and environment-specific overrides.</p>"},{"location":"features/configuration/#overview","title":"Overview","text":"<p>Odibi's configuration system provides: - YAML-based: Human-readable, version-controllable configuration files - Pydantic validation: Type-safe configuration with helpful error messages - Environment variables: Secure secret injection with <code>${VAR}</code> syntax - Environment overrides: Dev/staging/prod configurations in a single file - Hierarchical structure: Project \u2192 Pipelines \u2192 Nodes</p>"},{"location":"features/configuration/#project-configuration","title":"Project Configuration","text":"<p><code>ProjectConfig</code> is the root configuration defining the entire Odibi project.</p>"},{"location":"features/configuration/#required-fields","title":"Required Fields","text":"Field Type Description <code>project</code> string Project name <code>connections</code> object Named connections (at least one required) <code>pipelines</code> list Pipeline definitions (at least one required) <code>story</code> object Story generation configuration <code>system</code> object System Catalog configuration"},{"location":"features/configuration/#optional-fields","title":"Optional Fields","text":"Field Type Default Description <code>engine</code> string <code>pandas</code> Execution engine: <code>spark</code>, <code>pandas</code> <code>version</code> string <code>1.0.0</code> Project version <code>description</code> string - Project description <code>owner</code> string - Project owner/contact <code>vars</code> object <code>{}</code> Global variables for substitution <code>retry</code> object See below Retry configuration <code>logging</code> object See below Logging configuration <code>alerts</code> list <code>[]</code> Alert configurations <code>performance</code> object See below Performance tuning <code>lineage</code> object - OpenLineage configuration <code>environments</code> object - Environment-specific overrides"},{"location":"features/configuration/#basic-example","title":"Basic Example","text":"<pre><code>project: \"Customer360\"\nengine: \"spark\"\nversion: \"1.0.0\"\n\nconnections:\n  bronze:\n    type: \"local\"\n    base_path: \"./data/bronze\"\n  silver:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"silver_db\"\n\nstory:\n  connection: \"bronze\"\n  path: \"stories/\"\n  retention_days: 30\n\nsystem:\n  connection: \"bronze\"\n  path: \"_odibi_system\"\n\npipelines:\n  - pipeline: \"customer_ingestion\"\n    nodes:\n      - name: \"load_customers\"\n        read: { connection: \"bronze\", format: \"csv\", path: \"customers.csv\" }\n        write: { connection: \"silver\", table: \"customers\" }\n</code></pre>"},{"location":"features/configuration/#retry-configuration","title":"Retry Configuration","text":"<pre><code>retry:\n  enabled: true\n  max_attempts: 3        # 1-10\n  backoff: \"exponential\" # exponential, linear, constant\n</code></pre>"},{"location":"features/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>logging:\n  level: \"INFO\"          # DEBUG, INFO, WARNING, ERROR\n  structured: true       # JSON logs for Splunk/Datadog\n  metadata:\n    team: \"data-platform\"\n</code></pre>"},{"location":"features/configuration/#performance-configuration","title":"Performance Configuration","text":"<pre><code>performance:\n  use_arrow: true  # Use Apache Arrow-backed DataFrames (Pandas only)\n</code></pre>"},{"location":"features/configuration/#story-configuration","title":"Story Configuration","text":"<pre><code>story:\n  connection: \"bronze\"        # Must exist in connections\n  path: \"stories/\"            # Path relative to connection\n  max_sample_rows: 10         # 0-100\n  auto_generate: true\n  retention_days: 30          # Days to keep stories\n  retention_count: 100        # Max stories to keep\n</code></pre>"},{"location":"features/configuration/#system-configuration","title":"System Configuration","text":"<pre><code>system:\n  connection: \"bronze\"        # Must exist in connections\n  path: \"_odibi_system\"       # Path relative to connection root\n</code></pre>"},{"location":"features/configuration/#lineage-configuration","title":"Lineage Configuration","text":"<pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n  api_key: \"${LINEAGE_API_KEY}\"\n</code></pre>"},{"location":"features/configuration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p><code>PipelineConfig</code> groups related nodes into a logical unit.</p> Field Type Required Description <code>pipeline</code> string Yes Pipeline name <code>description</code> string No Pipeline description <code>layer</code> string No Logical layer: <code>bronze</code>, <code>silver</code>, <code>gold</code> <code>nodes</code> list Yes List of nodes (unique names required) <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"load_users\"\n        # ...\n      - name: \"clean_users\"\n        depends_on: [\"load_users\"]\n        # ...\n</code></pre>"},{"location":"features/configuration/#node-configuration","title":"Node Configuration","text":"<p><code>NodeConfig</code> defines individual data processing steps.</p>"},{"location":"features/configuration/#core-fields","title":"Core Fields","text":"Field Type Required Description <code>name</code> string Yes Unique node name <code>description</code> string No Human-readable description <code>enabled</code> bool No If <code>false</code>, node is skipped (default: <code>true</code>) <code>tags</code> list No Tags for selective execution (<code>odibi run --tag daily</code>) <code>depends_on</code> list No Parent nodes that must complete first"},{"location":"features/configuration/#operations-at-least-one-required","title":"Operations (at least one required)","text":"Field Type Description <code>read</code> object Input operation (load data) <code>transformer</code> string Built-in transformation app (e.g., <code>deduplicate</code>, <code>scd2</code>) <code>params</code> object Parameters for transformer <code>transform</code> object Chain of transformation steps <code>write</code> object Output operation (save data)"},{"location":"features/configuration/#execution-order","title":"Execution Order","text":"<ol> <li>Read (or dependency injection if no read block)</li> <li>Transformer (the \"App\" logic)</li> <li>Transform Steps (the \"Script\" logic)</li> <li>Validation</li> <li>Write</li> </ol>"},{"location":"features/configuration/#read-configuration","title":"Read Configuration","text":"<pre><code>read:\n  connection: \"bronze\"\n  format: \"parquet\"           # csv, parquet, delta, json, sql\n  path: \"customers/\"\n  # OR for SQL\n  query: \"SELECT * FROM customers WHERE active = 1\"\n\n  # Incremental loading\n  incremental:\n    mode: \"rolling_window\"    # or \"stateful\"\n    column: \"updated_at\"\n    lookback: 3\n    unit: \"day\"\n\n  # Time travel (Delta)\n  time_travel:\n    as_of_version: 10\n    # OR as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre>"},{"location":"features/configuration/#transform-configuration","title":"Transform Configuration","text":"<pre><code>transform:\n  steps:\n    # SQL step\n    - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n    # Function step\n    - function: \"clean_text\"\n      params:\n        columns: [\"email\"]\n        case: \"lower\"\n\n    # Operation step\n    - operation: \"detect_deletes\"\n      params:\n        mode: \"sql_compare\"\n        keys: [\"customer_id\"]\n</code></pre>"},{"location":"features/configuration/#write-configuration","title":"Write Configuration","text":"<pre><code>write:\n  connection: \"silver\"\n  format: \"delta\"\n  table: \"customers\"\n  mode: \"upsert\"              # overwrite, append, upsert, append_once\n\n  # Metadata columns\n  add_metadata: true          # or selective: {extracted_at: true, source_file: false}\n</code></pre>"},{"location":"features/configuration/#validation-configuration","title":"Validation Configuration","text":"<pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id, email]\n      on_fail: quarantine     # fail, warn, quarantine\n\n    - type: unique\n      columns: [customer_id]\n\n    - type: accepted_values\n      column: status\n      values: [\"active\", \"inactive\", \"pending\"]\n\n    - type: custom_sql\n      sql: \"COUNT(*) FILTER (WHERE age &lt; 0) = 0\"\n      message: \"Negative ages found\"\n\n  quarantine:\n    connection: \"silver\"\n    path: \"quarantine/customers\"\n\n  gate:\n    require_pass_rate: 0.95   # Block if &lt; 95% pass\n</code></pre>"},{"location":"features/configuration/#contracts-pre-conditions","title":"Contracts (Pre-conditions)","text":"<pre><code>contracts:\n  - type: row_count\n    min: 1000\n    on_fail: fail\n\n  - type: freshness\n    column: \"updated_at\"\n    max_age_hours: 24\n\n  - type: schema\n    columns:\n      id: \"int\"\n      name: \"string\"\n</code></pre>"},{"location":"features/configuration/#privacy-configuration","title":"Privacy Configuration","text":"<pre><code>privacy:\n  enabled: true\n  rules:\n    - column: \"email\"\n      method: \"hash\"          # hash, mask, redact, fake\n    - column: \"ssn\"\n      method: \"mask\"\n      params:\n        pattern: \"XXX-XX-####\"\n</code></pre>"},{"location":"features/configuration/#error-handling","title":"Error Handling","text":"<pre><code>on_error: \"fail_later\"        # fail_fast, fail_later, ignore\n</code></pre> Strategy Description <code>fail_fast</code> Stop pipeline immediately on error <code>fail_later</code> Continue pipeline, skip dependents (default) <code>ignore</code> Treat as success with warning, dependents run"},{"location":"features/configuration/#complete-node-example","title":"Complete Node Example","text":"<pre><code>- name: \"process_orders\"\n  description: \"Clean and deduplicate orders\"\n  tags: [\"daily\", \"critical\"]\n  depends_on: [\"load_orders\"]\n\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"order_id\"]\n    order_by: \"updated_at DESC\"\n\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status != 'cancelled'\"\n\n  validation:\n    tests:\n      - type: not_null\n        columns: [order_id, customer_id]\n        on_fail: quarantine\n    quarantine:\n      connection: \"silver\"\n      path: \"quarantine/orders\"\n    gate:\n      require_pass_rate: 0.98\n\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"orders_clean\"\n    mode: \"upsert\"\n\n  on_error: \"fail_fast\"\n  cache: true\n  log_level: \"DEBUG\"\n</code></pre>"},{"location":"features/configuration/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR_NAME}</code> syntax to inject environment variables:</p> <pre><code>connections:\n  azure_blob:\n    type: \"azure_blob\"\n    account_name: \"myaccount\"\n    container: \"data\"\n    auth:\n      mode: \"account_key\"\n      account_key: \"${AZURE_STORAGE_KEY}\"\n\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n</code></pre> <p>Variables are resolved at configuration load time. Missing variables raise an error.</p>"},{"location":"features/configuration/#global-variables","title":"Global Variables","text":"<p>Define reusable variables in <code>vars</code>:</p> <pre><code>vars:\n  env: \"production\"\n  team: \"data-platform\"\n\nlogging:\n  metadata:\n    environment: \"${vars.env}\"\n    team: \"${vars.team}\"\n</code></pre>"},{"location":"features/configuration/#environment-overrides","title":"Environment Overrides","text":"<p>Define environment-specific configurations that override base settings:</p> <pre><code>project: \"Customer360\"\nengine: \"pandas\"\n\nconnections:\n  database:\n    type: \"sql_server\"\n    host: \"dev-server.database.windows.net\"\n    database: \"dev_db\"\n\nenvironments:\n  staging:\n    connections:\n      database:\n        host: \"staging-server.database.windows.net\"\n        database: \"staging_db\"\n\n  production:\n    engine: \"spark\"\n    connections:\n      database:\n        host: \"prod-server.database.windows.net\"\n        database: \"prod_db\"\n    logging:\n      level: \"WARNING\"\n      structured: true\n</code></pre> <p>Select environment at runtime: <pre><code>odibi run --env production\n</code></pre></p>"},{"location":"features/configuration/#validation","title":"Validation","text":"<p>Odibi uses Pydantic for configuration validation, providing:</p>"},{"location":"features/configuration/#type-checking","title":"Type Checking","text":"<pre><code># This will fail: max_attempts must be integer 1-10\nretry:\n  max_attempts: 100  # Error: ensure this value is less than or equal to 10\n</code></pre>"},{"location":"features/configuration/#required-field-validation","title":"Required Field Validation","text":"<pre><code># This will fail: 'project' is required\nengine: \"spark\"\npipelines: []\n# Error: field required - project\n</code></pre>"},{"location":"features/configuration/#cross-field-validation","title":"Cross-Field Validation","text":"<pre><code># This will fail: story.connection must exist in connections\nconnections:\n  bronze:\n    type: \"local\"\n    base_path: \"./data\"\n\nstory:\n  connection: \"silver\"  # Error: Story connection 'silver' not found\n  path: \"stories/\"\n</code></pre>"},{"location":"features/configuration/#node-validation","title":"Node Validation","text":"<pre><code># This will fail: node must have at least one operation\n- name: \"empty_node\"\n  # Error: Node 'empty_node' must have at least one of: read, transform, write, transformer\n</code></pre>"},{"location":"features/configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from odibi.config import load_config_from_file, ProjectConfig\n\n# From file (with env var substitution)\nconfig = load_config_from_file(\"odibi.yaml\")\n\n# From dict (programmatic)\nconfig = ProjectConfig(\n    project=\"MyProject\",\n    connections={\"local\": {\"type\": \"local\", \"base_path\": \"./data\"}},\n    pipelines=[...],\n    story={\"connection\": \"local\", \"path\": \"stories/\"},\n    system={\"connection\": \"local\"},\n)\n</code></pre>"},{"location":"features/configuration/#complete-example","title":"Complete Example","text":"<pre><code>project: \"E-Commerce Analytics\"\nversion: \"2.0.0\"\nengine: \"spark\"\nowner: \"data-team@company.com\"\n\nvars:\n  env: \"production\"\n\n# Resilience\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n\n# Observability\nlogging:\n  level: \"INFO\"\n  structured: true\n  metadata:\n    environment: \"${vars.env}\"\n\n# Alerting\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n\n# Performance\nperformance:\n  use_arrow: true\n\n# Lineage\nlineage:\n  url: \"http://marquez:5000\"\n  namespace: \"ecommerce\"\n\n# Connections\nconnections:\n  landing:\n    type: \"azure_blob\"\n    account_name: \"datalake\"\n    container: \"landing\"\n    auth:\n      mode: \"aad_msi\"\n\n  bronze:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"bronze\"\n\n  silver:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"silver\"\n\n  gold:\n    type: \"delta\"\n    catalog: \"spark_catalog\"\n    schema: \"gold\"\n\n# Story output\nstory:\n  connection: \"bronze\"\n  path: \"_stories/\"\n  max_sample_rows: 10\n  retention_days: 30\n\n# System catalog\nsystem:\n  connection: \"bronze\"\n  path: \"_odibi_system\"\n\n# Pipelines\npipelines:\n  - pipeline: \"orders_bronze\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: \"landing\"\n          format: \"json\"\n          path: \"orders/*.json\"\n          incremental:\n            mode: \"stateful\"\n            column: \"order_date\"\n        write:\n          connection: \"bronze\"\n          table: \"raw_orders\"\n          mode: \"append\"\n          add_metadata: true\n\n  - pipeline: \"orders_silver\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_orders\"\n        depends_on: [\"ingest_orders\"]\n\n        transformer: \"deduplicate\"\n        params:\n          keys: [\"order_id\"]\n          order_by: \"updated_at DESC\"\n\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_total &gt; 0\"\n            - function: \"clean_text\"\n              params:\n                columns: [\"customer_email\"]\n                case: \"lower\"\n\n        validation:\n          tests:\n            - type: not_null\n              columns: [order_id, customer_id]\n              on_fail: quarantine\n            - type: range\n              column: \"order_total\"\n              min: 0\n          quarantine:\n            connection: \"silver\"\n            path: \"quarantine/orders\"\n          gate:\n            require_pass_rate: 0.95\n\n        write:\n          connection: \"silver\"\n          table: \"orders\"\n          mode: \"upsert\"\n\n# Environment overrides\nenvironments:\n  dev:\n    engine: \"pandas\"\n    logging:\n      level: \"DEBUG\"\n    connections:\n      landing:\n        type: \"local\"\n        base_path: \"./test_data/landing\"\n      bronze:\n        type: \"local\"\n        base_path: \"./test_data/bronze\"\n</code></pre>"},{"location":"features/configuration/#related","title":"Related","text":"<ul> <li>YAML Schema Reference - Complete field reference</li> <li>Alerting - Alert configuration details</li> <li>Quality Gates - Validation and gates</li> <li>Quarantine Tables - Quarantine configuration</li> </ul>"},{"location":"features/connections/","title":"Connections","text":"<p>Unified connection system for accessing local filesystems, cloud storage, databases, and HTTP endpoints with pluggable authentication.</p>"},{"location":"features/connections/#overview","title":"Overview","text":"<p>Odibi's connection system provides: - Multiple backends: Local filesystem, Azure ADLS, Azure SQL, HTTP APIs - Flexible authentication: Service principals, managed identity, Key Vault, connection strings - Environment variables: Secure secret injection via <code>${VAR}</code> syntax - Plugin architecture: Register custom connection types via factory pattern</p>"},{"location":"features/connections/#built-in-connection-types","title":"Built-in Connection Types","text":"Type Description <code>local</code> Local filesystem or URI-based paths <code>local_dbfs</code> Databricks File System mock for local development <code>azure_adls</code> Azure Data Lake Storage Gen2 <code>azure_sql</code> Azure SQL Database <code>http</code> HTTP/REST API endpoints <code>delta</code> Delta Lake tables (path-based or catalog)"},{"location":"features/connections/#configuration","title":"Configuration","text":""},{"location":"features/connections/#basic-structure","title":"Basic Structure","text":"<pre><code>connections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n\n  silver:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    path_prefix: silver\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-key\n</code></pre>"},{"location":"features/connections/#connection-config-options","title":"Connection Config Options","text":"Field Type Required Description <code>type</code> string Yes Connection type (see table above) <code>auth</code> object No Authentication configuration <code>auth_mode</code> string No Authentication mode (auto-detected if omitted) <code>validation_mode</code> string No <code>eager</code> or <code>lazy</code> validation (default: <code>lazy</code>)"},{"location":"features/connections/#local-connection","title":"Local Connection","text":"<p>Simple filesystem connection for local development or mounted volumes.</p> <pre><code>connections:\n  raw_data:\n    type: local\n    base_path: ./data/raw\n\n  mounted_volume:\n    type: local\n    base_path: /mnt/storage/data\n</code></pre>"},{"location":"features/connections/#uri-based-paths","title":"URI-Based Paths","text":"<p>Supports URI schemes like <code>file://</code> or <code>dbfs:/</code>:</p> <pre><code>connections:\n  dbfs_data:\n    type: local\n    base_path: dbfs:/FileStore/data\n</code></pre>"},{"location":"features/connections/#config-options","title":"Config Options","text":"Field Type Default Description <code>base_path</code> string <code>./data</code> Base directory for all paths"},{"location":"features/connections/#local-dbfs-connection","title":"Local DBFS Connection","text":"<p>Mock DBFS for testing Databricks pipelines locally.</p> <pre><code>connections:\n  dbfs:\n    type: local_dbfs\n    root: .dbfs\n</code></pre> <p>Maps <code>dbfs:/FileStore/data.csv</code> to <code>.dbfs/FileStore/data.csv</code>.</p>"},{"location":"features/connections/#config-options_1","title":"Config Options","text":"Field Type Default Description <code>root</code> string <code>.dbfs</code> Local directory to use as DBFS root"},{"location":"features/connections/#azure-data-lake-storage-adls-connection","title":"Azure Data Lake Storage (ADLS) Connection","text":"<p>Azure Data Lake Storage Gen2 with multi-mode authentication.</p> <pre><code>connections:\n  datalake:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: datalake\n    path_prefix: bronze\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n</code></pre>"},{"location":"features/connections/#config-options_2","title":"Config Options","text":"Field Type Required Description <code>account_name</code> string Yes Storage account name <code>container</code> string Yes Container/filesystem name <code>path_prefix</code> string No Optional prefix for all paths <code>auth_mode</code> string No Authentication mode (auto-detected)"},{"location":"features/connections/#authentication-modes","title":"Authentication Modes","text":""},{"location":"features/connections/#key-vault-recommended","title":"Key Vault (Recommended)","text":"<p>Retrieves storage account key from Azure Key Vault:</p> <pre><code>connections:\n  secure_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n</code></pre>"},{"location":"features/connections/#service-principal","title":"Service Principal","text":"<p>OAuth authentication with Azure AD service principal:</p> <pre><code>connections:\n  sp_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: service_principal\n    auth:\n      tenant_id: ${AZURE_TENANT_ID}\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n</code></pre>"},{"location":"features/connections/#managed-identity","title":"Managed Identity","text":"<p>Use Azure Managed Identity (recommended for Azure-hosted workloads):</p> <pre><code>connections:\n  msi_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: managed_identity\n</code></pre>"},{"location":"features/connections/#sas-token","title":"SAS Token","text":"<p>Shared Access Signature for time-limited access:</p> <pre><code>connections:\n  sas_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: sas_token\n    auth:\n      sas_token: ${STORAGE_SAS_TOKEN}\n</code></pre>"},{"location":"features/connections/#direct-key-development-only","title":"Direct Key (Development Only)","text":"<p>\u26a0\ufe0f Not recommended for production</p> <pre><code>connections:\n  dev_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    auth_mode: direct_key\n    auth:\n      account_key: ${STORAGE_ACCOUNT_KEY}\n</code></pre>"},{"location":"features/connections/#path-resolution","title":"Path Resolution","text":"<p>ADLS connections generate <code>abfss://</code> URIs:</p> <pre><code>conn.get_path(\"folder/file.parquet\")\n# Returns: abfss://data@mystorageaccount.dfs.core.windows.net/bronze/folder/file.parquet\n</code></pre>"},{"location":"features/connections/#azure-sql-connection","title":"Azure SQL Connection","text":"<p>Azure SQL Database with SQL auth, Managed Identity, or Key Vault.</p> <pre><code>connections:\n  warehouse:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: analytics\n    auth_mode: aad_msi\n</code></pre>"},{"location":"features/connections/#config-options_3","title":"Config Options","text":"Field Type Default Description <code>host</code> / <code>server</code> string Required SQL Server hostname <code>database</code> string Required Database name <code>driver</code> string <code>ODBC Driver 18 for SQL Server</code> ODBC driver <code>port</code> int <code>1433</code> SQL Server port <code>timeout</code> int <code>30</code> Connection timeout (seconds) <code>auth_mode</code> string Auto <code>sql</code>, <code>aad_msi</code>, <code>key_vault</code>"},{"location":"features/connections/#authentication-modes_1","title":"Authentication Modes","text":""},{"location":"features/connections/#sql-authentication","title":"SQL Authentication","text":"<pre><code>connections:\n  sql_auth:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: sql\n    auth:\n      username: ${SQL_USERNAME}\n      password: ${SQL_PASSWORD}\n</code></pre>"},{"location":"features/connections/#managed-identity_1","title":"Managed Identity","text":"<pre><code>connections:\n  msi_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: aad_msi\n</code></pre>"},{"location":"features/connections/#key-vault","title":"Key Vault","text":"<pre><code>connections:\n  keyvault_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: mydb\n    auth_mode: key_vault\n    auth:\n      username: sqladmin\n      key_vault_name: my-keyvault\n      secret_name: sql-password\n</code></pre>"},{"location":"features/connections/#usage","title":"Usage","text":"<pre><code>from odibi.connections.azure_sql import AzureSQL\n\nconn = AzureSQL(\n    server=\"myserver.database.windows.net\",\n    database=\"analytics\",\n    auth_mode=\"aad_msi\",\n)\n\n# Read data\ndf = conn.read_sql(\"SELECT * FROM customers WHERE region = 'US'\")\n\n# Read entire table\ndf = conn.read_table(\"orders\", schema=\"dbo\")\n\n# Write data\nconn.write_table(df, \"processed_orders\", if_exists=\"replace\")\n\n# Execute statements\nconn.execute(\"DELETE FROM staging WHERE processed = 1\")\n</code></pre>"},{"location":"features/connections/#http-connection","title":"HTTP Connection","text":"<p>Connect to REST APIs with various authentication methods.</p> <pre><code>connections:\n  api:\n    type: http\n    base_url: https://api.example.com/v1/\n    auth:\n      token: ${API_TOKEN}\n</code></pre>"},{"location":"features/connections/#config-options_4","title":"Config Options","text":"Field Type Required Description <code>base_url</code> string Yes Base URL for API <code>headers</code> object No Default request headers <code>auth</code> object No Authentication configuration"},{"location":"features/connections/#authentication-methods","title":"Authentication Methods","text":""},{"location":"features/connections/#bearer-token","title":"Bearer Token","text":"<pre><code>connections:\n  bearer_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      token: ${API_BEARER_TOKEN}\n</code></pre>"},{"location":"features/connections/#basic-auth","title":"Basic Auth","text":"<pre><code>connections:\n  basic_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      username: ${API_USER}\n      password: ${API_PASSWORD}\n</code></pre>"},{"location":"features/connections/#api-key","title":"API Key","text":"<pre><code>connections:\n  apikey_api:\n    type: http\n    base_url: https://api.example.com/\n    auth:\n      api_key: ${API_KEY}\n      header_name: X-API-Key  # Optional, defaults to X-API-Key\n</code></pre>"},{"location":"features/connections/#custom-headers","title":"Custom Headers","text":"<pre><code>connections:\n  custom_api:\n    type: http\n    base_url: https://api.example.com/\n    headers:\n      Content-Type: application/json\n      X-Custom-Header: custom-value\n    auth:\n      token: ${API_TOKEN}\n</code></pre>"},{"location":"features/connections/#delta-connection","title":"Delta Connection","text":"<p>Delta Lake tables via path or Unity Catalog.</p>"},{"location":"features/connections/#path-based-delta","title":"Path-Based Delta","text":"<pre><code>connections:\n  delta_lake:\n    type: delta\n    path: /mnt/delta/tables\n</code></pre>"},{"location":"features/connections/#catalog-based-delta-spark","title":"Catalog-Based Delta (Spark)","text":"<pre><code>connections:\n  unity_catalog:\n    type: delta\n    catalog: main\n    schema: analytics\n</code></pre>"},{"location":"features/connections/#environment-variables","title":"Environment Variables","text":"<p>Use <code>${VAR}</code> syntax to inject secrets from environment variables:</p> <pre><code>connections:\n  secure:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: data\n    auth:\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n      tenant_id: ${AZURE_TENANT_ID}\n</code></pre> <p>Environment variables are resolved at runtime, keeping secrets out of configuration files.</p>"},{"location":"features/connections/#connection-factory","title":"Connection Factory","text":"<p>Odibi uses a plugin system for connection types. Built-in types are registered automatically.</p>"},{"location":"features/connections/#registering-custom-connections","title":"Registering Custom Connections","text":"<pre><code>from odibi.plugins import register_connection_factory\nfrom odibi.connections.base import BaseConnection\n\nclass MyCustomConnection(BaseConnection):\n    def __init__(self, endpoint: str, api_key: str):\n        self.endpoint = endpoint\n        self.api_key = api_key\n\n    def get_path(self, relative_path: str) -&gt; str:\n        return f\"{self.endpoint}/{relative_path}\"\n\n    def validate(self) -&gt; None:\n        if not self.endpoint:\n            raise ValueError(\"Endpoint is required\")\n\ndef create_custom_connection(name: str, config: dict):\n    return MyCustomConnection(\n        endpoint=config[\"endpoint\"],\n        api_key=config.get(\"api_key\", \"\"),\n    )\n\n# Register the factory\nregister_connection_factory(\"my_custom\", create_custom_connection)\n</code></pre> <p>Then use in YAML:</p> <pre><code>connections:\n  custom:\n    type: my_custom\n    endpoint: https://custom-service.example.com\n    api_key: ${CUSTOM_API_KEY}\n</code></pre>"},{"location":"features/connections/#built-in-factory-registration","title":"Built-in Factory Registration","text":"<p>Built-in connections are registered via <code>register_builtins()</code>:</p> Factory Name Connection Class <code>local</code> <code>LocalConnection</code> <code>http</code> <code>HttpConnection</code> <code>azure_blob</code> <code>AzureADLS</code> <code>azure_adls</code> <code>AzureADLS</code> <code>delta</code> <code>LocalConnection</code> or <code>DeltaCatalogConnection</code> <code>sql_server</code> <code>AzureSQL</code> <code>azure_sql</code> <code>AzureSQL</code>"},{"location":"features/connections/#complete-examples","title":"Complete Examples","text":""},{"location":"features/connections/#multi-environment-setup","title":"Multi-Environment Setup","text":"<pre><code>project: DataPipeline\nengine: spark\n\nconnections:\n  # Local development\n  local_bronze:\n    type: local\n    base_path: ./data/bronze\n\n  local_silver:\n    type: local\n    base_path: ./data/silver\n\n  # Azure production\n  azure_bronze:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    path_prefix: bronze\n    auth_mode: managed_identity\n\n  azure_silver:\n    type: azure_adls\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    path_prefix: silver\n    auth_mode: managed_identity\n\n  # SQL database\n  warehouse:\n    type: azure_sql\n    host: ${SQL_SERVER}\n    database: analytics\n    auth_mode: aad_msi\n\n  # External API\n  weather_api:\n    type: http\n    base_url: https://api.weather.com/v1/\n    auth:\n      api_key: ${WEATHER_API_KEY}\n\npipelines:\n  - pipeline: ingest_orders\n    nodes:\n      - name: read_orders\n        source:\n          connection: azure_bronze\n          path: orders/\n        # ...\n</code></pre>"},{"location":"features/connections/#service-principal-authentication","title":"Service Principal Authentication","text":"<pre><code>connections:\n  adls_sp:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data\n    path_prefix: ingestion\n    auth_mode: service_principal\n    auth:\n      tenant_id: ${AZURE_TENANT_ID}\n      client_id: ${AZURE_CLIENT_ID}\n      client_secret: ${AZURE_CLIENT_SECRET}\n\n  sql_sp:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: warehouse\n    auth_mode: sql\n    auth:\n      username: ${SQL_USER}\n      password: ${SQL_PASSWORD}\n</code></pre>"},{"location":"features/connections/#key-vault-integration","title":"Key Vault Integration","text":"<pre><code>connections:\n  secure_storage:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: sensitive-data\n    auth_mode: key_vault\n    auth:\n      key_vault_name: my-keyvault\n      secret_name: storage-account-key\n\n  secure_sql:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: secure_db\n    auth_mode: key_vault\n    auth:\n      username: sqladmin\n      key_vault_name: my-keyvault\n      secret_name: sql-admin-password\n</code></pre>"},{"location":"features/connections/#best-practices","title":"Best Practices","text":"<ol> <li>Use Managed Identity - Preferred for Azure-hosted workloads (no secrets to manage)</li> <li>Use Key Vault - Store secrets in Key Vault, not config files</li> <li>Environment variables - Use <code>${VAR}</code> for any sensitive values</li> <li>Lazy validation - Default <code>validation_mode: lazy</code> defers validation until first use</li> <li>Separate connections - Use different connections for different security zones</li> <li>Register secrets - Secrets are automatically registered for log redaction</li> </ol>"},{"location":"features/connections/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/connections/#connection-not-found-error","title":"\"Connection not found\" error","text":"<p>Symptom: <code>ConnectionError: Connection 'my_conn' not found</code></p> <p>Causes: - Typo in connection name (check spelling, case-sensitive) - Connection defined in wrong environment block - YAML indentation error</p> <p>Fix: <pre><code># Validate your config\nodibi validate config.yaml\n</code></pre></p>"},{"location":"features/connections/#azure-authentication-failures","title":"Azure authentication failures","text":"<p>Symptom: <code>AuthenticationError: DefaultAzureCredential failed</code></p> <p>Causes: - Service principal credentials incorrect or expired - Managed Identity not enabled on compute - Missing RBAC permissions on storage account</p> <p>Fixes: <pre><code># Check if Azure CLI is authenticated\naz account show\n\n# For Service Principal, verify credentials\naz login --service-principal -u $CLIENT_ID -p $CLIENT_SECRET --tenant $TENANT_ID\n\n# For Managed Identity, ensure it's enabled and has Storage Blob Data Contributor role\n</code></pre></p>"},{"location":"features/connections/#path-not-found-on-azure-adls","title":"\"Path not found\" on Azure ADLS","text":"<p>Symptom: File reads fail with path errors</p> <p>Causes: - Container name missing or incorrect - Path prefix doesn't match actual structure - SAS token doesn't have read permissions</p> <p>Fix: Verify the full path: <pre><code>connections:\n  adls_data:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: data          # Container name\n    path_prefix: bronze      # Prefix within container\n</code></pre></p> <p>The actual path read will be: <code>abfss://data@mystorageaccount.dfs.core.windows.net/bronze/&lt;your_path&gt;</code></p>"},{"location":"features/connections/#environment-variable-not-substituted","title":"Environment variable not substituted","text":"<p>Symptom: Literal <code>${VAR}</code> appears in logs or errors</p> <p>Causes: - Environment variable not set - Variable name typo - Running in wrong shell/environment</p> <p>Fix: <pre><code># Check if variable is set\necho $MY_SECRET\n\n# Use odibi secrets to validate\nodibi secrets validate config.yaml\n</code></pre></p>"},{"location":"features/connections/#related","title":"Related","text":"<ul> <li>YAML Schema Reference</li> <li>Pipeline Configuration</li> <li>Secrets Management</li> </ul>"},{"location":"features/cross-pipeline-dependencies/","title":"Cross-Pipeline Dependencies","text":"<p>Last Updated: 2025-12-03 Status: \u2705 Implemented</p>"},{"location":"features/cross-pipeline-dependencies/#overview","title":"Overview","text":"<p>Cross-pipeline dependencies enable pipelines to reference outputs from other pipelines using the <code>$pipeline.node</code> syntax. This is essential for implementing the medallion architecture pattern where silver nodes depend on bronze outputs, and gold nodes depend on silver outputs.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Bronze    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Silver    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    Gold     \u2502\n\u2502  Pipeline   \u2502     \u2502  Pipeline   \u2502     \u2502  Pipeline   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n   writes to          reads from          reads from\n   meta_outputs       $read_bronze.*      $transform_silver.*\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#use-cases","title":"Use Cases","text":""},{"location":"features/cross-pipeline-dependencies/#1-medallion-architecture-bronze-silver-gold","title":"1. Medallion Architecture (Bronze \u2192 Silver \u2192 Gold)","text":"<p>The most common pattern: ingest raw data in bronze, clean/enrich in silver, aggregate for business in gold.</p> <pre><code># Bronze: Raw ingestion\npipeline: read_bronze\nnodes:\n  - name: raw_orders\n    read:\n      connection: source_db\n      table: sales.orders\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"bronze/orders\"\n\n# Silver: Enriched data\npipeline: transform_silver\nnodes:\n  - name: enriched_orders\n    inputs:\n      orders: $read_bronze.raw_orders        # \u2190 Cross-pipeline reference\n      customers: $read_bronze.raw_customers\n    transform:\n      steps:\n        - operation: join\n          left: orders\n          right: customers\n          on: [customer_id]\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"silver/enriched_orders\"\n\n# Gold: Business aggregates\npipeline: build_gold\nnodes:\n  - name: daily_sales\n    inputs:\n      orders: $transform_silver.enriched_orders\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              DATE(order_date) as date,\n              SUM(amount) as total_sales\n            FROM orders\n            GROUP BY 1\n    write:\n      connection: lakehouse\n      format: delta\n      path: \"gold/daily_sales\"\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#2-multi-source-joins","title":"2. Multi-Source Joins","text":"<p>When a node needs to join data from multiple sources:</p> <pre><code>- name: enriched_downtime\n  inputs:\n    events: $read_bronze.shift_events\n    calendar: $read_bronze.calendar_dim\n    store: $read_bronze.store_dim\n  transform:\n    steps:\n      - operation: join\n        left: events\n        right: calendar\n        on: [date_id]\n      - operation: join\n        right: store\n        on: [store_id]\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#3-mixing-references-with-explicit-reads","title":"3. Mixing References with Explicit Reads","text":"<p>You can combine cross-pipeline references with explicit read configs:</p> <pre><code>- name: combined_data\n  inputs:\n    # Cross-pipeline reference\n    events: $read_bronze.events\n\n    # Explicit read (for data not from another pipeline)\n    reference_data:\n      connection: static_files\n      path: \"reference/lookup_table.csv\"\n      format: csv\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#yaml-syntax","title":"YAML Syntax","text":""},{"location":"features/cross-pipeline-dependencies/#the-inputs-block","title":"The <code>inputs</code> Block","text":"<pre><code>nodes:\n  - name: node_name\n    inputs:\n      &lt;input_name&gt;: $&lt;pipeline_name&gt;.&lt;node_name&gt;    # Cross-pipeline reference\n      &lt;input_name&gt;:                                  # Explicit read config\n        connection: &lt;connection_name&gt;\n        path: &lt;path&gt;\n        format: &lt;format&gt;\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#reference-syntax-pipelinenode","title":"Reference Syntax: <code>$pipeline.node</code>","text":"Component Description <code>$</code> Prefix indicating a cross-pipeline reference <code>pipeline</code> Name of the source pipeline (from <code>pipeline:</code> field) <code>.</code> Separator <code>node</code> Name of the source node (from <code>name:</code> field) <p>Examples: - <code>$read_bronze.orders</code> \u2192 Output from node <code>orders</code> in pipeline <code>read_bronze</code> - <code>$ingest_daily.customers</code> \u2192 Output from node <code>customers</code> in pipeline <code>ingest_daily</code></p>"},{"location":"features/cross-pipeline-dependencies/#how-the-meta_outputs-catalog-table-works","title":"How the <code>meta_outputs</code> Catalog Table Works","text":"<p>When a node with a <code>write</code> block completes, its output metadata is recorded in the system catalog.</p>"},{"location":"features/cross-pipeline-dependencies/#schema","title":"Schema","text":"Column Type Description <code>pipeline_name</code> STRING Pipeline identifier <code>node_name</code> STRING Node identifier <code>output_type</code> STRING <code>\"external_table\"</code> or <code>\"managed_table\"</code> <code>connection_name</code> STRING Connection used (for external tables) <code>path</code> STRING Storage path <code>format</code> STRING Data format (delta, parquet, etc.) <code>table_name</code> STRING Registered table name (if any) <code>last_run</code> TIMESTAMP Last execution time <code>row_count</code> LONG Row count at last write <code>updated_at</code> TIMESTAMP Record update time"},{"location":"features/cross-pipeline-dependencies/#resolution-flow","title":"Resolution Flow","text":"<pre><code>1. Silver node has: inputs: {events: $read_bronze.shift_events}\n\n2. At load time, Odibi queries meta_outputs:\n   SELECT * FROM meta_outputs\n   WHERE pipeline_name = 'read_bronze' AND node_name = 'shift_events'\n\n3. Returns: {connection: 'goat_prod', path: 'bronze/sales/shift_events', format: 'delta'}\n\n4. At runtime: engine.read(connection='goat_prod', path='bronze/sales/shift_events', format='delta')\n</code></pre>"},{"location":"features/cross-pipeline-dependencies/#performance-notes","title":"Performance Notes","text":""},{"location":"features/cross-pipeline-dependencies/#batch-writes-only","title":"Batch Writes Only","text":"<p>Output metadata is collected in-memory during pipeline execution and written to the catalog once at pipeline completion. This avoids per-node I/O overhead.</p> <p>Before optimization: 17 nodes \u00d7 ~2-3s = ~40s overhead After optimization: Single batch MERGE = ~2s total</p>"},{"location":"features/cross-pipeline-dependencies/#caching","title":"Caching","text":"<p>The <code>get_node_output()</code> method uses caching to avoid repeated catalog queries within the same session.</p>"},{"location":"features/cross-pipeline-dependencies/#validate-early","title":"Validate Early","text":"<p>All <code>$references</code> are validated at pipeline load time (fail fast), not at execution time. This provides immediate feedback if a referenced pipeline hasn't run.</p>"},{"location":"features/cross-pipeline-dependencies/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/cross-pipeline-dependencies/#error-no-output-found-for-pipelinenode","title":"Error: \"No output found for $pipeline.node\"","text":"<pre><code>ReferenceResolutionError: No output found for $read_bronze.shift_events.\nEnsure pipeline 'read_bronze' has run and node 'shift_events' has a write block.\n</code></pre> <p>Causes: 1. The referenced pipeline hasn't been run yet 2. The referenced node doesn't have a <code>write</code> block 3. Typo in pipeline or node name</p> <p>Solutions: 1. Run the source pipeline first: <code>odibi run bronze.yaml</code> 2. Add a <code>write</code> block to the source node 3. Check spelling matches exactly (case-sensitive)</p>"},{"location":"features/cross-pipeline-dependencies/#error-cannot-have-both-read-and-inputs","title":"Error: \"Cannot have both 'read' and 'inputs'\"","text":"<pre><code>ValidationError: Node 'my_node': Cannot have both 'read' and 'inputs'.\nUse 'read' for single-source nodes or 'inputs' for multi-source cross-pipeline dependencies.\n</code></pre> <p>Solution: Choose one approach: - Use <code>read</code> for simple single-source reads - Use <code>inputs</code> for multi-source or cross-pipeline reads</p>"},{"location":"features/cross-pipeline-dependencies/#error-invalid-reference-format","title":"Error: \"Invalid reference format\"","text":"<pre><code>ValueError: Invalid reference format: $read_bronze. Expected $pipeline.node\n</code></pre> <p>Solution: Ensure the reference includes both pipeline and node names separated by a dot.</p>"},{"location":"features/cross-pipeline-dependencies/#engine-compatibility","title":"Engine Compatibility","text":"Feature Spark Pandas Polars <code>meta_outputs</code> writes \u2705 \u2705 \u2705 <code>$pipeline.node</code> (path-based) \u2705 \u2705 \u2705 <code>$pipeline.node</code> (managed table) \u2705 \u274c \u274c <code>inputs:</code> block \u2705 \u2705 \u2705 <p>Best Practice: Always use <code>path:</code> in write config for cross-engine compatibility.</p>"},{"location":"features/cross-pipeline-dependencies/#files-changed-in-implementation","title":"Files Changed in Implementation","text":"File Changes <code>odibi/catalog.py</code> Added <code>meta_outputs</code> table, <code>register_outputs_batch()</code>, <code>get_node_output()</code> <code>odibi/config.py</code> Added <code>inputs</code> field to <code>NodeConfig</code> <code>odibi/node.py</code> Added <code>_execute_inputs_phase()</code>, <code>_create_output_record()</code> <code>odibi/pipeline.py</code> Added batch output registration at pipeline end <code>odibi/references.py</code> New module for reference resolution <code>tests/unit/test_cross_pipeline_dependencies.py</code> 26 new tests"},{"location":"features/cross-pipeline-dependencies/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Reference - NodeConfig with <code>inputs</code> field</li> <li>YAML Schema Reference - Full schema documentation</li> <li>Catalog Feature - System catalog details</li> <li>Pipelines - Pipeline execution flow</li> </ul>"},{"location":"features/diagnostics/","title":"Diagnostics","text":"<p>Tools for debugging, monitoring, and comparing pipeline runs with Delta Lake version analysis and data diff capabilities.</p>"},{"location":"features/diagnostics/#overview","title":"Overview","text":"<p>Odibi's diagnostics module provides: - Delta Diagnostics: Table history, version comparison, metrics extraction - Data Diff: Row-level comparison, schema comparison, change detection - Run Comparison: Compare pipeline executions to identify drift - History Management: Access and analyze historical pipeline runs</p>"},{"location":"features/diagnostics/#delta-diagnostics","title":"Delta Diagnostics","text":""},{"location":"features/diagnostics/#table-version-comparison","title":"Table Version Comparison","text":"<p>Compare two versions of a Delta table to understand what changed:</p> <pre><code>from odibi.diagnostics import get_delta_diff\n\n# Basic comparison (metadata only)\ndiff = get_delta_diff(\n    table_path=\"/path/to/delta/table\",\n    version_a=5,\n    version_b=10,\n    spark=spark,  # Optional: use Spark or deltalake (Pandas)\n)\n\nprint(f\"Rows changed: {diff.rows_change}\")\nprint(f\"Files changed: {diff.files_change}\")\nprint(f\"Size change: {diff.size_change_bytes} bytes\")\nprint(f\"Operations: {diff.operations}\")\n</code></pre>"},{"location":"features/diagnostics/#deltadiffresult-fields","title":"DeltaDiffResult Fields","text":"Field Type Description <code>table_path</code> str Path to the Delta table <code>version_a</code> int Start version <code>version_b</code> int End version <code>rows_change</code> int Net row count change <code>files_change</code> int Net file count change <code>size_change_bytes</code> int Net size change in bytes <code>schema_added</code> List[str] Columns added between versions <code>schema_removed</code> List[str] Columns removed between versions <code>schema_current</code> List[str] Current schema columns <code>schema_previous</code> List[str] Previous schema columns <code>operations</code> List[str] Operations that occurred between versions"},{"location":"features/diagnostics/#deep-diff-mode","title":"Deep Diff Mode","text":"<p>Enable row-level comparison for detailed analysis:</p> <pre><code># Deep comparison with key-based diff\ndiff = get_delta_diff(\n    table_path=\"/path/to/delta/table\",\n    version_a=5,\n    version_b=10,\n    spark=spark,\n    deep=True,\n    keys=[\"order_id\"],  # Primary key columns for update detection\n)\n\nprint(f\"Rows added: {diff.rows_added}\")\nprint(f\"Rows removed: {diff.rows_removed}\")\nprint(f\"Rows updated: {diff.rows_updated}\")\n\n# Sample data\nprint(\"Added rows:\", diff.sample_added[:5])\nprint(\"Removed rows:\", diff.sample_removed[:5])\nprint(\"Updated rows:\", diff.sample_updated[:5])\n</code></pre>"},{"location":"features/diagnostics/#drift-detection","title":"Drift Detection","text":"<p>Automatically detect significant changes between versions:</p> <pre><code>from odibi.diagnostics import detect_drift\n\nwarning = detect_drift(\n    table_path=\"/path/to/delta/table\",\n    current_version=10,\n    baseline_version=5,\n    spark=spark,\n    threshold_pct=10.0,  # Alert if &gt;10% row count change\n)\n\nif warning:\n    print(f\"Drift detected: {warning}\")\n</code></pre> <p>Drift detection checks for: - Schema drift: Columns added or removed - Data volume drift: Row count changes exceeding threshold</p>"},{"location":"features/diagnostics/#data-diff","title":"Data Diff","text":""},{"location":"features/diagnostics/#node-comparison","title":"Node Comparison","text":"<p>Compare two executions of the same node:</p> <pre><code>from odibi.diagnostics import diff_nodes\n\ndiff = diff_nodes(node_a, node_b)\n\nprint(f\"Status change: {diff.status_change}\")\nprint(f\"Rows diff: {diff.rows_diff}\")\nprint(f\"Schema changed: {diff.schema_change}\")\nprint(f\"SQL changed: {diff.sql_changed}\")\nprint(f\"Has drift: {diff.has_drift}\")\n</code></pre>"},{"location":"features/diagnostics/#nodediffresult-fields","title":"NodeDiffResult Fields","text":"Field Type Description <code>node_name</code> str Name of the node <code>status_change</code> str Status change (e.g., \"success -&gt; failed\") <code>rows_out_a</code> int Output rows from run A <code>rows_out_b</code> int Output rows from run B <code>rows_diff</code> int Row count difference (B - A) <code>schema_change</code> bool Whether schema changed <code>columns_added</code> List[str] Columns added in run B <code>columns_removed</code> List[str] Columns removed in run B <code>sql_changed</code> bool Whether SQL logic changed <code>config_changed</code> bool Whether configuration changed <code>transformation_changed</code> bool Whether transformation stack changed <code>delta_version_change</code> str Delta version change (e.g., \"v1 -&gt; v2\") <code>has_drift</code> bool True if any significant drift occurred"},{"location":"features/diagnostics/#run-comparison","title":"Run Comparison","text":"<p>Compare two complete pipeline runs:</p> <pre><code>from odibi.diagnostics import diff_runs\n\nrun_diff = diff_runs(run_a, run_b)\n\nprint(f\"Nodes added: {run_diff.nodes_added}\")\nprint(f\"Nodes removed: {run_diff.nodes_removed}\")\nprint(f\"Drift sources: {run_diff.drift_source_nodes}\")\nprint(f\"Impacted downstream: {run_diff.impacted_downstream_nodes}\")\n\n# Examine individual node diffs\nfor name, node_diff in run_diff.node_diffs.items():\n    if node_diff.has_drift:\n        print(f\"  {name}: {node_diff.status_change or 'data drift'}\")\n</code></pre>"},{"location":"features/diagnostics/#rundiffresult-fields","title":"RunDiffResult Fields","text":"Field Type Description <code>run_id_a</code> str Run ID of baseline <code>run_id_b</code> str Run ID of current run <code>node_diffs</code> Dict[str, NodeDiffResult] Per-node comparison results <code>nodes_added</code> List[str] Nodes present in B but not A <code>nodes_removed</code> List[str] Nodes present in A but not B <code>drift_source_nodes</code> List[str] Nodes where logic changed <code>impacted_downstream_nodes</code> List[str] Nodes affected by upstream drift"},{"location":"features/diagnostics/#historymanager","title":"HistoryManager","text":"<p>Manage and access pipeline run history:</p> <pre><code>from odibi.diagnostics import HistoryManager\n\nmanager = HistoryManager(history_path=\"stories/\")\n\n# List available runs\nruns = manager.list_runs(\"process_orders\")\nfor run in runs:\n    print(f\"Run: {run['run_id']} at {run['timestamp']}\")\n\n# Get specific runs\nlatest = manager.get_latest_run(\"process_orders\")\nspecific = manager.get_run_by_id(\"process_orders\", \"20240130_101500\")\nprevious = manager.get_previous_run(\"process_orders\", \"20240130_101500\")\n</code></pre>"},{"location":"features/diagnostics/#historymanager-methods","title":"HistoryManager Methods","text":"Method Description <code>list_runs(pipeline_name)</code> List all runs for a pipeline (newest first) <code>get_latest_run(pipeline_name)</code> Get the most recent run metadata <code>get_run_by_id(pipeline_name, run_id)</code> Get specific run by ID <code>get_previous_run(pipeline_name, run_id)</code> Get the run immediately before specified run <code>load_run(path)</code> Load run metadata from JSON file"},{"location":"features/diagnostics/#examples","title":"Examples","text":""},{"location":"features/diagnostics/#debugging-a-failed-pipeline","title":"Debugging a Failed Pipeline","text":"<pre><code>from odibi.diagnostics import HistoryManager, diff_runs\n\nmanager = HistoryManager(\"stories/\")\n\n# Get the failed run and the last successful run\nfailed_run = manager.get_latest_run(\"process_orders\")\nprevious_run = manager.get_previous_run(\"process_orders\", failed_run.run_id)\n\nif previous_run:\n    diff = diff_runs(previous_run, failed_run)\n\n    # Find what changed\n    print(\"Changes that may have caused failure:\")\n    for node in diff.drift_source_nodes:\n        node_diff = diff.node_diffs[node]\n        if node_diff.sql_changed:\n            print(f\"  - {node}: SQL logic changed\")\n        if node_diff.config_changed:\n            print(f\"  - {node}: Configuration changed\")\n</code></pre>"},{"location":"features/diagnostics/#monitoring-data-quality-over-time","title":"Monitoring Data Quality Over Time","text":"<pre><code>from odibi.diagnostics import get_delta_diff, detect_drift\n\n# Check for unexpected changes after a pipeline run\ntable_path = \"/delta/silver/orders\"\n\n# Compare with yesterday's version\ndrift_warning = detect_drift(\n    table_path=table_path,\n    current_version=100,\n    baseline_version=95,\n    spark=spark,\n    threshold_pct=5.0,  # Alert on &gt;5% change\n)\n\nif drift_warning:\n    # Get detailed diff\n    diff = get_delta_diff(\n        table_path=table_path,\n        version_a=95,\n        version_b=100,\n        spark=spark,\n        deep=True,\n    )\n\n    print(f\"Warning: {drift_warning}\")\n    print(f\"Details: +{diff.rows_added} / -{diff.rows_removed} rows\")\n\n    if diff.schema_added:\n        print(f\"New columns: {diff.schema_added}\")\n</code></pre>"},{"location":"features/diagnostics/#comparing-spark-vs-pandas-execution","title":"Comparing Spark vs Pandas Execution","text":"<p>The diagnostics module supports both Spark and Pandas (via <code>deltalake</code> library):</p> <pre><code>from odibi.diagnostics import get_delta_diff\n\n# With Spark (for large tables)\ndiff_spark = get_delta_diff(\n    table_path=\"/delta/table\",\n    version_a=1,\n    version_b=5,\n    spark=spark,\n    deep=True,\n)\n\n# With Pandas/deltalake (for local development)\ndiff_pandas = get_delta_diff(\n    table_path=\"/delta/table\",\n    version_a=1,\n    version_b=5,\n    spark=None,  # Uses deltalake library\n    deep=True,\n)\n</code></pre>"},{"location":"features/diagnostics/#tracking-schema-evolution","title":"Tracking Schema Evolution","text":"<pre><code>from odibi.diagnostics import get_delta_diff\n\ndiff = get_delta_diff(\n    table_path=\"/delta/silver/customers\",\n    version_a=0,  # Initial version\n    version_b=50,  # Current version\n    spark=spark,\n)\n\nprint(\"Schema Evolution:\")\nprint(f\"  Initial columns: {diff.schema_previous}\")\nprint(f\"  Current columns: {diff.schema_current}\")\nprint(f\"  Added over time: {diff.schema_added}\")\nprint(f\"  Removed over time: {diff.schema_removed}\")\n</code></pre>"},{"location":"features/diagnostics/#best-practices","title":"Best Practices","text":"<ol> <li>Use deep mode sparingly - Deep diff is expensive; use metadata-only diffs for routine monitoring</li> <li>Define primary keys - Key-based diff enables update detection, not just add/remove</li> <li>Set appropriate thresholds - Tune drift detection thresholds based on expected data patterns</li> <li>Store history - Enable story persistence to enable run comparisons over time</li> <li>Automate drift checks - Integrate drift detection into pipeline post-run hooks</li> </ol>"},{"location":"features/diagnostics/#related","title":"Related","text":"<ul> <li>Stories - Pipeline execution history</li> <li>Schema Tracking - Schema change monitoring</li> <li>Quality Gates - Data quality validation</li> <li>Lineage - Data lineage tracking</li> </ul>"},{"location":"features/engines/","title":"Execution Engines","text":"<p>Multi-engine architecture for flexible data processing across local development, high-performance workloads, and big data environments.</p>"},{"location":"features/engines/#overview","title":"Overview","text":"<p>Odibi's engine system provides: - Multiple backends: Pandas, Spark, Polars - Unified API: Consistent interface across engines - Automatic selection: Choose based on workload and environment - Performance tuning: Engine-specific optimizations</p>"},{"location":"features/engines/#supported-engines","title":"Supported Engines","text":"Engine Best For Dependencies <code>pandas</code> Local development, small datasets (&lt;1GB) <code>pip install odibi</code> <code>spark</code> Big data, Databricks, distributed processing <code>pip install odibi[spark]</code> <code>polars</code> High-performance local processing, medium datasets <code>pip install polars</code>"},{"location":"features/engines/#pandasengine","title":"PandasEngine","text":"<p>Default engine for local development with broad format support.</p> <p>Strengths: - Extensive format support (CSV, Parquet, JSON, Excel, Avro, Delta) - Rich ecosystem integration - Familiar API for data scientists - SQL support via DuckDB (optional)</p> <p>Best for: - Local development and testing - Small to medium datasets (&lt;1GB) - Complex transformations with pandas operations</p>"},{"location":"features/engines/#sparkengine","title":"SparkEngine","text":"<p>Distributed processing engine for big data workloads.</p> <p>Strengths: - Horizontal scalability - Native Databricks integration - Delta Lake support with ACID transactions - Streaming pipelines - Multi-account ADLS support</p> <p>Best for: - Large datasets (&gt;1GB) - Production Databricks workflows - Distributed processing - Real-time streaming</p>"},{"location":"features/engines/#polarsengine","title":"PolarsEngine","text":"<p>High-performance engine with lazy evaluation.</p> <p>Strengths: - Extremely fast (Rust-based) - Memory efficient with lazy execution - Multi-threaded by default - Native scan operations (scan_csv, scan_parquet)</p> <p>Best for: - High-performance local processing - Medium to large datasets (1GB-10GB) - CPU-bound transformations</p>"},{"location":"features/engines/#configuration","title":"Configuration","text":""},{"location":"features/engines/#basic-engine-setup","title":"Basic Engine Setup","text":"<pre><code>project: DataPipeline\nengine: pandas  # or spark, polars\n\nconnections:\n  # ...\n\npipelines:\n  # ...\n</code></pre>"},{"location":"features/engines/#engine-options","title":"Engine Options","text":"Field Type Description <code>engine</code> string Engine type: <code>pandas</code>, <code>spark</code>, <code>polars</code> <code>performance</code> object Performance tuning options"},{"location":"features/engines/#engine-selection-guide","title":"Engine Selection Guide","text":"Scenario Recommended Engine Local development <code>pandas</code> Unit testing <code>pandas</code> Databricks production <code>spark</code> Large datasets (&gt;1GB) <code>spark</code> or <code>polars</code> CPU-bound local processing <code>polars</code> Streaming pipelines <code>spark</code> Quick prototyping <code>pandas</code>"},{"location":"features/engines/#engine-api","title":"Engine API","text":"<p>All engines implement the same core interface defined in <code>Engine</code> base class.</p>"},{"location":"features/engines/#core-methods","title":"Core Methods","text":"Method Description <code>read()</code> Read data from source <code>write()</code> Write data to destination <code>execute_sql()</code> Execute SQL query <code>execute_operation()</code> Execute built-in operation (pivot, sort, etc.) <code>get_schema()</code> Get DataFrame schema <code>get_shape()</code> Get DataFrame dimensions <code>count_rows()</code> Count rows in DataFrame <code>count_nulls()</code> Count nulls in specified columns"},{"location":"features/engines/#data-operations","title":"Data Operations","text":"Method Description <code>validate_schema()</code> Validate DataFrame schema against rules <code>validate_data()</code> Validate data against validation config <code>get_sample()</code> Get sample rows as dictionaries <code>profile_nulls()</code> Calculate null percentage per column <code>harmonize_schema()</code> Match DataFrame to target schema <code>anonymize()</code> Anonymize columns (hash, mask, redact)"},{"location":"features/engines/#table-operations","title":"Table Operations","text":"Method Description <code>table_exists()</code> Check if table/path exists <code>get_table_schema()</code> Get schema of existing table <code>maintain_table()</code> Run maintenance (optimize, vacuum) <code>materialize()</code> Materialize lazy dataset into memory"},{"location":"features/engines/#custom-format-support","title":"Custom Format Support","text":"<pre><code>from odibi.engine import PandasEngine\n\ndef read_netcdf(path, **options):\n    import xarray as xr\n    return xr.open_dataset(path).to_dataframe()\n\ndef write_netcdf(df, path, **options):\n    import xarray as xr\n    xr.Dataset.from_dataframe(df).to_netcdf(path)\n\nPandasEngine.register_format(\"netcdf\", reader=read_netcdf, writer=write_netcdf)\n</code></pre>"},{"location":"features/engines/#performance-configuration","title":"Performance Configuration","text":""},{"location":"features/engines/#pandas-performance","title":"Pandas Performance","text":"<pre><code>engine: pandas\nperformance:\n  use_arrow: true    # Use PyArrow backend (faster, less memory)\n  use_duckdb: false  # Use DuckDB for SQL (experimental)\n</code></pre> <p>Arrow backend benefits: - Faster I/O for Parquet files - Reduced memory usage - Better type preservation</p>"},{"location":"features/engines/#spark-performance","title":"Spark Performance","text":"<pre><code>engine: spark\n</code></pre> <p>Spark is automatically configured with: - Arrow-based PySpark conversions - Adaptive Query Execution (AQE) - Dynamic partition overwrite mode</p> <p>Additional optimizations via write options: <pre><code>pipelines:\n  - pipeline: optimize_example\n    nodes:\n      - name: write_optimized\n        write:\n          connection: silver\n          format: delta\n          path: optimized_table\n          options:\n            optimize_write: true\n            zorder_by: [customer_id, date]\n</code></pre></p>"},{"location":"features/engines/#polars-performance","title":"Polars Performance","text":"<pre><code>engine: polars\n</code></pre> <p>Polars features: - Lazy evaluation by default (scan operations) - Automatic query optimization - Multi-threaded execution - Streaming writes (sink operations)</p>"},{"location":"features/engines/#examples","title":"Examples","text":""},{"location":"features/engines/#switching-engines","title":"Switching Engines","text":"<p>Same pipeline, different engines:</p> <pre><code># Local development\nproject: DataPipeline\nengine: pandas\n\nconnections:\n  bronze:\n    type: local\n    path: ./data/bronze\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          format: csv\n          path: orders.csv\n</code></pre> <pre><code># Production (Databricks)\nproject: DataPipeline\nengine: spark\n\nconnections:\n  bronze:\n    type: azure_adls\n    storage_account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          format: delta\n          path: orders\n</code></pre>"},{"location":"features/engines/#using-different-connections","title":"Using Different Connections","text":"<pre><code>project: MultiSource\nengine: spark\n\nconnections:\n  raw_data:\n    type: azure_adls\n    storage_account: rawstorage\n    container: raw\n\n  processed:\n    type: azure_adls\n    storage_account: procstorage\n    container: silver\n\n  sql_source:\n    type: azure_sql\n    server: myserver.database.windows.net\n    database: mydb\n\npipelines:\n  - pipeline: ingest_sql\n    nodes:\n      - name: read_sql\n        read:\n          connection: sql_source\n          format: sql\n          table: dbo.customers\n\n      - name: write_delta\n        write:\n          connection: processed\n          format: delta\n          path: customers\n</code></pre>"},{"location":"features/engines/#lazy-vs-eager-execution","title":"Lazy vs Eager Execution","text":"<pre><code># Polars with lazy execution (default)\nengine: polars\n\npipelines:\n  - pipeline: lazy_example\n    nodes:\n      - name: scan_data\n        read:\n          connection: bronze\n          format: parquet\n          path: large_dataset/*.parquet\n        # Returns LazyFrame - no data loaded yet\n\n      - name: filter_transform\n        sql: |\n          SELECT * FROM scan_data WHERE status = 'active'\n        # Still lazy - builds query plan\n\n      - name: write_result\n        write:\n          connection: silver\n          format: parquet\n          path: filtered_data\n        # Execution happens here (sink_parquet)\n</code></pre>"},{"location":"features/engines/#engine-specific-features","title":"Engine-Specific Features","text":"<p>Pandas with Delta Time Travel: <pre><code>engine: pandas\n\npipelines:\n  - pipeline: time_travel\n    nodes:\n      - name: read_historical\n        read:\n          connection: bronze\n          format: delta\n          path: orders\n          options:\n            versionAsOf: 5  # Read version 5\n</code></pre></p> <p>Spark with Streaming: <pre><code>engine: spark\n\npipelines:\n  - pipeline: streaming_ingest\n    nodes:\n      - name: stream_read\n        read:\n          connection: bronze\n          format: delta\n          path: events\n          streaming: true\n</code></pre></p>"},{"location":"features/engines/#programmatic-engine-usage","title":"Programmatic Engine Usage","text":"<pre><code>from odibi.engine import get_engine_class\n\n# Get engine by name\nEngineClass = get_engine_class(\"pandas\")\nengine = EngineClass(connections=my_connections)\n\n# Read data\ndf = engine.read(\n    connection=my_connection,\n    format=\"parquet\",\n    path=\"data/*.parquet\"\n)\n\n# Execute SQL\nfrom odibi.context import PandasContext\nctx = PandasContext()\nctx.register(\"orders\", df)\nresult = engine.execute_sql(\"SELECT * FROM orders WHERE total &gt; 100\", ctx)\n\n# Write data\nengine.write(\n    df=result,\n    connection=output_connection,\n    format=\"delta\",\n    path=\"filtered_orders\",\n    mode=\"overwrite\"\n)\n</code></pre>"},{"location":"features/engines/#register-custom-engine","title":"Register Custom Engine","text":"<pre><code>from odibi.engine import Engine, register_engine\n\nclass DuckDBEngine(Engine):\n    name = \"duckdb\"\n\n    def read(self, connection, format, **kwargs):\n        # Custom implementation\n        pass\n\n    # Implement other required methods...\n\nregister_engine(\"duckdb\", DuckDBEngine)\n</code></pre>"},{"location":"features/engines/#testing-by-engine","title":"Testing by Engine","text":"Engine Test Approach CI Coverage Pandas Full pytest suite, test campaign \u2705 100% Spark Mock-based tests + Databricks validation Partial (mocks only) Polars pytest with <code>HAS_POLARS</code> skipif markers \u2705 When installed <p>Spark validation strategy: - CI runs mock-based tests (no JVM required) - Production validation happens on Databricks - Local testing requires WSL on Windows</p> <p>See Testing Guide for details.</p>"},{"location":"features/engines/#best-practices","title":"Best Practices","text":"<ol> <li>Match engine to workload - Use pandas for development, spark for production</li> <li>Use lazy execution - Polars and Spark defer computation until needed</li> <li>Enable Arrow - Faster I/O and reduced memory for Pandas</li> <li>Partition large tables - Use <code>partition_by</code> for write performance</li> <li>Run maintenance - Enable auto-optimize for Delta tables</li> <li>Test locally first - Develop with pandas, deploy with spark</li> </ol>"},{"location":"features/engines/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/engines/#switching-from-pandas-to-spark-breaks-my-pipeline","title":"Switching from Pandas to Spark breaks my pipeline","text":"<p>Symptom: Code works locally with pandas but fails on Spark.</p> <p>Common Causes: - Using pandas-specific operations (<code>.apply()</code>, <code>.iterrows()</code>) - Column name case sensitivity (Spark is case-insensitive by default) - Type coercion differences</p> <p>Fixes: - Use SQL transforms instead of Python where possible - Use <code>transform.steps</code> with SQL strings (engine-agnostic) - Test with <code>--dry-run</code> before switching engines</p>"},{"location":"features/engines/#engine-polars-not-found","title":"\"Engine 'polars' not found\"","text":"<p>Cause: Polars not installed.</p> <p>Fix: <pre><code>pip install polars\n</code></pre></p>"},{"location":"features/engines/#memory-errors-with-pandas-engine","title":"Memory errors with Pandas engine","text":"<p>Symptom: <code>MemoryError</code> or system becomes unresponsive.</p> <p>Causes: - Dataset too large for available RAM - Multiple DataFrames held in memory</p> <p>Fixes: - Switch to Spark for datasets &gt;1GB - Use chunked reading if staying on Pandas: <pre><code>read:\n  options:\n    chunksize: 100000\n</code></pre></p>"},{"location":"features/engines/#polars-lazy-vs-eager-mode-issues","title":"Polars lazy vs eager mode issues","text":"<p>Symptom: Operations don't execute or return LazyFrame instead of DataFrame.</p> <p>Fix: Odibi handles collection automatically, but if using custom transforms: <pre><code># Force collection in custom transforms\nif hasattr(df, 'collect'):\n    df = df.collect()\n</code></pre></p>"},{"location":"features/engines/#related","title":"Related","text":"<ul> <li>Connections - Data source configuration</li> <li>Pipelines - Pipeline definition</li> <li>YAML Schema Reference - Full configuration options</li> </ul>"},{"location":"features/lineage/","title":"Cross-Pipeline Lineage","text":"<p>Track table-level lineage relationships across pipelines for impact analysis and data governance.</p>"},{"location":"features/lineage/#overview","title":"Overview","text":"<p>Odibi tracks lineage at two levels: - OpenLineage integration: Standards-based lineage emission - Cross-pipeline lineage: Table-to-table relationships in the System Catalog</p> <p>This document covers the cross-pipeline lineage tracking stored in <code>meta_lineage</code>.</p>"},{"location":"features/lineage/#how-it-works","title":"How It Works","text":"<ol> <li>During pipeline execution, read/write operations are recorded</li> <li>Source \u2192 Target relationships are stored in <code>meta_lineage</code></li> <li>CLI commands query the lineage graph</li> <li>Impact analysis identifies affected downstream tables</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   bronze/   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   silver/   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   gold/     \u2502\n\u2502  customers  \u2502     \u2502 dim_customer\u2502     \u2502customer_360 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2502                   \u2502                   \u2502\n   Pipeline A          Pipeline B          Pipeline C\n</code></pre>"},{"location":"features/lineage/#cli-commands","title":"CLI Commands","text":""},{"location":"features/lineage/#trace-upstream-lineage","title":"Trace Upstream Lineage","text":"<p>Find all sources for a table:</p> <pre><code>odibi lineage upstream &lt;table&gt; --config config.yaml\n</code></pre> <p>Example: <pre><code>$ odibi lineage upstream gold/customer_360 --config pipeline.yaml\n\nUpstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre></p> <p>Options: <pre><code>odibi lineage upstream gold/customer_360 --config config.yaml \\\n    --depth 5 \\         # Traverse up to 5 levels (default: 3)\n    --format json       # Output as JSON\n</code></pre></p>"},{"location":"features/lineage/#trace-downstream-lineage","title":"Trace Downstream Lineage","text":"<p>Find all consumers of a table:</p> <pre><code>odibi lineage downstream &lt;table&gt; --config config.yaml\n</code></pre> <p>Example: <pre><code>$ odibi lineage downstream bronze/customers_raw --config pipeline.yaml\n\nDownstream Lineage: bronze/customers_raw\n============================================================\nbronze/customers_raw\n\u251c\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n\u2502   \u251c\u2500\u2500 gold/customer_360 (gold_pipeline.build_360)\n\u2502   \u2514\u2500\u2500 gold/churn_features (ml_pipeline.build_features)\n\u2514\u2500\u2500 silver/customer_events (silver_pipeline.process_events)\n</code></pre></p>"},{"location":"features/lineage/#impact-analysis","title":"Impact Analysis","text":"<p>Assess the impact of changes to a table:</p> <pre><code>odibi lineage impact &lt;table&gt; --config config.yaml\n</code></pre> <p>Example: <pre><code>$ odibi lineage impact bronze/customers_raw --config pipeline.yaml\n\n\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n    - silver/customer_events (pipeline: silver_pipeline)\n\n  Summary:\n    Total: 4 downstream table(s) in 3 pipeline(s)\n</code></pre></p>"},{"location":"features/lineage/#programmatic-access","title":"Programmatic Access","text":""},{"location":"features/lineage/#using-lineagetracker","title":"Using LineageTracker","text":"<pre><code>from odibi.lineage import LineageTracker\nfrom odibi.catalog import CatalogManager\n\n# Initialize\ncatalog = CatalogManager(spark, config, base_path, engine)\ntracker = LineageTracker(catalog)\n\n# Record lineage manually\ntracker.record_lineage(\n    read_config=node.read,\n    write_config=node.write,\n    pipeline=\"my_pipeline\",\n    node=\"process_data\",\n    run_id=\"run-12345\",\n    connections=connections,\n)\n\n# Query upstream\nupstream = tracker.get_upstream(\"gold/customer_360\", depth=3)\nfor record in upstream:\n    print(f\"{record['source_table']} \u2192 {record['target_table']}\")\n\n# Query downstream\ndownstream = tracker.get_downstream(\"bronze/customers_raw\", depth=3)\n\n# Impact analysis\nimpact = tracker.get_impact_analysis(\"bronze/customers_raw\")\nprint(f\"Affected tables: {impact['affected_tables']}\")\nprint(f\"Affected pipelines: {impact['affected_pipelines']}\")\n</code></pre>"},{"location":"features/lineage/#direct-catalog-access","title":"Direct Catalog Access","text":"<pre><code># Record lineage directly\ncatalog.record_lineage(\n    source_table=\"bronze/customers_raw\",\n    target_table=\"silver/dim_customers\",\n    target_pipeline=\"silver_pipeline\",\n    target_node=\"process_customers\",\n    run_id=\"run-12345\",\n    relationship=\"feeds\",\n)\n\n# Query upstream\nupstream = catalog.get_upstream(\"gold/customer_360\", depth=3)\n\n# Query downstream\ndownstream = catalog.get_downstream(\"bronze/customers_raw\", depth=3)\n</code></pre>"},{"location":"features/lineage/#lineage-record-structure","title":"Lineage Record Structure","text":"<p>Each lineage record includes:</p> Field Description <code>source_table</code> Source table path <code>target_table</code> Target table path <code>source_pipeline</code> Pipeline reading from source <code>source_node</code> Node reading from source <code>target_pipeline</code> Pipeline writing to target <code>target_node</code> Node writing to target <code>relationship</code> Type: \"feeds\" or \"derived_from\" <code>last_observed</code> Last time this relationship was seen <code>run_id</code> Run ID when recorded"},{"location":"features/lineage/#automatic-tracking","title":"Automatic Tracking","text":"<p>Lineage is automatically tracked when: 1. A node has both <code>read</code> and <code>write</code> configurations 2. The System Catalog is configured 3. The pipeline runs successfully</p> <pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n      format: delta\n\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE active = true\"\n\n    write:\n      connection: silver\n      path: dim_customers\n      format: delta\n</code></pre> <p>This automatically records: <code>bronze/customers_raw \u2192 silver/dim_customers</code></p>"},{"location":"features/lineage/#dependency-based-lineage","title":"Dependency-Based Lineage","text":"<p>Lineage is also tracked for <code>depends_on</code> relationships:</p> <pre><code>nodes:\n  - name: source_node\n    read: { connection: bronze, path: raw_data }\n    write: { connection: silver, path: processed_data }\n\n  - name: consumer_node\n    depends_on: [source_node]  # Lineage tracked!\n    transform:\n      steps:\n        - sql: \"SELECT * FROM source_node\"\n    write: { connection: gold, path: final_data }\n</code></pre>"},{"location":"features/lineage/#storage-location","title":"Storage Location","text":"<p>Lineage is stored in the System Catalog:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n</code></pre> <p>Location: <code>{connection_base_path}/_odibi_system/meta_lineage/</code></p>"},{"location":"features/lineage/#example-pre-deployment-impact-check","title":"Example: Pre-Deployment Impact Check","text":"<p>Before deploying schema changes, check impact:</p> <pre><code>def pre_deployment_check(catalog, table_to_change):\n    \"\"\"Check impact before deploying changes.\"\"\"\n    downstream = catalog.get_downstream(table_to_change, depth=5)\n\n    if not downstream:\n        print(f\"\u2705 No downstream dependencies for {table_to_change}\")\n        return True\n\n    affected_tables = set()\n    affected_pipelines = set()\n\n    for record in downstream:\n        affected_tables.add(record['target_table'])\n        if record.get('target_pipeline'):\n            affected_pipelines.add(record['target_pipeline'])\n\n    print(f\"\u26a0\ufe0f  Changes to {table_to_change} will affect:\")\n    print(f\"   - {len(affected_tables)} tables\")\n    print(f\"   - {len(affected_pipelines)} pipelines\")\n\n    for table in sorted(affected_tables):\n        print(f\"     \u2022 {table}\")\n\n    return len(downstream) == 0\n</code></pre>"},{"location":"features/lineage/#integration-with-schema-tracking","title":"Integration with Schema Tracking","text":"<p>Combine lineage with schema tracking for comprehensive governance:</p> <pre><code>def assess_schema_change_impact(catalog, table_path):\n    \"\"\"Assess impact of recent schema changes.\"\"\"\n    # Get schema changes\n    history = catalog.get_schema_history(table_path, limit=2)\n    if len(history) &lt; 2:\n        return\n\n    latest = history[0]\n    removed = latest.get('columns_removed', [])\n\n    if removed:\n        # Check downstream impact\n        downstream = catalog.get_downstream(table_path)\n        print(f\"\u26a0\ufe0f  Columns {removed} were removed from {table_path}\")\n        print(f\"   This may break {len(downstream)} downstream tables\")\n</code></pre>"},{"location":"features/lineage/#best-practices","title":"Best Practices","text":"<ol> <li>Run impact analysis before changes - Know what you'll affect</li> <li>Use consistent table naming - Makes lineage easier to follow</li> <li>Document cross-pipeline boundaries - Clarify ownership</li> <li>Monitor lineage depth - Deep chains may indicate complexity</li> <li>Integrate with CI/CD - Block deployments with unknown impact</li> </ol>"},{"location":"features/lineage/#related","title":"Related","text":"<ul> <li>Schema Version Tracking - Track schema changes</li> <li>OpenLineage Integration - Standards-based lineage</li> </ul>"},{"location":"features/observability/","title":"Observability Tables","text":"<p>Auto-populating observability tables for leadership dashboards with zero manual effort.</p>"},{"location":"features/observability/#overview","title":"Overview","text":"<p>Odibi automatically populates observability tables on every pipeline run, enabling Power BI dashboards for leadership without manual intervention.</p>"},{"location":"features/observability/#what-leadership-gets","title":"What Leadership Gets","text":"Dashboard Source Table Auto-Updated Platform Health <code>meta_pipeline_health</code> \u2705 Every run Cost Trends <code>meta_daily_stats</code> \u2705 Every run SLA Compliance <code>meta_sla_status</code> \u2705 Every run Failure Analysis <code>meta_failures</code> \u2705 On failure"},{"location":"features/observability/#key-features","title":"Key Features","text":"<ul> <li>Zero-touch: Tables auto-populate on pipeline completion</li> <li>Exactly-once: Guard table prevents duplicate updates</li> <li>Engine parity: Works on Spark, Pandas/delta-rs, and SQL Server</li> <li>Failure-resilient: Observability errors never fail pipelines</li> </ul>"},{"location":"features/observability/#table-taxonomy","title":"Table Taxonomy","text":"<p>Observability tables are divided into two categories:</p>"},{"location":"features/observability/#fact-tables-append-only","title":"Fact Tables (Append-Only)","text":"<p>Immutable records that capture what happened. Never modified after initial write.</p> Table Purpose Granularity <code>meta_pipeline_runs</code> Pipeline execution log One row per pipeline execution <code>meta_node_runs</code> Node execution log One row per node execution <code>meta_failures</code> Failure details One row per failure event <code>meta_observability_errors</code> Observability system failures One row per observability failure <code>meta_derived_applied_runs</code> Idempotency guard One row per (derived_table, run_id)"},{"location":"features/observability/#derived-tables-incrementally-maintained","title":"Derived Tables (Incrementally Maintained)","text":"<p>Aggregated views that are upserted on each pipeline completion.</p> Table Purpose Update Trigger <code>meta_daily_stats</code> Daily aggregates Upsert on pipeline completion <code>meta_pipeline_health</code> Current health snapshot Upsert on pipeline completion <code>meta_sla_status</code> Freshness compliance Upsert on pipeline completion"},{"location":"features/observability/#configuration","title":"Configuration","text":""},{"location":"features/observability/#pipeline-level-config","title":"Pipeline-Level Config","text":"<p>Enable SLA tracking by adding <code>owner</code> and <code>freshness_sla</code> to your pipeline:</p> <pre><code>pipelines:\n  - pipeline: orders_silver\n    description: \"Transform orders to silver layer\"\n    layer: silver\n    owner: \"data-team@company.com\"      # Pipeline owner for SLA alerts\n    freshness_sla: \"6h\"                  # Expected freshness (6 hours)\n    freshness_anchor: run_completion     # Default: when pipeline last ran\n    nodes:\n      # ...\n</code></pre> Field Type Required Description <code>owner</code> string No Pipeline owner (email or name). Shown in health/SLA dashboards <code>freshness_sla</code> string No Expected freshness: <code>30m</code>, <code>6h</code>, <code>1d</code>, <code>1w</code> <code>freshness_anchor</code> string No What defines freshness. Default: <code>run_completion</code> <p>Freshness SLA Required for SLA Tracking</p> <p>The <code>meta_sla_status</code> table is only updated if <code>freshness_sla</code> is configured.</p>"},{"location":"features/observability/#system-level-config","title":"System-Level Config","text":"<p>Configure cost tracking and retention in the <code>system</code> section:</p> <pre><code>system:\n  connection: catalog_storage\n  path: _odibi_system\n  cost_per_compute_hour: 2.50           # Estimated cost per compute hour (USD)\n  retention_days:\n    daily_stats: 365                    # Keep daily stats for 1 year\n    failures: 90                        # Keep failure records for 90 days\n    observability_errors: 90            # Keep observability errors for 90 days\n</code></pre> Field Type Default Description <code>cost_per_compute_hour</code> float None Estimated cost per compute hour (USD) for cost tracking <code>retention_days.daily_stats</code> int 365 Days to retain daily stats <code>retention_days.failures</code> int 90 Days to retain failure records <code>retention_days.observability_errors</code> int 90 Days to retain observability errors"},{"location":"features/observability/#schema-reference","title":"Schema Reference","text":""},{"location":"features/observability/#meta_pipeline_runs","title":"meta_pipeline_runs","text":"<p>Pipeline execution log. One row per pipeline execution.</p> Column Type Description <code>run_id</code> STRING Primary key (UUID) <code>pipeline_name</code> STRING Pipeline name <code>owner</code> STRING Pipeline owner (nullable) <code>layer</code> STRING Medallion layer (nullable) <code>run_start_at</code> TIMESTAMP Execution start time <code>run_end_at</code> TIMESTAMP Execution end time <code>duration_ms</code> BIGINT Duration in milliseconds <code>status</code> STRING <code>SUCCESS</code> or <code>FAILURE</code> <code>nodes_total</code> INT Total nodes in pipeline <code>nodes_succeeded</code> INT Nodes that succeeded <code>nodes_failed</code> INT Nodes that failed <code>nodes_skipped</code> INT Nodes that were skipped <code>rows_processed</code> BIGINT Sum of terminal node rows (nullable) <code>error_summary</code> STRING First 500 chars of error (nullable) <code>terminal_nodes</code> STRING Comma-separated terminal node names (nullable) <code>environment</code> STRING Environment tag (nullable) <code>created_at</code> TIMESTAMP Record creation time"},{"location":"features/observability/#meta_node_runs","title":"meta_node_runs","text":"<p>Node execution log. One row per node execution.</p> Column Type Description <code>run_id</code> STRING FK to pipeline run <code>node_id</code> STRING UUID for this node execution <code>pipeline_name</code> STRING Pipeline name <code>node_name</code> STRING Node name <code>status</code> STRING <code>SUCCESS</code>, <code>FAILURE</code>, or <code>SKIPPED</code> <code>run_start_at</code> TIMESTAMP Node execution start time <code>run_end_at</code> TIMESTAMP Node execution end time <code>duration_ms</code> BIGINT Duration in milliseconds <code>rows_processed</code> BIGINT Rows processed (nullable) <code>metrics_json</code> STRING Flat dict of metrics (scalars only) <code>environment</code> STRING Environment tag (nullable) <code>created_at</code> TIMESTAMP Record creation time"},{"location":"features/observability/#meta_failures","title":"meta_failures","text":"<p>Failure details. One row per failure event.</p> Column Type Description <code>failure_id</code> STRING Primary key (UUID) <code>run_id</code> STRING FK to pipeline run <code>pipeline_name</code> STRING Pipeline name <code>node_name</code> STRING Node name <code>error_type</code> STRING Exception class name <code>error_message</code> STRING Error message (max 1000 chars) <code>error_code</code> STRING Error code for taxonomy (nullable) <code>stack_trace</code> STRING Stack trace (max 2000 chars, nullable) <code>timestamp</code> TIMESTAMP When failure occurred <code>date</code> DATE For partitioning"},{"location":"features/observability/#meta_observability_errors","title":"meta_observability_errors","text":"<p>Observability system failures. Self-heals by logging its own errors.</p> Column Type Description <code>error_id</code> STRING Primary key (UUID) <code>run_id</code> STRING Pipeline run ID (nullable) <code>pipeline_name</code> STRING Pipeline name (nullable) <code>component</code> STRING Component that failed (e.g., <code>catalog_update</code>, <code>derived_updates</code>) <code>error_message</code> STRING Error message (max 500 chars) <code>timestamp</code> TIMESTAMP When error occurred <code>date</code> DATE For partitioning"},{"location":"features/observability/#meta_derived_applied_runs-guard-table","title":"meta_derived_applied_runs (Guard Table)","text":"<p>Idempotency guard for derived table updates. Ensures exactly-once semantics.</p> Column Type Description <code>derived_table</code> STRING PK (with run_id): Derived table name <code>run_id</code> STRING PK (with derived_table): Pipeline run ID <code>claim_token</code> STRING UUID of claiming process <code>status</code> STRING <code>CLAIMED</code>, <code>APPLIED</code>, or <code>FAILED</code> <code>claimed_at</code> TIMESTAMP When claim was acquired <code>applied_at</code> TIMESTAMP When update completed (nullable) <code>error_message</code> STRING Error if failed (max 500 chars, nullable)"},{"location":"features/observability/#meta_daily_stats","title":"meta_daily_stats","text":"<p>Daily aggregates. Primary key: <code>(date, pipeline_name)</code>.</p> Column Type Description <code>date</code> DATE Stats date <code>pipeline_name</code> STRING Pipeline name <code>runs</code> BIGINT Total runs on this day <code>successes</code> BIGINT Successful runs <code>failures</code> BIGINT Failed runs <code>total_rows</code> BIGINT Total rows processed <code>total_duration_ms</code> BIGINT Total execution time <code>estimated_cost_usd</code> DOUBLE Estimated cost (nullable) <code>actual_cost_usd</code> DOUBLE Actual cost from billing (nullable) <code>cost_source</code> STRING <code>configured_rate</code>, <code>databricks_billing</code>, <code>none</code>, or <code>mixed</code> <code>cost_is_actual</code> BOOLEAN Whether cost is from billing"},{"location":"features/observability/#meta_pipeline_health","title":"meta_pipeline_health","text":"<p>Current health snapshot. Primary key: <code>pipeline_name</code>.</p> Column Type Description <code>pipeline_name</code> STRING Pipeline name <code>owner</code> STRING Pipeline owner (nullable) <code>layer</code> STRING Medallion layer (nullable) <code>total_runs</code> BIGINT Lifetime total runs <code>total_successes</code> BIGINT Lifetime successes <code>total_failures</code> BIGINT Lifetime failures <code>success_rate_7d</code> DOUBLE 7-day success rate (nullable) <code>success_rate_30d</code> DOUBLE 30-day success rate (nullable) <code>avg_duration_ms_7d</code> DOUBLE 7-day average duration (nullable) <code>total_rows_30d</code> BIGINT 30-day total rows (nullable) <code>estimated_cost_30d</code> DOUBLE 30-day estimated cost (nullable) <code>last_success_at</code> TIMESTAMP Last successful run (nullable) <code>last_failure_at</code> TIMESTAMP Last failed run (nullable) <code>last_run_at</code> TIMESTAMP Most recent run <code>updated_at</code> TIMESTAMP Record update time"},{"location":"features/observability/#meta_sla_status","title":"meta_sla_status","text":"<p>Freshness compliance. Primary key: <code>pipeline_name</code>.</p> Column Type Description <code>pipeline_name</code> STRING Pipeline name <code>owner</code> STRING Pipeline owner (nullable) <code>freshness_sla</code> STRING SLA string (e.g., <code>6h</code>) <code>freshness_anchor</code> STRING <code>run_completion</code>, <code>table_max_timestamp</code>, or <code>watermark_state</code> <code>freshness_sla_minutes</code> INT SLA in minutes <code>last_success_at</code> TIMESTAMP Last successful run (nullable) <code>minutes_since_success</code> INT Minutes since last success (nullable) <code>sla_met</code> BOOLEAN Whether SLA is currently met <code>hours_overdue</code> DOUBLE Hours overdue if SLA breached (nullable) <code>updated_at</code> TIMESTAMP Record update time"},{"location":"features/observability/#how-auto-population-works","title":"How Auto-Population Works","text":"<p>When a pipeline completes, the following sequence occurs:</p> <pre><code>Pipeline Execution\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Write Facts (append-only)              \u2502\n\u2502    \u2022 meta_pipeline_runs                   \u2502\n\u2502    \u2022 meta_node_runs                       \u2502\n\u2502    \u2022 meta_failures (if any)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2. Update Derived Tables (with guard)     \u2502\n\u2502    For each derived table:                \u2502\n\u2502    \u2022 Try to claim via guard table         \u2502\n\u2502    \u2022 If claimed, update derived table     \u2502\n\u2502    \u2022 Mark applied or failed               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 3. On Any Error                           \u2502\n\u2502    \u2022 Log to meta_observability_errors     \u2502\n\u2502    \u2022 Continue pipeline (never fail)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/observability/#guard-semantics-exactly-once","title":"Guard Semantics (Exactly-Once)","text":"<p>The guard table (<code>meta_derived_applied_runs</code>) ensures each derived table update happens exactly once per run.</p>"},{"location":"features/observability/#status-values","title":"Status Values","text":"Status Meaning Can Reclaim? <code>CLAIMED</code> Update in progress Yes, if stale (&gt;60 min) <code>APPLIED</code> Update completed successfully No (terminal) <code>FAILED</code> Update failed Yes (always)"},{"location":"features/observability/#claim-lifecycle","title":"Claim Lifecycle","text":"<ol> <li>Try Claim: Insert <code>CLAIMED</code> row with unique token</li> <li>On Success: Update to <code>APPLIED</code> (terminal state)</li> <li>On Failure: Update to <code>FAILED</code> (reclaimable)</li> <li>Stale Claims: <code>CLAIMED</code> entries older than 60 minutes are reclaimable</li> </ol> <p>APPLIED is Terminal</p> <p>Once a row reaches <code>APPLIED</code> status, it can never be reclaimed. This prevents double-counting in derived tables.</p>"},{"location":"features/observability/#concurrent-safety","title":"Concurrent Safety","text":"<p>The guard table uses atomic operations (MERGE in Spark/SQL Server, append+verify in Pandas) to handle concurrent updates safely.</p>"},{"location":"features/observability/#cli-commands","title":"CLI Commands","text":""},{"location":"features/observability/#rebuild-summaries","title":"rebuild-summaries","text":"<p>Recompute derived tables from fact tables. Use after failures or when derived tables become inconsistent.</p> <pre><code># Rebuild specific pipeline since a date\nodibi system rebuild-summaries config.yaml --pipeline orders_silver --since 2024-01-01\n\n# Rebuild all pipelines since a date\nodibi system rebuild-summaries config.yaml --all --since 2024-01-01\n\n# Custom stale claim threshold (default: 60 minutes)\nodibi system rebuild-summaries config.yaml --all --since 2024-01-01 --max-age-minutes 30\n</code></pre> Option Required Description <code>--pipeline</code> No* Specific pipeline to rebuild <code>--all</code> No* Rebuild all pipelines <code>--since</code> Yes Start date (YYYY-MM-DD) <code>--max-age-minutes</code> No Max age for stale CLAIMED entries (default: 60) <code>--env</code> No Environment override <p>*Must specify either <code>--pipeline</code> or <code>--all</code></p> <p>When to use:</p> <ul> <li>After system outages that left updates incomplete</li> <li>After fixing bugs in derived table logic</li> <li>When derived tables show incorrect aggregates</li> <li>To backfill historical data</li> </ul>"},{"location":"features/observability/#cleanup","title":"cleanup","text":"<p>Delete old records based on retention configuration.</p> <pre><code># Preview what would be deleted\nodibi system cleanup config.yaml --dry-run\n\n# Actually delete old records\nodibi system cleanup config.yaml\n\n# With environment override\nodibi system cleanup config.yaml --env prod\n</code></pre> Option Required Description <code>--dry-run</code> No Preview without deleting <code>--env</code> No Environment override <p>Tables affected:</p> Table Retention Default <code>meta_daily_stats</code> <code>retention_days.daily_stats</code> 365 days <code>meta_failures</code> <code>retention_days.failures</code> 90 days <code>meta_observability_errors</code> <code>retention_days.observability_errors</code> 90 days"},{"location":"features/observability/#engine-parity","title":"Engine Parity","text":"<p>All operations work across Spark, Pandas/delta-rs, and SQL Server with semantic equivalence.</p> Operation Spark Pandas/delta-rs SQL Server try_claim Atomic MERGE Append + verify Atomic MERGE mark_applied UPDATE Read-modify-write UPDATE mark_failed UPDATE Read-modify-write UPDATE daily_stats MERGE + deltas Groupby + overwrite MERGE pipeline_health MERGE + window Filter + overwrite MERGE + CTE sla_status SQL CTE Python datetime DATEDIFF <p>Pandas Mode Limitations</p> <p>Pandas/delta-rs mode uses optimistic concurrency with retries. Under very high concurrency, some operations may need multiple attempts.</p>"},{"location":"features/observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/observability/#derived-updates-failing","title":"Derived Updates Failing","text":"<p>Symptoms: <code>meta_derived_applied_runs</code> has <code>FAILED</code> entries.</p> <p>Check status:</p> <pre><code>-- Find failed updates\nSELECT derived_table, run_id, error_message, claimed_at\nFROM meta_derived_applied_runs\nWHERE status = 'FAILED'\nORDER BY claimed_at DESC\nLIMIT 20\n</code></pre> <p>Resolution:</p> <pre><code># Rebuild failed updates\nodibi system rebuild-summaries config.yaml --all --since 2024-01-01\n</code></pre>"},{"location":"features/observability/#stale-claimed-entries","title":"Stale CLAIMED Entries","text":"<p>Symptoms: Updates stuck in <code>CLAIMED</code> status for &gt;60 minutes.</p> <p>Cause: Process crashed or was killed before completing.</p> <p>Resolution:</p> <pre><code># Rebuild with shorter stale threshold\nodibi system rebuild-summaries config.yaml --all --since 2024-01-01 --max-age-minutes 30\n</code></pre>"},{"location":"features/observability/#derived-tables-out-of-sync","title":"Derived Tables Out of Sync","text":"<p>Symptoms: <code>meta_daily_stats</code> doesn't match <code>meta_pipeline_runs</code> aggregates.</p> <p>Cause: Failed updates, race conditions, or bug in derived logic.</p> <p>Resolution:</p> <pre><code># Full rebuild from fact tables\nodibi system rebuild-summaries config.yaml --all --since 2024-01-01\n</code></pre>"},{"location":"features/observability/#guard-table-full-scan-performance","title":"Guard Table Full Scan Performance","text":"<p>For large guard tables, consider partitioning by date or periodic cleanup of old <code>APPLIED</code> entries.</p>"},{"location":"features/observability/#observability-errors","title":"Observability Errors","text":"<p>Check <code>meta_observability_errors</code> for internal issues:</p> <pre><code>SELECT component, error_message, COUNT(*) as count\nFROM meta_observability_errors\nWHERE timestamp &gt; current_date - 7\nGROUP BY component, error_message\nORDER BY count DESC\n</code></pre>"},{"location":"features/observability/#complete-example","title":"Complete Example","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nsystem:\n  connection: catalog_storage\n  path: _odibi_system\n  environment: prod\n  cost_per_compute_hour: 2.50\n  retention_days:\n    daily_stats: 365\n    failures: 90\n    observability_errors: 90\n\nconnections:\n  catalog_storage:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: metadata\n\n  bronze:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: bronze\n\n  silver:\n    type: adls\n    account: \"${STORAGE_ACCOUNT}\"\n    container: silver\n\npipelines:\n  - pipeline: orders_silver\n    description: \"Transform orders to silver layer\"\n    layer: silver\n    owner: \"data-team@company.com\"\n    freshness_sla: \"6h\"\n    nodes:\n      - name: read_orders\n        type: read\n        connection: bronze\n        path: raw/orders\n        format: delta\n\n      - name: transform\n        type: transform\n        input: read_orders\n        transform: |\n          SELECT * FROM {input}\n          WHERE order_date &gt;= '2024-01-01'\n\n      - name: write_orders\n        type: write\n        input: transform\n        connection: silver\n        path: orders\n        format: delta\n        mode: merge\n        merge_keys: [order_id]\n</code></pre> <p>After running this pipeline:</p> <ul> <li><code>meta_pipeline_runs</code>: New row with execution details</li> <li><code>meta_node_runs</code>: 3 rows (one per node)</li> <li><code>meta_daily_stats</code>: Upserted with today's aggregates</li> <li><code>meta_pipeline_health</code>: Upserted with lifetime stats</li> <li><code>meta_sla_status</code>: Upserted with freshness compliance</li> </ul>"},{"location":"features/observability/#executive-dashboard-views","title":"Executive Dashboard Views","text":"<p>When using Catalog Sync to replicate data to SQL Server, Odibi automatically creates pre-built views optimized for visualization tools.</p>"},{"location":"features/observability/#available-views","title":"Available Views","text":"View Purpose Key Metrics <code>vw_pipeline_health_status</code> RAG status per pipeline <code>health_status</code> (RED/AMBER/GREEN), <code>health_reason</code> <code>vw_exec_overview</code> Executive summary by project Success rates (7d/30d/90d), cost trends, reliability score <code>vw_table_freshness</code> Data staleness monitoring <code>freshness_status</code>, <code>hours_since_update</code> <code>vw_pipeline_sla_status</code> SLA compliance dashboard <code>sla_met</code>, <code>hours_overdue</code>, <code>sla_rag</code> <code>vw_exec_current_issues</code> What's broken now Failed pipelines with error details, priority order <code>vw_pipeline_risk</code> Risk scoring <code>risk_score</code>, <code>risk_level</code> (CRITICAL/HIGH/MEDIUM/LOW) <code>vw_cost_summary</code> Cost tracking 7d/30d costs, runtime hours, cost trends"},{"location":"features/observability/#health-status-logic","title":"Health Status Logic","text":"<p>The <code>vw_pipeline_health_status</code> view uses this RAG logic:</p> Status Condition RED Last run failed, success rate &lt;90%, or no runs in 7 days AMBER Success rate &lt;100% or no run in 48+ hours GREEN 100% success rate and recent runs"},{"location":"features/observability/#sla-status-logic","title":"SLA Status Logic","text":"<p>The <code>vw_pipeline_sla_status</code> view combines SLA tracking with business context:</p> RAG Condition GREEN SLA met AMBER SLA breached by \u22641 hour RED SLA breached by &gt;1 hour or high-criticality pipeline not successful"},{"location":"features/observability/#risk-scoring","title":"Risk Scoring","text":"<p>The <code>vw_pipeline_risk</code> view calculates risk as:</p> <pre><code>risk_score = criticality_weight \u00d7 (failure_rate \u00d7 100 + log10(runtime_hours) \u00d7 5)\n</code></pre> <p>Where <code>criticality_weight</code> is 3 (High), 2 (Medium), or 1 (Low).</p>"},{"location":"features/observability/#business-context-dim_pipeline_context","title":"Business Context (dim_pipeline_context)","text":"<p>The <code>dim_pipeline_context</code> table is manually populated to add business metadata to your pipelines. This enriches the executive views with ownership and criticality information.</p> Column Type Description <code>project</code> STRING Project name (PK) <code>pipeline_name</code> STRING Pipeline name (PK) <code>environment</code> STRING Environment (PK) <code>business_criticality</code> STRING <code>High</code>, <code>Medium</code>, or <code>Low</code> <code>business_owner</code> STRING Business stakeholder name/email <code>business_process</code> STRING Business process this pipeline supports <code>notes</code> STRING Optional notes <p>Example:</p> <pre><code>INSERT INTO [odibi_system].[dim_pipeline_context] \n    (project, pipeline_name, environment, business_criticality, business_owner, business_process)\nVALUES \n    ('SalesAnalytics', 'orders_silver', 'prod', 'High', 'Jane Smith', 'Daily Sales Reporting'),\n    ('SalesAnalytics', 'inventory_bronze', 'prod', 'Medium', 'Bob Jones', 'Inventory Sync');\n</code></pre> <p>Optional but Recommended</p> <p>Views work without business context (columns will be NULL), but populating this table enables priority-based alerting and risk scoring.</p>"},{"location":"features/observability/#best-practices","title":"Best Practices","text":"<ol> <li>Set owners - Configure <code>owner</code> on all pipelines for accountability</li> <li>Define SLAs - Set <code>freshness_sla</code> for business-critical pipelines</li> <li>Monitor health - Build dashboards on <code>meta_pipeline_health</code></li> <li>Periodic cleanup - Run <code>odibi system cleanup</code> weekly/monthly</li> <li>Check observability errors - Review <code>meta_observability_errors</code> regularly</li> <li>Use rebuild sparingly - Only when derived tables are actually inconsistent</li> </ol>"},{"location":"features/observability/#related","title":"Related","text":"<ul> <li>System Catalog - Core catalog tables and configuration</li> <li>Alerting - Notifications for pipeline events</li> <li>CLI Reference - Full CLI command reference</li> <li>Diagnostics - Pipeline debugging tools</li> </ul>"},{"location":"features/orchestration/","title":"Orchestration","text":"<p>Generate production-ready workflow definitions for Apache Airflow and Dagster from your Odibi pipelines.</p>"},{"location":"features/orchestration/#overview","title":"Overview","text":"<p>Odibi's orchestration module provides: - Airflow Integration: Generate DAG files with proper task dependencies - Dagster Integration: Create asset definitions with dependency graphs - Automatic Dependency Mapping: Node dependencies become task/asset dependencies - CLI Execution: Each node runs via <code>odibi run</code> for isolation</p>"},{"location":"features/orchestration/#airflow-integration","title":"Airflow Integration","text":""},{"location":"features/orchestration/#airflowexporter-class","title":"AirflowExporter Class","text":"<p>The <code>AirflowExporter</code> generates Airflow DAG Python files from Odibi pipeline configurations.</p> <pre><code>from odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\nconfig = load_config(\"odibi.yaml\")\nexporter = AirflowExporter(config)\n\n# Generate DAG code for a specific pipeline\ndag_code = exporter.generate_code(\"process_orders\")\n\n# Write to Airflow DAGs folder\nwith open(\"/airflow/dags/odibi_process_orders.py\", \"w\") as f:\n    f.write(dag_code)\n</code></pre>"},{"location":"features/orchestration/#generated-dag-structure","title":"Generated DAG Structure","text":"<p>The exporter creates a DAG with: - <code>BashOperator</code> tasks for each node - Proper upstream/downstream dependencies - Configurable retries from your Odibi config - Tags for filtering (<code>odibi</code>, layer name)</p> <pre><code># Generated DAG example\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\nwith DAG(\n    'odibi_process_orders',\n    default_args=default_args,\n    description='Process incoming orders',\n    schedule_interval=None,\n    catchup=False,\n    tags=['odibi', 'silver'],\n) as dag:\n\n    ingest_orders = BashOperator(\n        task_id='ingest_orders',\n        bash_command='odibi run --pipeline process_orders --node ingest_orders',\n    )\n\n    validate_orders = BashOperator(\n        task_id='validate_orders',\n        bash_command='odibi run --pipeline process_orders --node validate_orders',\n    )\n\n    # Dependencies\n    [ingest_orders] &gt;&gt; validate_orders\n</code></pre>"},{"location":"features/orchestration/#airflow-configuration-options","title":"Airflow Configuration Options","text":"Option Source Description <code>dag_id</code> Auto-generated <code>odibi_{pipeline_name}</code> <code>owner</code> <code>config.owner</code> DAG owner for Airflow UI <code>retries</code> <code>config.retry.max_attempts</code> Retry count (0 if disabled) <code>tags</code> <code>pipeline.layer</code> Includes <code>odibi</code> and layer name <code>description</code> <code>pipeline.description</code> Pipeline description"},{"location":"features/orchestration/#dagster-integration","title":"Dagster Integration","text":""},{"location":"features/orchestration/#dagsterfactory-class","title":"DagsterFactory Class","text":"<p>The <code>DagsterFactory</code> creates Dagster asset definitions directly from your Odibi configuration.</p> <pre><code># definitions.py\nfrom odibi.config import load_config\nfrom odibi.orchestration.dagster import DagsterFactory\n\nconfig = load_config(\"odibi.yaml\")\ndefs = DagsterFactory(config).create_definitions()\n</code></pre>"},{"location":"features/orchestration/#asset-features","title":"Asset Features","text":"<p>Each Odibi node becomes a Dagster asset with: - Dependency tracking: <code>depends_on</code> becomes asset dependencies - Grouping: Assets grouped by pipeline name - Compute kind: Tagged as <code>odibi</code> for UI identification - Op tags: Pipeline and node names for filtering</p>"},{"location":"features/orchestration/#generated-assets","title":"Generated Assets","text":"<pre><code># Dagster creates assets like:\n@asset(\n    name=\"validate_orders\",\n    deps=[\"ingest_orders\"],\n    group_name=\"process_orders\",\n    description=\"Validate order data quality\",\n    compute_kind=\"odibi\",\n    op_tags={\"odibi/pipeline\": \"process_orders\", \"odibi/node\": \"validate_orders\"},\n)\ndef validate_orders(context: AssetExecutionContext):\n    # Runs: odibi run --pipeline process_orders --node validate_orders\n    ...\n</code></pre>"},{"location":"features/orchestration/#dagster-installation","title":"Dagster Installation","text":"<p>Dagster is an optional dependency:</p> <pre><code>pip install dagster dagster-webserver\n</code></pre>"},{"location":"features/orchestration/#configuration","title":"Configuration","text":""},{"location":"features/orchestration/#project-configuration-for-orchestration","title":"Project Configuration for Orchestration","text":"<pre><code>project: DataPipeline\nowner: data-team      # Used as Airflow DAG owner\n\nretry:\n  enabled: true\n  max_attempts: 3      # Airflow retry count\n\npipelines:\n  - pipeline: process_orders\n    layer: silver      # Used as Airflow tag\n    description: \"Process incoming orders\"\n    nodes:\n      - name: ingest_orders\n        # ...\n\n      - name: validate_orders\n        depends_on:\n          - ingest_orders\n        # ...\n\n      - name: transform_orders\n        depends_on:\n          - validate_orders\n        # ...\n</code></pre>"},{"location":"features/orchestration/#dependency-mapping","title":"Dependency Mapping","text":"<p>Node dependencies in Odibi map directly to orchestrator dependencies:</p> Odibi Config Airflow Dagster <code>depends_on: [node_a]</code> <code>[node_a] &gt;&gt; node_b</code> <code>deps=[\"node_a\"]</code> <code>depends_on: [a, b]</code> <code>[a, b] &gt;&gt; node_c</code> <code>deps=[\"a\", \"b\"]</code> No dependencies First task No deps"},{"location":"features/orchestration/#examples","title":"Examples","text":""},{"location":"features/orchestration/#complete-airflow-integration","title":"Complete Airflow Integration","text":"<pre><code># scripts/generate_dags.py\nfrom pathlib import Path\nfrom odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\ndef generate_all_dags(config_path: str, output_dir: str):\n    config = load_config(config_path)\n    exporter = AirflowExporter(config)\n    output = Path(output_dir)\n\n    for pipeline in config.pipelines:\n        dag_code = exporter.generate_code(pipeline.pipeline)\n        dag_file = output / f\"odibi_{pipeline.pipeline}.py\"\n        dag_file.write_text(dag_code)\n        print(f\"Generated: {dag_file}\")\n\nif __name__ == \"__main__\":\n    generate_all_dags(\"odibi.yaml\", \"/opt/airflow/dags\")\n</code></pre>"},{"location":"features/orchestration/#complete-dagster-integration","title":"Complete Dagster Integration","text":"<pre><code># definitions.py\nfrom odibi.config import load_config\nfrom odibi.orchestration.dagster import DagsterFactory\n\n# Load Odibi configuration\nconfig = load_config(\"odibi.yaml\")\n\n# Create Dagster definitions\ndefs = DagsterFactory(config).create_definitions()\n\n# Run with: dagster dev -f definitions.py\n</code></pre>"},{"location":"features/orchestration/#multi-pipeline-setup","title":"Multi-Pipeline Setup","text":"<pre><code># odibi.yaml\nproject: DataWarehouse\nowner: platform-team\n\npipelines:\n  - pipeline: bronze_ingestion\n    layer: bronze\n    nodes:\n      - name: ingest_customers\n        source:\n          connection: raw_db\n          path: customers\n\n      - name: ingest_orders\n        source:\n          connection: raw_db\n          path: orders\n\n  - pipeline: silver_transformation\n    layer: silver\n    nodes:\n      - name: clean_customers\n        depends_on: []\n        source:\n          connection: bronze\n          path: customers\n\n      - name: clean_orders\n        depends_on: []\n        source:\n          connection: bronze\n          path: orders\n\n      - name: join_customer_orders\n        depends_on:\n          - clean_customers\n          - clean_orders\n</code></pre> <pre><code># Generate DAGs for all pipelines\nfrom odibi.config import load_config\nfrom odibi.orchestration.airflow import AirflowExporter\n\nconfig = load_config(\"odibi.yaml\")\nexporter = AirflowExporter(config)\n\n# Generates separate DAGs:\n# - odibi_bronze_ingestion\n# - odibi_silver_transformation\nfor pipeline in config.pipelines:\n    code = exporter.generate_code(pipeline.pipeline)\n    print(f\"--- {pipeline.pipeline} ---\")\n    print(code)\n</code></pre>"},{"location":"features/orchestration/#best-practices","title":"Best Practices","text":"<ol> <li>Use CLI execution - Both adapters use <code>odibi run</code> for process isolation</li> <li>Set owner - Configure <code>owner</code> in YAML for Airflow ownership</li> <li>Enable retries - Configure retry settings in Odibi config</li> <li>Layer tags - Use <code>layer</code> field for organizing DAGs in Airflow</li> <li>Generate on deploy - Regenerate DAG files during CI/CD deployment</li> </ol>"},{"location":"features/orchestration/#related","title":"Related","text":"<ul> <li>Pipeline Configuration - YAML schema reference</li> <li>CLI Reference - <code>odibi run</code> command details</li> <li>Retry configuration is defined in your YAML config under the <code>retry</code> section</li> </ul>"},{"location":"features/patterns/","title":"Loading Patterns","text":"<p>Pre-built execution patterns for common data warehouse loading scenarios including SCD2 and Merge operations.</p>"},{"location":"features/patterns/#overview","title":"Overview","text":"<p>Odibi's pattern system provides: - Declarative loading: Configure complex loading logic via YAML - Engine agnostic: Works with Spark and Pandas engines - Built-in validation: Patterns validate required parameters before execution - Extensible: Create custom patterns by extending the <code>Pattern</code> base class</p>"},{"location":"features/patterns/#available-patterns","title":"Available Patterns","text":"Pattern Description Use Case <code>scd2</code> Slowly Changing Dimension Type 2 Historical tracking of dimension changes <code>merge</code> Upsert/merge operations Incremental updates, CDC <p>Note: For simple append or overwrite operations, use <code>write.mode: append</code> or <code>write.mode: overwrite</code> directly\u2014no pattern needed.</p>"},{"location":"features/patterns/#configuration","title":"Configuration","text":"<p>Patterns are configured via the <code>transformer</code> field in node configuration:</p> <pre><code>nodes:\n  - name: load_customers\n    transformer: scd2\n    params:\n      target: \"gold/customers\"\n      keys: [\"customer_id\"]\n      track_cols: [\"address\", \"tier\"]\n      effective_time_col: \"updated_at\"\n</code></pre>"},{"location":"features/patterns/#config-options","title":"Config Options","text":"Field Type Required Description <code>transformer</code> string Yes Pattern name: <code>scd2</code>, <code>merge</code> <code>params</code> object Yes Pattern-specific parameters"},{"location":"features/patterns/#pattern-parameters","title":"Pattern Parameters","text":""},{"location":"features/patterns/#scd2-pattern","title":"SCD2 Pattern","text":"<p>Tracks history by creating new rows for updates. When a tracked column changes, the old record is closed and a new record is inserted.</p> Parameter Type Required Default Description <code>target</code> string Yes - Target table name or path containing history <code>keys</code> list Yes - Natural keys to identify unique entities <code>track_cols</code> list Yes - Columns to monitor for changes <code>effective_time_col</code> string Yes - Source column indicating when the change occurred <code>end_time_col</code> string No <code>valid_to</code> Name of the end timestamp column <code>current_flag_col</code> string No <code>is_current</code> Name of the current record flag column <code>delete_col</code> string No <code>null</code> Column indicating soft deletion (boolean)"},{"location":"features/patterns/#merge-pattern","title":"Merge Pattern","text":"<p>Upsert/merge logic with support for multiple strategies.</p> Parameter Type Required Default Description <code>target</code> string Yes - Target table name or path <code>keys</code> list Yes - Join keys for matching records <code>strategy</code> string No <code>upsert</code> <code>upsert</code>, <code>append_only</code>, <code>delete_match</code> <code>audit_cols</code> object No <code>null</code> <code>{created_col: \"...\", updated_col: \"...\"}</code> <code>update_condition</code> string No <code>null</code> SQL condition for update clause <code>insert_condition</code> string No <code>null</code> SQL condition for insert clause <code>delete_condition</code> string No <code>null</code> SQL condition for delete clause <code>optimize_write</code> bool No <code>false</code> Run OPTIMIZE after write (Spark only) <code>zorder_by</code> list No <code>null</code> Columns to Z-Order by <code>cluster_by</code> list No <code>null</code> Columns to Liquid Cluster by (Delta)"},{"location":"features/patterns/#merge-strategies","title":"Merge Strategies","text":"Strategy Description <code>upsert</code> Update existing records, insert new ones <code>append_only</code> Ignore duplicates, only insert new keys <code>delete_match</code> Delete records in target that match keys in source"},{"location":"features/patterns/#pattern-api","title":"Pattern API","text":"<p>All patterns extend the <code>Pattern</code> base class:</p> <pre><code>from abc import ABC, abstractmethod\nfrom odibi.config import NodeConfig\nfrom odibi.context import EngineContext\nfrom odibi.engine.base import Engine\n\nclass Pattern(ABC):\n    \"\"\"Base class for Execution Patterns.\"\"\"\n\n    def __init__(self, engine: Engine, config: NodeConfig):\n        self.engine = engine\n        self.config = config\n        self.params = config.params\n\n    @abstractmethod\n    def execute(self, context: EngineContext) -&gt; Any:\n        \"\"\"\n        Execute the pattern logic.\n\n        Args:\n            context: EngineContext containing current DataFrame and helpers.\n\n        Returns:\n            The transformed DataFrame.\n        \"\"\"\n        pass\n\n    def validate(self) -&gt; None:\n        \"\"\"\n        Validate pattern configuration.\n        Raises ValueError if invalid.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"features/patterns/#creating-custom-patterns","title":"Creating Custom Patterns","text":"<ol> <li>Extend the <code>Pattern</code> base class</li> <li>Implement <code>execute()</code> method</li> <li>Optionally override <code>validate()</code> for parameter validation</li> </ol> <pre><code>from odibi.patterns.base import Pattern\nfrom odibi.context import EngineContext\n\nclass MyCustomPattern(Pattern):\n\n    def validate(self) -&gt; None:\n        if not self.params.get(\"required_param\"):\n            raise ValueError(\"MyCustomPattern: 'required_param' is required.\")\n\n    def execute(self, context: EngineContext):\n        df = context.df\n        # Custom transformation logic\n        return df\n</code></pre>"},{"location":"features/patterns/#examples","title":"Examples","text":""},{"location":"features/patterns/#scd2-customer-dimension-with-history","title":"SCD2: Customer Dimension with History","text":"<p>Track customer address and tier changes over time:</p> <pre><code>project: CustomerDW\nengine: spark\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/gold\n\npipelines:\n  - pipeline: load_customer_dim\n    nodes:\n      - name: customer_scd2\n        read:\n          connection: bronze\n          path: customers\n        transformer: scd2\n        params:\n          target: \"gold/customers\"\n          keys: [\"customer_id\"]\n          track_cols: [\"address\", \"city\", \"state\", \"tier\"]\n          effective_time_col: \"updated_at\"\n          end_time_col: \"valid_to\"\n          current_flag_col: \"is_active\"\n</code></pre>"},{"location":"features/patterns/#merge-incremental-customer-updates","title":"Merge: Incremental Customer Updates","text":"<p>Upsert with audit columns:</p> <pre><code>pipelines:\n  - pipeline: sync_customers\n    nodes:\n      - name: customers_merge\n        read:\n          connection: bronze\n          path: customer_updates\n        transformer: merge\n        params:\n          target: \"silver.customers\"\n          keys: [\"customer_id\"]\n          strategy: upsert\n          audit_cols:\n            created_col: \"dw_created_at\"\n            updated_col: \"dw_updated_at\"\n</code></pre>"},{"location":"features/patterns/#merge-gdpr-delete-request","title":"Merge: GDPR Delete Request","text":"<p>Delete records matching source keys:</p> <pre><code>pipelines:\n  - pipeline: gdpr_delete\n    nodes:\n      - name: delete_customers\n        read:\n          connection: compliance\n          path: deletion_requests\n        transformer: merge\n        params:\n          target: \"silver.customers\"\n          keys: [\"customer_id\"]\n          strategy: delete_match\n</code></pre>"},{"location":"features/patterns/#merge-conditional-update","title":"Merge: Conditional Update","text":"<p>Only update if source record is newer:</p> <pre><code>pipelines:\n  - pipeline: sync_products\n    nodes:\n      - name: products_merge\n        read:\n          connection: bronze\n          path: product_updates\n        transformer: merge\n        params:\n          target: \"silver.products\"\n          keys: [\"product_id\"]\n          strategy: upsert\n          update_condition: \"source.updated_at &gt; target.updated_at\"\n          insert_condition: \"source.is_deleted = false\"\n</code></pre>"},{"location":"features/patterns/#simple-append-no-pattern-needed","title":"Simple Append (No Pattern Needed)","text":"<p>For event/fact data, just use write mode:</p> <pre><code>pipelines:\n  - pipeline: load_events\n    nodes:\n      - name: events\n        read:\n          connection: bronze\n          path: events\n        write:\n          connection: gold\n          path: events\n          mode: append\n</code></pre>"},{"location":"features/patterns/#simple-overwrite-no-pattern-needed","title":"Simple Overwrite (No Pattern Needed)","text":"<p>For full refresh of small tables:</p> <pre><code>pipelines:\n  - pipeline: refresh_products\n    nodes:\n      - name: products\n        read:\n          connection: bronze\n          path: products\n        write:\n          connection: gold\n          path: products\n          mode: overwrite\n</code></pre>"},{"location":"features/patterns/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right pattern - Use SCD2 for dimensions needing history, Merge for incremental CDC</li> <li>Use write.mode for simple cases - <code>append</code> for events, <code>overwrite</code> for full refresh</li> <li>Define business keys - Ensure <code>keys</code> uniquely identify records in your domain</li> <li>Monitor tracked columns - For SCD2, only track columns that represent meaningful business changes</li> <li>Use audit columns - Track <code>created_at</code> and <code>updated_at</code> for debugging and lineage</li> <li>Optimize large tables - Use <code>zorder_by</code> or <code>cluster_by</code> for frequently queried columns</li> </ol>"},{"location":"features/patterns/#related","title":"Related","text":"<ul> <li>Transformers - Built-in transformation functions</li> <li>Pipelines - Pipeline configuration</li> <li>YAML Schema Reference - Full schema documentation</li> </ul>"},{"location":"features/pipelines/","title":"Pipelines","text":"<p>Orchestrate complex data workflows with dependency-aware execution, parallel processing, and intelligent error handling.</p>"},{"location":"features/pipelines/#overview","title":"Overview","text":"<p>Odibi's pipeline system provides: - DAG-based execution: Automatic dependency resolution with cycle detection - Parallel processing: Execute independent nodes concurrently - Error strategies: Fine-grained control over failure behavior - Resume capability: Skip successfully completed nodes on retry - Drift detection: Compare local config against deployed definitions</p>"},{"location":"features/pipelines/#pipeline-vs-pipelinemanager","title":"Pipeline vs PipelineManager","text":"Component Purpose <code>Pipeline</code> Executes a single pipeline (nodes, graph, engine) <code>PipelineManager</code> Manages multiple pipelines from a YAML config file"},{"location":"features/pipelines/#core-concepts","title":"Core Concepts","text":""},{"location":"features/pipelines/#pipeline","title":"Pipeline","text":"<p>The <code>Pipeline</code> class is the executor and orchestrator for a single pipeline. It: - Builds a dependency graph from node configurations - Resolves execution order via topological sort - Manages the execution engine (Pandas or Spark) - Generates execution stories for observability</p>"},{"location":"features/pipelines/#node","title":"Node","text":"<p>A <code>Node</code> is the atomic unit of work. Each node follows a four-phase execution pattern:</p> <pre><code>Read \u2192 Transform \u2192 Validate \u2192 Write\n</code></pre> Phase Description Read Load data from a connection (file, table, API) Transform Apply transformations (SQL, functions, patterns) Validate Run data quality tests, quarantine bad rows Write Persist output to a connection"},{"location":"features/pipelines/#dependencygraph","title":"DependencyGraph","text":"<p>The <code>DependencyGraph</code> class builds and validates the DAG:</p> Feature Description Missing dependency check Fails if <code>depends_on</code> references undefined nodes Cycle detection Detects circular dependencies before execution Topological sort Returns nodes in valid execution order Execution layers Groups independent nodes for parallel execution"},{"location":"features/pipelines/#pipelineresults","title":"PipelineResults","text":"<p>Execution results are captured in <code>PipelineResults</code>:</p> Field Type Description <code>pipeline_name</code> string Name of the executed pipeline <code>completed</code> list Successfully completed node names <code>failed</code> list Failed node names <code>skipped</code> list Skipped node names (dependency failures) <code>node_results</code> dict Detailed <code>NodeResult</code> per node <code>duration</code> float Total execution time in seconds <code>story_path</code> string Path to generated execution story"},{"location":"features/pipelines/#configuration","title":"Configuration","text":""},{"location":"features/pipelines/#yaml-structure","title":"YAML Structure","text":"<pre><code>project: MyDataPipeline\nengine: spark  # or 'pandas'\n\nconnections:\n  bronze:\n    type: local\n    path: ./data/bronze\n  silver:\n    type: local\n    path: ./data/silver\n\npipelines:\n  - pipeline: bronze_to_silver\n    nodes:\n      - name: load_orders\n        read:\n          connection: bronze\n          path: orders.parquet\n          format: parquet\n\n      - name: clean_orders\n        depends_on: [load_orders]\n        transform:\n          steps:\n            - function: drop_nulls\n              params:\n                columns: [order_id, customer_id]\n\n      - name: write_orders\n        depends_on: [clean_orders]\n        write:\n          connection: silver\n          path: orders_clean.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"features/pipelines/#pipeline-config-options","title":"Pipeline Config Options","text":"Field Type Required Description <code>pipeline</code> string Yes Unique pipeline name <code>nodes</code> list Yes List of node configurations"},{"location":"features/pipelines/#node-config-options","title":"Node Config Options","text":"Field Type Required Description <code>name</code> string Yes Unique node name within pipeline <code>depends_on</code> list No List of upstream node names <code>read</code> object No Read configuration <code>transform</code> object No Transform steps configuration <code>validation</code> object No Data quality tests <code>write</code> object No Write configuration <code>on_error</code> string No Error strategy: <code>fail_fast</code>, <code>fail_later</code>, <code>ignore</code> <code>cache</code> bool No Cache output in memory"},{"location":"features/pipelines/#execution-modes","title":"Execution Modes","text":""},{"location":"features/pipelines/#serial-vs-parallel","title":"Serial vs Parallel","text":"<pre><code># Serial execution (default)\nresults = manager.run()\n\n# Parallel execution with 4 workers\nresults = manager.run(parallel=True, max_workers=4)\n</code></pre> <p>In parallel mode, nodes are grouped into execution layers. Nodes within the same layer have no dependencies on each other and execute concurrently.</p>"},{"location":"features/pipelines/#dry-run-mode","title":"Dry Run Mode","text":"<p>Simulate execution without performing actual read/write operations:</p> <pre><code>results = manager.run(dry_run=True)\n</code></pre> <p>Dry run validates: - Configuration syntax - Dependency graph structure - Connection availability</p>"},{"location":"features/pipelines/#resume-from-failure","title":"Resume from Failure","text":"<p>Skip nodes that completed successfully in the previous run:</p> <pre><code>results = manager.run(resume_from_failure=True)\n</code></pre> <p>Resume capability: - Tracks node version hashes to detect config changes - Restores output from persisted writes - Invalidates downstream nodes when upstream re-executes</p>"},{"location":"features/pipelines/#error-strategies","title":"Error Strategies","text":"<p>Control how the pipeline handles node failures:</p> Strategy Behavior <code>fail_fast</code> Stop immediately on first failure <code>fail_later</code> Complete current layer, then stop <code>ignore</code> Log error and continue execution <pre><code>nodes:\n  - name: optional_enrichment\n    on_error: ignore  # Continue even if this fails\n    # ...\n</code></pre> <p>Override at runtime:</p> <pre><code>results = manager.run(on_error=\"fail_fast\")\n</code></pre>"},{"location":"features/pipelines/#features","title":"Features","text":""},{"location":"features/pipelines/#dependency-resolution","title":"Dependency Resolution","text":"<p>The pipeline automatically determines execution order:</p> <pre><code># Get execution order\norder = pipeline.graph.topological_sort()\n# ['load_orders', 'clean_orders', 'write_orders']\n\n# Visualize the graph\nprint(pipeline.visualize())\n</code></pre> <p>Output: <pre><code>Dependency Graph:\n\nLayer 1:\n  - load_orders\n\nLayer 2:\n  - clean_orders (depends on: load_orders)\n\nLayer 3:\n  - write_orders (depends on: clean_orders)\n</code></pre></p>"},{"location":"features/pipelines/#execution-layers","title":"Execution Layers","text":"<p>For parallel execution, nodes are grouped into layers:</p> <pre><code>layers = pipeline.get_execution_layers()\n# [['load_orders'], ['clean_orders'], ['write_orders']]\n</code></pre> <p>Nodes in the same layer can run simultaneously.</p>"},{"location":"features/pipelines/#drift-detection","title":"Drift Detection","text":"<p>When a System Catalog is configured, the pipeline detects drift between local and deployed configurations:</p> <pre><code>\u26a0\ufe0f DRIFT DETECTED: Local pipeline definition differs from Catalog.\n   Local Hash: a1b2c3d4\n   Catalog Hash: e5f6g7h8\n   Advice: Deploy changes using 'odibi deploy' before running in production.\n</code></pre> <p>Deploy to sync:</p> <pre><code>manager.deploy()  # Deploy all pipelines\nmanager.deploy(\"bronze_to_silver\")  # Deploy specific pipeline\n</code></pre>"},{"location":"features/pipelines/#lineage-integration","title":"Lineage Integration","text":"<p>Pipelines automatically emit OpenLineage events when configured:</p> <pre><code>lineage:\n  enabled: true\n  backend: file\n  path: ./lineage\n</code></pre> <p>Events include: - Pipeline start/complete - Node start/complete - Input/output datasets - Schema information</p>"},{"location":"features/pipelines/#api-examples","title":"API Examples","text":""},{"location":"features/pipelines/#create-from-yaml","title":"Create from YAML","text":"<pre><code>from odibi.pipeline import Pipeline, PipelineManager\n\n# Recommended: Use Pipeline.from_yaml() for convenience\nmanager = Pipeline.from_yaml(\"config.yaml\")\n\n# Or directly use PipelineManager\nmanager = PipelineManager.from_yaml(\"config.yaml\")\n\n# With environment overrides\nmanager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n</code></pre>"},{"location":"features/pipelines/#run-pipelines","title":"Run Pipelines","text":"<pre><code># Run all pipelines\nresults = manager.run()\n\n# Run specific pipeline\nresults = manager.run(\"bronze_to_silver\")\n\n# Run multiple pipelines\nresults = manager.run([\"bronze_to_silver\", \"silver_to_gold\"])\n\n# Run with options\nresults = manager.run(\n    parallel=True,\n    max_workers=8,\n    dry_run=False,\n    resume_from_failure=True,\n    on_error=\"fail_fast\"\n)\n</code></pre>"},{"location":"features/pipelines/#check-results","title":"Check Results","text":"<pre><code># Single pipeline returns PipelineResults\nif not results.failed:\n    print(f\"Success! Processed {len(results.completed)} nodes in {results.duration:.2f}s\")\nelse:\n    print(f\"Failed nodes: {results.failed}\")\n\n# Access individual node results\nfor node_name, node_result in results.node_results.items():\n    print(f\"{node_name}: {node_result.rows_processed} rows in {node_result.duration:.2f}s\")\n\n# Get story path\nif results.story_path:\n    print(f\"Execution story: {results.story_path}\")\n</code></pre>"},{"location":"features/pipelines/#pipeline-validation","title":"Pipeline Validation","text":"<pre><code># Validate without executing\nvalidation = pipeline.validate()\n\nif validation[\"valid\"]:\n    print(f\"Pipeline valid with {validation['node_count']} nodes\")\n    print(f\"Execution order: {validation['execution_order']}\")\nelse:\n    print(f\"Errors: {validation['errors']}\")\n\nif validation[\"warnings\"]:\n    print(f\"Warnings: {validation['warnings']}\")\n</code></pre>"},{"location":"features/pipelines/#list-and-access-pipelines","title":"List and Access Pipelines","text":"<pre><code># List available pipelines\nprint(manager.list_pipelines())\n# ['bronze_to_silver', 'silver_to_gold']\n\n# Get specific pipeline instance\npipeline = manager.get_pipeline(\"bronze_to_silver\")\n\n# Execute single node (for debugging)\nresult = pipeline.execute_node(\"clean_orders\")\n</code></pre>"},{"location":"features/pipelines/#complete-example","title":"Complete Example","text":"<pre><code>project: SalesAnalytics\nengine: spark\n\nconnections:\n  raw:\n    type: azure_adls\n    account: ${AZURE_STORAGE_ACCOUNT}\n    container: raw\n  silver:\n    type: delta\n    path: abfss://silver@${AZURE_STORAGE_ACCOUNT}.dfs.core.windows.net/\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    on_events: [on_failure]\n\npipelines:\n  - pipeline: sales_daily\n    nodes:\n      - name: ingest_transactions\n        read:\n          connection: raw\n          path: transactions/\n          format: parquet\n          incremental:\n            mode: rolling_window\n            column: transaction_date\n            lookback: 7\n            unit: day\n\n      - name: validate_transactions\n        depends_on: [ingest_transactions]\n        validation:\n          tests:\n            - type: not_null\n              columns: [transaction_id, amount]\n            - type: positive\n              columns: [amount]\n          on_fail: quarantine\n          quarantine:\n            connection: silver\n            path: quarantine/transactions\n\n      - name: aggregate_daily\n        depends_on: [validate_transactions]\n        transform:\n          steps:\n            - function: group_by_sum\n              params:\n                group_cols: [store_id, transaction_date]\n                sum_cols: [amount]\n        on_error: fail_fast\n\n      - name: write_daily_sales\n        depends_on: [aggregate_daily]\n        write:\n          connection: silver\n          path: sales/daily\n          format: delta\n          mode: merge\n          merge_keys: [store_id, transaction_date]\n</code></pre> <pre><code>from odibi.pipeline import Pipeline\n\nmanager = Pipeline.from_yaml(\"sales_config.yaml\")\nresults = manager.run(parallel=True, max_workers=4)\n\nif results.failed:\n    print(f\"Pipeline failed: {results.failed}\")\nelse:\n    print(f\"Daily sales updated: {results.story_path}\")\n</code></pre>"},{"location":"features/pipelines/#related","title":"Related","text":"<ul> <li>Alerting - Configure notifications for pipeline events</li> <li>Quality Gates - Block pipelines on data quality failures</li> <li>Quarantine Tables - Handle invalid rows</li> <li>Lineage - Track data flow across pipelines</li> </ul>"},{"location":"features/plugins/","title":"Plugins &amp; Extensibility","text":"<p>Extend Odibi with custom connections, transforms, and engines through a flexible plugin system.</p>"},{"location":"features/plugins/#overview","title":"Overview","text":"<p>Odibi's plugin system provides: - Connection plugins: Add custom data source connectors - Transform plugins: Register custom data transformation functions - Engine plugins: Support for different processing engines - Auto-discovery: Automatic loading of <code>transforms.py</code> and <code>plugins.py</code> - Entry points: Standard Python packaging for distributable plugins</p>"},{"location":"features/plugins/#plugin-types","title":"Plugin Types","text":""},{"location":"features/plugins/#connection-plugins","title":"Connection Plugins","text":"<p>Add support for new data sources by registering connection factories:</p> <pre><code>from odibi.plugins import register_connection_factory\n\ndef create_my_connection(name: str, config: dict):\n    \"\"\"Factory function for custom connection.\"\"\"\n    return MyCustomConnection(\n        host=config.get(\"host\"),\n        port=config.get(\"port\", 5432),\n    )\n\nregister_connection_factory(\"my_db\", create_my_connection)\n</code></pre> <p>Once registered, use in YAML config:</p> <pre><code>connections:\n  my_source:\n    type: my_db\n    host: localhost\n    port: 5432\n</code></pre>"},{"location":"features/plugins/#transform-plugins","title":"Transform Plugins","text":"<p>Register custom data transformation functions using the <code>@transform</code> decorator:</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef clean_phone_numbers(context, current, country_code=\"US\"):\n    \"\"\"Standardize phone number format.\"\"\"\n    df = current\n    # Transform logic here\n    return df\n\n@transform(\"custom_name\")\ndef my_transform(context, current):\n    \"\"\"Transform with custom registration name.\"\"\"\n    return current\n</code></pre> <p>Use in pipeline YAML:</p> <pre><code>pipelines:\n  - pipeline: process_customers\n    nodes:\n      - name: standardize\n        transform: clean_phone_numbers\n        params:\n          country_code: \"UK\"\n</code></pre>"},{"location":"features/plugins/#engine-plugins","title":"Engine Plugins","text":"<p>Odibi supports multiple processing engines. The engine is specified at the project level:</p> <pre><code>project: MyProject\nengine: spark  # or 'pandas', 'polars'\n</code></pre>"},{"location":"features/plugins/#functionregistry","title":"FunctionRegistry","text":"<p>The <code>FunctionRegistry</code> is the central registry for transform functions.</p>"},{"location":"features/plugins/#registration-methods","title":"Registration Methods","text":"<pre><code>from odibi.registry import FunctionRegistry, transform\n\n# Method 1: Using decorator\n@transform\ndef my_transform(context, current, param1: str):\n    return current\n\n# Method 2: Direct registration\ndef another_transform(context, current):\n    return current\n\nFunctionRegistry.register(another_transform, name=\"alt_transform\")\n</code></pre>"},{"location":"features/plugins/#registry-api","title":"Registry API","text":"Method Description <code>register(func, name, param_model)</code> Register a function with optional name and Pydantic model <code>get(name)</code> Retrieve a registered function <code>validate_params(name, params)</code> Validate parameters against signature <code>list_functions()</code> List all registered function names <code>get_function_info(name)</code> Get function metadata and signature <code>get_param_model(name)</code> Get Pydantic model for parameter validation"},{"location":"features/plugins/#parameter-validation","title":"Parameter Validation","text":"<p>Use Pydantic models for strict parameter validation:</p> <pre><code>from pydantic import BaseModel\nfrom odibi.registry import transform, FunctionRegistry\n\nclass FilterParams(BaseModel):\n    column: str\n    min_value: float\n    max_value: float = 100.0\n\n@transform(param_model=FilterParams)\ndef filter_range(context, current, column: str, min_value: float, max_value: float = 100.0):\n    return current.filter((current[column] &gt;= min_value) &amp; (current[column] &lt;= max_value))\n\n# Validation happens automatically\nFunctionRegistry.validate_params(\"filter_range\", {\"column\": \"price\", \"min_value\": 10})\n</code></pre>"},{"location":"features/plugins/#connection-factory","title":"Connection Factory","text":"<p>The connection factory system allows registering custom connection types.</p>"},{"location":"features/plugins/#built-in-connections","title":"Built-in Connections","text":"Type Description <code>local</code> Local filesystem <code>http</code> HTTP/REST endpoints <code>azure_blob</code> / <code>azure_adls</code> Azure Blob Storage / ADLS Gen2 <code>delta</code> Delta Lake tables <code>sql_server</code> / <code>azure_sql</code> SQL Server / Azure SQL"},{"location":"features/plugins/#custom-factory-pattern","title":"Custom Factory Pattern","text":"<pre><code>from odibi.plugins import register_connection_factory, get_connection_factory\n\ndef create_postgres_connection(name: str, config: dict):\n    \"\"\"Create a PostgreSQL connection.\"\"\"\n    from my_connections import PostgresConnection\n\n    return PostgresConnection(\n        host=config[\"host\"],\n        port=config.get(\"port\", 5432),\n        database=config[\"database\"],\n        username=config.get(\"username\"),\n        password=config.get(\"password\"),\n    )\n\n# Register the factory\nregister_connection_factory(\"postgres\", create_postgres_connection)\n\n# Retrieve a factory (if needed)\nfactory = get_connection_factory(\"postgres\")\n</code></pre>"},{"location":"features/plugins/#auto-discovery","title":"Auto-Discovery","text":"<p>Odibi automatically loads extension files from your project directory.</p>"},{"location":"features/plugins/#supported-files","title":"Supported Files","text":"File Purpose <code>transforms.py</code> Custom transform functions <code>plugins.py</code> Connection factories and other plugins"},{"location":"features/plugins/#search-locations","title":"Search Locations","text":"<ol> <li>Config directory: Same directory as your YAML config</li> <li>Current working directory: Where you run the CLI</li> </ol>"},{"location":"features/plugins/#example-structure","title":"Example Structure","text":"<pre><code>my_project/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 transforms.py      # Auto-loaded\n\u251c\u2500\u2500 plugins.py         # Auto-loaded\n\u2514\u2500\u2500 data/\n</code></pre>"},{"location":"features/plugins/#transformspy-example","title":"transforms.py Example","text":"<pre><code>\"\"\"Custom transforms for my project.\"\"\"\n\nfrom odibi.registry import transform\n\n@transform\ndef calculate_metrics(context, current, metrics: list):\n    \"\"\"Calculate custom business metrics.\"\"\"\n    df = current\n    for metric in metrics:\n        df = df.withColumn(f\"{metric}_calculated\", ...)\n    return df\n\n@transform\ndef apply_business_rules(context, current, rule_set: str):\n    \"\"\"Apply business rules based on rule set name.\"\"\"\n    # Implementation\n    return current\n</code></pre>"},{"location":"features/plugins/#pluginspy-example","title":"plugins.py Example","text":"<pre><code>\"\"\"Custom plugins for my project.\"\"\"\n\nfrom odibi.plugins import register_connection_factory\n\ndef create_snowflake_connection(name, config):\n    from snowflake.connector import connect\n    # Create connection\n    return SnowflakeWrapper(connect(**config))\n\nregister_connection_factory(\"snowflake\", create_snowflake_connection)\n</code></pre>"},{"location":"features/plugins/#plugin-configuration","title":"Plugin Configuration","text":""},{"location":"features/plugins/#entry-points-setup","title":"Entry Points Setup","text":"<p>For distributable plugins, use Python entry points in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"odibi.connections\"]\npostgres = \"my_plugin.connections:create_postgres_connection\"\nsnowflake = \"my_plugin.connections:create_snowflake_connection\"\n</code></pre> <p>Or in <code>setup.py</code>:</p> <pre><code>setup(\n    name=\"odibi-postgres-plugin\",\n    entry_points={\n        \"odibi.connections\": [\n            \"postgres = my_plugin.connections:create_postgres_connection\",\n        ],\n    },\n)\n</code></pre>"},{"location":"features/plugins/#entry-point-groups","title":"Entry Point Groups","text":"Group Purpose <code>odibi.connections</code> Connection factory functions"},{"location":"features/plugins/#loading-plugins","title":"Loading Plugins","text":"<p>Plugins are loaded automatically at startup:</p> <pre><code>from odibi.plugins import load_plugins\n\n# Called automatically, but can be invoked manually\nload_plugins()\n</code></pre>"},{"location":"features/plugins/#complete-example","title":"Complete Example","text":""},{"location":"features/plugins/#project-structure","title":"Project Structure","text":"<pre><code>my_etl_project/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 transforms.py\n\u251c\u2500\u2500 plugins.py\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_etl/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 connections/\n            \u2514\u2500\u2500 custom.py\n</code></pre>"},{"location":"features/plugins/#transformspy","title":"transforms.py","text":"<pre><code>\"\"\"Project-specific transforms.\"\"\"\n\nfrom odibi.registry import transform\nfrom pydantic import BaseModel\n\nclass EnrichmentParams(BaseModel):\n    lookup_table: str\n    key_column: str\n    value_columns: list[str]\n\n@transform(param_model=EnrichmentParams)\ndef enrich_with_lookup(\n    context,\n    current,\n    lookup_table: str,\n    key_column: str,\n    value_columns: list[str],\n):\n    \"\"\"Enrich data with lookup table values.\"\"\"\n    lookup_df = context.read(lookup_table)\n    return current.join(\n        lookup_df.select(key_column, *value_columns),\n        on=key_column,\n        how=\"left\",\n    )\n\n@transform\ndef deduplicate(context, current, key_columns: list, order_by: str = None):\n    \"\"\"Remove duplicate rows based on key columns.\"\"\"\n    if order_by:\n        from pyspark.sql import Window\n        from pyspark.sql.functions import row_number\n\n        window = Window.partitionBy(*key_columns).orderBy(order_by)\n        return current.withColumn(\"_rn\", row_number().over(window)) \\\n                      .filter(\"_rn = 1\") \\\n                      .drop(\"_rn\")\n    return current.dropDuplicates(key_columns)\n</code></pre>"},{"location":"features/plugins/#pluginspy","title":"plugins.py","text":"<pre><code>\"\"\"Connection plugins.\"\"\"\n\nfrom odibi.plugins import register_connection_factory\n\ndef create_redis_connection(name: str, config: dict):\n    \"\"\"Redis cache connection.\"\"\"\n    import redis\n\n    return redis.Redis(\n        host=config.get(\"host\", \"localhost\"),\n        port=config.get(\"port\", 6379),\n        db=config.get(\"db\", 0),\n        password=config.get(\"password\"),\n    )\n\nregister_connection_factory(\"redis\", create_redis_connection)\n</code></pre>"},{"location":"features/plugins/#configyaml","title":"config.yaml","text":"<pre><code>project: CustomerETL\nengine: spark\n\nconnections:\n  source:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: raw\n\n  cache:\n    type: redis\n    host: localhost\n    port: 6379\n\npipelines:\n  - pipeline: process_customers\n    nodes:\n      - name: load_customers\n        source: source\n        path: customers/\n\n      - name: enrich\n        input: load_customers\n        transform: enrich_with_lookup\n        params:\n          lookup_table: reference/regions\n          key_column: region_code\n          value_columns:\n            - region_name\n            - country\n\n      - name: dedupe\n        input: enrich\n        transform: deduplicate\n        params:\n          key_columns:\n            - customer_id\n          order_by: updated_at desc\n</code></pre>"},{"location":"features/plugins/#best-practices","title":"Best Practices","text":"<ol> <li>Use Pydantic models - Validate transform parameters with type safety</li> <li>Keep plugins focused - One connection type per factory function</li> <li>Handle imports lazily - Import heavy dependencies inside factory functions</li> <li>Log appropriately - Use <code>logger.info()</code> for successful loads</li> <li>Provide good defaults - Make configuration optional where sensible</li> <li>Document parameters - Use docstrings for transform functions</li> </ol>"},{"location":"features/plugins/#related","title":"Related","text":"<ul> <li>Connections - Built-in connection types</li> <li>Transformers - Built-in transform functions</li> <li>Engines - Supported processing engines</li> <li>Configuration - YAML configuration reference</li> </ul>"},{"location":"features/quality_gates/","title":"Quality Gates","text":"<p>Batch-level quality validation that evaluates the entire dataset before writing.</p>"},{"location":"features/quality_gates/#overview","title":"Overview","text":"<p>While validation tests run per-row, quality gates evaluate aggregate metrics: - Overall pass rate - What percentage of rows passed all tests? - Per-test thresholds - Different requirements for different tests - Row count anomalies - Detect unexpected batch sizes</p>"},{"location":"features/quality_gates/#configuration","title":"Configuration","text":""},{"location":"features/quality_gates/#basic-gate-setup","title":"Basic Gate Setup","text":"<pre><code>nodes:\n  - name: load_silver_customers\n    read:\n      connection: bronze\n      path: customers\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n        - type: unique\n          columns: [customer_id]\n\n      gate:\n        require_pass_rate: 0.95  # 95% must pass\n        on_fail: abort           # Stop if gate fails\n</code></pre>"},{"location":"features/quality_gates/#gate-config-options","title":"Gate Config Options","text":"Field Type Required Default Description <code>require_pass_rate</code> float No 0.95 Minimum % of rows passing ALL tests <code>on_fail</code> string No \"abort\" Action on failure <code>thresholds</code> list No [] Per-test thresholds <code>row_count</code> object No null Row count validation"},{"location":"features/quality_gates/#on-fail-actions","title":"On-Fail Actions","text":"Action Description <code>abort</code> Stop pipeline, write nothing (default) <code>warn_and_write</code> Log warning, write all rows anyway <code>write_valid_only</code> Write only rows that passed validation"},{"location":"features/quality_gates/#per-test-thresholds","title":"Per-Test Thresholds","text":"<p>Set different requirements for specific tests:</p> <pre><code>gate:\n  require_pass_rate: 0.95  # Global: 95% must pass all tests\n\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99  # 99% for not_null (stricter)\n    - test: unique\n      min_pass_rate: 1.0   # 100% unique (no duplicates allowed)\n    - test: email_format   # Named test\n      min_pass_rate: 0.90  # 90% for email format (more lenient)\n</code></pre>"},{"location":"features/quality_gates/#row-count-validation","title":"Row Count Validation","text":"<p>Detect anomalies in batch size:</p> <pre><code>gate:\n  row_count:\n    min: 100              # Fail if fewer than 100 rows\n    max: 1000000          # Fail if more than 1M rows\n    change_threshold: 0.5 # Fail if count changes &gt;50% vs last run\n</code></pre>"},{"location":"features/quality_gates/#row-count-options","title":"Row Count Options","text":"Field Type Description <code>min</code> int Minimum expected row count <code>max</code> int Maximum expected row count <code>change_threshold</code> float Max allowed change vs previous run (0.5 = 50%)"},{"location":"features/quality_gates/#complete-example","title":"Complete Example","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: bronze\n      path: orders_raw\n\n    validation:\n      tests:\n        # Critical fields\n        - type: not_null\n          name: required_fields\n          columns: [order_id, customer_id, order_date]\n\n        # Uniqueness\n        - type: unique\n          name: unique_orders\n          columns: [order_id]\n\n        # Business rules\n        - type: range\n          name: valid_amount\n          column: amount\n          min: 0\n\n        - type: accepted_values\n          name: valid_status\n          column: status\n          values: [pending, completed, cancelled]\n\n      gate:\n        # Global threshold\n        require_pass_rate: 0.95\n\n        # Per-test overrides\n        thresholds:\n          - test: required_fields\n            min_pass_rate: 0.99\n          - test: unique_orders\n            min_pass_rate: 1.0\n\n        # Row count checks\n        row_count:\n          min: 1000\n          change_threshold: 0.3\n\n        # What to do on failure\n        on_fail: abort\n\n    write:\n      connection: silver\n      path: orders\n      format: delta\n</code></pre>"},{"location":"features/quality_gates/#combining-gates-with-quarantine","title":"Combining Gates with Quarantine","text":"<p>Use both for comprehensive data quality:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine  # Route failures to quarantine\n\n    - type: unique\n      columns: [customer_id]\n      on_fail: fail        # Critical - must pass\n\n  quarantine:\n    connection: silver\n    path: quarantine/customers\n\n  gate:\n    require_pass_rate: 0.95  # Still need 95% overall\n    on_fail: abort\n</code></pre> <p>Flow: 1. Rows failing <code>not_null</code> are quarantined 2. Gate evaluates remaining rows 3. If &lt;95% pass, pipeline aborts 4. Otherwise, valid rows are written</p>"},{"location":"features/quality_gates/#gate-failure-alerts","title":"Gate Failure Alerts","text":"<p>Get notified when gates fail:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-alerts\"\n</code></pre> <p>Alert payload includes: - Pass rate achieved vs required - Number of failed rows - Failure reasons</p>"},{"location":"features/quality_gates/#gatefailederror","title":"GateFailedError","text":"<p>When a gate fails with <code>on_fail: abort</code>, a <code>GateFailedError</code> is raised:</p> <pre><code>from odibi.exceptions import GateFailedError\n\ntry:\n    pipeline.run()\nexcept GateFailedError as e:\n    print(f\"Gate failed: {e.pass_rate:.1%} &lt; {e.required_rate:.1%}\")\n    print(f\"Reasons: {e.failure_reasons}\")\n</code></pre>"},{"location":"features/quality_gates/#best-practices","title":"Best Practices","text":"<ol> <li>Start with high thresholds - Be strict initially, relax as needed</li> <li>Use per-test thresholds - Critical tests (uniqueness) should be 100%</li> <li>Monitor row count changes - Sudden changes often indicate problems</li> <li>Combine with quarantine - Don't lose failed data, route it for analysis</li> <li>Set up alerts - Know immediately when gates fail</li> </ol>"},{"location":"features/quality_gates/#related","title":"Related","text":"<ul> <li>Quarantine Tables - Route failed rows</li> <li>Alerting - Alert on gate failures</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/quarantine/","title":"Quarantine Tables","text":"<p>Route failed validation rows to a dedicated quarantine table with rejection metadata for later analysis and reprocessing.</p>"},{"location":"features/quarantine/#overview","title":"Overview","text":"<p>When validation tests fail, Odibi provides three options via <code>on_fail</code>: - <code>fail</code> - Stop the entire pipeline (default) - <code>warn</code> - Log and continue with all rows - <code>quarantine</code> - Route failed rows to a quarantine table, continue with valid rows</p> <p>The quarantine option preserves bad data for debugging without blocking production pipelines.</p>"},{"location":"features/quarantine/#configuration","title":"Configuration","text":""},{"location":"features/quarantine/#basic-quarantine-setup","title":"Basic Quarantine Setup","text":"<pre><code>nodes:\n  - name: process_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id, email]\n          on_fail: quarantine  # Route failures to quarantine\n        - type: regex_match\n          column: email\n          pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n          on_fail: quarantine\n\n      quarantine:\n        connection: silver\n        path: quarantine/customers\n        add_columns:\n          _rejection_reason: true\n          _rejected_at: true\n          _source_batch_id: true\n          _failed_tests: true\n</code></pre>"},{"location":"features/quarantine/#quarantine-config-options","title":"Quarantine Config Options","text":"Field Type Required Description <code>connection</code> string Yes Connection for quarantine writes <code>path</code> string No* Path for quarantine data <code>table</code> string No* Table name for quarantine <code>add_columns</code> object No Metadata columns to add <code>retention_days</code> int No Days to retain (default: 90) <p>*Either <code>path</code> or <code>table</code> is required.</p>"},{"location":"features/quarantine/#metadata-columns","title":"Metadata Columns","text":"<p>Control which metadata columns are added to quarantined rows:</p> <pre><code>quarantine:\n  connection: silver\n  path: quarantine/customers\n  add_columns:\n    _rejection_reason: true    # Description of why row failed\n    _rejected_at: true         # UTC timestamp of rejection\n    _source_batch_id: true     # Run ID for traceability\n    _failed_tests: true        # Comma-separated list of failed tests\n    _original_node: false      # Node name (disabled by default)\n</code></pre>"},{"location":"features/quarantine/#how-it-works","title":"How It Works","text":"<ol> <li>Test Evaluation: Each test with <code>on_fail: quarantine</code> is evaluated per-row</li> <li>Row Splitting: DataFrame is split into valid and invalid portions</li> <li>Metadata Addition: Failed rows receive metadata columns</li> <li>Quarantine Write: Invalid rows are appended to the quarantine table</li> <li>Pipeline Continues: Valid rows proceed through the pipeline</li> </ol> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Input Data    \u2502\n\u2502   (100 rows)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Validation    \u2502\n\u2502   Tests Run     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n    \u25bc         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Valid \u2502  \u2502  Invalid  \u2502\n\u2502(95)   \u2502  \u2502   (5)     \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502            \u2502\n    \u25bc            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Target \u2502  \u2502Quarantine \u2502\n\u2502 Table \u2502  \u2502  Table    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/quarantine/#example-complete-quarantine-pipeline","title":"Example: Complete Quarantine Pipeline","text":"<pre><code>project: CustomerData\nengine: spark\n\nconnections:\n  bronze:\n    type: local\n    base_path: ./data/bronze\n  silver:\n    type: local\n    base_path: ./data/silver\n\npipelines:\n  - pipeline: ingest_customers\n    layer: silver\n    nodes:\n      - name: validate_customers\n        read:\n          connection: bronze\n          path: customers_raw\n          format: parquet\n\n        validation:\n          tests:\n            # Required fields\n            - type: not_null\n              columns: [customer_id, email, created_at]\n              on_fail: quarantine\n\n            # Email format\n            - type: regex_match\n              column: email\n              pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n              on_fail: quarantine\n\n            # Age validation\n            - type: range\n              column: age\n              min: 0\n              max: 150\n              on_fail: quarantine\n\n          quarantine:\n            connection: silver\n            path: quarantine/customers\n            add_columns:\n              _rejection_reason: true\n              _rejected_at: true\n              _source_batch_id: true\n              _failed_tests: true\n\n        write:\n          connection: silver\n          path: customers\n          format: delta\n          mode: append\n</code></pre>"},{"location":"features/quarantine/#querying-quarantine-data","title":"Querying Quarantine Data","text":"<p>After running the pipeline, query the quarantine table to analyze failures:</p> <pre><code>-- View recent quarantined rows\nSELECT\n    customer_id,\n    email,\n    _rejection_reason,\n    _failed_tests,\n    _rejected_at,\n    _source_batch_id\nFROM quarantine.customers\nWHERE _rejected_at &gt;= current_date() - INTERVAL 7 DAYS\nORDER BY _rejected_at DESC;\n\n-- Count failures by test type\nSELECT\n    _failed_tests,\n    COUNT(*) as count\nFROM quarantine.customers\nGROUP BY _failed_tests\nORDER BY count DESC;\n</code></pre>"},{"location":"features/quarantine/#alerts-for-quarantine-events","title":"Alerts for Quarantine Events","text":"<p>Configure alerts to notify when rows are quarantined:</p> <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_quarantine\n    metadata:\n      throttle_minutes: 15\n      channel: \"#data-quality\"\n</code></pre>"},{"location":"features/quarantine/#best-practices","title":"Best Practices","text":"<ol> <li>Use meaningful test names - Helps identify failures in quarantine data</li> <li>Set appropriate retention - Balance storage costs vs debugging needs</li> <li>Monitor quarantine rates - High rates may indicate upstream data issues</li> <li>Combine with gates - Use quality gates to abort if too many rows are quarantined</li> <li>Automate reprocessing - Build workflows to reprocess fixed quarantine data</li> </ol>"},{"location":"features/quarantine/#related","title":"Related","text":"<ul> <li>Quality Gates - Batch-level validation</li> <li>Alerting - Alert on quarantine events</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/schema_tracking/","title":"Schema Version Tracking","text":"<p>Track schema changes over time with automatic versioning, change detection, and CLI tools.</p>"},{"location":"features/schema_tracking/#overview","title":"Overview","text":"<p>Odibi automatically tracks schema changes in the System Catalog: - Version history: Every schema change creates a new version - Change detection: Identifies added, removed, and modified columns - CLI tools: Query history and compare versions</p>"},{"location":"features/schema_tracking/#how-it-works","title":"How It Works","text":"<ol> <li>After each pipeline run, the output schema is captured</li> <li>Schema is hashed and compared to the previous version</li> <li>If changed, a new version is recorded with change details</li> <li>History is stored in <code>meta_schemas</code> table</li> </ol>"},{"location":"features/schema_tracking/#automatic-tracking","title":"Automatic Tracking","text":"<p>Schema tracking happens automatically during pipeline execution when: - A node writes to a table/path - The System Catalog is configured</p> <p>No additional configuration is required.</p>"},{"location":"features/schema_tracking/#cli-commands","title":"CLI Commands","text":""},{"location":"features/schema_tracking/#view-schema-history","title":"View Schema History","text":"<pre><code>odibi schema history &lt;table&gt; --config config.yaml\n</code></pre> <p>Example: <pre><code>$ odibi schema history silver/customers --config pipeline.yaml\n\nSchema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre></p> <p>Options: <pre><code>odibi schema history &lt;table&gt; --config config.yaml \\\n    --limit 20 \\        # Show last 20 versions (default: 10)\n    --format json       # Output as JSON\n</code></pre></p>"},{"location":"features/schema_tracking/#compare-schema-versions","title":"Compare Schema Versions","text":"<pre><code>odibi schema diff &lt;table&gt; --config config.yaml --from-version 3 --to-version 5\n</code></pre> <p>Example: <pre><code>$ odibi schema diff silver/customers --config pipeline.yaml --from-version 3 --to-version 5\n\nSchema Diff: silver/customers\nFrom v3 \u2192 v5\n============================================================\n  customer_id                  STRING               (unchanged)\n  email                        STRING               (unchanged)\n  name                         STRING               (unchanged)\n- legacy_id                    STRING               (removed in v5)\n+ loyalty_tier                 STRING               (added in v5)\n+ created_at                   TIMESTAMP            (added in v5)\n+ updated_at                   TIMESTAMP            (added in v5)\n</code></pre></p> <p>Without versions (compares latest two): <pre><code>odibi schema diff silver/customers --config pipeline.yaml\n</code></pre></p>"},{"location":"features/schema_tracking/#programmatic-access","title":"Programmatic Access","text":""},{"location":"features/schema_tracking/#track-schema-manually","title":"Track Schema Manually","text":"<pre><code>from odibi.catalog import CatalogManager\n\ncatalog = CatalogManager(spark, config, base_path, engine)\n\n# Track a schema change\nresult = catalog.track_schema(\n    table_path=\"silver/customers\",\n    schema={\"customer_id\": \"STRING\", \"email\": \"STRING\", \"age\": \"INT\"},\n    pipeline=\"customer_pipeline\",\n    node=\"process_customers\",\n    run_id=\"run-12345\",\n)\n\nprint(f\"Changed: {result['changed']}\")\nprint(f\"Version: {result['version']}\")\nprint(f\"Columns added: {result.get('columns_added', [])}\")\nprint(f\"Columns removed: {result.get('columns_removed', [])}\")\nprint(f\"Types changed: {result.get('columns_type_changed', [])}\")\n</code></pre>"},{"location":"features/schema_tracking/#query-schema-history","title":"Query Schema History","text":"<pre><code># Get history for a table\nhistory = catalog.get_schema_history(\"silver/customers\", limit=10)\n\nfor record in history:\n    print(f\"v{record['schema_version']}: {record['captured_at']}\")\n    if record.get('columns_added'):\n        print(f\"  Added: {record['columns_added']}\")\n</code></pre>"},{"location":"features/schema_tracking/#schema-record-structure","title":"Schema Record Structure","text":"<p>Each schema version record includes:</p> Field Description <code>table_path</code> Full path to the table <code>schema_version</code> Auto-incrementing version number <code>schema_hash</code> MD5 hash of column definitions <code>columns</code> JSON of column names and types <code>captured_at</code> Timestamp of capture <code>pipeline</code> Pipeline that made the change <code>node</code> Node that made the change <code>run_id</code> Execution run ID <code>columns_added</code> List of new columns <code>columns_removed</code> List of removed columns <code>columns_type_changed</code> List of columns with type changes"},{"location":"features/schema_tracking/#storage-location","title":"Storage Location","text":"<p>Schema history is stored in the System Catalog:</p> <pre><code>system:\n  connection: adls_bronze\n  path: _odibi_system\n</code></pre> <p>Location: <code>{connection_base_path}/_odibi_system/meta_schemas/</code></p>"},{"location":"features/schema_tracking/#example-detecting-breaking-changes","title":"Example: Detecting Breaking Changes","text":"<p>Use schema tracking to detect breaking changes before they impact downstream:</p> <pre><code>def check_for_breaking_changes(catalog, table_path):\n    \"\"\"Check if recent schema changes might break downstream.\"\"\"\n    history = catalog.get_schema_history(table_path, limit=2)\n\n    if len(history) &lt; 2:\n        return False  # No previous version\n\n    latest = history[0]\n    removed = latest.get('columns_removed', [])\n    type_changes = latest.get('columns_type_changed', [])\n\n    if removed or type_changes:\n        print(f\"\u26a0\ufe0f Potential breaking changes in {table_path}\")\n        if removed:\n            print(f\"  Removed columns: {removed}\")\n        if type_changes:\n            print(f\"  Type changes: {type_changes}\")\n        return True\n\n    return False\n</code></pre>"},{"location":"features/schema_tracking/#integration-with-lineage","title":"Integration with Lineage","text":"<p>Combine schema tracking with lineage to assess impact:</p> <pre><code># Check what would be affected by a schema change\nodibi lineage impact silver/customers --config config.yaml\n</code></pre> <pre><code>\u26a0\ufe0f  Impact Analysis: silver/customers\n============================================================\n\nChanges to silver/customers would affect:\n\n  Affected Tables:\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 2 downstream table(s) in 2 pipeline(s)\n</code></pre>"},{"location":"features/schema_tracking/#best-practices","title":"Best Practices","text":"<ol> <li>Review schema changes - Check history after deployments</li> <li>Monitor for removals - Removed columns often break downstream</li> <li>Document type changes - Type changes may affect queries</li> <li>Use lineage for impact - Know what's affected before changing</li> <li>Automate checks - Add schema validation to CI/CD</li> </ol>"},{"location":"features/schema_tracking/#related","title":"Related","text":"<ul> <li>Cross-Pipeline Lineage - Impact analysis</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/state/","title":"State Management","text":"<p>Odibi tracks pipeline execution state, high-water marks (HWM), and run history for resumable, incremental processing.</p>"},{"location":"features/state/#overview","title":"Overview","text":"<p>State management enables:</p> <ul> <li>Resume from failure: Skip successfully completed nodes</li> <li>High-water marks: Track last processed timestamp/ID for incremental loads</li> <li>Run history: Query past executions and their outcomes</li> <li>Concurrent writes: Safe multi-pipeline execution with retry logic</li> </ul>"},{"location":"features/state/#state-backends","title":"State Backends","text":""},{"location":"features/state/#localjsonstatebackend","title":"LocalJSONStateBackend","text":"<p>Simple JSON file storage for local development:</p> <pre><code>from odibi.state import LocalJSONStateBackend\n\nbackend = LocalJSONStateBackend(\".odibi/state.json\")\n\n# Used automatically when no system catalog is configured\n</code></pre> <p>Storage location: <code>.odibi/state.json</code></p>"},{"location":"features/state/#catalogstatebackend","title":"CatalogStateBackend","text":"<p>Delta Lake-based storage for production (supports Spark and local deltalake):</p> <pre><code>from odibi.state import CatalogStateBackend\n\nbackend = CatalogStateBackend(\n    meta_runs_path=\"/path/to/meta_runs\",\n    meta_state_path=\"/path/to/meta_state\",\n    spark_session=spark,           # Optional\n    storage_options={\"key\": \"val\"} # For Azure/S3\n)\n</code></pre> <p>Configured via <code>system</code> in your YAML config:</p> <pre><code>system:\n  connection: my_storage\n  path: _system\n\nconnections:\n  my_storage:\n    type: local  # or azure_blob\n    base_path: ./data\n</code></pre>"},{"location":"features/state/#statemanager-api","title":"StateManager API","text":"<p>The <code>StateManager</code> wraps a backend and provides high-level operations:</p> <pre><code>from odibi.state import StateManager, create_state_backend\nfrom odibi.config import load_config_from_file\n\n# Create from config\nconfig = load_config_from_file(\"odibi.yaml\")\nbackend = create_state_backend(config, project_root=\".\")\nstate_mgr = StateManager(backend=backend)\n</code></pre>"},{"location":"features/state/#high-water-marks","title":"High-Water Marks","text":"<pre><code># Get HWM value\nlast_id = state_mgr.get_hwm(\"orders.last_processed_id\")\n\n# Set HWM value\nstate_mgr.set_hwm(\"orders.last_processed_id\", 12345)\n\n# Batch set (efficient for parallel pipelines)\nstate_mgr.set_hwm_batch([\n    {\"key\": \"orders.hwm\", \"value\": 100},\n    {\"key\": \"customers.hwm\", \"value\": 200},\n])\n</code></pre>"},{"location":"features/state/#run-status","title":"Run Status","text":"<pre><code># Check if node succeeded in last run\nsuccess = state_mgr.get_last_run_status(\"pipeline_name\", \"node_name\")\n\n# Get full run info (metadata, timestamp)\ninfo = state_mgr.get_last_run_info(\"pipeline_name\", \"node_name\")\n# Returns: {\"success\": True, \"metadata\": {...}}\n</code></pre>"},{"location":"features/state/#save-pipeline-run","title":"Save Pipeline Run","text":"<pre><code># Called automatically by PipelineManager\nstate_mgr.save_pipeline_run(\"my_pipeline\", results)\n</code></pre>"},{"location":"features/state/#concurrent-write-handling","title":"Concurrent Write Handling","text":"<p>The <code>CatalogStateBackend</code> handles Delta Lake concurrent write conflicts with automatic retry:</p> <ul> <li>Exponential backoff: 1s, 2s, 4s, 8s, 16s delays</li> <li>Jitter: Random 0-1s added to prevent thundering herd</li> <li>Max retries: 5 attempts before failing</li> <li>Conflict detection: Catches <code>ConcurrentAppendException</code> and similar</li> </ul> <p>This enables safe parallel pipeline execution on shared state tables.</p>"},{"location":"features/state/#configuration","title":"Configuration","text":""},{"location":"features/state/#using-system-catalog-recommended","title":"Using System Catalog (Recommended)","text":"<pre><code>project: MyProject\nengine: spark\n\nsystem:\n  connection: catalog_storage\n  path: _system\n\nconnections:\n  catalog_storage:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: odibi\n    auth:\n      mode: account_key\n      account_key: ${AZURE_STORAGE_KEY}\n</code></pre>"},{"location":"features/state/#local-development-auto-fallback","title":"Local Development (Auto-fallback)","text":"<p>If no <code>system</code> config is provided, Odibi uses <code>LocalJSONStateBackend</code> automatically:</p> <pre><code>\u26a0\ufe0f No system catalog configured. Using local JSON state backend (local-only mode).\n</code></pre>"},{"location":"features/state/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/state/#resume-not-working-nodes-re-run-every-time","title":"Resume not working - nodes re-run every time","text":"<p>Symptom: <code>--resume</code> flag doesn't skip completed nodes.</p> <p>Causes: - No system catalog configured (state not persisted) - State file deleted or corrupted - Node name changed between runs</p> <p>Fixes: <pre><code># Ensure system catalog is configured\nsystem:\n  connection: catalog_conn\n  meta_runs_path: meta/runs\n  meta_state_path: meta/state\n</code></pre></p>"},{"location":"features/state/#high-water-mark-hwm-not-updating","title":"High Water Mark (HWM) not updating","text":"<p>Symptom: Incremental reads fetch all data instead of new records.</p> <p>Causes: - First run always does full load (expected) - HWM column has NULL values - State backend not persisting</p> <p>Fixes: <pre><code># Check current HWM state\nodibi catalog state --config config.yaml\n\n# Verify HWM column has no NULLs\n# HWM is set to MAX(column) after successful run\n</code></pre></p>"},{"location":"features/state/#state-corruption-after-failed-run","title":"State corruption after failed run","text":"<p>Symptom: Pipeline behaves unexpectedly after a failure.</p> <p>Fix: Reset state for specific node: <pre><code># View current state\nodibi catalog state --config config.yaml\n\n# If needed, delete and re-run (full load)\n# State will be rebuilt on next successful run\n</code></pre></p>"},{"location":"features/state/#local-json-state-lost-between-environments","title":"Local JSON state lost between environments","text":"<p>Cause: <code>LocalJSONStateBackend</code> stores state in <code>.odibi/state.json</code> locally.</p> <p>Fix: Use <code>CatalogStateBackend</code> with Delta Lake for shared/persistent state: <pre><code>system:\n  connection: shared_storage\n  meta_runs_path: _odibi/runs\n  meta_state_path: _odibi/state\n</code></pre></p>"},{"location":"features/state/#concurrent-pipeline-runs-corrupt-state","title":"Concurrent pipeline runs corrupt state","text":"<p>Symptom: State inconsistent when multiple pipelines run simultaneously.</p> <p>Fix: Use Delta Lake catalog backend (supports concurrent writes with retry): <pre><code>system:\n  connection: delta_catalog\n  meta_state_path: _odibi/state  # Delta table with ACID support\n</code></pre></p>"},{"location":"features/state/#related","title":"Related","text":"<ul> <li>Incremental Loading \u2014 HWM-based incremental</li> <li>Catalog \u2014 System catalog for metadata</li> <li>CLI Guide \u2014 <code>odibi catalog state</code> command</li> </ul>"},{"location":"features/stories/","title":"Execution Stories","text":"<p>Auto-generated pipeline execution documentation with rich metadata, sample data, and multiple output formats.</p>"},{"location":"features/stories/#overview","title":"Overview","text":"<p>Odibi's Story system provides: - Execution timeline: Complete record of pipeline runs with timestamps - Node-level metrics: Duration, row counts, schema changes per node - Sample data capture: Input/output samples with automatic redaction - Multiple renderers: HTML, Markdown, JSON output formats - Themes: Customizable styling for HTML reports - Retention policies: Automatic cleanup of old stories</p>"},{"location":"features/stories/#configuration","title":"Configuration","text":""},{"location":"features/stories/#basic-story-setup","title":"Basic Story Setup","text":"<pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre>"},{"location":"features/stories/#story-config-options","title":"Story Config Options","text":"Field Type Required Default Description <code>connection</code> string Yes - Connection name for story output <code>path</code> string Yes - Path for stories (relative to connection base_path) <code>max_sample_rows</code> int No <code>10</code> Maximum rows to include in samples <code>retention_days</code> int No <code>30</code> Days to keep stories before cleanup <code>retention_count</code> int No <code>100</code> Maximum number of stories to retain <code>failure_sample_size</code> int No <code>100</code> Rows to capture per validation failure <code>max_failure_samples</code> int No <code>500</code> Total failed rows across all validations <code>max_sampled_validations</code> int No <code>5</code> After this many validations, show only counts <code>theme</code> string No <code>default</code> Theme name or path to YAML theme file <code>include_samples</code> bool No <code>true</code> Whether to include data samples"},{"location":"features/stories/#remote-storage","title":"Remote Storage","text":"<p>Stories can be written to remote storage (ADLS, S3) using fsspec:</p> <pre><code>story:\n  output_path: abfss://container@account.dfs.core.windows.net/stories/\n  storage_options:\n    account_key: \"${STORAGE_ACCOUNT_KEY}\"\n</code></pre>"},{"location":"features/stories/#story-contents","title":"Story Contents","text":"<p>Each story captures comprehensive execution metadata:</p>"},{"location":"features/stories/#execution-timeline","title":"Execution Timeline","text":"Metric Description <code>started_at</code> ISO timestamp when pipeline started <code>completed_at</code> ISO timestamp when pipeline finished <code>duration</code> Total execution time in seconds <code>run_id</code> Unique identifier for the run"},{"location":"features/stories/#node-results","title":"Node Results","text":"<p>For each node in the pipeline:</p> Metric Description <code>node_name</code> Name of the node <code>operation</code> Operation type (read, transform, write) <code>status</code> Execution status: <code>success</code>, <code>failed</code>, <code>skipped</code> <code>duration</code> Node execution time in seconds <code>rows_in</code> Input row count <code>rows_out</code> Output row count <code>rows_change</code> Row count difference <code>rows_change_pct</code> Percentage change in row count"},{"location":"features/stories/#sample-data","title":"Sample Data","text":"<p>Sample data is captured with automatic redaction of sensitive values:</p> <pre><code>sample_data:\n  - order_id: 12345\n    customer_email: \"[REDACTED]\"\n    amount: 99.99\n  - order_id: 12346\n    customer_email: \"[REDACTED]\"\n    amount: 149.99\n</code></pre> <p>Configure sample capture:</p> <pre><code>story:\n  max_sample_rows: 5      # Limit sample size\n  include_samples: true   # Enable/disable samples\n</code></pre>"},{"location":"features/stories/#schema-changes","title":"Schema Changes","text":"<p>Stories track schema evolution:</p> Field Description <code>schema_in</code> Input column names <code>schema_out</code> Output column names <code>columns_added</code> New columns added <code>columns_removed</code> Columns removed <code>columns_renamed</code> Renamed columns"},{"location":"features/stories/#validation-results","title":"Validation Results","text":"<p>Validation warnings and errors are captured:</p> <pre><code>validation_warnings:\n  - \"Column 'email' has 5% null values\"\n  - \"Date range extends beyond expected bounds\"\n</code></pre> <p>Error details for failed nodes:</p> <pre><code>error_type: ValueError\nerror_message: \"Column 'order_id' contains duplicate values\"\nerror_traceback: \"Full Python traceback...\"\nerror_traceback_cleaned: \"Cleaned traceback (Spark/Java noise removed)\"\n</code></pre>"},{"location":"features/stories/#execution-steps","title":"Execution Steps","text":"<p>Stories capture the execution steps taken during node processing for debugging:</p> <pre><code>execution_steps:\n  - \"Read from bronze_db\"\n  - \"Applied pattern 'deduplicate'\"\n  - \"Executed 2 pre-SQL statement(s)\"\n  - \"Passed 3 contract checks\"\n</code></pre>"},{"location":"features/stories/#failed-rows-samples","title":"Failed Rows Samples","text":"<p>When validations fail, stories capture sample rows that failed each validation:</p> <pre><code>failed_rows_samples:\n  not_null_customer_id:\n    - { order_id: 123, customer_id: null, amount: 50.00 }\n    - { order_id: 456, customer_id: null, amount: 75.00 }\n  positive_amount:\n    - { order_id: 789, customer_id: \"C001\", amount: -10.00 }\n\nfailed_rows_counts:\n  not_null_customer_id: 150\n  positive_amount: 25\n</code></pre> <p>Configure failure sample limits:</p> <pre><code>story:\n  failure_sample_size: 100        # Max rows per validation\n  max_failure_samples: 500        # Total rows across all validations\n  max_sampled_validations: 5      # After 5 validations, show only counts\n</code></pre>"},{"location":"features/stories/#retry-history","title":"Retry History","text":"<p>When retries occur, the full history is captured:</p> <pre><code>retry_history:\n  - attempt: 1\n    success: false\n    error: \"Connection timeout\"\n    error_type: \"TimeoutError\"\n    duration: 1.2\n  - attempt: 2\n    success: false\n    error: \"Connection timeout\"\n    error_type: \"TimeoutError\"\n    duration: 2.4\n  - attempt: 3\n    success: true\n    duration: 0.8\n</code></pre>"},{"location":"features/stories/#delta-lake-info","title":"Delta Lake Info","text":"<p>For Delta Lake writes, version and operation metrics are captured:</p> <pre><code>delta_info:\n  version: 42\n  operation: MERGE\n  operation_metrics:\n    numTargetRowsInserted: 150\n    numTargetRowsUpdated: 25\n</code></pre>"},{"location":"features/stories/#themes","title":"Themes","text":"<p>Customize HTML story appearance with built-in or custom themes.</p>"},{"location":"features/stories/#built-in-themes","title":"Built-in Themes","text":"Theme Description <code>default</code> Clean, professional blue theme <code>corporate</code> Traditional business styling with serif headings <code>dark</code> Dark mode with high-contrast colors <code>minimal</code> Simple black and white, compact layout"},{"location":"features/stories/#using-themes","title":"Using Themes","text":"<pre><code>story:\n  theme: dark\n</code></pre>"},{"location":"features/stories/#custom-theme-file","title":"Custom Theme File","text":"<p>Create a custom theme YAML file:</p> <pre><code># my_theme.yaml\nname: company_brand\nprimary_color: \"#003366\"\nsuccess_color: \"#2e7d32\"\nerror_color: \"#c62828\"\nwarning_color: \"#ff9900\"\nbg_color: \"#ffffff\"\ntext_color: \"#333333\"\nfont_family: \"Arial, sans-serif\"\nheading_font: \"Georgia, serif\"\nlogo_url: \"https://example.com/logo.png\"\ncompany_name: \"Acme Corp\"\nfooter_text: \"Confidential - Internal Use Only\"\n</code></pre> <p>Reference in config:</p> <pre><code>story:\n  theme: path/to/my_theme.yaml\n</code></pre>"},{"location":"features/stories/#theme-options","title":"Theme Options","text":"Option Type Description <code>name</code> string Theme identifier <code>primary_color</code> hex Main accent color <code>success_color</code> hex Success status color <code>error_color</code> hex Error status color <code>warning_color</code> hex Warning status color <code>bg_color</code> hex Background color <code>text_color</code> hex Primary text color <code>border_color</code> hex Border color <code>code_bg</code> hex Code block background <code>font_family</code> string Body font stack <code>heading_font</code> string Heading font stack <code>code_font</code> string Monospace font stack <code>font_size</code> string Base font size <code>max_width</code> string Container max width <code>logo_url</code> string URL to company logo <code>company_name</code> string Company name for branding <code>footer_text</code> string Custom footer text <code>custom_css</code> string Additional CSS rules"},{"location":"features/stories/#renderers","title":"Renderers","text":"<p>Stories can be rendered in multiple formats.</p>"},{"location":"features/stories/#html-renderer","title":"HTML Renderer","text":"<p>Default format with interactive, responsive design:</p> <pre><code>from odibi.story.renderers import HTMLStoryRenderer, get_renderer\nfrom odibi.story.themes import get_theme\n\n# Using the factory\nrenderer = get_renderer(\"html\")\nhtml = renderer.render(metadata)\n\n# With custom theme\ntheme = get_theme(\"dark\")\nrenderer = HTMLStoryRenderer(theme=theme)\nhtml = renderer.render(metadata)\n</code></pre> <p>Features: - Collapsible node sections - Status indicators with color coding - Summary statistics dashboard - Responsive layout</p>"},{"location":"features/stories/#json-renderer","title":"JSON Renderer","text":"<p>Machine-readable format for API integration:</p> <pre><code>from odibi.story.renderers import JSONStoryRenderer\n\nrenderer = JSONStoryRenderer()\njson_str = renderer.render(metadata)\n</code></pre> <p>Output structure: <pre><code>{\n  \"pipeline_name\": \"process_orders\",\n  \"run_id\": \"20240130_101500\",\n  \"started_at\": \"2024-01-30T10:15:00\",\n  \"completed_at\": \"2024-01-30T10:15:45\",\n  \"duration\": 45.23,\n  \"total_nodes\": 5,\n  \"completed_nodes\": 4,\n  \"failed_nodes\": 1,\n  \"skipped_nodes\": 0,\n  \"success_rate\": 80.0,\n  \"total_rows_processed\": 15000,\n  \"nodes\": [...]\n}\n</code></pre></p>"},{"location":"features/stories/#markdown-renderer","title":"Markdown Renderer","text":"<p>GitHub-flavored markdown for documentation:</p> <pre><code>from odibi.story.renderers import MarkdownStoryRenderer\n\nrenderer = MarkdownStoryRenderer()\nmd = renderer.render(metadata)\n</code></pre>"},{"location":"features/stories/#renderer-factory","title":"Renderer Factory","text":"<p>Use the factory function to get a renderer by format:</p> <pre><code>from odibi.story.renderers import get_renderer\n\n# Supported formats: \"html\", \"markdown\", \"md\", \"json\"\nrenderer = get_renderer(\"json\")\noutput = renderer.render(metadata)\n</code></pre>"},{"location":"features/stories/#retention","title":"Retention","text":"<p>Stories are automatically cleaned up based on retention policies.</p>"},{"location":"features/stories/#retention-configuration","title":"Retention Configuration","text":"<pre><code>story:\n  retention_days: 30    # Delete stories older than 30 days\n  retention_count: 100  # Keep maximum 100 stories per pipeline\n</code></pre>"},{"location":"features/stories/#how-retention-works","title":"How Retention Works","text":"<ol> <li>Count-based: When story count exceeds <code>retention_count</code>, oldest stories are deleted first</li> <li>Time-based: Stories older than <code>retention_days</code> are deleted</li> <li>Both apply: A story is deleted if it exceeds either limit</li> </ol>"},{"location":"features/stories/#storage-structure","title":"Storage Structure","text":"<p>Stories are organized by pipeline and date:</p> <pre><code>stories/\n\u251c\u2500\u2500 process_orders/\n\u2502   \u251c\u2500\u2500 2024-01-30/\n\u2502   \u2502   \u251c\u2500\u2500 run_10-15-00.html\n\u2502   \u2502   \u251c\u2500\u2500 run_10-15-00.json\n\u2502   \u2502   \u251c\u2500\u2500 run_14-30-00.html\n\u2502   \u2502   \u2514\u2500\u2500 run_14-30-00.json\n\u2502   \u2514\u2500\u2500 2024-01-31/\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 process_customers/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"features/stories/#remote-storage-cleanup","title":"Remote Storage Cleanup","text":"<p>Note: Automatic cleanup for remote storage (ADLS, S3) is not yet implemented. Monitor storage usage manually.</p>"},{"location":"features/stories/#examples","title":"Examples","text":""},{"location":"features/stories/#complete-story-configuration","title":"Complete Story Configuration","text":"<pre><code>project: DataPipeline\nengine: spark\n\nstory:\n  output_path: stories/\n  max_sample_rows: 10\n  retention_days: 30\n  retention_count: 100\n  theme: corporate\n  include_samples: true\n\npipelines:\n  - pipeline: process_orders\n    nodes:\n      - name: read_orders\n        read:\n          connection: bronze\n          path: orders/\n\n      - name: transform_orders\n        transform:\n          operation: sql\n          query: |\n            SELECT order_id, customer_id, amount\n            FROM {read_orders}\n            WHERE amount &gt; 0\n\n      - name: write_orders\n        write:\n          connection: silver\n          path: orders/\n          mode: merge\n</code></pre>"},{"location":"features/stories/#generated-story-output-json","title":"Generated Story Output (JSON)","text":"<pre><code>{\n  \"pipeline_name\": \"process_orders\",\n  \"pipeline_layer\": \"silver\",\n  \"run_id\": \"20240130_101500\",\n  \"started_at\": \"2024-01-30T10:15:00\",\n  \"completed_at\": \"2024-01-30T10:15:45\",\n  \"duration\": 45.23,\n  \"total_nodes\": 3,\n  \"completed_nodes\": 3,\n  \"failed_nodes\": 0,\n  \"skipped_nodes\": 0,\n  \"success_rate\": 100.0,\n  \"total_rows_processed\": 15000,\n  \"project\": \"DataPipeline\",\n  \"nodes\": [\n    {\n      \"node_name\": \"read_orders\",\n      \"operation\": \"read\",\n      \"status\": \"success\",\n      \"duration\": 5.12,\n      \"rows_out\": 15500,\n      \"schema_out\": [\"order_id\", \"customer_id\", \"amount\", \"created_at\"]\n    },\n    {\n      \"node_name\": \"transform_orders\",\n      \"operation\": \"transform\",\n      \"status\": \"success\",\n      \"duration\": 2.34,\n      \"rows_in\": 15500,\n      \"rows_out\": 15000,\n      \"rows_change\": -500,\n      \"rows_change_pct\": -3.2,\n      \"columns_removed\": [\"created_at\"]\n    },\n    {\n      \"node_name\": \"write_orders\",\n      \"operation\": \"write\",\n      \"status\": \"success\",\n      \"duration\": 37.77,\n      \"rows_out\": 15000,\n      \"delta_info\": {\n        \"version\": 42,\n        \"operation\": \"MERGE\",\n        \"operation_metrics\": {\n          \"numTargetRowsInserted\": 500,\n          \"numTargetRowsUpdated\": 14500\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"features/stories/#programmatic-story-generation","title":"Programmatic Story Generation","text":"<pre><code>from odibi.story.generator import StoryGenerator\nfrom odibi.story.metadata import PipelineStoryMetadata\nfrom odibi.story.themes import get_theme\n\n# Create generator\ngenerator = StoryGenerator(\n    pipeline_name=\"process_orders\",\n    max_sample_rows=10,\n    output_path=\"stories/\",\n    retention_days=30,\n    retention_count=100,\n)\n\n# Generate story after pipeline execution\nstory_path = generator.generate(\n    node_results=node_results,\n    completed=[\"read_orders\", \"transform_orders\", \"write_orders\"],\n    failed=[],\n    skipped=[],\n    duration=45.23,\n    start_time=\"2024-01-30T10:15:00\",\n    end_time=\"2024-01-30T10:15:45\",\n)\n\n# Get summary for alerts\nalert_summary = generator.get_alert_summary()\n</code></pre>"},{"location":"features/stories/#documentation-stories","title":"Documentation Stories","text":"<p>Generate stakeholder-ready documentation from pipeline config:</p> <pre><code>from odibi.story.doc_story import DocStoryGenerator\nfrom odibi.config import PipelineConfig\n\n# Load pipeline config\npipeline_config = PipelineConfig.from_yaml(\"pipeline.yaml\")\n\n# Generate documentation\ndoc_generator = DocStoryGenerator(pipeline_config)\ndoc_path = doc_generator.generate(\n    output_path=\"docs/pipeline_doc.html\",\n    format=\"html\",\n    include_flow_diagram=True,\n)\n</code></pre>"},{"location":"features/stories/#related","title":"Related","text":"<ul> <li>Alerting - Stories linked in alert payloads</li> <li>Quality Gates - Gate results captured in stories</li> <li>Schema Tracking - Schema changes in stories</li> <li>YAML Schema Reference</li> </ul>"},{"location":"features/transformers/","title":"Transformers","text":"<p>Declarative data transformations with SQL-first semantics, dual-engine support (Spark/Pandas), and extensible custom transforms.</p>"},{"location":"features/transformers/#overview","title":"Overview","text":"<p>Odibi's transformer system provides: - SQL-First Design: All core operations leverage SQL for optimal engine performance - Dual-Engine Support: Seamless execution on Spark or Pandas/DuckDB - Built-in Library: 30+ production-ready transformers - Extensibility: Register custom transforms with the <code>@transform</code> decorator - Chained Operations: Compose multiple transforms in <code>transform.steps</code></p>"},{"location":"features/transformers/#configuration","title":"Configuration","text":""},{"location":"features/transformers/#basic-transformer-usage","title":"Basic Transformer Usage","text":"<pre><code>nodes:\n  - name: clean_orders\n    source: raw_orders\n    transformer: \"filter_rows\"\n    params:\n      condition: \"status = 'active'\"\n</code></pre>"},{"location":"features/transformers/#transformer-config-options","title":"Transformer Config Options","text":"Field Type Required Description <code>transformer</code> string Yes Transformer name (e.g., <code>filter_rows</code>, <code>scd2</code>) <code>params</code> object Yes Transformer-specific parameters"},{"location":"features/transformers/#transform-steps","title":"Transform Steps","text":"<p>Chain multiple transformations in sequence using <code>transform.steps</code>:</p> <pre><code>nodes:\n  - name: process_customers\n    source: raw_customers\n    transform:\n      steps:\n        - transformer: \"clean_text\"\n          params:\n            columns: [\"email\", \"name\"]\n            trim: true\n            case: \"lower\"\n\n        - transformer: \"filter_rows\"\n          params:\n            condition: \"email IS NOT NULL\"\n\n        - transformer: \"derive_columns\"\n          params:\n            derivations:\n              full_name: \"concat(first_name, ' ', last_name)\"\n\n        - transformer: \"deduplicate\"\n          params:\n            keys: [\"customer_id\"]\n            order_by: \"updated_at DESC\"\n</code></pre>"},{"location":"features/transformers/#built-in-transformers","title":"Built-in Transformers","text":""},{"location":"features/transformers/#sql-core-transformers","title":"SQL Core Transformers","text":"<p>Basic SQL operations that work across all engines.</p>"},{"location":"features/transformers/#filter_rows","title":"filter_rows","text":"<p>Filter rows using SQL WHERE conditions.</p> <pre><code>transformer: \"filter_rows\"\nparams:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre>"},{"location":"features/transformers/#derive_columns","title":"derive_columns","text":"<p>Add new columns using SQL expressions.</p> <pre><code>transformer: \"derive_columns\"\nparams:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre>"},{"location":"features/transformers/#cast_columns","title":"cast_columns","text":"<p>Cast columns to different types.</p> <pre><code>transformer: \"cast_columns\"\nparams:\n  casts:\n    age: \"int\"\n    salary: \"double\"\n    created_at: \"timestamp\"\n</code></pre>"},{"location":"features/transformers/#clean_text","title":"clean_text","text":"<p>Apply text cleaning operations (trim, case conversion).</p> <pre><code>transformer: \"clean_text\"\nparams:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"  # Options: lower, upper, preserve\n</code></pre>"},{"location":"features/transformers/#extract_date_parts","title":"extract_date_parts","text":"<p>Extract year, month, day, hour from timestamps.</p> <pre><code>transformer: \"extract_date_parts\"\nparams:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\", \"day\"]\n</code></pre>"},{"location":"features/transformers/#normalize_schema","title":"normalize_schema","text":"<p>Rename, drop, and reorder columns.</p> <pre><code>transformer: \"normalize_schema\"\nparams:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre>"},{"location":"features/transformers/#sort","title":"sort","text":"<p>Sort data by columns.</p> <pre><code>transformer: \"sort\"\nparams:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre>"},{"location":"features/transformers/#limit-sample","title":"limit / sample","text":"<p>Limit or randomly sample rows.</p> <pre><code># Limit\ntransformer: \"limit\"\nparams:\n  n: 100\n  offset: 0\n\n# Sample\ntransformer: \"sample\"\nparams:\n  fraction: 0.1\n  seed: 42\n</code></pre>"},{"location":"features/transformers/#distinct","title":"distinct","text":"<p>Remove duplicate rows.</p> <pre><code>transformer: \"distinct\"\nparams:\n  columns: [\"category\", \"status\"]  # Optional: subset of columns\n</code></pre>"},{"location":"features/transformers/#fill_nulls","title":"fill_nulls","text":"<p>Replace null values with defaults.</p> <pre><code>transformer: \"fill_nulls\"\nparams:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre>"},{"location":"features/transformers/#split_part","title":"split_part","text":"<p>Extract parts of strings by delimiter.</p> <pre><code>transformer: \"split_part\"\nparams:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre>"},{"location":"features/transformers/#date_add-date_trunc-date_diff","title":"date_add / date_trunc / date_diff","text":"<p>Date arithmetic operations.</p> <pre><code># Add interval\ntransformer: \"date_add\"\nparams:\n  col: \"created_at\"\n  value: 7\n  unit: \"day\"\n\n# Truncate to precision\ntransformer: \"date_trunc\"\nparams:\n  col: \"created_at\"\n  unit: \"month\"\n\n# Calculate difference\ntransformer: \"date_diff\"\nparams:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre>"},{"location":"features/transformers/#case_when","title":"case_when","text":"<p>Conditional logic.</p> <pre><code>transformer: \"case_when\"\nparams:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre>"},{"location":"features/transformers/#convert_timezone","title":"convert_timezone","text":"<p>Convert timestamps between timezones.</p> <pre><code>transformer: \"convert_timezone\"\nparams:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre>"},{"location":"features/transformers/#concat_columns","title":"concat_columns","text":"<p>Concatenate multiple columns.</p> <pre><code>transformer: \"concat_columns\"\nparams:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre>"},{"location":"features/transformers/#relational-transformers","title":"Relational Transformers","text":"<p>Operations involving multiple datasets.</p>"},{"location":"features/transformers/#join","title":"join","text":"<p>Join with another dataset.</p> <pre><code>transformer: \"join\"\nparams:\n  right_dataset: \"customers\"  # Must be in depends_on\n  on: [\"customer_id\"]\n  how: \"left\"  # inner, left, right, full, cross\n  prefix: \"cust\"  # Prefix for right columns (avoid collisions)\n</code></pre>"},{"location":"features/transformers/#union","title":"union","text":"<p>Union multiple datasets.</p> <pre><code>transformer: \"union\"\nparams:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true  # Match columns by name\n</code></pre>"},{"location":"features/transformers/#pivot","title":"pivot","text":"<p>Pivot rows into columns.</p> <pre><code>transformer: \"pivot\"\nparams:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n  values: [\"Jan\", \"Feb\", \"Mar\"]  # Optional: explicit pivot values\n</code></pre>"},{"location":"features/transformers/#unpivot","title":"unpivot","text":"<p>Unpivot (melt) columns into rows.</p> <pre><code>transformer: \"unpivot\"\nparams:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre>"},{"location":"features/transformers/#aggregate","title":"aggregate","text":"<p>Group and aggregate data.</p> <pre><code>transformer: \"aggregate\"\nparams:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre>"},{"location":"features/transformers/#advanced-transformers","title":"Advanced Transformers","text":"<p>Complex data processing operations.</p>"},{"location":"features/transformers/#deduplicate","title":"deduplicate","text":"<p>Remove duplicates using window functions.</p> <pre><code>transformer: \"deduplicate\"\nparams:\n  keys: [\"customer_id\"]\n  order_by: \"updated_at DESC\"  # Keep most recent\n</code></pre>"},{"location":"features/transformers/#explode_list_column","title":"explode_list_column","text":"<p>Flatten array/list columns into rows.</p> <pre><code>transformer: \"explode_list_column\"\nparams:\n  column: \"items\"\n  outer: true  # Keep rows with empty lists\n</code></pre>"},{"location":"features/transformers/#dict_based_mapping","title":"dict_based_mapping","text":"<p>Map values using a dictionary.</p> <pre><code>transformer: \"dict_based_mapping\"\nparams:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre>"},{"location":"features/transformers/#regex_replace","title":"regex_replace","text":"<p>Replace patterns using regex.</p> <pre><code>transformer: \"regex_replace\"\nparams:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre>"},{"location":"features/transformers/#unpack_struct","title":"unpack_struct","text":"<p>Flatten struct/dict columns.</p> <pre><code>transformer: \"unpack_struct\"\nparams:\n  column: \"user_info\"\n</code></pre>"},{"location":"features/transformers/#hash_columns","title":"hash_columns","text":"<p>Hash columns for PII anonymization.</p> <pre><code>transformer: \"hash_columns\"\nparams:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"  # or \"md5\"\n</code></pre>"},{"location":"features/transformers/#generate_surrogate_key","title":"generate_surrogate_key","text":"<p>Create deterministic surrogate keys.</p> <pre><code>transformer: \"generate_surrogate_key\"\nparams:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre>"},{"location":"features/transformers/#parse_json","title":"parse_json","text":"<p>Parse JSON strings into structured data.</p> <pre><code>transformer: \"parse_json\"\nparams:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre>"},{"location":"features/transformers/#validate_and_flag","title":"validate_and_flag","text":"<p>Flag rows that fail validation rules.</p> <pre><code>transformer: \"validate_and_flag\"\nparams:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre>"},{"location":"features/transformers/#window_calculation","title":"window_calculation","text":"<p>Apply window functions.</p> <pre><code>transformer: \"window_calculation\"\nparams:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre>"},{"location":"features/transformers/#normalize_json","title":"normalize_json","text":"<p>Flatten nested JSON/struct into columns.</p> <pre><code>transformer: \"normalize_json\"\nparams:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre>"},{"location":"features/transformers/#sessionize","title":"sessionize","text":"<p>Assign session IDs based on inactivity threshold.</p> <pre><code>transformer: \"sessionize\"\nparams:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800  # 30 minutes\n  session_col: \"session_id\"\n</code></pre>"},{"location":"features/transformers/#scd-slowly-changing-dimensions","title":"SCD (Slowly Changing Dimensions)","text":"<p>Track historical changes with SCD Type 2.</p> <pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"  # Registered table name\n  keys: [\"customer_id\"]           # Entity keys\n  track_cols: [\"address\", \"tier\"] # Columns to monitor for changes\n  effective_time_col: \"txn_date\"  # When change occurred\n  end_time_col: \"valid_to\"        # End timestamp column\n  current_flag_col: \"is_current\"  # Current record flag\n</code></pre> <p>Connection-Based Path (ADLS):</p> <pre><code>transformer: \"scd2\"\nparams:\n  connection: adls_prod           # Connection name\n  path: sales/silver/dim_customers  # Relative path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre> <p>How SCD2 Works: 1. Match: Finds existing records using <code>keys</code> 2. Compare: Checks <code>track_cols</code> to detect changes 3. Close: Updates old record's <code>end_time_col</code> if changed 4. Insert: Adds new record with open-ended validity</p> <p>Note: SCD2 returns a DataFrame. You must add a <code>write:</code> block with <code>mode: overwrite</code>.</p>"},{"location":"features/transformers/#merge-transformer","title":"Merge Transformer","text":"<p>Upsert, append, or delete records in target tables.</p> <pre><code># Upsert (Update + Insert)\ntransformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"upsert\"\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre> <p>Merge Strategies:</p> Strategy Description <code>upsert</code> Update existing, insert new (default) <code>append_only</code> Only insert new keys, ignore duplicates <code>delete_match</code> Delete records matching source keys <p>Advanced Merge Options:</p> <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n  insert_condition: \"source.is_deleted = false\"\n  delete_condition: \"source.is_deleted = true\"\n  optimize_write: true\n  zorder_by: [\"customer_id\"]\n  cluster_by: [\"region\"]\n</code></pre> <p>Connection-Based Path (ADLS):</p> <p>Use <code>connection</code> + <code>path</code> instead of <code>target</code> to leverage connection-based path resolution:</p> <pre><code>transform:\n  steps:\n    - function: merge\n      params:\n        connection: adls_prod           # Connection name\n        path: sales/silver/customers    # Relative path\n        register_table: silver.customers  # Register in metastore\n        keys: [\"customer_id\"]\n        strategy: \"upsert\"\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre>"},{"location":"features/transformers/#validation-transformers","title":"Validation Transformers","text":"<p>Cross-dataset validation checks.</p> <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"  # or \"schema_match\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre>"},{"location":"features/transformers/#delete-detection","title":"Delete Detection","text":"<p>Detect deleted records for CDC-like behavior.</p> <pre><code>transformer: \"detect_deletes\"\nparams:\n  mode: \"snapshot_diff\"  # Compare Delta versions\n  keys: [\"customer_id\"]\n  soft_delete_col: \"is_deleted\"  # Add flag column\n  max_delete_percent: 10.0  # Safety threshold\n  on_threshold_breach: \"error\"  # error, warn, skip\n</code></pre> <p>Delete Detection Modes:</p> Mode Description <code>none</code> Disabled <code>snapshot_diff</code> Compare current vs previous Delta version <code>sql_compare</code> Compare against live source via JDBC"},{"location":"features/transformers/#creating-custom-transformers","title":"Creating Custom Transformers","text":"<p>Use the <code>@transform</code> decorator with <code>FunctionRegistry</code> to create custom transformers.</p>"},{"location":"features/transformers/#basic-custom-transformer","title":"Basic Custom Transformer","text":"<pre><code>from pydantic import BaseModel, Field\nfrom odibi.context import EngineContext\nfrom odibi.registry import transform\n\n\nclass MyTransformParams(BaseModel):\n    \"\"\"Parameters for my custom transform.\"\"\"\n    column: str = Field(..., description=\"Column to process\")\n    multiplier: float = Field(default=1.0, description=\"Multiplier value\")\n\n\n@transform(\"my_custom_transform\", param_model=MyTransformParams)\ndef my_custom_transform(context: EngineContext, **params) -&gt; EngineContext:\n    \"\"\"My custom transformation.\"\"\"\n    config = MyTransformParams(**params)\n\n    # Use SQL for cross-engine compatibility\n    sql_query = f\"\"\"\n        SELECT *, {config.column} * {config.multiplier} AS {config.column}_scaled\n        FROM df\n    \"\"\"\n    return context.sql(sql_query)\n</code></pre>"},{"location":"features/transformers/#using-custom-transformers-in-yaml","title":"Using Custom Transformers in YAML","text":"<pre><code>nodes:\n  - name: process_data\n    source: raw_data\n    transformer: \"my_custom_transform\"\n    params:\n      column: \"price\"\n      multiplier: 1.1\n</code></pre>"},{"location":"features/transformers/#engine-specific-logic","title":"Engine-Specific Logic","text":"<pre><code>from odibi.enums import EngineType\n\n@transform(\"dual_engine_transform\", param_model=MyParams)\ndef dual_engine_transform(context: EngineContext, **params) -&gt; EngineContext:\n    config = MyParams(**params)\n\n    if context.engine_type == EngineType.SPARK:\n        # Spark-specific implementation\n        import pyspark.sql.functions as F\n        df = context.df.withColumn(\"new_col\", F.lit(\"spark\"))\n        return context.with_df(df)\n\n    elif context.engine_type == EngineType.PANDAS:\n        # Pandas-specific implementation\n        df = context.df.copy()\n        df[\"new_col\"] = \"pandas\"\n        return context.with_df(df)\n</code></pre>"},{"location":"features/transformers/#complete-example","title":"Complete Example","text":"<pre><code>project: ECommerceETL\nengine: spark\n\nconnections:\n  bronze:\n    type: delta\n    path: \"dbfs:/bronze\"\n  silver:\n    type: delta\n    path: \"dbfs:/silver\"\n  gold:\n    type: delta\n    path: \"dbfs:/gold\"\n\npipelines:\n  - pipeline: orders_to_gold\n    nodes:\n      # Clean raw data\n      - name: clean_orders\n        source:\n          connection: bronze\n          path: orders\n        transform:\n          steps:\n            - transformer: \"clean_text\"\n              params:\n                columns: [\"customer_email\"]\n                trim: true\n                case: \"lower\"\n\n            - transformer: \"cast_columns\"\n              params:\n                casts:\n                  order_date: \"timestamp\"\n                  total_amount: \"double\"\n\n            - transformer: \"filter_rows\"\n              params:\n                condition: \"total_amount &gt; 0\"\n\n      # Deduplicate and enrich\n      - name: enriched_orders\n        source: clean_orders\n        depends_on: [clean_orders, customers]\n        transform:\n          steps:\n            - transformer: \"deduplicate\"\n              params:\n                keys: [\"order_id\"]\n                order_by: \"updated_at DESC\"\n\n            - transformer: \"join\"\n              params:\n                right_dataset: \"customers\"\n                on: [\"customer_id\"]\n                how: \"left\"\n\n            - transformer: \"derive_columns\"\n              params:\n                derivations:\n                  order_year: \"YEAR(order_date)\"\n                  order_month: \"MONTH(order_date)\"\n\n      # Final merge to gold\n      - name: gold_orders\n        source: enriched_orders\n        transformer: \"merge\"\n        params:\n          target: \"gold.orders\"\n          keys: [\"order_id\"]\n          strategy: \"upsert\"\n          audit_cols:\n            created_col: \"dw_created_at\"\n            updated_col: \"dw_updated_at\"\n        destination:\n          connection: gold\n          path: orders\n</code></pre>"},{"location":"features/transformers/#best-practices","title":"Best Practices","text":"<ol> <li>Use SQL-first transforms - They push computation to the engine for optimal performance</li> <li>Chain with transform.steps - Compose multiple operations declaratively</li> <li>Prefer built-in transforms - They're tested for dual-engine compatibility</li> <li>Use Pydantic models - Define parameter schemas for custom transforms</li> <li>Handle nulls explicitly - Use <code>fill_nulls</code> or <code>COALESCE</code> in derivations</li> <li>Document custom transforms - Include docstrings and param descriptions</li> </ol>"},{"location":"features/transformers/#related","title":"Related","text":"<ul> <li>Quality Gates - Validate transform outputs</li> <li>Quarantine Tables - Handle failed validations</li> <li>YAML Schema Reference - Complete configuration options</li> </ul>"},{"location":"features/ui/","title":"Web UI","text":"<p>Odibi includes a web dashboard for viewing pipeline status, browsing story reports, and inspecting configuration.</p>"},{"location":"features/ui/#overview","title":"Overview","text":"<p>The UI is a FastAPI application with three main views:</p> Endpoint Description <code>/</code> Pipeline dashboard \u2014 status, last run, node counts <code>/stories</code> Story browser \u2014 HTML reports from past runs <code>/config</code> Config viewer \u2014 current YAML configuration"},{"location":"features/ui/#starting-the-ui","title":"Starting the UI","text":"<pre><code># Start with a specific config file\nodibi ui config.yaml\n\n# Or set via environment variable\nexport ODIBI_CONFIG=config.yaml\nodibi ui\n</code></pre> <p>The server runs on <code>http://localhost:8000</code> by default.</p>"},{"location":"features/ui/#dashboard","title":"Dashboard (<code>/</code>)","text":"<p>The main dashboard shows:</p> <ul> <li>Pipeline name: Each pipeline defined in your config</li> <li>Last run: Timestamp of most recent execution</li> <li>Status: SUCCESS, FAILED, or UNKNOWN</li> <li>Node counts: Total nodes and successful nodes</li> </ul> <p>Status is determined by checking node results: - All nodes succeeded \u2192 SUCCESS - Any node failed \u2192 FAILED - No nodes \u2192 UNKNOWN</p>"},{"location":"features/ui/#stories-browser-stories","title":"Stories Browser (<code>/stories</code>)","text":"<p>Browse HTML story reports generated by pipeline runs.</p> <p>Features: - Lists all runs organized by pipeline \u2192 date \u2192 report - Links to view full HTML reports - Sorted by date (newest first)</p> <p>Story files are served from the configured story path:</p> <pre><code>story:\n  connection: system\n  path: stories/\n</code></pre>"},{"location":"features/ui/#config-viewer-config","title":"Config Viewer (<code>/config</code>)","text":"<p>View the current YAML configuration:</p> <ul> <li>Read-only view of your pipeline config</li> <li>Useful for debugging and verification</li> <li>Shows which config file is loaded</li> </ul>"},{"location":"features/ui/#configuration","title":"Configuration","text":"<p>The UI reads configuration from:</p> <ol> <li><code>ODIBI_CONFIG</code> environment variable</li> <li>Default locations: <code>odibi.yaml</code>, <code>odibi.yml</code>, <code>project.yaml</code></li> </ol> <p>Story paths are resolved from the <code>story</code> and <code>connections</code> config.</p>"},{"location":"features/ui/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the FastAPI app directly:</p> <pre><code>from odibi.ui.app import app\nimport uvicorn\n\n# Run with custom settings\nuvicorn.run(app, host=\"0.0.0.0\", port=8080)\n</code></pre>"},{"location":"features/ui/#related","title":"Related","text":"<ul> <li>Stories \u2014 Pipeline execution reports</li> <li>CLI Guide \u2014 All CLI commands</li> <li>Configuration \u2014 YAML config reference</li> </ul>"},{"location":"features/validation_performance/","title":"Validation Performance Optimization Guide","text":"<p>This document details the performance optimizations made to odibi's contracts and validation system.</p>"},{"location":"features/validation_performance/#summary-of-issues-found-and-fixed","title":"Summary of Issues Found and Fixed","text":""},{"location":"features/validation_performance/#1-critical-bottlenecks-identified","title":"1. Critical Bottlenecks Identified","text":"Issue Engine Impact Fix Applied Double scan in UNIQUE check Pandas 2x slower Single <code>duplicated()</code> call with cached result Full DataFrame copy for invalid rows Pandas O(N) memory waste Mask-based operations only Eager LazyFrame collection Polars Defeats optimizer Lazy aggregations, scalar-only collects Missing contract types Polars Inconsistent behavior Added UNIQUE, ACCEPTED_VALUES, RANGE, REGEX, ROW_COUNT Per-row test_results lists Quarantine O(N\u00d7tests) memory Aggregate counts only No fail-fast mode All Full scan even on early failure Added <code>fail_fast</code> config option No FRESHNESS minutes support Spark/Pandas Missing 'm' unit Added minutes parsing"},{"location":"features/validation_performance/#2-performance-anti-patterns-fixed","title":"2. Performance Anti-Patterns Fixed","text":""},{"location":"features/validation_performance/#pandas-engine","title":"Pandas Engine","text":"<p>Before (UNIQUE check): <pre><code># TWO full scans + two boolean Series allocations\nif df.duplicated(subset=cols).any():\n    dup_count = df.duplicated(subset=cols).sum()\n</code></pre></p> <p>After: <pre><code># ONE scan, cached result\ndups = df.duplicated(subset=cols)\ndup_count = int(dups.sum())\nif dup_count &gt; 0:\n    ...\n</code></pre></p> <p>Before (ACCEPTED_VALUES): <pre><code># Creates full invalid DataFrame in memory\ninvalid = df[~df[col].isin(test.values)]\nif not invalid.empty:\n    examples = invalid[col].unique()[:3]\n</code></pre></p> <p>After: <pre><code># Mask-only, minimal memory\nmask = ~df[col].isin(test.values)\ninvalid_count = int(mask.sum())\nif invalid_count &gt; 0:\n    examples = df.loc[mask, col].dropna().unique()[:3]\n</code></pre></p>"},{"location":"features/validation_performance/#polars-engine","title":"Polars Engine","text":"<p>Before: <pre><code># Forces full collection, defeats lazy optimization\nif isinstance(df, pl.LazyFrame):\n    df = df.collect()  # Materializes everything!\n</code></pre></p> <p>After: <pre><code># Keeps lazy, collects only scalars\nif is_lazy:\n    row_count = df.select(pl.len()).collect().item()\n    null_count = df.select(pl.col(col).is_null().sum()).collect().item()\n</code></pre></p>"},{"location":"features/validation_performance/#quarantine-system","title":"Quarantine System","text":"<p>Before: <pre><code># O(N \u00d7 num_tests) memory usage\nfor name, mask in test_masks.items():\n    test_results[name] = mask.tolist()  # Huge Python list per test!\n</code></pre></p> <p>After: <pre><code># O(num_tests) memory - aggregate counts only\nfor name, mask in test_masks.items():\n    pass_count = int(mask.sum())\n    fail_count = len(df) - pass_count\n    test_results[name] = {\"pass_count\": pass_count, \"fail_count\": fail_count}\n</code></pre></p>"},{"location":"features/validation_performance/#new-configuration-options","title":"New Configuration Options","text":""},{"location":"features/validation_performance/#fail-fast-mode","title":"Fail-Fast Mode","text":"<p>Stop validation on first failure for faster feedback:</p> <pre><code>validation:\n  fail_fast: true  # Stop on first failure\n  tests:\n    - type: not_null\n      columns: [id, customer_id]\n    - type: unique\n      columns: [id]\n</code></pre>"},{"location":"features/validation_performance/#dataframe-caching-spark","title":"DataFrame Caching (Spark)","text":"<p>Cache DataFrame before validation when running many tests:</p> <pre><code>validation:\n  cache_df: true  # Cache for multi-test validation\n  tests:\n    - type: not_null\n      columns: [id]\n    # ... 10+ more tests\n</code></pre>"},{"location":"features/validation_performance/#quarantine-sampling","title":"Quarantine Sampling","text":"<p>Limit quarantined rows to prevent storage blowup:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    max_rows: 10000         # Cap at 10K rows\n    sample_fraction: 0.1    # Or sample 10% of invalid rows\n</code></pre>"},{"location":"features/validation_performance/#engine-parity","title":"Engine Parity","text":"<p>All three engines now support the same contract types:</p> Contract Type Pandas Polars Spark not_null \u2705 \u2705 \u2705 unique \u2705 \u2705 \u2705 accepted_values \u2705 \u2705 \u2705 row_count \u2705 \u2705 \u2705 range \u2705 \u2705 \u2705 regex_match \u2705 \u2705 \u2705 freshness \u2705 \u2705 \u2705 schema \u2705 \u2705 \u2705 custom_sql \u2705 \u26a0\ufe0f Skipped \u2705"},{"location":"features/validation_performance/#freshness-duration-units","title":"FRESHNESS Duration Units","text":"<p>All engines now support: - <code>\"24h\"</code> - hours - <code>\"7d\"</code> - days - <code>\"30m\"</code> - minutes (NEW)</p>"},{"location":"features/validation_performance/#benchmark-tests","title":"Benchmark Tests","text":"<p>Run benchmarks to verify performance:</p> <pre><code>pytest tests/benchmarks/test_validation_perf.py -v -s\n</code></pre> <p>Benchmark scenarios: - 10 contracts on 100K rows (Pandas) - 15 contracts on 100K rows (Pandas) - Fail-fast vs full validation comparison - Polars eager vs lazy comparison - Quarantine split performance - Memory efficiency verification</p>"},{"location":"features/validation_performance/#expected-performance-gains","title":"Expected Performance Gains","text":"Scenario Before After Improvement 10 contracts / 100K rows (Pandas) ~5s ~3s 40% faster Fail-fast on early failure Full scan Early exit Up to 10x faster Polars LazyFrame (10 contracts) Eager collect Lazy 30-50% faster Quarantine memory (100K rows, 5 tests) ~4MB lists ~1KB dicts 4000x less memory"},{"location":"features/validation_performance/#best-practices","title":"Best Practices","text":"<ol> <li>Use fail-fast for CI/CD: Quick feedback when data is clearly bad</li> <li>Enable cache_df for Spark: When running 5+ contracts per node</li> <li>Set quarantine limits: Prevent storage blowup on high-failure batches</li> <li>Prefer Polars LazyFrame: Let the query optimizer work for you</li> <li>Batch similar tests: Multiple NOT_NULL columns in one test</li> </ol>"},{"location":"guides/MIGRATION_GUIDE/","title":"Odibi V3 Migration Guide","text":""},{"location":"guides/MIGRATION_GUIDE/#1-privacy-inheritance-safety-upgrade","title":"1. Privacy Inheritance (Safety Upgrade)","text":"<p>Change: PII status now inherits from upstream nodes. If a column is marked as <code>pii: true</code> in a source node, it will remain PII in all downstream nodes unless explicitly declassified.</p> <p>Impact: *   Existing Pipelines: Pipelines that relied on implicit declassification (i.e., assuming PII status is lost after one node) may now trigger anonymization in downstream nodes if privacy is configured. *   New Behavior: Safer by default. You cannot accidentally expose PII by forgetting to re-tag it.</p> <p>Action Required: If you have columns that are no longer PII (e.g., you hashed them or dropped the sensitive part), you must now explicitly declassify them in the node configuration if you want to stop tracking them as PII.</p> <pre><code>- name: downstream_node\n  privacy:\n    method: \"hash\"\n    declassify:\n      - \"hashed_email\"  # Stop tracking this as PII\n</code></pre>"},{"location":"guides/MIGRATION_GUIDE/#2-spark-write-modes","title":"2. Spark Write Modes","text":"<p>Change: The Spark engine now supports <code>upsert</code> and <code>append_once</code> modes, bringing it to parity with the Pandas engine.</p> <p>Usage: These modes require <code>keys</code> to be defined in the <code>write.options</code> (or <code>params</code> if using a transformer that passes them). They are supported only for Delta Lake format.</p> <pre><code>- name: merge_users\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"users\"\n    mode: \"upsert\"\n    options:\n      keys: [\"user_id\"]\n</code></pre>"},{"location":"guides/MIGRATION_GUIDE/#3-context-api","title":"3. Context API","text":"<p>Change: The <code>NodeExecutionContext</code> (available in custom transformers as <code>ctx</code>) has been updated. *   Added <code>ctx.schema</code>: Returns a dictionary of column types. *   Added <code>ctx.pii_metadata</code>: Dictionary of active PII columns.</p>"},{"location":"guides/ai-assistant-setup/","title":"AI Assistant Setup Guide","text":"<p>This guide explains how to set up AI coding assistants (Continue, Cline, Amp) to work with odibi using MCP (Model Context Protocol) servers.</p>"},{"location":"guides/ai-assistant-setup/#overview","title":"Overview","text":"<p>Odibi includes an MCP server (<code>odibi-knowledge</code>) that gives AI assistants perfect knowledge of: - All 52+ transformers and their signatures - All 6 DWH patterns (Dimension, Fact, SCD2, Merge, Aggregation, Date Dimension) - Exact YAML pipeline structure - Framework documentation (2,300+ lines)</p>"},{"location":"guides/ai-assistant-setup/#installation","title":"Installation","text":""},{"location":"guides/ai-assistant-setup/#1-install-odibi-with-mcp-support","title":"1. Install odibi with MCP support","text":"<pre><code># Clone the repository\ngit clone https://github.com/henryodibi11/Odibi.git\ncd odibi\n\n# Create virtual environment\npython -m venv .venv\n\n# Activate (Windows)\n.venv\\Scripts\\activate\n\n# Activate (Linux/Mac)\nsource .venv/bin/activate\n\n# Install with MCP support\npip install -e \".[mcp]\"\n\n# Or with RAG (semantic codebase search)\npip install -e \".[mcp-rag]\"\n</code></pre>"},{"location":"guides/ai-assistant-setup/#2-verify-installation","title":"2. Verify installation","text":"<pre><code>python -m odibi_mcp.server --help\n</code></pre>"},{"location":"guides/ai-assistant-setup/#ai-assistant-configuration","title":"AI Assistant Configuration","text":""},{"location":"guides/ai-assistant-setup/#continue-recommended-for-byokazure-openai","title":"Continue (Recommended for BYOK/Azure OpenAI)","text":"<p>Continue uses your own API keys (Azure OpenAI, OpenAI, etc.).</p> <p>Step 1: Create user config (<code>~/.continue/config.yaml</code>):</p> <pre><code>models:\n  - name: Azure GPT-4o\n    provider: azure\n    model: gpt-4o\n    apiBase: https://YOUR-RESOURCE.openai.azure.com/\n    apiKey: YOUR-AZURE-KEY\n    apiVersion: \"2024-02-15-preview\"\n    engine: YOUR-DEPLOYMENT-NAME\n</code></pre> <p>Step 2: Open odibi workspace</p> <p>The <code>.continuerc.json</code> in the odibi root auto-loads: - odibi-knowledge MCP (your framework docs) - filesystem, git, memory, fetch MCPs - context7 (up-to-date library docs) - sequential-thinking (complex reasoning)</p>"},{"location":"guides/ai-assistant-setup/#cline","title":"Cline","text":"<p>Cline also supports BYOK. Configure MCPs in VS Code settings or <code>cline_mcp_settings.json</code>:</p> <pre><code>{\n  \"odibi-knowledge\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"odibi_mcp.server\"],\n    \"cwd\": \"/path/to/odibi\"\n  }\n}\n</code></pre> <p>The <code>.clinerules</code> file in the odibi root provides framework guidance.</p>"},{"location":"guides/ai-assistant-setup/#amp","title":"Amp","text":"<p>Amp is a managed service (no BYOK). Add MCP in VS Code settings:</p> <pre><code>\"amp.mcpServers\": {\n  \"odibi-knowledge\": {\n    \"command\": \"python\",\n    \"args\": [\"-m\", \"odibi_mcp.server\"],\n    \"cwd\": \"/path/to/odibi\"\n  }\n}\n</code></pre> <p>The <code>AGENTS.md</code> file in the odibi root provides framework guidance.</p>"},{"location":"guides/ai-assistant-setup/#available-mcp-servers","title":"Available MCP Servers","text":""},{"location":"guides/ai-assistant-setup/#included-in-odibi-workspace-config","title":"Included in odibi workspace config","text":"MCP Purpose Requires odibi-knowledge Framework patterns, signatures, docs <code>pip install -e \".[mcp]\"</code> filesystem File read/write/search npx (auto-downloads) git Git operations npx (auto-downloads) memory Persistent context across sessions npx (auto-downloads) sequential-thinking Complex reasoning npx (auto-downloads) fetch Fetch web pages npx (auto-downloads) context7 Up-to-date library docs npx (auto-downloads)"},{"location":"guides/ai-assistant-setup/#odibi-knowledge-tools-21-total","title":"odibi-knowledge Tools (21 total)","text":"<p>Core Tools: | Tool | Description | |------|-------------| | <code>list_transformers</code> | List all 52+ transformers | | <code>list_patterns</code> | List all 6 DWH patterns | | <code>list_connections</code> | List all connection types | | <code>explain(name)</code> | Get detailed docs for any feature |</p> <p>Code Generation Tools: | Tool | Description | |------|-------------| | <code>get_transformer_signature</code> | Get exact function signature for custom transformers | | <code>get_yaml_structure</code> | Get exact YAML pipeline structure | | <code>generate_transformer</code> | Generate complete transformer Python code | | <code>generate_pipeline_yaml</code> | Generate complete pipeline YAML config | | <code>validate_yaml</code> | Validate YAML before saving |</p> <p>Decision Support Tools: | Tool | Description | |------|-------------| | <code>suggest_pattern</code> | Recommend the right pattern for your use case | | <code>get_engine_differences</code> | Spark vs Pandas vs Polars SQL differences | | <code>get_validation_rules</code> | All validation rule types with examples |</p> <p>Documentation Tools: | Tool | Description | |------|-------------| | <code>get_deep_context</code> | Get full ODIBI_DEEP_CONTEXT.md (2,300+ lines) | | <code>get_doc(path)</code> | Get specific doc file | | <code>list_docs(category)</code> | List available documentation | | <code>search_docs(query)</code> | Search all documentation | | <code>get_example(name)</code> | Get working example for any pattern/transformer |</p> <p>Debugging Tools: | Tool | Description | |------|-------------| | <code>diagnose_error</code> | Diagnose odibi errors and get fix suggestions | | <code>query_codebase(question)</code> | Semantic search (requires <code>mcp-rag</code>) | | <code>reindex</code> | Rebuild the semantic search index | | <code>get_index_stats</code> | Check index status |</p>"},{"location":"guides/ai-assistant-setup/#usage-tips","title":"Usage Tips","text":""},{"location":"guides/ai-assistant-setup/#prompt-examples","title":"Prompt Examples","text":"<pre><code># Get transformer signature before writing custom transformer\n\"Show me the exact signature for creating a custom odibi transformer\"\n\n# Get YAML structure before writing config\n\"What's the exact YAML structure for an odibi pipeline?\"\n\n# Use context7 for library docs\n\"How do I use PySpark window functions? use context7\"\n\n# Complex reasoning\n\"Plan the implementation of a new SCD2 pattern variant\"\n</code></pre>"},{"location":"guides/ai-assistant-setup/#auto-invoke-rules","title":"Auto-invoke Rules","text":"<p>Add to your AI assistant's rules to auto-use odibi-knowledge:</p> <pre><code>When working with odibi code or YAML configs, always use the odibi-knowledge\nMCP tools first: get_transformer_signature, get_yaml_structure, list_transformers.\n</code></pre>"},{"location":"guides/ai-assistant-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ai-assistant-setup/#mcp-server-not-starting","title":"MCP server not starting","text":"<pre><code># Check if odibi_mcp is installed\npython -c \"import odibi_mcp; print('OK')\"\n\n# Test the server directly\npython -m odibi_mcp.server\n</code></pre>"},{"location":"guides/ai-assistant-setup/#module-not-found-errors","title":"\"Module not found\" errors","text":"<pre><code># Reinstall with MCP support\npip install -e \".[mcp]\"\n</code></pre>"},{"location":"guides/ai-assistant-setup/#npx-mcps-not-downloading","title":"npx MCPs not downloading","text":"<pre><code># Ensure Node.js is installed\nnode --version  # Should be v18+\n\n# Clear npx cache if needed\nnpx clear-npx-cache\n</code></pre>"},{"location":"guides/ai-assistant-setup/#multi-machine-setup","title":"Multi-Machine Setup","text":"<p>The odibi workspace config files are portable:</p> File Contains Travels with git <code>.continuerc.json</code> MCP configs, rules reference \u2705 Yes <code>.continue/rules.md</code> Framework guidance \u2705 Yes <code>.clinerules</code> Cline guidance \u2705 Yes <code>AGENTS.md</code> Amp guidance \u2705 Yes <p>Per-machine setup (one time):</p> <ol> <li>Clone odibi repo</li> <li><code>pip install -e \".[mcp]\"</code></li> <li>Create <code>~/.continue/config.yaml</code> with your API keys</li> </ol> <p>Everything else auto-loads from the workspace.</p>"},{"location":"guides/avoiding_the_builder_trap/","title":"Avoiding the Builder Trap","text":"<p>A Guide for Odibi Maintainers and Contributors</p>"},{"location":"guides/avoiding_the_builder_trap/#what-is-the-builder-trap","title":"What is the Builder Trap?","text":"<p>The Builder Trap is the tendency to continuously add features, refactor code, and \"improve\" a framework without ever using it on real problems. It feels productive, but it creates:</p> <ul> <li>Blind spots: Features that seem useful but aren't</li> <li>Complexity: Code that solves imaginary problems</li> <li>Burnout: Endless work with no validation</li> <li>Drift: The framework diverges from real user needs</li> </ul> <p>The antidote: Use the framework more than you build it.</p>"},{"location":"guides/avoiding_the_builder_trap/#the-golden-ratio","title":"The Golden Ratio","text":"<p>Aim for this balance:</p> Activity Time Allocation Using Odibi (real pipelines, real data) 60% Fixing/Improving (based on usage pain) 30% New Features (planned, prioritized) 10% <p>If you're spending more than 30% of your time building new features, you're probably in the trap.</p>"},{"location":"guides/avoiding_the_builder_trap/#issue-driven-development","title":"Issue-Driven Development","text":""},{"location":"guides/avoiding_the_builder_trap/#the-rule","title":"The Rule","text":"<p>If it's not critical, create an issue instead of fixing it immediately.</p>"},{"location":"guides/avoiding_the_builder_trap/#the-decision-framework","title":"The Decision Framework","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Is this blocking you?           \u2502\n\u2502         (Tests fail, can't run)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502               \u2502\n        YES              NO\n         \u2502               \u2502\n         \u25bc               \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Fix Now \u2502    \u2502 Is it &lt; 5 min   \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 AND obvious?    \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502               \u2502\n                  YES              NO\n                   \u2502               \u2502\n                   \u25bc               \u25bc\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502 Fix Now \u2502    \u2502 CREATE      \u2502\n             \u2502 No Issue\u2502    \u2502 AN ISSUE    \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/avoiding_the_builder_trap/#why-this-works","title":"Why This Works","text":"<ol> <li>Prevents scope creep - \"Just one quick fix\" becomes 3 hours of yak shaving</li> <li>Forces prioritization - Is this actually important, or just visible right now?</li> <li>Creates documentation - Future you can see what was decided and why</li> <li>Enables batching - Related issues can be fixed together efficiently</li> <li>Protects focus - You stay on your current task</li> </ol>"},{"location":"guides/avoiding_the_builder_trap/#issue-hygiene","title":"Issue Hygiene","text":"<p>When creating issues, include:</p> <pre><code>## Problem\nWhat's broken or missing?\n\n## Impact\nWho is affected? How badly?\n\n## Proposed Solution (optional)\nQuick sketch of the fix\n\n## Effort Estimate\n- Trivial (&lt; 30 min)\n- Small (1-2 hours)\n- Medium (half day)\n- Large (1+ days)\n</code></pre> <p>Use labels consistently: - <code>bug</code> - Something is broken - <code>enhancement</code> - New feature or improvement - <code>documentation</code> - Docs updates - <code>good-first-issue</code> - Easy wins for new contributors - <code>priority:high</code> - Needs attention soon - <code>priority:low</code> - Nice to have</p>"},{"location":"guides/avoiding_the_builder_trap/#dogfooding-practices","title":"Dogfooding Practices","text":""},{"location":"guides/avoiding_the_builder_trap/#1-build-real-pipelines","title":"1. Build Real Pipelines","text":"<p>Don't just test with toy data. Use Odibi for actual work:</p> <ul> <li>Personal projects: Analyze your finances, fitness data, reading list</li> <li>Work tasks: If appropriate, use Odibi for real ETL jobs</li> <li>Side projects: Build something you actually need</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#2-experience-the-onboarding","title":"2. Experience the Onboarding","text":"<p>Periodically, pretend you're a new user:</p> <pre><code># Start fresh\nrm -rf .venv\npython -m venv .venv\npip install odibi\n\n# Follow your own Getting Started guide\n# Note every point of friction\n</code></pre>"},{"location":"guides/avoiding_the_builder_trap/#3-run-the-full-workflow","title":"3. Run the Full Workflow","text":"<p>Regularly exercise the complete user journey:</p> <pre><code>odibi init-pipeline mytest\nodibi validate odibi.yaml\nodibi run odibi.yaml --dry-run\nodibi run odibi.yaml\nodibi doctor odibi.yaml\nodibi story list\n</code></pre> <p>Ask yourself: - Was anything confusing? - Did error messages help or frustrate? - What would I Google if I got stuck?</p>"},{"location":"guides/avoiding_the_builder_trap/#4-break-it-on-purpose","title":"4. Break It On Purpose","text":"<p>Try to make Odibi fail:</p> <ul> <li>Missing connections</li> <li>Circular dependencies</li> <li>Invalid YAML</li> <li>Wrong column names in SQL</li> <li>Network failures (disconnect wifi mid-run)</li> </ul> <p>Good frameworks fail gracefully. Bad ones fail mysteriously.</p>"},{"location":"guides/avoiding_the_builder_trap/#signs-youre-in-the-builder-trap","title":"Signs You're in the Builder Trap","text":"<p>Watch for these warning signs:</p> Sign Reality Check \"I need to refactor X before I can use it\" No, you don't. Use it messy. \"Just one more feature, then it's ready\" It's ready now. Ship it. \"Nobody can use this until I fix Y\" Let them try. Their feedback &gt; your assumptions. \"I'll write docs after I finish building\" Docs ARE building. Write them now. \"This code isn't clean enough\" Clean code that isn't used is worthless."},{"location":"guides/avoiding_the_builder_trap/#the-cure","title":"The Cure","text":"<p>When you catch yourself building instead of using:</p> <ol> <li>Stop immediately</li> <li>Create an issue for what you were about to do</li> <li>Open <code>odibi.yaml</code> and run a real pipeline</li> <li>Note what actually bothers you during usage</li> <li>Those are your real priorities</li> </ol>"},{"location":"guides/avoiding_the_builder_trap/#weekly-review-ritual","title":"Weekly Review Ritual","text":"<p>Every week, spend 30 minutes on this:</p>"},{"location":"guides/avoiding_the_builder_trap/#1-triage-issues-10-min","title":"1. Triage Issues (10 min)","text":"<ul> <li>Review new issues</li> <li>Close duplicates or \"won't fix\"</li> <li>Prioritize the backlog</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#2-usage-reflection-10-min","title":"2. Usage Reflection (10 min)","text":"<ul> <li>What pipelines did I run this week?</li> <li>What frustrated me?</li> <li>What worked well?</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#3-plan-next-week-10-min","title":"3. Plan Next Week (10 min)","text":"<ul> <li>Pick 1-2 issues to address</li> <li>Schedule time for USING, not just building</li> <li>Resist the urge to add \"just one more thing\"</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#the-north-star-question","title":"The North Star Question","text":"<p>Before any work session, ask:</p> <p>\"Am I building this because a real user (including myself) hit this problem, or because I think someone might need it someday?\"</p> <p>If the answer is \"someday\" \u2192 Create an issue and move on.</p> <p>If the answer is \"I hit this yesterday\" \u2192 Fix it.</p>"},{"location":"guides/avoiding_the_builder_trap/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>The Mom Test - How to validate ideas through usage</li> <li>Shape Up - Basecamp's approach to shipping</li> <li>Just Fucking Ship - Amy Hoy on finishing things</li> </ul>"},{"location":"guides/avoiding_the_builder_trap/#summary","title":"Summary","text":"Do This Not This Use Odibi on real data Build features in isolation Create issues for future work Fix everything immediately Experience your own onboarding Assume the UX is fine Ship small, validate, iterate Wait until it's \"perfect\" Ask \"did I hit this problem?\" Ask \"might someone need this?\" <p>The framework is ready. Go use it.</p>"},{"location":"guides/best_practices/","title":"Odibi Best Practices Guide","text":"<p>Version: 2.4.0 Last Updated: 2025-12-03 Audience: Data Engineers, Analytics Engineers, Team Leads</p> <p>This guide covers recommended patterns for building maintainable, scalable, and production-ready Odibi pipelines.</p>"},{"location":"guides/best_practices/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Project Organization</li> <li>Pipeline Design</li> <li>Node Design</li> <li>Naming Conventions</li> <li>Configuration Management</li> <li>Performance</li> <li>Data Quality</li> <li>Cross-Pipeline Dependencies</li> <li>Security</li> <li>Version Control</li> </ol>"},{"location":"guides/best_practices/#1-project-organization","title":"1. Project Organization","text":""},{"location":"guides/best_practices/#recommended-folder-structure","title":"Recommended Folder Structure","text":"<pre><code>my-odibi-project/\n\u251c\u2500\u2500 project.yaml                    # Core config (connections, settings)\n\u251c\u2500\u2500 pipelines/\n\u2502   \u251c\u2500\u2500 bronze/\n\u2502   \u2502   \u2514\u2500\u2500 read_bronze.yaml        # Bronze layer pipeline\n\u2502   \u251c\u2500\u2500 silver/\n\u2502   \u2502   \u2514\u2500\u2500 transform_silver.yaml   # Silver layer pipeline\n\u2502   \u2514\u2500\u2500 gold/\n\u2502       \u2514\u2500\u2500 build_gold.yaml         # Gold layer pipeline\n\u251c\u2500\u2500 transformations/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 custom_transforms.py        # Custom Python transformations\n\u251c\u2500\u2500 sql/\n\u2502   \u2514\u2500\u2500 complex_queries.sql         # Complex SQL (optional)\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_pipelines.py           # Pipeline tests\n\u251c\u2500\u2500 .env                            # Local secrets (git-ignored)\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"guides/best_practices/#separation-of-concerns","title":"Separation of Concerns","text":"File Contains Does NOT Contain <code>project.yaml</code> Connections, system config, story config, imports Pipeline definitions <code>pipelines/*.yaml</code> Pipeline and node definitions Connection details <code>transformations/</code> Custom Python logic YAML configuration"},{"location":"guides/best_practices/#example-projectyaml","title":"Example <code>project.yaml</code>","text":"<pre><code>project: SalesAnalytics\ndescription: \"Sales Analytics Platform\"\nengine: spark\nversion: \"1.0.0\"\nowner: \"Data Team\"\n\n# === Connections (defined once, used everywhere) ===\nconnections:\n  source_db:\n    type: sql_server\n    host: ${DB_HOST}\n    database: ${DB_NAME}\n    auth:\n      mode: sql_login\n      username: ${DB_USER}\n      password: ${DB_PASS}\n\n  lakehouse:\n    type: azure_blob\n    account_name: ${STORAGE_ACCOUNT}\n    container: datalake\n    auth:\n      mode: account_key\n      account_key: ${STORAGE_KEY}\n\n# === System Catalog ===\nsystem:\n  connection: lakehouse\n  path: _odibi_system\n\n# === Story Configuration ===\nstory:\n  connection: lakehouse\n  path: stories/\n  retention_days: 30\n  auto_generate: true\n\n# === Global Settings ===\nperformance:\n  use_arrow: true\n  skip_null_profiling: true\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\nlogging:\n  level: INFO\n  structured: true\n\n# === Import Pipelines ===\nimports:\n  - pipelines/bronze/read_bronze.yaml\n  - pipelines/silver/transform_silver.yaml\n  - pipelines/gold/build_gold.yaml\n</code></pre>"},{"location":"guides/best_practices/#example-pipeline-file","title":"Example Pipeline File","text":"<p>pipelines/bronze/read_bronze.yaml: <pre><code>pipelines:\n  - pipeline: read_bronze\n    description: \"Ingest raw data from source systems\"\n    layer: bronze\n    nodes:\n      - name: orders\n        description: \"Raw orders from ERP\"\n        read:\n          connection: source_db\n          format: sql\n          table: sales.orders\n          incremental:\n            mode: stateful\n            column: updated_at\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/orders\"\n          mode: append\n          add_metadata: true\n\n      - name: customers\n        description: \"Customer master data\"\n        read:\n          connection: source_db\n          format: sql\n          table: sales.customers\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/customers\"\n          mode: append\n          add_metadata: true\n          skip_if_unchanged: true\n          skip_hash_columns: [customer_id]\n</code></pre></p>"},{"location":"guides/best_practices/#2-pipeline-design","title":"2. Pipeline Design","text":""},{"location":"guides/best_practices/#one-pipeline-per-layer-per-domain","title":"One Pipeline Per Layer Per Domain","text":"<p>\u2705 Good: <pre><code>pipelines/\n\u251c\u2500\u2500 bronze/\n\u2502   \u2514\u2500\u2500 read_bronze.yaml           # All bronze ingestion\n\u251c\u2500\u2500 silver/\n\u2502   \u2514\u2500\u2500 transform_silver.yaml      # All silver transformations\n\u2514\u2500\u2500 gold/\n    \u251c\u2500\u2500 gold_sales.yaml            # Sales domain aggregates\n    \u2514\u2500\u2500 gold_inventory.yaml        # Inventory domain aggregates\n</code></pre></p> <p>\u274c Avoid: <pre><code>pipelines/\n\u251c\u2500\u2500 orders_bronze_silver_gold.yaml  # Too many concerns in one file\n\u2514\u2500\u2500 everything.yaml                 # Unmaintainable\n</code></pre></p>"},{"location":"guides/best_practices/#pipeline-sizing-guidelines","title":"Pipeline Sizing Guidelines","text":"Node Count Recommendation 1-20 nodes Single pipeline file 20-50 nodes Consider splitting by sub-domain 50+ nodes Split into multiple pipelines"},{"location":"guides/best_practices/#keep-nodes-with-their-pipeline","title":"Keep Nodes with Their Pipeline","text":"<p>\u274c Don't split nodes into separate files: <pre><code># nodes/orders.yaml - BAD: nodes scattered across files\npipelines:\n  - pipeline: read_bronze\n    nodes:\n      - name: orders\n</code></pre></p> <p>\u2705 Keep nodes together: <pre><code># read_bronze.yaml - GOOD: all nodes in one place\npipelines:\n  - pipeline: read_bronze\n    nodes:\n      - name: orders\n        # ...\n      - name: customers\n        # ...\n      - name: products\n        # ...\n</code></pre></p> <p>Why? - <code>depends_on</code> relationships are visible in one file - Easier to understand the full pipeline flow - One file = one pipeline = one commit for changes</p>"},{"location":"guides/best_practices/#3-node-design","title":"3. Node Design","text":""},{"location":"guides/best_practices/#single-responsibility","title":"Single Responsibility","text":"<p>Each node should do one thing well:</p> <p>\u2705 Good: <pre><code>- name: load_orders\n  read: ...\n  write: ...\n\n- name: clean_orders\n  depends_on: [load_orders]\n  transform:\n    steps:\n      - sql: \"SELECT * FROM load_orders WHERE status IS NOT NULL\"\n  write: ...\n\n- name: enrich_orders\n  depends_on: [clean_orders, customers]\n  transform:\n    steps:\n      - operation: join\n        left: clean_orders\n        right: customers\n        on: [customer_id]\n  write: ...\n</code></pre></p> <p>\u274c Avoid: <pre><code>- name: do_everything\n  read: ...\n  transform:\n    steps:\n      - sql: \"...\"  # 500 lines of SQL doing everything\n  write: ...\n</code></pre></p>"},{"location":"guides/best_practices/#use-descriptions","title":"Use Descriptions","text":"<p>Always add descriptions for documentation and debugging:</p> <pre><code>- name: calculate_daily_revenue\n  description: \"Aggregates order amounts by day for finance reporting\"\n  tags: [daily, finance, critical]\n</code></pre>"},{"location":"guides/best_practices/#cache-strategically","title":"Cache Strategically","text":"<p>Use <code>cache: true</code> for nodes that are: - Read by multiple downstream nodes - Expensive to compute - Small enough to fit in memory</p> <pre><code>- name: dimension_products\n  description: \"Product dimension - cached for multiple joins\"\n  read: ...\n  cache: true  # Multiple nodes will join to this\n</code></pre>"},{"location":"guides/best_practices/#4-naming-conventions","title":"4. Naming Conventions","text":""},{"location":"guides/best_practices/#pipeline-names","title":"Pipeline Names","text":"<p>Use <code>snake_case</code> with layer prefix:</p> Pattern Example <code>{action}_{layer}</code> <code>read_bronze</code>, <code>transform_silver</code>, <code>build_gold</code> <code>{layer}_{domain}</code> <code>bronze_sales</code>, <code>silver_inventory</code>"},{"location":"guides/best_practices/#node-names","title":"Node Names","text":"<p>Use descriptive <code>snake_case</code>:</p> Pattern Example Source nodes <code>orders</code>, <code>customers</code>, <code>products</code> Transformed nodes <code>clean_orders</code>, <code>enriched_customers</code> Aggregated nodes <code>daily_sales</code>, <code>monthly_revenue</code> Dimension nodes <code>dim_product</code>, <code>dim_customer</code> Fact nodes <code>fact_orders</code>, <code>fact_inventory</code>"},{"location":"guides/best_practices/#connection-names","title":"Connection Names","text":"<p>Use environment + purpose:</p> <pre><code>connections:\n  prod_source_db:    # Production source database\n  prod_lakehouse:    # Production data lake\n  dev_lakehouse:     # Development data lake\n</code></pre>"},{"location":"guides/best_practices/#5-configuration-management","title":"5. Configuration Management","text":""},{"location":"guides/best_practices/#environment-variables-for-secrets","title":"Environment Variables for Secrets","text":"<p>\u2705 Always use environment variables for sensitive data: <pre><code>connections:\n  database:\n    host: ${DB_HOST}\n    username: ${DB_USER}\n    password: ${DB_PASSWORD}\n</code></pre></p> <p>\u274c Never hardcode secrets: <pre><code>connections:\n  database:\n    password: \"my_secret_password\"  # NEVER DO THIS\n</code></pre></p>"},{"location":"guides/best_practices/#use-env-for-local-development","title":"Use <code>.env</code> for Local Development","text":"<pre><code># .env (git-ignored)\nDB_HOST=localhost\nDB_USER=dev_user\nDB_PASSWORD=dev_password\nSTORAGE_ACCOUNT=devaccount\nSTORAGE_KEY=abc123...\n</code></pre>"},{"location":"guides/best_practices/#environment-specific-overrides","title":"Environment-Specific Overrides","text":"<pre><code># In project.yaml\nenvironments:\n  dev:\n    connections:\n      lakehouse:\n        container: dev-datalake\n  prod:\n    logging:\n      level: WARNING\n    connections:\n      lakehouse:\n        container: prod-datalake\n</code></pre> <p>Run with: <code>odibi run project.yaml --env prod</code></p>"},{"location":"guides/best_practices/#6-performance","title":"6. Performance","text":""},{"location":"guides/best_practices/#enable-arrow-for-pandas","title":"Enable Arrow for Pandas","text":"<pre><code>performance:\n  use_arrow: true  # Major speedup for Parquet I/O\n</code></pre>"},{"location":"guides/best_practices/#use-incremental-loading","title":"Use Incremental Loading","text":"<p>Don't reload full tables every time:</p> <pre><code>read:\n  connection: source_db\n  table: orders\n  incremental:\n    mode: stateful\n    column: updated_at\n    watermark_lag: \"1d\"\n</code></pre>"},{"location":"guides/best_practices/#skip-unchanged-data","title":"Skip Unchanged Data","text":"<p>For dimension tables that rarely change:</p> <pre><code>write:\n  mode: append\n  skip_if_unchanged: true\n  skip_hash_columns: [id]\n</code></pre>"},{"location":"guides/best_practices/#optimize-delta-writes-spark","title":"Optimize Delta Writes (Spark)","text":"<pre><code>write:\n  format: delta\n  options:\n    optimize_write: true\n    cluster_by: [date, region]\n</code></pre>"},{"location":"guides/best_practices/#skip-null-profiling-for-large-tables","title":"Skip Null Profiling for Large Tables","text":"<pre><code>performance:\n  skip_null_profiling: true  # Faster for very large DataFrames\n</code></pre>"},{"location":"guides/best_practices/#7-data-quality-validation","title":"7. Data Quality &amp; Validation","text":""},{"location":"guides/best_practices/#validation-strategy-overview","title":"Validation Strategy Overview","text":"<p>Odibi provides three validation mechanisms for different use cases:</p> Mechanism When Executed Purpose On Failure Contracts Before processing Input validation Always stops pipeline Validation After transformation Output checks Configurable (warn/error) Gates Before write Critical path checks Blocks downstream nodes"},{"location":"guides/best_practices/#use-contracts-for-input-validation","title":"Use Contracts for Input Validation","text":"<p>Fail fast if source data is bad:</p> <pre><code>- name: process_orders\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read: ...\n  transform: ...\n</code></pre>"},{"location":"guides/best_practices/#use-validation-for-output-checks","title":"Use Validation for Output Checks","text":"<p>Warn (or fail) if output doesn't meet expectations:</p> <pre><code>- name: daily_revenue\n  transform: ...\n  validation:\n    tests:\n      - type: not_null\n        columns: [date, revenue]\n      - type: unique\n        columns: [date]\n      - type: range\n        column: revenue\n        min: 0\n    on_failure: warn  # or \"error\" to fail the pipeline\n</code></pre>"},{"location":"guides/best_practices/#available-validation-types","title":"Available Validation Types","text":"Type Description Example <code>not_null</code> Check for null values <code>columns: [id, name]</code> <code>unique</code> Check for duplicates <code>columns: [id]</code> <code>row_count</code> Validate row counts <code>min: 100, max: 1000000</code> <code>freshness</code> Check data recency <code>column: updated_at, max_age: \"24h\"</code> <code>range</code> Numeric bounds <code>column: amount, min: 0, max: 10000</code> <code>regex</code> Pattern matching <code>column: email, pattern: \"^.+@.+$\"</code> <code>referential</code> FK validation <code>column: customer_id, reference: dim_customer.id</code> <code>custom</code> Custom Python function <code>function: my_validation_func</code>"},{"location":"guides/best_practices/#use-quality-gates-for-critical-paths","title":"Use Quality Gates for Critical Paths","text":"<pre><code>- name: load_orders\n  gate:\n    - type: row_count\n      min: 1000\n      on_failure: block  # Stops pipeline if &lt; 1000 rows\n</code></pre>"},{"location":"guides/best_practices/#fk-validation-for-fact-tables","title":"FK Validation for Fact Tables","text":"<p>Ensure referential integrity before loading fact tables:</p> <pre><code>- name: fact_orders\n  depends_on: [dim_customer, dim_product]\n  read:\n    connection: staging\n    path: orders\n  validation:\n    tests:\n      - type: referential\n        column: customer_id\n        reference: dim_customer.customer_id\n        on_orphan: warn\n      - type: referential\n        column: product_id\n        reference: dim_product.product_id\n        on_orphan: filter  # Remove orphan rows\n  write:\n    connection: warehouse\n    path: fact_orders\n</code></pre>"},{"location":"guides/best_practices/#custom-validation-functions","title":"Custom Validation Functions","text":"<p>Register custom validation logic:</p> <pre><code>from odibi import transform\n\n@transform(\"validate_business_rules\")\ndef validate_business_rules(context, current):\n    \"\"\"Custom business rule validation.\"\"\"\n    errors = []\n\n    # Rule 1: Order amount must match line items\n    mismatched = current[current['total'] != current['line_items_sum']]\n    if len(mismatched) &gt; 0:\n        errors.append(f\"{len(mismatched)} orders with mismatched totals\")\n\n    # Rule 2: Future dates not allowed\n    future_orders = current[current['order_date'] &gt; pd.Timestamp.now()]\n    if len(future_orders) &gt; 0:\n        errors.append(f\"{len(future_orders)} orders with future dates\")\n\n    if errors:\n        context.log_warning(f\"Validation issues: {'; '.join(errors)}\")\n\n    return current\n</code></pre> <p>Use in YAML: <pre><code>transform:\n  steps:\n    - function: validate_business_rules\n</code></pre></p>"},{"location":"guides/best_practices/#quarantine-bad-records","title":"Quarantine Bad Records","text":"<p>Separate bad data for review instead of failing:</p> <pre><code>- name: process_orders\n  validation:\n    tests:\n      - type: not_null\n        columns: [order_id, amount]\n    on_failure: quarantine\n    quarantine:\n      connection: warehouse\n      path: quarantine/orders\n      include_reason: true  # Adds _quarantine_reason column\n</code></pre>"},{"location":"guides/best_practices/#8-cross-pipeline-dependencies","title":"8. Cross-Pipeline Dependencies","text":""},{"location":"guides/best_practices/#use-pipelinenode-references","title":"Use <code>$pipeline.node</code> References","text":"<p>When silver needs bronze outputs:</p> <pre><code># pipelines/silver/transform_silver.yaml\npipelines:\n  - pipeline: transform_silver\n    nodes:\n      - name: enriched_orders\n        inputs:\n          orders: $read_bronze.orders           # Cross-pipeline reference\n          customers: $read_bronze.customers\n        transform:\n          steps:\n            - operation: join\n              left: orders\n              right: customers\n              on: [customer_id]\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"silver/enriched_orders\"\n</code></pre>"},{"location":"guides/best_practices/#run-pipelines-in-order","title":"Run Pipelines in Order","text":"<pre><code># Bronze first\nodibi run project.yaml --pipeline read_bronze\n\n# Then silver (references bronze outputs)\nodibi run project.yaml --pipeline transform_silver\n</code></pre>"},{"location":"guides/best_practices/#best-practices-for-references","title":"Best Practices for References","text":"<ol> <li>Always use <code>path:</code> in write config \u2014 ensures cross-engine compatibility</li> <li>Run source pipeline first \u2014 references require catalog entries</li> <li>Use meaningful node names \u2014 <code>$read_bronze.orders</code> is clearer than <code>$p1.n1</code></li> </ol>"},{"location":"guides/best_practices/#9-security","title":"9. Security","text":""},{"location":"guides/best_practices/#mask-sensitive-columns-in-stories","title":"Mask Sensitive Columns in Stories","text":"<pre><code>- name: process_users\n  sensitive: [email, ssn, phone]  # Masked in Data Stories\n</code></pre>"},{"location":"guides/best_practices/#full-node-masking-for-pii-heavy-nodes","title":"Full Node Masking for PII-Heavy Nodes","text":"<pre><code>- name: medical_records\n  sensitive: true  # Entire sample redacted\n</code></pre>"},{"location":"guides/best_practices/#use-key-vault-in-production","title":"Use Key Vault in Production","text":"<pre><code>connections:\n  lakehouse:\n    auth:\n      mode: key_vault\n      key_vault: my-key-vault\n      secret: storage-account-key\n</code></pre>"},{"location":"guides/best_practices/#never-log-secrets","title":"Never Log Secrets","text":"<p>Odibi automatically redacts values that look like secrets, but be careful in custom transformations:</p> <pre><code>@transform\ndef my_transform(context, params):\n    # \u274c NEVER do this\n    print(f\"Using password: {params['password']}\")\n\n    # \u2705 Do this instead\n    logger.info(\"Connecting to database...\")\n</code></pre>"},{"location":"guides/best_practices/#10-version-control","title":"10. Version Control","text":""},{"location":"guides/best_practices/#git-ignore-list","title":"Git Ignore List","text":"<pre><code># .gitignore\n.env\n*.pyc\n__pycache__/\n.odibi/\nstories/\n*.log\n.venv/\n</code></pre>"},{"location":"guides/best_practices/#commit-guidelines","title":"Commit Guidelines","text":"Change Type Commit Message New pipeline <code>feat(bronze): add customer ingestion pipeline</code> New node <code>feat(silver): add order enrichment node</code> Bug fix <code>fix(gold): correct revenue calculation</code> Config change <code>chore: update retry settings</code>"},{"location":"guides/best_practices/#branch-strategy","title":"Branch Strategy","text":"<pre><code>main           # Production-ready pipelines\n\u251c\u2500\u2500 develop    # Integration branch\n\u251c\u2500\u2500 feature/*  # New pipelines/nodes\n\u2514\u2500\u2500 fix/*      # Bug fixes\n</code></pre>"},{"location":"guides/best_practices/#pr-checklist","title":"PR Checklist","text":"<ul> <li>[ ] Pipeline runs locally without errors</li> <li>[ ] Node descriptions added</li> <li>[ ] Sensitive columns marked</li> <li>[ ] Incremental config for large tables</li> <li>[ ] Tests pass</li> </ul>"},{"location":"guides/best_practices/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/best_practices/#project-organization-cheat-sheet","title":"Project Organization Cheat Sheet","text":"<pre><code>project.yaml          \u2192 Connections, settings, imports (NO pipelines)\npipelines/{layer}/    \u2192 One YAML per pipeline\ntransformations/      \u2192 Custom Python code\n.env                  \u2192 Local secrets (git-ignored)\n</code></pre>"},{"location":"guides/best_practices/#node-checklist","title":"Node Checklist","text":"<ul> <li>[ ] Descriptive name (<code>clean_orders</code> not <code>node_1</code>)</li> <li>[ ] Description explaining purpose</li> <li>[ ] Tags for filtering (<code>daily</code>, <code>critical</code>)</li> <li>[ ] <code>cache: true</code> if used by multiple nodes</li> <li>[ ] <code>sensitive</code> for PII columns</li> <li>[ ] Incremental config for large tables</li> </ul>"},{"location":"guides/best_practices/#performance-checklist","title":"Performance Checklist","text":"<ul> <li>[ ] <code>use_arrow: true</code> for Pandas</li> <li>[ ] Incremental loading for large sources</li> <li>[ ] <code>skip_if_unchanged</code> for dimensions</li> <li>[ ] <code>skip_null_profiling</code> for very large tables</li> <li>[ ] <code>cluster_by</code> for Spark/Delta</li> </ul>"},{"location":"guides/best_practices/#related-documentation","title":"Related Documentation","text":"<ul> <li>The Definitive Guide \u2014 Deep dive into architecture</li> <li>Performance Tuning \u2014 Optimization details</li> <li>Production Deployment \u2014 Going to production</li> <li>Cross-Pipeline Dependencies \u2014 <code>$pipeline.node</code> references</li> <li>YAML Schema Reference \u2014 Full configuration options</li> <li>Validation Overview \u2014 Data quality framework</li> <li>Glossary \u2014 Terminology reference</li> </ul>"},{"location":"guides/catalog_sync/","title":"Catalog Sync Guide","text":"<p>Sync your system catalog to SQL Server for dashboards, cross-environment visibility, and easy SQL queries.</p>"},{"location":"guides/catalog_sync/#why-sync","title":"Why Sync?","text":"<p>The Odibi System Catalog stores metadata in Delta tables, which provide ACID transactions, time travel, and schema evolution. However, querying Delta tables requires Spark or a compatible engine.</p> <p>Catalog Sync replicates this data to SQL Server (or another blob storage), giving you:</p> <ul> <li>SQL Access: Query run history with plain SQL</li> <li>Power BI: Build dashboards without Spark</li> <li>Cross-Environment Visibility: See dev/qat/prod in one place</li> <li>Analyst Friendly: Share observability with the team</li> </ul>"},{"location":"guides/catalog_sync/#architecture","title":"Architecture","text":"<pre><code>Pipeline Execution\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PRIMARY: Delta Tables                \u2502\n\u2502  \u2022 Source of truth                    \u2502\n\u2502  \u2022 ACID, time travel, schema evolution\u2502\n\u2502  \u2022 Connection: system.connection      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n        \u2502  sync_to (automatic after run)\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SECONDARY: SQL Server                \u2502\n\u2502  \u2022 Query with SQL                     \u2502\n\u2502  \u2022 Power BI dashboards                \u2502\n\u2502  \u2022 Cross-environment views            \u2502\n\u2502  \u2022 Connection: system.sync_to         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/catalog_sync/#quick-setup","title":"Quick Setup","text":"<p>Add <code>sync_to</code> to your system configuration:</p> <pre><code>system:\n  connection: adls_prod           # Primary - blob storage (Delta tables)\n  environment: prod\n  sync_to:\n    connection: sql_server_prod   # Secondary - SQL Server\n    schema_name: odibi_system\n</code></pre> <p>That's it! After each pipeline run, the catalog will automatically sync to SQL Server.</p>"},{"location":"guides/catalog_sync/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/catalog_sync/#full-example","title":"Full Example","text":"<pre><code>system:\n  connection: adls_prod           # Primary - must be blob/local storage\n  path: _odibi_system             # Default path (optional)\n  environment: prod\n\n  sync_to:\n    connection: sql_server_prod   # Target connection\n    schema_name: odibi_system     # SQL Server schema\n    mode: incremental             # incremental | full\n    on: after_run                 # after_run | manual\n    async_sync: true              # Don't block pipeline\n    tables:                       # Optional: specific tables\n      - meta_runs\n      - meta_pipeline_runs\n      - meta_node_runs\n      - meta_failures\n    sync_last_days: 90            # For large tables\n</code></pre>"},{"location":"guides/catalog_sync/#options-reference","title":"Options Reference","text":"Option Default Description <code>connection</code> required Target connection name <code>schema_name</code> <code>odibi_system</code> SQL Server schema <code>path</code> <code>_odibi_system</code> Path for blob targets <code>mode</code> <code>incremental</code> <code>incremental</code> or <code>full</code> <code>on</code> <code>after_run</code> <code>after_run</code> or <code>manual</code> <code>async_sync</code> <code>true</code> Run sync in background <code>tables</code> (defaults) Tables to sync <code>sync_last_days</code> <code>null</code> Limit to recent data"},{"location":"guides/catalog_sync/#default-tables","title":"Default Tables","text":"<p>When <code>tables</code> is not specified, these high-priority tables are synced:</p> <ul> <li><code>meta_runs</code> - Node-level run history</li> <li><code>meta_pipeline_runs</code> - Pipeline summaries</li> <li><code>meta_node_runs</code> - Detailed node runs</li> <li><code>meta_tables</code> - Asset registry</li> <li><code>meta_failures</code> - Failure records</li> </ul>"},{"location":"guides/catalog_sync/#sync-to-another-blob-storage","title":"Sync to Another Blob Storage","text":"<p>You can also sync Delta tables to another blob storage for cross-region replication or backup:</p> <pre><code>system:\n  connection: adls_us_east\n  sync_to:\n    connection: adls_us_west      # Another blob connection\n    path: _odibi_system_replica\n    mode: incremental\n</code></pre> <p>This creates a Delta-to-Delta replica with all original features preserved.</p>"},{"location":"guides/catalog_sync/#cli-commands","title":"CLI Commands","text":""},{"location":"guides/catalog_sync/#manual-sync","title":"Manual Sync","text":"<pre><code># Sync all configured tables\nodibi catalog sync config.yaml\n\n# Sync specific tables\nodibi catalog sync config.yaml --tables meta_runs,meta_failures\n\n# Full sync (replace all data)\nodibi catalog sync config.yaml --mode full\n\n# Dry run - see what would sync\nodibi catalog sync config.yaml --dry-run\n</code></pre>"},{"location":"guides/catalog_sync/#check-sync-status","title":"Check Sync Status","text":"<pre><code>odibi catalog sync-status config.yaml\n</code></pre> <p>Output: <pre><code>=== Catalog Sync Status ===\n\nPrimary: goat_prod_dm\nPath: _odibi_system\n\nSync Target: goat_qat\nMode: incremental\nTrigger: after_run\nAsync: True\nTables: (default high-priority tables)\n\n--- Last Sync Timestamps ---\n  meta_runs: 2026-01-11T16:59:50+00:00\n  meta_pipeline_runs: 2026-01-11T16:59:50+00:00\n  meta_node_runs: 2026-01-11T16:59:50+00:00\n  meta_tables: never synced\n  meta_failures: never synced\n</code></pre></p>"},{"location":"guides/catalog_sync/#sql-server-tables","title":"SQL Server Tables","text":"<p>Odibi automatically creates tables in SQL Server with appropriate schemas:</p> <pre><code>-- Query recent runs\nSELECT pipeline_name, node_name, status, timestamp\nFROM odibi_system.meta_runs\nWHERE date &gt;= DATEADD(day, -7, GETDATE())\nORDER BY timestamp DESC;\n\n-- Get pipeline success rates\nSELECT\n    pipeline_name,\n    COUNT(*) as total_runs,\n    SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as success_rate\nFROM odibi_system.meta_pipeline_runs\nWHERE date &gt;= DATEADD(day, -30, GETDATE())\nGROUP BY pipeline_name;\n</code></pre>"},{"location":"guides/catalog_sync/#error-handling","title":"Error Handling","text":"<p>Sync failures never fail your pipeline. If sync fails:</p> <ol> <li>A warning is logged</li> <li>The pipeline continues successfully</li> <li>You can manually retry with <code>odibi catalog sync</code></li> </ol> <pre><code>[WARNING] Catalog sync failed (non-fatal): Connection timeout\n          Suggestion: Run 'odibi catalog sync' manually to retry\n</code></pre>"},{"location":"guides/catalog_sync/#migrating-from-sql-server-primary","title":"Migrating from SQL Server Primary","text":"<p>If you previously had SQL Server as your primary system connection, migrate to this pattern:</p> <p>Before (broken): <pre><code>system:\n  connection: sql_server_prod     # \u274c Causes path error\n  schema_name: odibi_system\n</code></pre></p> <p>After (works): <pre><code>system:\n  connection: blob_prod           # \u2713 Primary - Delta tables\n  environment: prod\n  sync_to:\n    connection: sql_server_prod   # \u2713 Secondary - SQL visibility\n    schema_name: odibi_system\n</code></pre></p>"},{"location":"guides/catalog_sync/#best-practices","title":"Best Practices","text":"<ol> <li>Use incremental mode for regular syncs to minimize load</li> <li>Set <code>sync_last_days</code> for large tables to avoid syncing years of history</li> <li>Use <code>async_sync: true</code> (default) to not slow down pipelines</li> <li>Monitor with <code>sync-status</code> to ensure syncs are working</li> <li>Run <code>--mode full</code> occasionally to catch any missed records</li> </ol>"},{"location":"guides/cli_master_guide/","title":"Odibi CLI: Zero to Hero","text":"<p>Ultimate Cheatsheet &amp; Reference (v2.4.0)</p> <p>The Command Line Interface (CLI) is your primary tool for managing Odibi projects.</p>"},{"location":"guides/cli_master_guide/#level-1-the-basics","title":"\ud83d\udfe2 Level 1: The Basics","text":""},{"location":"guides/cli_master_guide/#1-create-a-new-pipeline-file","title":"1. Create a New Pipeline File","text":"<p>Generate a \"Master Kitchen Sink\" reference file with all features enabled. <pre><code>odibi create my_pipeline.yaml\n</code></pre></p>"},{"location":"guides/cli_master_guide/#2-run-a-pipeline","title":"2. Run a Pipeline","text":"<p>Execute the pipeline defined in your YAML file. <pre><code>odibi run my_pipeline.yaml\n</code></pre></p> <p>Common Flags: *   <code>--dry-run</code>: Simulate execution (don't write data). *   <code>--resume</code>: Resume from the last failure (skips successful nodes). *   <code>--env prod</code>: Load production environment variables.</p>"},{"location":"guides/cli_master_guide/#level-2-intermediate-management","title":"\ud83d\udfe1 Level 2: Intermediate (Management)","text":""},{"location":"guides/cli_master_guide/#1-initialize-a-full-project","title":"1. Initialize a Full Project","text":"<p>Don't just create a file; create a full folder structure with best practices (Bronze/Silver/Gold layers). <pre><code># Creates folder 'my_project' with organized subfolders\nodibi init-pipeline my_project --template kitchen-sink\n</code></pre></p>"},{"location":"guides/cli_master_guide/#2-validate-configuration","title":"2. Validate Configuration","text":"<p>Check if your YAML is valid before running it. <pre><code>odibi validate my_pipeline.yaml\n</code></pre></p>"},{"location":"guides/cli_master_guide/#3-visualize-dependencies","title":"3. Visualize Dependencies","text":"<p>Generate a dependency graph to understand flow. <pre><code># ASCII Art (Default)\nodibi graph my_pipeline.yaml\n\n# Mermaid Diagram (for Markdown)\nodibi graph my_pipeline.yaml --format mermaid\n</code></pre></p>"},{"location":"guides/cli_master_guide/#level-3-hero-advanced-tools","title":"\ud83d\udd34 Level 3: Hero (Advanced Tools)","text":""},{"location":"guides/cli_master_guide/#1-deep-diff-compare-runs","title":"1. Deep Diff (Compare Runs)","text":"<p>Did a pipeline run suddenly output fewer rows? Use <code>story diff</code> to compare two runs. <pre><code># List available runs\nodibi story list\n\n# Compare two story JSON files\nodibi story diff stories/runs/20231027_120000.json stories/runs/20231027_120500.json\n</code></pre> Output: Shows execution time differences, row count changes, and success rates.</p>"},{"location":"guides/cli_master_guide/#2-manage-secrets","title":"2. Manage Secrets","text":"<p>Securely manage local secrets for your pipelines. <pre><code># Initialize secrets store (creates .env.template)\nodibi secrets init\n\n# Validate all secrets are configured\nodibi secrets validate\n</code></pre></p>"},{"location":"guides/cli_master_guide/#level-4-system-catalog-the-brain","title":"\ud83e\udde0 Level 4: System Catalog (The Brain)","text":""},{"location":"guides/cli_master_guide/#query-the-system-catalog","title":"Query the System Catalog","text":"<p>The System Catalog stores metadata about all your runs, pipelines, nodes, and state. Query it without manually reading Delta tables.</p> <pre><code># List recent runs\nodibi catalog runs config.yaml\n\n# Filter by pipeline and status\nodibi catalog runs config.yaml --pipeline my_etl --status SUCCESS --days 14\n\n# List registered pipelines\nodibi catalog pipelines config.yaml\n\n# List nodes (optionally filter by pipeline)\nodibi catalog nodes config.yaml --pipeline my_etl\n\n# View HWM state checkpoints\nodibi catalog state config.yaml\n\n# Get execution statistics\nodibi catalog stats config.yaml --days 30\n</code></pre> <p>Catalog Subcommands: | Subcommand | Description | | :--- | :--- | | <code>runs</code> | List execution runs from <code>meta_runs</code> | | <code>pipelines</code> | List registered pipelines from <code>meta_pipelines</code> | | <code>nodes</code> | List registered nodes from <code>meta_nodes</code> | | <code>state</code> | List HWM state checkpoints from <code>meta_state</code> | | <code>tables</code> | List registered assets from <code>meta_tables</code> | | <code>metrics</code> | List metrics definitions from <code>meta_metrics</code> | | <code>patterns</code> | List pattern compliance from <code>meta_patterns</code> | | <code>stats</code> | Show execution statistics (success rate, avg duration, etc.) |</p> <p>Common Flags: * <code>--format json</code>: Output as JSON instead of ASCII table * <code>--pipeline &lt;name&gt;</code>: Filter by pipeline name * <code>--days &lt;n&gt;</code>: Show data from last N days (default: 7) * <code>--limit &lt;n&gt;</code>: Limit number of results (default: 20)</p>"},{"location":"guides/cli_master_guide/#level-5-schema-lineage-tracking","title":"\ud83d\udd0d Level 5: Schema &amp; Lineage Tracking","text":""},{"location":"guides/cli_master_guide/#schema-version-history","title":"Schema Version History","text":"<p>Track how table schemas evolve over time.</p> <pre><code># View schema history for a table\nodibi schema history silver/customers --config config.yaml\n\n# Compare two schema versions\nodibi schema diff silver/customers --config config.yaml --from-version 3 --to-version 5\n\n# Output as JSON\nodibi schema history silver/customers --config config.yaml --format json\n</code></pre> <p>Example Output: <pre><code>Schema History: silver/customers\n================================================================================\nVersion    Captured At            Changes\n--------------------------------------------------------------------------------\nv5         2024-01-30 10:15:00    +loyalty_tier\nv4         2024-01-15 08:30:00    ~email (VARCHAR\u2192STRING)\nv3         2024-01-01 12:00:00    -legacy_id\nv2         2023-12-15 09:00:00    +created_at, +updated_at\nv1         2023-12-01 10:00:00    Initial schema (12 columns)\n</code></pre></p>"},{"location":"guides/cli_master_guide/#cross-pipeline-lineage","title":"Cross-Pipeline Lineage","text":"<p>Trace data dependencies across pipelines.</p> <pre><code># Trace upstream sources\nodibi lineage upstream gold/customer_360 --config config.yaml\n\n# Trace downstream consumers\nodibi lineage downstream bronze/customers_raw --config config.yaml\n\n# Impact analysis - what would be affected by changes?\nodibi lineage impact bronze/customers_raw --config config.yaml\n</code></pre> <p>Example Output (upstream): <pre><code>Upstream Lineage: gold/customer_360\n============================================================\ngold/customer_360\n\u2514\u2500\u2500 silver/dim_customers (silver_pipeline.process_customers)\n    \u2514\u2500\u2500 bronze/customers_raw (bronze_pipeline.ingest_customers)\n</code></pre></p> <p>Example Output (impact): <pre><code>\u26a0\ufe0f  Impact Analysis: bronze/customers_raw\n============================================================\n\nChanges to bronze/customers_raw would affect:\n\n  Affected Tables:\n    - silver/dim_customers (pipeline: silver_pipeline)\n    - gold/customer_360 (pipeline: gold_pipeline)\n    - gold/churn_features (pipeline: ml_pipeline)\n\n  Summary:\n    Total: 3 downstream table(s) in 2 pipeline(s)\n</code></pre></p> <p>Schema Subcommands: | Subcommand | Description | | :--- | :--- | | <code>history</code> | View schema version history for a table | | <code>diff</code> | Compare two schema versions |</p> <p>Lineage Subcommands: | Subcommand | Description | | :--- | :--- | | <code>upstream</code> | Trace upstream sources of a table | | <code>downstream</code> | Trace downstream consumers of a table | | <code>impact</code> | Impact analysis for schema changes |</p> <p>Common Flags: * <code>--config &lt;path&gt;</code>: Path to YAML config file (required) * <code>--depth &lt;n&gt;</code>: Maximum depth to traverse (default: 3) * <code>--format json</code>: Output as JSON * <code>--limit &lt;n&gt;</code>: Limit results (schema history only)</p>"},{"location":"guides/cli_master_guide/#level-5-ai-friendly-introspection","title":"\ud83e\udd16 Level 5: AI-Friendly Introspection","text":"<p>These commands help AI tools (and developers) discover available features programmatically.</p>"},{"location":"guides/cli_master_guide/#list-available-features","title":"List Available Features","text":"<pre><code># List all 52+ transformers\nodibi list transformers\n\n# List all 6 patterns with descriptions\nodibi list patterns\n\n# List all connection types\nodibi list connections\n\n# JSON output (for AI parsing)\nodibi list transformers --format json\n</code></pre>"},{"location":"guides/cli_master_guide/#explain-any-feature","title":"Explain Any Feature","text":"<pre><code># Get detailed docs for a transformer\nodibi explain fill_nulls\n\n# Get detailed docs for a pattern (includes example YAML)\nodibi explain dimension\n\n# Get detailed docs for a connection type\nodibi explain azure_sql\n</code></pre> <p>AI Workflow Example: <pre><code># AI checks what's available\nodibi list transformers --format json | jq '.[] | .name'\n\n# AI looks up specific usage\nodibi explain derive_columns\n\n# AI validates generated config\nodibi validate generated_pipeline.yaml\n</code></pre></p>"},{"location":"guides/cli_master_guide/#command-reference","title":"\ud83d\udcc4 Command Reference","text":"Command Description <code>run</code> Execute a pipeline. <code>create</code> Create a single YAML config file. <code>init-pipeline</code> Scaffold a full project directory. <code>validate</code> Check YAML syntax and logic. <code>graph</code> Visualize pipeline dependencies. <code>story</code> Manage and compare execution reports (<code>generate</code>, <code>diff</code>, <code>list</code>). <code>secrets</code> Manage local secure secrets (<code>init</code>, <code>validate</code>). <code>catalog</code> Query System Catalog (<code>runs</code>, <code>pipelines</code>, <code>nodes</code>, <code>state</code>, <code>stats</code>). <code>schema</code> Schema version tracking (<code>history</code>, <code>diff</code>). <code>lineage</code> Cross-pipeline lineage (<code>upstream</code>, <code>downstream</code>, <code>impact</code>). <code>list</code> List available features (<code>transformers</code>, <code>patterns</code>, <code>connections</code>). <code>explain</code> Get detailed documentation for any feature. <code>init-vscode</code> Setup VS Code environment."},{"location":"guides/decision_guide/","title":"Decision Guide","text":"<p>Rules of thumb for common Odibi decisions.</p>"},{"location":"guides/decision_guide/#engine-choice","title":"Engine Choice","text":"Scenario Engine Why Local dev, files &lt; 1GB <code>pandas</code> Fast startup, no dependencies Local dev, files 1-10GB <code>polars</code> Faster than Pandas, lazy eval Production, Delta Lake, &gt; 10GB <code>spark</code> Distributed, Delta support Databricks <code>spark</code> Native integration <p>Rule: Start with <code>pandas</code> locally, switch to <code>spark</code> for production.</p>"},{"location":"guides/decision_guide/#validation-contracts-vs-tests","title":"Validation: Contracts vs Tests","text":"Use... When... Behavior <code>contracts:</code> Checking source data Runs before read, fail-fast <code>validation.tests:</code> Checking output data Runs after transform, configurable <p>Rule: Use contracts for freshness/schema/volume. Use tests for row-level quality.</p> <pre><code># Contracts: Source quality (fail-fast)\ncontracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"\n\n# Tests: Output quality (with gates)\nvalidation:\n  tests:\n    - type: not_null\n      columns: [id, name]\n  gate:\n    on_failure: warn  # or 'fail'\n</code></pre>"},{"location":"guides/decision_guide/#when-to-use-quality-gates","title":"When to Use Quality Gates","text":"Scenario Gate Setting Development/testing <code>on_failure: warn</code> Production, non-critical <code>on_failure: warn</code> + alerting Production, critical data <code>on_failure: fail</code> Regulatory/compliance <code>on_failure: fail</code> + quarantine <p>Rule: Start with <code>warn</code>, tighten to <code>fail</code> as you trust the data.</p>"},{"location":"guides/decision_guide/#when-to-enable-alerting","title":"When to Enable Alerting","text":"Scenario Alert Local dev No alerts Scheduled production jobs <code>on_failure</code> Critical SLA pipelines <code>on_failure</code> + <code>on_quality_gate_fail</code> All runs (audit trail) <code>on_success</code> + <code>on_failure</code> <p>Rule: Enable alerting when someone needs to act on failure.</p> <pre><code>alerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK}\n    on_events: [on_failure, on_quality_gate_fail]\n</code></pre>"},{"location":"guides/decision_guide/#bronze-vs-silver-vs-gold-logic","title":"Bronze vs Silver vs Gold Logic","text":"Logic Type Layer Why Ingestion, format conversion Bronze Raw preservation Deduplication, cleaning Silver Single source of truth Business transforms, aggregation Gold Consumption-ready <p>Rule: If it changes the semantic meaning, it belongs in Gold.</p>"},{"location":"guides/decision_guide/#incremental-mode-selection","title":"Incremental Mode Selection","text":"<pre><code>Has reliable timestamp column?\n\u251c\u2500\u25ba Yes\n\u2502   \u2514\u2500\u25ba Need exact row tracking? \u2192 mode: stateful\n\u2502   \u2514\u2500\u25ba OK with overlap? \u2192 mode: rolling_window\n\u2514\u2500\u25ba No\n    \u2514\u2500\u25ba Data is immutable? \u2192 mode: append\n    \u2514\u2500\u25ba Data can change? \u2192 skip_if_unchanged: true\n</code></pre> Mode State Use Case <code>stateful</code> Persisted HWM CDC, database extraction <code>rolling_window</code> Lookback period Event logs, files with dates <code>append</code> None Immutable streams"},{"location":"guides/decision_guide/#scd-type-selection","title":"SCD Type Selection","text":"Need history? Changes often? Recommendation No - SCD Type 1 (overwrite) Yes Slowly (&lt; 1/day) SCD Type 2 (versioned) Yes Frequently Daily snapshots instead <p>Rule: SCD2 is for slowly changing dimensions. Fast changes = snapshot approach.</p>"},{"location":"guides/decision_guide/#merge-vs-overwrite-write-mode","title":"Merge vs Overwrite Write Mode","text":"Scenario Mode Why First load / full refresh <code>overwrite</code> Clean slate Incremental updates <code>merge</code> Preserve existing, update keys Append-only events <code>append</code> Immutable, no updates SCD2 result <code>overwrite</code> Transformer returns full history Aggregations <code>overwrite</code> Idempotent recalculation <p>Rule: When in doubt, use <code>overwrite</code> for transforms and <code>append</code> for raw.</p>"},{"location":"guides/decision_guide/#retry-configuration","title":"Retry Configuration","text":"Scenario Retry Config Transient network issues <code>max_attempts: 3</code> Database locks <code>backoff: exponential</code> Stable local files <code>enabled: false</code> <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n</code></pre>"},{"location":"guides/decision_guide/#when-to-use-quarantine","title":"When to Use Quarantine","text":"Scenario Quarantine? Dev/testing No (just fail or warn) Data with expected bad rows Yes (review later) Strict quality required Yes (fail + quarantine for audit) High-volume, low-quality tolerance Yes (separate good from bad) <p>Rule: Quarantine when you can't afford data loss but also can't accept bad data.</p>"},{"location":"guides/decision_guide/#file-format-selection","title":"File Format Selection","text":"Format When to Use <code>csv</code> Source files, human-readable <code>parquet</code> Local analytics, single-machine <code>delta</code> Production, ACID, time travel <code>json</code> API responses, nested data <p>Rule: Use Delta for anything that needs reliability or history.</p>"},{"location":"guides/decision_guide/#quick-checklist-production-ready","title":"Quick Checklist: Production Ready?","text":"<pre><code>[ ] Engine set to 'spark' (or appropriate for scale)\n[ ] Contracts on source nodes (freshness, row_count)\n[ ] Validation tests on transform outputs\n[ ] Quality gates configured\n[ ] Alerting enabled\n[ ] Retry configured\n[ ] Stories enabled for audit trail\n[ ] Environment variables for secrets\n</code></pre>"},{"location":"guides/decision_guide/#see-also","title":"See Also","text":"<ul> <li>THE_REFERENCE.md \u2014 All patterns in one runnable example</li> <li>Playbook \u2014 Problem \u2192 solution lookup</li> <li>Best Practices \u2014 Code organization</li> <li>Production Deployment \u2014 Going live</li> </ul>"},{"location":"guides/dimensional_modeling_guide/","title":"Dimensional Modeling Guide for ODIBI","text":"<p>A practical reference for building data warehouses with ODIBI.</p>"},{"location":"guides/dimensional_modeling_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Problem We're Solving</li> <li>Facts and Dimensions</li> <li>The Star Schema</li> <li>Natural Keys vs Surrogate Keys</li> <li>Bronze \u2192 Silver \u2192 Gold Flow</li> <li>Where Do IDs Come From?</li> <li>Lookup Tables vs Dimension Tables</li> <li>Slowly Changing Dimensions (SCD)</li> <li>The Date Dimension</li> <li>Aggregations</li> <li>Common Mistakes to Avoid</li> <li>ODIBI Current State vs Target</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#the-problem-were-solving","title":"The Problem We're Solving","text":""},{"location":"guides/dimensional_modeling_guide/#raw-data-is-hard-to-query","title":"Raw Data is Hard to Query","text":"<p>Your source systems store data for operations, not analysis:</p> <pre><code>orders.csv\n| customer_email    | product_name | quantity | price | timestamp           |\n|-------------------|--------------|----------|-------|---------------------|\n| john@mail.com     | Latte        | 2        | 11.00 | 2024-01-15 09:15:00 |\n</code></pre> <p>To answer \"What was revenue by product category for Q4 weekends?\", you need: - Product category (not in orders) - Whether it's a weekend (calculated from timestamp) - Q4 filter (calculated from timestamp)</p> <p>Dimensional modeling pre-calculates and organizes this context.</p>"},{"location":"guides/dimensional_modeling_guide/#facts-and-dimensions","title":"Facts and Dimensions","text":""},{"location":"guides/dimensional_modeling_guide/#fact-table-what-happened","title":"Fact Table = What Happened","text":"<p>Events, transactions, measurements. Numbers you can add/count/average.</p> Contains Example Measures (numbers) <code>quantity</code>, <code>price</code>, <code>total_amount</code> Foreign keys (pointers to dimensions) <code>customer_sk</code>, <code>product_sk</code>, <code>date_sk</code> Degenerate dimensions (IDs with no table) <code>order_id</code>, <code>invoice_number</code> <p>Key insight: Facts are usually append-only. Once it happened, it happened.</p>"},{"location":"guides/dimensional_modeling_guide/#dimension-table-context-about-what-happened","title":"Dimension Table = Context About What Happened","text":"<p>Who, what, when, where, why. Descriptive attributes.</p> Contains Example Primary key <code>customer_sk</code> Natural key <code>customer_id</code> (from source system) Descriptive attributes <code>name</code>, <code>email</code>, <code>city</code>, <code>segment</code> Hierarchies <code>city</code> \u2192 <code>state</code> \u2192 <code>country</code> <p>Key insight: Dimensions change over time. A customer might move cities.</p>"},{"location":"guides/dimensional_modeling_guide/#the-star-schema","title":"The Star Schema","text":"<pre><code>                    dim_customer\n                         \u2502\n                         \u2502 customer_sk\n                         \u2502\ndim_product \u2500\u2500\u2500\u2500\u2500\u2500\u2500 fact_orders \u2500\u2500\u2500\u2500\u2500\u2500\u2500 dim_date\n         product_sk      \u2502        date_sk\n                         \u2502\n                    dim_region\n                         \u2502\n                    region_sk\n</code></pre> <p>Why this shape?</p> <ol> <li>Storage efficiency \u2014 Store \"John Smith, Premium, NYC\" once, reference by ID</li> <li>Query speed \u2014 Filter small dimension tables first, then join to facts</li> <li>Flexibility \u2014 Add new attributes to dimensions, all reports get them</li> <li>Single source of truth \u2014 Customer's segment defined in ONE place</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#natural-keys-vs-surrogate-keys","title":"Natural Keys vs Surrogate Keys","text":"Type What It Is Example Who Creates It Natural Key Business identifier <code>customer_id</code>, <code>email</code>, <code>product_sku</code> Source system or you (via hash) Surrogate Key Warehouse-generated integer <code>customer_sk = 1001</code> Data warehouse (auto-increment)"},{"location":"guides/dimensional_modeling_guide/#why-use-surrogate-keys","title":"Why Use Surrogate Keys?","text":"<ol> <li>Faster JOINs \u2014 Integers are faster than strings</li> <li>SCD2 support \u2014 One <code>customer_id</code> can have multiple <code>customer_sk</code> values (history)</li> <li>Survives source changes \u2014 If source system changes IDs, yours don't</li> <li>Unknown member \u2014 <code>customer_sk = 0</code> for orphan records</li> </ol>"},{"location":"guides/dimensional_modeling_guide/#the-unknown-member","title":"The \"Unknown\" Member","text":"<p>What if an order references <code>customer_id = 999</code> but that customer doesn't exist?</p> <p>Solution: Create a special row in every dimension:</p> customer_sk customer_id name city 0 -1 Unknown Unknown 1001 47 John NYC <p>Orphan facts JOIN to <code>customer_sk = 0</code> \u2014 you never lose data.</p>"},{"location":"guides/dimensional_modeling_guide/#bronze-silver-gold-flow","title":"Bronze \u2192 Silver \u2192 Gold Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           BRONZE                                     \u2502\n\u2502   Raw data, as-is from source. Messy, duplicates, nulls, no IDs.    \u2502\n\u2502   Example: raw_orders.csv, raw_customers.json                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           SILVER                                     \u2502\n\u2502   Cleaned, validated, deduplicated. NATURAL KEYS assigned.          \u2502\n\u2502   Example: clean_orders, clean_customers                             \u2502\n\u2502   Keys: customer_id (hash or from source), product_id, order_id     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            GOLD                                      \u2502\n\u2502   Dimensional model. SURROGATE KEYS, SCD history, aggregates.       \u2502\n\u2502   Example: dim_customer, dim_product, fact_orders, agg_daily_sales  \u2502\n\u2502   Keys: customer_sk, product_sk, date_sk (integers)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#key-point-silver-uses-natural-keys-gold-uses-surrogate-keys","title":"Key Point: Silver Uses Natural Keys, Gold Uses Surrogate Keys","text":"Layer Keys Used Bronze Whatever source provides (emails, names, raw IDs) Silver Natural keys \u2014 business identifiers (customer_id, product_id) Gold Surrogate keys \u2014 warehouse-generated integers (customer_sk)"},{"location":"guides/dimensional_modeling_guide/#where-do-ids-come-from","title":"Where Do IDs Come From?","text":""},{"location":"guides/dimensional_modeling_guide/#scenario-a-source-system-has-ids-common","title":"Scenario A: Source System Has IDs (Common)","text":"<p>Your CRM already has <code>customer_id = 47</code>. Just pass it through.</p> <pre><code>Source \u2192 Bronze \u2192 Silver \u2192 Gold\n  47       47       47      + customer_sk = 1001\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#scenario-b-source-system-has-no-ids-your-data","title":"Scenario B: Source System Has NO IDs (Your Data)","text":"<p>You only have <code>email</code> and <code>product_name</code>. You must generate IDs.</p> <p>Option 1: Hash (Recommended)</p> <pre><code>SELECT MD5(LOWER(TRIM(email))) as customer_id, email, name\nFROM clean_customers\n</code></pre> <ul> <li>Same email = same ID, forever</li> <li>Deterministic, stateless, simple</li> <li>Ugly IDs but who cares</li> </ul> <p>Option 2: Persistent Lookup Table</p> <ul> <li>Store <code>email \u2192 customer_id</code> mapping</li> <li>On each run, only assign new IDs to new emails</li> <li>Nice sequential numbers (1, 2, 3...)</li> <li>More complex, requires state</li> </ul> <p>\u26a0\ufe0f NEVER use RANK() or ROW_NUMBER() alone \u2014 IDs will shift when data changes!</p>"},{"location":"guides/dimensional_modeling_guide/#lookup-tables-vs-dimension-tables","title":"Lookup Tables vs Dimension Tables","text":"<p>These are NOT the same thing.</p> Lookup Table Dimension Table Silver layer Gold layer Maps <code>email \u2192 customer_id</code> Has <code>customer_sk, customer_id, name, city, ...</code> Just a helper for ID generation Official, versioned, surrogate-keyed entity No history SCD2 history tracking Simple key-value Rich with attributes"},{"location":"guides/dimensional_modeling_guide/#the-flow","title":"The Flow","text":"<pre><code>Bronze: raw_customers (email, name, city)\n    \u2502\n    \u25bc\nSilver: customer_lookup (email \u2192 customer_id via hash)\n    \u2502\n    \u25bc\nSilver: clean_customers (customer_id, email, name, city)\n    \u2502\n    \u25bc\nGold: dim_customer (customer_SK, customer_id, name, city, valid_from, valid_to, is_current)\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>What happens when a customer moves from NYC to LA?</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-0-keep-original","title":"SCD Type 0: Keep Original","text":"<p>Never update. Always shows original value.</p> <p>Use case: Birth date, original signup date</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-1-overwrite","title":"SCD Type 1: Overwrite","text":"<p>Update in place. Lose history.</p> customer_sk customer_id city 1001 47 LA <p>Use case: Correcting typos, current-state-only reporting</p>"},{"location":"guides/dimensional_modeling_guide/#scd-type-2-track-history-most-common","title":"SCD Type 2: Track History (Most Common)","text":"<p>Create new row, close old row.</p> customer_sk customer_id city valid_from valid_to is_current 1001 47 NYC 2020-01-01 2023-03-15 false 1002 47 LA 2023-03-15 NULL true <p>Use case: Historical analysis, \"What was their city when they ordered?\"</p>"},{"location":"guides/dimensional_modeling_guide/#looking-up-surrogate-keys-for-facts","title":"Looking Up Surrogate Keys for Facts","text":"<p>When building fact tables, filter by <code>is_current = true</code>:</p> <pre><code>SELECT \n  o.order_id,\n  dc.customer_sk\nFROM clean_orders o\nLEFT JOIN dim_customer dc \n  ON o.customer_id = dc.customer_id \n  AND dc.is_current = true  -- Only match current version!\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#the-date-dimension","title":"The Date Dimension","text":"<p>Every business question involves time. Pre-calculate all date attributes.</p> Column Example Why Useful date_sk 20240115 Primary key full_date 2024-01-15 Actual date day_of_week Monday Filter by weekday is_weekend false Weekend analysis month 1 Monthly aggregation month_name January Display-friendly quarter 1 Quarterly reporting year 2024 Annual comparison is_holiday false Holiday impact"},{"location":"guides/dimensional_modeling_guide/#why-pre-calculate","title":"Why Pre-Calculate?","text":"<p>Without date dimension: <pre><code>-- Calculates for every row\nWHERE DAYOFWEEK(order_date) IN (1, 7)\n</code></pre></p> <p>With date dimension: <pre><code>-- Pre-calculated, indexed, fast\nWHERE d.is_weekend = true\n</code></pre></p>"},{"location":"guides/dimensional_modeling_guide/#aggregations","title":"Aggregations","text":""},{"location":"guides/dimensional_modeling_guide/#why-pre-aggregate","title":"Why Pre-Aggregate?","text":"<p>Without aggregates: <pre><code>-- Scans 1 BILLION rows every time\nSELECT SUM(revenue) FROM fact_sales WHERE month = 'January'\n</code></pre></p> <p>With aggregates: <pre><code>-- Scans 31 rows\nSELECT SUM(daily_revenue) FROM agg_daily_sales WHERE month = 'January'\n</code></pre></p>"},{"location":"guides/dimensional_modeling_guide/#the-grain-concept","title":"The Grain Concept","text":"<p>Grain = what one row represents</p> Table Grain fact_sales One order line item agg_daily_sales All sales for a day + product + region agg_monthly_sales All sales for a month + product + region <p>Rule: You can roll UP (daily \u2192 monthly) but not drill DOWN (monthly \u2192 daily) without the source.</p>"},{"location":"guides/dimensional_modeling_guide/#when-to-use-natural-keys-vs-surrogate-keys-practical-decision","title":"When to Use Natural Keys vs Surrogate Keys (Practical Decision)","text":"<p>The theory says \"always use surrogate keys in gold.\" Reality is more nuanced.</p>"},{"location":"guides/dimensional_modeling_guide/#use-natural-keys-when","title":"Use Natural Keys When:","text":"<ul> <li>Source system IDs are stable \u2014 e.g., <code>entity_id</code> from operational systems, <code>location_id</code> from reference tables</li> <li>You're a solo DE and simplicity matters \u2014 fewer moving parts = fewer bugs</li> <li>No multi-source integration with conflicting IDs \u2014 one source per entity</li> <li>No need for unknown member (SK=0) handling \u2014 your FKs always resolve</li> </ul> <p>Example: Analytics project uses <code>entity_id</code>, <code>location_id</code>, <code>facility_id</code> as natural keys because they're system-generated and never change.</p>"},{"location":"guides/dimensional_modeling_guide/#use-surrogate-keys-when","title":"Use Surrogate Keys When:","text":"<ul> <li>Natural keys might change or get recycled \u2014 e.g., product codes reassigned</li> <li>Joining on composite keys (3+ columns) is unwieldy \u2014 <code>JOIN ON a=a AND b=b AND c=c AND d=d</code></li> <li>Integrating same entity from multiple sources \u2014 same <code>customer_id</code> means different things in CRM vs ERP</li> <li>Need unknown member rows for orphan FK handling \u2014 SK=0 catches missing references</li> </ul>"},{"location":"guides/dimensional_modeling_guide/#when-to-use-scd2-function-vs-dimensionpattern","title":"When to Use <code>scd2</code> Function vs <code>DimensionPattern</code>","text":"<p>Both handle SCD Type 2, but they serve different purposes.</p> Scenario Use Why Need custom SQL transforms before SCD <code>scd2</code> function Pattern doesn't support mid-pipeline SQL All-in-one with surrogate keys + unknown member <code>DimensionPattern</code> Handles everything declaratively Natural keys sufficient, no surrogates needed <code>scd2</code> function Lighter weight, less overhead Building classic Kimball star schema from scratch <code>DimensionPattern</code> Full feature set"},{"location":"guides/dimensional_modeling_guide/#scd2-function-example-custom-sql-scd","title":"<code>scd2</code> Function Example (Custom SQL + SCD)","text":"<pre><code>transform:\n  steps:\n    - sql_file: \"sql/cleaned_vw_dim_location.sql\"  # Custom transform first\n    - function: scd2\n      params:\n        connection: warehouse_prod\n        path: \"analytics/silver/cleaned_vw_dim_location\"\n        keys: [location_id]\n        track_cols: [LocationCode, Region, Site, Department]\n        effective_time_col: _extracted_at\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#dimensionpattern-example-all-in-one","title":"<code>DimensionPattern</code> Example (All-in-One)","text":"<pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_cols: [name, email, address]\n    target: warehouse.dim_customer\n    unknown_member: true\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":""},{"location":"guides/dimensional_modeling_guide/#using-rank-to-generate-ids","title":"\u274c Using RANK() to Generate IDs","text":"<pre><code>-- WRONG: IDs will shift when new data arrives!\nSELECT DENSE_RANK() OVER (ORDER BY email) as customer_id\n</code></pre> <p>Fix: Use hash or persistent lookup table.</p>"},{"location":"guides/dimensional_modeling_guide/#building-lookups-from-bronze-dirty-data","title":"\u274c Building Lookups from Bronze (Dirty Data)","text":"<pre><code>-- WRONG: Bronze has duplicates, nulls, deleted records\nSELECT DISTINCT email FROM raw_customers\n</code></pre> <p>Fix: Build lookups from Silver (cleaned data).</p>"},{"location":"guides/dimensional_modeling_guide/#joining-fact-to-dimension-without-is_current-filter","title":"\u274c Joining Fact to Dimension Without is_current Filter","text":"<pre><code>-- WRONG: May get multiple matches (historical + current)\nSELECT * FROM fact_orders f\nJOIN dim_customer dc ON f.customer_id = dc.customer_id\n</code></pre> <p>Fix: Add <code>AND dc.is_current = true</code>.</p>"},{"location":"guides/dimensional_modeling_guide/#confusing-lookup-tables-with-dimension-tables","title":"\u274c Confusing Lookup Tables with Dimension Tables","text":"<ul> <li>Lookup = Silver, just maps keys</li> <li>Dimension = Gold, has surrogate keys + history + attributes</li> </ul>"},{"location":"guides/dimensional_modeling_guide/#not-having-an-unknown-member","title":"\u274c Not Having an Unknown Member","text":"<p>If a fact references a customer that doesn't exist, the JOIN fails and you lose data.</p> <p>Fix: Every dimension should have <code>SK = 0, name = 'Unknown'</code>.</p>"},{"location":"guides/dimensional_modeling_guide/#odibi-current-state-vs-target","title":"ODIBI Current State vs Target","text":""},{"location":"guides/dimensional_modeling_guide/#what-odibi-has-now","title":"What ODIBI Has Now","text":"Pattern What It Does Limitation <code>FactPattern</code> Dedup + pass through No SK lookup, no orphan handling <code>SCD2Pattern</code> Track history No auto surrogate key <code>MergePattern</code> Upsert logic \u2014 <code>SnapshotPattern</code> Point-in-time capture \u2014 <code>generate_surrogate_key</code> Hash-based key Not integrated into patterns"},{"location":"guides/dimensional_modeling_guide/#what-were-adding","title":"What We're Adding","text":"Pattern What It Will Do <code>DimensionPattern</code> Auto SK + SCD + unknown member + audit columns <code>DateDimensionPattern</code> Generate complete date dimension <code>Enhanced FactPattern</code> Auto SK lookups + orphan handling + grain validation <code>AggregationPattern</code> Declarative GROUP BY + time rollups"},{"location":"guides/dimensional_modeling_guide/#target-declarative-dimensional-modeling","title":"Target: Declarative Dimensional Modeling","text":"<pre><code># This should just work\n- name: dim_customer\n  pattern:\n    type: dimension\n    params:\n      natural_key: customer_id\n      surrogate_key: customer_sk\n      scd_type: 2\n      unknown_member: true\n\n- name: fact_orders\n  pattern:\n    type: fact\n    params:\n      grain: [order_id]\n      dimensions:\n        - source_column: customer_id\n          dimension_table: dim_customer\n          surrogate_key: customer_sk\n      orphan_handling: unknown\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#quick-reference-the-mental-model","title":"Quick Reference: The Mental Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        BUSINESS QUESTION                             \u2502\n\u2502   \"What was revenue by product category for Q4 weekends?\"           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         GOLD LAYER                                   \u2502\n\u2502   fact_orders \u2190\u2192 dim_product \u2190\u2192 dim_date                            \u2502\n\u2502   Surrogate keys, SCD2 history, pre-aggregated                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        SILVER LAYER                                  \u2502\n\u2502   clean_orders, clean_customers, customer_lookup                    \u2502\n\u2502   Natural keys (hash or source), validated, deduplicated            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        BRONZE LAYER                                  \u2502\n\u2502   raw_orders.csv, raw_customers.json                                \u2502\n\u2502   As-is from source, messy, no IDs if source doesn't have them     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/dimensional_modeling_guide/#glossary","title":"Glossary","text":"Term Definition Fact Table of events/transactions with measures Dimension Table of context (who, what, when, where) Grain What one row represents Natural Key Business identifier (customer_id) Surrogate Key Warehouse-generated integer (customer_sk) SCD Slowly Changing Dimension Star Schema Fact in center, dimensions around it Lookup Table Helper table to map values to IDs Unknown Member Row with SK=0 for orphan handling Aggregate Pre-calculated summary table"},{"location":"guides/dogfooding/","title":"\ud83d\udc36 Dogfooding Guide","text":"<p>\"Eat your own dog food.\"</p> <p>This guide explains how we use Odibi to build Odibi, and how you can use the <code>odibi-metrics</code> project to validate your own environment.</p>"},{"location":"guides/dogfooding/#what-is-dogfooding","title":"What is Dogfooding?","text":"<p>Dogfooding means using your own product to do your actual job.</p> <p>Instead of testing Odibi with synthetic data (like \"foo\", \"bar\", \"test_1\"), we use it to track the development velocity of the Odibi framework itself. This forces us to encounter real-world problems\u2014messy API data, rate limits, Unicode errors, schema drift\u2014before our users do.</p>"},{"location":"guides/dogfooding/#the-odibi-metrics-pipeline","title":"The <code>odibi-metrics</code> Pipeline","text":"<p>We have included a reference implementation in <code>examples/odibi-metrics</code>. This is a real pipeline that:</p> <ol> <li>Extracts: Connects to the GitHub API to fetch Issues and PRs from <code>henryodibi11/Odibi</code>.</li> <li>Transforms: Cleans the data, handles timezones, and calculates weekly velocity (opened vs. closed tasks).</li> <li>Loads: Saves the results to a Gold layer (<code>velocity.csv</code>) and updates the System Catalog.</li> </ol>"},{"location":"guides/dogfooding/#how-to-run-it","title":"How to Run It","text":"<pre><code># 1. Go to the example directory\ncd examples/odibi-metrics\n\n# 2. Run the pipeline\nodibi run odibi.yaml\n</code></pre>"},{"location":"guides/dogfooding/#what-it-teaches-you","title":"What It Teaches You","text":"<p>If this pipeline fails, it means Odibi is not stable enough for production. We found (and fixed) the following issues using this exact pipeline:</p> <ul> <li>\u274c Unicode Errors: Windows consoles crashing on emoji output.</li> <li>\u274c Schema Drift: The System Catalog failing when new metadata fields were added.</li> <li>\u274c Timezone Bugs: Pandas crashing when grouping TZ-aware dates.</li> </ul>"},{"location":"guides/dogfooding/#how-to-file-issues","title":"How to File Issues","text":"<p>When you find a bug while running Odibi (or the dogfood pipeline), you should track it using GitHub Issues.</p> <ol> <li>Go to the Issues Tab on GitHub.</li> <li>Click New Issue.</li> <li>Title: Short summary (e.g., \"Crash on Windows 11\").</li> <li>Body: Paste the error log and your <code>odibi.yaml</code>.</li> </ol> <p>The Meta-Loop: Once you file the issue, the <code>odibi-metrics</code> pipeline will actually download that issue the next time it runs, adding it to your project statistics. You are using Odibi to measure how fast you are fixing Odibi.</p>"},{"location":"guides/environments/","title":"Managing Environments","text":"<p>Odibi allows you to define a single pipeline configuration that adapts to different contexts (e.g., Local Development, Testing, Production) using the <code>environments</code> block. This prevents configuration drift and ensures your pipeline logic remains consistent while infrastructure details change.</p>"},{"location":"guides/environments/#how-it-works","title":"How it Works","text":"<p>Odibi uses a Base Configuration + Override model: 1.  Base Configuration: Defines your default settings (typically for local development). 2.  Environment Overrides: Specific blocks that patch or replace values in the base configuration when that environment is active.</p>"},{"location":"guides/environments/#configuration-structure","title":"Configuration Structure","text":"<p>Odibi supports two ways to define environments: 1.  Inline Block: Using an <code>environments</code> block in your main config file. 2.  External Files: Using separate <code>env.{env}.yaml</code> files (e.g., <code>env.prod.yaml</code>).</p>"},{"location":"guides/environments/#method-1-inline-block","title":"Method 1: Inline Block","text":"<p>Add an <code>environments</code> section to your <code>project.yaml</code>:</p> <pre><code># ... base config ...\nenvironments:\n  prod:\n    engine: spark\n</code></pre>"},{"location":"guides/environments/#method-2-external-files-recommended-for-large-configs","title":"Method 2: External Files (Recommended for large configs)","text":"<p>Keep your main <code>odibi.yaml</code> clean by putting overrides in separate files.</p> <p>File: <code>odibi.yaml</code> <pre><code>project: Sales Data Pipeline\nengine: pandas\nconnections:\n  data_lake:\n    type: local\n    base_path: ./data\n</code></pre></p> <p>File: <code>env.prod.yaml</code> <pre><code># Automatically merged when running with --env prod\nengine: spark\nconnections:\n  data_lake:\n    type: azure_adls\n    account: prod_acc\n</code></pre></p> <p>When you run <code>odibi run odibi.yaml --env prod</code>, Odibi will: 1. Load <code>odibi.yaml</code>. 2. Look for <code>env.prod.yaml</code> in the same directory. 3. Merge the prod config on top of the base config.</p>"},{"location":"guides/environments/#inline-example-method-1","title":"Inline Example (Method 1)","text":"<pre><code># --- 1. Base Configuration (Default / Local) ---\nproject: Sales Data Pipeline\nengine: pandas\nretry:\n  enabled: false\n\nconnections:\n  data_lake:\n    type: local\n    base_path: ./data/raw\n\npipelines:\n  - pipeline: ingest_sales\n    nodes:\n      - name: read_csv\n        read:\n          connection: data_lake\n          path: sales.csv\n\n# --- 2. Environment Overrides ---\nenvironments:\n  # Production Environment\n  prod:\n    engine: spark  # Switch to Spark for scale\n    retry:\n      enabled: true\n      max_attempts: 3\n    connections:\n      data_lake:\n        type: azure_adls\n        account: mycompanyprod\n        container: sales-data\n        auth_mode: managed_identity\n    story:\n      max_sample_rows: 0 # Disable data sampling for security\n\n  # Testing Environment\n  test:\n    connections:\n      data_lake:\n        type: local\n        base_path: ./data/test_fixtures\n</code></pre>"},{"location":"guides/environments/#usage","title":"Usage","text":""},{"location":"guides/environments/#cli","title":"CLI","text":"<p>Use the <code>--env</code> flag to activate an environment.</p> <p>Run in Default (Base) Environment: <pre><code>odibi run project.yaml\n</code></pre></p> <p>Run in Production: <pre><code>odibi run project.yaml --env prod\n</code></pre></p>"},{"location":"guides/environments/#python-api","title":"Python API","text":"<p>Pass the <code>env</code> parameter when initializing the <code>PipelineManager</code>.</p> <pre><code>from odibi.pipeline import PipelineManager\n\n# Load Prod Configuration\nmanager = PipelineManager.from_yaml(\"project.yaml\", env=\"prod\")\n\n# Run Pipeline\nmanager.run(\"ingest_sales\")\n</code></pre>"},{"location":"guides/environments/#databricks-example","title":"Databricks Example","text":"<p>In a Databricks notebook, you can use widgets to switch environments dynamically without changing code.</p> <pre><code># 1. Create Widget\ndbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"test\", \"prod\"])\n\n# 2. Get Selection\ncurrent_env = dbutils.widgets.get(\"environment\")\n\n# 3. Run Pipeline\nmanager = PipelineManager.from_yaml(\"/dbfs/project.yaml\", env=current_env)\nmanager.run()\n</code></pre>"},{"location":"guides/environments/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/environments/#1-swapping-storage-local-vs-cloud","title":"1. Swapping Storage (Local vs. Cloud)","text":"<p>Develop locally with CSVs, deploy to ADLS/S3 without changing pipeline code.</p> <pre><code>connections:\n  storage: { type: local, base_path: ./data }\n\nenvironments:\n  prod:\n    connections:\n      storage: { type: azure_adls, account: prod_acc, container: data }\n</code></pre>"},{"location":"guides/environments/#2-scaling-engines-pandas-vs-spark","title":"2. Scaling Engines (Pandas vs. Spark)","text":"<p>Use Pandas for fast local iteration and unit tests, but switch to Spark for distributed processing in production.</p> <pre><code>engine: pandas\n\nenvironments:\n  prod:\n    engine: spark\n</code></pre>"},{"location":"guides/environments/#3-security-privacy","title":"3. Security &amp; Privacy","text":"<p>Disable data sampling in stories for production to prevent PII leakage, while keeping it enabled in dev for debugging.</p> <pre><code>story:\n  max_sample_rows: 20\n\nenvironments:\n  prod:\n    story:\n      max_sample_rows: 0\n</code></pre>"},{"location":"guides/environments/#4-alerting","title":"4. Alerting","text":"<p>Only send Slack/Teams notifications when running in production.</p> <pre><code>alerts: []  # No alerts in dev\n\nenvironments:\n  prod:\n    alerts:\n      - type: slack\n        url: ${SLACK_WEBHOOK}\n</code></pre>"},{"location":"guides/environments/#5-system-environment-tagging","title":"5. System Environment Tagging","text":"<p>Tag all system catalog records (runs, state) with the environment for cross-environment observability:</p> <pre><code>system:\n  connection: catalog_storage\n  path: _odibi_system\n  environment: dev  # Default environment tag\n\nenvironments:\n  qat:\n    system:\n      environment: qat\n  prod:\n    system:\n      environment: prod\n</code></pre> <p>This enables queries across environments: <pre><code>SELECT * FROM meta_runs WHERE environment = 'prod' AND status = 'FAILED'\n</code></pre></p>"},{"location":"guides/environments/#6-centralized-sql-server-system-catalog","title":"6. Centralized SQL Server System Catalog","text":"<p>Store system metadata in a central SQL Server for unified observability:</p> <pre><code>system:\n  connection: local_storage\n  path: .odibi/system\n\nenvironments:\n  prod:\n    system:\n      connection: sql_server\n      schema_name: odibi_system\n      environment: prod\n      sync_from:\n        connection: local_storage\n        path: .odibi/system\n\nconnections:\n  local_storage:\n    type: local\n    base_path: ./\n  sql_server:\n    type: sql_server\n    server: central-server.database.windows.net\n    database: odibi_metadata\n</code></pre> <p>In production, the SQL Server backend: - Auto-creates schema and tables - Stores <code>meta_runs</code> and <code>meta_state</code> - Enables syncing local dev data to central location</p> <p>Sync local data to SQL Server: <pre><code>odibi system sync project.yaml --env prod\n</code></pre></p>"},{"location":"guides/mcp_guide/","title":"Odibi MCP Server Guide","text":"<p>The odibi-knowledge MCP (Model Context Protocol) server exposes 45 tools for AI assistants to interact with your odibi pipelines, understand the framework, and help build data pipelines.</p>"},{"location":"guides/mcp_guide/#quick-start","title":"Quick Start","text":""},{"location":"guides/mcp_guide/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code>pip install odibi mcp chromadb python-dotenv scikit-learn\n</code></pre>"},{"location":"guides/mcp_guide/#2-configure-your-ide","title":"2. Configure Your IDE","text":""},{"location":"guides/mcp_guide/#continue-vs-code","title":"Continue (VS Code)","text":"<p>Add to <code>~/.continue/config.yaml</code>:</p> <pre><code>mcpServers:\n  - name: odibi-knowledge\n    command: python\n    args:\n      - D:/odibi/run_mcp.py  # Path to odibi\n    env:\n      ODIBI_CONFIG: C:/Users/yourname/project/odibi.yaml  # Your project config\n</code></pre>"},{"location":"guides/mcp_guide/#other-mcp-clients","title":"Other MCP Clients","text":"<p>The server runs via stdio: <pre><code>python run_mcp.py\n</code></pre></p>"},{"location":"guides/mcp_guide/#3-test-the-connection","title":"3. Test the Connection","text":"<p>Ask your AI assistant: <pre><code>Use odibi MCP list_transformers\n</code></pre></p>"},{"location":"guides/mcp_guide/#available-tools-45-total","title":"Available Tools (45 Total)","text":""},{"location":"guides/mcp_guide/#start-here-bootstrap-tool","title":"\ud83d\ude80 Start Here: Bootstrap Tool","text":"Tool Description Example <code>bootstrap_context</code> CALL FIRST - Auto-gather full project context (connections, pipelines, outputs, patterns, YAML rules) <code>{}</code>"},{"location":"guides/mcp_guide/#knowledge-tools-21","title":"Knowledge Tools (21)","text":"Tool Description Example <code>list_transformers</code> List all 52+ transformers <code>{}</code> <code>list_patterns</code> List all 6 DWH patterns <code>{}</code> <code>list_connections</code> List connection types <code>{}</code> <code>explain</code> Get detailed docs for any feature <code>{\"name\": \"scd2\"}</code> <code>get_transformer_signature</code> Correct transformer signature <code>{}</code> <code>get_yaml_structure</code> Correct YAML structure <code>{}</code> <code>get_deep_context</code> Full 69K char framework docs <code>{}</code> <code>get_index_stats</code> Codebase index statistics <code>{}</code> <code>list_docs</code> List documentation files <code>{\"category\": \"patterns\"}</code> <code>search_docs</code> Search documentation <code>{\"query\": \"SCD2\"}</code> <code>get_doc</code> Get specific doc file <code>{\"doc_path\": \"docs/patterns/scd2.md\"}</code> <code>validate_yaml</code> Validate pipeline YAML <code>{\"yaml_content\": \"...\"}</code> <code>diagnose_error</code> Get fix suggestions for errors <code>{\"error_message\": \"KeyError: 'col'\"}</code> <code>get_example</code> Get pattern example YAML <code>{\"pattern_name\": \"dimension\"}</code> <code>suggest_pattern</code> Suggest pattern for use case <code>{\"use_case\": \"track changes\"}</code> <code>get_engine_differences</code> Spark/Pandas/Polars differences <code>{}</code> <code>get_validation_rules</code> Available validation rules <code>{}</code> <code>generate_transformer</code> Generate transformer code <code>{\"name\": \"my_func\", ...}</code> <code>generate_pipeline_yaml</code> Generate pipeline YAML <code>{\"project_name\": \"test\", ...}</code> <code>query_codebase</code> Search odibi source code <code>{\"question\": \"how does SCD2 work?\"}</code> <code>reindex</code> Rebuild codebase index <code>{\"force\": true}</code>"},{"location":"guides/mcp_guide/#storyrun-tools-4","title":"Story/Run Tools (4)","text":"Tool Description Example <code>story_read</code> Get pipeline run status <code>{\"pipeline\": \"bronze\"}</code> <code>story_diff</code> Compare two runs <code>{\"pipeline\": \"bronze\", \"run_a\": \"...\", \"run_b\": \"...\"}</code> <code>node_describe</code> Get node execution details <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\"}</code>"},{"location":"guides/mcp_guide/#sample-data-tools-3","title":"Sample Data Tools (3)","text":"Tool Description Example <code>node_sample</code> Get node output sample <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\", \"max_rows\": 10}</code> <code>node_sample_in</code> Get node input sample <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\"}</code> <code>node_failed_rows</code> Get validation failures <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\"}</code>"},{"location":"guides/mcp_guide/#catalogstats-tools-4","title":"Catalog/Stats Tools (4)","text":"Tool Description Example <code>node_stats</code> Node execution statistics <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\"}</code> <code>pipeline_stats</code> Pipeline-level statistics <code>{\"pipeline\": \"bronze\"}</code> <code>failure_summary</code> Recent failures across pipelines <code>{\"max_failures\": 100}</code> <code>schema_history</code> Schema changes over time <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\"}</code> <p>Note: Catalog tools require <code>skip_catalog_writes: false</code> in your config. They return empty results when disabled.</p>"},{"location":"guides/mcp_guide/#lineage-tools-3","title":"Lineage Tools (3)","text":"Tool Description Example <code>lineage_upstream</code> Find upstream dependencies <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\", \"depth\": 3}</code> <code>lineage_downstream</code> Find downstream dependents <code>{\"pipeline\": \"bronze\", \"node\": \"my_node\", \"depth\": 3}</code> <code>lineage_graph</code> Full pipeline lineage graph <code>{\"pipeline\": \"bronze\", \"include_external\": true}</code>"},{"location":"guides/mcp_guide/#schema-tools-3","title":"Schema Tools (3)","text":"Tool Description Example <code>list_outputs</code> List pipeline outputs <code>{\"pipeline\": \"bronze\"}</code> <code>output_schema</code> Get output column schema <code>{\"pipeline\": \"bronze\", \"output_name\": \"my_node\"}</code> <code>compare_schemas</code> Compare schemas between two sources <code>{\"source_connection\": \"raw\", \"source_path\": \"data.csv\", \"target_connection\": \"bronze\", \"target_path\": \"output.parquet\"}</code>"},{"location":"guides/mcp_guide/#discovery-tools-8","title":"Discovery Tools (8)","text":"Tool Description Example <code>list_files</code> List files in a connection <code>{\"connection\": \"my_conn\", \"path\": \"data/\", \"pattern\": \"*.csv\"}</code> <code>preview_source</code> Preview source data (supports sheet param for Excel) <code>{\"connection\": \"my_conn\", \"path\": \"data.csv\", \"max_rows\": 10}</code> <code>infer_schema</code> Infer schema from file <code>{\"connection\": \"my_conn\", \"path\": \"data.csv\"}</code> <code>list_schemas</code> List schemas in SQL database with table counts. Call FIRST before discover_database <code>{\"connection\": \"my_sql\"}</code> <code>list_tables</code> List SQL tables <code>{\"connection\": \"my_sql\", \"schema\": \"dbo\"}</code> <code>describe_table</code> Describe SQL table <code>{\"connection\": \"my_sql\", \"table\": \"my_table\"}</code> <code>list_sheets</code> List sheet names in Excel file <code>{\"connection\": \"my_conn\", \"path\": \"data.xlsx\"}</code> <code>discover_database</code> Discover SQL tables (structure-first, shallow). Samples OFF by default - use preview_source <code>{\"connection\": \"my_sql\", \"schema\": \"dbo\"}</code> <code>discover_storage</code> Discover files (structure-first, shallow). Samples/recursion OFF by default - use preview_source <code>{\"connection\": \"my_adls\", \"path\": \"raw/\"}</code> <code>debug_env</code> Debug environment setup - shows .env loading, env vars, and connection status <code>{}</code>"},{"location":"guides/mcp_guide/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/mcp_guide/#1-explore-available-data","title":"1. Explore Available Data","text":"<pre><code>Use odibi MCP list_files with connection=\"my_adls\", path=\"raw/\", pattern=\"*.csv\"\nUse odibi MCP preview_source with connection=\"my_adls\", path=\"raw/customers.csv\", max_rows=5\nUse odibi MCP infer_schema with connection=\"my_adls\", path=\"raw/customers.csv\"\n</code></pre>"},{"location":"guides/mcp_guide/#2-build-a-new-pipeline","title":"2. Build a New Pipeline","text":"<pre><code>Use odibi MCP get_yaml_structure\nUse odibi MCP suggest_pattern with use_case=\"build customer dimension with history\"\nUse odibi MCP get_example with pattern_name=\"dimension\"\nUse odibi MCP list_transformers\n</code></pre>"},{"location":"guides/mcp_guide/#3-debug-a-failed-run","title":"3. Debug a Failed Run","text":"<pre><code>Use odibi MCP story_read with pipeline=\"bronze\"\nUse odibi MCP node_describe with pipeline=\"bronze\", node=\"failed_node\"\nUse odibi MCP node_sample_in with pipeline=\"bronze\", node=\"failed_node\"\nUse odibi MCP node_failed_rows with pipeline=\"bronze\", node=\"failed_node\"\nUse odibi MCP diagnose_error with error_message=\"&lt;the error message&gt;\"\n</code></pre>"},{"location":"guides/mcp_guide/#4-understand-the-framework","title":"4. Understand the Framework","text":"<pre><code>Use odibi MCP get_deep_context\nUse odibi MCP explain with name=\"scd2\"\nUse odibi MCP search_docs with query=\"validation\"\nUse odibi MCP query_codebase with question=\"how does merge pattern work?\"\n</code></pre>"},{"location":"guides/mcp_guide/#exploration-mode-new","title":"Exploration Mode (New!)","text":"<p>Exploration mode lets AI explore your data sources without a full pipeline configuration. Perfect for: - Initial data discovery and profiling - Working with an analyst to understand source data - Quick data exploration before building pipelines</p>"},{"location":"guides/mcp_guide/#exploration-config","title":"Exploration Config","text":"<p>Create a minimal <code>exploration.yaml</code> (or <code>mcp_config.yaml</code>):</p> <pre><code># exploration.yaml - just connections, no pipelines needed\nproject: my_exploration  # optional\n\nconnections:\n  my_sql:\n    type: azure_sql\n    connection_string: ${SQL_CONN}\n  my_adls:\n    type: azure_adls\n    account_name: mystorageaccount\n    container: raw\n  local:\n    type: local\n    path: ./data/samples\n</code></pre>"},{"location":"guides/mcp_guide/#what-works-in-exploration-mode","title":"What Works in Exploration Mode","text":"Tool Works Description <code>list_files</code> \u2705 Browse any connection <code>list_tables</code> \u2705 List SQL database tables <code>describe_table</code> \u2705 Get column info from SQL <code>preview_source</code> \u2705 Sample data from files/tables <code>infer_schema</code> \u2705 Auto-detect types <code>list_sheets</code> \u2705 Excel sheet names <code>story_read</code> \u274c Needs full project.yaml <code>node_sample</code> \u274c Needs full project.yaml <code>lineage_*</code> \u274c Needs full project.yaml"},{"location":"guides/mcp_guide/#example-workflow","title":"Example Workflow","text":"<ol> <li>Create exploration.yaml with your connections</li> <li>Set environment variable: <code>ODIBI_CONFIG=./exploration.yaml</code></li> <li>Ask AI to explore:    <pre><code>Use odibi MCP list_tables with connection=\"my_sql\", schema=\"dbo\"\nUse odibi MCP preview_source with connection=\"my_sql\", source=\"dbo.customers\", limit=10\nUse odibi MCP describe_table with connection=\"my_sql\", table=\"dbo.orders\"\n</code></pre></li> </ol> <p>When ready to build pipelines, graduate to a full <code>project.yaml</code> with pipelines, story, and system sections.</p>"},{"location":"guides/mcp_guide/#configuration","title":"Configuration","text":""},{"location":"guides/mcp_guide/#project-configuration","title":"Project Configuration","text":"<p>The MCP server reads your <code>odibi.yaml</code> to access: - Connections - For discovery tools - Pipelines - For lineage, schema, story tools - Story path - For run status and samples - System catalog - For statistics (if enabled)</p>"},{"location":"guides/mcp_guide/#environment-variables","title":"Environment Variables","text":"Variable Description <code>ODIBI_CONFIG</code> Path to your odibi.yaml <code>MCP_CONFIG</code> Path to MCP-specific config (optional) <code>ODIBI_USE_TFIDF_EMBEDDINGS</code> Set to <code>1</code> on Windows to use keyword search fallback"},{"location":"guides/mcp_guide/#windows-notes","title":"Windows Notes","text":"<p>On Windows, the <code>query_codebase</code> and <code>reindex</code> tools use a keyword-based fallback instead of semantic embeddings due to PyTorch DLL issues. This is automatic when using <code>run_mcp.py</code>.</p>"},{"location":"guides/mcp_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/mcp_guide/#no-project-context-errors","title":"\"No project context\" errors","text":"<p>Make sure <code>ODIBI_CONFIG</code> points to a valid odibi.yaml file.</p>"},{"location":"guides/mcp_guide/#empty-results-from-catalog-tools","title":"Empty results from catalog tools","text":"<p>Check your config has <code>skip_catalog_writes: false</code>. Catalog tools need the system catalog to be populated.</p>"},{"location":"guides/mcp_guide/#no-outputs-found-for-list_outputs","title":"\"No outputs found\" for list_outputs","text":"<p>Ensure your pipeline nodes have <code>write:</code> blocks defined with <code>connection</code>, <code>path</code>, and <code>format</code>.</p>"},{"location":"guides/mcp_guide/#query_codebase-crashes-on-windows","title":"query_codebase crashes on Windows","text":"<p>This is handled automatically by the keyword fallback. If you still have issues, set: <pre><code>set ODIBI_USE_TFIDF_EMBEDDINGS=1\n</code></pre></p>"},{"location":"guides/mcp_guide/#see-also","title":"See Also","text":"<ul> <li>ODIBI_DEEP_CONTEXT.md - Full framework documentation</li> <li>YAML Reference - Complete YAML schema</li> <li>Patterns Guide - DWH pattern documentation</li> </ul>"},{"location":"guides/mcp_recipes/","title":"Odibi MCP Recipes for AI Assistants","text":"<p>This guide teaches AI assistants how to combine odibi MCP tools to help developers build, debug, and maintain data pipelines.</p>"},{"location":"guides/mcp_recipes/#recipe-1-help-me-build-a-new-pipeline","title":"Recipe 1: \"Help me build a new pipeline\"","text":"<p>When user says: \"I need to build a pipeline for...\", \"Create a pipeline that...\", \"Help me transform this data...\"</p> <p>Steps:</p> <ol> <li> <p>Understand the data source <pre><code>list_files(connection=\"&lt;conn&gt;\", path=\"&lt;path&gt;\", pattern=\"*\")\npreview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\", max_rows=10)\ninfer_schema(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\")\n</code></pre></p> </li> <li> <p>Suggest the right pattern <pre><code>suggest_pattern(use_case=\"&lt;user's description&gt;\")\n</code></pre></p> </li> <li> <p>Get the correct YAML structure <pre><code>get_yaml_structure()\nget_example(pattern_name=\"&lt;suggested_pattern&gt;\")\n</code></pre></p> </li> <li> <p>Find relevant transformers <pre><code>list_transformers()\nexplain(name=\"&lt;transformer_name&gt;\")\n</code></pre></p> </li> <li> <p>Generate and validate the YAML <pre><code>validate_yaml(yaml_content=\"&lt;generated_yaml&gt;\")\n</code></pre></p> </li> </ol> <p>Example conversation:</p> <p>User: \"I need to build a pipeline to track customer changes over time\"</p> <p>AI: Let me help you build that. First, I'll check what pattern fits best... [calls suggest_pattern with use_case=\"track customer changes over time\"]</p> <p>The SCD2 pattern is perfect for this. Let me get you an example... [calls get_example with pattern_name=\"scd2\"]</p>"},{"location":"guides/mcp_recipes/#recipe-2-why-did-my-pipeline-fail","title":"Recipe 2: \"Why did my pipeline fail?\"","text":"<p>When user says: \"Pipeline failed\", \"Got an error\", \"Node is failing\", \"Debug this...\"</p> <p>Steps:</p> <ol> <li> <p>Check the run status <pre><code>story_read(pipeline=\"&lt;pipeline_name&gt;\")\n</code></pre></p> </li> <li> <p>Get details on the failed node <pre><code>node_describe(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;failed_node&gt;\")\n</code></pre></p> </li> <li> <p>Examine the input data <pre><code>node_sample_in(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\", max_rows=10)\n</code></pre></p> </li> <li> <p>Check for validation failures <pre><code>node_failed_rows(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\")\n</code></pre></p> </li> <li> <p>Get diagnosis and fix suggestions <pre><code>diagnose_error(error_message=\"&lt;the_error&gt;\")\n</code></pre></p> </li> </ol> <p>Example conversation:</p> <p>User: \"My bronze pipeline failed with KeyError: 'customer_id'\"</p> <p>AI: Let me investigate... [calls story_read to see which node failed][calls node_sample_in to see the actual input data]</p> <p>I see the issue - the source column is named 'CustomerID' not 'customer_id'. You need to rename it in your transform steps.</p>"},{"location":"guides/mcp_recipes/#recipe-3-what-data-do-i-have-available","title":"Recipe 3: \"What data do I have available?\"","text":"<p>When user says: \"What files are in...\", \"Show me the data\", \"What's in this connection\", \"Explore the data\"</p> <p>Steps:</p> <ol> <li> <p>List available files <pre><code>list_files(connection=\"&lt;conn&gt;\", path=\"&lt;path&gt;\", pattern=\"*\")\n</code></pre></p> </li> <li> <p>For SQL connections, list tables <pre><code>list_tables(connection=\"&lt;sql_conn&gt;\", schema=\"dbo\")\ndescribe_table(connection=\"&lt;sql_conn&gt;\", table=\"&lt;table&gt;\", schema=\"dbo\")\n</code></pre></p> </li> <li> <p>Preview the data <pre><code>preview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\", max_rows=10)\n</code></pre></p> </li> <li> <p>For Excel files, discover sheets first <pre><code>list_sheets(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;.xlsx\")\npreview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;.xlsx\", sheet=\"&lt;sheet_name&gt;\", max_rows=10)\n</code></pre></p> </li> <li> <p>Infer schema <pre><code>infer_schema(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#recipe-4-how-does-x-work-in-odibi","title":"Recipe 4: \"How does X work in odibi?\"","text":"<p>When user says: \"How do I...\", \"What is...\", \"Explain...\", \"How does X work\"</p> <p>Steps:</p> <ol> <li> <p>Check if it's a known feature <pre><code>explain(name=\"&lt;feature_name&gt;\")\n</code></pre></p> </li> <li> <p>Search documentation <pre><code>search_docs(query=\"&lt;topic&gt;\")\nget_doc(doc_path=\"&lt;relevant_doc&gt;\")\n</code></pre></p> </li> <li> <p>For deep questions, search the codebase <pre><code>query_codebase(question=\"&lt;question&gt;\")\n</code></pre></p> </li> <li> <p>For comprehensive understanding <pre><code>get_deep_context()\n</code></pre></p> </li> </ol> <p>Example conversation:</p> <p>User: \"How does validation work in odibi?\"</p> <p>AI: Let me explain... [calls explain with name=\"validation\"][calls get_validation_rules to list all rules]</p> <p>Odibi has 12 built-in validation rules including not_null, unique, range...</p>"},{"location":"guides/mcp_recipes/#recipe-5-show-me-what-this-pipeline-does","title":"Recipe 5: \"Show me what this pipeline does\"","text":"<p>When user says: \"What does this pipeline do\", \"Explain this pipeline\", \"Show me the lineage\"</p> <p>Steps:</p> <ol> <li> <p>List pipeline outputs <pre><code>list_outputs(pipeline=\"&lt;pipeline&gt;\")\n</code></pre></p> </li> <li> <p>Get the lineage graph <pre><code>lineage_graph(pipeline=\"&lt;pipeline&gt;\", include_external=true)\n</code></pre></p> </li> <li> <p>For specific nodes, get details <pre><code>node_describe(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\")\noutput_schema(pipeline=\"&lt;pipeline&gt;\", output_name=\"&lt;node&gt;\")\n</code></pre></p> </li> <li> <p>Check recent execution <pre><code>story_read(pipeline=\"&lt;pipeline&gt;\")\npipeline_stats(pipeline=\"&lt;pipeline&gt;\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#recipe-6-check-my-pipeline-health","title":"Recipe 6: \"Check my pipeline health\"","text":"<p>When user says: \"Is my pipeline healthy\", \"Any failures\", \"Pipeline status\", \"What's been failing\"</p> <p>Steps:</p> <ol> <li> <p>Check overall failure summary <pre><code>failure_summary(max_failures=20)\n</code></pre></p> </li> <li> <p>Check specific pipeline stats <pre><code>pipeline_stats(pipeline=\"&lt;pipeline&gt;\")\n</code></pre></p> </li> <li> <p>Check node-level stats <pre><code>node_stats(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\")\n</code></pre></p> </li> <li> <p>Check for schema drift <pre><code>schema_history(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#recipe-7-write-a-custom-transformer","title":"Recipe 7: \"Write a custom transformer\"","text":"<p>When user says: \"I need a custom transformer\", \"Create a transformer that...\", \"Write a function to...\"</p> <p>Steps:</p> <ol> <li> <p>Get the correct signature <pre><code>get_transformer_signature()\n</code></pre></p> </li> <li> <p>Check if similar transformer exists <pre><code>list_transformers()\nexplain(name=\"&lt;similar_transformer&gt;\")\n</code></pre></p> </li> <li> <p>Search for implementation examples <pre><code>query_codebase(question=\"how to implement &lt;transformer_type&gt;\")\n</code></pre></p> </li> <li> <p>Generate the transformer <pre><code>generate_transformer(name=\"&lt;name&gt;\", params=[...], description=\"...\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#recipe-8-compare-two-pipeline-runs","title":"Recipe 8: \"Compare two pipeline runs\"","text":"<p>When user says: \"What changed between runs\", \"Compare runs\", \"Why is output different\"</p> <p>Steps:</p> <ol> <li> <p>Get recent runs <pre><code>story_read(pipeline=\"&lt;pipeline&gt;\")\n</code></pre></p> </li> <li> <p>Compare the runs <pre><code>story_diff(pipeline=\"&lt;pipeline&gt;\", run_a=\"&lt;run_id_1&gt;\", run_b=\"&lt;run_id_2&gt;\")\n</code></pre></p> </li> <li> <p>Check schema changes <pre><code>schema_history(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\")\n</code></pre></p> </li> <li> <p>Sample data from both runs <pre><code>node_sample(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\", run_id=\"&lt;run_1&gt;\")\nnode_sample(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\", run_id=\"&lt;run_2&gt;\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#recipe-9-validate-my-yaml-before-running","title":"Recipe 9: \"Validate my YAML before running\"","text":"<p>When user says: \"Check my YAML\", \"Is this correct\", \"Validate this config\"</p> <p>Steps:</p> <ol> <li> <p>Validate the YAML syntax <pre><code>validate_yaml(yaml_content=\"&lt;yaml&gt;\")\n</code></pre></p> </li> <li> <p>Compare against correct structure <pre><code>get_yaml_structure()\n</code></pre></p> </li> <li> <p>Check transformer usage <pre><code>explain(name=\"&lt;transformer_used&gt;\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#recipe-10-im-new-to-odibi-help-me-get-started","title":"Recipe 10: \"I'm new to odibi, help me get started\"","text":"<p>When user says: \"How do I start\", \"New to odibi\", \"Getting started\", \"Tutorial\"</p> <p>Steps:</p> <ol> <li> <p>Provide comprehensive context <pre><code>get_deep_context()\n</code></pre></p> </li> <li> <p>Show available patterns <pre><code>list_patterns()\n</code></pre></p> </li> <li> <p>Show available connections <pre><code>list_connections()\n</code></pre></p> </li> <li> <p>Get the YAML structure <pre><code>get_yaml_structure()\n</code></pre></p> </li> <li> <p>Show a simple example <pre><code>get_example(pattern_name=\"dimension\")\n</code></pre></p> </li> </ol>"},{"location":"guides/mcp_recipes/#tool-selection-quick-reference","title":"Tool Selection Quick Reference","text":"User Intent Primary Tools Build pipeline <code>suggest_pattern</code>, <code>get_example</code>, <code>get_yaml_structure</code>, <code>list_transformers</code> Debug failure <code>story_read</code>, <code>node_describe</code>, <code>node_sample_in</code>, <code>node_failed_rows</code>, <code>diagnose_error</code> Explore data <code>list_files</code>, <code>preview_source</code>, <code>infer_schema</code>, <code>list_tables</code> Learn odibi <code>explain</code>, <code>search_docs</code>, <code>get_deep_context</code>, <code>query_codebase</code> Check lineage <code>lineage_graph</code>, <code>lineage_upstream</code>, <code>lineage_downstream</code>, <code>list_outputs</code> Monitor health <code>pipeline_stats</code>, <code>node_stats</code>, <code>failure_summary</code>, <code>schema_history</code> Validate config <code>validate_yaml</code>, <code>get_yaml_structure</code> Write code <code>generate_transformer</code>, <code>get_transformer_signature</code>, <code>query_codebase</code>"},{"location":"guides/mcp_recipes/#context-first-workflows","title":"Context-First Workflows","text":"<p>These workflows teach AI to gather full context before taking action. This prevents hallucination and ensures suggestions are grounded in the actual project state.</p>"},{"location":"guides/mcp_recipes/#workflow-a-full-source-understanding-before-building-anything","title":"Workflow A: \"Full Source Understanding\" (Before Building Anything)","text":"<p>Purpose: Before generating any YAML or suggesting transformations, the AI MUST understand the source data completely.</p> <p>Mandatory Steps (in order):</p> <pre><code># Step 1: Discover what files/tables exist\nlist_files(connection=\"&lt;conn&gt;\", path=\"&lt;path&gt;\", pattern=\"*\")\n# or for SQL:\nlist_tables(connection=\"&lt;sql_conn&gt;\", schema=\"dbo\")\n\n# Step 2: Preview the actual data (see real values)\npreview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\", max_rows=20)\n\n# Step 3: Infer the schema (understand types)\ninfer_schema(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\")\n\n# Step 4: For SQL tables, get full structure\ndescribe_table(connection=\"&lt;sql_conn&gt;\", table=\"&lt;table&gt;\", schema=\"dbo\")\n</code></pre> <p>What AI should note: - Column names exactly as they appear (case-sensitive!) - Data types (string dates vs actual dates, numeric vs string IDs) - Null patterns - which columns have missing values? - Cardinality hints - is this a lookup table or transactional data? - Key candidates - what columns could be primary/business keys?</p> <p>Only AFTER completing all steps, the AI may suggest patterns or generate YAML.</p>"},{"location":"guides/mcp_recipes/#workflow-b-pipeline-deep-dive-understanding-existing-pipelines","title":"Workflow B: \"Pipeline Deep Dive\" (Understanding Existing Pipelines)","text":"<p>Purpose: Before modifying, debugging, or extending a pipeline, understand it completely.</p> <p>Mandatory Steps:</p> <pre><code># Step 1: What outputs does this pipeline produce?\nlist_outputs(pipeline=\"&lt;pipeline&gt;\")\n\n# Step 2: What's the schema of each output?\noutput_schema(pipeline=\"&lt;pipeline&gt;\", output_name=\"&lt;output&gt;\")\n# Repeat for each output\n\n# Step 3: Understand the full lineage\nlineage_graph(pipeline=\"&lt;pipeline&gt;\", include_external=true)\n\n# Step 4: For specific nodes of interest:\nnode_describe(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\")\n\n# Step 5: See actual data flowing through\nnode_sample_in(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\", max_rows=10)\nnode_sample(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;node&gt;\", max_rows=10)\n\n# Step 6: Check execution history\nstory_read(pipeline=\"&lt;pipeline&gt;\")\n</code></pre> <p>What AI should understand before acting: - The flow of data through all nodes - What transformations are applied at each step - The expected input/output schemas - Recent run success/failure history - Any validation or quality rules in place</p>"},{"location":"guides/mcp_recipes/#workflow-c-framework-mastery-check-before-suggesting-solutions","title":"Workflow C: \"Framework Mastery Check\" (Before Suggesting Solutions)","text":"<p>Purpose: Before suggesting a pattern, transformer, or approach, verify the AI understands the framework correctly.</p> <p>Steps:</p> <pre><code># Step 1: If suggesting a pattern, verify understanding\nexplain(name=\"&lt;pattern_name&gt;\")\nget_example(pattern_name=\"&lt;pattern_name&gt;\")\n\n# Step 2: If using transformers, verify each one\nexplain(name=\"&lt;transformer_name&gt;\")\n# Check parameters, expected inputs, edge cases\n\n# Step 3: For complex questions, get full context\nget_deep_context()\n\n# Step 4: For implementation details, search docs\nsearch_docs(query=\"&lt;topic&gt;\")\nget_doc(doc_path=\"&lt;relevant_doc&gt;\")\n\n# Step 5: For code-level understanding\nquery_codebase(question=\"how does &lt;feature&gt; work internally\")\n</code></pre> <p>AI should NEVER: - Guess transformer parameters - always use <code>explain</code> first - Assume YAML structure - always reference <code>get_yaml_structure</code> - Invent column names - always derive from actual <code>preview_source</code> or <code>infer_schema</code></p>"},{"location":"guides/mcp_recipes/#workflow-d-complete-debug-investigation-before-suggesting-fixes","title":"Workflow D: \"Complete Debug Investigation\" (Before Suggesting Fixes)","text":"<p>Purpose: Before suggesting any fix, the AI must diagnose the root cause with evidence.</p> <p>Investigation Sequence:</p> <pre><code># Step 1: Get the error context\nstory_read(pipeline=\"&lt;pipeline&gt;\")  # See which node failed\n\n# Step 2: Understand what the node expects\nnode_describe(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;failed_node&gt;\")\n\n# Step 3: See what data actually arrived\nnode_sample_in(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;failed_node&gt;\", max_rows=20)\n\n# Step 4: Check for validation failures\nnode_failed_rows(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;failed_node&gt;\")\n\n# Step 5: Get AI diagnosis of the error\ndiagnose_error(error_message=\"&lt;exact_error&gt;\")\n\n# Step 6: Trace back to source if needed\nlineage_upstream(pipeline=\"&lt;pipeline&gt;\", node=\"&lt;failed_node&gt;\")\n# Then check those upstream nodes too\n</code></pre> <p>Fix suggestions should include: - The exact root cause (with evidence from data samples) - The specific line/config that needs to change - A validated YAML snippet (run through <code>validate_yaml</code>)</p>"},{"location":"guides/mcp_recipes/#workflow-e-pre-flight-check-before-running-any-pipeline","title":"Workflow E: \"Pre-Flight Check\" (Before Running Any Pipeline)","text":"<p>Purpose: Validate everything before execution to prevent failures.</p> <pre><code># Step 1: Validate the YAML config\nvalidate_yaml(yaml_content=\"&lt;full_yaml&gt;\")\n\n# Step 2: Verify source data is accessible\nlist_files(connection=\"&lt;source_conn&gt;\", path=\"&lt;source_path&gt;\")\npreview_source(connection=\"&lt;source_conn&gt;\", path=\"&lt;source_file&gt;\", max_rows=5)\n\n# Step 3: For each transformer used, verify parameters\nexplain(name=\"&lt;transformer_1&gt;\")\nexplain(name=\"&lt;transformer_2&gt;\")\n# ...for each transformer in the pipeline\n\n# Step 4: Check the expected output schema\nget_yaml_structure()  # Verify output config is valid\n\n# Step 5: If extending existing pipeline, check compatibility\noutput_schema(pipeline=\"&lt;existing&gt;\", output_name=\"&lt;output&gt;\")\n</code></pre>"},{"location":"guides/mcp_recipes/#workflow-f-new-project-onboarding-complete-context-acquisition","title":"Workflow F: \"New Project Onboarding\" (Complete Context Acquisition)","text":"<p>Purpose: When AI joins a project or user asks for help on an unfamiliar pipeline, gather ALL context first.</p> <p>Full Context Acquisition:</p> <pre><code># Step 1: Framework understanding\nget_deep_context()\n\n# Step 2: Available patterns and their purposes\nlist_patterns()\n\n# Step 3: Available connections (where data lives)\nlist_connections()\n\n# Step 4: Available transformers (what operations exist)\nlist_transformers()\n\n# Step 5: For each pipeline in scope:\nlist_outputs(pipeline=\"&lt;pipeline_1&gt;\")\nlineage_graph(pipeline=\"&lt;pipeline_1&gt;\")\nstory_read(pipeline=\"&lt;pipeline_1&gt;\")\n\n# Step 6: Explore actual source data\nlist_files(connection=\"&lt;primary_source&gt;\", path=\"/\")\npreview_source(connection=\"&lt;primary_source&gt;\", path=\"&lt;key_file&gt;\")\n\n# Step 7: Check documentation for project-specific patterns\nsearch_docs(query=\"&lt;project_name&gt;\")\n</code></pre> <p>AI should build a mental model of: - Where source data comes from (connections, file patterns) - What pipelines exist and their purposes - The flow from sources \u2192 bronze \u2192 silver \u2192 gold - Project-specific conventions or custom transformers - Recent issues or failures to be aware of</p>"},{"location":"guides/mcp_recipes/#anti-patterns-to-avoid","title":"Anti-Patterns to Avoid","text":"\u274c Don't \u2705 Do Instead Guess column names Use <code>preview_source</code> or <code>infer_schema</code> to see actual names Assume transformer parameters Use <code>explain</code> to verify exact parameter names and types Generate YAML without validation Always run <code>validate_yaml</code> before presenting to user Suggest fixes without evidence Use <code>node_sample_in</code> and <code>node_failed_rows</code> to see actual data Skip lineage understanding Use <code>lineage_graph</code> before modifying downstream nodes Assume schema Use <code>output_schema</code> to see actual column types"},{"location":"guides/mcp_recipes/#context-checklist-for-ai","title":"Context Checklist for AI","text":"<p>Before suggesting ANY pipeline changes, verify:</p> <ul> <li>[ ] I have seen the actual source data (<code>preview_source</code>)</li> <li>[ ] I know the exact column names and types (<code>infer_schema</code>)</li> <li>[ ] I understand the pattern being used (<code>explain</code>)</li> <li>[ ] I have validated my YAML (<code>validate_yaml</code>)</li> <li>[ ] I have checked lineage impact (<code>lineage_graph</code>)</li> <li>[ ] I have grounded my response in real data, not assumptions</li> </ul>"},{"location":"guides/mcp_recipes/#best-practices-for-ai-assistants","title":"Best Practices for AI Assistants","text":"<ol> <li>Always start with context - Use <code>story_read</code> or <code>list_outputs</code> to understand the current state</li> <li>Show real data - Use <code>preview_source</code> and <code>node_sample</code> to ground responses in actual data</li> <li>Validate before suggesting - Use <code>validate_yaml</code> before presenting YAML to users</li> <li>Chain tools logically - Debug flows: status \u2192 details \u2192 samples \u2192 diagnosis</li> <li>Use explain liberally - Always clarify transformers/patterns before using them</li> <li>Prefer specific tools - Use <code>explain</code> over <code>get_deep_context</code> for focused questions</li> <li>Never guess - always verify - If unsure about column names, types, or parameters, use the tools to check</li> <li>Build complete mental models - Use the Context-First Workflows before taking action</li> </ol>"},{"location":"guides/mcp_recipes/#recipe-11-analyze-unknown-connection","title":"Recipe 11: \"Analyze Unknown Connection\"","text":"<p>When user says: \"What's in this connection?\", \"Explore [connection]\", \"Analyze [connection]\", \"Catalog [connection]\"</p> <p>Two-Step Pattern: Structure first, samples second.</p> <p>For Storage Connections:</p> <pre><code># Step 1: Discover structure (shallow, no samples by default)\ndiscover_storage(connection=\"&lt;conn&gt;\", path=\"\")\n# Returns: file names, formats, schemas - lightweight response\n\n# Step 2: For interesting files, get samples\npreview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\", max_rows=20)\n\n# Step 3: For Excel files, discover sheets first\nlist_sheets(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;.xlsx\")\npreview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;.xlsx\", sheet=\"&lt;sheet_name&gt;\")\n\n# Optional: Deep scan if needed\ndiscover_storage(connection=\"&lt;conn&gt;\", path=\"\", recursive=true, max_files=50)\n</code></pre> <p>For Database Connections:</p> <pre><code># Step 0: List available schemas FIRST\nlist_schemas(connection=\"&lt;sql_conn&gt;\")\n# Returns: schema names with table counts - know what exists before diving in\n\n# Step 1: Discover structure (shallow, no samples by default)\ndiscover_database(connection=\"&lt;sql_conn&gt;\", schema=\"dbo\")\n# Returns: table names, columns, row counts - lightweight response\n\n# Step 2: For interesting tables, get samples\npreview_source(connection=\"&lt;sql_conn&gt;\", path=\"&lt;table&gt;\", max_rows=20)\n\n# Optional: Include samples if context budget allows\ndiscover_database(connection=\"&lt;sql_conn&gt;\", schema=\"dbo\", sample_rows=3)\n</code></pre> <p>AI should summarize: - File/table inventory (count by type/format) - Key tables/files and their purposes - Column patterns (potential keys, dimensions, facts) - Data quality observations (nulls, duplicates) - Suggested next steps (which tables to load first, recommended patterns)</p>"},{"location":"guides/mcp_recipes/#recipe-12-compare-source-vs-target-schema","title":"Recipe 12: \"Compare Source vs Target Schema\"","text":"<p>When user says: \"Compare schemas\", \"Is source compatible with target?\", \"What's different between...\"</p> <p>Steps:</p> <pre><code># Direct comparison with one call\ncompare_schemas(\n    source_connection=\"&lt;source_conn&gt;\",\n    source_path=\"&lt;source_path&gt;\",\n    target_connection=\"&lt;target_conn&gt;\",\n    target_path=\"&lt;target_path&gt;\",\n    source_sheet=\"&lt;optional_sheet&gt;\",  # For Excel\n    target_sheet=\"&lt;optional_sheet&gt;\"\n)\n</code></pre> <p>Response should include: - Compatibility status (breaking vs non-breaking) - Added columns (in target, not in source) - Removed columns (in source, missing in target - BREAKING) - Type mismatches (BREAKING) - Nullability changes - Suggestions for handling differences</p> <p>Example conversation:</p> <p>User: \"Is my staging table compatible with the production target?\"</p> <p>AI: Let me compare the schemas... [calls compare_schemas with source and target]</p> <p>The schemas have 2 breaking differences: - Column <code>customer_id</code> type changed: <code>string</code> \u2192 <code>int</code> (BREAKING) - Column <code>created_date</code> is missing in target (BREAKING)</p> <p>You'll need to either: 1. Add a type cast in your transform: <code>CAST(customer_id AS INT)</code> 2. Add the missing column to the target table</p>"},{"location":"guides/mcp_recipes/#recipe-13-catalog-a-database","title":"Recipe 13: \"Catalog a Database\"","text":"<p>When user says: \"Catalog [database]\", \"Document all tables in...\", \"Create data dictionary for...\"</p> <p>Steps:</p> <pre><code># Step 0: List available schemas\nlist_schemas(connection=\"&lt;sql_conn&gt;\")\n\n# Step 1: Discover all tables in each schema\ndiscover_database(connection=\"&lt;sql_conn&gt;\", schema=\"dbo\", max_tables=100, sample_rows=3)\n\n# Step 2: For each important table, AI analyzes:\n# - Column names and types\n# - Sample values\n# - Potential keys (columns ending in _id, unique values)\n# - Table purpose (dim_, fact_, stg_, etc.)\n# - Relationships (FK patterns)\n\n# Step 3: Output structured catalog\n</code></pre> <p>AI Output Format:</p> <pre><code>catalog:\n  connection: &lt;connection_name&gt;\n  schema: dbo\n  tables:\n    - name: dim_customer\n      purpose: Customer master data (dimension table)\n      row_count: 15000\n      primary_key: customer_id\n      columns:\n        - name: customer_id\n          type: int\n          role: primary_key\n        - name: customer_name\n          type: varchar\n          role: business_name\n      relationships:\n        - references: fact_orders.customer_id\n\n    - name: fact_orders\n      purpose: Sales transactions (fact table)\n      row_count: 1500000\n      primary_key: order_id\n      foreign_keys:\n        - customer_id \u2192 dim_customer.customer_id\n        - product_id \u2192 dim_product.product_id\n</code></pre>"},{"location":"guides/mcp_recipes/#recipe-14-suggest-pipeline-from-discovered-data","title":"Recipe 14: \"Suggest Pipeline from Discovered Data\"","text":"<p>When user says: \"Build pipeline for what you found\", \"Create pipeline from discovery\", \"Load this as...\"</p> <p>Prerequisites: AI has already run discovery tools and understands the source data.</p> <p>Steps:</p> <pre><code># Step 1: Confirm understanding\npreview_source(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\")\ninfer_schema(connection=\"&lt;conn&gt;\", path=\"&lt;file&gt;\")\n\n# Step 2: Recommend pattern based on data shape\nsuggest_pattern(use_case=\"&lt;inferred from data&gt;\")\n\n# Step 3: Get pattern example\nget_example(pattern_name=\"&lt;suggested_pattern&gt;\")\n\n# Step 4: Find relevant transformers\nlist_transformers()\nexplain(name=\"&lt;transformer_name&gt;\")\n\n# Step 5: Generate YAML\ngenerate_pipeline_yaml(...)  # Or manually construct\n\n# Step 6: Validate before presenting\nvalidate_yaml(yaml_content=\"&lt;generated&gt;\")\n</code></pre> <p>AI should propose: - Pattern choice with reasoning - Input configuration (connection, path, format) - Required transforms (type casts, renames, null handling) - Output configuration - Validation rules (if applicable)</p>"},{"location":"guides/mcp_system_prompt/","title":"Odibi MCP System Prompt","text":"<p>Use this as a system prompt or context injection for AI assistants working with odibi.</p>"},{"location":"guides/mcp_system_prompt/#system-prompt","title":"System Prompt","text":"<pre><code>You are an expert data engineering assistant specialized in the odibi framework. You have access to 46 MCP tools through the \"odibi-knowledge\" server that help you:\n\n## FIRST: Call bootstrap_context()\n\nAt the START of every conversation, call `bootstrap_context()` to auto-gather:\n- Project name and config path\n- All connections and their types\n- All pipelines with their outputs\n- Available patterns and transformer count\n- Critical YAML syntax rules\n- Suggested next steps\n\nThis gives you full project context in ONE call. Then proceed with:\n\n1. **Explore data**: list_files, preview_source, infer_schema, list_tables, describe_table\n2. **Build pipelines**: suggest_pattern, get_example, get_yaml_structure, list_transformers, generate_pipeline_yaml\n3. **Debug runs**: story_read, node_describe, node_sample, node_sample_in, node_failed_rows, diagnose_error\n4. **Understand lineage**: lineage_graph, lineage_upstream, lineage_downstream, list_outputs, output_schema\n5. **Learn odibi**: explain, search_docs, get_doc, get_deep_context, query_codebase\n6. **Monitor health**: pipeline_stats, node_stats, failure_summary, schema_history\n7. **Validate configs**: validate_yaml, get_yaml_structure\n\n## CONTEXT-FIRST RULE (MANDATORY)\n\n**Before suggesting ANY solution, you MUST gather full context first.**\n\n### Before Building Anything:\n1. `list_files` \u2192 discover what files exist\n2. `preview_source` \u2192 see ACTUAL data values (not assumed)\n3. `infer_schema` \u2192 get exact column names &amp; types\n\n**ONLY AFTER seeing real data** may you suggest patterns or generate YAML.\n\n### Before Modifying Anything:\n1. `list_outputs` \u2192 what does this pipeline produce?\n2. `output_schema` \u2192 what's the schema?\n3. `lineage_graph` \u2192 how do nodes connect?\n4. `node_describe` \u2192 what does the target node do?\n5. `story_read` \u2192 recent run history\n\n### Before Suggesting Solutions:\n1. `explain(name)` \u2192 verify your understanding of transformer/pattern\n2. `get_example` \u2192 get working example\n3. `get_yaml_structure` \u2192 verify YAML structure\n\n**NEVER guess transformer parameters \u2014 always use `explain` first.**\n\n### Before Suggesting Fixes:\n1. `story_read` \u2192 which node failed?\n2. `node_sample_in` \u2192 what data actually arrived?\n3. `node_failed_rows` \u2192 what rows failed validation?\n4. `diagnose_error` \u2192 get AI diagnosis\n\n## Anti-Patterns (NEVER DO)\n\n- \u274c Guess column names \u2192 \u2705 Use `preview_source` or `infer_schema`\n- \u274c Assume transformer params \u2192 \u2705 Use `explain` to verify\n- \u274c Generate YAML without validation \u2192 \u2705 Run `validate_yaml`\n- \u274c Suggest fixes without evidence \u2192 \u2705 Use `node_sample_in` to see data\n- \u274c Skip lineage understanding \u2192 \u2705 Use `lineage_graph`\n\n## Your Workflow\n\nWhen helping users:\n\n1. **ALWAYS gather context FIRST** - use the workflows above before acting\n2. **Chain tools together** - explore data \u2192 suggest pattern \u2192 generate YAML \u2192 validate\n3. **Show real samples** - use preview_source and node_sample to ground your responses\n4. **Validate before presenting** - always run validate_yaml on generated configs\n5. **Explain your reasoning** - tell users which tools you're using and why\n\n## Exploration Mode\n\nIf the project only has connections (no pipelines), you're in **exploration mode**:\n- Discovery tools work: `list_files`, `preview_source`, `list_tables`, `describe_table`, `infer_schema`, `list_sheets`\n- Story/lineage tools return \"exploration mode active\" - that's expected\n- Use this mode to explore data before building pipelines\n\n## Quick Tool Reference\n\n- Need to understand a feature? \u2192 `explain(name=\"...\")`\n- Building a new pipeline? \u2192 `suggest_pattern()` \u2192 `get_example()` \u2192 `validate_yaml()`\n- Debugging a failure? \u2192 `story_read()` \u2192 `node_describe()` \u2192 `node_sample_in()` \u2192 `diagnose_error()`\n- Exploring new data? \u2192 `list_files()` \u2192 `preview_source()` \u2192 `infer_schema()`\n- Checking pipeline health? \u2192 `pipeline_stats()` \u2192 `failure_summary()`\n\n## Critical YAML Rules (Never Violate)\n\n- Use `inputs:` and `outputs:` (NEVER `source:` or `sink:`)\n- Use `transform.steps[].function` + `params` (NEVER `transform: [name]`)\n- Node names: alphanumeric + underscore ONLY (no hyphens, dots, spaces)\n- ALWAYS specify `format: csv|parquet|json|delta`\n\n## Response Style\n\n- Be concise and direct\n- Show real data samples when relevant\n- Present YAML in proper code blocks\n- Explain complex patterns with examples\n- Proactively suggest next steps\n</code></pre>"},{"location":"guides/mcp_system_prompt/#continue-rules-file","title":"Continue Rules File","text":"<p>Add to your project's <code>.continuerules</code>:</p> <pre><code># Odibi Development Assistant Rules\n\n## MCP Usage\n- Always use odibi-knowledge MCP tools for data exploration\n- Use preview_source before building pipelines to understand the data\n- Validate all generated YAML with validate_yaml before presenting\n- Use diagnose_error when users report failures\n\n## Pipeline Development\n- Start with suggest_pattern to recommend the right approach\n- Use get_example to show working YAML templates\n- Chain: explore data \u2192 suggest pattern \u2192 generate yaml \u2192 validate\n\n## Debugging\n- Check story_read first to see run status\n- Use node_sample_in to see what data the node received\n- Use node_failed_rows to see validation failures\n- Always provide actionable fix suggestions\n\n## Code Style\n- Follow existing odibi conventions for new code\n- Use Pydantic models for configuration\n- Register transformers with @register_function decorator\n</code></pre>"},{"location":"guides/mcp_system_prompt/#vs-code-snippets","title":"VS Code Snippets","text":"<p>Add to <code>.vscode/odibi.code-snippets</code>:</p> <pre><code>{\n  \"Odibi Debug Pipeline\": {\n    \"prefix\": \"odibi-debug\",\n    \"body\": [\n      \"Use odibi MCP story_read with pipeline=\\\"${1:pipeline_name}\\\"\",\n      \"Use odibi MCP node_describe with pipeline=\\\"${1:pipeline_name}\\\", node=\\\"${2:node_name}\\\"\",\n      \"Use odibi MCP node_sample_in with pipeline=\\\"${1:pipeline_name}\\\", node=\\\"${2:node_name}\\\", max_rows=10\"\n    ],\n    \"description\": \"Debug an odibi pipeline\"\n  },\n  \"Odibi Explore Data\": {\n    \"prefix\": \"odibi-explore\",\n    \"body\": [\n      \"Use odibi MCP list_files with connection=\\\"${1:connection}\\\", path=\\\"${2:path}\\\", pattern=\\\"*\\\"\",\n      \"Use odibi MCP preview_source with connection=\\\"${1:connection}\\\", path=\\\"${3:file}\\\", max_rows=10\",\n      \"Use odibi MCP infer_schema with connection=\\\"${1:connection}\\\", path=\\\"${3:file}\\\"\"\n    ],\n    \"description\": \"Explore data in an odibi connection\"\n  },\n  \"Odibi Build Pipeline\": {\n    \"prefix\": \"odibi-build\",\n    \"body\": [\n      \"Use odibi MCP suggest_pattern with use_case=\\\"${1:description}\\\"\",\n      \"Use odibi MCP get_example with pattern_name=\\\"${2:pattern}\\\"\",\n      \"Use odibi MCP get_yaml_structure\"\n    ],\n    \"description\": \"Start building an odibi pipeline\"\n  }\n}\n</code></pre>"},{"location":"guides/mcp_system_prompt/#example-conversations","title":"Example Conversations","text":""},{"location":"guides/mcp_system_prompt/#building-a-pipeline","title":"Building a Pipeline","text":"<p>User: I have customer data in Azure and need to track changes over time</p> <p>AI: I'll help you build that. Let me first check what pattern fits best...</p> <p>[Uses suggest_pattern with use_case=\"track customer changes over time\"]</p> <p>The SCD2 pattern is ideal for tracking historical changes. Let me show you an example:</p> <p>[Uses get_example with pattern_name=\"scd2\"]</p> <p>Here's the structure you need...</p>"},{"location":"guides/mcp_system_prompt/#debugging-a-failure","title":"Debugging a Failure","text":"<p>User: My pipeline failed with \"KeyError: 'amount'\"</p> <p>AI: Let me investigate the failure...</p> <p>[Uses story_read to find the failed node]</p> <p>I see <code>transform_sales</code> failed. Let me check what data it received:</p> <p>[Uses node_sample_in with node=\"transform_sales\"]</p> <p>The issue is clear - the source column is <code>Amount</code> (capitalized), but your config references <code>amount</code>.</p> <p>Fix: Change your SQL to <code>SELECT Amount as amount</code> or update your transform to use <code>Amount</code>.</p>"},{"location":"guides/mcp_system_prompt/#exploring-new-data","title":"Exploring New Data","text":"<p>User: What's in the raw data folder?</p> <p>AI: [Uses list_files with path=\"raw/\"]</p> <p>I found 3 CSV files: - customers.csv - orders.csv - products.csv</p> <p>Let me preview the customers file:</p> <p>[Uses preview_source with path=\"raw/customers.csv\"]</p> <p>It has 5 columns: id, name, email, created_at, status...</p>"},{"location":"guides/medallion_architecture/","title":"Medallion Architecture Guide","text":"<p>A beginner-friendly guide to understanding Bronze, Silver, and Gold data layers.</p>"},{"location":"guides/medallion_architecture/#what-is-medallion-architecture","title":"What is Medallion Architecture?","text":"<p>Medallion Architecture organizes your data into three layers, like refining raw ore into polished gold:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                     \u2502\n\u2502   Source Systems          Bronze           Silver           Gold   \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500   \u2502\n\u2502                                                                     \u2502\n\u2502   [SQL Server] \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  [Raw Copy]  \u2500\u2500\u2500\u25ba  [Cleaned]  \u2500\u2500\u2500\u25ba  [Facts] \u2502\n\u2502   [API]        \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  [Raw Copy]  \u2500\u2500\u2500\u25ba  [Cleaned]  \u2500\u2500\u2500\u25ba  [Dims]  \u2502\n\u2502   [Files]      \u2500\u2500\u2500\u2500\u2500\u2500\u25ba  [Raw Copy]  \u2500\u2500\u2500\u25ba  [Cleaned]  \u2500\u2500\u2500\u25ba  [KPIs]  \u2502\n\u2502                                                                     \u2502\n\u2502   \"Just land it\"      \"Fix it\"         \"Make it                    \u2502\n\u2502                                          business-ready\"           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Think of it like cooking: - Bronze = Raw ingredients from the store (as-is, untouched) - Silver = Ingredients washed, chopped, and prepped (cleaned, standardized) - Gold = The finished dish ready to serve (combined, calculated, business-ready)</p>"},{"location":"guides/medallion_architecture/#layer-1-bronze-raw-data","title":"Layer 1: Bronze (Raw Data)","text":""},{"location":"guides/medallion_architecture/#purpose","title":"Purpose","text":"<p>Land data exactly as it comes from the source. No transformations. Just copy it.</p>"},{"location":"guides/medallion_architecture/#what-happens-here","title":"What Happens Here","text":"Operation Example Why Raw ingestion Copy SQL table to Delta Preserve original data Add metadata <code>_extracted_at</code> timestamp Track when data arrived Schema preservation Keep all columns, even unused Don't lose anything"},{"location":"guides/medallion_architecture/#what-does-not-happen-here","title":"What Does NOT Happen Here","text":"<p>\u274c No cleaning \u274c No transformations \u274c No joins \u274c No filtering (except maybe date ranges for incremental loads)</p>"},{"location":"guides/medallion_architecture/#example-bronze-node","title":"Example Bronze Node","text":"<pre><code>- name: bronze_sales_orders\n  read:\n    connection: erp_database\n    table: dbo.SalesOrders\n  write:\n    connection: datalake\n    path: bronze/sales_orders\n    format: delta\n</code></pre>"},{"location":"guides/medallion_architecture/#the-golden-rule-of-bronze","title":"The Golden Rule of Bronze","text":"<p>\"If the source system has garbage data, Bronze has garbage data. That's okay.\"</p>"},{"location":"guides/medallion_architecture/#layer-2-silver-cleaned-conformed","title":"Layer 2: Silver (Cleaned &amp; Conformed)","text":""},{"location":"guides/medallion_architecture/#purpose_1","title":"Purpose","text":"<p>Clean and standardize ONE source at a time. Make it trustworthy.</p>"},{"location":"guides/medallion_architecture/#the-key-question","title":"The Key Question","text":"<p>\"Could this node run if only ONE source system existed?\"</p> <p>If YES \u2192 Silver \u2713 If NO \u2192 Probably Gold</p> <p>Reference Tables Are Allowed in Silver</p> <p>The One-Source Test refers to business source systems, not reference/lookup data.</p> <p>Silver CAN join with:</p> <ul> <li>Reference/lookup tables (code mappings, static lists)</li> <li>Dimension lookups for enrichment (product_code \u2192 product_name)</li> <li>Self-joins within the same source</li> </ul> <p>Silver should NOT join:</p> <ul> <li>Multiple business source systems (SAP + Salesforce \u2192 use Gold)</li> </ul>"},{"location":"guides/medallion_architecture/#what-happens-here_1","title":"What Happens Here","text":""},{"location":"guides/medallion_architecture/#1-data-cleaning","title":"1. Data Cleaning","text":"<p>Fixing problems in the source data.</p> Operation Example Category Deduplication <code>ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC)</code> Cleaning Remove bad characters <code>REPLACE(name, '\"', '')</code> Cleaning Fix typos <code>REPLACE(status, 'Actve', 'Active')</code> Cleaning Handle nulls <code>COALESCE(middle_name, '')</code> Cleaning Trim whitespace <code>TRIM(customer_name)</code> Cleaning <pre><code>-- Example: Cleaning product codes\nCASE\n    WHEN LEFT(REPLACE(product_code, '\"', ''), 1) = 'X'\n    THEN SUBSTRING(REPLACE(product_code, '\"', ''), 2)\n    ELSE REPLACE(product_code, '\"', '')\nEND AS product_code\n</code></pre>"},{"location":"guides/medallion_architecture/#2-type-casting-standardization","title":"2. Type Casting &amp; Standardization","text":"<p>Making data types consistent.</p> Operation Example Category Cast types <code>CAST(date_string AS DATE)</code> Standardization Parse timestamps <code>to_timestamp(date_col)</code> Standardization Unit conversion <code>hours * 60 AS duration_minutes</code> Standardization Standardize casing <code>UPPER(country_code)</code> Standardization <pre><code>-- Example: Standardizing dates\nto_timestamp(order_date) AS order_date,\nDATEDIFF(to_date(order_date), to_date('2020-01-01')) + 1 AS date_id\n</code></pre>"},{"location":"guides/medallion_architecture/#3-conforming-to-standard-schema","title":"3. Conforming to Standard Schema","text":"<p>Mapping source-specific values to enterprise-standard values.</p> Operation Example Category Code mapping <code>'M1' \u2192 'Machine 1'</code> Conforming Category standardization <code>'Sched%' \u2192 'Scheduled'</code> Conforming Rename columns <code>cust_id AS customer_id</code> Conforming Add source context <code>'West Region' AS region_name</code> Conforming <pre><code>-- Example: Mapping source codes to standard names\nCASE\n    WHEN machine_code = 'M1' THEN 'Machine 1'\n    WHEN machine_code = 'M2' THEN 'Machine 2'\n    WHEN machine_code = 'M3' THEN 'Machine 3'\nEND AS machine_name,\n\nCASE\n    WHEN category LIKE '%Sched%' THEN 'Scheduled'\n    WHEN category LIKE '%Maint%' THEN 'Maintenance'\n    WHEN category LIKE '%Breakdown%' THEN 'Unplanned'\n    ELSE 'Other'\nEND AS downtime_category\n</code></pre>"},{"location":"guides/medallion_architecture/#4-enrichment-via-lookups","title":"4. Enrichment via Lookups","text":"<p>Adding dimension attributes from reference tables.</p> Operation Example Category Join to calendar Get <code>date_id</code> from date dimension Enrichment Join to location Get <code>location_id</code> from location dimension Enrichment Join to reason codes Get <code>reason_id</code> from reason lookup Enrichment Join to product master Get <code>product_name</code> from product dim Enrichment <pre><code>-- Example: Enriching with dimension lookups\nSELECT\n    e.event_id,\n    e.event_date,\n    e.duration_minutes,\n    r.reason_id,           -- From reason code lookup\n    l.location_id          -- From location dimension\nFROM events e\nLEFT JOIN reason_codes r\n    ON r.category = e.category \nLEFT JOIN dim_location l\n    ON e.site_code = l.site_code\n</code></pre>"},{"location":"guides/medallion_architecture/#5-soft-delete-detection","title":"5. Soft Delete Detection","text":"<p>Tracking records that exist in Bronze but no longer exist in the source.</p> Operation Example Category Compare snapshots Find missing keys Delete Detection Flag deleted records <code>_is_deleted = true</code> Delete Detection Filter active records <code>WHERE _is_deleted = false</code> Delete Detection"},{"location":"guides/medallion_architecture/#what-does-not-happen-here_1","title":"What Does NOT Happen Here","text":"<p>\u274c Combining data from multiple source systems \u274c Business calculations (like KPIs, ratios) \u274c Aggregations for reporting \u274c Creating facts that span multiple sources</p>"},{"location":"guides/medallion_architecture/#example-silver-node","title":"Example Silver Node","text":"<pre><code>- name: cleaned_warehouse_events\n  inputs:\n    input_name: $bronze.warehouse_event_log\n  depends_on:\n    - cleaned_reason_codes     # Lookup table\n    - cleaned_dim_location     # Dimension table\n  transformer: deduplicate\n  params:\n    keys: [event_id]\n    order_by: \"_extracted_at DESC\"\n  transform:\n    steps:\n      - sql: |\n          SELECT\n              to_timestamp(event_date) AS event_date,\n              DATEDIFF(to_date(event_date), '2020-01-01') + 1 AS date_id,\n              'Warehouse A' AS location_name,\n              CASE WHEN machine = 'M1' THEN 'Machine 1' ... END AS machine_name,\n              duration_hours * 60 AS duration_minutes\n          FROM df\n      - function: detect_deletes\n        params:\n          mode: sql_compare\n          keys: [event_id]\n</code></pre>"},{"location":"guides/medallion_architecture/#the-golden-rule-of-silver","title":"The Golden Rule of Silver","text":"<p>\"One source in, one cleaned version out. The output should be the best possible version of that single source.\"</p>"},{"location":"guides/medallion_architecture/#layer-3-gold-business-ready","title":"Layer 3: Gold (Business-Ready)","text":""},{"location":"guides/medallion_architecture/#purpose_2","title":"Purpose","text":"<p>Combine cleaned Silver data into business-meaningful outputs.</p>"},{"location":"guides/medallion_architecture/#the-key-question_1","title":"The Key Question","text":"<p>\"Does this require data from MULTIPLE Silver sources?\"</p> <p>If YES \u2192 Gold \u2713 If NO \u2192 Probably Silver</p>"},{"location":"guides/medallion_architecture/#what-happens-here_2","title":"What Happens Here","text":""},{"location":"guides/medallion_architecture/#1-combining-multiple-sources-union","title":"1. Combining Multiple Sources (UNION)","text":"<p>Merging the same type of data from different systems.</p> Operation Example Category Union facts Combine events from System A + System B + System C Combining Reconciliation UNION (not UNION ALL) to dedupe across sources Combining Cross-system dedup Same event recorded in multiple systems Combining <pre><code>-- Example: Combining events from all sources\nSELECT date_id, location_id, duration_minutes, notes\nFROM cleaned_system_a_events\n\nUNION ALL\n\nSELECT date_id, location_id, duration_minutes, notes\nFROM cleaned_system_b_events\n\nUNION ALL\n\nSELECT date_id, location_id, duration_minutes, notes\nFROM cleaned_system_c_events\n</code></pre>"},{"location":"guides/medallion_architecture/#2-business-calculations","title":"2. Business Calculations","text":"<p>Applying business definitions and formulas.</p> Operation Example Category Define metrics <code>total_output = COALESCE(revised_qty, original_qty)</code> Business Rule Calculate KPIs <code>efficiency = actual_output / expected_output * 100</code> Business Rule Apply business logic \"If negative, treat as zero\" Business Rule Default values \"Use default reason if null and duration &lt; 10 min\" Business Rule <pre><code>-- Example: Business definition of Total Output\nCOALESCE(\n    CASE\n        WHEN COALESCE(revised_quantity, original_quantity) &lt;= 0 THEN 0\n        ELSE COALESCE(revised_quantity, original_quantity)\n    END, \n0) AS total_output\n</code></pre> <p>This is Gold because it answers: \"What does 'output' MEAN to the business?\"</p>"},{"location":"guides/medallion_architecture/#3-cross-fact-joins","title":"3. Cross-Fact Joins","text":"<p>Joining multiple fact tables together.</p> Operation Example Category Join facts Production + Downtime + Quality \u2192 Efficiency Cross-Fact Build wide tables Denormalized reporting tables Cross-Fact Calculate ratios Downtime / Available Hours Cross-Fact <pre><code>-- Example: Joining facts for efficiency calculation\nSELECT \n    c.date_id,\n    c.location_id,\n    p.total_output,\n    d.downtime_minutes,\n    q.defect_count,\n    -- Efficiency uses multiple facts\n    (p.total_output / p.target_output) * 100 AS efficiency_pct\nFROM calendar_scaffold c\nLEFT JOIN combined_production p \n    ON c.date_id = p.date_id AND c.location_id = p.location_id\nLEFT JOIN combined_downtime d \n    ON c.date_id = d.date_id AND c.location_id = d.location_id\nLEFT JOIN combined_quality q \n    ON c.date_id = q.date_id AND c.location_id = q.location_id\n</code></pre>"},{"location":"guides/medallion_architecture/#4-aggregations-for-reporting","title":"4. Aggregations for Reporting","text":"<p>Pre-computing summaries for dashboards and reports.</p> Operation Example Category Daily rollups SUM(production) GROUP BY date, location Aggregation Weekly summaries AVG(efficiency) by week Aggregation YTD calculations Running totals Aggregation"},{"location":"guides/medallion_architecture/#5-derived-dimensions","title":"5. Derived Dimensions","text":"<p>Creating dimensions that don't exist in source systems.</p> Operation Example Category Date spine Calendar \u00d7 Locations for all combinations Derived Dim Distinct lists All locations with any activity Derived Dim <pre><code>-- Example: Create all Date \u00d7 Location combinations\nSELECT *\nFROM dim_calendar\nCROSS JOIN distinct_locations\n</code></pre>"},{"location":"guides/medallion_architecture/#example-gold-node","title":"Example Gold Node","text":"<pre><code>- name: fact_daily_efficiency\n  description: \"Daily efficiency metrics by location\"\n  depends_on:\n    - combined_production   # Multiple sources unioned\n    - combined_downtime     # Multiple sources unioned\n    - combined_quality      # Multiple sources unioned\n    - calendar_scaffold     # Date \u00d7 Location scaffold\n  transform:\n    steps:\n      - sql: |\n          SELECT\n              c.date_id,\n              c.location_id,\n              COALESCE(p.total_output, 0) AS output_units,\n              COALESCE(d.downtime_minutes, 0) AS downtime_min,\n              COALESCE(q.defect_count, 0) AS defects,\n              -- Efficiency Calculation (Business Formula)\n              CASE \n                  WHEN p.target_output &gt; 0 \n                  THEN (p.total_output / p.target_output) * 100\n                  ELSE 0 \n              END AS efficiency_pct\n          FROM calendar_scaffold c\n          LEFT JOIN combined_production p \n              ON c.date_id = p.date_id AND c.location_id = p.location_id\n          LEFT JOIN combined_downtime d \n              ON c.date_id = d.date_id AND c.location_id = d.location_id\n          LEFT JOIN combined_quality q \n              ON c.date_id = q.date_id AND c.location_id = q.location_id\n  write:\n    connection: gold\n    table: fact_daily_efficiency\n    format: delta\n</code></pre>"},{"location":"guides/medallion_architecture/#the-golden-rule-of-gold","title":"The Golden Rule of Gold","text":"<p>\"This is what the business sees. Every row and column should have business meaning.\"</p>"},{"location":"guides/medallion_architecture/#quick-reference-where-does-this-belong","title":"Quick Reference: Where Does This Belong?","text":""},{"location":"guides/medallion_architecture/#by-operation-type","title":"By Operation Type","text":"Operation Bronze Silver Gold Raw ingestion \u2713 Add <code>_extracted_at</code> \u2713 Deduplication \u2713 Remove bad characters \u2713 Fix typos \u2713 Type casting \u2713 Unit conversion \u2713 Map codes to standard names \u2713 Join to dimension/lookup tables \u2713 Soft delete detection \u2713 UNION multiple sources \u2713 Business calculations \u2713 Cross-fact joins \u2713 Aggregations for reporting \u2713 KPI definitions \u2713"},{"location":"guides/medallion_architecture/#by-question","title":"By Question","text":"Question Layer \"How do I get data from the source?\" Bronze \"How do I fix this source's data quality issues?\" Silver \"How do I standardize this source to our schema?\" Silver \"How do I look up IDs from a dimension table?\" Silver \"How do I combine data from System A + B + C?\" Gold \"What does 'Total Output' mean to the business?\" Gold \"How do I calculate efficiency?\" Gold \"What should the dashboard show?\" Gold"},{"location":"guides/medallion_architecture/#the-one-source-test","title":"The One-Source Test","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                             \u2502\n\u2502   \"Could this node work with only ONE source system?\"       \u2502\n\u2502                                                             \u2502\n\u2502   YES \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba SILVER                \u2502\n\u2502    \u2502                                                        \u2502\n\u2502    \u2502   Examples:                                            \u2502\n\u2502    \u2502   \u2022 Cleaning System A data                             \u2502\n\u2502    \u2502   \u2022 Joining System B data to calendar dimension        \u2502\n\u2502    \u2502   \u2022 Mapping System C codes to standard categories      \u2502\n\u2502    \u2502                                                        \u2502\n\u2502   NO \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba GOLD                  \u2502\n\u2502    \u2502                                                        \u2502\n\u2502    \u2502   Examples:                                            \u2502\n\u2502    \u2502   \u2022 Combining all event sources                        \u2502\n\u2502    \u2502   \u2022 Calculating efficiency from production + downtime  \u2502\n\u2502    \u2502   \u2022 Creating unified fact tables                       \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/medallion_architecture/#common-mistakes","title":"Common Mistakes","text":""},{"location":"guides/medallion_architecture/#mistake-1-business-logic-in-silver","title":"\u274c Mistake 1: Business Logic in Silver","text":"<pre><code># WRONG - Business calculation in Silver\n- name: cleaned_production\n  transform:\n    steps:\n      - sql: |\n          SELECT \n              *,\n              (actual / target) * 100 AS efficiency  -- Business formula!\n          FROM df\n</code></pre> <p>Why it's wrong: Efficiency is a business definition. Silver should just clean the data.</p> <p>Fix: Move the efficiency calculation to Gold.</p>"},{"location":"guides/medallion_architecture/#mistake-2-raw-data-in-silver","title":"\u274c Mistake 2: Raw Data in Silver","text":"<pre><code># WRONG - No Bronze layer, reading directly from source\n- name: cleaned_orders\n  read:\n    connection: erp\n    table: dbo.Orders  # Reading directly from source!\n  transform:\n    steps:\n      - sql: SELECT * FROM df WHERE status != 'DELETED'\n</code></pre> <p>Why it's wrong: If the source changes, you lose the original data.</p> <p>Fix: Add a Bronze layer that preserves the raw data first.</p>"},{"location":"guides/medallion_architecture/#mistake-3-combining-sources-in-silver","title":"\u274c Mistake 3: Combining Sources in Silver","text":"<pre><code># WRONG - UNION in Silver\n- name: cleaned_all_events\n  depends_on:\n    - cleaned_system_a_events\n    - cleaned_system_b_events\n  transform:\n    steps:\n      - sql: |\n          SELECT * FROM cleaned_system_a_events\n          UNION ALL\n          SELECT * FROM cleaned_system_b_events  -- Combining sources!\n</code></pre> <p>Why it's wrong: Silver should process one source at a time.</p> <p>Fix: Move the UNION to a Gold layer node.</p>"},{"location":"guides/medallion_architecture/#project-structure-example","title":"Project Structure Example","text":"<pre><code>pipelines/\n\u251c\u2500\u2500 bronze/\n\u2502   \u2514\u2500\u2500 bronze.yaml\n\u2502       # Nodes: bronze_system_a, bronze_system_b, bronze_system_c\n\u2502\n\u251c\u2500\u2500 silver/\n\u2502   \u2514\u2500\u2500 silver.yaml\n\u2502       # Nodes: cleaned_system_a, cleaned_system_b, cleaned_system_c\n\u2502       # Each cleans ONE source\n\u2502\n\u2514\u2500\u2500 gold/\n    \u2514\u2500\u2500 gold.yaml\n        # Nodes: combined_events, combined_production, fact_daily_efficiency\n        # Combines Silver outputs, applies business logic\n</code></pre>"},{"location":"guides/medallion_architecture/#summary","title":"Summary","text":"Layer Input Output Key Activities Bronze Source systems Raw copy Ingest, add metadata Silver Bronze (one source) Cleaned version Clean, standardize, enrich with lookups Gold Silver (multiple sources) Business facts Combine, calculate, aggregate <p>Remember: - Bronze = \"Land it as-is\" - Silver = \"Clean this ONE source\" - Gold = \"Combine and calculate for business\"</p>"},{"location":"guides/performance_tuning/","title":"Performance Tuning Guide","text":"<p>Odibi v2.2 introduces a \"High-Performance Core\" designed to handle everything from local laptop development to petabyte-scale Spark jobs. This guide explains the optimizations available and how to use them.</p>"},{"location":"guides/performance_tuning/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>For 90% of users, just add this to your <code>odibi.yaml</code>:</p> <pre><code>performance:\n  use_arrow: true  # Massive speedup for Pandas I/O\n</code></pre>"},{"location":"guides/performance_tuning/#1-pandas-engine-optimizations","title":"1. Pandas Engine Optimizations","text":"<p>The Pandas engine is designed for speed on a single machine.</p>"},{"location":"guides/performance_tuning/#apache-arrow-backend-use_arrow-true","title":"\ud83c\udff9 Apache Arrow Backend (<code>use_arrow: true</code>)","text":"<p>What it does: Replaces standard NumPy memory layout with Apache Arrow. Arrow is a columnar memory format that allows \"Zero-Copy\" data transfer.</p> <p>Why use it? -   Speed: Reading Parquet files becomes nearly instant because the data maps directly from disk to memory without conversion overhead. -   Memory: Reduces RAM usage by ~50% for string-heavy datasets (no more Python objects for strings).</p> <p>Configuration: <pre><code># odibi.yaml\nperformance:\n  use_arrow: true\n</code></pre></p>"},{"location":"guides/performance_tuning/#parallel-file-io-multi-threading","title":"\u26a1 Parallel File I/O (Multi-Threading)","text":"<p>What it does: When reading multiple files (e.g., <code>path: data/sales_*.csv</code>), Odibi now uses a thread pool to read them in parallel instead of one by one.</p> <p>Why use it? Pandas is normally single-threaded. If you have 8 CPU cores, reading 50 CSV files sequentially wastes 7 of them. Parallel I/O saturates your CPU/Disk bandwidth for linear speedups.</p> <p>How to use: Automatic! Just use a glob pattern in your path: <pre><code>read:\n  path: raw/data_*.csv  # &lt;--- Parallel reading activates automatically\n</code></pre></p>"},{"location":"guides/performance_tuning/#2-spark-engine-optimizations","title":"2. Spark Engine Optimizations","text":"<p>The Spark engine focuses on \"Data Layout\" optimizations\u2014making sure downstream queries are fast.</p>"},{"location":"guides/performance_tuning/#liquid-clustering-cluster_by","title":"\ud83d\udca7 Liquid Clustering (<code>cluster_by</code>)","text":"<p>What it does: Replaces traditional Hive-style partitioning (<code>year=2023/month=01</code>) with a flexible, dynamic clustering system. It physically groups related data together in the files.</p> <p>Why use it? -   No \"Small File\" Problem: Traditional partitioning creates too many tiny files if you pick the wrong column (e.g., <code>user_id</code>). Liquid handles this automatically. -   Skew Resistance: Handles uneven data (e.g., 90% of users in US, 1% in JP) without performance cliffs. -   Query Speed: Massive data skipping. Queries filtering by clustered columns skip 99% of the file scans.</p> <p>How to use: Add <code>cluster_by</code> to your write node. If the table doesn't exist, Odibi creates it with clustering enabled.</p> <pre><code>- name: write_sales\n  write:\n    table: silver.sales\n    mode: append\n    options:\n      cluster_by: [region, date]  # &lt;--- Enables Liquid Clustering\n      optimize_write: true        # &lt;--- Keeps clustering healthy\n</code></pre>"},{"location":"guides/performance_tuning/#auto-optimization-optimize_write","title":"\ud83e\uddf9 Auto-Optimization (<code>optimize_write</code>)","text":"<p>What it does: Runs the Delta Lake <code>OPTIMIZE</code> command immediately after a write/merge operation.</p> <p>Why use it? Streaming and frequent batch jobs create \"small files\" (fragmentation) which kill read performance. This option compacts them into larger, efficient files (Bin-packing) and enforces clustering (Z-Order/Liquid).</p> <p>Configuration: <pre><code># In a standard Write node\noptions:\n  optimize_write: true\n\n# In a Merge Transformer\nparams:\n  optimize_write: true\n</code></pre></p>"},{"location":"guides/performance_tuning/#streaming-support","title":"\ud83c\udf0a Streaming Support","text":"<p>What it does: Allows you to switch from Batch (<code>read</code>/<code>write</code>) to Streaming (<code>readStream</code>/<code>writeStream</code>) with a single flag.</p> <p>Why use it? For real-time latency or processing infinite datasets (Kafka, Auto-Loader) without managing state manually.</p> <p>How to use: <pre><code>- name: read_stream\n  read:\n    streaming: true  # &lt;--- Activates Spark Structured Streaming\n    format: cloudFiles\n    path: raw_landing/\n</code></pre></p>"},{"location":"guides/performance_tuning/#3-polars-engine-optimizations","title":"3. Polars Engine Optimizations","text":"<p>The Polars engine is a lightweight, fast alternative to Pandas with native Rust performance.</p>"},{"location":"guides/performance_tuning/#lazy-execution","title":"\ud83e\uddba Lazy Execution","text":"<p>What it does: Polars uses a lazy execution model where queries are not executed until you call <code>.collect()</code>. This allows the query optimizer to reorder, combine, and skip operations.</p> <p>Why use it? -   Query Optimization: Predicate pushdown, projection pruning, and filter hoisting happen automatically. -   Memory Efficiency: Only columns you need are loaded into memory. -   Performance: Often 5-10x faster than Pandas for analytical workloads.</p> <p>How to use: Set your engine to Polars and Odibi handles lazy evaluation automatically: <pre><code>engine: polars\n</code></pre></p>"},{"location":"guides/performance_tuning/#scan-methods-streaming-large-files","title":"\ud83d\udcc2 Scan Methods (Streaming Large Files)","text":"<p>What it does: Uses <code>scan_csv</code>, <code>scan_parquet</code>, and <code>scan_ndjson</code> to read files lazily without loading them entirely into memory.</p> <p>Why use it? Process files larger than RAM by streaming them in chunks.</p> <p>Automatic: Odibi's Polars engine uses scan methods by default for supported formats.</p>"},{"location":"guides/performance_tuning/#native-parallelism","title":"\u26a1 Native Parallelism","text":"<p>What it does: Polars uses all available CPU cores automatically\u2014no configuration needed.</p> <p>Why use it? Unlike Pandas (single-threaded), Polars parallelizes operations like groupby, join, and filter across all cores.</p>"},{"location":"guides/performance_tuning/#summary-cheat-sheet","title":"Summary Cheat Sheet","text":"Optimization Engine Use Case Impact <code>use_arrow: true</code> Pandas Local processing, large Parquet files High (Speed + Memory) Parallel I/O Pandas Reading split CSV/JSON files High (Linear I/O speedup) <code>cluster_by</code> Spark High-cardinality filters, skewed data High (Read performance) <code>optimize_write</code> Spark Frequent writes, streaming, \"small files\" High (Prevents degradation) <code>streaming: true</code> Spark Real-time ingestion Architectural Lazy Execution Polars Analytical workloads, large datasets High (Speed + Memory) Scan Methods Polars Files larger than RAM High (Streaming) Native Parallelism Polars Multi-core utilization High (Automatic)"},{"location":"guides/planning_walkthroughs/","title":"Data Engineering Planning Walkthroughs","text":"<p>\"30 minutes of planning saves 3 hours of debugging.\"</p> <p>Professional-grade Excel workbooks that guide you through planning data pipelines before writing code. By the time you finish filling them out, writing YAML is just typing.</p>"},{"location":"guides/planning_walkthroughs/#download-the-walkthroughs","title":"Download the Walkthroughs","text":"Workbook Sheets Download Bronze 7 sheets (B0-B5 + Reference) <code>Bronze_Walkthrough.xlsx</code> Silver 6 sheets (S0-S4 + Reference) <code>Silver_Walkthrough.xlsx</code> Gold 10 sheets (G0-G8 + Reference) <code>Gold_Walkthrough.xlsx</code> <p>Or regenerate them locally:</p> <pre><code>python scripts/create_de_walkthroughs.py\n</code></pre>"},{"location":"guides/planning_walkthroughs/#philosophy","title":"Philosophy","text":"<p>Plan first, code second.</p> Traditional Approach Walkthrough Approach Jump into code Answer hard questions upfront Hit edge cases Document decisions Backtrack Write code once Discover missing requirements Ship"},{"location":"guides/planning_walkthroughs/#the-three-layers","title":"The Three Layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GOLD    Business sees this. Dimensions, Facts, KPIs.       \u2502\n\u2502          Combines sources. Applies business logic.          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  SILVER  Clean &amp; standardize ONE source at a time.          \u2502\n\u2502          Deduplicate, cast types, map codes.                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  BRONZE  Land raw data as-is. No transforms.                 \u2502\n\u2502          Pure append. Add metadata. Duplicates expected.     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/planning_walkthroughs/#layer-rules-definitive","title":"Layer Rules (Definitive)","text":""},{"location":"guides/planning_walkthroughs/#bronze-layer-rules","title":"Bronze Layer Rules","text":"\u2705 ALLOWED \u274c NOT ALLOWED Land data exactly as-is from source Any data transformation Append mode only (accumulate history) Merge, upsert, or overwrite Add metadata columns (<code>_extracted_at</code>, <code>_batch_id</code>, <code>_source_file</code>) Filter or remove rows Schema evolution (allow new columns) Clean or standardize data Smart Read (rolling_window, stateful) for incremental Join any tables Route bad records to quarantine path Apply business logic Duplicates (expected - Silver handles them) Deduplicate <p>Bronze is your undo button. If something goes wrong downstream, you can always reprocess from Bronze.</p>"},{"location":"guides/planning_walkthroughs/#silver-layer-rules","title":"Silver Layer Rules","text":"\u2705 ALLOWED \u274c NOT ALLOWED Deduplicate (remove exact duplicates) Join multiple business source systems Clean text (trim, case, remove bad chars) UNION multiple source systems Cast data types Build dimensions with surrogate keys Standardize codes (M1 \u2192 Machine 1) SCD2 history tracking Join with reference/lookup tables Cross-source conformed dimensions Enrich via dimension lookups (code \u2192 name) Business KPIs or aggregations Validate and flag bad data Self-joins within the same source <p>The One-Source Test: \"Could this node run if only ONE business source system existed?\"</p> <ul> <li>Reference tables don't count as a \"source system\" - they're supporting data</li> <li>Joining <code>orders</code> with <code>product_codes</code> lookup table = \u2705 Silver</li> <li>Joining <code>sap_orders</code> with <code>salesforce_customers</code> = \u274c Gold</li> </ul>"},{"location":"guides/planning_walkthroughs/#gold-layer-rules","title":"Gold Layer Rules","text":"\u2705 ALLOWED \u274c NOT ALLOWED Dimensions with surrogate keys Silver-level cleaning (data should arrive clean) SCD2 history tracking SCD2 on fact tables DateDimension generation Undefined grain on facts Fact tables with dimension lookups SCD2 without prior deduplication UNION multiple source systems JOIN across source systems Business KPIs and calculated metrics Aggregations (daily, monthly, etc.) Semantic layer metrics <p>The Multi-Source Test: \"Does this require MULTIPLE source systems OR business modeling?\"</p> <ul> <li>If combining SAP + Salesforce + Excel \u2192 Gold</li> <li>If building a dimension with surrogate keys \u2192 Gold</li> <li>If creating business KPIs \u2192 Gold</li> </ul>"},{"location":"guides/planning_walkthroughs/#who-fills-what","title":"Who Fills What","text":""},{"location":"guides/planning_walkthroughs/#business-stakeholders","title":"Business Stakeholders","text":"Layer Sheet What to Fill Bronze B1_Source_Inventory What data exists, who owns it, what it's for Gold G1_Business_Questions What questions need answers (plain English) Gold G8_Semantic_Metrics How to calculate KPIs (plain English) <p>No SQL Required</p> <p>Describe things like you're explaining to a new employee.</p>"},{"location":"guides/planning_walkthroughs/#data-engineers","title":"Data Engineers","text":"Layer Sheets What to Fill Bronze B2-B5 How to land data technically Silver S2-S4 How to clean and standardize Gold G2-G7 How to model dimensions, facts, relationships"},{"location":"guides/planning_walkthroughs/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"guides/planning_walkthroughs/#step-1-bronze-layer","title":"Step 1: Bronze Layer","text":"<p>Start here. You can't clean data you haven't landed.</p> <pre><code>Bronze_Walkthrough.xlsx\n\u251c\u2500\u2500 B0_Overview        \u2192 Read first. Understand Bronze principles.\n\u251c\u2500\u2500 B1_Source_Inventory \u2192 List ALL source systems\n\u251c\u2500\u2500 B2_Node_Design     \u2192 One row per Bronze node\n\u251c\u2500\u2500 B3_Column_Mapping  \u2192 Document what columns land\n\u251c\u2500\u2500 B4_Validation      \u2192 Plan quality checks\n\u251c\u2500\u2500 B5_Migration       \u2192 For legacy ETL migrations only\n\u2514\u2500\u2500 Reference          \u2192 Best practices &amp; anti-patterns\n</code></pre> <p>The Bronze Rule: Pure append, no transforms, no cleaning. Add metadata (<code>_extracted_at</code>). Duplicates are expected.</p> <p>Odibi Patterns: <code>append</code>, <code>rolling_window</code>, <code>stateful</code></p>"},{"location":"guides/planning_walkthroughs/#step-2-silver-layer","title":"Step 2: Silver Layer","text":"<p>Only start when Bronze is landing correctly.</p> <pre><code>Silver_Walkthrough.xlsx\n\u251c\u2500\u2500 S0_Overview        \u2192 Read the \"One-Source Test\"\n\u251c\u2500\u2500 S1_Domain_Overview \u2192 Group nodes by business domain\n\u251c\u2500\u2500 S2_Node_Design     \u2192 One row per Silver node\n\u251c\u2500\u2500 S3_Column_Mapping  \u2192 Source \u2192 Target with transformations\n\u251c\u2500\u2500 S4_Validation      \u2192 Ensure data is trustworthy\n\u2514\u2500\u2500 Reference          \u2192 What belongs (and doesn't)\n</code></pre> <p>The One-Source Test: \"Could this node run if only ONE source system existed?\"</p> <ul> <li>YES \u2192 Silver \u2713</li> <li>NO \u2192 Probably Gold</li> </ul> <p>Reference Tables Are Allowed</p> <p>The One-Source Test refers to business source systems, not reference data.</p> <p>Silver CAN join with:</p> <ul> <li>Reference/lookup tables (code mappings, static lists)</li> <li>Dimension lookups for enrichment (product_code \u2192 product_name)</li> <li>Self-joins within the same source</li> </ul> <p>Silver should NOT join:</p> <ul> <li>Multiple business source systems (SAP + Salesforce \u2192 Gold)</li> <li>Cross-source conformed dimensions \u2192 Gold</li> </ul> <p>Odibi Patterns: <code>deduplicate</code>, <code>merge</code>, <code>clean_text</code>, <code>validate_and_flag</code></p>"},{"location":"guides/planning_walkthroughs/#step-3-gold-layer","title":"Step 3: Gold Layer","text":"<p>Only start when Silver data is clean and deduplicated.</p> <pre><code>Gold_Walkthrough.xlsx\n\u251c\u2500\u2500 G0_Overview        \u2192 Read the \"Multi-Source Test\"\n\u251c\u2500\u2500 G1_Business_Questions \u2192 START HERE - what does business need?\n\u251c\u2500\u2500 G2_Dimension_Design \u2192 Plan dimensions with SCD strategy\n\u251c\u2500\u2500 G3_Fact_Planning   \u2192 Design fact tables (grain is critical)\n\u251c\u2500\u2500 G4_Node_Design     \u2192 One row per Gold node\n\u251c\u2500\u2500 G5_Column_Mapping  \u2192 SQL transformation spec\n\u251c\u2500\u2500 G6_Validation      \u2192 Reconciliation checks\n\u251c\u2500\u2500 G7_FK_Relationships \u2192 Foreign key definitions\n\u251c\u2500\u2500 G8_Semantic_Metrics \u2192 Business metrics for self-service\n\u2514\u2500\u2500 Reference          \u2192 Best practices &amp; anti-patterns\n</code></pre> <p>Grain is Critical</p> <p>Every fact table needs a grain statement: \"One row = one ___ per ___.\" If you can't say it, you haven't defined the grain.</p> <p>Odibi Patterns: <code>Dimension</code>, <code>SCD2</code>, <code>DateDimension</code>, <code>Fact</code>, <code>Aggregation</code>, <code>union</code>, <code>join</code></p>"},{"location":"guides/planning_walkthroughs/#walkthrough-yaml-translation","title":"Walkthrough \u2192 YAML Translation","text":"<p>Once your walkthrough is complete, translation is mechanical:</p> Walkthrough Field YAML Location <code>Node_ID</code> / <code>Node_Name</code> <code>nodes.name</code> <code>Read_Connection</code> <code>read.connection</code> <code>Read_Format</code> <code>read.format</code> <code>Incremental_Mode</code> <code>incremental.mode</code> <code>Incremental_Column</code> <code>incremental.column</code> <code>Dedup_Keys</code> <code>params.keys</code> <code>Write_Mode</code> <code>write.mode</code> <code>Contract_Type</code> <code>validation.contracts[].type</code> <code>On_Failure</code> <code>validation.contracts[].on_failure</code> <code>Is_Quality_Gate</code> <code>validation.contracts[].is_quality_gate</code>"},{"location":"guides/planning_walkthroughs/#validation-fields","title":"Validation Fields","text":"<p>Every validation sheet uses these fields:</p> Field Purpose Values <code>Contract_Type</code> What check to run <code>not_null</code>, <code>unique</code>, <code>row_count</code>, <code>freshness</code>, <code>range</code>, <code>regex_match</code>, <code>accepted_values</code>, <code>custom_sql</code> <code>On_Failure</code> What happens when it fails <code>error</code> (stop), <code>warn</code> (log), <code>filter</code> (remove rows), <code>quarantine</code> (save bad rows) <code>Is_Quality_Gate</code> Does this block the pipeline? <code>Yes</code> / <code>No</code>"},{"location":"guides/planning_walkthroughs/#common-mistakes","title":"Common Mistakes","text":""},{"location":"guides/planning_walkthroughs/#bronze","title":"Bronze","text":"Mistake Why It's Wrong Transforming or cleaning data That's Silver's job Filtering out \"bad\" rows Land everything, filter later Forgetting <code>_extracted_at</code> You lose audit trail Using merge/upsert mode Always append in Bronze"},{"location":"guides/planning_walkthroughs/#silver","title":"Silver","text":"Mistake Why It's Wrong Joining multiple sources That's Gold Building dimensions with SKs That's Gold Adding business logic Keep it to standardization only"},{"location":"guides/planning_walkthroughs/#gold","title":"Gold","text":"Mistake Why It's Wrong Doing Silver-level cleaning Data should arrive clean Undefined grain on facts You'll get duplicates or gaps SCD2 without prior dedup History will be wrong Building without business questions You'll build the wrong thing"},{"location":"guides/planning_walkthroughs/#quick-start-checklist","title":"Quick Start Checklist","text":"<ul> <li>[ ] Download or regenerate the workbooks</li> <li>[ ] Open <code>Bronze_Walkthrough.xlsx</code>, read B0_Overview</li> <li>[ ] Complete the \"BEFORE YOU START\" checklist</li> <li>[ ] Fill B1 with business stakeholder help</li> <li>[ ] Fill B2 (one row per source to land)</li> <li>[ ] Fill B3 (column-level documentation)</li> <li>[ ] Fill B4 (validation contracts)</li> <li>[ ] Generate YAML from B2</li> <li>[ ] Build, test, deploy Bronze</li> <li>[ ] Repeat for Silver, then Gold</li> </ul>"},{"location":"guides/planning_walkthroughs/#example-bronze-node","title":"Example: Bronze Node","text":"<p>Scenario: Land Production Orders from SAP ERP</p>"},{"location":"guides/planning_walkthroughs/#filled-walkthrough-b2_node_design","title":"Filled Walkthrough (B2_Node_Design)","text":"Field Value Node_ID BRZ_001 Node_Name bronze_production_orders Read_Connection sap_erp_prod Read_Format sql Read_Table_or_Path SAPPR1.dbo.AFKO Incremental_Mode rolling_window Incremental_Column AEDAT Lookback 3 Lookback_Unit day Write_Path bronze/production/orders"},{"location":"guides/planning_walkthroughs/#generated-yaml","title":"Generated YAML","text":"<pre><code>pipelines:\n  - pipeline: bronze_production\n    nodes:\n      - name: bronze_production_orders\n        read:\n          connection: sap_erp_prod\n          format: sql\n          table: SAPPR1.dbo.AFKO\n        incremental:\n          mode: rolling_window\n          column: AEDAT\n          lookback: 3\n          lookback_unit: day\n        write:\n          connection: delta_lake\n          path: bronze/production/orders\n          mode: append\n</code></pre> <p>The walkthrough made this trivial.</p>"},{"location":"guides/planning_walkthroughs/#see-also","title":"See Also","text":"<ul> <li>Medallion Architecture - Layer philosophy</li> <li>YAML Schema Reference - Configuration details</li> <li>Validation Contracts - Quality check options</li> <li>Best Practices - General guidance</li> </ul>"},{"location":"guides/production_deployment/","title":"\ud83c\udfed Production Deployment","text":"<p>Moving from your laptop to production (e.g., Databricks, Azure Data Factory, Airflow) requires handling secrets, environments, and logging differently.</p>"},{"location":"guides/production_deployment/#1-secrets-management","title":"1. Secrets Management","text":"<p>NEVER commit passwords to Git.</p> <p>Odibi supports environment variable substitution in <code>odibi.yaml</code>. Use the <code>${VAR_NAME}</code> syntax.</p> <p>Bad: <pre><code>connections:\n  db:\n    password: \"super_secret_password\"  # \u274c Security Risk\n</code></pre></p> <p>Good: <pre><code>connections:\n  db:\n    password: \"${DB_PASSWORD}\"         # \u2705 Safe\n</code></pre></p> <p>Then, set the environment variable <code>DB_PASSWORD</code> in your production environment (or <code>.env</code> file locally).</p>"},{"location":"guides/production_deployment/#automatic-redaction","title":"Automatic Redaction","text":"<p>Odibi automatically detects values that look like secrets (keys, tokens, passwords) and replaces them with <code>[REDACTED]</code> in logs and Data Stories.</p>"},{"location":"guides/production_deployment/#2-data-privacy-pii","title":"2. Data Privacy &amp; PII","text":"<p>When processing personal data (GDPR/HIPAA), you must ensure that sensitive data does not leak into your logs or execution reports.</p>"},{"location":"guides/production_deployment/#column-level-redaction","title":"Column-Level Redaction","text":"<p>If you want to see non-sensitive data in your reports but hide PII (Personally Identifiable Information), specify the columns list.</p> <pre><code>nodes:\n  - name: ingest_users\n    read: ...\n    # Only masks these columns in the HTML report\n    sensitive: [\"email\", \"ssn\", \"phone\", \"credit_card\"]\n</code></pre>"},{"location":"guides/production_deployment/#full-node-redaction","title":"Full Node Redaction","text":"<p>For highly sensitive nodes (e.g., medical records, financial transactions), you can mask the entire sample.</p> <pre><code>nodes:\n  - name: process_health_records\n    transform: ...\n    # Replaces entire sample with \"[REDACTED: Sensitive Data]\"\n    sensitive: true\n</code></pre> <p>Note: This only affects the Data Story (logs/html). The actual data moving through the pipeline is not modified.</p>"},{"location":"guides/production_deployment/#3-azure-integration","title":"3. Azure Integration","text":"<p>Odibi has native support for Azure resources.</p>"},{"location":"guides/production_deployment/#authentication","title":"Authentication","text":"<p>We support DefaultAzureCredential. This means you don't need to manage keys manually. 1.  Local: It uses your Azure CLI login (<code>az login</code>). 2.  Production: It uses the Managed Identity of the VM/Pod.</p> <pre><code>connections:\n  data_lake:\n    type: azure_adls\n    account: mydatalake\n    auth_mode: key_vault  # Fetches keys from Key Vault automatically\n    key_vault: my-key-vault-name\n</code></pre>"},{"location":"guides/production_deployment/#3-running-on-databricks","title":"3. Running on Databricks","text":"<p>Odibi runs natively on Databricks clusters.</p> <ol> <li>Install: Add <code>odibi[spark,azure]</code> to your cluster libraries.</li> <li>Deploy: Copy your project folder (YAML + SQL) to DBFS or git checkout.</li> <li>Job: Create a job that runs:     <pre><code>odibi run odibi.yaml\n</code></pre></li> </ol> <p>Tip: Use the \"Spark\" engine for clusters or \"Polars\" engine for high-performance single-node tasks.</p> <pre><code>project: My Big Data Project\nengine: spark  # Options: pandas, polars, spark\n</code></pre>"},{"location":"guides/production_deployment/#4-system-catalog-unified-state","title":"4. System Catalog (Unified State)","text":"<p>Odibi uses a System Catalog (Delta Tables) to track execution history, high-water marks, and metadata. This unifies state management for both local and distributed environments.</p>"},{"location":"guides/production_deployment/#1-local-development-default","title":"1. Local Development (Default)","text":"<p>When running locally, the catalog is automatically created in a hidden directory (<code>.odibi/system/</code>). This uses the <code>deltalake</code> library (Rust core) for high-performance ACID transactions without needing Spark.</p>"},{"location":"guides/production_deployment/#2-production-distributed","title":"2. Production (Distributed)","text":"<p>In production (e.g., Databricks, Kubernetes), you should configure the System Catalog to store state in your Data Lake (ADLS/S3). This allows multiple concurrent pipelines to share state safely.</p> <pre><code>system:\n  connection: \"adls_bronze\"  # Points to your data lake connection\n  path: \"_odibi_system\"      # Directory for system tables\n</code></pre> <p>If utilizing Spark, Odibi leverages Delta Lake's optimistic concurrency control automatically.</p>"},{"location":"guides/production_deployment/#5-monitoring-observability","title":"5. Monitoring &amp; Observability","text":""},{"location":"guides/production_deployment/#openlineage-integration","title":"OpenLineage Integration","text":"<p>Odibi emits standard OpenLineage events. To integrate with DataHub, Marquez, or Atlan:</p> <pre><code>lineage:\n  url: \"http://marquez-api:5000\"\n  namespace: \"odibi-production\"\n</code></pre>"},{"location":"guides/production_deployment/#logging","title":"Logging","text":"<p>Odibi logs structured JSON to stdout by default in production. This is easily ingested by Datadog, Splunk, or Azure Monitor.</p> <pre><code># Force JSON logging\nexport ODIBI_LOG_FORMAT=json\nodibi run odibi.yaml\n</code></pre>"},{"location":"guides/production_deployment/#data-stories-as-artifacts","title":"Data Stories as Artifacts","text":"<p>Configure Odibi to save Data Stories to a permanent location (like an S3 bucket or ADLS container) so you have a permanent audit trail.</p> <pre><code>story:\n  connection: data_lake  # Save reports to the cloud\n  path: audit_reports/\n</code></pre>"},{"location":"guides/python_api_guide/","title":"\ud83d\udc0d Odibi Python API: Zero to Hero","text":"<p>Ultimate Cheatsheet &amp; Reference (v2.4.0)</p> <p>Welcome to the Python API guide. While the CLI is great for running pipelines, the Python API allows you to automate, test, and extend Odibi deeply into your infrastructure.</p>"},{"location":"guides/python_api_guide/#level-1-the-basics-running-pipelines","title":"\ud83d\udfe2 Level 1: The Basics (Running Pipelines)","text":"<p>The core entry point is the <code>PipelineManager</code>. It reads your YAML configuration and manages execution.</p>"},{"location":"guides/python_api_guide/#1-load-and-run","title":"1. Load and Run","text":"<pre><code>from odibi.pipeline import PipelineManager\n\n# 1. Load your project configuration\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\n\n# 2. Run EVERYTHING (All pipelines defined in yaml)\nresults = manager.run()\n\n# 3. Check if it worked\nif results['main_pipeline'].failed:\n    print(\"\u274c Pipeline Failed!\")\nelse:\n    print(\"\u2705 Success!\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-run-a-specific-pipeline","title":"2. Run a Specific Pipeline","text":"<p>If your YAML has multiple pipelines (e.g., <code>ingest</code>, <code>transform</code>, <code>export</code>), run just one: <pre><code># Returns a single PipelineResults object instead of a dict\nresult = manager.run(\"ingest\")\n\nprint(f\"Duration: {result.duration:.2f}s\")\n</code></pre></p>"},{"location":"guides/python_api_guide/#3-dry-run-simulation","title":"3. Dry Run (Simulation)","text":"<p>Check logic without moving data: <pre><code>manager.run(\"ingest\", dry_run=True)\n</code></pre></p>"},{"location":"guides/python_api_guide/#level-2-intermediate-inspection-automation","title":"\ud83d\udfe1 Level 2: Intermediate (Inspection &amp; Automation)","text":"<p>Once you have <code>PipelineResults</code>, you can inspect exactly what happened.</p>"},{"location":"guides/python_api_guide/#1-inspect-node-results","title":"1. Inspect Node Results","text":"<pre><code>result = manager.run(\"ingest\")\n\nfor node_name, node_result in result.node_results.items():\n    status = \"\u2705\" if node_result.success else \"\u274c\"\n    print(f\"{status} {node_name}: {node_result.duration:.2f}s\")\n\n    # See metadata (row counts, schema output, etc.)\n    if node_result.metadata:\n        print(f\"   Rows: {node_result.metadata.get('rows_out', 0)}\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-resume-from-failure","title":"2. Resume from Failure","text":"<p>If a pipeline fails at step 5 of 10, you don't want to re-run steps 1-4. <pre><code># Automatically skips successfully completed nodes from the last run\nmanager.run(\"ingest\", resume_from_failure=True)\n</code></pre></p>"},{"location":"guides/python_api_guide/#level-3-hero-advanced-usage","title":"\ud83d\udd34 Level 3: Hero (Advanced Usage)","text":"<p>This is where Odibi shines. You can unit test individual logic units without running the full pipeline.</p>"},{"location":"guides/python_api_guide/#1-unit-testing-nodes","title":"1. Unit Testing Nodes","text":"<p>You don't need to run the whole pipeline to test one complex SQL transformation.</p> <pre><code>from odibi.pipeline import PipelineManager\nimport pandas as pd\n\n# 1. Setup Manager\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\npipeline = manager.get_pipeline(\"main_etl\")\n\n# 2. Mock Input Data (Inject test data into context)\nmock_data = {\n    \"read_customers\": pd.DataFrame([\n        {\"id\": 1, \"email\": \"BAD_EMAIL\"},\n        {\"id\": 2, \"email\": \"good@test.com\"}\n    ])\n}\n\n# 3. Run ONE specific node with mocked input\nresult = pipeline.run_node(\"clean_customers\", mock_data=mock_data)\n\n# 4. Assertions\noutput_df = pipeline.context.get(\"clean_customers\")\nassert len(output_df) == 1  # Should have filtered bad email\nassert output_df.iloc[0]['email'] == \"good@test.com\"\nprint(\"\u2705 Unit Test Passed\")\n</code></pre>"},{"location":"guides/python_api_guide/#2-accessing-story-data","title":"2. Accessing Story Data","text":"<p>Want to send the pipeline report to Slack or Email programmatically?</p> <pre><code>result = manager.run(\"ingest\")\n\nif result.story_path:\n    print(f\"HTML Report generated at: {result.story_path}\")\n\n    # Read the HTML content\n    with open(result.story_path, \"r\") as f:\n        html_content = f.read()\n\n    # send_email(to=\"team@company.com\", subject=\"Pipeline Report\", body=html_content)\n</code></pre>"},{"location":"guides/python_api_guide/#3-deep-diff-pipeline-runs","title":"3. Deep Diff (Pipeline Runs)","text":"<p>Programmatically detect changes between two pipeline runs (schema drift, row count changes, logic changes).</p> <pre><code>from odibi.diagnostics.diff import diff_runs\nfrom odibi.story.metadata import PipelineStoryMetadata\n\n# Load run metadata (generated by odibi run in odibi_stories/metadata/)\n# Note: You need to know the paths to the JSON files\nrun_a = PipelineStoryMetadata.from_json(\"odibi_stories/metadata/run_20231027_120000.json\")\nrun_b = PipelineStoryMetadata.from_json(\"odibi_stories/metadata/run_20231027_120500.json\")\n\n# Calculate differences\ndiff = diff_runs(run_a, run_b)\n\n# Inspect Results\nif diff.nodes_added:\n    print(f\"New Nodes: {diff.nodes_added}\")\n\nfor node_name, node_diff in diff.node_diffs.items():\n    if node_diff.has_drift:\n        print(f\"\u26a0\ufe0f DRIFT in {node_name}:\")\n        if node_diff.schema_change:\n            print(f\"   - Schema changed! Added: {node_diff.columns_added}\")\n        if node_diff.sql_changed:\n            print(f\"   - SQL Logic changed\")\n        if node_diff.rows_diff != 0:\n            print(f\"   - Row count changed by {node_diff.rows_diff}\")\n</code></pre>"},{"location":"guides/python_api_guide/#4-deep-diff-delta-lake","title":"4. Deep Diff (Delta Lake)","text":"<p>Directly compare two versions of a Delta table to see what changed (rows added, removed, updated).</p> <pre><code>from odibi.diagnostics.delta import get_delta_diff\n\ntable_path = \"data/delta_tables/silver/customers\"\n\n# Compare version 1 vs version 2\ndiff = get_delta_diff(\n    table_path=table_path,\n    version_a=1,\n    version_b=2,\n    deep=True,          # Perform row-by-row comparison\n    keys=[\"id\"]         # Primary key for detecting updates\n)\n\nprint(f\"Rows Added: {diff.rows_added}\")\nprint(f\"Rows Removed: {diff.rows_removed}\")\nprint(f\"Rows Updated: {diff.rows_updated}\")\n\nif diff.sample_updated:\n    print(\"Sample Updates:\", diff.sample_updated[0])\n</code></pre>"},{"location":"guides/python_api_guide/#reference-custom-transformations","title":"\ud83d\udcda Reference: Custom Transformations","text":"<p>To extend the Python API with your own functions, see the Writing Custom Transformations guide.</p> <p>Quick Snippet: <pre><code>from odibi import transform\n\n@transform\ndef my_custom_logic(context, current, threshold=100):\n    return current[current['value'] &gt; threshold]\n</code></pre></p>"},{"location":"guides/recipes/","title":"Odibi Cookbook: Recipes for Common Patterns","text":"<p>This guide provides copy-pasteable solutions for real-world Data Engineering problems.</p>"},{"location":"guides/recipes/#recipe-1-the-unstable-api-ingestion","title":"Recipe 1: The \"Unstable API\" Ingestion \ud83c\udf2a\ufe0f","text":"<p>Problem: \"My source JSON adds new fields constantly and is deeply nested. My pipeline breaks whenever the schema changes.\"</p> <p>Solution: Use <code>schema_policy: { mode: \"evolve\" }</code> to automatically adapt to new columns, and <code>normalize_json</code> to flatten the structure.</p> <pre><code>- name: \"ingest_unstable_api\"\n  read:\n    connection: \"api_source\"\n    format: \"json\"\n    path: \"events/v1/*.json\"\n\n  # 1. Handle Drift: Automatically add new columns as NULLable\n  schema_policy:\n    mode: \"evolve\"\n    on_new_columns: \"add_nullable\"\n\n  # 2. Flatten: Convert nested JSON into columns (e.g. payload.id -&gt; payload_id)\n  transformer: \"normalize_json\"\n  params:\n    column: \"payload\"\n    sep: \"_\"\n\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"events_flat\"\n</code></pre>"},{"location":"guides/recipes/#recipe-2-the-privacy-first-customer-table","title":"Recipe 2: The \"Privacy-First\" Customer Table \ud83d\udd12","text":"<p>Problem: \"I need to ingest customer data but Hash emails and Mask credit card numbers for compliance (GDPR/CCPA).\"</p> <p>Solution: Use the <code>privacy</code> block for global anonymization and <code>sensitive</code> columns for masking in stories. You can also mix methods using <code>hash_columns</code> transformer.</p> <pre><code>- name: \"load_secure_customers\"\n  read:\n    connection: \"s3_raw\"\n    format: \"parquet\"\n    path: \"customers/\"\n\n  # 1. Global Privacy Policy (Applies to PII columns)\n  privacy:\n    method: \"hash\"\n    salt: \"${PRIVACY_SALT}\"  # Load from env var\n\n  # 2. Mark Columns as PII (Triggers Privacy Policy)\n  columns:\n    email:\n      pii: true\n    phone:\n      pii: true\n\n  # 3. Explicit Masking for Credit Cards (Transformers run before Write)\n  transform:\n    steps:\n      # Mask CCNs (keep last 4)\n      - function: \"regex_replace\"\n        params:\n          column: \"credit_card\"\n          pattern: \".(?=.{4})\"  # Regex to match all except last 4\n          replacement: \"*\"\n\n  # 4. Hide from Stories (Documentation)\n  sensitive: [\"email\", \"credit_card\", \"phone\"]\n\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"dim_customers_anonymized\"\n</code></pre>"},{"location":"guides/recipes/#recipe-3-sessionizing-clickstream-data","title":"Recipe 3: Sessionizing Clickstream Data \u23f1\ufe0f","text":"<p>Problem: \"I have raw events. I need to group them into User Sessions (30-minute timeout) and load them incrementally.\"</p> <p>Solution: Combine the <code>sessionize</code> transformer with <code>incremental: { mode: \"stateful\" }</code> to process only new data while maintaining session logic.</p> <pre><code>- name: \"clickstream_sessions\"\n  read:\n    connection: \"kafka_landing\"\n    format: \"json\"\n    path: \"clicks/\"\n\n    # 1. Incremental Loading (Stateful)\n    # Tracks the last processed timestamp to only read new events\n    incremental:\n      mode: \"stateful\"\n      state_key: \"clickstream_hwm\"\n      watermark_lag: \"1h\"  # Handle late arriving data\n\n  # 2. Session Logic (30 min timeout)\n  transformer: \"sessionize\"\n  params:\n    timestamp_col: \"event_time\"\n    user_col: \"user_id\"\n    threshold_seconds: 1800  # 30 minutes\n    session_col: \"session_id\"\n\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sessions\"\n    mode: \"append\"\n</code></pre>"},{"location":"guides/secrets/","title":"Managing Secrets","text":"<p>Odibi provides a unified way to handle secrets (API keys, database passwords, storage tokens) across local development and production environments. It supports <code>.env</code> files for local use and native Azure Key Vault integration for production.</p>"},{"location":"guides/secrets/#1-variable-substitution","title":"1. Variable Substitution","text":"<p>You can reference environment variables in your <code>project.yaml</code> using the <code>${VAR_NAME}</code> syntax.</p> <pre><code>connections:\n  my_database:\n    type: azure_sql\n    host: ${DB_HOST}\n    auth:\n      username: ${DB_USER}\n      password: ${DB_PASS}\n</code></pre>"},{"location":"guides/secrets/#2-local-development-env","title":"2. Local Development (<code>.env</code>)","text":"<p>For local development, store your secrets in a <code>.env</code> file in your project root. Odibi automatically loads this file.</p> <p>Note: Always add <code>.env</code> to your <code>.gitignore</code> to prevent committing secrets.</p>"},{"location":"guides/secrets/#cli-commands","title":"CLI Commands","text":"<p>Initialize a template: Generate a <code>.env.template</code> file based on the variables used in your config. <pre><code>odibi secrets init project.yaml\n</code></pre></p> <p>Validate your environment: Check if all required variables are set in your current environment. <pre><code>odibi secrets validate project.yaml\n</code></pre></p>"},{"location":"guides/secrets/#3-production-azure-key-vault","title":"3. Production (Azure Key Vault)","text":"<p>In production (e.g., Databricks, Azure Functions), relying on environment variables for everything can be insecure. Odibi supports fetching secrets directly from Azure Key Vault.</p>"},{"location":"guides/secrets/#configuration","title":"Configuration","text":"<p>To use Key Vault, specify <code>key_vault_name</code> and <code>secret_name</code> in your connection config. Odibi will automatically fetch the secret securely using <code>DefaultAzureCredential</code> (Managed Identity / Service Principal).</p> <pre><code>connections:\n  adls_prod:\n    type: azure_adls\n    account: myprodstorage\n    container: data\n    # Instead of a hardcoded key or env var:\n    key_vault_name: \"my-key-vault\"\n    secret_name: \"adls-prod-key\"\n</code></pre>"},{"location":"guides/secrets/#how-it-works","title":"How it Works","text":"<ol> <li>Auth Detection: If <code>key_vault_name</code> is present, Odibi attempts to authenticate with Azure using the environment's identity (e.g., the Databricks cluster's Managed Identity).</li> <li>Parallel Fetching: If multiple connections use Key Vault, Odibi fetches them in parallel during startup to minimize latency.</li> <li>Caching: Secrets are cached in memory for the duration of the run.</li> </ol>"},{"location":"guides/secrets/#best-practices","title":"Best Practices","text":"<ol> <li>Never Commit Secrets: Do not put actual passwords in <code>project.yaml</code>. Use <code>${VAR}</code> placeholders.</li> <li>Use <code>.env.template</code>: Commit a template file with empty values so other developers know which variables they need to set.</li> <li>Use Key Vault in Prod: Avoid setting sensitive environment variables in cloud compute configurations if possible. Use Key Vault integration for rotation and auditing.</li> <li>Redaction: Odibi automatically attempts to redact known secret values from logs and generated stories.</li> </ol>"},{"location":"guides/setup_azure/","title":"Azure Integration Setup Guide (v2.1.0)","text":"<p>This guide covers authenticating and connecting Odibi to Azure services (ADLS Gen2, Azure SQL, Key Vault) using the latest v2.1.0 standards.</p> <p>Key Features in v2.1.0: - Auto-Auth: Zero-config authentication using Managed Identity or Environment Variables (<code>DefaultAzureCredential</code>). - Universal Key Vault: Retrieve ANY secret (Account Key, SAS Token, SQL Password) from Key Vault by referencing it in the config.</p>"},{"location":"guides/setup_azure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription</li> <li>Azure CLI installed: Install Azure CLI</li> <li>Odibi installed with Azure extras: <code>pip install \"odibi[azure]\"</code></li> </ul>"},{"location":"guides/setup_azure/#1-azure-data-lake-storage-gen2-adls","title":"1. Azure Data Lake Storage Gen2 (ADLS)","text":"<p>Use the <code>azure_blob</code> connection type.</p>"},{"location":"guides/setup_azure/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/setup_azure/#option-a-auto-auth-recommended","title":"Option A: Auto-Auth (Recommended)","text":"<p>Best for: Production (Managed Identity) or Local Dev (Azure CLI login). How it works: Odibi uses <code>DefaultAzureCredential</code> to find your identity automatically. No secrets in the config!</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    # No 'auth' section needed!\n    # Odibi will automatically try Managed Identity, CLI, or Env Vars.\n</code></pre> <p>Setup: 1. Azure: Grant your identity (User or Managed Identity) the Storage Blob Data Contributor role on the storage account. 2. Local: Run <code>az login</code>. 3. Production: Assign Managed Identity to the VM/Function/Databricks cluster.</p>"},{"location":"guides/setup_azure/#option-b-universal-key-vault-account-key","title":"Option B: Universal Key Vault (Account Key)","text":"<p>Best for: Scenarios where Managed Identity is not possible. How it works: Store the Account Key in Key Vault, and tell Odibi where to find it.</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    auth:\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"adls-account-key\"  # The secret containing the Account Key\n</code></pre>"},{"location":"guides/setup_azure/#option-c-universal-key-vault-sas-token","title":"Option C: Universal Key Vault (SAS Token)","text":"<p>Best for: Restricted access with SAS tokens.</p> <pre><code>connections:\n  my_datalake:\n    type: azure_blob\n    account_name: \"odibistorage\"\n    container: \"data\"\n    auth:\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"adls-sas-token\"  # The secret containing the SAS Token\n</code></pre>"},{"location":"guides/setup_azure/#2-azure-sql-database","title":"2. Azure SQL Database","text":"<p>Use the <code>sql_server</code> connection type.</p>"},{"location":"guides/setup_azure/#configuration-patterns_1","title":"Configuration Patterns","text":""},{"location":"guides/setup_azure/#option-a-auto-auth-managed-identity","title":"Option A: Auto-Auth (Managed Identity)","text":"<p>Best for: Production pipelines running in Azure.</p> <pre><code>connections:\n  my_sql_db:\n    type: sql_server\n    host: \"odibi-sql.database.windows.net\"\n    database: \"analytics_db\"\n    port: 1433\n    auth: {}  # Empty auth dict signals \"Use Default Driver Auth / Managed Identity\"\n</code></pre> <p>Setup: 1. Enable Managed Identity on your compute resource. 2. In Azure SQL, create a user for the identity: <code>CREATE USER [my-identity] FROM EXTERNAL PROVIDER;</code>. 3. Grant permissions: <code>ALTER ROLE db_datareader ADD MEMBER [my-identity];</code>.</p>"},{"location":"guides/setup_azure/#option-b-universal-key-vault-sql-password","title":"Option B: Universal Key Vault (SQL Password)","text":"<p>Best for: Legacy SQL Auth (Username/Password).</p> <pre><code>connections:\n  my_sql_db:\n    type: sql_server\n    host: \"odibi-sql.database.windows.net\"\n    database: \"analytics_db\"\n    port: 1433\n    auth:\n      username: \"sqladmin\"\n      key_vault_name: \"my-keyvault\"\n      secret_name: \"sql-password\"  # The secret containing the password\n</code></pre>"},{"location":"guides/setup_azure/#3-key-vault-setup","title":"3. Key Vault Setup","text":"<p>If you use Key Vault references, Odibi needs to authenticate to the Key Vault first. It uses Auto-Auth for this too!</p> <ol> <li> <p>Create Key Vault: <pre><code>az keyvault create --name my-keyvault --resource-group my-rg\n</code></pre></p> </li> <li> <p>Grant Access:    Grant your identity (User or Managed Identity) the Key Vault Secrets User role.    <pre><code>az role assignment create \\\n  --role \"Key Vault Secrets User\" \\\n  --assignee &lt;your-email-or-identity-id&gt; \\\n  --scope /subscriptions/.../resourceGroups/my-rg/providers/Microsoft.KeyVault/vaults/my-keyvault\n</code></pre></p> </li> <li> <p>Store Secrets: <pre><code>az keyvault secret set --vault-name my-keyvault --name adls-account-key --value \"your-key-here\"\n</code></pre></p> </li> </ol>"},{"location":"guides/setup_azure/#summary-of-changes-v20-v21","title":"Summary of Changes (v2.0 -&gt; v2.1)","text":"Feature v2.0 (Old) v2.1 (New) Connection Type <code>azure_adls</code>, <code>azure_sql</code> <code>azure_blob</code>, <code>sql_server</code> Auth Mode <code>auth_mode: key_vault</code> (top-level) Removed. Use <code>auth: { key_vault_name: ... }</code> Managed Identity Explicit <code>auth_mode: managed_identity</code> Implicit (Auto-Auth) via <code>DefaultAzureCredential</code> Key Vault Limited to specific auth modes Universal (works for any secret in <code>auth</code> dict) <p>For a complete configuration reference, see docs/reference/configuration.md.</p>"},{"location":"guides/testing/","title":"Testing Guide","text":"<p>Test your Odibi pipelines with built-in utilities for assertions, fixtures, and deterministic data.</p>"},{"location":"guides/testing/#overview","title":"Overview","text":"<p>Odibi provides testing utilities in <code>odibi.testing</code>:</p> <ul> <li>Assertions: Compare DataFrames and schemas</li> <li>Fixtures: Generate sample data and temporary directories</li> <li>Source Pools: Deterministic, frozen test data for replay</li> </ul>"},{"location":"guides/testing/#assertions","title":"Assertions","text":""},{"location":"guides/testing/#assert_frame_equal","title":"assert_frame_equal","text":"<p>Compare two DataFrames for equality (supports Pandas and Spark):</p> <pre><code>from odibi.testing.assertions import assert_frame_equal\n\n# Compare two DataFrames\nassert_frame_equal(actual_df, expected_df)\n\n# With options\nassert_frame_equal(\n    actual_df,\n    expected_df,\n    check_dtype=True,      # Check column types\n    check_exact=False,     # Allow float tolerance\n    atol=1e-8,             # Absolute tolerance\n    rtol=1e-5              # Relative tolerance\n)\n</code></pre>"},{"location":"guides/testing/#assert_schema_equal","title":"assert_schema_equal","text":"<p>Compare schemas (column names and types):</p> <pre><code>from odibi.testing.assertions import assert_schema_equal\n\nassert_schema_equal(df_a, df_b)\n</code></pre>"},{"location":"guides/testing/#fixtures","title":"Fixtures","text":""},{"location":"guides/testing/#temp_directory","title":"temp_directory","text":"<p>Create a temporary directory that auto-cleans:</p> <pre><code>from odibi.testing.fixtures import temp_directory\n\nwith temp_directory() as temp_dir:\n    path = os.path.join(temp_dir, \"test.csv\")\n    df.to_csv(path)\n    # Directory is deleted after context exits\n</code></pre>"},{"location":"guides/testing/#generate_sample_data","title":"generate_sample_data","text":"<p>Generate sample DataFrames for testing:</p> <pre><code>from odibi.testing.fixtures import generate_sample_data\n\n# Default schema: id (int), value (float), category (str), timestamp (date)\ndf = generate_sample_data(rows=100)\n\n# Custom schema\ndf = generate_sample_data(\n    rows=50,\n    engine_type=\"spark\",  # or \"pandas\"\n    schema={\n        \"user_id\": \"int\",\n        \"score\": \"float\",\n        \"name\": \"str\",\n        \"created_at\": \"date\"\n    }\n)\n</code></pre>"},{"location":"guides/testing/#unit-testing-nodes","title":"Unit Testing Nodes","text":"<p>Test individual pipeline nodes with mock data:</p> <pre><code>from odibi.pipeline import PipelineManager\nimport pandas as pd\n\nmanager = PipelineManager.from_yaml(\"odibi.yaml\")\npipeline = manager.get_pipeline(\"main_etl\")\n\n# Mock input data\nmock_data = {\n    \"read_customers\": pd.DataFrame([\n        {\"id\": 1, \"email\": \"BAD_EMAIL\"},\n        {\"id\": 2, \"email\": \"good@test.com\"}\n    ])\n}\n\n# Run single node with mock\nresult = pipeline.run_node(\"clean_customers\", mock_data=mock_data)\n\n# Assert output\noutput_df = pipeline.context.get(\"clean_customers\")\nassert len(output_df) == 1\n</code></pre>"},{"location":"guides/testing/#source-pools","title":"Source Pools","text":"<p>For deterministic, replayable tests, see Source Pools Design.</p> <p>Source pools provide:</p> <ul> <li>Frozen data: Hash-verified, immutable test datasets</li> <li>Quality variants: Clean, messy, and mixed data</li> <li>Schema definitions: Explicit, no runtime inference</li> <li>Test coverage hints: Know what scenarios each pool covers</li> </ul>"},{"location":"guides/testing/#end-to-end-test-campaign","title":"End-to-End Test Campaign","text":"<p>For comprehensive validation of core patterns, run the test campaign:</p> <pre><code>python scripts/run_test_campaign.py\n</code></pre> <p>This validates:</p> Phase What It Tests Phase 1 CSV read, Parquet write, schema validation Phase 3 State/HWM persistence Phase 4 Merge pattern (upsert) Phase 5 SCD2 pattern Phase 6 Logical path resolution Phase 11 10k row scaling <p>All phases run on the Pandas engine. For Spark validation, see Spark Engine Testing.</p>"},{"location":"guides/testing/#spark-engine-testing","title":"Spark Engine Testing","text":"<p>The Spark engine is validated in production on Databricks rather than in CI due to JVM/environment complexity.</p> <p>Local Spark testing (WSL required on Windows):</p> <pre><code>wsl -d Ubuntu-20.04 -- bash -c \"cd /mnt/d/odibi &amp;&amp; python3.9 -m pytest tests/ -k spark\"\n</code></pre> <p>Databricks validation: - Deploy pipeline to Databricks workspace - Run with <code>engine: spark</code> configuration - Validate outputs match Pandas engine results</p> <p>Mock-based Spark tests (no JVM required):</p> <pre><code># tests/integration/test_patterns_spark_mock.py\n# Uses mocked SparkSession to test logic without real Spark\n</code></pre>"},{"location":"guides/testing/#related","title":"Related","text":"<ul> <li>Python API Guide \u2014 Programmatic pipeline execution</li> <li>Source Pools Design \u2014 Deterministic test data</li> <li>Best Practices \u2014 Testing recommendations</li> <li>Spark Engine Tutorial \u2014 Spark-specific setup</li> </ul>"},{"location":"guides/the_definitive_guide/","title":"The Definitive Guide to Odibi","text":"<p>Version: 2.4.0 Audience: Data Engineers, Analytics Engineers, Architects Prerequisites: Basic Python and SQL knowledge Goal: From \"Hello World\" to Enterprise Production</p>"},{"location":"guides/the_definitive_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction &amp; Philosophy</li> <li>Architecture Deep Dive</li> <li>The Object Model (Python API)</li> <li>Context, State, and Dependencies</li> <li>Transformation Engineering</li> <li>The Dual Engine: Pandas &amp; Spark</li> <li>Declarative Configuration (YAML)</li> <li>Observability: Data Stories</li> <li>Reliability &amp; Governance</li> <li>Production Deployment Patterns</li> <li>Comprehensive End-to-End Example</li> </ol>"},{"location":"guides/the_definitive_guide/#1-introduction-philosophy","title":"1. Introduction &amp; Philosophy","text":"<p>Odibi is a Declarative Data Framework designed to solve the \"Two Language Problem\" in data engineering: 1.  Local Development is often done in Python/Pandas on a laptop (fast iteration, small data). 2.  Production runs on distributed clusters like Spark/Databricks (high latency, massive data).</p> <p>Traditionally, this requires rewriting code or wrapping Spark in complex local docker containers. Odibi provides a unified abstraction layer that allows the exact same pipeline definition to run on your MacBook (using Pandas) and on a 100-node Databricks cluster (using Spark).</p>"},{"location":"guides/the_definitive_guide/#core-principles","title":"Core Principles","text":"<ol> <li>Declarative over Imperative: Define what you want (Input -&gt; Transform -&gt; Output), not how to loop through files.</li> <li>Code First, Configuration Second: Learn the Python API to understand the system; use YAML for deployment.</li> <li>Engine Agnostic: Logic written for Odibi runs on Pandas (Local) and Spark (Scale) without code changes.</li> <li>Observability by Default: Every run generates a rich HTML \"Data Story\" with lineage, profile, and logs.</li> </ol>"},{"location":"guides/the_definitive_guide/#2-architecture-deep-dive","title":"2. Architecture Deep Dive","text":"<p>Odibi is built on a \"Three Layer\" Architecture. Understanding these layers helps you debug and extend the framework.</p> <pre><code>graph TD\n    subgraph \"Layer 1: Declarative (User Interface)\"\n        A[odibi.yaml] --&gt;|Parsed by| B(ProjectConfig)\n        CLI[odibi run] --&gt;|Invokes| B\n    end\n\n    subgraph \"Layer 2: Object Model (The Brain)\"\n        B --&gt; C[PipelineConfig]\n        C --&gt; D[NodeConfig]\n        D --&gt;|Validates| E[DependencyGraph]\n        E --&gt;|Orchestrates| F[Pipeline Executor]\n    end\n\n    subgraph \"Layer 3: Runtime Engine (The Muscle)\"\n        F --&gt;|Delegates to| G{Engine Interface}\n        G --&gt;|Local Mode| H[Pandas Engine]\n        G --&gt;|Scale Mode| I[Spark Engine]\n        H --&gt; J[(Local Files / DuckDB)]\n        I --&gt; K[(Delta Lake / ADLS / Databricks)]\n    end\n</code></pre> <ul> <li>Layer 1 (YAML): Simple configuration files.</li> <li>Layer 2 (Pydantic Objects): The <code>odibi.config</code> module. This is where validation happens. If your graph has a cycle, or you miss a parameter, Layer 2 catches it before execution starts.</li> <li>Layer 3 (Engines): The <code>odibi.engine</code> module. This adapts your abstract \"Read CSV\" command into <code>pd.read_csv(...)</code> or <code>spark.read.csv(...)</code>.</li> </ul>"},{"location":"guides/the_definitive_guide/#3-the-object-model-python-api","title":"3. The Object Model (Python API)","text":"<p>The most effective way to learn Odibi is to build a pipeline using the Python classes directly. This demystifies the YAML tags.</p>"},{"location":"guides/the_definitive_guide/#31-the-hierarchy","title":"3.1 The Hierarchy","text":"<ol> <li><code>ProjectConfig</code>: Top-level container. Holds global settings (Engine type, Retry policy, Connections).</li> <li><code>PipelineConfig</code>: A logical grouping of tasks (e.g., \"Daily Sales Batch\").</li> <li><code>NodeConfig</code>: A single step in the pipeline.</li> <li>Operations: <code>ReadConfig</code>, <code>TransformConfig</code>, <code>WriteConfig</code>.</li> </ol>"},{"location":"guides/the_definitive_guide/#32-building-a-complex-node-programmatically","title":"3.2 Building a Complex Node Programmatically","text":"<p>Let's look at a feature-rich node configuration.</p> <pre><code>from odibi.config import NodeConfig, ReadConfig, TransformConfig, WriteConfig, ErrorStrategy\n\nnode = NodeConfig(\n    name=\"process_orders\",\n    description=\"Cleans orders and calculates tax\",\n    tags=[\"daily\", \"critical\"],\n\n    # 1. Dependency Management\n    depends_on=[\"raw_orders\", \"ref_tax_rates\"],\n\n    # 2. Transformation Logic\n    transform=TransformConfig(\n        steps=[\n            # Step A: Filter (SQL)\n            \"SELECT * FROM raw_orders WHERE status != 'CANCELLED'\",\n            # Step B: Custom Python Function\n            {\"function\": \"calculate_tax\", \"params\": {\"rate_source\": \"ref_tax_rates\"}}\n        ]\n    ),\n\n    # 3. Output Configuration\n    write=WriteConfig(\n        connection=\"silver_db\",\n        format=\"delta\",\n        table=\"orders_cleaned\",\n        mode=\"append\"\n    ),\n\n    # 4. Reliability &amp; Governance\n    on_error=ErrorStrategy.FAIL_FAST,  # Stop whole pipeline if this fails\n    sensitive=[\"customer_email\"],      # Mask this column in logs/stories\n    cache=True                         # Cache result in memory for downstream nodes\n)\n</code></pre>"},{"location":"guides/the_definitive_guide/#33-node-operations-guide","title":"3.3 Node Operations Guide","text":"Operation Description Key Fields Read Ingests data from a connection. <code>connection</code>, <code>format</code> (<code>csv</code>, <code>parquet</code>, <code>delta</code>, <code>sql</code>), <code>path</code>, <code>options</code> Transform Runs a sequence of steps. <code>steps</code> (List of SQL strings or <code>{function: name, params: {}}</code> dicts) Transformer Runs a single \"App-like\" transformation. <code>transformer</code> (Name string), <code>params</code> (Dict). Used for complex logic like SCD2 or Deduplication. Write Saves the result. <code>connection</code>, <code>format</code>, <code>path</code>/<code>table</code>, <code>mode</code> (<code>overwrite</code>, <code>append</code>, <code>upsert</code>)"},{"location":"guides/the_definitive_guide/#4-context-state-and-dependencies","title":"4. Context, State, and Dependencies","text":"<p>In Odibi, you don't pass variables between functions. You rely on the Dependency Graph.</p>"},{"location":"guides/the_definitive_guide/#41-the-global-context-odibicontextcontext","title":"4.1 The Global Context (<code>odibi.context.Context</code>)","text":"<p>The Pipeline Executor maintains a Global Context. *   Registry: When a node named <code>raw_orders</code> finishes, its result (DataFrame) is registered in the context under the key <code>\"raw_orders\"</code>. *   Access: Downstream nodes request data from the context by name. *   Memory Management: In the Pandas engine, this is a dictionary of DataFrames in RAM. In Spark, these are Temp Views registered in the Spark Session.</p>"},{"location":"guides/the_definitive_guide/#42-dependency-resolution","title":"4.2 Dependency Resolution","text":"<p>When you define <code>depends_on=[\"node_A\"]</code> for <code>node_B</code>: 1.  Graph Construction: Odibi builds a DAG (Directed Acyclic Graph). 2.  Topological Sort: It determines the execution order (A -&gt; B). 3.  Execution:     *   <code>node_A</code> runs. Output registered as <code>\"node_A\"</code>.     *   <code>node_B</code> starts. It can now execute <code>SELECT * FROM node_A</code>.</p>"},{"location":"guides/the_definitive_guide/#43-the-enginecontext-odibicontextenginecontext","title":"4.3 The EngineContext (<code>odibi.context.EngineContext</code>)","text":"<p>When writing custom transformations, you interact with <code>EngineContext</code>, not the Global Context directly. This wraps the global state and provides uniform APIs.</p> Method Description <code>context.df</code> The DataFrame resulting from the previous step in the current node. <code>context.get(name)</code> Retrieve a DataFrame from an upstream node (Global Context). <code>context.sql(query)</code> Run SQL on <code>context.df</code>. Returns a new Context with updated <code>df</code>. <code>context.register_temp_view(name, df)</code> Register a DF manually for complex SQL joins."},{"location":"guides/the_definitive_guide/#5-transformation-engineering","title":"5. Transformation Engineering","text":"<p>While SQL is great for filtering and projection, complex logic belongs in Python. Odibi uses a registry pattern.</p>"},{"location":"guides/the_definitive_guide/#51-the-transform-decorator","title":"5.1 The <code>@transform</code> Decorator","text":"<p>You must register functions so Odibi can find them by name from the YAML configuration.</p> <pre><code>from odibi.registry import transform\n\n@transform\ndef categorize_users(context, params: dict):\n    \"\"\"\n    Categorizes users based on spend.\n\n    YAML:\n      function: categorize_users\n      params:\n        threshold: 1000\n    \"\"\"\n    threshold = params.get(\"threshold\", 1000)\n\n    # 1. Get Input\n    df = context.df\n\n    # 2. Use Engine-Native commands (Pandas/Spark agnostic via SQL)\n    # Or check context.engine_type if you need specific optimization\n\n    return context.sql(f\"\"\"\n        SELECT\n            *,\n            CASE WHEN total_spend &gt; {threshold} THEN 'VIP' ELSE 'Regular' END as category\n        FROM df\n    \"\"\").df\n</code></pre>"},{"location":"guides/the_definitive_guide/#52-transformer-vs-transform-steps","title":"5.2 Transformer vs. Transform Steps","text":"<p>There are two ways to define logic in a node:</p> <ol> <li> <p>Top-Level Transformer (<code>transformer: \"name\"</code>):</p> <ul> <li>The node acts as a specific \"App\" (e.g., <code>deduplicate</code>, <code>scd2</code>).</li> <li>It usually takes the dependencies as input and produces one output.</li> <li>Use this for heavy, reusable logic (Slowly Changing Dimensions, Merges).</li> </ul> </li> <li> <p>Transform Steps (<code>transform: { steps: [...] }</code>):</p> <ul> <li>A \"Script\" of sequential operations.</li> <li>Mixes SQL and lightweight Python functions.</li> <li>Use this for business logic specific to that pipeline (Filter -&gt; Join -&gt; Map -&gt; Reduce).</li> </ul> </li> </ol>"},{"location":"guides/the_definitive_guide/#6-the-dual-engine-pandas-spark","title":"6. The Dual Engine: Pandas &amp; Spark","text":"<p>Odibi abstracts the engine, but knowing how they differ is crucial for performance.</p>"},{"location":"guides/the_definitive_guide/#61-comparison","title":"6.1 Comparison","text":"Feature Pandas Engine Spark Engine Compute Single Node (CPU/RAM bound) Distributed (Cluster) SQL DuckDB / PandasQL Spark SQL (Catalyst) IO Local FS, S3/Blob (via fsspec) HDFS, S3, ADLS (Native) Setup <code>pip install odibi</code> <code>pip install odibi[spark]</code> Best For Dev, Testing, Small Data (&lt;10GB) Prod, Big Data (TB/PB)"},{"location":"guides/the_definitive_guide/#62-spark-specific-features","title":"6.2 Spark-Specific Features","text":"<p>The Spark engine enables advanced Data Lakehouse features via <code>odibi.engine.spark_engine.SparkEngine</code>.</p>"},{"location":"guides/the_definitive_guide/#delta-lake-integration","title":"Delta Lake Integration","text":"<p>Odibi treats Delta Lake as a first-class citizen.</p> <ul> <li> <p>Time Travel: <pre><code>ReadConfig(..., options={\"as_of_version\": 5})\n# OR\nReadConfig(..., options={\"as_of_timestamp\": \"2023-10-01\"})\n</code></pre></p> </li> <li> <p>Upserts (MERGE): <pre><code>WriteConfig(\n    ...,\n    format=\"delta\",\n    mode=\"upsert\",\n    options={\"keys\": [\"user_id\"]} # Matches on key, updates all other cols\n)\n</code></pre></p> </li> <li> <p>Optimization (Write Options): <pre><code>WriteConfig(..., options={\n    \"optimize_write\": \"true\",  # Auto-compaction\n    \"zorder_by\": [\"region\"]    # Spatial indexing\n})\n</code></pre></p> </li> </ul>"},{"location":"guides/the_definitive_guide/#7-declarative-configuration-yaml","title":"7. Declarative Configuration (YAML)","text":"<p>The YAML configuration is the deployment artifact. It maps 1:1 to the Pydantic models.</p>"},{"location":"guides/the_definitive_guide/#71-project-structure","title":"7.1 Project Structure","text":"<p>Recommended folder structure for an Odibi project:</p> <pre><code>my_project/\n\u251c\u2500\u2500 odibi.yaml            # Main entry point\n\u251c\u2500\u2500 transforms.py         # Custom python logic (@transform)\n\u251c\u2500\u2500 data/                 # Local data (for dev)\n\u251c\u2500\u2500 stories/              # Generated reports\n\u2514\u2500\u2500 .env                  # Secrets (API keys)\n</code></pre>"},{"location":"guides/the_definitive_guide/#72-advanced-yaml-features","title":"7.2 Advanced YAML Features","text":""},{"location":"guides/the_definitive_guide/#environment-variables","title":"Environment Variables","text":"<p>You can inject secrets or environment-specific paths using <code>${VAR_NAME}</code>.</p> <pre><code>connections:\n  snowflake:\n    type: \"sql_server\"\n    host: \"${DB_HOST}\"\n    password: \"${DB_PASSWORD}\" # Redacted in logs automatically\n</code></pre>"},{"location":"guides/the_definitive_guide/#yaml-anchors-aliases","title":"YAML Anchors &amp; Aliases","text":"<p>Reuse configuration blocks to keep YAML DRY (Don't Repeat Yourself).</p> <pre><code># Define a template\n.default_write: &amp;default_write\n  connection: \"datalake\"\n  format: \"delta\"\n  mode: \"overwrite\"\n\nnodes:\n  - name: \"customers\"\n    write:\n      &lt;&lt;: *default_write\n      table: \"dim_customers\"\n\n  - name: \"orders\"\n    write:\n      &lt;&lt;: *default_write\n      table: \"fact_orders\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#8-observability-data-stories","title":"8. Observability: Data Stories","text":"<p>Running a pipeline blindly is dangerous. Odibi generates Data Stories.</p>"},{"location":"guides/the_definitive_guide/#81-what-is-a-story","title":"8.1 What is a Story?","text":"<p>A Story is an HTML file generated at the end of a run. It answers: 1.  Lineage: \"Where did this data come from?\" (Visual Graph) 2.  Profile: \"How many rows? How many nulls?\" (Schema &amp; Stats) 3.  Sample: \"What does the data look like?\" (Preview rows) 4.  Logic: \"What code actually ran?\" (SQL/Python snippet)</p>"},{"location":"guides/the_definitive_guide/#82-configuration","title":"8.2 Configuration","text":"<p>Enable story generation in <code>ProjectConfig</code>.</p> <pre><code>story:\n  connection: \"local_data\" # Where to save the HTML\n  path: \"stories/\"\n  max_sample_rows: 20      # Number of rows to preview\n  retention_days: 30       # Auto-cleanup old reports\n</code></pre>"},{"location":"guides/the_definitive_guide/#83-openlineage","title":"8.3 OpenLineage","text":"<p>Odibi supports the OpenLineage standard for integration with tools like Marquez or Atlan.</p> <pre><code>lineage:\n  url: \"http://localhost:5000\" # Marquez URL\n  namespace: \"odibi_prod\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#9-reliability-governance","title":"9. Reliability &amp; Governance","text":""},{"location":"guides/the_definitive_guide/#91-retries-backoff","title":"9.1 Retries &amp; Backoff","text":"<p>Network blips happen. Configure retries globally or per node.</p> <pre><code># Global setting in project config\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\" # linear, constant\n</code></pre>"},{"location":"guides/the_definitive_guide/#92-alerting","title":"9.2 Alerting","text":"<p>Send notifications when pipelines fail or succeed.</p> <pre><code>alerts:\n  - type: \"slack\"\n    url: \"${SLACK_WEBHOOK}\"\n    on_events: [\"on_failure\"] # on_start, on_success\n    metadata:\n      env: \"production\"\n</code></pre>"},{"location":"guides/the_definitive_guide/#93-pii-protection","title":"9.3 PII Protection","text":"<p>Prevent sensitive data from leaking into logs or Data Stories.</p> <pre><code>nodes:\n  - name: \"load_users\"\n    # Masks columns in the 'Sample Data' section of the Story\n    sensitive: [\"email\", \"ssn\", \"phone_number\"]\n</code></pre>"},{"location":"guides/the_definitive_guide/#10-production-deployment-patterns","title":"10. Production Deployment Patterns","text":""},{"location":"guides/the_definitive_guide/#101-the-hybrid-pattern","title":"10.1 The \"Hybrid\" Pattern","text":"<ol> <li>Develop Locally: Use <code>engine: pandas</code> and local CSVs. Iterate fast.</li> <li>Deploy to Cloud:<ul> <li>Switch <code>engine: spark</code>.</li> <li>Change connections to ADLS/S3.</li> <li>Use <code>odibi run</code> via a job scheduler (Airflow, Databricks Workflows).</li> </ul> </li> </ol>"},{"location":"guides/the_definitive_guide/#102-cicd","title":"10.2 CI/CD","text":"<p>Since Odibi logic is Python code and YAML config: 1.  Linting: Run <code>black</code> and <code>ruff</code> on your <code>transforms.py</code>. 2.  Validation: Run a script that loads <code>ProjectConfig(path=\"odibi.yaml\")</code> to validate the graph structure before deployment. 3.  Testing: Use the Python API to run single nodes with mock data (Unit Tests).</p>"},{"location":"guides/the_definitive_guide/#11-comprehensive-end-to-end-example","title":"11. Comprehensive End-to-End Example","text":"<p>This script demonstrates a complete workflow: 1.  Mocking data generation. 2.  Defining custom logic. 3.  Configuring a multi-stage pipeline (Bronze -&gt; Silver -&gt; Gold). 4.  Executing with error handling.</p> <pre><code>import pandas as pd\nimport os\nimport logging\nfrom odibi.config import (\n    PipelineConfig, NodeConfig, ReadConfig, TransformConfig, WriteConfig,\n    RetryConfig, ErrorStrategy\n)\nfrom odibi.pipeline import Pipeline\nfrom odibi.registry import transform\nfrom odibi.connections import LocalConnection\n\n# ==========================================\n# 0. Setup Environment\n# ==========================================\n# Create dummy data for the example\nos.makedirs(\"data/landing\", exist_ok=True)\nos.makedirs(\"data/silver\", exist_ok=True)\nos.makedirs(\"data/gold\", exist_ok=True)\n\n# Generate Raw JSON Data (Simulating an API dump)\npd.DataFrame([\n    {\"id\": 1, \"user\": \"Alice\", \"tx_amount\": 150.0, \"ts\": \"2023-10-01T10:00:00\"},\n    {\"id\": 2, \"user\": \"Bob\",   \"tx_amount\": 20.0,  \"ts\": \"2023-10-01T10:05:00\"},\n    {\"id\": 3, \"user\": \"Alice\", \"tx_amount\": -50.0, \"ts\": \"2023-10-01T10:10:00\"}, # Invalid\n    {\"id\": 4, \"user\": \"Eve\",   \"tx_amount\": 1000.0, \"ts\": \"2023-10-01T10:15:00\"},\n]).to_json(\"data/landing/transactions.json\", orient=\"records\")\n\n# ==========================================\n# 1. Custom Logic (Business Rules)\n# ==========================================\n@transform\ndef anomaly_detection(context, params):\n    \"\"\"\n    Flags transactions that are 3 std devs above the mean.\n    \"\"\"\n    df = context.df\n\n    # Using SQL for statistical window function\n    # This works in DuckDB (Pandas) and Spark SQL\n    query = \"\"\"\n        SELECT\n            *,\n            AVG(tx_amount) OVER () as mean_amount,\n            STDDEV(tx_amount) OVER () as std_amount,\n            CASE\n                WHEN tx_amount &gt; (AVG(tx_amount) OVER () + 3 * STDDEV(tx_amount) OVER ())\n                THEN true\n                ELSE false\n            END as is_anomaly\n        FROM df\n    \"\"\"\n    return context.sql(query).df\n\n# ==========================================\n# 2. Pipeline Definition\n# ==========================================\npipeline_conf = PipelineConfig(\n    pipeline=\"fraud_detection_batch\",\n    description=\"Ingests transactions and flags anomalies\",\n    nodes=[\n        # --- Bronze Layer: Ingest Raw ---\n        NodeConfig(\n            name=\"bronze_tx\",\n            read=ReadConfig(\n                connection=\"landing\",\n                format=\"json\",\n                path=\"transactions.json\"\n            ),\n            # Keep raw data safe, minimal transform\n            write=WriteConfig(\n                connection=\"silver\",\n                format=\"parquet\",\n                path=\"bronze_tx.parquet\",\n                mode=\"overwrite\"\n            )\n        ),\n\n        # --- Silver Layer: Clean &amp; Enrich ---\n        NodeConfig(\n            name=\"silver_tx\",\n            depends_on=[\"bronze_tx\"],\n            transform=TransformConfig(\n                steps=[\n                    # 1. Filter invalid amounts\n                    \"SELECT * FROM bronze_tx WHERE tx_amount &gt; 0\",\n                    # 2. Run Anomaly Detection\n                    {\"function\": \"anomaly_detection\", \"params\": {}}\n                ]\n            ),\n            write=WriteConfig(\n                connection=\"silver\",\n                format=\"parquet\",\n                path=\"silver_tx.parquet\",\n                mode=\"overwrite\"\n            )\n        ),\n\n        # --- Gold Layer: Business Aggregates ---\n        NodeConfig(\n            name=\"gold_user_summary\",\n            depends_on=[\"silver_tx\"],\n            transform=TransformConfig(\n                steps=[\n                    \"\"\"\n                    SELECT\n                        user,\n                        COUNT(*) as tx_count,\n                        SUM(tx_amount) as total_volume,\n                        SUM(CASE WHEN is_anomaly THEN 1 ELSE 0 END) as suspicious_tx_count\n                    FROM silver_tx\n                    GROUP BY user\n                    \"\"\"\n                ]\n            ),\n            write=WriteConfig(\n                connection=\"gold\",\n                format=\"csv\",\n                path=\"user_risk_report.csv\",\n                mode=\"overwrite\"\n            ),\n            # Fail fast if report generation breaks\n            on_error=ErrorStrategy.FAIL_FAST\n        )\n    ]\n)\n\n# ==========================================\n# 3. Execution Wrapper\n# ==========================================\ndef run_pipeline():\n    print(\"\ud83d\ude80 Starting Fraud Detection Pipeline...\")\n\n    # Connections Definition\n    connections = {\n        \"landing\": LocalConnection(base_path=\"./data/landing\"),\n        \"silver\":  LocalConnection(base_path=\"./data/silver\"),\n        \"gold\":    LocalConnection(base_path=\"./data/gold\")\n    }\n\n    # Initialize Pipeline\n    pipeline = Pipeline(\n        pipeline_conf,\n        connections=connections,\n        generate_story=True,\n        story_config={\n            \"output_path\": \"data/stories\",\n            \"max_sample_rows\": 50\n        },\n        retry_config=RetryConfig(enabled=True, max_attempts=2)\n    )\n\n    # Run\n    results = pipeline.run()\n\n    # Logging Results\n    print(f\"\\n\u23f1\ufe0f Duration: {results.duration:.2f}s\")\n\n    if results.failed:\n        print(f\"\u274c Failed Nodes: {results.failed}\")\n        # Inspect specific error\n        for node in results.failed:\n            res = results.get_node_result(node)\n            print(f\"   Reason ({node}): {res.error}\")\n    else:\n        print(f\"\u2705 Success! Report generated at: {results.story_path}\")\n\n        # Verify output\n        print(\"\\n--- Risk Report ---\")\n        df = pd.read_csv(\"data/gold/user_risk_report.csv\")\n        print(df.to_string(index=False))\n\nif __name__ == \"__main__\":\n    run_pipeline()\n</code></pre> <p>This guide provides a solid foundation for using Odibi. Start with the Python API to understand the mechanics, and transition to YAML for production operations.</p>"},{"location":"guides/writing_transformations/","title":"Writing Transformation Functions in Odibi","text":"<p>This guide explains how to write custom Python transformation functions for Odibi pipelines, focusing on how to access data and manage state.</p>"},{"location":"guides/writing_transformations/#the-basics","title":"The Basics","text":"<p>Every transformation function in Odibi must be decorated with <code>@transform</code>. The Odibi engine automatically injects dependencies based on your function signature.</p>"},{"location":"guides/writing_transformations/#the-context-object","title":"The <code>context</code> Object","text":"<p>The first argument to any transformation function is always <code>context</code>. This object is your gateway to the entire state of the pipeline execution.</p> <p>Through <code>context</code>, you can: - Access the output of any previous node. - Retrieve datasets declared in <code>depends_on</code>. - Inspect available data using <code>context.list_names()</code>.</p>"},{"location":"guides/writing_transformations/#the-current-argument","title":"The <code>current</code> Argument","text":"<p>If your function includes an argument named <code>current</code>, Odibi will automatically pass the output of the immediately preceding step to it.</p> <ul> <li>With <code>current</code>: Continues the \"chain\" of data transformation.</li> <li>Without <code>current</code>: Breaks the chain (useful for generators or starting fresh logic).</li> </ul>"},{"location":"guides/writing_transformations/#accessing-other-datasets","title":"Accessing Other Datasets","text":"<p>While <code>current</code> is great for linear transformations (A \u2192 B \u2192 C), complex logic often requires accessing multiple datasets (e.g., for joins, lookups, or comparisons). You do this using <code>context.get()</code>.</p>"},{"location":"guides/writing_transformations/#pattern-explicit-data-fetching","title":"Pattern: Explicit Data Fetching","text":"<ol> <li>Define the Function: Add a parameter for the dataset name you want to fetch.</li> <li>Fetch from Context: Use <code>context.get(name)</code>.</li> <li>Configure in YAML: Pass the node name as a parameter.</li> </ol>"},{"location":"guides/writing_transformations/#python-implementation-transformspy","title":"Python Implementation (<code>transforms.py</code>)","text":"<pre><code>from odibi import transform\nimport pandas as pd\n\n@transform\ndef enrich_with_lookup(context, current: pd.DataFrame, lookup_node: str):\n    \"\"\"\n    Enriches the current stream with data from a lookup node.\n\n    Args:\n        context: The Odibi execution context.\n        current: The dataframe from the previous step.\n        lookup_node: The name of the node containing lookup data (passed from YAML).\n    \"\"\"\n    # 1. Fetch the other dataset using context\n    if not context.has(lookup_node):\n        raise ValueError(f\"Lookup node '{lookup_node}' not found in context!\")\n\n    lookup_df = context.get(lookup_node)\n\n    # 2. Perform the logic (e.g., merge)\n    # Note: For simple merges, SQL is often preferred, but Python is useful\n    # for fuzzy matching, complex logic, or API-based enrichment.\n    result = current.merge(\n        lookup_df,\n        on=\"common_id\",\n        how=\"left\",\n        suffixes=(\"\", \"_lookup\")\n    )\n\n    return result\n</code></pre>"},{"location":"guides/writing_transformations/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>nodes:\n  - name: main_process\n    depends_on:\n      - raw_orders      # The 'current' stream\n      - customer_info   # The lookup table\n    transform:\n      steps:\n        - function: enrich_with_lookup\n          params:\n            lookup_node: \"customer_info\"\n</code></pre>"},{"location":"guides/writing_transformations/#sql-vs-python-when-to-use-what","title":"SQL vs. Python: When to use what?","text":"<p>Odibi supports mixing SQL and Python steps in the same node.</p> Use SQL when... Use Python when... Joining tables (Standard Joins) Making API calls (e.g., Geocoding, REST APIs) Aggregations (GROUP BY, SUM) Complex loops or procedural logic Filtering (WHERE clauses) Using libraries (NumPy, SciPy, AI models) Renaming/Reordering columns File operations or custom parsing <p>Example of SQL for Multi-Dataset Access: If you just need a standard join, you don't need a Python function. You can reference nodes directly in SQL:</p> <pre><code>transform:\n  steps:\n    - sql: |\n        SELECT o.*, c.email\n        FROM current_df AS o\n        LEFT JOIN customer_info AS c ON o.id = c.id\n</code></pre>"},{"location":"guides/writing_transformations/#sql-transformations","title":"SQL Transformations","text":"<p>For standard data transformations, SQL is often cleaner than Python. Odibi supports inline SQL and SQL file references.</p>"},{"location":"guides/writing_transformations/#inline-sql","title":"Inline SQL","text":"<pre><code>nodes:\n  - name: clean_orders\n    depends_on: [raw_orders]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              order_id,\n              customer_id,\n              UPPER(TRIM(status)) AS status,\n              CAST(amount AS DECIMAL(10,2)) AS amount,\n              COALESCE(discount, 0) AS discount\n            FROM raw_orders\n            WHERE order_id IS NOT NULL\n</code></pre>"},{"location":"guides/writing_transformations/#multi-table-sql-joins","title":"Multi-Table SQL Joins","text":"<p>Reference any node from <code>depends_on</code>:</p> <pre><code>nodes:\n  - name: enriched_orders\n    depends_on: [clean_orders, customers, products]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              o.*,\n              c.customer_name,\n              c.segment,\n              p.product_name,\n              p.category\n            FROM clean_orders o\n            LEFT JOIN customers c ON o.customer_id = c.id\n            LEFT JOIN products p ON o.product_id = p.id\n</code></pre>"},{"location":"guides/writing_transformations/#sql-file-reference","title":"SQL File Reference","text":"<p>For complex queries, use external SQL files. Paths are resolved relative to the YAML file where the node is defined:</p> <pre><code># In silver.yaml\ntransform:\n  steps:\n    - sql_file: sql/complex_aggregation.sql  # relative to silver.yaml\n</code></pre> <p>Example project structure: <pre><code>project/\n\u251c\u2500\u2500 project.yaml              # imports pipelines/silver/silver.yaml\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 silver/\n        \u251c\u2500\u2500 silver.yaml       # defines the node\n        \u2514\u2500\u2500 sql/\n            \u2514\u2500\u2500 transform.sql\n</code></pre></p> <p>In <code>silver.yaml</code>, use a path relative to <code>silver.yaml</code>: <pre><code># silver.yaml\ntransform:\n  steps:\n    - sql_file: sql/transform.sql   # \u2713 Correct: relative to silver.yaml\n</code></pre></p> <p>Important: Do NOT use absolute paths or paths relative to project.yaml: <pre><code># \u2717 Wrong - absolute path\n- sql_file: /pipelines/silver/sql/transform.sql\n\n# \u2717 Wrong - relative to project.yaml instead of silver.yaml  \n- sql_file: pipelines/silver/sql/transform.sql\n</code></pre></p> <p>sql/complex_aggregation.sql: <pre><code>WITH daily_totals AS (\n    SELECT \n        DATE(order_date) AS order_day,\n        customer_id,\n        SUM(amount) AS daily_amount\n    FROM orders\n    GROUP BY DATE(order_date), customer_id\n)\nSELECT \n    order_day,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    SUM(daily_amount) AS revenue\nFROM daily_totals\nGROUP BY order_day\n</code></pre></p>"},{"location":"guides/writing_transformations/#window-functions-in-sql","title":"Window Functions in SQL","text":"<pre><code>transform:\n  steps:\n    - sql: |\n        SELECT \n          *,\n          ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS order_rank,\n          SUM(amount) OVER (PARTITION BY customer_id) AS customer_lifetime_value,\n          LAG(amount) OVER (PARTITION BY customer_id ORDER BY order_date) AS prev_order_amount\n        FROM orders\n</code></pre>"},{"location":"guides/writing_transformations/#combining-sql-and-python-steps","title":"Combining SQL and Python Steps","text":"<pre><code>transform:\n  steps:\n    # Step 1: SQL for standard transformations\n    - sql: |\n        SELECT * FROM raw_orders \n        WHERE status != 'CANCELLED'\n\n    # Step 2: Python for complex logic\n    - function: enrich_with_api_data\n      params:\n        api_endpoint: \"https://api.example.com/enrichment\"\n\n    # Step 3: SQL for final shaping\n    - sql: |\n        SELECT order_id, customer_id, amount, enriched_data\n        FROM current_df\n        ORDER BY order_date\n</code></pre>"},{"location":"guides/writing_transformations/#registering-custom-transforms-with-transform","title":"Registering Custom Transforms with @transform","text":"<p>The <code>@transform</code> decorator registers your function so Odibi can find it by name in YAML configurations.</p>"},{"location":"guides/writing_transformations/#basic-registration","title":"Basic Registration","text":"<pre><code>from odibi import transform\n\n@transform\ndef clean_names(context, current):\n    \"\"\"Function is registered as 'clean_names' (uses function name).\"\"\"\n    current['name'] = current['name'].str.strip().str.title()\n    return current\n</code></pre>"},{"location":"guides/writing_transformations/#custom-name-registration","title":"Custom Name Registration","text":"<pre><code>@transform(\"normalize_addresses\")\ndef my_address_normalizer(context, current):\n    \"\"\"Function is registered as 'normalize_addresses'.\"\"\"\n    # ... address normalization logic\n    return current\n</code></pre>"},{"location":"guides/writing_transformations/#registration-with-category-and-parameter-model","title":"Registration with Category and Parameter Model","text":"<pre><code>from pydantic import BaseModel\n\nclass EnrichmentParams(BaseModel):\n    lookup_table: str\n    join_key: str\n    columns: list[str]\n\n@transform(name=\"enrich_data\", category=\"enrichment\", param_model=EnrichmentParams)\ndef enrich_data(context, current, lookup_table: str, join_key: str, columns: list):\n    \"\"\"\n    Registered as 'enrich_data' with parameter validation.\n\n    Parameters are validated against EnrichmentParams before execution.\n    \"\"\"\n    lookup_df = context.get(lookup_table)\n    return current.merge(lookup_df[columns + [join_key]], on=join_key, how='left')\n</code></pre>"},{"location":"guides/writing_transformations/#where-to-put-your-transforms","title":"Where to Put Your Transforms","text":"<ol> <li>Project-level: Create <code>transformations/custom_transforms.py</code></li> <li>Import in project.yaml: <pre><code>python_imports:\n  - transformations.custom_transforms\n</code></pre></li> <li>Use in nodes: <pre><code>transform:\n  steps:\n    - function: normalize_addresses\n</code></pre></li> </ol>"},{"location":"guides/writing_transformations/#summary-of-function-signature-rules","title":"Summary of Function Signature Rules","text":"Signature Behavior <code>def func(context):</code> Receives context only. Does not receive previous step output. <code>def func(context, current):</code> Receives context AND the result of the previous step. <code>def func(context, my_param):</code> Receives context and a parameter from YAML. <code>def func(context, current, my_param):</code> Receives all three."},{"location":"guides/writing_transformations/#see-also","title":"See Also","text":"<ul> <li>Patterns Overview - Built-in transformation patterns</li> <li>Best Practices - Code organization guidelines</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"guides/wsl_setup/","title":"\ud83d\udc27 WSL Setup Guide","text":"<p>If you are on Windows, Windows Subsystem for Linux (WSL 2) is the only supported way to develop with Odibi (especially for Spark compatibility).</p>"},{"location":"guides/wsl_setup/#the-golden-rule","title":"The Golden Rule","text":"<p>Edit code in Windows. Run code in WSL.</p> <ul> <li>VS Code: Runs in Windows, but connects to WSL.</li> <li>Terminal: You type commands in the WSL (Ubuntu) terminal.</li> <li>Files: Live in the Linux file system (or <code>/mnt/d/</code>).</li> </ul>"},{"location":"guides/wsl_setup/#1-install-requirements-inside-wsl","title":"1. Install Requirements (Inside WSL)","text":"<p>Open your Ubuntu terminal and run:</p> <pre><code># 1. Update system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# 2. Install Python 3.10\nsudo apt install -y python3.10 python3.10-venv python3.10-dev\n\n# 3. Install Java (Required for Spark)\nsudo apt install -y openjdk-17-jdk\n</code></pre>"},{"location":"guides/wsl_setup/#2-setup-environment","title":"2. Setup Environment","text":"<ol> <li> <p>Clone/Go to your project: <pre><code>cd /mnt/d/odibi  # Accessing D: drive from Linux\n</code></pre></p> </li> <li> <p>Create Virtual Environment: <pre><code>python3.10 -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> </li> <li> <p>Install Odibi: <pre><code>pip install \"odibi[spark]\"\n</code></pre></p> </li> </ol>"},{"location":"guides/wsl_setup/#3-configure-vs-code","title":"3. Configure VS Code","text":"<ol> <li>Install the \"WSL\" extension in VS Code.</li> <li>Open your folder in Windows.</li> <li>Click the green button in the bottom-left corner (\"Open a Remote Window\").</li> <li>Select \"Reopen in WSL\".</li> </ol> <p>Now your terminal inside VS Code is a Linux terminal!</p>"},{"location":"guides/wsl_setup/#troubleshooting","title":"Troubleshooting","text":"<p>\"Java not found\" Make sure you installed <code>openjdk-17-jdk</code>. Add this to your <code>~/.bashrc</code>: <pre><code>export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n</code></pre></p> <p>\"Command not found: odibi\" Did you activate your virtual environment? <pre><code>source .venv/bin/activate\n</code></pre></p>"},{"location":"learning/curriculum/","title":"\ud83c\udf93 Odibi Learning Curriculum","text":"<p>A 4-Week Journey from Zero to Data Engineer</p> <p>This curriculum is designed for complete beginners with no prior data engineering experience. By the end, you'll be able to build production-ready data pipelines.</p>"},{"location":"learning/curriculum/#how-this-course-works","title":"How This Course Works","text":"<ul> <li>Pace: ~1-2 hours per day, 5 days per week</li> <li>Style: Learn by doing \u2014 every concept has hands-on exercises</li> <li>Format: Read \u2192 Try \u2192 Check \u2192 Repeat</li> </ul> <p>Each week builds on the previous one, like stacking building blocks.</p>"},{"location":"learning/curriculum/#week-1-bronze-layer-basic-concepts","title":"\ud83d\udcc5 Week 1: Bronze Layer + Basic Concepts","text":""},{"location":"learning/curriculum/#learning-objectives","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Understand what data is and common file formats - Know what a data pipeline does and why it matters - Install Odibi and run your first pipeline - Load raw data into a Bronze layer</p>"},{"location":"learning/curriculum/#prerequisites","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - A computer (Windows, Mac, or Linux) - Python 3.9+ installed (Download Python) - A text editor (VS Code recommended) - Basic comfort using a terminal/command prompt</p>"},{"location":"learning/curriculum/#day-1-what-is-data","title":"Day 1: What is Data?","text":""},{"location":"learning/curriculum/#kitchen-analogy","title":"\ud83c\udf73 Kitchen Analogy","text":"<p>Think of data like ingredients in your kitchen. You have: - Raw ingredients (flour, eggs, sugar) = raw data files - Recipes = data transformations - Finished dishes = clean, usable reports</p> <p>Data comes in many \"containers\":</p> Format What it looks like When to use CSV Spreadsheet-like rows and columns Simple tabular data JSON Nested key-value pairs API responses, configs Parquet Binary columnar format Large datasets, analytics Delta Parquet + versioning + ACID Production data lakes"},{"location":"learning/curriculum/#hands-on-create-your-first-data-file","title":"\ud83d\udcbb Hands-On: Create Your First Data File","text":"<p>Create a folder called <code>my_first_pipeline</code> and inside it create <code>customers.csv</code>:</p> <pre><code>id,name,email,signup_date\n1,Alice,alice@example.com,2024-01-15\n2,Bob,bob@example.com,2024-02-20\n3,Charlie,charlie@example.com,2024-03-10\n</code></pre> <p>This is tabular data: rows (records) and columns (fields).</p>"},{"location":"learning/curriculum/#self-check","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Can you explain what a CSV file is?</li> <li>[ ] What's the difference between a row and a column?</li> </ul>"},{"location":"learning/curriculum/#day-2-what-is-a-data-pipeline","title":"Day 2: What is a Data Pipeline?","text":""},{"location":"learning/curriculum/#assembly-line-analogy","title":"\ud83c\udfed Assembly Line Analogy","text":"<p>Imagine a car factory. Raw materials enter one end, go through multiple stations (welding, painting, assembly), and a finished car comes out the other end.</p> <p>A data pipeline works the same way: 1. Extract \u2014 Get raw data from somewhere (files, databases, APIs) 2. Transform \u2014 Clean, reshape, and enrich the data 3. Load \u2014 Save the result somewhere useful</p> <p>This is called ETL (Extract, Transform, Load).</p>"},{"location":"learning/curriculum/#the-medallion-architecture","title":"The Medallion Architecture","text":"<p>Odibi uses a \"layered\" approach to organize data:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      YOUR DATA LAKE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   BRONZE    \u2502     SILVER      \u2502           GOLD              \u2502\n\u2502   (Raw)     \u2502    (Cleaned)    \u2502       (Business-Ready)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 As-is     \u2502 \u2022 Deduplicated  \u2502 \u2022 Aggregated                \u2502\n\u2502 \u2022 Untouched \u2502 \u2022 Typed         \u2502 \u2022 Joined                    \u2502\n\u2502 \u2022 Archived  \u2502 \u2022 Validated     \u2502 \u2022 Ready for reporting       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why layers? - If something breaks, you can always go back to Bronze - Each layer has a clear purpose - Teams can work on different layers independently</p> <p>\ud83d\udcd6 Deep Dive: Medallion Architecture Guide</p>"},{"location":"learning/curriculum/#self-check_1","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does ETL stand for?</li> <li>[ ] Why do we have separate Bronze, Silver, and Gold layers?</li> </ul>"},{"location":"learning/curriculum/#day-3-introduction-to-odibi","title":"Day 3: Introduction to Odibi","text":""},{"location":"learning/curriculum/#what-is-odibi","title":"What is Odibi?","text":"<p>Odibi is a YAML-first data pipeline framework. Instead of writing hundreds of lines of code, you describe what you want in simple configuration files.</p>"},{"location":"learning/curriculum/#installation","title":"\ud83d\udd27 Installation","text":"<p>Open your terminal and run:</p> <pre><code># Create a virtual environment (recommended)\npython -m venv .venv\n\n# Activate it\n# Windows:\n.venv\\Scripts\\activate\n# Mac/Linux:\nsource .venv/bin/activate\n\n# Install Odibi\npip install odibi\n</code></pre> <p>Verify it works: <pre><code>odibi --version\n</code></pre></p>"},{"location":"learning/curriculum/#your-first-odibi-project","title":"Your First Odibi Project","text":"<p>Let Odibi create a project structure for you:</p> <pre><code>odibi init-pipeline my_first_project --template local-medallion\ncd my_first_project\n</code></pre> <p>This creates: <pre><code>my_first_project/\n\u251c\u2500\u2500 odibi.yaml          # Your pipeline configuration\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 landing/        # Where raw files arrive\n\u2502   \u251c\u2500\u2500 bronze/         # Raw data preserved\n\u2502   \u251c\u2500\u2500 silver/         # Cleaned data\n\u2502   \u2514\u2500\u2500 gold/           # Business-ready data\n\u2514\u2500\u2500 README.md\n</code></pre></p> <p>\ud83d\udcd6 Deep Dive: Getting Started Tutorial</p>"},{"location":"learning/curriculum/#self-check_2","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What command installs Odibi?</li> <li>[ ] What folder does raw data go into?</li> </ul>"},{"location":"learning/curriculum/#day-4-the-bronze-layer","title":"Day 4: The Bronze Layer","text":""},{"location":"learning/curriculum/#filing-cabinet-analogy","title":"\ud83d\udce6 Filing Cabinet Analogy","text":"<p>Think of Bronze as your filing cabinet where you store original documents. You never write on the originals \u2014 you make copies first.</p> <p>The Bronze layer: - Stores data exactly as received - Never modifies or cleans anything - Acts as your \"source of truth\" backup</p>"},{"location":"learning/curriculum/#hands-on-build-a-bronze-pipeline","title":"\ud83d\udcbb Hands-On: Build a Bronze Pipeline","text":"<ol> <li> <p>Copy your <code>customers.csv</code> to <code>data/landing/</code></p> </li> <li> <p>Edit <code>odibi.yaml</code>:</p> </li> </ol> <pre><code>project: \"my_first_project\"\nengine: \"pandas\"\n\nconnections:\n  local:\n    type: local\n    base_path: \"./data\"\n\nstory:\n  connection: local\n  path: stories\n\nsystem:\n  connection: local\n  path: system\n\npipelines:\n  - pipeline: bronze_customers\n    layer: bronze\n    description: \"Load raw customer data\"\n    nodes:\n      - name: raw_customers\n        description: \"Ingest customers from landing zone\"\n\n        read:\n          connection: local\n          path: landing/customers.csv\n          format: csv\n\n        write:\n          connection: local\n          path: bronze/customers\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run your pipeline:</li> </ol> <pre><code>odibi run odibi.yaml\n</code></pre> <ol> <li>Check your output: <pre><code># You should see a parquet file in data/bronze/customers/\nls data/bronze/customers/\n</code></pre></li> </ol>"},{"location":"learning/curriculum/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>Odibi read your CSV file</li> <li>Converted it to Parquet format (more efficient for analytics)</li> <li>Saved it to the Bronze layer</li> </ol> <p>No data was modified \u2014 just preserved in a better format.</p> <p>\ud83d\udcd6 Deep Dive: Bronze Layer Tutorial</p>"},{"location":"learning/curriculum/#self-check_3","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why don't we clean data in Bronze?</li> <li>[ ] What format did we convert the CSV to?</li> </ul>"},{"location":"learning/curriculum/#day-5-multi-node-pipelines","title":"Day 5: Multi-Node Pipelines","text":""},{"location":"learning/curriculum/#train-cars-analogy","title":"\ud83d\ude82 Train Cars Analogy","text":"<p>A pipeline is like a train. Each node is a train car \u2014 they're connected and run in sequence.</p>"},{"location":"learning/curriculum/#hands-on-add-more-data","title":"\ud83d\udcbb Hands-On: Add More Data","text":"<ol> <li>Create <code>data/landing/orders.csv</code>:</li> </ol> <pre><code>order_id,customer_id,product,amount,order_date\n1001,1,Widget A,29.99,2024-01-20\n1002,2,Widget B,49.99,2024-02-25\n1003,1,Widget C,19.99,2024-03-15\n1004,3,Widget A,29.99,2024-03-20\n</code></pre> <ol> <li>Add a second node to your pipeline:</li> </ol> <pre><code>pipelines:\n  - pipeline: bronze_ingest\n    layer: bronze\n    description: \"Load all raw data\"\n    nodes:\n      - name: raw_customers\n        description: \"Ingest customers\"\n        read:\n          connection: local\n          path: landing/customers.csv\n          format: csv\n        write:\n          connection: local\n          path: bronze/customers\n          format: parquet\n          mode: overwrite\n\n      - name: raw_orders\n        description: \"Ingest orders\"\n        read:\n          connection: local\n          path: landing/orders.csv\n          format: csv\n        write:\n          connection: local\n          path: bronze/orders\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run it: <pre><code>odibi run odibi.yaml\n</code></pre></li> </ol> <p>Both datasets are now in your Bronze layer.</p>"},{"location":"learning/curriculum/#self-check_4","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What is a \"node\" in Odibi?</li> <li>[ ] Can a pipeline have multiple nodes?</li> </ul>"},{"location":"learning/curriculum/#week-1-summary","title":"\ud83d\udcdd Week 1 Summary","text":"<p>You learned: - Data comes in different formats (CSV, JSON, Parquet) - Pipelines move data through stages (ETL) - Medallion architecture organizes data into layers - Bronze layer stores raw, unmodified data - Odibi uses YAML configuration to define pipelines</p> <p>Congratulations! You've built your first data pipeline. \ud83c\udf89</p>"},{"location":"learning/curriculum/#week-2-silver-layer-scd2-data-quality","title":"\ud83d\udcc5 Week 2: Silver Layer + SCD2 + Data Quality","text":""},{"location":"learning/curriculum/#learning-objectives_1","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Clean and transform data in the Silver layer - Understand and implement SCD2 (history tracking) - Add data quality checks to catch bad data - Handle missing values and invalid data</p>"},{"location":"learning/curriculum/#prerequisites_1","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - Completed Week 1 - A working Bronze layer with customer and order data</p>"},{"location":"learning/curriculum/#day-1-why-data-cleaning-matters","title":"Day 1: Why Data Cleaning Matters","text":""},{"location":"learning/curriculum/#dirty-kitchen-analogy","title":"\ud83e\uddf9 Dirty Kitchen Analogy","text":"<p>Imagine cooking with ingredients covered in dirt, or using expired milk. The result would be... unpleasant.</p> <p>Bad data causes: - Wrong business decisions - Broken reports - Angry users - Lost revenue</p>"},{"location":"learning/curriculum/#common-data-problems","title":"Common Data Problems","text":"Problem Example Impact Missing values <code>email: NULL</code> Can't contact customer Invalid format <code>date: \"not-a-date\"</code> Calculations fail Duplicates Same order twice Revenue doubled incorrectly Inconsistent \"CA\", \"California\", \"ca\" Grouping breaks"},{"location":"learning/curriculum/#the-silver-layers-job","title":"The Silver Layer's Job","text":"<p>The Silver layer is your cleaning station: - Fix data types (strings to dates, etc.) - Remove duplicates - Handle missing values - Validate data quality</p>"},{"location":"learning/curriculum/#self-check_5","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Name 3 common data quality problems</li> <li>[ ] What layer handles data cleaning?</li> </ul>"},{"location":"learning/curriculum/#day-2-building-a-silver-pipeline","title":"Day 2: Building a Silver Pipeline","text":""},{"location":"learning/curriculum/#hands-on-clean-your-customer-data","title":"\ud83d\udcbb Hands-On: Clean Your Customer Data","text":"<ol> <li>Update <code>odibi.yaml</code> to add a Silver pipeline:</li> </ol> <pre><code>pipelines:\n  # ... your bronze pipeline from Week 1 ...\n\n  - pipeline: silver_customers\n    layer: silver\n    description: \"Clean and standardize customers\"\n    nodes:\n      - name: clean_customers\n        description: \"Apply cleaning transformations\"\n\n        read:\n          connection: local\n          path: bronze/customers\n          format: parquet\n\n        transform:\n          - type: rename\n            columns:\n              id: customer_id\n\n          - type: cast\n            columns:\n              customer_id: integer\n              signup_date: date\n\n          - type: fill_null\n            columns:\n              email: \"unknown@example.com\"\n\n          - type: lower\n            columns:\n              - email\n\n        write:\n          connection: local\n          path: silver/customers\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run it: <pre><code>odibi run odibi.yaml --pipeline silver_customers\n</code></pre></li> </ol>"},{"location":"learning/curriculum/#what-each-transform-does","title":"What Each Transform Does","text":"Transform Purpose Example <code>rename</code> Change column names <code>id</code> \u2192 <code>customer_id</code> <code>cast</code> Change data types String \u2192 Date <code>fill_null</code> Replace missing values NULL \u2192 default value <code>lower</code> Lowercase text \"BOB@EMAIL.COM\" \u2192 \"bob@email.com\" <p>\ud83d\udcd6 Deep Dive: Silver Layer Tutorial</p>"},{"location":"learning/curriculum/#self-check_6","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does <code>cast</code> do?</li> <li>[ ] Why lowercase email addresses?</li> </ul>"},{"location":"learning/curriculum/#day-3-scd2-tracking-history","title":"Day 3: SCD2 \u2014 Tracking History","text":""},{"location":"learning/curriculum/#time-machine-analogy","title":"\u23f0 Time Machine Analogy","text":"<p>Imagine you could look at a customer's record as it was 6 months ago. Where did they live? What tier were they?</p> <p>SCD2 (Slowly Changing Dimension Type 2) makes this possible by: - Never deleting old records - Adding new versions when data changes - Tracking when each version was valid</p>"},{"location":"learning/curriculum/#visual-example","title":"Visual Example","text":"<p>Customer moves from CA to NY on Feb 1:</p> customer_id address valid_from valid_to is_current 101 CA 2024-01-01 2024-02-01 false 101 NY 2024-02-01 NULL true <p>Now you can answer: \"Where did customer 101 live on January 15th?\" \u2192 CA</p>"},{"location":"learning/curriculum/#hands-on-add-scd2-to-customers","title":"\ud83d\udcbb Hands-On: Add SCD2 to Customers","text":"<ol> <li>First, update your source data. Create <code>data/landing/customers_update.csv</code>:</li> </ol> <pre><code>id,name,email,signup_date,tier\n1,Alice,alice@example.com,2024-01-15,Gold\n2,Bob,bob_new@example.com,2024-02-20,Silver\n3,Charlie,charlie@example.com,2024-03-10,Bronze\n4,Diana,diana@example.com,2024-04-01,Gold\n</code></pre> <p>(Notice: Bob has a new email, and Diana is a new customer)</p> <ol> <li>Add an SCD2 node:</li> </ol> <pre><code>  - pipeline: silver_customers_scd2\n    layer: silver\n    description: \"Track customer history\"\n    nodes:\n      - name: customers_with_history\n        description: \"Apply SCD2 for full history\"\n\n        read:\n          connection: local\n          path: landing/customers_update.csv\n          format: csv\n\n        transformer: scd2\n        params:\n          connection: local\n          path: silver/dim_customers\n          keys:\n            - id\n          track_cols:\n            - email\n            - tier\n          effective_time_col: signup_date\n\n        write:\n          connection: local\n          path: silver/dim_customers\n          format: parquet\n          mode: overwrite\n</code></pre> <ol> <li>Run it: <pre><code>odibi run odibi.yaml --pipeline silver_customers_scd2\n</code></pre></li> </ol> <p>\ud83d\udcd6 Deep Dive: SCD2 Pattern</p>"},{"location":"learning/curriculum/#self-check_7","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does SCD2 stand for?</li> <li>[ ] What column tells you if a record is the current version?</li> </ul>"},{"location":"learning/curriculum/#day-4-data-quality-validation","title":"Day 4: Data Quality Validation","text":""},{"location":"learning/curriculum/#security-guard-analogy","title":"\ud83d\udea8 Security Guard Analogy","text":"<p>Before entering a building, security checks your ID. Data quality validation checks your data before it enters the Silver layer.</p>"},{"location":"learning/curriculum/#types-of-checks","title":"Types of Checks","text":"Check Type What it does Example not_null Ensure value exists <code>customer_id</code> can't be empty unique No duplicates Each <code>email</code> is unique range Value in bounds <code>age</code> between 0 and 150 regex Pattern matching Email contains <code>@</code> foreign_key Reference exists <code>customer_id</code> exists in customers table"},{"location":"learning/curriculum/#hands-on-add-validation","title":"\ud83d\udcbb Hands-On: Add Validation","text":"<ol> <li>Add validation to your node:</li> </ol> <pre><code>      - name: clean_customers\n        description: \"Clean with validation\"\n\n        read:\n          connection: local\n          path: bronze/customers\n          format: parquet\n\n        validation:\n          rules:\n            - column: customer_id\n              check: not_null\n              severity: error\n\n            - column: email\n              check: not_null\n              severity: warn\n\n            - column: email\n              check: regex\n              pattern: \".*@.*\\\\..*\"\n              severity: error\n\n          on_failure: quarantine  # Bad rows go to quarantine\n\n        write:\n          connection: local\n          path: silver/customers\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"learning/curriculum/#severity-levels","title":"Severity Levels","text":"Level What happens <code>warn</code> Log warning, continue processing <code>error</code> Quarantine bad rows, continue with good rows <code>fatal</code> Stop the entire pipeline <p>\ud83d\udcd6 Deep Dive: Data Validation</p>"},{"location":"learning/curriculum/#self-check_8","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does <code>quarantine</code> mean?</li> <li>[ ] What's the difference between <code>warn</code> and <code>error</code> severity?</li> </ul>"},{"location":"learning/curriculum/#day-5-putting-it-together","title":"Day 5: Putting It Together","text":""},{"location":"learning/curriculum/#hands-on-complete-silver-pipeline","title":"\ud83d\udcbb Hands-On: Complete Silver Pipeline","text":"<p>Create a complete Silver pipeline that: 1. Reads from Bronze 2. Cleans and transforms 3. Validates quality 4. Tracks history with SCD2</p> <pre><code>  - pipeline: silver_complete\n    layer: silver\n    description: \"Complete silver processing\"\n    nodes:\n      - name: stg_customers\n        description: \"Stage and clean customers\"\n\n        read:\n          connection: local\n          path: bronze/customers\n          format: parquet\n\n        transform:\n          - type: rename\n            columns:\n              id: customer_id\n          - type: cast\n            columns:\n              customer_id: integer\n              signup_date: date\n          - type: trim\n            columns:\n              - name\n              - email\n\n        validation:\n          rules:\n            - column: customer_id\n              check: not_null\n              severity: error\n            - column: email\n              check: regex\n              pattern: \".*@.*\"\n              severity: warn\n          on_failure: quarantine\n\n        write:\n          connection: local\n          path: silver/stg_customers\n          format: parquet\n          mode: overwrite\n\n      - name: dim_customers\n        description: \"Create customer dimension with history\"\n        depends_on:\n          - stg_customers\n\n        read:\n          connection: local\n          path: silver/stg_customers\n          format: parquet\n\n        transformer: scd2\n        params:\n          connection: local\n          path: silver/dim_customers\n          keys:\n            - customer_id\n          track_cols:\n            - name\n            - email\n          effective_time_col: signup_date\n\n        write:\n          connection: local\n          path: silver/dim_customers\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"learning/curriculum/#self-check_9","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does <code>depends_on</code> do?</li> <li>[ ] Why do we stage data before applying SCD2?</li> </ul>"},{"location":"learning/curriculum/#week-2-summary","title":"\ud83d\udcdd Week 2 Summary","text":"<p>You learned: - Why data cleaning is critical - How to transform data (rename, cast, fill_null) - SCD2 tracks historical changes - Validation catches bad data before it causes problems - Quarantine isolates bad rows for review</p> <p>Great progress! Your data is now clean and trackable. \ud83c\udf89</p>"},{"location":"learning/curriculum/#week-3-gold-layer-dimensional-modeling","title":"\ud83d\udcc5 Week 3: Gold Layer + Dimensional Modeling","text":""},{"location":"learning/curriculum/#learning-objectives_2","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Understand Facts vs Dimensions - Build a star schema - Use surrogate keys - Create aggregations for reporting - Build a complete data warehouse</p>"},{"location":"learning/curriculum/#prerequisites_2","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - Completed Weeks 1 and 2 - Working Bronze and Silver layers</p>"},{"location":"learning/curriculum/#day-1-facts-vs-dimensions","title":"Day 1: Facts vs Dimensions","text":""},{"location":"learning/curriculum/#theater-analogy","title":"\ud83c\udfad Theater Analogy","text":"<p>Think of a theater production: - Facts = The events (ticket sales, performances) - Dimensions = The context (who, what, when, where)</p> <p>Every fact answers: \"What happened?\" Every dimension answers: \"Tell me more about...\"</p>"},{"location":"learning/curriculum/#examples","title":"Examples","text":"Facts (Events) Dimensions (Context) Order placed Customer, Product, Date Payment received Customer, Account, Date Page viewed User, Page, Date"},{"location":"learning/curriculum/#visual-a-sales-transaction","title":"Visual: A Sales Transaction","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     FACT: Order                             \u2502\n\u2502  order_id=1001, amount=49.99, quantity=2                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502          \u2502          \u2502\n           \u25bc          \u25bc          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 DIM: Customer\u2502 \u2502 DIM: Product \u2502 \u2502 DIM: Date    \u2502\n\u2502 name=Alice   \u2502 \u2502 name=Widget  \u2502 \u2502 date=2024-01 \u2502\n\u2502 tier=Gold    \u2502 \u2502 category=HW  \u2502 \u2502 quarter=Q1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"learning/curriculum/#self-check_10","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Is \"order amount\" a fact or dimension?</li> <li>[ ] Is \"customer name\" a fact or dimension?</li> </ul>"},{"location":"learning/curriculum/#day-2-star-schema-basics","title":"Day 2: Star Schema Basics","text":""},{"location":"learning/curriculum/#star-analogy","title":"\u2b50 Star Analogy","text":"<p>A star schema looks like a star: the fact table is in the center, with dimension tables around it like points.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 dim_product \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dim_customer\u2502\u2500\u2500\u2500\u2500\u2500\u2502  fact_sales \u2502\u2500\u2500\u2500\u2500\u2500\u2502  dim_date   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 dim_location\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why stars? - Simple to understand - Fast to query (fewer joins) - Works with every BI tool</p>"},{"location":"learning/curriculum/#dimension-table-structure","title":"Dimension Table Structure","text":"<pre><code># dim_customers\ncustomer_key: 1          # Surrogate key (system-generated)\ncustomer_id: \"C001\"      # Natural key (from source)\nname: \"Alice\"\nemail: \"alice@example.com\"\ntier: \"Gold\"\neffective_from: \"2024-01-01\"\neffective_to: null\nis_current: true\n</code></pre>"},{"location":"learning/curriculum/#fact-table-structure","title":"Fact Table Structure","text":"<pre><code># fact_orders\norder_key: 1001\ncustomer_key: 1          # Points to dim_customers\nproduct_key: 42          # Points to dim_products\ndate_key: 20240120       # Points to dim_date\nquantity: 2\namount: 49.99\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Dimensional Modeling Guide</p>"},{"location":"learning/curriculum/#self-check_11","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why is it called a \"star\" schema?</li> <li>[ ] What's in the center of the star?</li> </ul>"},{"location":"learning/curriculum/#day-3-surrogate-keys","title":"Day 3: Surrogate Keys","text":""},{"location":"learning/curriculum/#hotel-room-key-analogy","title":"\ud83d\udd11 Hotel Room Key Analogy","text":"<p>When you check into a hotel, they give you a room key. This key is: - Unique to your stay (not your name) - System-generated (you don't choose it) - Internal (the hotel manages it)</p> <p>A surrogate key works the same way: - Unique identifier for each record - Generated by the system (not from source data) - Never changes, even if source data changes</p>"},{"location":"learning/curriculum/#why-not-use-natural-keys","title":"Why Not Use Natural Keys?","text":"Problem Natural Key Example Issue Changes SSN gets corrected Breaks all references Duplicates \"John Smith\" Too common Missing New customer, no ID yet Can't insert Composite firstName + lastName + DOB Slow to join"},{"location":"learning/curriculum/#hands-on-generate-surrogate-keys","title":"\ud83d\udcbb Hands-On: Generate Surrogate Keys","text":"<p>Odibi can auto-generate surrogate keys:</p> <pre><code>  - pipeline: gold_dimensions\n    layer: gold\n    description: \"Build dimension tables\"\n    nodes:\n      - name: dim_customers\n        description: \"Customer dimension with surrogate keys\"\n\n        read:\n          connection: local\n          path: silver/dim_customers\n          format: parquet\n\n        transform:\n          - type: generate_surrogate_key\n            key_column: customer_key\n            source_columns:\n              - customer_id\n\n        write:\n          connection: local\n          path: gold/dim_customers\n          format: parquet\n          mode: overwrite\n</code></pre> <p>The <code>generate_surrogate_key</code> transform creates a unique integer for each unique combination of source columns.</p> <p>\ud83d\udcd6 Deep Dive: Dimension Pattern</p>"},{"location":"learning/curriculum/#self-check_12","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What's wrong with using email as a primary key?</li> <li>[ ] Who generates surrogate keys \u2014 the source system or our data warehouse?</li> </ul>"},{"location":"learning/curriculum/#day-4-building-fact-tables","title":"Day 4: Building Fact Tables","text":""},{"location":"learning/curriculum/#hands-on-create-a-sales-fact-table","title":"\ud83d\udcbb Hands-On: Create a Sales Fact Table","text":"<ol> <li>First, ensure you have a date dimension. Create <code>data/landing/dates.csv</code>:</li> </ol> <pre><code>date_key,full_date,year,quarter,month,day_of_week\n20240115,2024-01-15,2024,Q1,January,Monday\n20240120,2024-01-20,2024,Q1,January,Saturday\n20240220,2024-02-20,2024,Q1,February,Tuesday\n20240225,2024-02-25,2024,Q1,February,Sunday\n20240310,2024-03-10,2024,Q1,March,Sunday\n20240315,2024-03-15,2024,Q1,March,Friday\n20240320,2024-03-20,2024,Q1,March,Wednesday\n</code></pre> <ol> <li>Build the fact table:</li> </ol> <pre><code>      - name: fact_orders\n        description: \"Order fact table\"\n        depends_on:\n          - dim_customers\n\n        read:\n          - connection: local\n            path: silver/orders\n            format: parquet\n            alias: orders\n          - connection: local\n            path: gold/dim_customers\n            format: parquet\n            alias: customers\n\n        transform:\n          - type: join\n            left: orders\n            right: customers\n            on:\n              - left: customer_id\n                right: customer_id\n            how: left\n            filter: \"is_current = true\"  # Only join to current customer version\n\n          - type: select\n            columns:\n              - order_id\n              - customer_key\n              - product\n              - amount\n              - order_date\n\n          - type: cast\n            columns:\n              order_date: date\n\n          - type: add_column\n            name: date_key\n            expression: \"date_format(order_date, 'yyyyMMdd')\"\n\n        write:\n          connection: local\n          path: gold/fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Fact Pattern</p>"},{"location":"learning/curriculum/#self-check_13","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why do we filter for <code>is_current = true</code> when joining?</li> <li>[ ] What's the purpose of <code>date_key</code>?</li> </ul>"},{"location":"learning/curriculum/#day-5-aggregations-for-reporting","title":"Day 5: Aggregations for Reporting","text":""},{"location":"learning/curriculum/#summary-reports-analogy","title":"\ud83d\udcca Summary Reports Analogy","text":"<p>Instead of reading every receipt, store managers want: - \"Total sales this month\" - \"Average order size by customer tier\" - \"Top 10 products\"</p> <p>Aggregations pre-compute these summaries.</p>"},{"location":"learning/curriculum/#hands-on-build-an-aggregation","title":"\ud83d\udcbb Hands-On: Build an Aggregation","text":"<pre><code>  - pipeline: gold_aggregations\n    layer: gold\n    description: \"Pre-computed summaries\"\n    nodes:\n      - name: agg_sales_by_customer\n        description: \"Sales summary per customer\"\n\n        read:\n          connection: local\n          path: gold/fact_orders\n          format: parquet\n\n        pattern:\n          type: aggregation\n          params:\n            group_by:\n              - customer_key\n            metrics:\n              - name: total_orders\n                expression: \"count(*)\"\n              - name: total_revenue\n                expression: \"sum(amount)\"\n              - name: avg_order_value\n                expression: \"avg(amount)\"\n              - name: first_order_date\n                expression: \"min(order_date)\"\n              - name: last_order_date\n                expression: \"max(order_date)\"\n\n        write:\n          connection: local\n          path: gold/agg_sales_by_customer\n          format: parquet\n          mode: overwrite\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Aggregation Pattern</p>"},{"location":"learning/curriculum/#self-check_14","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why pre-compute aggregations instead of calculating on-the-fly?</li> <li>[ ] What does <code>group_by</code> do?</li> </ul>"},{"location":"learning/curriculum/#week-3-summary","title":"\ud83d\udcdd Week 3 Summary","text":"<p>You learned: - Facts record events, dimensions provide context - Star schemas are simple and fast - Surrogate keys are stable, system-generated identifiers - Fact tables link to dimensions via keys - Aggregations pre-compute summaries for fast reporting</p> <p>Amazing work! You've built a complete data warehouse. \ud83c\udf89</p>"},{"location":"learning/curriculum/#week-4-production-deployment-best-practices","title":"\ud83d\udcc5 Week 4: Production Deployment + Best Practices","text":""},{"location":"learning/curriculum/#learning-objectives_3","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this week, you will: - Configure connections for different environments - Implement error handling and retry logic - Add monitoring and logging - Tune performance for large datasets - Deploy to production with confidence</p>"},{"location":"learning/curriculum/#prerequisites_3","title":"\u2705 Prerequisites","text":"<p>Before starting, make sure you have: - Completed Weeks 1-3 - A complete Bronze \u2192 Silver \u2192 Gold pipeline</p>"},{"location":"learning/curriculum/#day-1-connections-and-environments","title":"Day 1: Connections and Environments","text":""},{"location":"learning/curriculum/#different-homes-analogy","title":"\ud83c\udfe0 Different Homes Analogy","text":"<p>Your pipeline needs to work in different \"homes\": - Development \u2014 Your laptop, small test data - Staging \u2014 Test server, realistic data - Production \u2014 Real deal, live data</p> <p>Each environment has different connection details.</p>"},{"location":"learning/curriculum/#hands-on-configure-environments","title":"\ud83d\udcbb Hands-On: Configure Environments","text":"<pre><code>project: \"my_project\"\nengine: \"pandas\"\n\n# Global variables\nvars:\n  env: ${ODIBI_ENV:dev}  # Default to 'dev' if not set\n\n# Environment-specific overrides\nenvironments:\n  dev:\n    connections:\n      data_lake:\n        type: local\n        base_path: \"./data\"\n\n  staging:\n    connections:\n      data_lake:\n        type: azure_blob\n        account_name: \"mystorageacct\"\n        container: \"staging-data\"\n        credential: ${AZURE_STORAGE_KEY}\n\n  prod:\n    connections:\n      data_lake:\n        type: azure_blob\n        account_name: \"prodstorageacct\"\n        container: \"prod-data\"\n        credential: ${AZURE_STORAGE_KEY}\n\nconnections:\n  data_lake:\n    type: local\n    base_path: \"./data\"\n</code></pre> <p>Run for a specific environment: <pre><code>ODIBI_ENV=staging odibi run odibi.yaml\n</code></pre></p> <p>\ud83d\udcd6 Deep Dive: Environments Guide</p>"},{"location":"learning/curriculum/#self-check_15","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why use environment variables for credentials?</li> <li>[ ] What's the default environment if <code>ODIBI_ENV</code> isn't set?</li> </ul>"},{"location":"learning/curriculum/#day-2-error-handling-and-retry-logic","title":"Day 2: Error Handling and Retry Logic","text":""},{"location":"learning/curriculum/#retry-analogy","title":"\ud83d\udd04 Retry Analogy","text":"<p>If your phone call fails, you try again. Networks are unreliable; databases timeout. Retries handle temporary failures.</p>"},{"location":"learning/curriculum/#hands-on-configure-retries","title":"\ud83d\udcbb Hands-On: Configure Retries","text":"<pre><code>project: \"production_pipeline\"\nengine: \"spark\"\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential    # Wait longer between each retry\n  initial_delay: 5        # First retry after 5 seconds\n  max_delay: 300          # Never wait more than 5 minutes\n\npipelines:\n  - pipeline: bronze_ingest\n    nodes:\n      - name: fetch_api_data\n        retry:\n          max_attempts: 5  # Override for this node\n        read:\n          connection: external_api\n          path: /customers\n          format: json\n</code></pre>"},{"location":"learning/curriculum/#backoff-strategies","title":"Backoff Strategies","text":"Strategy Wait times (5s initial) Best for <code>constant</code> 5s, 5s, 5s Simple cases <code>linear</code> 5s, 10s, 15s Gradual increase <code>exponential</code> 5s, 10s, 20s, 40s API rate limits"},{"location":"learning/curriculum/#handling-failures","title":"Handling Failures","text":"<pre><code>        on_failure: continue  # Options: fail, continue, skip\n</code></pre> Action Behavior <code>fail</code> Stop entire pipeline (default) <code>continue</code> Log error, continue to next node <code>skip</code> Skip downstream nodes that depend on this one"},{"location":"learning/curriculum/#self-check_16","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What does \"exponential backoff\" mean?</li> <li>[ ] When would you use <code>on_failure: continue</code>?</li> </ul>"},{"location":"learning/curriculum/#day-3-monitoring-and-logging","title":"Day 3: Monitoring and Logging","text":""},{"location":"learning/curriculum/#dashboard-analogy","title":"\ud83d\udcfa Dashboard Analogy","text":"<p>A pilot needs instruments to fly safely. You need monitoring to run pipelines safely.</p>"},{"location":"learning/curriculum/#hands-on-configure-logging","title":"\ud83d\udcbb Hands-On: Configure Logging","text":"<pre><code>logging:\n  level: INFO              # DEBUG, INFO, WARNING, ERROR\n  structured: true         # JSON format for log aggregators\n  include_metrics: true    # Row counts, timing\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK_URL}\n    on_events:\n      - on_failure\n      - on_success\n\n  - type: email\n    to:\n      - data-team@company.com\n    on_events:\n      - on_failure\n</code></pre>"},{"location":"learning/curriculum/#what-gets-logged","title":"What Gets Logged","text":"<p>Every pipeline run generates a Data Story with: - Start/end timestamps - Row counts (read/written/quarantined) - Validation results - Error messages</p> <p>View your story: <pre><code>odibi story show --latest\n</code></pre></p>"},{"location":"learning/curriculum/#self-check_17","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What's the difference between INFO and DEBUG logging?</li> <li>[ ] What is a \"Data Story\"?</li> </ul>"},{"location":"learning/curriculum/#day-4-performance-tuning","title":"Day 4: Performance Tuning","text":""},{"location":"learning/curriculum/#race-car-analogy","title":"\ud83c\udfce\ufe0f Race Car Analogy","text":"<p>A race car needs tuning to go fast. Data pipelines need tuning for large datasets.</p>"},{"location":"learning/curriculum/#key-performance-levers","title":"Key Performance Levers","text":"Lever When to use Configuration Partitioning Large tables (&gt;1M rows) Split data by date/category Caching Reused datasets Keep in memory Parallelism Multiple nodes Run independent nodes together Batch size Memory limits Process in chunks"},{"location":"learning/curriculum/#hands-on-add-partitioning","title":"\ud83d\udcbb Hands-On: Add Partitioning","text":"<pre><code>        write:\n          connection: data_lake\n          path: gold/fact_orders\n          format: delta\n          mode: overwrite\n          partition_by:\n            - order_year\n            - order_month\n</code></pre>"},{"location":"learning/curriculum/#hands-on-enable-caching","title":"\ud83d\udcbb Hands-On: Enable Caching","text":"<pre><code>      - name: dim_customers\n        cache: true          # Keep in memory for downstream nodes\n        read:\n          connection: local\n          path: silver/dim_customers\n</code></pre>"},{"location":"learning/curriculum/#hands-on-performance-config","title":"\ud83d\udcbb Hands-On: Performance Config","text":"<pre><code>performance:\n  max_parallel_nodes: 4    # Run up to 4 nodes simultaneously\n  batch_size: 100000       # Process 100k rows at a time\n  shuffle_partitions: 200  # Spark shuffle partitions\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Performance Tuning Guide</p>"},{"location":"learning/curriculum/#self-check_18","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] Why partition by date?</li> <li>[ ] What does caching do?</li> </ul>"},{"location":"learning/curriculum/#day-5-production-deployment-checklist","title":"Day 5: Production Deployment Checklist","text":""},{"location":"learning/curriculum/#launch-checklist","title":"\ud83d\ude80 Launch Checklist","text":"<p>Before deploying to production, verify:</p>"},{"location":"learning/curriculum/#configuration","title":"Configuration","text":"<ul> <li>[ ] All secrets use environment variables (never hardcoded)</li> <li>[ ] Correct environment settings for prod</li> <li>[ ] Retry logic enabled</li> <li>[ ] Alerts configured</li> </ul>"},{"location":"learning/curriculum/#data-quality","title":"Data Quality","text":"<ul> <li>[ ] Validation rules on all critical columns</li> <li>[ ] Quarantine configured for bad rows</li> <li>[ ] Foreign key checks enabled</li> </ul>"},{"location":"learning/curriculum/#performance","title":"Performance","text":"<ul> <li>[ ] Partitioning on large tables</li> <li>[ ] Appropriate parallelism</li> <li>[ ] Tested with production-scale data</li> </ul>"},{"location":"learning/curriculum/#operations","title":"Operations","text":"<ul> <li>[ ] Logging at INFO level</li> <li>[ ] Monitoring dashboard set up</li> <li>[ ] Runbook for common failures</li> <li>[ ] Backup/restore procedures documented</li> </ul>"},{"location":"learning/curriculum/#complete-production-config","title":"Complete Production Config","text":"<pre><code>project: \"customer360\"\nengine: \"spark\"\nversion: \"1.0.0\"\nowner: \"data-team@company.com\"\ndescription: \"Customer analytics pipeline\"\n\nvars:\n  env: ${ODIBI_ENV:prod}\n\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: exponential\n\nlogging:\n  level: INFO\n  structured: true\n  include_metrics: true\n\nalerts:\n  - type: slack\n    url: ${SLACK_WEBHOOK}\n    on_events: [on_failure]\n\nperformance:\n  max_parallel_nodes: 8\n  batch_size: 500000\n\nconnections:\n  data_lake:\n    type: azure_blob\n    account_name: ${AZURE_STORAGE_ACCOUNT}\n    container: \"prod-data\"\n    credential: ${AZURE_STORAGE_KEY}\n\nstory:\n  connection: data_lake\n  path: _odibi/stories\n\nsystem:\n  connection: data_lake\n  path: _odibi/system\n\npipelines:\n  # ... your pipelines ...\n</code></pre> <p>\ud83d\udcd6 Deep Dive: Production Deployment Guide</p>"},{"location":"learning/curriculum/#self-check_19","title":"\ud83e\uddea Self-Check","text":"<ul> <li>[ ] What should NEVER be hardcoded in config?</li> <li>[ ] What logging level is recommended for production?</li> </ul>"},{"location":"learning/curriculum/#week-4-summary","title":"\ud83d\udcdd Week 4 Summary","text":"<p>You learned: - Environments separate dev/staging/prod configurations - Retry logic handles temporary failures - Logging and alerts keep you informed - Partitioning and caching improve performance - A production checklist prevents common mistakes</p> <p>Congratulations! You've completed the Odibi curriculum! \ud83c\udf93\ud83c\udf89</p>"},{"location":"learning/curriculum/#whats-next","title":"\ud83c\udfaf What's Next?","text":"<p>Now that you've completed the basics:</p> <ol> <li>Build a real project \u2014 Apply what you learned to actual data</li> <li>Explore advanced patterns \u2014 Browse all patterns</li> <li>Learn the CLI \u2014 CLI Master Guide</li> <li>Join the community \u2014 Share your projects, ask questions</li> </ol>"},{"location":"learning/curriculum/#quick-reference-links","title":"Quick Reference Links","text":"Topic Link All Patterns ../patterns/README.md YAML Reference ../reference/yaml_schema.md Best Practices ../guides/best_practices.md Troubleshooting ../troubleshooting.md <p>Built with \u2764\ufe0f for data engineers who are just getting started.</p>"},{"location":"learning/data_engineering_101/","title":"Data Engineering 101: A Complete Beginner's Guide","text":"<p>Welcome to the world of data engineering. If you've never heard this term before, that's perfectly fine. This guide assumes you're starting from absolute zero. We'll explain every concept, every term, and every idea from the ground up.</p> <p>By the end of this guide, you'll understand what data engineers do, why their work matters, and how tools like Odibi help solve real problems.</p>"},{"location":"learning/data_engineering_101/#what-is-data","title":"What is Data?","text":"<p>Before we talk about \"data engineering,\" let's talk about \"data.\" You interact with data every single day, even if you don't realize it.</p>"},{"location":"learning/data_engineering_101/#data-is-just-information-stored-somewhere","title":"Data is Just Information Stored Somewhere","text":"<p>When you check your bank account online, you see a list of transactions. That list\u2014the dates, amounts, descriptions\u2014is data. It's information that's been recorded and stored so you can look at it later.</p> <p>When you shop online and receive an email receipt, that receipt contains data: what you bought, how much you paid, when the order was placed, where it's being shipped.</p> <p>When you scroll through your phone's photo gallery, each photo has data attached to it: the date it was taken, the location (if GPS was on), the file size, the camera settings.</p> <p>Data is simply recorded information. It can be numbers, text, dates, images, or anything else that can be stored.</p>"},{"location":"learning/data_engineering_101/#where-does-data-live","title":"Where Does Data Live?","text":"<p>Data needs a home. Just like you keep physical documents in folders and filing cabinets, digital data lives in specific places:</p> <p>Files are the simplest form. A file is just a container for data sitting on a computer's hard drive: - A <code>.csv</code> file (Comma-Separated Values) is like a spreadsheet saved as plain text - A <code>.json</code> file stores data in a structured, nested format - A <code>.txt</code> file is just raw text - An <code>.xlsx</code> file is a Microsoft Excel spreadsheet</p> <p>Databases are specialized software designed to store, organize, and retrieve large amounts of data efficiently. Think of a database as a super-powered filing cabinet that can find any document instantly, even if you have millions of them. Examples include PostgreSQL, MySQL, SQL Server, and Oracle.</p> <p>APIs (Application Programming Interfaces) are not storage themselves, but they're how different systems share data. When a weather app on your phone shows today's forecast, it's calling a weather API to get that data. The API is like a waiter in a restaurant\u2014you tell it what you want, and it brings it to you from the kitchen (the database).</p> <p>Data Lakes and Warehouses are massive storage systems designed for analytics. A data lake stores raw data in its original format (like a lake that collects water from many streams). A data warehouse stores cleaned, organized data ready for analysis (like a warehouse with neatly arranged shelves).</p>"},{"location":"learning/data_engineering_101/#types-of-data","title":"Types of Data","text":"<p>Not all data is created equal. Data comes in three main flavors:</p>"},{"location":"learning/data_engineering_101/#structured-data","title":"Structured Data","text":"<p>Structured data fits neatly into rows and columns, like a spreadsheet. Every row follows the same pattern. Every column has a specific type of information.</p> <p>Example: A customer table <pre><code>| customer_id | name          | email                | signup_date |\n|-------------|---------------|----------------------|-------------|\n| 1           | Alice Smith   | alice@email.com      | 2024-01-15  |\n| 2           | Bob Johnson   | bob@email.com        | 2024-02-20  |\n| 3           | Carol White   | carol@email.com      | 2024-03-10  |\n</code></pre></p> <p>This is structured data because: - Every customer has exactly the same fields - Each field has a consistent type (IDs are numbers, names are text, dates are dates) - You can easily search, sort, and filter this data</p> <p>Most business data is structured: sales records, inventory counts, employee information, financial transactions.</p>"},{"location":"learning/data_engineering_101/#semi-structured-data","title":"Semi-Structured Data","text":"<p>Semi-structured data has some organization, but it's flexible. Not every record needs to have the same fields.</p> <p>Example: JSON data from an API <pre><code>{\n  \"customer_id\": 1,\n  \"name\": \"Alice Smith\",\n  \"email\": \"alice@email.com\",\n  \"preferences\": {\n    \"newsletter\": true,\n    \"theme\": \"dark\"\n  },\n  \"tags\": [\"premium\", \"early-adopter\"]\n}\n</code></pre></p> <p>This is semi-structured because: - There's a clear organization (fields have names) - But the structure can vary (some customers might have a <code>phone</code> field, others might not) - Nested data is allowed (preferences contains its own sub-fields)</p> <p>JSON (JavaScript Object Notation) and XML are common semi-structured formats. They're often used for web data and configuration files.</p>"},{"location":"learning/data_engineering_101/#unstructured-data","title":"Unstructured Data","text":"<p>Unstructured data has no predefined format. It's raw and messy by nature.</p> <p>Examples: - A paragraph of text from a customer support email - An image file - A video recording - A voice memo</p> <p>Unstructured data is the hardest to work with because computers can't easily understand its meaning without special processing.</p>"},{"location":"learning/data_engineering_101/#data-you-encounter-daily","title":"Data You Encounter Daily","text":"<p>To make this concrete, here are data examples from everyday life:</p> Everyday Activity The Data Behind It Checking your bank balance Account numbers, transaction history, balances Getting a grocery receipt Product names, prices, quantities, totals, timestamps Using GPS navigation Location coordinates, road maps, traffic patterns Streaming music Song titles, artists, play counts, user preferences Ordering food delivery Menu items, prices, addresses, delivery times <p>Every digital interaction creates data. Data engineering is about managing, moving, and transforming all of this information so it can be useful.</p>"},{"location":"learning/data_engineering_101/#what-is-data-engineering","title":"What is Data Engineering?","text":"<p>Now that you understand what data is, let's talk about what data engineers actually do.</p>"},{"location":"learning/data_engineering_101/#the-plumbing-of-the-data-world","title":"The \"Plumbing\" of the Data World","text":"<p>Imagine a city's water system. Water needs to flow from reservoirs, through treatment plants, into pipes, and eventually out of your faucet. Someone has to design those pipes, build them, maintain them, and make sure clean water arrives when you turn the handle.</p> <p>Data engineering is the same idea, but for data instead of water.</p> <p>Data engineers build and maintain the systems that move data from where it's created to where it's needed.</p> <p>Just like you don't think about water pipes when you take a shower, most people don't think about data pipelines when they look at a dashboard. But those pipelines are essential. Without them, the data would never arrive.</p>"},{"location":"learning/data_engineering_101/#what-data-engineers-actually-do","title":"What Data Engineers Actually Do","text":"<p>On a typical day, a data engineer might:</p> <ol> <li> <p>Build data pipelines: Create automated processes that move data from one place to another. For example, extracting sales data from an e-commerce system and loading it into a data warehouse.</p> </li> <li> <p>Transform data: Clean, reshape, and combine data so it's ready for analysis. Raw data is often messy\u2014data engineers make it usable.</p> </li> <li> <p>Ensure data quality: Check that data is accurate, complete, and consistent. If bad data gets into a system, every report and decision based on it will be wrong.</p> </li> <li> <p>Maintain infrastructure: Keep databases running, optimize queries that are too slow, and troubleshoot when something breaks.</p> </li> <li> <p>Collaborate with analysts and scientists: Data engineers don't usually create the final reports or machine learning models\u2014they provide the clean data that makes those things possible.</p> </li> </ol>"},{"location":"learning/data_engineering_101/#data-engineering-vs-data-science-vs-data-analytics","title":"Data Engineering vs Data Science vs Data Analytics","text":"<p>These three roles are related but distinct:</p> Role Focus Key Question Data Engineer Building systems to move and prepare data \"How do we get clean data from A to B?\" Data Analyst Creating reports and dashboards from data \"What happened? What does the data show?\" Data Scientist Building predictive models and finding patterns \"What will happen? Can we predict X?\" <p>Analogy: Think of a restaurant. - The Data Engineer is the chef who prepares the ingredients\u2014washing vegetables, cutting meat, making sure everything is fresh and ready. - The Data Analyst is the cook who follows recipes to create dishes (reports) from those ingredients. - The Data Scientist is the chef who invents new recipes, experimenting with ingredients to create something new (predictions, recommendations).</p> <p>You can't have great dishes without properly prepared ingredients. That's why data engineering is foundational.</p>"},{"location":"learning/data_engineering_101/#a-real-job-scenario","title":"A Real Job Scenario","text":"<p>Let's say you work at an online clothing retailer. The company wants to know which products are selling best in each region.</p> <p>Without data engineering, someone would: 1. Log into the sales system manually 2. Export data to Excel 3. Spend hours cleaning up the data (fixing typos, removing duplicates) 4. Copy-paste data from different sources 5. Manually calculate totals 6. Repeat this process every week</p> <p>With data engineering, you would: 1. Build a pipeline that automatically extracts sales data every night 2. Join it with product and region data 3. Clean and standardize everything automatically 4. Load it into a warehouse where analysts can query it instantly 5. The pipeline runs every night without human intervention</p> <p>The first approach might work for a small company. But when you have millions of transactions, hundreds of products, and dozens of regions, manual work becomes impossible. That's when you need data engineering.</p>"},{"location":"learning/data_engineering_101/#what-is-a-data-pipeline","title":"What is a Data Pipeline?","text":"<p>A data pipeline is an automated process that moves data from one place to another, often transforming it along the way.</p>"},{"location":"learning/data_engineering_101/#the-factory-assembly-line-analogy","title":"The Factory Assembly Line Analogy","text":"<p>Imagine a car factory. Raw materials (steel, glass, rubber) enter at one end. They pass through various stations where workers and machines shape them, assemble them, and quality-check them. At the other end, finished cars roll off the line.</p> <p>A data pipeline works the same way:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Raw Data  \u2502\u2500\u2500\u2500\u25b6\u2502  Station 1  \u2502\u2500\u2500\u2500\u25b6\u2502  Station 2  \u2502\u2500\u2500\u2500\u25b6\u2502 Final Data  \u2502\n\u2502   (Input)   \u2502    \u2502  (Extract)  \u2502    \u2502 (Transform) \u2502    \u2502  (Output)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Input: The raw data enters the pipeline. This might be CSV files, API responses, database tables, or event streams.</p> <p>Processing Stations: Each station does something specific to the data: - Clean up messy values - Convert data types (text to dates, for example) - Join data from multiple sources - Calculate new fields - Filter out irrelevant records</p> <p>Output: Clean, transformed data arrives at its destination\u2014ready to be used for reports, dashboards, or further analysis.</p>"},{"location":"learning/data_engineering_101/#a-visual-explanation","title":"A Visual Explanation","text":"<p>Let's trace a specific piece of data through a pipeline:</p> <pre><code>                        DATA PIPELINE EXAMPLE\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udce6 INPUT: Raw sales data from a point-of-sale system\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 transaction_id,product,price,date,store                           \u2502\n\u2502 T001,SHRT-BLU-M,$29.99,2024-03-15,Store #42                       \u2502\n\u2502 T002,SHRT-BLU-M,29.99,15/03/2024,42                               \u2502\n\u2502 T003,,29.99,2024-03-15,Store 42                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nNotice the problems? Inconsistent date formats, missing product, \ndollar signs in some prices, store names formatted differently.\n\n              \u2502\n              \u25bc\n\n\u2699\ufe0f STATION 1: Parse and Validate\n- Convert all dates to YYYY-MM-DD format\n- Remove currency symbols from prices\n- Flag records with missing required fields\n\n              \u2502\n              \u25bc\n\n\u2699\ufe0f STATION 2: Standardize\n- Normalize store names to IDs\n- Convert product codes to full names\n- Ensure prices are decimal numbers\n\n              \u2502\n              \u25bc\n\n\u2699\ufe0f STATION 3: Enrich\n- Look up product category from product catalog\n- Look up store region from store table\n- Calculate sales tax based on region\n\n              \u2502\n              \u25bc\n\n\u2705 OUTPUT: Clean, enriched sales data\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 transaction_id \u2502 product_name   \u2502 price \u2502 date       \u2502 store_id \u2502  \u2502\n\u2502 T001           \u2502 Blue Shirt (M) \u2502 29.99 \u2502 2024-03-15 \u2502 42       \u2502  \u2502\n\u2502 T002           \u2502 Blue Shirt (M) \u2502 29.99 \u2502 2024-03-15 \u2502 42       \u2502  \u2502\n\u2502 T003           \u2502 NULL           \u2502 29.99 \u2502 2024-03-15 \u2502 42       \u2502  \u2502 \u2190 Quarantined\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The pipeline took messy, inconsistent input and produced clean, standardized output. It also flagged the problematic record (T003) so someone can investigate the missing product.</p>"},{"location":"learning/data_engineering_101/#why-manual-excel-work-doesnt-scale","title":"Why Manual Excel Work Doesn't Scale","text":"<p>You might wonder: \"Can't I just do this in Excel?\"</p> <p>For small amounts of data, yes. But consider these scenarios:</p> Scenario Manual Approach Pipeline Approach 100 rows, once \u2705 Totally fine Overkill 100 rows, daily \ud83d\ude13 Tedious, error-prone \u2705 Automated 100,000 rows \u274c Excel slows down \u2705 No problem 10 million rows \u274c Excel crashes \u2705 Still fine 1 billion rows \u274c Impossible \u2705 Use distributed processing <p>Scale is one reason. But there are others:</p> <p>Reproducibility: If you clean data manually in Excel, can you do it exactly the same way next time? What if you're sick and a colleague has to do it? With a pipeline, the logic is written in code\u2014it runs the same way every time.</p> <p>Audit trail: When something goes wrong with a report, you need to trace back to see what happened. With Excel, you'd have to remember every click and formula change. With a pipeline, you can review the code and logs.</p> <p>Speed: A pipeline can run automatically at 3 AM while you sleep. Manual work requires a human to be present.</p>"},{"location":"learning/data_engineering_101/#what-problems-do-pipelines-solve","title":"What Problems Do Pipelines Solve?","text":"<p>Let's get specific about the problems data pipelines address.</p>"},{"location":"learning/data_engineering_101/#problem-1-messy-data","title":"Problem 1: Messy Data","text":"<p>Real-world data is almost never clean. Here are common issues:</p> <p>Inconsistent formatting: <pre><code># These all mean the same thing, but are written differently\n\"United States\", \"USA\", \"U.S.A.\", \"US\", \"united states\", \"UNITED STATES\"\n</code></pre></p> <p>Duplicate records: <pre><code># Same customer entered twice\ncustomer_id: 1001, name: \"John Smith\", email: \"john@email.com\"\ncustomer_id: 1002, name: \"John Smith\", email: \"john@email.com\"\n</code></pre></p> <p>Invalid values: <pre><code># Age can't be negative\nage: -5\n\n# Birth date can't be in the future\nbirth_date: 2050-01-01\n\n# Email missing @ symbol\nemail: \"invalidemail.com\"\n</code></pre></p> <p>Mixed data types: <pre><code># Price column has different formats\nprice: 29.99\nprice: \"$29.99\"\nprice: \"29,99\"  (European format)\nprice: \"twenty-nine ninety-nine\"\n</code></pre></p> <p>A pipeline includes validation and cleaning steps that catch and fix these issues automatically.</p>"},{"location":"learning/data_engineering_101/#problem-2-manual-work-and-human-error","title":"Problem 2: Manual Work and Human Error","text":"<p>Every time a human touches data manually, there's a chance for error:</p> <ul> <li>Copy-pasting the wrong range</li> <li>Forgetting to apply a filter</li> <li>Typing a formula incorrectly</li> <li>Saving over the wrong file</li> <li>Using last month's template instead of the updated one</li> </ul> <p>These mistakes are expensive. A single typo in a financial report could cost millions. A forgotten filter could lead to wrong business decisions.</p> <p>\ud83d\udca1 Automation removes human error from repetitive tasks. A pipeline runs the same logic every time, exactly as written.</p>"},{"location":"learning/data_engineering_101/#problem-3-scale","title":"Problem 3: Scale","text":"<p>Consider this: A small retail store might have 100 transactions per day. That's 36,500 transactions per year\u2014manageable in Excel.</p> <p>Now consider Amazon. They process roughly 8,500 transactions per second. That's 734 million transactions per day. 268 billion per year.</p> <p>You cannot process 268 billion rows in Excel. You need distributed computing systems that can split the work across hundreds or thousands of machines. Data pipelines built with tools like Apache Spark are designed for this scale.</p> <p>Even at smaller scales, performance matters. If generating a report takes 4 hours manually but 4 minutes with a pipeline, that's meaningful time savings.</p>"},{"location":"learning/data_engineering_101/#problem-4-reproducibility","title":"Problem 4: Reproducibility","text":"<p>Imagine this scenario:</p> <p>\"Hey, the revenue numbers in last month's report look wrong. Can you re-run the analysis?\"</p> <p>If you did it manually in Excel: - Do you still have the original files? - Did you document every step? - Can you replicate exactly what you did?</p> <p>If you have a pipeline: - Run the pipeline with last month's parameters - Get the same result - Compare to find the discrepancy</p> <p>Reproducibility is essential for trust. If you can't reproduce a result, how do you know it was right in the first place?</p>"},{"location":"learning/data_engineering_101/#problem-5-freshness","title":"Problem 5: Freshness","text":"<p>Business moves fast. Executives want to see yesterday's numbers this morning, not next week.</p> <p>Manual processes have inherent delays: - Someone has to be available to run them - They can only run during business hours - If something fails, you might not notice until someone complains</p> <p>Automated pipelines can: - Run on a schedule (every night at 2 AM) - Run on triggers (whenever new data arrives) - Alert you immediately if something fails - Retry automatically after transient failures</p>"},{"location":"learning/data_engineering_101/#etl-vs-elt","title":"ETL vs ELT","text":"<p>You'll often hear these acronyms in data engineering. They describe two different approaches to moving and transforming data.</p>"},{"location":"learning/data_engineering_101/#etl-extract-transform-load","title":"ETL: Extract, Transform, Load","text":"<p>ETL is the traditional approach. It works like this:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    SOURCE    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   TRANSFORM   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  DESTINATION  \u2502\n\u2502   Systems    \u2502 Extract \u2502   (Outside)   \u2502  Load   \u2502  (Warehouse)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Extract: Pull data from source systems (databases, APIs, files)</li> <li>Transform: Clean and reshape the data before loading it</li> <li>Load: Insert the transformed data into the destination</li> </ol> <p>Why ETL? In the past, storage and computing power in data warehouses were expensive. It made sense to clean and shrink the data before loading it\u2014you'd only store what you actually needed.</p> <p>ETL Example: <pre><code>Source: 10 million raw transaction records\nTransform: Filter to only completed orders, aggregate by day\nLoad: 365 daily summary records into the warehouse\n</code></pre></p> <p>You've reduced 10 million records to 365, saving storage and making queries faster.</p>"},{"location":"learning/data_engineering_101/#elt-extract-load-transform","title":"ELT: Extract, Load, Transform","text":"<p>ELT is the modern approach, enabled by cheap cloud storage and powerful cloud warehouses.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    SOURCE    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  DESTINATION  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   TRANSFORM   \u2502\n\u2502   Systems    \u2502 Extract \u2502  (Warehouse)  \u2502  Load   \u2502   (Inside)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li>Extract: Pull data from source systems</li> <li>Load: Load the raw data into the destination first</li> <li>Transform: Transform the data inside the warehouse using SQL</li> </ol> <p>Why ELT? Modern cloud warehouses (Snowflake, BigQuery, Databricks) can store huge amounts of data cheaply and process it incredibly fast. Loading raw data first gives you: - A complete historical record (nothing is lost) - Flexibility to transform in different ways later - The ability to re-transform if requirements change</p> <p>ELT Example: <pre><code>Source: 10 million raw transaction records\nLoad: All 10 million records into the warehouse (raw layer)\nTransform: Create views or tables for different purposes\n  - Daily summaries for executives\n  - Detailed records for fraud analysis\n  - Customer-level aggregations for marketing\n</code></pre></p>"},{"location":"learning/data_engineering_101/#visual-comparison","title":"Visual Comparison","text":"<pre><code>                    ETL (Traditional)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    SOURCE              ETL SERVER              WAREHOUSE\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Sales   \u2502          \u2502         \u2502           \u2502         \u2502\n  \u2502 System  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Clean   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Clean   \u2502\n  \u2502         \u2502          \u2502 Filter  \u2502           \u2502 Data    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502 Aggregate           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502         \u2502\n  \u2502 CRM     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u26a0\ufe0f Transformation happens OUTSIDE the warehouse\n  \u26a0\ufe0f Raw data is discarded after transformation\n\n\n                    ELT (Modern)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    SOURCE              WAREHOUSE                WAREHOUSE\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Sales   \u2502          \u2502 RAW     \u2502           \u2502 CLEAN   \u2502\n  \u2502 System  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Layer   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502 Layer   \u2502\n  \u2502         \u2502          \u2502 (All    \u2502   SQL     \u2502 (Views  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502  Data)  \u2502  Transforms  &amp; Tables)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502         \u2502           \u2502         \u2502\n  \u2502 CRM     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 \u2502         \u2502           \u2502         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u2705 Raw data is preserved\n  \u2705 Transformations use warehouse compute power\n  \u2705 Can re-transform any time\n</code></pre>"},{"location":"learning/data_engineering_101/#when-to-use-which","title":"When to Use Which","text":"Consideration ETL ELT Warehouse storage is expensive \u2705 Preferred Warehouse compute is powerful \u2705 Preferred Need to preserve raw data \u2705 Preferred Complex transformations before loading \u2705 Preferred Privacy/compliance requires filtering before loading \u2705 Preferred Want flexibility to change transformations later \u2705 Preferred"},{"location":"learning/data_engineering_101/#how-odibi-supports-both","title":"How Odibi Supports Both","text":"<p>Odibi is designed to work with either approach:</p> <p>For ETL workflows: You can define transformation patterns that run during the extract phase, cleaning and reshaping data before it reaches your destination.</p> <p>For ELT workflows: You can use Odibi to manage the \"E\" and \"L\" (extracting from sources and loading to raw layers), then define separate transformation jobs that run inside your warehouse.</p> <p>Hybrid approaches: Many real-world pipelines use both. You might: 1. Extract data from a source 2. Do light cleaning (ETL-style) to fix obvious issues 3. Load to a raw layer 4. Do heavy transformations (ELT-style) in the warehouse</p> <p>Odibi's pattern-based approach lets you mix and match as needed.</p>"},{"location":"learning/data_engineering_101/#what-is-a-schema-why-does-it-matter","title":"What is a Schema? Why Does It Matter?","text":"<p>A schema is the structure or blueprint of your data. It defines what fields exist, what types they are, and how they relate to each other.</p>"},{"location":"learning/data_engineering_101/#the-spreadsheet-headers-analogy","title":"The Spreadsheet Headers Analogy","text":"<p>When you create a spreadsheet, the first row usually contains headers:</p> <pre><code>| Name       | Age | Email              | Salary   |\n|------------|-----|--------------------|----------|\n| Alice      | 30  | alice@email.com    | 75000    |\n| Bob        | 25  | bob@email.com      | 65000    |\n</code></pre> <p>Those headers are a simple schema. They tell you: - What each column means - In what order columns appear</p> <p>A database schema goes further: - Name: Text, maximum 100 characters, cannot be empty - Age: Integer, must be between 0 and 150 - Email: Text, must contain \"@\", must be unique - Salary: Decimal number, cannot be negative</p>"},{"location":"learning/data_engineering_101/#type-safety-why-it-matters","title":"Type Safety: Why It Matters","text":"<p>Every piece of data has a type. Common types include:</p> Type What It Stores Examples String/Text Letters, words, sentences \"Hello\", \"Product-123\" Integer Whole numbers 1, 42, -7, 1000000 Float/Decimal Numbers with decimals 3.14, 29.99, -0.5 Boolean True or false true, false Date Calendar dates 2024-03-15 Timestamp Date and time 2024-03-15 10:30:00 <p>Why does this matter?</p> <p>Imagine someone puts text in a number column:</p> <pre><code>| product_id | quantity |\n|------------|----------|\n| P001       | 10       |\n| P002       | 5        |\n| P003       | \"TBD\"    |  \u2190 This is text, not a number!\n</code></pre> <p>Now try to calculate total quantity: <code>10 + 5 + \"TBD\" = ???</code></p> <p>The calculation fails because you can't add a number to text. This is a type mismatch, and it breaks things.</p> <p>A schema with proper types prevents this. If the <code>quantity</code> column is defined as an integer, the system will reject \"TBD\" when someone tries to insert it.</p>"},{"location":"learning/data_engineering_101/#what-happens-when-schemas-dont-match","title":"What Happens When Schemas Don't Match","text":"<p>Schema mismatches are a major source of pipeline failures. Here are common scenarios:</p> <p>Scenario 1: Source adds a new column <pre><code># Your schema expects:\nname, email, signup_date\n\n# Source now sends:\nname, email, signup_date, phone_number  \u2190 New column!\n</code></pre> Your pipeline might fail because it doesn't know what to do with <code>phone_number</code>.</p> <p>Scenario 2: Source changes a column type <pre><code># Previously:\norder_id: 12345 (integer)\n\n# Now:\norder_id: \"ORD-12345\" (string)  \u2190 Type changed!\n</code></pre> Your pipeline expects an integer but receives a string. Crash.</p> <p>Scenario 3: Source renames a column <pre><code># Previously:\ncustomer_name\n\n# Now:\ncust_name  \u2190 Different name, same data\n</code></pre> Your pipeline looks for <code>customer_name</code>, finds nothing, and fails.</p>"},{"location":"learning/data_engineering_101/#schema-evolution","title":"Schema Evolution","text":"<p>Schemas change over time. Business requirements evolve. New data becomes available. This is called schema evolution, and it's one of the trickiest parts of data engineering.</p> <p>Good data engineering practices handle schema evolution gracefully:</p> <ol> <li> <p>Additive changes (new columns): Usually safe. Ignore new columns or add them as optional.</p> </li> <li> <p>Removal changes (dropped columns): Dangerous. If your pipeline depends on that column, it will break.</p> </li> <li> <p>Type changes: Very dangerous. Requires careful migration.</p> </li> <li> <p>Rename changes: Dangerous. Requires mapping old names to new names.</p> </li> </ol> <p>Odibi helps with schema evolution by: - Validating incoming data against expected schemas - Providing clear error messages when schemas don't match - Supporting schema transformation patterns (renaming, type conversion)</p>"},{"location":"learning/data_engineering_101/#data-quality-basics","title":"Data Quality Basics","text":"<p>\"Garbage in, garbage out\" is the oldest saying in data. If the data going into your pipeline is bad, the output will be bad too\u2014no matter how sophisticated your transformations are.</p>"},{"location":"learning/data_engineering_101/#the-five-pillars-of-data-quality","title":"The Five Pillars of Data Quality","text":""},{"location":"learning/data_engineering_101/#1-completeness-are-required-values-present","title":"1. Completeness: Are required values present?","text":"<p>Nulls (missing values) are the most common data quality issue.</p> <pre><code>| order_id | customer_id | total   |\n|----------|-------------|---------|\n| 1001     | C100        | 59.99   |\n| 1002     | NULL        | 29.99   |  \u2190 Who placed this order?\n| 1003     | C102        | NULL    |  \u2190 How much was it?\n</code></pre> <p>Some nulls are acceptable (an optional phone number might be null). Others are critical failures (an order without a total is useless).</p> <p>Questions to ask: - Which fields are required? - What should happen when a required field is null? - Should we reject the record? Fill in a default? Quarantine for review?</p>"},{"location":"learning/data_engineering_101/#2-uniqueness-are-there-duplicate-records","title":"2. Uniqueness: Are there duplicate records?","text":"<p>Duplicates inflate counts and totals, leading to wrong conclusions.</p> <pre><code>| transaction_id | amount |\n|----------------|--------|\n| T001           | 100.00 |\n| T001           | 100.00 |  \u2190 Duplicate!\n| T002           | 50.00  |\n\nTotal without duplicates: $150.00\nTotal with duplicates: $250.00  \u2190 Wrong!\n</code></pre> <p>Questions to ask: - What makes a record unique? (Usually a primary key) - How did duplicates get created? (Fix the source if possible) - How do we handle duplicates? (Keep first? Keep last? Merge?)</p>"},{"location":"learning/data_engineering_101/#3-validity-are-values-within-acceptable-ranges","title":"3. Validity: Are values within acceptable ranges?","text":"<p>Invalid values don't make sense in context.</p> <pre><code># Invalid examples:\nage: -5              # Age cannot be negative\ntemperature: 500\u00b0C   # Unless it's a volcano, this is wrong\ndiscount: 150%       # Can't discount more than 100%\nbirth_date: 2099-01-01  # This date hasn't happened yet\nstatus: \"Pneding\"    # Typo in status value\n</code></pre> <p>Questions to ask: - What are the valid ranges for numeric fields? - What are the valid values for categorical fields? - What date ranges make sense?</p>"},{"location":"learning/data_engineering_101/#4-consistency-does-the-same-thing-have-the-same-representation","title":"4. Consistency: Does the same thing have the same representation?","text":"<p>Inconsistent data is technically valid but difficult to work with.</p> <pre><code># Inconsistent country names:\n\"United States\"\n\"USA\"\n\"U.S.A.\"\n\"United States of America\"\n\"US\"\n\n# Inconsistent date formats:\n\"2024-03-15\"\n\"03/15/2024\"\n\"15-Mar-2024\"\n\"March 15, 2024\"\n</code></pre> <p>All of these mean the same thing, but a computer sees them as different values. Reports will show separate counts for \"USA\" and \"United States\" when they should be combined.</p> <p>Solution: Standardize during transformation. Pick one format and convert everything to it.</p>"},{"location":"learning/data_engineering_101/#5-referential-integrity-do-relationships-make-sense","title":"5. Referential Integrity: Do relationships make sense?","text":"<p>When records refer to other records, those references should be valid.</p> <pre><code># Orders table:\n| order_id | customer_id |\n|----------|-------------|\n| 1001     | C100        |\n| 1002     | C999        |  \u2190 This customer doesn't exist!\n\n# Customers table:\n| customer_id | name    |\n|-------------|---------|\n| C100        | Alice   |\n| C101        | Bob     |\n</code></pre> <p>Order 1002 references customer C999, but that customer isn't in the customers table. This is called an orphan record\u2014it has no parent.</p> <p>Why it matters: - You can't look up the customer name for order 1002 - Reports will show orders without customer information - Joins between tables will lose data</p>"},{"location":"learning/data_engineering_101/#garbage-in-garbage-out-in-practice","title":"\"Garbage In, Garbage Out\" in Practice","text":"<p>Imagine building a dashboard showing \"Average Order Value by Region.\"</p> <p>If your data has: - \u274c Missing order totals \u2192 Averages will be wrong - \u274c Duplicate orders \u2192 Totals will be inflated - \u274c Invalid region codes \u2192 Some orders won't be assigned to any region - \u274c Inconsistent region names \u2192 \"West Coast\" and \"WEST COAST\" counted separately</p> <p>Your dashboard will show wrong numbers, and executives will make wrong decisions based on it.</p> <p>\ud83d\udca1 Data quality is not optional. It's the foundation that everything else depends on.</p> <p>Odibi includes built-in validation patterns to catch these issues before they pollute your data.</p>"},{"location":"learning/data_engineering_101/#the-medallion-architecture-bronzesilvergold","title":"The Medallion Architecture (Bronze/Silver/Gold)","text":"<p>The Medallion Architecture is a design pattern for organizing data in layers. Each layer has a specific purpose and quality level.</p>"},{"location":"learning/data_engineering_101/#the-manufacturing-analogy","title":"The Manufacturing Analogy","text":"<p>Think of a factory that turns raw materials into finished products:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    RAW ORE      \u2502    \u2502  REFINED METAL  \u2502    \u2502 FINISHED PRODUCT\u2502\n\u2502   (Bronze)      \u2502\u2500\u2500\u2500\u25b6\u2502    (Silver)     \u2502\u2500\u2500\u2500\u25b6\u2502     (Gold)      \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 Rocks, dirt,    \u2502    \u2502 Pure metal      \u2502    \u2502 Jewelry, tools  \u2502\n\u2502 impurities      \u2502    \u2502 ingots          \u2502    \u2502 ready to sell   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Data flows through similar stages, becoming more refined at each step.</p>"},{"location":"learning/data_engineering_101/#visual-diagram-of-the-medallion-architecture","title":"Visual Diagram of the Medallion Architecture","text":"<pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                        MEDALLION ARCHITECTURE\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n   SOURCES                BRONZE               SILVER               GOLD\n  (External)            (Raw Data)          (Cleaned)          (Business-Ready)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sales API    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502 Exact copy  \u2502     \u2502 Validated   \u2502     \u2502 Aggregated  \u2502\n\u2502 CRM Database \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 of source   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Cleaned     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Joined      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502 data        \u2502     \u2502 Standardized\u2502     \u2502 Business    \u2502\n\u2502 CSV Files    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502             \u2502     \u2502             \u2502     \u2502 Metrics     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502 No changes  \u2502     \u2502 Quality     \u2502     \u2502             \u2502\n\u2502 Event Stream \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 whatsoever  \u2502     \u2502 checked     \u2502     \u2502 Ready for   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502 dashboards  \u2502\n                                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                      \ud83d\udce6 STORAGE:          \ud83d\udce6 STORAGE:        \ud83d\udce6 STORAGE:\n                      Everything           Reliable data      Curated datasets\n                      (even garbage)       for analysis       for specific uses\n</code></pre>"},{"location":"learning/data_engineering_101/#what-belongs-in-each-layer","title":"What Belongs in Each Layer?","text":""},{"location":"learning/data_engineering_101/#bronze-layer-raw","title":"\ud83d\udce6 Bronze Layer (Raw)","text":"<p>Purpose: Preserve the original data exactly as received.</p> <p>Contents: - Exact copies of source data - No transformations - No cleaning - No filtering</p> <p>Why keep raw data? - If something goes wrong, you can reprocess from scratch - If requirements change, you have the original data - For auditing and compliance, you can prove what was received</p> <p>Example Bronze table: <pre><code>| _raw_json                                          | _source   | _loaded_at          |\n|----------------------------------------------------|-----------|---------------------|\n| {\"id\":1,\"name\":\"Alice\",\"date\":\"2024-03-15\"}       | crm_api   | 2024-03-15 10:00:00 |\n| {\"id\":2,\"name\":\"Bob\",\"date\":\"15/03/2024\"}         | crm_api   | 2024-03-15 10:00:00 |\n| {\"id\":\"X\",\"name\":null,\"date\":\"invalid\"}           | crm_api   | 2024-03-15 10:00:00 |\n</code></pre></p> <p>Even the bad data (ID \"X\" with null name and invalid date) is preserved.</p>"},{"location":"learning/data_engineering_101/#silver-layer-cleaned","title":"\ud83e\udd48 Silver Layer (Cleaned)","text":"<p>Purpose: Validated, cleaned, standardized data ready for analysis.</p> <p>Contents: - Data types enforced (dates are dates, numbers are numbers) - Invalid records filtered out or quarantined - Formats standardized (consistent date formats, normalized text) - Duplicates removed - Basic business logic applied</p> <p>Example Silver table: <pre><code>| id   | name   | registration_date | _is_valid | _quality_score |\n|------|--------|-------------------|-----------|----------------|\n| 1    | Alice  | 2024-03-15        | true      | 100            |\n| 2    | Bob    | 2024-03-15        | true      | 100            |\n</code></pre></p> <p>Notice the bad record from Bronze didn't make it to Silver. It was quarantined because it had invalid values.</p>"},{"location":"learning/data_engineering_101/#gold-layer-business-ready","title":"\ud83e\udd47 Gold Layer (Business-Ready)","text":"<p>Purpose: Aggregated, joined, business-specific datasets for end users.</p> <p>Contents: - Pre-calculated metrics - Data from multiple Silver tables joined together - Optimized for specific use cases (dashboards, reports, ML models) - Business terminology (not technical column names)</p> <p>Example Gold table (Daily Sales Summary): <pre><code>| report_date | region      | total_orders | total_revenue | avg_order_value |\n|-------------|-------------|--------------|---------------|-----------------|\n| 2024-03-15  | Northeast   | 1,234        | $98,456.78    | $79.78          |\n| 2024-03-15  | Southeast   | 987          | $76,543.21    | $77.55          |\n| 2024-03-15  | Midwest     | 1,567        | $123,456.89   | $78.78          |\n</code></pre></p> <p>This is what business users see in their dashboards. They don't need to know about raw JSON or data cleaning\u2014they just want the numbers.</p>"},{"location":"learning/data_engineering_101/#why-this-pattern-exists","title":"Why This Pattern Exists","text":"<ol> <li> <p>Separation of concerns: Each layer has one job. Bronze preserves. Silver cleans. Gold serves.</p> </li> <li> <p>Reprocessing: If cleaning logic changes, reprocess Silver from Bronze. If aggregation logic changes, reprocess Gold from Silver.</p> </li> <li> <p>Debugging: When something looks wrong in Gold, you can trace back to Silver, then Bronze, to find where the problem started.</p> </li> <li> <p>Performance: Gold tables are pre-aggregated, so dashboards load instantly instead of calculating on the fly.</p> </li> <li> <p>Access control: Raw data in Bronze might be sensitive. You can restrict access to Bronze while allowing wider access to sanitized Gold data.</p> </li> </ol>"},{"location":"learning/data_engineering_101/#how-odibi-fits-into-the-ecosystem","title":"How Odibi Fits Into the Ecosystem","text":"<p>There are many tools in the data engineering world. Here's how Odibi compares.</p>"},{"location":"learning/data_engineering_101/#odibi-vs-dbt","title":"Odibi vs dbt","text":"<p>dbt (data build tool) is popular for transforming data inside a warehouse using SQL.</p> Aspect dbt Odibi Primary language SQL Python (with YAML config) Transform location Inside the warehouse Flexible (Spark, Pandas, Polars) Focus Transformations only Full pipeline patterns Approach Write SQL, dbt manages Declarative patterns <p>When to use dbt: Your team is SQL-strong, your data is already in a warehouse, and you want a SQL-centric workflow.</p> <p>When to use Odibi: You need to work with data before it reaches the warehouse, want Python flexibility, or prefer pattern-based configuration over writing SQL.</p>"},{"location":"learning/data_engineering_101/#odibi-vs-airflow","title":"Odibi vs Airflow","text":"<p>Airflow is an orchestrator\u2014it schedules and monitors when jobs run, but it doesn't define what those jobs do.</p> Aspect Airflow Odibi Purpose Orchestration (when) Transformation (what) What it does Schedules tasks, manages dependencies Defines data transformations Scope Runs any task (data, ML, DevOps) Data pipelines specifically <p>Relationship: You can use Airflow to schedule Odibi jobs. Airflow says \"run this at 3 AM,\" Odibi says \"here's what to do when you run.\"</p>"},{"location":"learning/data_engineering_101/#odibi-vs-raw-sparkpandas","title":"Odibi vs Raw Spark/Pandas","text":"<p>Spark and Pandas are processing engines. They're extremely powerful but require you to write all the logic yourself.</p> Aspect Raw Spark/Pandas Odibi Approach Write code for everything Use pre-built patterns SCD2 logic Write 50+ lines of code One config option Data quality Build it yourself Built-in validation Learning curve Steep Gentler <p>Odibi adds patterns on top of these engines. Instead of writing the same boilerplate code for SCD2 (Slowly Changing Dimension Type 2) over and over, you declare what you want in YAML and Odibi generates the code.</p> <p>When to use raw Spark/Pandas: You need custom logic that doesn't fit any pattern, or you're learning how these engines work.</p> <p>When to use Odibi: You're building production pipelines and want to move fast with proven patterns.</p>"},{"location":"learning/data_engineering_101/#when-to-use-odibi","title":"When to Use Odibi","text":"<p>Odibi is a good fit when:</p> <p>\u2705 You're building data pipelines in Python \u2705 You want to use Spark, Pandas, or Polars \u2705 You need SCD2, merge patterns, or data quality checks \u2705 You prefer configuration over code \u2705 You're a small team (or solo) and need to move fast  </p> <p>Odibi might not be the best fit when:</p> <p>\u274c Your entire workflow is SQL-based (consider dbt) \u274c You need a scheduler/orchestrator (use Airflow, Dagster, or Prefect) \u274c You're doing real-time streaming (consider Kafka, Flink)  </p>"},{"location":"learning/data_engineering_101/#your-first-mental-model","title":"Your First Mental Model","text":"<p>Let's put everything together with a complete example. We'll trace data from source to dashboard.</p>"},{"location":"learning/data_engineering_101/#the-journey-source-bronze-silver-gold-dashboard","title":"The Journey: Source \u2192 Bronze \u2192 Silver \u2192 Gold \u2192 Dashboard","text":"<pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                    THE COMPLETE DATA JOURNEY\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n    \ud83d\udcc4 SOURCE            \ud83d\udce6 BRONZE           \ud83e\udd48 SILVER           \ud83e\udd47 GOLD\n    (CSV file)          (Raw copy)         (Cleaned)          (Aggregated)\n        \u2502                   \u2502                  \u2502                  \u2502\n        \u25bc                   \u25bc                  \u25bc                  \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 orders  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  raw_   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 clean_  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 daily_  \u2502\n   \u2502  .csv   \u2502         \u2502 orders  \u2502        \u2502 orders  \u2502       \u2502 summary \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                  \u2502\n                                                                  \u25bc\n                                                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                                            \u2502 Dashboard\u2502\n                                                            \u2502  \ud83d\udcca\ud83d\udcc8    \u2502\n                                                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"learning/data_engineering_101/#step-1-sample-source-data-csv","title":"Step 1: Sample Source Data (CSV)","text":"<p>This is what arrives from the source system\u2014an orders CSV file:</p> <pre><code>order_id,customer_name,product,quantity,price,order_date,region\nORD001,Alice Smith,Widget A,2,29.99,2024-03-15,Northeast\nORD002,BOB JOHNSON,Widget B,1,$49.99,03/15/2024,northeast\nORD003,Carol White,Widget A,-1,29.99,2024-03-15,Southeast\nORD004,,Widget C,3,19.99,2024-03-15,Midwest\nORD001,Alice Smith,Widget A,2,29.99,2024-03-15,Northeast\nORD005,Dave Brown,Widget B,1,49.99,2024-03-99,West\n</code></pre> <p>Problems in this data: - \ud83d\udcdb Row 2: Name in ALL CAPS, price has \"$\", date format different, region lowercase - \u274c Row 3: Negative quantity (invalid) - \u274c Row 4: Missing customer name - \u274c Row 5: Duplicate of row 1 - \u274c Row 6: Invalid date (March 99th doesn't exist)</p>"},{"location":"learning/data_engineering_101/#step-2-bronze-layer-raw-copy","title":"Step 2: Bronze Layer (Raw Copy)","text":"<p>The Bronze layer stores this data exactly as received:</p> <pre><code>| order_id | customer_name | product  | quantity | price   | order_date | region    | _loaded_at          |\n|----------|---------------|----------|----------|---------|------------|-----------|---------------------|\n| ORD001   | Alice Smith   | Widget A | 2        | 29.99   | 2024-03-15 | Northeast | 2024-03-16 02:00:00 |\n| ORD002   | BOB JOHNSON   | Widget B | 1        | $49.99  | 03/15/2024 | northeast | 2024-03-16 02:00:00 |\n| ORD003   | Carol White   | Widget A | -1       | 29.99   | 2024-03-15 | Southeast | 2024-03-16 02:00:00 |\n| ORD004   |               | Widget C | 3        | 19.99   | 2024-03-15 | Midwest   | 2024-03-16 02:00:00 |\n| ORD001   | Alice Smith   | Widget A | 2        | 29.99   | 2024-03-15 | Northeast | 2024-03-16 02:00:00 |\n| ORD005   | Dave Brown    | Widget B | 1        | 49.99   | 2024-03-99 | West      | 2024-03-16 02:00:00 |\n</code></pre> <p>Nothing changed except adding a timestamp of when we loaded it. All the messy data is preserved.</p>"},{"location":"learning/data_engineering_101/#step-3-silver-layer-cleaned","title":"Step 3: Silver Layer (Cleaned)","text":"<p>The Silver layer applies transformations and validations:</p> <p>Transformations applied: 1. Standardize customer names to Title Case 2. Remove \"$\" from prices, convert to decimal 3. Parse dates to YYYY-MM-DD format 4. Standardize region names to Title Case 5. Remove duplicates based on order_id</p> <p>Validations applied: 1. Quantity must be positive 2. Customer name cannot be empty 3. Date must be valid</p> <p>Resulting Silver table: <pre><code>| order_id | customer_name | product  | quantity | price | order_date | region    | _is_valid |\n|----------|---------------|----------|----------|-------|------------|-----------|-----------|\n| ORD001   | Alice Smith   | Widget A | 2        | 29.99 | 2024-03-15 | Northeast | \u2705        |\n| ORD002   | Bob Johnson   | Widget B | 1        | 49.99 | 2024-03-15 | Northeast | \u2705        |\n</code></pre></p> <p>Quarantined records (sent to a separate table for review): <pre><code>| order_id | reason                              |\n|----------|-------------------------------------|\n| ORD003   | Quantity must be positive: -1       |\n| ORD004   | Customer name is required           |\n| ORD005   | Invalid date: 2024-03-99            |\n</code></pre></p> <p>Only 2 of the original 6 records (after deduplication) made it through. The bad records are quarantined so someone can investigate and fix the source.</p>"},{"location":"learning/data_engineering_101/#step-4-gold-layer-aggregated","title":"Step 4: Gold Layer (Aggregated)","text":"<p>The Gold layer aggregates for business use:</p> <p>Daily Sales Summary: <pre><code>| report_date | region    | total_orders | total_quantity | total_revenue |\n|-------------|-----------|--------------|----------------|---------------|\n| 2024-03-15  | Northeast | 2            | 3              | $79.98        |\n</code></pre></p> <p>Product Performance: <pre><code>| product  | units_sold | revenue |\n|----------|------------|---------|\n| Widget A | 2          | $59.98  |\n| Widget B | 1          | $49.99  |\n</code></pre></p>"},{"location":"learning/data_engineering_101/#step-5-dashboard-visualization","title":"Step 5: Dashboard Visualization","text":"<p>Finally, users see clean visualizations:</p> <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                    DAILY SALES DASHBOARD                              \u2551\n\u2551                       March 15, 2024                                  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                                       \u2551\n\u2551   TOTAL REVENUE        TOTAL ORDERS        AVG ORDER VALUE            \u2551\n\u2551   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2551\n\u2551   \u2502  $79.98     \u2502     \u2502     2       \u2502     \u2502   $39.99    \u2502            \u2551\n\u2551   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2551\n\u2551                                                                       \u2551\n\u2551   SALES BY REGION                    TOP PRODUCTS                     \u2551\n\u2551   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2551\n\u2551   \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Northeast  \u2502         \u2502 Widget A  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 $59.98\u2502     \u2551\n\u2551   \u2502            $79.98     \u2502         \u2502 Widget B  \u2588\u2588\u2588\u2588     $49.99\u2502     \u2551\n\u2551   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2551\n\u2551                                                                       \u2551\n\u2551   \u26a0\ufe0f DATA QUALITY ALERT: 3 records quarantined for review            \u2551\n\u2551                                                                       \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre> <p>What the user sees: Clean numbers, clear charts, actionable insights.</p> <p>What they don't see: All the messy data, parsing errors, and transformations that happened behind the scenes.</p> <p>That's data engineering. You built the invisible plumbing that makes this dashboard possible.</p>"},{"location":"learning/data_engineering_101/#whats-next","title":"What's Next?","text":"<p>Congratulations! You now understand the fundamentals of data engineering:</p> <ul> <li>\u2705 What data is and where it lives</li> <li>\u2705 What data engineers do</li> <li>\u2705 How data pipelines work</li> <li>\u2705 ETL vs ELT approaches</li> <li>\u2705 Schemas and data types</li> <li>\u2705 Data quality fundamentals</li> <li>\u2705 The Medallion Architecture</li> <li>\u2705 How Odibi fits in the ecosystem</li> </ul>"},{"location":"learning/data_engineering_101/#continue-your-learning","title":"Continue Your Learning","text":"<p>\ud83d\udcd8 Curriculum - A structured learning path from beginner to advanced</p> <p>\ud83d\udcd6 Glossary - Quick definitions for data engineering terms</p> <p>\ud83d\udee0\ufe0f First Tutorial - Build your first Odibi pipeline</p>"},{"location":"learning/data_engineering_101/#key-concepts-to-explore-next","title":"Key Concepts to Explore Next","text":"<ol> <li>Slowly Changing Dimensions (SCD2) - How to track historical changes</li> <li>Data Modeling - Designing efficient schemas</li> <li>Orchestration - Scheduling and monitoring pipelines</li> <li>Testing - Validating that your pipelines work correctly</li> </ol>"},{"location":"learning/data_engineering_101/#remember","title":"Remember","text":"<p>Data engineering is a journey. You don't need to understand everything at once. Start with simple pipelines, add complexity gradually, and always focus on data quality.</p> <p>The best data engineers aren't those who know the most tools\u2014they're the ones who understand the fundamentals deeply and can apply them to any situation.</p> <p>Welcome to data engineering. \ud83c\udf89</p>"},{"location":"learning/glossary/","title":"Odibi Glossary","text":"<p>A beginner-friendly guide to every data engineering term you'll encounter in Odibi.</p>"},{"location":"learning/glossary/#a","title":"A","text":""},{"location":"learning/glossary/#aggregation","title":"Aggregation","text":"<p>What it is: Combining many rows of data into summary numbers\u2014like counting, averaging, or totaling.</p> <p>Real-world analogy: Imagine counting votes in an election. You don't care about each individual ballot; you just want the total for each candidate. That's aggregation.</p> <p>Example: <pre><code>pattern: aggregation\naggregations:\n  - column: sales_amount\n    function: sum\n    alias: total_sales\n  - column: order_id\n    function: count\n    alias: order_count\ngroup_by:\n  - store_id\n  - sale_date\n</code></pre></p> <p>Why it matters: Raw data has millions of rows. Business users need summaries like \"total sales by store\" or \"average order value by month.\" Aggregation turns overwhelming detail into actionable insights.</p> <p>Learn more: Aggregation Pattern</p>"},{"location":"learning/glossary/#append","title":"Append","text":"<p>What it is: Adding new rows to a table without touching the rows that already exist.</p> <p>Real-world analogy: Adding new entries to a guest book. You write on the next blank page\u2014you don't erase or change what previous guests wrote.</p> <p>Example: <pre><code>write_mode: append\n</code></pre></p> <p>Why it matters: When you receive daily sales data, you want to add today's transactions without accidentally deleting yesterday's. Append mode keeps your historical data safe.</p> <p>Learn more: Write Modes</p>"},{"location":"learning/glossary/#b","title":"B","text":""},{"location":"learning/glossary/#bronze-layer","title":"Bronze Layer","text":"<p>What it is: The first storage layer where raw data lands exactly as it arrived\u2014no cleaning, no changes.</p> <p>Real-world analogy: A mailroom. Letters arrive and get sorted into bins, but nobody opens or edits them. They're stored exactly as received.</p> <p>Example: <pre><code>layer: bronze\nnodes:\n  - name: raw_sales\n    source: landing/sales_*.csv\n    write_mode: append\n    # No transformations - just store it raw\n</code></pre></p> <p>Why it matters: If something goes wrong later, you can always go back to the original data. Bronze is your \"undo button\" for the entire pipeline.</p> <p>Learn more: Medallion Architecture</p>"},{"location":"learning/glossary/#c","title":"C","text":""},{"location":"learning/glossary/#connection","title":"Connection","text":"<p>What it is: Saved credentials and settings that tell Odibi how to access a data source (database, file storage, API).</p> <p>Real-world analogy: A saved password in your browser. Instead of typing your username and password every time, you save it once and reuse it.</p> <p>Example: <pre><code>connections:\n  warehouse_db:\n    type: postgres\n    host: db.company.com\n    port: 5432\n    database: analytics\n    # Credentials stored securely, not in YAML\n</code></pre></p> <p>Why it matters: Connections let you reuse access settings across many pipelines. Change the password once, and all pipelines using that connection keep working.</p> <p>Learn more: Connections Reference</p>"},{"location":"learning/glossary/#d","title":"D","text":""},{"location":"learning/glossary/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"<p>What it is: A map showing which pipeline steps must happen before others. \"Directed\" means arrows show order. \"Acyclic\" means no loops\u2014you can't go in circles.</p> <p>Real-world analogy: A recipe. You must chop vegetables before you can saut\u00e9 them. You can't frost a cake before baking it. The steps have a required order.</p> <p>Example: <pre><code>[Load Sales] \u2192 [Clean Sales] \u2192 [Join with Products] \u2192 [Calculate Metrics]\n                                       \u2191\n                              [Load Products]\n</code></pre></p> <p>Why it matters: Odibi uses the DAG to know what can run in parallel (Load Sales and Load Products) and what must wait (Join can't start until both loads finish).</p> <p>Learn more: Pipeline Concepts</p>"},{"location":"learning/glossary/#data-quality","title":"Data Quality","text":"<p>What it is: Measuring whether your data is correct, complete, and trustworthy.</p> <p>Real-world analogy: Quality control in a factory. Before products ship, inspectors check for defects. Data quality is the same\u2014checking for missing values, wrong formats, or impossible numbers.</p> <p>Example: <pre><code>validation:\n  rules:\n    - column: email\n      rule: not_null\n      severity: error\n    - column: age\n      rule: range\n      min: 0\n      max: 150\n      severity: warning\n    - column: order_total\n      rule: positive\n      severity: error\n</code></pre></p> <p>Why it matters: Bad data leads to bad decisions. If 20% of your sales records have missing amounts, your revenue reports are wrong. Data quality catches problems before they spread.</p> <p>Learn more: Validation Guide</p>"},{"location":"learning/glossary/#delta-lake","title":"Delta Lake","text":"<p>What it is: A smart file format that stores data in folders but adds superpowers: undo changes, time travel to past versions, and handle updates efficiently.</p> <p>Real-world analogy: Google Docs version history. You can see every change ever made, go back to any previous version, and multiple people can edit without conflicts.</p> <p>Example: <pre><code>format: delta\nwrite_mode: merge\n# Delta enables merge, time travel, and ACID transactions\n</code></pre></p> <p>Why it matters: Regular files (CSV, Parquet) can't handle updates well\u2014you'd have to rewrite the entire file. Delta Lake lets you update just the rows that changed, and if something goes wrong, you can undo it.</p> <p>Learn more: Delta Lake Integration</p>"},{"location":"learning/glossary/#dimension-table","title":"Dimension Table","text":"<p>What it is: A lookup table containing descriptive information about things\u2014like products, customers, or locations.</p> <p>Real-world analogy: A phone book or contact list. It doesn't record what calls you made (that's a fact table). It just stores information about people: name, address, phone number.</p> <p>Example: <pre><code>pattern: dimension\ntable_type: scd2\nnatural_key:\n  - customer_id\ntracked_columns:\n  - customer_name\n  - email\n  - address\n  - loyalty_tier\n</code></pre></p> <p>Why it matters: Dimension tables give meaning to your facts. A sales record might say \"customer_id: 12345 bought product_id: 789.\" The dimension tables tell you WHO customer 12345 is and WHAT product 789 is.</p> <p>Learn more: Dimension Pattern</p>"},{"location":"learning/glossary/#e","title":"E","text":""},{"location":"learning/glossary/#engine-spark-vs-pandas-vs-polars","title":"Engine (Spark vs Pandas vs Polars)","text":"<p>What it is: The processing tool that actually does the data work. Different engines handle different data sizes.</p> <p>Real-world analogy:  - Pandas = Kitchen blender. Great for small batches, easy to use. - Polars = Food processor. Faster than a blender, handles bigger jobs. - Spark = Industrial food processing plant. Handles massive volumes across many machines.</p> <p>Example: <pre><code>engine: spark  # For big data (millions+ rows)\n# engine: pandas  # For small data (fits in memory)\n# engine: polars  # For medium data (fast single-machine)\n</code></pre></p> <p>Why it matters: Using Spark for 100 rows is overkill (slow startup). Using Pandas for 100 million rows crashes your computer. Picking the right engine means your pipeline runs efficiently.</p> <p>Learn more: Engine Guide</p>"},{"location":"learning/glossary/#etl-vs-elt","title":"ETL vs ELT","text":"<p>What it is: Two approaches to moving and transforming data. - ETL (Extract, Transform, Load): Clean data BEFORE storing it. - ELT (Extract, Load, Transform): Store raw data first, clean it AFTER.</p> <p>Real-world analogy:  - ETL = Sorting mail before putting it in your filing cabinet. - ELT = Dumping all mail in a box, then sorting when you need something.</p> <p>Example: <pre><code># ELT approach (Odibi's default - medallion architecture)\n# 1. Load raw to Bronze (Extract, Load)\n# 2. Transform in Silver/Gold (Transform)\n\nbronze_node:\n  source: raw_file.csv\n  write_mode: append  # Just load it\n\nsilver_node:\n  source: bronze_table\n  transformations:    # Transform after loading\n    - type: clean_nulls\n    - type: standardize_dates\n</code></pre></p> <p>Why it matters: ELT is more flexible because you keep the raw data. If business rules change, you can re-transform from Bronze. ETL might have thrown away data you now need.</p> <p>Learn more: Pipeline Architecture</p>"},{"location":"learning/glossary/#f","title":"F","text":""},{"location":"learning/glossary/#fact-table","title":"Fact Table","text":"<p>What it is: A table storing events or transactions\u2014things that happened at a point in time with measurable values.</p> <p>Real-world analogy: Receipts. Each receipt records: when (timestamp), who (customer), what (products), and how much (amounts). That's a fact.</p> <p>Example: <pre><code>pattern: fact\ntable_type: transaction\nnatural_key:\n  - order_id\n  - line_item_id\nmeasures:\n  - quantity\n  - unit_price\n  - discount_amount\n  - line_total\nforeign_keys:\n  - column: customer_id\n    references: dim_customer\n  - column: product_id\n    references: dim_product\n</code></pre></p> <p>Why it matters: Fact tables are where the numbers live. When someone asks \"What were our total sales last quarter?\", you're querying a fact table.</p> <p>Learn more: Fact Pattern</p>"},{"location":"learning/glossary/#foreign-key-fk","title":"Foreign Key (FK)","text":"<p>What it is: A column that links one table to another by referencing the other table's unique identifier.</p> <p>Real-world analogy: A reference on a job application. The application says \"Reference: Jane Smith, phone: 555-1234.\" That phone number is a \"foreign key\" linking to a person who exists elsewhere.</p> <p>Example: <pre><code>validation:\n  foreign_key_checks:\n    - column: customer_id\n      reference_table: dim_customer\n      reference_column: customer_id\n      on_violation: quarantine  # Don't load orphan records\n</code></pre></p> <p>Why it matters: Foreign keys ensure data integrity. If an order references \"customer_id: 99999\" but that customer doesn't exist, something is wrong. FK validation catches these broken links.</p> <p>Learn more: FK Validation</p>"},{"location":"learning/glossary/#g","title":"G","text":""},{"location":"learning/glossary/#gold-layer","title":"Gold Layer","text":"<p>What it is: The final, business-ready layer with curated, aggregated, and report-ready data.</p> <p>Real-world analogy: A finished meal, plated and ready to serve. The raw ingredients (Bronze) were cleaned (Silver) and now it's restaurant-quality (Gold).</p> <p>Example: <pre><code>layer: gold\nnodes:\n  - name: monthly_sales_summary\n    source: silver.fact_sales\n    pattern: aggregation\n    aggregations:\n      - column: total_amount\n        function: sum\n        alias: monthly_revenue\n    group_by:\n      - year\n      - month\n      - region\n</code></pre></p> <p>Why it matters: Business users and dashboards consume Gold tables directly. These are optimized for fast queries and contain pre-calculated metrics so reports load instantly.</p> <p>Learn more: Medallion Architecture</p>"},{"location":"learning/glossary/#i","title":"I","text":""},{"location":"learning/glossary/#idempotent","title":"Idempotent","text":"<p>What it is: An operation that gives the same result no matter how many times you run it.</p> <p>Real-world analogy: Pressing an elevator button. Pressing it once calls the elevator. Pressing it 10 more times doesn't call 10 elevators\u2014you get the same result.</p> <p>Example: <pre><code># Idempotent merge - safe to rerun\nwrite_mode: merge\nmerge_keys:\n  - order_id\n# Running twice with same data = same result\n\n# NOT idempotent - DON'T do this for reruns\nwrite_mode: append\n# Running twice = duplicate rows!\n</code></pre></p> <p>Why it matters: Pipelines fail and get retried. If your pipeline isn't idempotent, retrying it corrupts your data (duplicates, wrong totals). Idempotent pipelines are safe to rerun.</p> <p>Learn more: Write Modes</p>"},{"location":"learning/glossary/#incremental-load","title":"Incremental Load","text":"<p>What it is: Only processing data that's new or changed since the last run, instead of reprocessing everything.</p> <p>Real-world analogy: Syncing photos to the cloud. Your phone doesn't upload all 10,000 photos every time\u2014just the new ones since last sync.</p> <p>Example: <pre><code>incremental:\n  enabled: true\n  watermark_column: updated_at\n  lookback_period: 2 days\n# Only process rows where updated_at &gt; last_run_time - 2 days\n</code></pre></p> <p>Why it matters: Full reloads waste time and compute. If you have 5 years of data but only 1 day is new, why process all 5 years? Incremental loads are faster and cheaper.</p> <p>Learn more: Incremental Processing</p>"},{"location":"learning/glossary/#j","title":"J","text":""},{"location":"learning/glossary/#join","title":"Join","text":"<p>What it is: Combining rows from two or more tables based on matching values in a column.</p> <p>Real-world analogy: Matching students to their grades. The student roster has names and IDs. The grade sheet has IDs and scores. A join combines them so you see \"Name: Alice, Score: 95.\"</p> <p>Example: <pre><code>transformations:\n  - type: join\n    right_source: dim_product\n    join_type: left\n    on:\n      - left: product_id\n        right: product_id\n    select:\n      - orders.*\n      - dim_product.product_name\n      - dim_product.category\n</code></pre></p> <p>Why it matters: Data lives in separate tables. Joins connect them. Without joins, you'd have order numbers but no customer names, product IDs but no descriptions.</p> <p>Learn more: Join Transformer</p>"},{"location":"learning/glossary/#m","title":"M","text":""},{"location":"learning/glossary/#medallion-architecture","title":"Medallion Architecture","text":"<p>What it is: A three-layer data organization pattern: Bronze (raw) \u2192 Silver (cleaned) \u2192 Gold (business-ready).</p> <p>Real-world analogy: A water treatment plant: - Bronze = Water from the lake (raw, unfiltered) - Silver = Filtered and treated (clean but not packaged) - Gold = Bottled water on store shelves (ready for consumers)</p> <p>Example: <pre><code># Bronze: Land raw data\nbronze_orders:\n  source: kafka/orders_topic\n  layer: bronze\n  write_mode: append\n\n# Silver: Clean and validate\nsilver_orders:\n  source: bronze_orders\n  layer: silver\n  validation:\n    rules:\n      - column: order_id\n        rule: not_null\n\n# Gold: Aggregate for reports\ngold_daily_sales:\n  source: silver_orders\n  layer: gold\n  pattern: aggregation\n</code></pre></p> <p>Why it matters: This structure makes debugging easy (check Bronze for raw data), ensures data quality (Silver validates), and provides fast analytics (Gold is optimized for queries).</p> <p>Learn more: Architecture Guide</p>"},{"location":"learning/glossary/#merge-upsert","title":"Merge (Upsert)","text":"<p>What it is: A smart write that inserts new rows and updates existing rows in one operation. \"Upsert\" = Update + Insert.</p> <p>Real-world analogy: A contact list sync. New contacts get added. Existing contacts get their info updated (new phone number, new address). Nothing gets duplicated.</p> <p>Example: <pre><code>write_mode: merge\nmerge_keys:\n  - customer_id\n# If customer_id exists \u2192 update the row\n# If customer_id is new \u2192 insert new row\n</code></pre></p> <p>Why it matters: Without merge, you'd have to delete all matching rows, then insert\u2014risky and slow. Merge handles both cases atomically, keeping your data consistent.</p> <p>Learn more: Merge Pattern</p>"},{"location":"learning/glossary/#n","title":"N","text":""},{"location":"learning/glossary/#natural-key","title":"Natural Key","text":"<p>What it is: A column (or columns) that uniquely identifies a row using real business data, not a generated number.</p> <p>Real-world analogy: Your email address or Social Security Number\u2014something from the real world that identifies you, not a made-up internal ID.</p> <p>Example: <pre><code>natural_key:\n  - employee_id      # HR system's real ID\n  - effective_date   # For SCD2, identifies the version\n# NOT a surrogate key (generated number)\n</code></pre></p> <p>Why it matters: Natural keys connect your data to the real world. When someone asks about \"employee E12345,\" you can find them. Surrogate keys like \"row 847291\" mean nothing to business users.</p> <p>Learn more: Keys and Identifiers</p>"},{"location":"learning/glossary/#node","title":"Node","text":"<p>What it is: A single step in a pipeline that reads data, optionally transforms it, and writes output.</p> <p>Real-world analogy: A station on an assembly line. Each station does one job: one paints, one installs wheels, one does quality check. Together, they build a car.</p> <p>Example: <pre><code>nodes:\n  - name: load_customers\n    source: raw/customers.csv\n    target: bronze.customers\n\n  - name: clean_customers\n    source: bronze.customers\n    target: silver.customers\n    transformations:\n      - type: trim_strings\n      - type: standardize_phone\n\n  - name: customer_metrics\n    source: silver.customers\n    target: gold.customer_360\n    pattern: aggregation\n</code></pre></p> <p>Why it matters: Breaking work into nodes makes pipelines easier to understand, debug, and maintain. If something fails, you know exactly which step broke.</p> <p>Learn more: Node Configuration</p>"},{"location":"learning/glossary/#o","title":"O","text":""},{"location":"learning/glossary/#orphan-record","title":"Orphan Record","text":"<p>What it is: A row with a foreign key value that doesn't exist in the parent table.</p> <p>Real-world analogy: A letter addressed to someone who doesn't live at that address. The recipient doesn't exist, so the letter has nowhere to go.</p> <p>Example: <pre><code># Order has customer_id: 999\n# But dim_customer has no customer_id: 999\n# \u2192 This order is an orphan\n\nvalidation:\n  foreign_key_checks:\n    - column: customer_id\n      reference_table: dim_customer\n      on_violation: quarantine\n      # Orphans go to quarantine table for review\n</code></pre></p> <p>Why it matters: Orphan records break joins and analytics. Queries for \"sales by customer region\" can't work if the customer doesn't exist. Catching orphans prevents broken reports.</p> <p>Learn more: Orphan Detection</p>"},{"location":"learning/glossary/#p","title":"P","text":""},{"location":"learning/glossary/#pattern","title":"Pattern","text":"<p>What it is: A pre-built template for common data processing tasks. Instead of writing complex logic, you declare what pattern to use.</p> <p>Real-world analogy: A recipe. You don't invent how to make bread from scratch\u2014you follow a proven recipe. Patterns are tested recipes for data work.</p> <p>Example: <pre><code># Instead of writing complex SCD2 logic...\npattern: scd2\nnatural_key:\n  - product_id\ntracked_columns:\n  - product_name\n  - price\n  - category\n# Odibi handles all the history tracking automatically\n</code></pre></p> <p>Available patterns: - <code>scd2</code> - History tracking with versioning - <code>merge</code> - Upsert operations - <code>aggregation</code> - Summarization - <code>dimension</code> - Lookup table management - <code>fact</code> - Transaction table handling</p> <p>Why it matters: Patterns encode best practices. Writing SCD2 logic from scratch takes hours and often has bugs. Using the pattern takes 5 lines and works correctly.</p> <p>Learn more: Patterns Reference</p>"},{"location":"learning/glossary/#pipeline","title":"Pipeline","text":"<p>What it is: A series of connected nodes that move data from sources to targets, with transformations along the way.</p> <p>Real-world analogy: An assembly line in a factory. Raw materials enter, go through stations (cutting, welding, painting), and finished products come out.</p> <p>Example: <pre><code>pipeline:\n  name: daily_sales_pipeline\n  schedule: \"0 6 * * *\"  # 6 AM daily\n\n  nodes:\n    - name: extract_sales\n      source: pos_system.transactions\n      target: bronze.sales\n\n    - name: clean_sales\n      source: bronze.sales\n      target: silver.sales\n      depends_on: [extract_sales]\n\n    - name: aggregate_sales\n      source: silver.sales\n      target: gold.daily_summary\n      depends_on: [clean_sales]\n</code></pre></p> <p>Why it matters: Pipelines automate data flow. Instead of manually running scripts, pipelines run on schedule, handle failures gracefully, and process data consistently every time.</p> <p>Learn more: Pipeline Guide</p>"},{"location":"learning/glossary/#q","title":"Q","text":""},{"location":"learning/glossary/#quarantine","title":"Quarantine","text":"<p>What it is: A holding area for data that failed validation rules. Bad data is separated so it doesn't contaminate good data.</p> <p>Real-world analogy: Airport customs. If something suspicious is found in your luggage, it's held aside for inspection. It doesn't get through to the destination until it's reviewed.</p> <p>Example: <pre><code>validation:\n  quarantine:\n    enabled: true\n    table: quarantine.failed_records\n    include_reason: true\n  rules:\n    - column: email\n      rule: regex\n      pattern: \"^[^@]+@[^@]+\\\\.[^@]+$\"\n      severity: error  # Failures go to quarantine\n</code></pre></p> <p>Why it matters: Without quarantine, bad data silently corrupts your analytics. With quarantine, good data flows through while problems are captured for review and correction.</p> <p>Learn more: Quarantine Setup</p>"},{"location":"learning/glossary/#s","title":"S","text":""},{"location":"learning/glossary/#schema","title":"Schema","text":"<p>What it is: The structure of a table\u2014what columns exist, what data type each column holds, and any constraints.</p> <p>Real-world analogy: A form template. It defines: Name (text), Age (number), Email (text with @ symbol). The schema says what information goes where and in what format.</p> <p>Example: <pre><code>schema:\n  columns:\n    - name: customer_id\n      type: string\n      nullable: false\n    - name: email\n      type: string\n      nullable: true\n    - name: signup_date\n      type: date\n      nullable: false\n    - name: lifetime_value\n      type: decimal(10,2)\n      nullable: true\n</code></pre></p> <p>Why it matters: Schemas catch errors early. If someone tries to put \"hello\" in an integer column, the schema rejects it immediately instead of corrupting downstream reports.</p> <p>Learn more: Schema Definition</p>"},{"location":"learning/glossary/#scd-type-1","title":"SCD Type 1","text":"<p>What it is: Slowly Changing Dimension handling that overwrites old values with new ones. No history is kept.</p> <p>Real-world analogy: Updating your address with the post office. They replace your old address with the new one. They don't keep a record of where you used to live.</p> <p>Example: <pre><code>pattern: scd1\nnatural_key:\n  - employee_id\n# Old values are overwritten:\n# Before: employee_id: 123, department: \"Sales\"\n# After:  employee_id: 123, department: \"Marketing\"\n# No history of \"Sales\" is kept\n</code></pre></p> <p>Why it matters: Use SCD1 when history doesn't matter (typo corrections, updated contact info). It's simpler and uses less storage than SCD2.</p> <p>Learn more: SCD Patterns</p>"},{"location":"learning/glossary/#scd-type-2","title":"SCD Type 2","text":"<p>What it is: Slowly Changing Dimension handling that keeps full history. Old values are marked as inactive; new values get new rows.</p> <p>Real-world analogy: A medical record. When your weight changes, the doctor doesn't erase the old weight\u2014they add a new entry with today's date. You can see your weight history over time.</p> <p>Example: <pre><code>pattern: scd2\nnatural_key:\n  - customer_id\ntracked_columns:\n  - loyalty_tier\n  - region\nvalid_from_column: effective_start\nvalid_to_column: effective_end\nis_current_column: is_current\n</code></pre></p> <p>Result: <pre><code>customer_id | loyalty_tier | effective_start | effective_end | is_current\n123         | Bronze       | 2023-01-01      | 2024-06-15    | false\n123         | Gold         | 2024-06-15      | 9999-12-31    | true\n</code></pre></p> <p>Why it matters: Historical analysis requires history. \"What tier was this customer when they made this purchase?\" Without SCD2, you can't answer that question.</p> <p>Learn more: SCD2 Pattern</p>"},{"location":"learning/glossary/#silver-layer","title":"Silver Layer","text":"<p>What it is: The middle layer where data is cleaned, validated, and standardized\u2014but not yet aggregated.</p> <p>Real-world analogy: A restaurant's prep kitchen. Raw ingredients (Bronze) are washed, chopped, and portioned (Silver). They're ready to cook but not yet finished dishes (Gold).</p> <p>Example: <pre><code>layer: silver\nnodes:\n  - name: clean_orders\n    source: bronze.raw_orders\n    validation:\n      rules:\n        - column: order_id\n          rule: not_null\n        - column: total\n          rule: positive\n    transformations:\n      - type: deduplicate\n        keys: [order_id]\n      - type: standardize_dates\n        columns: [order_date]\n</code></pre></p> <p>Why it matters: Silver is your \"single source of truth.\" Bronze might have duplicates and errors. Silver has clean, validated data that Gold and other consumers can trust.</p> <p>Learn more: Medallion Architecture</p>"},{"location":"learning/glossary/#star-schema","title":"Star Schema","text":"<p>What it is: A database design where a central fact table connects to multiple dimension tables, forming a star shape.</p> <p>Real-world analogy: A wheel with spokes. The hub (fact table) is at the center. Each spoke leads to a dimension (who, what, where, when). All analysis starts at the center and reaches out.</p> <p>Diagram: <pre><code>                    dim_customer\n                         |\n    dim_product ---- fact_sales ---- dim_date\n                         |\n                    dim_store\n</code></pre></p> <p>Example: <pre><code># Fact at center\nfact_sales:\n  pattern: fact\n  foreign_keys:\n    - column: customer_id\n      references: dim_customer\n    - column: product_id\n      references: dim_product\n    - column: date_id\n      references: dim_date\n    - column: store_id\n      references: dim_store\n</code></pre></p> <p>Why it matters: Star schemas are optimized for analytics. Queries like \"sales by region by month by product category\" are fast because the structure matches how business users think.</p> <p>Learn more: Dimensional Modeling</p>"},{"location":"learning/glossary/#surrogate-key","title":"Surrogate Key","text":"<p>What it is: An internally generated unique identifier (usually a number) that has no business meaning. Created by the system, not from source data.</p> <p>Real-world analogy: A library book's barcode number. It's not the ISBN or title\u2014it's a number the library made up to track that specific copy internally.</p> <p>Example: <pre><code>generate_surrogate_key:\n  column_name: customer_sk\n  strategy: hash  # or: sequence, uuid\n  source_columns:\n    - customer_id\n    - effective_start_date\n</code></pre></p> <p>Why it matters: Surrogate keys are stable (never change), performant (integers join faster than strings), and handle SCD2 (each version gets its own key). They're the internal \"address\" for each row.</p> <p>Learn more: Key Generation</p>"},{"location":"learning/glossary/#t","title":"T","text":""},{"location":"learning/glossary/#transformer","title":"Transformer","text":"<p>What it is: A reusable operation that modifies data\u2014like a function you can apply to any dataset.</p> <p>Real-world analogy: A coffee grinder. You put in beans (input), it grinds them (transformation), you get ground coffee (output). The same grinder works for any type of bean.</p> <p>Example: <pre><code>transformations:\n  - type: rename_columns\n    mapping:\n      cust_nm: customer_name\n      ord_dt: order_date\n\n  - type: add_column\n    name: order_year\n    expression: \"year(order_date)\"\n\n  - type: filter\n    condition: \"order_total &gt; 0\"\n</code></pre></p> <p>Available transformers: - <code>rename_columns</code> - Change column names - <code>add_column</code> - Create calculated columns - <code>filter</code> - Keep only matching rows - <code>deduplicate</code> - Remove duplicate rows - <code>join</code> - Combine with other tables - And many more...</p> <p>Why it matters: Transformers are composable building blocks. Complex data processing becomes a readable list of simple steps.</p> <p>Learn more: Transformers Reference</p>"},{"location":"learning/glossary/#v","title":"V","text":""},{"location":"learning/glossary/#validation","title":"Validation","text":"<p>What it is: Checking that data meets defined rules before accepting it into your system.</p> <p>Real-world analogy: A bouncer at a club checking IDs. No valid ID? You don't get in. Validation checks if data \"has valid ID\" before letting it into your tables.</p> <p>Example: <pre><code>validation:\n  rules:\n    # Must have a value\n    - column: order_id\n      rule: not_null\n      severity: error\n\n    # Must be a valid email format\n    - column: email\n      rule: regex\n      pattern: \"^[^@]+@[^@]+$\"\n      severity: warning\n\n    # Must be a real date\n    - column: order_date\n      rule: not_in_future\n      severity: error\n\n    # Must be positive\n    - column: quantity\n      rule: positive\n      severity: error\n</code></pre></p> <p>Severity levels: - <code>error</code> - Stop processing, quarantine the row - <code>warning</code> - Log the issue, continue processing</p> <p>Why it matters: Bad data in = bad decisions out. Validation catches problems at the door instead of letting them corrupt your analytics.</p> <p>Learn more: Validation Guide</p>"},{"location":"learning/glossary/#quick-reference-table","title":"Quick Reference Table","text":"Term One-Line Definition Aggregation Summarizing many rows into totals/averages Append Adding rows without changing existing ones Bronze Layer Raw data storage, untouched Connection Saved credentials for data sources DAG Map of step dependencies Data Quality Measuring data correctness Delta Lake Smart file format with versioning Dimension Table Lookup/reference data Engine Processing tool (Spark/Pandas/Polars) ETL vs ELT When transformation happens Fact Table Transaction/event data Foreign Key Link between tables Gold Layer Business-ready, curated data Idempotent Safe to run multiple times Incremental Load Only process new/changed data Join Combining data from multiple tables Medallion Architecture Bronze \u2192 Silver \u2192 Gold layering Merge Insert new, update existing Natural Key Business identifier Node Single pipeline step Orphan Record FK with no matching parent Pattern Reusable template for common tasks Pipeline Series of connected processing steps Quarantine Holding area for bad data Schema Structure of data SCD Type 1 Overwrite old with new SCD Type 2 Keep full history Silver Layer Cleaned, validated data Star Schema Facts center, dimensions around Surrogate Key Generated internal ID Transformer Reusable data operation Validation Checking data meets rules"},{"location":"learning/glossary/#next-steps","title":"Next Steps","text":"<ul> <li>New to Odibi? Start with Getting Started</li> <li>Building your first pipeline? See Tutorial</li> <li>Looking for specific syntax? Check YAML Schema Reference</li> </ul>"},{"location":"mcp/AI_PROMPT/","title":"Odibi MCP Implementation \u2014 AI Assistant Prompt","text":"<p>Copy this prompt at the start of each session with Cline, Continue, or other AI coding assistants</p>"},{"location":"mcp/AI_PROMPT/#session-start-prompt","title":"Session Start Prompt","text":"<p><pre><code>You are helping implement the Odibi MCP Facade \u2014 a read-only AI interface for the Odibi data engineering framework.\n\n## Your Available Tools (USE THESE EXACT NAMES)\n\n### Filesystem Tools (Primary)\n| Tool | When to Use |\n|------|-------------|\n| `read_file` | Read spec files, existing code, dependencies |\n| `create_new_file` | Create new Python modules (contracts, tools, tests) |\n| `edit_existing_file` | Line-based edits (add/delete/replace lines) |\n| `single_find_and_replace` | Find and replace exact strings |\n| `ls` | List directory contents |\n| `file_glob_search` | Find files by pattern (e.g., `*.py`, `test_*.py`) |\n| `filesystem_directory_tree` | Get JSON tree view of project structure |\n| `view_diff` | Check working directory changes before commit |\n\n### Terminal Tools\n| Tool | When to Use |\n|------|-------------|\n| `run_terminal_command` | Run pytest, ruff, python -c, any CLI command |\n\n### Odibi Knowledge Tools (USE FOR CONTEXT)\n| Tool | When to Use |\n|------|-------------|\n| `odibi_knowledge_get_deep_context` | Get full 2,200+ line framework docs |\n| `odibi_knowledge_list_transformers` | See all built-in transformers |\n| `odibi_knowledge_list_patterns` | See all DWH patterns |\n| `odibi_knowledge_list_connections` | See all connection types |\n| `odibi_knowledge_explain` | Get detailed docs for any feature |\n| `odibi_knowledge_validate_yaml` | Validate pipeline YAML |\n| `odibi_knowledge_query_codebase` | Semantic search over odibi code |\n\n### Sequential Thinking (USE FOR PLANNING)\n| Tool | When to Use |\n|------|-------------|\n| `sequential_thinking_sequentialthinking` | Multi-step planning, debugging, architecture decisions |\n\n**USE sequential_thinking BEFORE implementing:**\n- Planning implementation order across multiple files\n- Debugging complex issues with multiple causes\n- Analyzing task dependencies\n- Breaking down large tasks into atomic steps\n\n### Git Tools (After Implementation)\n| Tool | When to Use |\n|------|-------------|\n| `git_status` | Check what files changed |\n| `git_diff_unstaged` | Review changes before staging |\n| `git_add` | Stage files for commit |\n| `git_commit` | Commit completed phase |\n\n## Step 1: Read Spec Files First\nBefore starting ANY task, run these tools:\n</code></pre> read_file docs/mcp/SPEC.md read_file docs/mcp/IMPLEMENTATION_PLAN.md read_file docs/mcp/CHECKLIST.md <pre><code>## Step 2: Understand Existing Code\n</code></pre> ls odibi_mcp/ ls odibi_mcp/contracts/ read_file odibi_mcp/server.py read_file odibi_mcp/knowledge.py file_glob_search pattern=\".py\" path=\"odibi_mcp/\" <pre><code>## Step 3: Use Sequential Thinking for Planning\nBefore implementing, call:\n</code></pre> sequential_thinking_sequentialthinking <pre><code>With thought: \"I need to implement [TASK]. Let me analyze dependencies, existing code patterns, and plan the implementation.\"\n\n## Core Principles (MUST Follow)\n1. READ-ONLY: MCP never mutates data, triggers runs, or alters state\n2. SINGLE-PROJECT: Each request operates within one project context\n3. DENY-BY-DEFAULT: Path discovery requires explicit allowlists\n4. TYPED RESPONSES: Use Pydantic models, never Dict[str, Any]\n5. PHYSICAL REFS GATED: 3 conditions must pass to include physical paths\n\n## Existing Code Context\n- odibi_mcp/ already exists with server.py and knowledge.py\n- We are ADDING to it, not replacing\n- Existing tools (list_transformers, validate_yaml, etc.) stay untouched\n- New code goes in subfolders: contracts/, access/, tools/, etc.\n\n## Coding Standards\n- Use Pydantic v2 syntax (model_dump, not dict)\n- Follow existing Odibi patterns (check odibi/config.py for examples)\n- Add type hints to all functions\n- No Dict[str, Any] in response models \u2014 use typed classes\n\n## Step 4: Implement the Task\nFor new files:\n</code></pre> create_new_file path=\"odibi_mcp/contracts/[filename].py\" content=\"...\" <pre><code>For existing files:\n</code></pre> edit_existing_file path=\"odibi_mcp/contracts/[filename].py\" edits=[...] <pre><code>## Step 5: Verify Implementation\nAfter writing code, ALWAYS run:\n</code></pre> run_terminal_command command=\"python -c \\\"from odibi_mcp.contracts.[module] import [Class]; print('OK')\\\"\" run_terminal_command command=\"pytest tests/unit/mcp/test_[module].py -v\" run_terminal_command command=\"ruff check odibi_mcp/\" <pre><code>## Step 6: Update Checklist &amp; Commit\n</code></pre> single_find_and_replace path=\"docs/mcp/CHECKLIST.md\" find=\"- [ ] [TASK_ID].\" replace=\"- [x] [TASK_ID].*\" git_status git_add files=[\"odibi_mcp/contracts/[file].py\", \"docs/mcp/CHECKLIST.md\"] git_commit message=\"feat(mcp): implement [TASK_ID] - [description]\" <pre><code>## Current Task\nI am working on task [TASK_ID] from the implementation plan.\n[PASTE TASK DETAILS HERE]\n\nPlease implement this task following the specification.\n</code></pre></p>"},{"location":"mcp/AI_PROMPT/#task-specific-prompts","title":"Task-Specific Prompts","text":""},{"location":"mcp/AI_PROMPT/#starting-a-new-phase","title":"Starting a New Phase","text":"<pre><code>I'm starting Phase [N]: [PHASE_NAME].\n\nExecute these tools in order:\n1. sequential_thinking_sequentialthinking \u2192 Plan the phase\n2. read_file docs/mcp/SPEC.md \u2192 Focus on: [RELEVANT_SECTION]\n3. read_file docs/mcp/IMPLEMENTATION_PLAN.md \u2192 Find Phase [N] tasks\n4. ls odibi_mcp/ \u2192 See current structure\n5. file_glob_search pattern=\"[RELEVANT_PATTERN]\" path=\"odibi_mcp/\"\n\nLet's begin with task [N]a.\n</code></pre>"},{"location":"mcp/AI_PROMPT/#implementing-a-single-task","title":"Implementing a Single Task","text":"<pre><code>Implement task [TASK_ID] from docs/mcp/IMPLEMENTATION_PLAN.md.\n\nTask: [TASK_DESCRIPTION]\nFile: [FILE_PATH]\nDependencies: [LIST_DEPENDENCIES]\n\nExecute this workflow:\n1. sequential_thinking_sequentialthinking \u2192 Plan implementation\n2. read_file docs/mcp/SPEC.md \u2192 Get exact model definition\n3. read_file [DEPENDENCY_FILES] \u2192 Understand dependencies\n4. create_new_file OR edit_existing_file \u2192 Implement\n5. run_terminal_command command=\"pytest tests/unit/mcp/[TEST_FILE] -v\"\n6. run_terminal_command command=\"ruff check odibi_mcp/\"\n7. single_find_and_replace \u2192 Mark task complete in CHECKLIST.md\n</code></pre>"},{"location":"mcp/AI_PROMPT/#fixing-a-failed-test","title":"Fixing a Failed Test","text":"<pre><code>Task [TASK_ID] failed verification.\n\nError output:\n[PASTE_ERROR]\n\nExecute this workflow:\n1. sequential_thinking_sequentialthinking \u2192 Analyze the error\n2. read_file [FAILING_FILE] \u2192 Understand current implementation\n3. file_glob_search pattern=\"*[related]*\" \u2192 Find related code\n4. edit_existing_file \u2192 Apply fix\n5. run_terminal_command command=\"pytest [TEST] -v\" \u2192 Re-verify\n</code></pre>"},{"location":"mcp/AI_PROMPT/#reviewing-before-commit","title":"Reviewing Before Commit","text":"<pre><code>I've completed Phase [N]. Execute this review:\n\n1. ls odibi_mcp/[FOLDER]/ \u2192 Verify files exist\n2. run_terminal_command command=\"pytest tests/unit/mcp/ -v\"\n3. run_terminal_command command=\"python -c \\\"from odibi_mcp import *\\\"\"\n4. run_terminal_command command=\"ruff check odibi_mcp/\"\n5. view_diff \u2192 Review all changes\n6. git_status \u2192 Check status\n7. git_add files=[...] \u2192 Stage files\n8. git_commit message=\"feat(mcp): complete Phase [N]\"\n\nReport any failures.\n</code></pre>"},{"location":"mcp/AI_PROMPT/#quick-reference-file-locations","title":"Quick Reference: File Locations","text":"Component Location Enums <code>odibi_mcp/contracts/enums.py</code> Envelope models <code>odibi_mcp/contracts/envelope.py</code> Run selector <code>odibi_mcp/contracts/selectors.py</code> Resource refs <code>odibi_mcp/contracts/resources.py</code> Access context <code>odibi_mcp/contracts/access.py</code> Time window <code>odibi_mcp/contracts/time.py</code> Schema models <code>odibi_mcp/contracts/schema.py</code> Graph models <code>odibi_mcp/contracts/graph.py</code> Diff models <code>odibi_mcp/contracts/diff.py</code> Stats models <code>odibi_mcp/contracts/stats.py</code> Discovery models <code>odibi_mcp/contracts/discovery.py</code> Access enforcement <code>odibi_mcp/access/</code> Audit logging <code>odibi_mcp/audit/</code> Story loader <code>odibi_mcp/loaders/story.py</code> Discovery limits <code>odibi_mcp/discovery/limits.py</code> Tool implementations <code>odibi_mcp/tools/</code> Utilities <code>odibi_mcp/utils/</code> Unit tests <code>tests/unit/mcp/</code> Integration tests <code>tests/integration/mcp/</code>"},{"location":"mcp/AI_PROMPT/#quick-reference-verification-commands","title":"Quick Reference: Verification Commands","text":"<pre><code># Check imports work\nrun_terminal_command command=\"python -c \\\"from odibi_mcp.contracts.enums import TruncatedReason; print('OK')\\\"\"\n\n# Run specific test\nrun_terminal_command command=\"pytest tests/unit/mcp/test_envelope.py -v\"\n\n# Run all MCP tests\nrun_terminal_command command=\"pytest tests/unit/mcp/ -v\"\n\n# Lint check\nrun_terminal_command command=\"ruff check odibi_mcp/\"\n\n# Format check\nrun_terminal_command command=\"ruff format odibi_mcp/ --check\"\n</code></pre>"},{"location":"mcp/AI_PROMPT/#quick-reference-common-tool-patterns","title":"Quick Reference: Common Tool Patterns","text":""},{"location":"mcp/AI_PROMPT/#exploring-the-codebase","title":"Exploring the codebase","text":"<pre><code>ls odibi_mcp/\nls odibi_mcp/contracts/\nfile_glob_search pattern=\"*.py\" path=\"odibi_mcp/\"\nfile_glob_search pattern=\"test_*.py\" path=\"tests/unit/mcp/\"\nodibi_knowledge_query_codebase query=\"Pydantic model\"\n</code></pre>"},{"location":"mcp/AI_PROMPT/#reading-files","title":"Reading files","text":"<pre><code>read_file docs/mcp/SPEC.md\nread_file docs/mcp/IMPLEMENTATION_PLAN.md\nread_file docs/mcp/CHECKLIST.md\nread_file odibi/config.py  # For patterns\n</code></pre>"},{"location":"mcp/AI_PROMPT/#creating-new-files","title":"Creating new files","text":"<pre><code>create_new_file path=\"odibi_mcp/contracts/new_model.py\" content=\"...\"\n</code></pre>"},{"location":"mcp/AI_PROMPT/#editing-existing-files","title":"Editing existing files","text":"<pre><code>edit_existing_file path=\"odibi_mcp/contracts/enums.py\" edits=[{\"type\": \"insert\", \"line\": 10, \"content\": \"...\"}]\nsingle_find_and_replace path=\"docs/mcp/CHECKLIST.md\" find=\"- [ ]\" replace=\"- [x]\"\n</code></pre>"},{"location":"mcp/AI_PROMPT/#verifying-changes","title":"Verifying changes","text":"<pre><code>run_terminal_command command=\"pytest tests/unit/mcp/ -v\"\nrun_terminal_command command=\"ruff check odibi_mcp/\"\nrun_terminal_command command=\"python -c \\\"from odibi_mcp import *\\\"\"\n</code></pre>"},{"location":"mcp/AI_PROMPT/#git-workflow","title":"Git workflow","text":"<pre><code>git_status\nview_diff\ngit_add files=[\"odibi_mcp/contracts/enums.py\"]\ngit_commit message=\"feat(mcp): add TruncatedReason enum\"\n</code></pre>"},{"location":"mcp/AI_PROMPT/#reminders","title":"Reminders","text":"<p>After each task: 1. <code>single_find_and_replace</code> \u2192 Mark task complete in CHECKLIST.md 2. <code>run_terminal_command</code> \u2192 Verify with pytest and ruff 3. <code>git_commit</code> \u2192 Commit after each phase (not each task)</p>"},{"location":"mcp/CHECKLIST/","title":"Odibi MCP Facade \u2014 Implementation Checklist","text":"<p>Instructions: Mark tasks with <code>[x]</code> as you complete them. Run the verification command after each task.</p>"},{"location":"mcp/CHECKLIST/#phase-1-core-contracts-4-hours","title":"Phase 1: Core Contracts (4 hours)","text":"<ul> <li>[x] 1a. Create <code>TruncatedReason</code> enum \u2192 <code>odibi_mcp/contracts/enums.py</code></li> <li>[x] 1b. Create <code>PolicyApplied</code> model \u2192 <code>odibi_mcp/contracts/envelope.py</code></li> <li>[x] 1c. Create <code>MCPEnvelope</code> base model \u2192 <code>odibi_mcp/contracts/envelope.py</code></li> <li>[x] 1d. Create <code>RunSelector</code> type \u2192 <code>odibi_mcp/contracts/selectors.py</code></li> <li>[x] 1e. Create <code>ResourceRef</code> model \u2192 <code>odibi_mcp/contracts/resources.py</code></li> <li>[x] 1f. Create <code>ConnectionPolicy</code> model \u2192 <code>odibi_mcp/contracts/access.py</code></li> <li>[x] 1g. Create <code>AccessContext</code> model \u2192 <code>odibi_mcp/contracts/access.py</code></li> <li>[x] 1h. Create <code>TimeWindow</code> model \u2192 <code>odibi_mcp/contracts/time.py</code></li> </ul> <p>Phase 1 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-2-typed-response-models-3-hours","title":"Phase 2: Typed Response Models (3 hours)","text":"<ul> <li>[x] 2a. Create <code>ColumnSpec</code> model \u2192 <code>odibi_mcp/contracts/schema.py</code></li> <li>[x] 2b. Create <code>SchemaResponse</code> model \u2192 <code>odibi_mcp/contracts/schema.py</code></li> <li>[x] 2c. Create <code>GraphNode</code> and <code>GraphEdge</code> models \u2192 <code>odibi_mcp/contracts/graph.py</code></li> <li>[x] 2d. Create <code>GraphData</code> model \u2192 <code>odibi_mcp/contracts/graph.py</code></li> <li>[x] 2e. Create <code>SchemaChange</code> model \u2192 <code>odibi_mcp/contracts/schema.py</code></li> <li>[x] 2f. Create <code>DiffSummary</code> model \u2192 <code>odibi_mcp/contracts/diff.py</code></li> <li>[x] 2g. Create <code>NodeStatsResponse</code> model \u2192 <code>odibi_mcp/contracts/stats.py</code></li> <li>[x] 2h. Create <code>FileInfo</code> and <code>ListFilesResponse</code> models \u2192 <code>odibi_mcp/contracts/discovery.py</code></li> </ul> <p>Phase 2 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-3-access-enforcement-3-hours","title":"Phase 3: Access Enforcement (3 hours)","text":"<ul> <li>[x] 3a. Create access context injection \u2192 <code>odibi_mcp/access/context.py</code></li> <li>[x] 3b. Add project scoping to <code>CatalogManager</code> \u2192 <code>odibi/catalog.py</code></li> <li>[x] 3c. Add access checks to <code>StoryLoader</code> \u2192 <code>odibi_mcp/loaders/story.py</code></li> <li>[x] 3d. Create path validation for discovery \u2192 <code>odibi_mcp/access/path_validator.py</code></li> <li>[x] 3e. Create physical ref gate \u2192 <code>odibi_mcp/access/physical_gate.py</code></li> </ul> <p>Phase 3 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-4-audit-logger-1-hour","title":"Phase 4: Audit Logger (1 hour)","text":"<ul> <li>[x] 4a. Create <code>AuditEntry</code> model \u2192 <code>odibi_mcp/audit/entry.py</code></li> <li>[x] 4b. Create <code>AuditLogger</code> \u2192 <code>odibi_mcp/audit/logger.py</code></li> <li>[x] 4c. Integrate audit logger with server \u2192 <code>odibi_mcp/server.py</code></li> </ul> <p>Phase 4 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-5-story-tools-3-hours","title":"Phase 5: Story Tools (3 hours)","text":"<ul> <li> <p>[x] 5a. Create <code>story_read</code> tool \u2192 <code>odibi_mcp/tools/story.py</code></p> </li> <li> <p>[x] 5b. Create <code>story_diff</code> tool \u2192 <code>odibi_mcp/tools/story.py</code></p> </li> <li>[x] 5c. Create <code>node_describe</code> tool \u2192 <code>odibi_mcp/tools/story.py</code></li> </ul> <p>Phase 5 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-6-sample-tools-2-hours","title":"Phase 6: Sample Tools (2 hours)","text":"<ul> <li>[x] 6a. Create sample limiter \u2192 <code>odibi_mcp/utils/limiter.py</code></li> <li>[x] 6b. Create <code>node_sample</code> tool \u2192 <code>odibi_mcp/tools/sample.py</code></li> <li>[x] 6c. Create <code>node_sample_in</code> tool \u2192 <code>odibi_mcp/tools/sample.py</code></li> <li>[x] 6d. Create <code>node_failed_rows</code> tool \u2192 <code>odibi_mcp/tools/sample.py</code></li> </ul> <p>Phase 6 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-7-catalog-tools-3-hours","title":"Phase 7: Catalog Tools (3 hours)","text":"<ul> <li>[x] 7a. Create <code>node_stats</code> tool \u2192 <code>odibi_mcp/tools/catalog.py</code></li> <li>[x] 7b. Create <code>pipeline_stats</code> tool \u2192 <code>odibi_mcp/tools/catalog.py</code></li> <li>[x] 7c. Create <code>failure_summary</code> tool \u2192 <code>odibi_mcp/tools/catalog.py</code></li> <li>[x] 7d. Create <code>schema_history</code> tool \u2192 <code>odibi_mcp/tools/catalog.py</code></li> </ul> <p>Phase 7 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-8-lineage-tools-2-hours","title":"Phase 8: Lineage Tools (2 hours)","text":"<ul> <li>[x] 8a. Create <code>lineage_upstream</code> tool \u2192 <code>odibi_mcp/tools/lineage.py</code></li> <li>[x] 8b. Create <code>lineage_downstream</code> tool \u2192 <code>odibi_mcp/tools/lineage.py</code></li> <li>[x] 8c. Create <code>lineage_graph</code> tool \u2192 <code>odibi_mcp/tools/lineage.py</code></li> </ul> <p>Phase 8 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-9-schema-tools-2-hours","title":"Phase 9: Schema Tools (2 hours)","text":"<ul> <li>[x] 9a. Create <code>output_schema</code> tool \u2192 <code>odibi_mcp/tools/schema.py</code></li> <li>[x] 9b. Create <code>list_outputs</code> tool \u2192 <code>odibi_mcp/tools/schema.py</code></li> </ul> <p>Phase 9 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-10-source-discovery-tools-5-hours","title":"Phase 10: Source Discovery Tools (5 hours)","text":"<ul> <li>[x] 10a. Create <code>DiscoveryLimits</code> config \u2192 <code>odibi_mcp/discovery/limits.py</code></li> <li>[x] 10b. Create <code>list_files</code> tool \u2192 <code>odibi_mcp/tools/discovery.py</code></li> <li>[x] 10c. Create <code>list_tables</code> tool \u2192 <code>odibi_mcp/tools/discovery.py</code></li> <li>[x] 10d. Create <code>infer_schema</code> tool \u2192 <code>odibi_mcp/tools/discovery.py</code></li> <li>[x] 10e. Create <code>describe_table</code> tool \u2192 <code>odibi_mcp/tools/discovery.py</code></li> <li>[x] 10f. Create <code>preview_source</code> tool \u2192 <code>odibi_mcp/tools/discovery.py</code></li> </ul> <p>Phase 10 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-11-error-handling-1-hour","title":"Phase 11: Error Handling (1 hour)","text":"<ul> <li>[x] 11a. Create error response helper \u2192 <code>odibi_mcp/utils/errors.py</code></li> <li>[x] 11b. Integrate error handling in server \u2192 <code>odibi_mcp/server.py</code></li> </ul> <p>Phase 11 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#phase-12-integration-tests-6-hours","title":"Phase 12: Integration Tests (6 hours)","text":"<ul> <li>[x] 12a. Create mock Story fixtures \u2192 <code>tests/fixtures/mcp_stories.py</code></li> <li>[x] 12b. Create mock Catalog fixtures \u2192 <code>tests/fixtures/mcp_catalog.py</code></li> <li>[x] 12c. Integration test: Story tools E2E \u2192 <code>tests/integration/mcp/test_story_e2e.py</code></li> <li>[x] 12d. Integration test: Discovery tools E2E \u2192 <code>tests/integration/mcp/test_discovery_e2e.py</code></li> <li>[x] 12e. Integration test: Access enforcement \u2192 <code>tests/integration/mcp/test_access_e2e.py</code></li> <li>[x] 12f. Integration test: Full AI workflow \u2192 <code>tests/integration/mcp/test_ai_workflow.py</code></li> </ul> <p>Phase 12 Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#post-implementation","title":"Post-Implementation","text":"<ul> <li>[x] Update <code>odibi_mcp/README.md</code> with new tools</li> <li>[x] Add examples to <code>docs/mcp/examples/</code></li> <li>[x] Update <code>CHANGELOG.md</code> with v2.11.0 MCP enhancements</li> <li>[x] Create <code>mcp_config.example.yaml</code></li> <li>[x] Add MCP config to <code>odibi init</code> scaffolding</li> </ul> <p>Post-Implementation Complete: [x]</p>"},{"location":"mcp/CHECKLIST/#summary","title":"Summary","text":"Phase Tasks Status 1. Core Contracts 8 \u2705 2. Typed Models 8 \u2705 3. Access Enforcement 5 \u2705 4. Audit Logger 3 \u2705 5. Story Tools 3 \u2705 6. Sample Tools 4 \u2705 7. Catalog Tools 4 \u2705 8. Lineage Tools 3 \u2705 9. Schema Tools 2 \u2705 10. Discovery Tools 6 \u2705 11. Error Handling 2 \u2705 12. Integration Tests 6 \u2705 Post-Implementation 5 \u2705 Total 59 \u2705"},{"location":"mcp/CHECKLIST/#progress-tracking","title":"Progress Tracking","text":"<p>Started: 2026-01-23 Phase 1 Done: 2026-01-23 Phase 6 Done (MVP): 2026-01-23 Phase 12 Done (Complete): 2026-01-23 Post-Implementation Done: 2026-01-23</p>"},{"location":"mcp/CHECKLIST/#notes","title":"Notes","text":"<p>Use this space to track blockers, decisions, or deviations from the plan:</p> <pre><code>-\n-\n-\n</code></pre>"},{"location":"mcp/IMPLEMENTATION_PLAN/","title":"Odibi MCP Facade \u2014 Implementation Plan","text":"<p>Version: v4.1 Total Effort: ~35 hours Approach: Atomic tasks suitable for AI-assisted implementation (Continue + GPT 5.2)</p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#how-to-use-this-plan","title":"How to Use This Plan","text":"<ol> <li>Work through phases in order (dependencies noted)</li> <li>Each task is atomic: one file, one model, one test</li> <li>Run the verification command after each task</li> <li>Mark the checkbox when complete</li> <li>Commit after each phase</li> </ol>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-1-core-contracts-4-hours","title":"Phase 1: Core Contracts (4 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#1a-create-truncatedreason-enum","title":"1a. Create TruncatedReason Enum","text":"<p>File: <code>odibi_mcp/contracts/enums.py</code></p> <p>Task: Create the TruncatedReason enum for typed truncation responses.</p> <pre><code>from enum import Enum\n\nclass TruncatedReason(str, Enum):\n    \"\"\"Explicit reasons for response truncation.\"\"\"\n    ROW_LIMIT = \"row_limit\"\n    COLUMN_LIMIT = \"column_limit\"\n    BYTE_LIMIT = \"byte_limit\"\n    CELL_LIMIT = \"cell_limit\"\n    POLICY_MASKING = \"policy_masking\"\n    SAMPLING_ONLY = \"sampling_only\"\n    PAGINATION = \"pagination\"\n</code></pre> <p>Verification: <pre><code>python -c \"from odibi_mcp.contracts.enums import TruncatedReason; print(TruncatedReason.ROW_LIMIT.value)\"\n</code></pre></p> <p>Expected: <code>row_limit</code></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1b-create-policyapplied-model","title":"1b. Create PolicyApplied Model","text":"<p>File: <code>odibi_mcp/contracts/envelope.py</code></p> <p>Task: Create the PolicyApplied model to track which policies were applied.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_envelope.py::test_policy_applied -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1c-create-mcpenvelope-base-model","title":"1c. Create MCPEnvelope Base Model","text":"<p>File: <code>odibi_mcp/contracts/envelope.py</code> (append)</p> <p>Task: Create MCPEnvelope, MCPSuccessEnvelope, MCPErrorEnvelope.</p> <p>Dependencies: 1a, 1b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_envelope.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1d-create-runselector-type","title":"1d. Create RunSelector Type","text":"<p>File: <code>odibi_mcp/contracts/selectors.py</code></p> <p>Task: Create RunSelector union type with RunById model.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>python -c \"from odibi_mcp.contracts.selectors import RunSelector, DEFAULT_RUN_SELECTOR; print(DEFAULT_RUN_SELECTOR)\"\n</code></pre></p> <p>Expected: <code>latest_successful</code></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1e-create-resourceref-model","title":"1e. Create ResourceRef Model","text":"<p>File: <code>odibi_mcp/contracts/resources.py</code></p> <p>Task: Create ResourceRef with logical/physical ref gating.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_resources.py::test_resource_ref -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1f-create-connectionpolicy-model","title":"1f. Create ConnectionPolicy Model","text":"<p>File: <code>odibi_mcp/contracts/access.py</code></p> <p>Task: Create ConnectionPolicy with deny-by-default path checking.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_access.py::test_connection_policy_deny_by_default -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1g-create-accesscontext-model","title":"1g. Create AccessContext Model","text":"<p>File: <code>odibi_mcp/contracts/access.py</code> (append)</p> <p>Task: Create AccessContext with project/connection/path validation.</p> <p>Dependencies: 1f</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_access.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#1h-create-timewindow-model","title":"1h. Create TimeWindow Model","text":"<p>File: <code>odibi_mcp/contracts/time.py</code></p> <p>Task: Create TimeWindow with complete validation (ordering, presence, timezone).</p> <p>Dependencies: None</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_time.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-2-typed-response-models-3-hours","title":"Phase 2: Typed Response Models (3 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#2a-create-columnspec-model","title":"2a. Create ColumnSpec Model","text":"<p>File: <code>odibi_mcp/contracts/schema.py</code></p> <p>Task: Create ColumnSpec to replace parallel arrays.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>python -c \"from odibi_mcp.contracts.schema import ColumnSpec; print(ColumnSpec(name='id', dtype='int').model_dump())\"\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2b-create-schemaresponse-model","title":"2b. Create SchemaResponse Model","text":"<p>File: <code>odibi_mcp/contracts/schema.py</code> (append)</p> <p>Task: Create SchemaResponse with List[ColumnSpec].</p> <p>Dependencies: 2a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_schema.py::test_schema_response -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2c-create-graphnode-and-graphedge-models","title":"2c. Create GraphNode and GraphEdge Models","text":"<p>File: <code>odibi_mcp/contracts/graph.py</code></p> <p>Task: Create typed graph models for DAG visualization.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_graph.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2d-create-graphdata-model","title":"2d. Create GraphData Model","text":"<p>File: <code>odibi_mcp/contracts/graph.py</code> (append)</p> <p>Task: Create GraphData container with List[GraphNode], List[GraphEdge].</p> <p>Dependencies: 2c</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_graph.py::test_graph_data -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2e-create-schemachange-model","title":"2e. Create SchemaChange Model","text":"<p>File: <code>odibi_mcp/contracts/schema.py</code> (append)</p> <p>Task: Create ColumnChange and SchemaChange models.</p> <p>Dependencies: 2a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_schema.py::test_schema_change -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2f-create-diffsummary-model","title":"2f. Create DiffSummary Model","text":"<p>File: <code>odibi_mcp/contracts/diff.py</code></p> <p>Task: Create DiffSummary with hashed counts by default.</p> <p>Dependencies: 2e</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_diff.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2g-create-nodestatsresponse-model","title":"2g. Create NodeStatsResponse Model","text":"<p>File: <code>odibi_mcp/contracts/stats.py</code></p> <p>Task: Create StatPoint and NodeStatsResponse models.</p> <p>Dependencies: 1h (TimeWindow)</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_stats.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#2h-create-fileinfo-and-listfilesresponse-models","title":"2h. Create FileInfo and ListFilesResponse Models","text":"<p>File: <code>odibi_mcp/contracts/discovery.py</code></p> <p>Task: Create typed file listing responses.</p> <p>Dependencies: 1e (ResourceRef)</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_discovery.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-3-access-enforcement-3-hours","title":"Phase 3: Access Enforcement (3 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#3a-create-access-context-injection","title":"3a. Create Access Context Injection","text":"<p>File: <code>odibi_mcp/access/context.py</code></p> <p>Task: Create context manager for injecting AccessContext into layers.</p> <p>Dependencies: 1g</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_access_injection.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#3b-add-project-scoping-to-catalogmanager","title":"3b. Add Project Scoping to CatalogManager","text":"<p>File: <code>odibi/catalog.py</code> (modify)</p> <p>Task: Add <code>set_access_context()</code> and <code>_apply_project_scope()</code> methods.</p> <p>Dependencies: 3a</p> <p>Verification: <pre><code>pytest tests/unit/test_catalog.py::test_project_scoping -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#3c-add-access-checks-to-storyloader","title":"3c. Add Access Checks to StoryLoader","text":"<p>File: <code>odibi_mcp/loaders/story.py</code></p> <p>Task: Create StoryLoader with project access validation.</p> <p>Dependencies: 3a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_story_loader.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#3d-create-path-validation-for-discovery","title":"3d. Create Path Validation for Discovery","text":"<p>File: <code>odibi_mcp/access/path_validator.py</code></p> <p>Task: Implement path prefix checking with deny-by-default.</p> <p>Dependencies: 1f</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_path_validator.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#3e-create-physical-ref-gate","title":"3e. Create Physical Ref Gate","text":"<p>File: <code>odibi_mcp/access/physical_gate.py</code></p> <p>Task: Implement 3-gate physical ref resolution.</p> <p>Dependencies: 1e, 1g</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_physical_gate.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-4-audit-logger-1-hour","title":"Phase 4: Audit Logger (1 hour)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#4a-create-auditentry-model","title":"4a. Create AuditEntry Model","text":"<p>File: <code>odibi_mcp/audit/entry.py</code></p> <p>Task: Create AuditEntry dataclass with all required fields.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>python -c \"from odibi_mcp.audit.entry import AuditEntry; print(AuditEntry.__annotations__)\"\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#4b-create-auditlogger","title":"4b. Create AuditLogger","text":"<p>File: <code>odibi_mcp/audit/logger.py</code></p> <p>Task: Create AuditLogger with structured logging and arg redaction.</p> <p>Dependencies: 4a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_audit.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#4c-integrate-audit-logger-with-server","title":"4c. Integrate Audit Logger with Server","text":"<p>File: <code>odibi_mcp/server.py</code> (modify)</p> <p>Task: Add audit logging to tool dispatch.</p> <p>Dependencies: 4b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_server_audit.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-5-story-tools-3-hours","title":"Phase 5: Story Tools (3 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#5a-create-story_read-tool","title":"5a. Create story_read Tool","text":"<p>File: <code>odibi_mcp/tools/story.py</code></p> <p>Task: Implement story_read with RunSelector support.</p> <p>Dependencies: 1c, 1d, 2d, 2e, 3c</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_story.py::test_story_read -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#5b-create-story_diff-tool","title":"5b. Create story_diff Tool","text":"<p>File: <code>odibi_mcp/tools/story.py</code> (append)</p> <p>Task: Implement story_diff with DiffSummary response.</p> <p>Dependencies: 5a, 2f</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_story.py::test_story_diff -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#5c-create-node_describe-tool","title":"5c. Create node_describe Tool","text":"<p>File: <code>odibi_mcp/tools/story.py</code> (append)</p> <p>Task: Implement node_describe with schema and validation info.</p> <p>Dependencies: 5a, 2b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_story.py::test_node_describe -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-6-sample-tools-2-hours","title":"Phase 6: Sample Tools (2 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#6a-create-sample-limiter","title":"6a. Create Sample Limiter","text":"<p>File: <code>odibi_mcp/utils/limiter.py</code></p> <p>Task: Implement limit_sample with row/column/cell caps.</p> <p>Dependencies: 1a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_limiter.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#6b-create-node_sample-tool","title":"6b. Create node_sample Tool","text":"<p>File: <code>odibi_mcp/tools/sample.py</code></p> <p>Task: Implement node_sample with limiting and RunSelector.</p> <p>Dependencies: 6a, 1d, 3c</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_sample.py::test_node_sample -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#6c-create-node_sample_in-tool","title":"6c. Create node_sample_in Tool","text":"<p>File: <code>odibi_mcp/tools/sample.py</code> (append)</p> <p>Task: Implement node_sample_in for input samples.</p> <p>Dependencies: 6b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_sample.py::test_node_sample_in -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#6d-create-node_failed_rows-tool","title":"6d. Create node_failed_rows Tool","text":"<p>File: <code>odibi_mcp/tools/sample.py</code> (append)</p> <p>Task: Implement node_failed_rows with validation filter.</p> <p>Dependencies: 6b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_sample.py::test_node_failed_rows -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-7-catalog-tools-3-hours","title":"Phase 7: Catalog Tools (3 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#7a-create-node_stats-tool","title":"7a. Create node_stats Tool","text":"<p>File: <code>odibi_mcp/tools/catalog.py</code></p> <p>Task: Implement node_stats with TimeWindow and typed response.</p> <p>Dependencies: 1h, 2g, 3b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_catalog.py::test_node_stats -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#7b-create-pipeline_stats-tool","title":"7b. Create pipeline_stats Tool","text":"<p>File: <code>odibi_mcp/tools/catalog.py</code> (append)</p> <p>Task: Implement pipeline_stats with TimeWindow.</p> <p>Dependencies: 7a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_catalog.py::test_pipeline_stats -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#7c-create-failure_summary-tool","title":"7c. Create failure_summary Tool","text":"<p>File: <code>odibi_mcp/tools/catalog.py</code> (append)</p> <p>Task: Implement failure_summary with optional pipeline filter.</p> <p>Dependencies: 7a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_catalog.py::test_failure_summary -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#7d-create-schema_history-tool","title":"7d. Create schema_history Tool","text":"<p>File: <code>odibi_mcp/tools/catalog.py</code> (append)</p> <p>Task: Implement schema_history with SchemaChange list.</p> <p>Dependencies: 2e, 3b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_catalog.py::test_schema_history -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-8-lineage-tools-2-hours","title":"Phase 8: Lineage Tools (2 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#8a-create-lineage_upstream-tool","title":"8a. Create lineage_upstream Tool","text":"<p>File: <code>odibi_mcp/tools/lineage.py</code></p> <p>Task: Implement lineage_upstream with ResourceRef responses.</p> <p>Dependencies: 1e, 3e</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_lineage.py::test_lineage_upstream -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#8b-create-lineage_downstream-tool","title":"8b. Create lineage_downstream Tool","text":"<p>File: <code>odibi_mcp/tools/lineage.py</code> (append)</p> <p>Task: Implement lineage_downstream with ResourceRef responses.</p> <p>Dependencies: 8a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_lineage.py::test_lineage_downstream -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#8c-create-lineage_graph-tool","title":"8c. Create lineage_graph Tool","text":"<p>File: <code>odibi_mcp/tools/lineage.py</code> (append)</p> <p>Task: Implement lineage_graph with GraphData response.</p> <p>Dependencies: 2d, 8a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_lineage.py::test_lineage_graph -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-9-schema-tools-2-hours","title":"Phase 9: Schema Tools (2 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#9a-create-output_schema-tool","title":"9a. Create output_schema Tool","text":"<p>File: <code>odibi_mcp/tools/schema.py</code></p> <p>Task: Implement output_schema with SchemaResponse.</p> <p>Dependencies: 2b, 1d, 3b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_schema.py::test_output_schema -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#9b-create-list_outputs-tool","title":"9b. Create list_outputs Tool","text":"<p>File: <code>odibi_mcp/tools/schema.py</code> (append)</p> <p>Task: Implement list_outputs with ResourceRef list and physical gating.</p> <p>Dependencies: 9a, 3e</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_schema.py::test_list_outputs -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-10-source-discovery-tools-5-hours","title":"Phase 10: Source Discovery Tools (5 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#10a-create-discovery-limits-config","title":"10a. Create Discovery Limits Config","text":"<p>File: <code>odibi_mcp/discovery/limits.py</code></p> <p>Task: Create DiscoveryLimits with hard caps.</p> <p>Dependencies: None</p> <p>Verification: <pre><code>python -c \"from odibi_mcp.discovery.limits import DiscoveryLimits; print(DiscoveryLimits().max_files_per_call)\"\n</code></pre></p> <p>Expected: <code>100</code></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#10b-create-list_files-tool","title":"10b. Create list_files Tool","text":"<p>File: <code>odibi_mcp/tools/discovery.py</code></p> <p>Task: Implement list_files with pagination and path validation.</p> <p>Dependencies: 2h, 3d, 10a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_discovery.py::test_list_files -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#10c-create-list_tables-tool","title":"10c. Create list_tables Tool","text":"<p>File: <code>odibi_mcp/tools/discovery.py</code> (append)</p> <p>Task: Implement list_tables for SQL connections.</p> <p>Dependencies: 10b</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_discovery.py::test_list_tables -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#10d-create-infer_schema-tool","title":"10d. Create infer_schema Tool","text":"<p>File: <code>odibi_mcp/tools/discovery.py</code> (append)</p> <p>Task: Implement infer_schema with byte/row limits.</p> <p>Dependencies: 2b, 10a, 3d</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_discovery.py::test_infer_schema -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#10e-create-describe_table-tool","title":"10e. Create describe_table Tool","text":"<p>File: <code>odibi_mcp/tools/discovery.py</code> (append)</p> <p>Task: Implement describe_table for SQL connections.</p> <p>Dependencies: 2b, 10c</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_discovery.py::test_describe_table -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#10f-create-preview_source-tool","title":"10f. Create preview_source Tool","text":"<p>File: <code>odibi_mcp/tools/discovery.py</code> (append)</p> <p>Task: Implement preview_source with column projection and limits.</p> <p>Dependencies: 6a, 10d</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_tools_discovery.py::test_preview_source -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-11-error-handling-1-hour","title":"Phase 11: Error Handling (1 hour)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#11a-create-error-response-helper","title":"11a. Create Error Response Helper","text":"<p>File: <code>odibi_mcp/utils/errors.py</code></p> <p>Task: Create _error_response helper with envelope wrapping.</p> <p>Dependencies: 1c</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_errors.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#11b-integrate-error-handling-in-server","title":"11b. Integrate Error Handling in Server","text":"<p>File: <code>odibi_mcp/server.py</code> (modify)</p> <p>Task: Add try/except with typed error responses.</p> <p>Dependencies: 11a</p> <p>Verification: <pre><code>pytest tests/unit/mcp/test_server_errors.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#phase-12-integration-tests-6-hours","title":"Phase 12: Integration Tests (6 hours)","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#12a-create-mock-story-fixtures","title":"12a. Create Mock Story Fixtures","text":"<p>File: <code>tests/fixtures/mcp_stories.py</code></p> <p>Task: Create realistic Story fixtures for testing.</p> <p>Dependencies: All Phase 5</p> <p>Verification: <pre><code>python -c \"from tests.fixtures.mcp_stories import get_mock_story; print(get_mock_story().pipeline_name)\"\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#12b-create-mock-catalog-fixtures","title":"12b. Create Mock Catalog Fixtures","text":"<p>File: <code>tests/fixtures/mcp_catalog.py</code></p> <p>Task: Create Catalog data fixtures.</p> <p>Dependencies: All Phase 7</p> <p>Verification: <pre><code>python -c \"from tests.fixtures.mcp_catalog import get_mock_node_runs; print(len(get_mock_node_runs()))\"\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#12c-integration-test-story-tools-end-to-end","title":"12c. Integration Test: Story Tools End-to-End","text":"<p>File: <code>tests/integration/mcp/test_story_e2e.py</code></p> <p>Task: Test story_read \u2192 node_describe \u2192 node_sample flow.</p> <p>Dependencies: 12a, Phase 5, Phase 6</p> <p>Verification: <pre><code>pytest tests/integration/mcp/test_story_e2e.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#12d-integration-test-discovery-tools-end-to-end","title":"12d. Integration Test: Discovery Tools End-to-End","text":"<p>File: <code>tests/integration/mcp/test_discovery_e2e.py</code></p> <p>Task: Test list_files \u2192 infer_schema \u2192 preview_source flow.</p> <p>Dependencies: 12b, Phase 10</p> <p>Verification: <pre><code>pytest tests/integration/mcp/test_discovery_e2e.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#12e-integration-test-access-enforcement","title":"12e. Integration Test: Access Enforcement","text":"<p>File: <code>tests/integration/mcp/test_access_e2e.py</code></p> <p>Task: Test project scoping, path denial, physical ref gating.</p> <p>Dependencies: Phase 3</p> <p>Verification: <pre><code>pytest tests/integration/mcp/test_access_e2e.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#12f-integration-test-full-ai-workflow","title":"12f. Integration Test: Full AI Workflow","text":"<p>File: <code>tests/integration/mcp/test_ai_workflow.py</code></p> <p>Task: Simulate: list_files \u2192 infer_schema \u2192 suggest_pattern \u2192 generate_yaml \u2192 validate_yaml</p> <p>Dependencies: All phases</p> <p>Verification: <pre><code>pytest tests/integration/mcp/test_ai_workflow.py -v\n</code></pre></p>"},{"location":"mcp/IMPLEMENTATION_PLAN/#post-implementation","title":"Post-Implementation","text":""},{"location":"mcp/IMPLEMENTATION_PLAN/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>[ ] Update <code>odibi_mcp/README.md</code> with new tools</li> <li>[ ] Add examples to <code>docs/mcp/examples/</code></li> <li>[ ] Update <code>CHANGELOG.md</code> with v2.11.0 MCP enhancements</li> </ul>"},{"location":"mcp/IMPLEMENTATION_PLAN/#configuration-template","title":"Configuration Template","text":"<ul> <li>[ ] Create <code>mcp_config.example.yaml</code> with all options documented</li> <li>[ ] Add to <code>odibi init</code> scaffolding</li> </ul>"},{"location":"mcp/IMPLEMENTATION_PLAN/#quick-reference-file-structure","title":"Quick Reference: File Structure","text":"<pre><code>odibi_mcp/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 server.py                    # Main MCP server (existing, modified)\n\u251c\u2500\u2500 contracts/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 enums.py                 # TruncatedReason\n\u2502   \u251c\u2500\u2500 envelope.py              # PolicyApplied, MCPEnvelope, Success/Error\n\u2502   \u251c\u2500\u2500 selectors.py             # RunSelector, RunById\n\u2502   \u251c\u2500\u2500 resources.py             # ResourceRef\n\u2502   \u251c\u2500\u2500 access.py                # ConnectionPolicy, AccessContext\n\u2502   \u251c\u2500\u2500 time.py                  # TimeWindow\n\u2502   \u251c\u2500\u2500 schema.py                # ColumnSpec, SchemaResponse, SchemaChange\n\u2502   \u251c\u2500\u2500 graph.py                 # GraphNode, GraphEdge, GraphData\n\u2502   \u251c\u2500\u2500 diff.py                  # DiffSummary\n\u2502   \u251c\u2500\u2500 stats.py                 # StatPoint, NodeStatsResponse\n\u2502   \u2514\u2500\u2500 discovery.py             # FileInfo, ListFilesResponse\n\u251c\u2500\u2500 access/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 context.py               # Context injection\n\u2502   \u251c\u2500\u2500 path_validator.py        # Path prefix checking\n\u2502   \u2514\u2500\u2500 physical_gate.py         # 3-gate physical ref\n\u251c\u2500\u2500 audit/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 entry.py                 # AuditEntry\n\u2502   \u2514\u2500\u2500 logger.py                # AuditLogger\n\u251c\u2500\u2500 loaders/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 story.py                 # StoryLoader\n\u251c\u2500\u2500 discovery/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 limits.py                # DiscoveryLimits\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 story.py                 # story_read, story_diff, node_describe\n\u2502   \u251c\u2500\u2500 sample.py                # node_sample, node_sample_in, node_failed_rows\n\u2502   \u251c\u2500\u2500 catalog.py               # node_stats, pipeline_stats, failure_summary, schema_history\n\u2502   \u251c\u2500\u2500 lineage.py               # lineage_upstream, lineage_downstream, lineage_graph\n\u2502   \u251c\u2500\u2500 schema.py                # output_schema, list_outputs\n\u2502   \u2514\u2500\u2500 discovery.py             # list_files, list_tables, infer_schema, describe_table, preview_source\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 limiter.py               # Sample limiting\n    \u2514\u2500\u2500 errors.py                # Error response helpers\n\ntests/unit/mcp/\n\u251c\u2500\u2500 test_envelope.py\n\u251c\u2500\u2500 test_resources.py\n\u251c\u2500\u2500 test_access.py\n\u251c\u2500\u2500 test_time.py\n\u251c\u2500\u2500 test_schema.py\n\u251c\u2500\u2500 test_graph.py\n\u251c\u2500\u2500 test_diff.py\n\u251c\u2500\u2500 test_stats.py\n\u251c\u2500\u2500 test_discovery.py\n\u251c\u2500\u2500 test_access_injection.py\n\u251c\u2500\u2500 test_story_loader.py\n\u251c\u2500\u2500 test_path_validator.py\n\u251c\u2500\u2500 test_physical_gate.py\n\u251c\u2500\u2500 test_audit.py\n\u251c\u2500\u2500 test_server_audit.py\n\u251c\u2500\u2500 test_limiter.py\n\u251c\u2500\u2500 test_tools_story.py\n\u251c\u2500\u2500 test_tools_sample.py\n\u251c\u2500\u2500 test_tools_catalog.py\n\u251c\u2500\u2500 test_tools_lineage.py\n\u251c\u2500\u2500 test_tools_schema.py\n\u251c\u2500\u2500 test_tools_discovery.py\n\u251c\u2500\u2500 test_errors.py\n\u2514\u2500\u2500 test_server_errors.py\n\ntests/integration/mcp/\n\u251c\u2500\u2500 test_story_e2e.py\n\u251c\u2500\u2500 test_discovery_e2e.py\n\u251c\u2500\u2500 test_access_e2e.py\n\u2514\u2500\u2500 test_ai_workflow.py\n\ntests/fixtures/\n\u251c\u2500\u2500 mcp_stories.py\n\u2514\u2500\u2500 mcp_catalog.py\n</code></pre>"},{"location":"mcp/SPEC/","title":"Odibi MCP Facade \u2014 Complete Implementation Plan (v4.1)","text":"<p>Revision Notes: Final enterprise-grade spec with polish pass. Key changes from v3: unified AccessContext, physical ref gate enforced everywhere, deny-by-default discovery, universal RunSelector invariant, complete TimeWindow validation, typed lists replace parallel arrays. v4.1 adds: explicit read-only invariant, data source precedence, truncated_reason enum, single-project rule.</p>"},{"location":"mcp/SPEC/#overview","title":"Overview","text":"<p>A thin, read-only MCP layer over Odibi's existing manager, exposing Stories, Catalog, Lineage, and Source Discovery for AI consumption. YAML-first is preserved\u2014AI accelerates pipeline creation but you own the source of truth.</p> <p>Design Principles: - No re-resolution of configs or credentials - Read-only (no pipeline execution, no writes) - Unified access enforcement via AccessContext - Credentials stay server-side - Samples respect PII/privacy configuration - Logical references by default, physical paths require explicit gate - Explicit run selection on all stateful reads - Strongly typed responses (no parallel arrays) - Deny-by-default for path-based discovery - Single-project context per request</p> <p>Foundational Invariants:</p> <p>READ-ONLY INVARIANT: The MCP facade MUST NOT mutate data, trigger pipeline runs, or alter any state. It is a read-only projection over resolved Odibi artifacts.</p> <p>SINGLE-PROJECT INVARIANT: MCP tools MUST operate within a single project context per request. Cross-project joins, queries, or aggregations are explicitly forbidden to prevent payload explosion, lineage confusion, and isolation violations.</p> <p>Data Source Precedence:</p> <p>When resolving data for a request, tools MUST follow this precedence order: 1. Story artifacts \u2014 If present and complete for the selected run 2. Manager-resolved metadata \u2014 Pipeline/node config as resolved by PipelineManager 3. Materialized output reads \u2014 Only when explicitly requested AND no story/metadata available</p> <p>This ensures consistency and minimizes unnecessary reads from storage.</p>"},{"location":"mcp/SPEC/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        AI Client                            \u2502\n\u2502         \"Build me a bronze pipeline for sales data\"         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 MCP Protocol\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Odibi MCP Server                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Response Envelope: request_id, tool, project, timing  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  AccessContext: unified enforcement across all layers  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Story   \u2502 \u2502 Catalog \u2502 \u2502 Lineage \u2502 \u2502 Schema  \u2502 \u2502Source \u2502 \u2502\n\u2502  \u2502 Tools   \u2502 \u2502 Tools   \u2502 \u2502 Tools   \u2502 \u2502 Tools   \u2502 \u2502Discov.\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502          \u2502          \u2502          \u2502           \u2502       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Audit Logger (all tool calls)           \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Existing Odibi Core                       \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502StoryMetadata\u2502 \u2502CatalogManager\u2502 \u2502 Connections \u2502 \u2502 Engine \u2502 \u2502\n\u2502 \u2502DocGenerator \u2502 \u2502              \u2502 \u2502 (resolved)  \u2502 \u2502(read)  \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"mcp/SPEC/#core-contracts","title":"Core Contracts","text":""},{"location":"mcp/SPEC/#unified-access-context","title":"Unified Access Context","text":"<p>Single context injected once, enforced by every layer (Stories, Catalog, Lineage, Discovery):</p> <pre><code>from pydantic import BaseModel, Field, validator\nfrom typing import Set, Dict, List, Optional\n\nclass ConnectionPolicy(BaseModel):\n    \"\"\"Per-connection access policy. Deny-by-default for path discovery.\"\"\"\n    connection: str\n\n    # Path-based discovery: DENY by default\n    # Must have at least one allowed prefix OR explicit_allow_all=True\n    allowed_path_prefixes: List[str] = Field(default_factory=list)\n    denied_path_prefixes: List[str] = Field(default_factory=list)\n    explicit_allow_all: bool = False  # Must be explicitly set to allow all paths\n\n    # Limits\n    max_depth: int = 5\n\n    # Physical ref policy\n    allow_physical_refs: bool = False  # Must be explicitly enabled\n\n    @validator(\"allowed_path_prefixes\", always=True)\n    def validate_path_access(cls, v, values):\n        \"\"\"Enforce deny-by-default: require explicit allowlist or explicit_allow_all.\"\"\"\n        if not v and not values.get(\"explicit_allow_all\", False):\n            # Empty allowed list with no explicit_allow_all means deny all paths\n            pass  # Valid state - will deny all path-based discovery\n        return v\n\n    def is_path_allowed(self, path: str) -&gt; bool:\n        \"\"\"Check if path is allowed under this policy.\"\"\"\n        # Check denied first\n        if any(path.startswith(prefix) for prefix in self.denied_path_prefixes):\n            return False\n\n        # If explicit_allow_all, allow anything not denied\n        if self.explicit_allow_all:\n            return True\n\n        # Otherwise must match an allowed prefix\n        if not self.allowed_path_prefixes:\n            return False  # Deny by default\n\n        return any(path.startswith(prefix) for prefix in self.allowed_path_prefixes)\n\n\nclass AccessContext(BaseModel):\n    \"\"\"\n    Unified access enforcement context.\n    Injected once at MCP server init, enforced across all layers.\n    \"\"\"\n    # Project scoping\n    authorized_projects: Set[str]\n\n    # Environment (for env-specific access rules)\n    environment: str = \"production\"  # dev, test, production\n\n    # Connection policies (deny-by-default)\n    connection_policies: Dict[str, ConnectionPolicy] = Field(default_factory=dict)\n\n    # Global physical ref policy\n    physical_refs_enabled: bool = False  # Master switch\n\n    def check_project(self, project: str) -&gt; None:\n        \"\"\"Raise PermissionError if project not authorized.\"\"\"\n        if project not in self.authorized_projects:\n            raise PermissionError(f\"Access denied: project '{project}' not authorized\")\n\n    def check_connection(self, connection: str) -&gt; ConnectionPolicy:\n        \"\"\"Get connection policy, raise if not configured.\"\"\"\n        if connection not in self.connection_policies:\n            raise PermissionError(f\"Access denied: connection '{connection}' not configured\")\n        return self.connection_policies[connection]\n\n    def check_path(self, connection: str, path: str) -&gt; None:\n        \"\"\"Check path against connection policy.\"\"\"\n        policy = self.check_connection(connection)\n        if not policy.is_path_allowed(path):\n            raise PermissionError(f\"Access denied: path '{path}' not allowed for connection '{connection}'\")\n\n    def can_include_physical(self, connection: str) -&gt; bool:\n        \"\"\"Check if physical refs can be included for this connection.\"\"\"\n        if not self.physical_refs_enabled:\n            return False\n        policy = self.connection_policies.get(connection)\n        return policy is not None and policy.allow_physical_refs\n</code></pre>"},{"location":"mcp/SPEC/#response-envelope","title":"Response Envelope","text":"<p>Every MCP response is wrapped in a typed envelope:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional, List, Dict, Any, Literal\nfrom datetime import datetime\nfrom uuid import uuid4\nfrom odibi import __version__\n\nclass PolicyApplied(BaseModel):\n    \"\"\"Record of policies applied to this response.\"\"\"\n    project_scoped: bool = True\n    connection_policy: Optional[str] = None\n    path_filtered: bool = False\n    sample_capped: bool = False\n    physical_ref_allowed: bool = False  # Explicit indicator\n    physical_ref_included: bool = False  # Whether it was actually included\n\nclass MCPEnvelope(BaseModel):\n    \"\"\"Standard envelope for all MCP responses.\"\"\"\n    request_id: str = Field(default_factory=lambda: str(uuid4()))\n    tool_name: str\n    mcp_schema_version: str = __version__\n    project: str\n    environment: str = \"production\"\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    duration_ms: Optional[float] = None\n    truncated: bool = False\n    truncated_reason: Optional[TruncatedReason] = None\n    policy_applied: PolicyApplied = Field(default_factory=PolicyApplied)\n\nclass TruncatedReason(str, Enum):\n    \"\"\"Explicit reasons for response truncation.\"\"\"\n    ROW_LIMIT = \"row_limit\"           # Hit max rows cap\n    COLUMN_LIMIT = \"column_limit\"     # Hit max columns cap\n    BYTE_LIMIT = \"byte_limit\"         # Hit max bytes for schema inference\n    CELL_LIMIT = \"cell_limit\"         # Cell content truncated\n    POLICY_MASKING = \"policy_masking\" # Content masked by policy\n    SAMPLING_ONLY = \"sampling_only\"   # Only sample returned, full data available\n    PAGINATION = \"pagination\"         # More results available via next_token\n\nclass MCPErrorEnvelope(MCPEnvelope):\n    \"\"\"Error response envelope.\"\"\"\n    success: Literal[False] = False\n    error: str\n    error_type: str  # validation_failed, not_found, permission_denied, connection_error\n    retryable: bool = False\n    details: Optional[List[Dict[str, Any]]] = None\n\nclass MCPSuccessEnvelope(MCPEnvelope):\n    \"\"\"Success response envelope with typed payload.\"\"\"\n    success: Literal[True] = True\n    data: Any  # Typed per-tool response\n</code></pre>"},{"location":"mcp/SPEC/#run-selector-universal-invariant","title":"Run Selector (Universal Invariant)","text":"<p>INVARIANT: All tools that read node materializations, story artifacts, or any stateful data MUST accept RunSelector.</p> <pre><code>from typing import Literal, Union\n\nclass RunById(BaseModel):\n    run_id: str\n\nRunSelector = Union[\n    Literal[\"latest_successful\"],\n    Literal[\"latest_attempt\"],\n    RunById\n]\n\n# Default: latest_successful\nDEFAULT_RUN_SELECTOR: RunSelector = \"latest_successful\"\n\n# Tools that MUST accept RunSelector:\n# - story_read, story_diff, node_describe\n# - node_sample, node_sample_in, node_failed_rows\n# - output_schema, list_outputs\n# - lineage_graph\n# - Any future tool reading materialized state\n</code></pre>"},{"location":"mcp/SPEC/#resource-reference-physical-gate-enforced","title":"Resource Reference (Physical Gate Enforced)","text":"<p>Physical refs require: <code>include_physical=True</code> AND <code>policy.allow_physical_refs=True</code> AND <code>access_context.physical_refs_enabled=True</code></p> <pre><code>class ResourceRef(BaseModel):\n    \"\"\"Logical reference to a data resource. Physical ref gated by policy.\"\"\"\n    kind: Literal[\"delta_table\", \"delta_path\", \"sql_table\", \"file\", \"directory\"]\n    logical_name: str  # e.g., \"sales_bronze.clean_orders\"\n    connection: str    # logical connection name\n\n    # Physical ref: ONLY included when ALL gates pass\n    # Gate 1: Caller passes include_physical=True\n    # Gate 2: ConnectionPolicy.allow_physical_refs=True\n    # Gate 3: AccessContext.physical_refs_enabled=True\n    physical_ref: Optional[str] = None  # Guaranteed None unless all gates pass\n\ndef resolve_resource_ref(\n    logical_name: str,\n    connection: str,\n    kind: str,\n    physical_path: str,\n    include_physical: bool,\n    access_context: AccessContext,\n) -&gt; ResourceRef:\n    \"\"\"\n    Resolve a resource reference with physical ref gating.\n    \"\"\"\n    # Check all three gates\n    can_include = (\n        include_physical and\n        access_context.physical_refs_enabled and\n        access_context.can_include_physical(connection)\n    )\n\n    return ResourceRef(\n        kind=kind,\n        logical_name=logical_name,\n        connection=connection,\n        physical_ref=physical_path if can_include else None,\n    )\n</code></pre>"},{"location":"mcp/SPEC/#time-window-complete-validation","title":"Time Window (Complete Validation)","text":"<pre><code>from pydantic import BaseModel, Field, root_validator\nfrom datetime import datetime\nfrom typing import Optional\n\nclass TimeWindow(BaseModel):\n    \"\"\"\n    Explicit time semantics for catalog queries.\n    At least one of (days) or (start AND end) must be provided.\n    \"\"\"\n    days: Optional[int] = Field(default=None, ge=1, le=365)\n    start: Optional[datetime] = None  # Inclusive\n    end: Optional[datetime] = None    # Exclusive\n    timezone: str = \"UTC\"\n\n    @root_validator\n    def validate_window(cls, values):\n        days = values.get(\"days\")\n        start = values.get(\"start\")\n        end = values.get(\"end\")\n\n        # Check: cannot mix days with start/end\n        if days is not None and (start is not None or end is not None):\n            raise ValueError(\"Cannot specify both 'days' and 'start/end'\")\n\n        # Check: must have at least one\n        if days is None and start is None and end is None:\n            # Default to 7 days\n            values[\"days\"] = 7\n\n        # Check: if start/end provided, both must be present\n        if (start is None) != (end is None):\n            raise ValueError(\"Both 'start' and 'end' must be provided together\")\n\n        # Check: start &lt; end\n        if start is not None and end is not None:\n            if start &gt;= end:\n                raise ValueError(\"'start' must be before 'end'\")\n\n        return values\n\n    def to_range(self) -&gt; tuple[datetime, datetime]:\n        \"\"\"Convert to (start, end) tuple in UTC.\"\"\"\n        from datetime import timedelta\n        import pytz\n\n        tz = pytz.timezone(self.timezone)\n        now = datetime.now(tz)\n\n        if self.days is not None:\n            end = now\n            start = now - timedelta(days=self.days)\n        else:\n            start = self.start.astimezone(pytz.UTC) if self.start.tzinfo else tz.localize(self.start).astimezone(pytz.UTC)\n            end = self.end.astimezone(pytz.UTC) if self.end.tzinfo else tz.localize(self.end).astimezone(pytz.UTC)\n\n        return (start, end)\n</code></pre>"},{"location":"mcp/SPEC/#typed-response-models-no-parallel-arrays","title":"Typed Response Models (No Parallel Arrays)","text":""},{"location":"mcp/SPEC/#column-specification","title":"Column Specification","text":"<pre><code>class ColumnSpec(BaseModel):\n    \"\"\"Typed column specification. Replaces parallel arrays.\"\"\"\n    name: str\n    dtype: str\n    nullable: bool = True\n    description: Optional[str] = None\n    semantic_type: Optional[str] = None  # \"pii\", \"metric\", \"dimension\", \"key\"\n    sample_values: Optional[List[Any]] = None  # Max 3 examples\n</code></pre>"},{"location":"mcp/SPEC/#schema-response","title":"Schema Response","text":"<pre><code>class SchemaResponse(BaseModel):\n    \"\"\"Typed schema response. No parallel arrays.\"\"\"\n    columns: List[ColumnSpec]\n    row_count: Optional[int] = None\n    partition_columns: List[str] = Field(default_factory=list)\n</code></pre>"},{"location":"mcp/SPEC/#graph-data","title":"Graph Data","text":"<pre><code>class GraphNode(BaseModel):\n    id: str\n    type: Literal[\"source\", \"transform\", \"sink\"]\n    label: str\n    layer: Optional[str] = None  # bronze, silver, gold\n    status: Optional[str] = None  # success, failed, skipped\n\nclass GraphEdge(BaseModel):\n    from_node: str\n    to_node: str\n    edge_type: Literal[\"data_flow\", \"dependency\"] = \"data_flow\"\n\nclass GraphData(BaseModel):\n    nodes: List[GraphNode]\n    edges: List[GraphEdge]\n</code></pre>"},{"location":"mcp/SPEC/#schema-change","title":"Schema Change","text":"<pre><code>class ColumnChange(BaseModel):\n    name: str\n    old_type: str\n    new_type: str\n\nclass SchemaChange(BaseModel):\n    run_id: str\n    timestamp: datetime\n    added: List[ColumnSpec] = Field(default_factory=list)\n    removed: List[str] = Field(default_factory=list)\n    type_changed: List[ColumnChange] = Field(default_factory=list)\n</code></pre>"},{"location":"mcp/SPEC/#diff-summary","title":"Diff Summary","text":"<pre><code>class DiffSummary(BaseModel):\n    \"\"\"Schema and row-level diff without exposing raw values by default.\"\"\"\n    schema_diff: SchemaChange\n    row_count_diff: int  # run_b - run_a\n    null_count_changes: Dict[str, int]  # column -&gt; delta\n    distinct_count_changes: Dict[str, int]  # column -&gt; delta\n\n    # Raw sample diffs: ONLY when explicitly requested AND permitted\n    sample_diff_included: bool = False\n    sample_diff: Optional[List[Dict[str, Any]]] = None\n</code></pre>"},{"location":"mcp/SPEC/#node-stats","title":"Node Stats","text":"<pre><code>class StatPoint(BaseModel):\n    \"\"\"Single data point in a time series.\"\"\"\n    timestamp: datetime\n    value: float\n\nclass NodeStatsResponse(BaseModel):\n    \"\"\"Typed node statistics. No parallel arrays.\"\"\"\n    pipeline: str\n    node: str\n    window: TimeWindow\n    run_count: int\n    success_count: int\n    failure_count: int\n    failure_rate: float\n    avg_duration_seconds: float\n    duration_trend: List[StatPoint]\n    row_count_trend: List[StatPoint]\n</code></pre>"},{"location":"mcp/SPEC/#file-listing","title":"File Listing","text":"<pre><code>class FileInfo(BaseModel):\n    \"\"\"Typed file info. Replaces Dict[str, Any].\"\"\"\n    logical_name: str\n    size_bytes: int\n    modified: datetime\n    is_directory: bool = False\n    # Physical path: gated by policy\n    physical_path: Optional[str] = None\n\nclass ListFilesResponse(BaseModel):\n    \"\"\"Paginated file listing.\"\"\"\n    connection: str\n    path: str\n    files: List[FileInfo]\n    next_token: Optional[str] = None\n    truncated: bool = False\n    total_count: Optional[int] = None\n</code></pre>"},{"location":"mcp/SPEC/#tool-categories","title":"Tool Categories","text":""},{"location":"mcp/SPEC/#universal-invariants","title":"Universal Invariants","text":"<ol> <li>RunSelector Required: All tools reading materializations or artifacts accept <code>run_selector: RunSelector = \"latest_successful\"</code></li> <li>Physical Refs Gated: All tools returning ResourceRef accept <code>include_physical: bool = False</code></li> <li>AccessContext Enforced: All tools validate against the unified AccessContext</li> </ol>"},{"location":"mcp/SPEC/#1-project-tools","title":"1. Project Tools","text":"Tool Parameters Returns <code>project_scan</code> - pipelines[], last_run, resolution_status <code>project_health</code> <code>window: TimeWindow = {days: 7}</code> success_rate, failures, anomalies"},{"location":"mcp/SPEC/#2-story-tools-observe-runs","title":"2. Story Tools (observe runs)","text":"Tool Parameters Returns <code>story_read</code> <code>pipeline, run_selector = \"latest_successful\"</code> nodes[], status, duration, schema_changes: List[SchemaChange], graph_data: GraphData <code>story_diff</code> <code>pipeline, run_a: RunSelector, run_b: RunSelector, include_sample_diff: bool = False</code> DiffSummary <code>node_describe</code> <code>pipeline, node, run_selector = \"latest_successful\"</code> operation, validations, schema: SchemaResponse, duration"},{"location":"mcp/SPEC/#3-sample-tools-explicit-limited","title":"3. Sample Tools (explicit, limited)","text":"Tool Parameters Returns <code>node_sample</code> <code>pipeline, node, run_selector, limit=10, columns?: List[str]</code> rows[], truncated, truncated_reason, total_rows <code>node_sample_in</code> <code>pipeline, node, run_selector, limit=10, columns?: List[str]</code> rows[], truncated, truncated_reason, total_rows <code>node_failed_rows</code> <code>pipeline, node, validation, run_selector, limit=5</code> rows[], validation_name, fail_count"},{"location":"mcp/SPEC/#4-catalog-tools-historical-queries","title":"4. Catalog Tools (historical queries)","text":"Tool Parameters Returns <code>node_stats</code> <code>pipeline, node, window: TimeWindow</code> NodeStatsResponse <code>pipeline_stats</code> <code>pipeline, window: TimeWindow</code> PipelineStatsResponse <code>failure_summary</code> <code>pipeline?, window: TimeWindow</code> FailureSummaryResponse <code>schema_history</code> <code>pipeline, node, run_selector, limit=10</code> List[SchemaChange]"},{"location":"mcp/SPEC/#5-lineage-tools","title":"5. Lineage Tools","text":"Tool Parameters Returns <code>lineage_upstream</code> <code>resource: ResourceRef, include_physical: bool = False</code> upstream: List[ResourceRef], depth <code>lineage_downstream</code> <code>resource: ResourceRef, include_physical: bool = False</code> downstream: List[ResourceRef], depth <code>lineage_graph</code> <code>pipeline, run_selector, include_physical: bool = False</code> GraphData"},{"location":"mcp/SPEC/#6-schema-tools-for-building-downstream","title":"6. Schema Tools (for building downstream)","text":"Tool Parameters Returns <code>output_schema</code> <code>pipeline, node, run_selector</code> SchemaResponse <code>list_outputs</code> <code>project?, run_selector, include_physical: bool = False</code> List[ResourceRef]"},{"location":"mcp/SPEC/#7-source-discovery-tools-deny-by-default","title":"7. Source Discovery Tools (deny-by-default)","text":"<p>Policy: Path-based discovery is DENIED unless connection has <code>explicit_allow_all=True</code> or matching <code>allowed_path_prefixes</code>.</p> Tool Parameters Returns <code>list_files</code> <code>connection, path?, limit=100, next_token?, include_physical: bool = False</code> ListFilesResponse <code>list_schemas</code> <code>connection</code> ListSchemasResponse \u2014 List all schemas with table counts. Call FIRST before discover_database <code>list_tables</code> <code>connection, schema?, limit=100</code> ListTablesResponse <code>infer_schema</code> <code>connection, path, format, max_rows=100, max_bytes=1MB</code> SchemaResponse <code>describe_table</code> <code>connection, table</code> SchemaResponse <code>preview_source</code> <code>connection, path, format, limit=5, columns?: List[str]</code> PreviewResponse <code>discover_database</code> <code>connection, schema?, max_tables=20, sample_rows=0</code> DiscoverDatabaseResponse \u2014 Structure-first discovery <code>discover_storage</code> <code>connection, path?, pattern?, max_files=20, sample_rows=0, recursive=False</code> DiscoverStorageResponse \u2014 File discovery <code>debug_env</code> (none) DebugEnvResponse \u2014 Shows .env loading, env vars set, connection status <p>Discovery Limits:</p> <pre><code>class DiscoveryLimits(BaseModel):\n    \"\"\"Hard limits for source discovery operations.\"\"\"\n    max_files_per_call: int = 100\n    max_rows_for_schema: int = 100\n    max_bytes_for_schema: int = 1_048_576  # 1MB\n    max_preview_rows: int = 10\n    max_preview_columns: int = 15\n    max_cell_chars: int = 100\n</code></pre>"},{"location":"mcp/SPEC/#8-build-tools-existing-mcp-retained","title":"8. Build Tools (existing MCP, retained)","text":"Tool Description <code>suggest_pattern(use_case)</code> Recommend pattern for use case <code>list_transformers()</code> List all 52+ transformers <code>list_patterns()</code> List all 6 DWH patterns <code>list_connections()</code> List connection types <code>generate_pipeline_yaml(...)</code> Generate pipeline YAML <code>generate_transformer(...)</code> Generate custom transformer code <code>validate_yaml(yaml_content)</code> Validate YAML before save <code>diagnose_error(error_message)</code> Debug pipeline errors <code>get_example(pattern_name)</code> Get working example"},{"location":"mcp/SPEC/#authorization-implementation","title":"Authorization Implementation","text":""},{"location":"mcp/SPEC/#accesscontext-enforcement","title":"AccessContext Enforcement","text":"<pre><code>class OdibiMCPServer:\n    def __init__(\n        self,\n        catalog: CatalogManager,\n        story_loader: StoryLoader,\n        connection_resolver: ConnectionResolver,\n        engine: Engine,\n        access_context: AccessContext,\n    ):\n        self.catalog = catalog\n        self.story_loader = story_loader\n        self.connection_resolver = connection_resolver\n        self.engine = engine\n        self.access = access_context\n\n        # Inject access context into all layers\n        self.catalog.set_access_context(access_context)\n        self.story_loader.set_access_context(access_context)\n\n    async def call_tool(self, name: str, args: dict) -&gt; MCPEnvelope:\n        # AccessContext is enforced at each layer automatically\n        ...\n</code></pre>"},{"location":"mcp/SPEC/#layer-level-enforcement","title":"Layer-Level Enforcement","text":"<pre><code>class CatalogManager:\n    def set_access_context(self, ctx: AccessContext) -&gt; None:\n        self._access = ctx\n\n    def query_node_runs(self, pipeline: str, node: str, window: TimeWindow) -&gt; pd.DataFrame:\n        \"\"\"Query with automatic project scoping.\"\"\"\n        df = self._read_delta(\"meta_node_runs\")\n        df = self._apply_project_scope(df)  # Uses self._access\n        df = self._apply_time_window(df, window)\n        return df.query(f\"pipeline_name == '{pipeline}' and node_name == '{node}'\")\n\n    def _apply_project_scope(self, df: Any) -&gt; Any:\n        \"\"\"Canonical project filter - works with Spark/Pandas/Polars.\"\"\"\n        projects = list(self._access.authorized_projects)\n\n        if hasattr(df, \"filter\") and hasattr(df, \"columns\"):  # Spark\n            from pyspark.sql.functions import col\n            return df.filter(col(\"project\").isin(projects))\n        elif hasattr(df, \"query\"):  # Pandas\n            return df[df[\"project\"].isin(projects)]\n        else:  # Polars\n            import polars as pl\n            return df.filter(pl.col(\"project\").is_in(projects))\n\n\nclass StoryLoader:\n    def set_access_context(self, ctx: AccessContext) -&gt; None:\n        self._access = ctx\n\n    def load(self, pipeline: str, run_selector: RunSelector) -&gt; PipelineStoryMetadata:\n        story = self._load_story(pipeline, run_selector)\n        self._access.check_project(story.project)  # Enforce access\n        return story\n\n\nclass SourceDiscovery:\n    def __init__(self, access: AccessContext, engine: Engine):\n        self._access = access\n        self._engine = engine\n\n    def list_files(self, connection: str, path: str, ...) -&gt; ListFilesResponse:\n        # Check connection access\n        policy = self._access.check_connection(connection)\n\n        # Check path access (deny-by-default)\n        self._access.check_path(connection, path)\n\n        # Proceed with listing...\n</code></pre>"},{"location":"mcp/SPEC/#configuration","title":"Configuration","text":"<pre><code># mcp_config.yaml\naccess_context:\n  authorized_projects:\n    - \"sales_analytics\"\n    - \"inventory_pipeline\"\n\n  environment: \"production\"\n\n  physical_refs_enabled: false  # Master switch - default OFF\n\n  connection_policies:\n    adls_raw:\n      # Deny-by-default: must specify allowed prefixes\n      allowed_path_prefixes:\n        - \"sales/\"\n        - \"inventory/\"\n      denied_path_prefixes:\n        - \"sales/pii/\"\n        - \"sales/restricted/\"\n      max_depth: 3\n      allow_physical_refs: false\n\n    adls_curated:\n      # Explicit allow-all for curated layer\n      explicit_allow_all: true\n      max_depth: 5\n      allow_physical_refs: false\n\n    sql_warehouse:\n      # SQL connections: table-level access\n      explicit_allow_all: true  # Can list all tables\n      allow_physical_refs: false\n</code></pre>"},{"location":"mcp/SPEC/#audit-logging","title":"Audit Logging","text":"<pre><code>import logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\n\n@dataclass\nclass AuditEntry:\n    timestamp: datetime\n    request_id: str\n    tool_name: str\n    project: str\n    environment: str\n    connection: Optional[str]\n    resource_logical: Optional[str]  # Logical name only - never physical\n    args_summary: Dict[str, Any]  # Redacted sensitive values\n    duration_ms: float\n    success: bool\n    error_type: Optional[str]\n    bytes_read_estimate: Optional[int]\n    policy_applied: Dict[str, bool]\n\nclass AuditLogger:\n    def __init__(self, logger: logging.Logger):\n        self.logger = logger\n\n    def log(self, entry: AuditEntry) -&gt; None:\n        self.logger.info(\n            \"MCP tool call\",\n            extra={\n                \"request_id\": entry.request_id,\n                \"tool\": entry.tool_name,\n                \"project\": entry.project,\n                \"env\": entry.environment,\n                \"connection\": entry.connection,\n                \"resource\": entry.resource_logical,\n                \"duration_ms\": entry.duration_ms,\n                \"success\": entry.success,\n                \"error_type\": entry.error_type,\n                \"policy\": entry.policy_applied,\n            }\n        )\n\n    @staticmethod\n    def redact_args(args: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Redact sensitive arguments before logging.\"\"\"\n        redact_keys = {\"password\", \"token\", \"secret\", \"key\", \"credential\"}\n        return {\n            k: \"[REDACTED]\" if any(r in k.lower() for r in redact_keys) else v\n            for k, v in args.items()\n        }\n</code></pre>"},{"location":"mcp/SPEC/#sample-limiting","title":"Sample Limiting","text":"<pre><code>MAX_SAMPLE_ROWS = 10\nMAX_SAMPLE_COLS = 15\nMAX_CELL_CHARS = 100\n\ndef limit_sample(\n    rows: List[Dict[str, Any]],\n    limit: int = MAX_SAMPLE_ROWS,\n    columns: Optional[List[str]] = None,\n) -&gt; tuple[List[Dict[str, Any]], bool, Optional[str]]:\n    \"\"\"\n    Apply hard limits to sample data.\n    Returns: (limited_rows, truncated, truncated_reason)\n    \"\"\"\n    truncated = False\n    reason = None\n\n    # Row limit\n    if len(rows) &gt; limit:\n        rows = rows[:limit]\n        truncated = True\n        reason = \"row_limit\"\n\n    # Column limit\n    if rows:\n        available_cols = list(rows[0].keys())\n        if columns:\n            cols = [c for c in columns if c in available_cols][:MAX_SAMPLE_COLS]\n        else:\n            cols = available_cols[:MAX_SAMPLE_COLS]\n\n        if len(available_cols) &gt; len(cols):\n            truncated = True\n            reason = reason or \"column_limit\"\n\n        rows = [{k: v for k, v in row.items() if k in cols} for row in rows]\n\n    # Cell truncation\n    for row in rows:\n        for k, v in row.items():\n            if isinstance(v, str) and len(v) &gt; MAX_CELL_CHARS:\n                row[k] = v[:MAX_CELL_CHARS - 3] + \"...\"\n                truncated = True\n                reason = reason or \"cell_limit\"\n\n    return rows, truncated, reason\n</code></pre>"},{"location":"mcp/SPEC/#error-handling","title":"Error Handling","text":"<pre><code>from pydantic import ValidationError\nfrom uuid import uuid4\n\nasync def call_tool(name: str, arguments: dict) -&gt; list[TextContent]:\n    request_id = str(uuid4())\n    start_time = datetime.utcnow()\n    project = \"unknown\"\n\n    try:\n        result = await dispatch_tool(name, arguments)\n        project = result.project\n\n        duration_ms = (datetime.utcnow() - start_time).total_seconds() * 1000\n\n        envelope = MCPSuccessEnvelope(\n            request_id=request_id,\n            tool_name=name,\n            project=project,\n            environment=access_context.environment,\n            duration_ms=duration_ms,\n            truncated=result.truncated,\n            truncated_reason=result.truncated_reason,\n            policy_applied=result.policy_applied,\n            data=result.data,\n        )\n\n        audit_logger.log(AuditEntry(\n            timestamp=start_time,\n            request_id=request_id,\n            tool_name=name,\n            project=project,\n            environment=access_context.environment,\n            duration_ms=duration_ms,\n            success=True,\n            error_type=None,\n            policy_applied=envelope.policy_applied.dict(),\n            ...\n        ))\n\n        return [TextContent(type=\"text\", text=envelope.model_dump_json())]\n\n    except PermissionError as e:\n        return _error_response(request_id, name, start_time, str(e), \"permission_denied\", retryable=False)\n\n    except ValidationError as e:\n        return _error_response(request_id, name, start_time, \"Validation failed\", \"validation_failed\",\n                               retryable=False, details=e.errors())\n\n    except FileNotFoundError as e:\n        return _error_response(request_id, name, start_time, str(e), \"not_found\", retryable=False)\n\n    except ConnectionError as e:\n        return _error_response(request_id, name, start_time, str(e), \"connection_error\", retryable=True)\n\n    except Exception as e:\n        return _error_response(request_id, name, start_time, str(e), \"internal_error\", retryable=False)\n</code></pre>"},{"location":"mcp/SPEC/#example-ai-workflow","title":"Example AI Workflow","text":"<p>User: \"Build me a bronze pipeline for the new sales data in adls_raw\"</p> <pre><code>AI: [list_files(\"adls_raw\", \"sales/\", limit=20)]\n    \u2192 {\n        request_id: \"abc-123\",\n        tool_name: \"list_files\",\n        project: \"sales_analytics\",\n        environment: \"production\",\n        success: true,\n        policy_applied: {\n          project_scoped: true,\n          connection_policy: \"adls_raw\",\n          path_filtered: true,\n          physical_ref_allowed: false,\n          physical_ref_included: false\n        },\n        data: {\n          connection: \"adls_raw\",\n          path: \"sales/\",\n          files: [\n            {logical_name: \"sales/2025-01.csv\", size_bytes: 1024000, modified: \"...\", physical_path: null},\n            {logical_name: \"sales/2025-02.csv\", size_bytes: 1048000, modified: \"...\", physical_path: null}\n          ],\n          next_token: null,\n          truncated: false\n        }\n      }\n\nAI: [infer_schema(\"adls_raw\", \"sales/2025-01.csv\", \"csv\")]\n    \u2192 {\n        success: true,\n        policy_applied: {sample_capped: true},\n        data: {\n          columns: [\n            {name: \"order_id\", dtype: \"int\", nullable: false},\n            {name: \"customer_email\", dtype: \"str\", nullable: true, semantic_type: \"pii\"},\n            {name: \"amount\", dtype: \"float\", nullable: true},\n            {name: \"order_date\", dtype: \"date\", nullable: true}\n          ]\n        }\n      }\n\nAI: [preview_source(\"adls_raw\", \"sales/2025-01.csv\", \"csv\", limit=3)]\n    \u2192 sample rows with full typing\n\nAI: [suggest_pattern(\"ingest raw sales CSV to bronze delta\")]\n    \u2192 \"Use basic transform pattern with delta write\"\n\nAI: [list_outputs(\"sales_analytics\", run_selector=\"latest_successful\", include_physical=false)]\n    \u2192 List[ResourceRef] with logical names only\n\nAI: [generate_pipeline_yaml(...)]\n    \u2192 generates YAML\n\nAI: [validate_yaml(generated_yaml)]\n    \u2192 confirms valid\n</code></pre>"},{"location":"mcp/SPEC/#implementation-order","title":"Implementation Order","text":"Phase Tasks Effort 1. Core contracts AccessContext, MCPEnvelope, RunSelector, ResourceRef, PolicyApplied 4h 2. Typed models ColumnSpec, SchemaResponse, GraphData, SchemaChange, DiffSummary, NodeStatsResponse 3h 3. Access enforcement Layer injection, project scoping, connection policies, path checks 3h 4. Audit logger AuditEntry, logging integration, arg redaction 1h 5. Story tools story_read, story_diff, node_describe with typed responses 3h 6. Sample tools node_sample, node_sample_in, node_failed_rows with limits 2h 7. Catalog tools TimeWindow, node_stats, pipeline_stats, failure_summary, schema_history 3h 8. Lineage tools lineage_upstream, lineage_downstream, lineage_graph with ResourceRef 2h 9. Schema tools output_schema, list_outputs with ResourceRef gating 2h 10. Source discovery list_files, list_tables, infer_schema, describe_table, preview_source with deny-by-default 5h 11. Error handling Structured errors with envelope, retryable flag 1h 12. Tests Unit tests for access, limits, pagination, typed responses 6h <p>Total: ~35 hours</p>"},{"location":"mcp/SPEC/#what-this-does-not-do","title":"What This Does NOT Do","text":"Anti-pattern Why excluded Execute arbitrary SQL Credential leak risk, non-determinism Trigger pipeline runs MCP is read-only; write = second orchestrator Return physical paths by default Require explicit gate (3 conditions must pass) Allow unbounded discovery Deny-by-default; require explicit allowlist Auto-refresh Stories Expensive; keep as pipeline-run side effect Re-resolve YAML configs Manager already did this; reuse resolved state Modify YAML files AI generates, user saves. YAML-first preserved Expose raw row diffs Hashed summaries by default; raw requires explicit flag Use parallel arrays All responses use typed models"},{"location":"mcp/SPEC/#security-model","title":"Security Model","text":"Layer Mechanism Credentials Stay server-side (env vars / Key Vault / MSI). Never returned. Unified access AccessContext enforced across Stories, Catalog, Lineage, Discovery Project scoping Canonical filter in all layers (Spark/Pandas/Polars) Connection policies Deny-by-default; require explicit allowlist or <code>explicit_allow_all</code> Path filtering <code>allowed_path_prefixes</code> + <code>denied_path_prefixes</code> checked on every call Physical ref gate 3 conditions: caller flag + policy flag + master switch PII in samples Privacy Suite anonymizes before Story capture Sensitive columns <code>sensitive: true</code> redacts at sample collection Sample limits Hard caps: 10 rows, 15 cols, 100 chars/cell Discovery limits 100 files/call, 1MB schema inference, pagination required Read-only No writes, no execution, no mutations Audit trail All tool calls logged with request_id, resource refs (logical only)"},{"location":"mcp/SPEC/#success-criteria","title":"Success Criteria","text":"<ol> <li>AI can discover: \"What sources exist at this connection? What's the schema?\"</li> <li>AI can build: \"Generate a bronze pipeline for this CSV with proper PII handling\"</li> <li>AI can observe: \"What happened in the last run? Why did it fail?\"</li> <li>AI can trace: \"What upstream tables feed into this fact table?\"</li> <li>AI cannot: See unauthorized projects, access unallowed paths, see physical refs, trigger runs</li> <li>YAML-first preserved: AI accelerates creation, user owns source of truth</li> <li>Debuggable: Every response has request_id, timestamp, policies applied</li> <li>Auditable: All tool calls logged with redacted args and logical resource refs</li> <li>Type-safe: All responses use typed models, no parallel arrays</li> <li>Deterministic: Deny-by-default discovery, explicit run selection everywhere</li> </ol>"},{"location":"mcp/SPEC/#appendix-invariants-summary","title":"Appendix: Invariants Summary","text":"Invariant Enforcement Read-only MCP facade MUST NOT mutate data, trigger runs, or alter state Single-project MCP tools MUST operate within a single project context per request RunSelector universal All stateful reads accept <code>run_selector</code> parameter Physical refs gated Requires <code>include_physical=True</code> + <code>policy.allow_physical_refs</code> + <code>access.physical_refs_enabled</code> Deny-by-default discovery Path access requires explicit allowlist or <code>explicit_allow_all=True</code> No parallel arrays All schemas use <code>List[ColumnSpec]</code>, not <code>columns[] + types[]</code> AccessContext everywhere Single context injected once, enforced by all layers Typed responses All tools return Pydantic models, not Dict[str, Any] Data source precedence Story \u2192 Manager metadata \u2192 Materialized output (in order) Truncation typed All truncation uses <code>TruncatedReason</code> enum, not free-form strings"},{"location":"mcp/SPEC/#appendix-resolved-concerns-oracle-critiques-v1-v3","title":"Appendix: Resolved Concerns (Oracle Critiques v1-v3)","text":"Concern Resolution Auth filter assumes pandas Unified AccessContext with canonical <code>_apply_project_scope()</code> for all engines Paths leak despite policy ResourceRef with 3-gate physical ref enforcement Run selection undefined Universal <code>RunSelector</code> invariant on all stateful reads Weak typing (Dict[str, Any]) All responses use typed Pydantic models Source discovery unbounded Deny-by-default with <code>ConnectionPolicy</code> allowlists Errors lose context <code>MCPEnvelope</code> with <code>request_id</code>, <code>tool_name</code>, <code>policy_applied</code> Time semantics unclear <code>TimeWindow</code> with complete validation (ordering, presence) Diff can leak rows <code>DiffSummary</code> with hashed counts by default No audit trail <code>AuditLogger</code> with structured entries Parallel arrays fragile Replaced with <code>List[ColumnSpec]</code> everywhere Empty allowlist = all allowed Flipped to deny-by-default; require <code>explicit_allow_all=True</code> Physical ref gate unclear Explicit 3-gate check documented and enforced Read-only implicit Added explicit READ-ONLY INVARIANT statement Story vs output precedence Added Data Source Precedence order (Story \u2192 Metadata \u2192 Output) truncated_reason free-form Added <code>TruncatedReason</code> enum with explicit values Cross-project queries possible Added SINGLE-PROJECT INVARIANT banning cross-project operations"},{"location":"patterns/","title":"Odibi Data Patterns","text":"<p>This directory contains documentation for common data pipeline patterns used in Odibi. Each pattern solves a specific problem and includes step-by-step examples.</p>"},{"location":"patterns/#patterns","title":"Patterns","text":""},{"location":"patterns/#1-append-only-raw-layer","title":"1. Append-Only Raw Layer","text":"<p>Problem: How do I safely ingest data without losing audit trails?</p> <p>Pattern: All data from sources is appended to the Raw layer without modification. Raw is immutable and append-only.</p> <p>When to use: Always, for all source ingestion. Raw is your safety net.</p>"},{"location":"patterns/#2-high-water-mark-smart-read","title":"2. High Water Mark (Smart Read)","text":"<p>Problem: My source table has millions of rows. How do I efficiently read only new/changed data?</p> <p>Pattern: Use the <code>incremental</code> configuration (Smart Read) to automatically filter the source query. Odibi manages the state for you: First Run = Full Load, Subsequent Runs = Incremental Load.</p> <p>When to use: When your source has timestamps (created_at, updated_at) and you want incremental reads. Essential for daily incremental loads.</p> <p>See also: Manual HWM Guide - understanding the underlying SQL pattern.</p>"},{"location":"patterns/#3-mergeupsert-silver-layer","title":"3. Merge/Upsert (Silver Layer)","text":"<p>Problem: How do I deduplicate and keep the latest version of each record?</p> <p>Pattern: Use Delta Lake's MERGE operation (or Merge Transformer) to upsert records by key, with audit columns tracking created/updated timestamps.</p> <p>When to use: Refining Raw \u2192 Silver. Always use for stateful transformations.</p>"},{"location":"patterns/#4-scd-type-2-history-tracking","title":"4. SCD Type 2 (History Tracking)","text":"<p>Problem: \"I need to know what the address was last month, not just now.\"</p> <p>Pattern: Track full history. Old records are closed (valid_to set), new records are opened (valid_to NULL). Preserves point-in-time accuracy.</p> <p>When to use: Slowly Changing Dimensions (Customer address, Product category).</p>"},{"location":"patterns/#5-windowed-reprocess-gold-layer-aggregates","title":"5. Windowed Reprocess (Gold Layer Aggregates)","text":"<p>Problem: Late-arriving data can break my aggregates. How do I fix them without double-counting?</p> <p>Pattern: Instead of patching aggregates with updates, recalculate the entire time window and overwrite that partition.</p> <p>When to use: Building Gold-layer aggregates (KPIs, star schemas). Ensures idempotency and correctness.</p>"},{"location":"patterns/#6-skip-if-unchanged-snapshot-optimization","title":"6. Skip If Unchanged (Snapshot Optimization)","text":"<p>Problem: My hourly pipeline appends identical data 24 times/day when the source hasn't changed.</p> <p>Pattern: Compute a hash of the DataFrame content before writing. If hash matches previous write, skip the append entirely.</p> <p>When to use: Snapshot tables without timestamps, reference data that changes infrequently, or when change frequency is unknown.</p>"},{"location":"patterns/#dimensional-modeling-patterns","title":"Dimensional Modeling Patterns","text":"<p>These patterns are designed for building star schemas and data warehouses. Use them via <code>pattern: type: pattern_name</code> in your node config.</p>"},{"location":"patterns/#7-dimension-pattern","title":"7. Dimension Pattern","text":"<p>Problem: How do I build dimension tables with surrogate keys and SCD support?</p> <p>Pattern: Use <code>pattern: type: dimension</code> to auto-generate surrogate keys and handle SCD Type 0/1/2 with optional unknown member rows.</p> <p>When to use: Building any dimension table (dim_customer, dim_product, etc.)</p> <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_cols: [name, email, address]\n</code></pre>"},{"location":"patterns/#8-date-dimension-pattern","title":"8. Date Dimension Pattern","text":"<p>Problem: How do I generate a complete date dimension with fiscal calendars?</p> <p>Pattern: Use <code>pattern: type: date_dimension</code> to generate dates with 19 pre-calculated columns including fiscal year/quarter.</p> <p>When to use: Every data warehouse needs a date dimension. Generate once with a wide range (2015-2035).</p> <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    fiscal_year_start_month: 7\n    unknown_member: true\n</code></pre>"},{"location":"patterns/#9-fact-pattern","title":"9. Fact Pattern","text":"<p>Problem: How do I build fact tables with automatic surrogate key lookups?</p> <p>Pattern: Use <code>pattern: type: fact</code> to join source data to dimensions, retrieve SKs, handle orphans, and validate grain.</p> <p>When to use: Building any fact table that references dimensions.</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id, line_item_id]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n    orphan_handling: unknown\n</code></pre>"},{"location":"patterns/#10-aggregation-pattern","title":"10. Aggregation Pattern","text":"<p>Problem: How do I build aggregate tables with declarative GROUP BY and incremental refresh?</p> <p>Pattern: Use <code>pattern: type: aggregation</code> with grain (GROUP BY) and measure expressions.</p> <p>When to use: Building aggregate/summary tables, KPI tables, or materializing metrics.</p> <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk]\n    measures:\n      - name: total_revenue\n        expr: \"SUM(line_total)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n</code></pre>"},{"location":"patterns/#design-principles","title":"Design Principles","text":"<p>These patterns are built on the Odibi Architecture Manifesto:</p> <ol> <li>Robots Remember, Humans Forget \u2192 Use checkpoint bookkeeping, not manual state tracking</li> <li>Raw is Sacred \u2192 Append-only, immutable history. Never destroy original data.</li> <li>Rebuild the Bucket, Don't Patch the Hole \u2192 Reprocess entire time windows, don't patch aggregates</li> <li>SQL is for Humans, ADLS is for Robots \u2192 ADLS stores everything; SQL serves BI</li> <li>No Duplication \u2192 Test against production data; don't duplicate datasets</li> </ol>"},{"location":"patterns/#quick-reference","title":"Quick Reference","text":"Pattern Input Output Write Mode Idempotent? Append-Only Raw Source Raw <code>append</code> Yes (duplicates OK) High Water Mark Source + Timestamp Raw <code>append</code> Yes (filtered by timestamp) Smart Read Source + Timestamp Raw <code>append</code> Yes (auto-managed) Merge/Upsert Raw (micro-batch) Silver <code>merge</code> Yes (by key) SCD Type 2 Raw (micro-batch) Silver/Gold <code>overwrite</code> Yes (full history) Windowed Reprocess Silver (window) Gold <code>overwrite</code> (partition) Yes (recalculated) Skip If Unchanged Snapshot Source Raw <code>append</code> (conditional) Yes (hash-based) Dimension Staging Gold (dim_*) <code>overwrite</code> Yes (SK-based) Date Dimension Generated Gold (dim_date) <code>overwrite</code> Yes (no input) Fact Staging + Dims Gold (fact_*) <code>overwrite</code> Yes (grain-based) Aggregation Fact Gold (agg_*) <code>overwrite</code> Yes (grain-based)"},{"location":"patterns/#see-it-all-together","title":"See It All Together","text":"<p>THE_REFERENCE.md \u2014 A complete star schema that combines dimension, date_dimension, and fact patterns in one runnable pipeline. This is the canonical example showing how all the patterns work together.</p>"},{"location":"patterns/#further-reading","title":"Further Reading","text":"<ul> <li>Databricks: \"Incremental Processing\" documentation</li> <li>Book: Fundamentals of Data Engineering by Joe Reis &amp; Matt Housley</li> </ul>"},{"location":"patterns/aggregation/","title":"Aggregation Pattern","text":"<p>The <code>aggregation</code> pattern provides declarative aggregation with configurable grain, measures, and incremental merge strategies.</p>"},{"location":"patterns/aggregation/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The aggregation pattern works on data from the read block or a dependency, applies GROUP BY aggregation, and writes the results.</p> <pre><code>project: sales_analytics\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    nodes:\n      - name: agg_daily_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: avg_order_value\n                expr: \"AVG(line_total)\"\n            having: \"COUNT(*) &gt; 0\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#features","title":"Features","text":"<ul> <li>Declarative grain (GROUP BY columns)</li> <li>Flexible measure expressions (SUM, COUNT, AVG, etc.)</li> <li>Incremental aggregation (merge new data with existing)</li> <li>HAVING clause support</li> <li>Audit columns</li> </ul>"},{"location":"patterns/aggregation/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>grain</code> list Yes - Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes - Measure definitions with name and expr <code>having</code> str No - Optional HAVING clause <code>incremental</code> dict No - Incremental merge configuration <code>target</code> str For incremental - Target table for incremental merge <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/aggregation/#measure-definition","title":"Measure Definition","text":"<pre><code>params:\n  measures:\n    - name: total_revenue      # Output column name\n      expr: \"SUM(line_total)\"  # SQL aggregation expression\n</code></pre>"},{"location":"patterns/aggregation/#incremental-config","title":"Incremental Config","text":"<pre><code>params:\n  incremental:\n    timestamp_column: order_date  # Column to identify new data\n    merge_strategy: replace       # \"replace\" or \"sum\"\n  target: warehouse.agg_daily_sales\n</code></pre>"},{"location":"patterns/aggregation/#measure-expressions","title":"Measure Expressions","text":"<p>Use standard SQL aggregation functions:</p> <pre><code>params:\n  measures:\n    # Basic aggregations\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n\n    - name: max_order\n      expr: \"MAX(line_total)\"\n\n    # Complex expressions\n    - name: total_with_discount\n      expr: \"SUM(line_total - discount_amount)\"\n\n    - name: discount_rate\n      expr: \"SUM(discount_amount) / SUM(line_total)\"\n</code></pre>"},{"location":"patterns/aggregation/#incremental-merge-strategies","title":"Incremental Merge Strategies","text":""},{"location":"patterns/aggregation/#replace-strategy","title":"Replace Strategy","text":"<p>New aggregates overwrite existing for matching grain keys:</p> <pre><code>nodes:\n  - name: agg_daily_sales\n    read:\n      connection: warehouse\n      path: fact_orders\n    pattern:\n      type: aggregation\n      params:\n        grain: [date_sk, product_sk]\n        measures:\n          - name: total_revenue\n            expr: \"SUM(line_total)\"\n        incremental:\n          timestamp_column: order_date\n          merge_strategy: replace\n        target: warehouse.agg_daily_sales\n    write:\n      connection: warehouse\n      path: agg_daily_sales\n      mode: overwrite\n</code></pre> <p>Use case: Full recalculation of affected grains (idempotent, handles late data)</p>"},{"location":"patterns/aggregation/#sum-strategy","title":"Sum Strategy","text":"<p>Add new measure values to existing aggregates:</p> <pre><code>params:\n  incremental:\n    timestamp_column: order_date\n    merge_strategy: sum\n</code></pre> <p>Use case: Additive metrics only (counts, sums) where data is append-only</p> <p>Warning: Don't use for AVG, DISTINCT counts, or ratios.</p>"},{"location":"patterns/aggregation/#time-rollups","title":"Time Rollups","text":"<p>Build aggregate hierarchies at multiple time grains:</p> <pre><code>pipelines:\n  - pipeline: build_aggregates\n    nodes:\n      # Daily aggregate (from fact)\n      - name: agg_daily_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          mode: overwrite\n\n      # Monthly rollup (from daily aggregate)\n      - name: agg_monthly_sales\n        depends_on: [agg_daily_sales]\n        transform:\n          steps:\n            - sql: |\n                SELECT \n                  FLOOR(date_sk / 100) AS month_sk,\n                  product_sk,\n                  total_revenue,\n                  order_count\n                FROM df\n        pattern:\n          type: aggregation\n          params:\n            grain: [month_sk, product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(total_revenue)\"\n              - name: order_count\n                expr: \"SUM(order_count)\"\n        write:\n          connection: warehouse\n          path: agg_monthly_sales\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#full-yaml-example","title":"Full YAML Example","text":"<p>Complete aggregation pipeline with incremental refresh:</p> <pre><code>project: sales_analytics\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_aggregates\n    description: \"Build daily and monthly sales aggregates\"\n    nodes:\n      - name: agg_daily_product_sales\n        description: \"Daily product sales aggregate\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n        pattern:\n          type: aggregation\n          params:\n            grain:\n              - date_sk\n              - product_sk\n              - region\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: total_cost\n                expr: \"SUM(cost_amount)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: units_sold\n                expr: \"SUM(quantity)\"\n              - name: unique_customers\n                expr: \"COUNT(DISTINCT customer_sk)\"\n              - name: avg_unit_price\n                expr: \"AVG(unit_price)\"\n            having: \"SUM(line_total) &gt; 0\"\n            incremental:\n              timestamp_column: load_timestamp\n              merge_strategy: replace\n            target: warehouse.agg_daily_product_sales\n            audit:\n              load_timestamp: true\n              source_system: \"aggregation_pipeline\"\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/aggregation/#see-also","title":"See Also","text":"<ul> <li>Fact Pattern - Build fact tables</li> <li>Semantic Layer - Define metrics for ad-hoc queries</li> <li>Materializing Metrics - Schedule metric materialization</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/anti_patterns/","title":"\u274c Anti-Patterns: What NOT to Do","text":"<p>This guide documents common mistakes when building data pipelines. For each anti-pattern, we show what NOT to do, why it's bad, and the correct approach.</p> <p>Learning what NOT to do is just as important as learning what TO do.</p>"},{"location":"patterns/anti_patterns/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Transforming in Bronze Layer</li> <li>Not Deduplicating Before SCD2</li> <li>Using SCD2 for Fact Tables</li> <li>Hardcoding Paths Instead of Connections</li> <li>Not Handling NULLs in Key Columns</li> <li>Mixing Business Logic Across Layers</li> <li>Skipping the Silver Layer</li> <li>Not Adding Extracted Timestamps in Bronze</li> <li>Using Append Mode Without Deduplication</li> <li>Ignoring Schema Evolution</li> </ol>"},{"location":"patterns/anti_patterns/#1-transforming-in-bronze-layer","title":"1. Transforming in Bronze Layer","text":""},{"location":"patterns/anti_patterns/#what-not-to-do","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Cleaning data in the Bronze layer\npipelines:\n  - pipeline: \"bronze_customers\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_customers\"\n        read:\n          connection: landing\n          path: customers.csv\n\n        # \u274c DON'T transform in Bronze!\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE status != 'inactive'\"\n            - function: \"clean_text\"\n              params: { columns: [\"name\", \"email\"] }\n\n        write:\n          connection: bronze\n          path: customers\n          format: delta\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad","title":"Why It's Bad","text":"<p>You lose the original data forever.</p> <p>Imagine this scenario: 1. You filter out \"inactive\" customers in Bronze 2. 6 months later, business says \"We need to analyze inactive customers too\" 3. You can't\u2014because you threw that data away</p> <p>Bronze is your \"undo button.\" If you transform data there, you lose the ability to go back to the source.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Keep Bronze raw, transform in Silver\npipelines:\n  # Step 1: Bronze - Store raw data exactly as received\n  - pipeline: \"bronze_customers\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_customers\"\n        read:\n          connection: landing\n          path: customers.csv\n\n        # No transformations! Just land the data\n        write:\n          connection: bronze\n          path: customers\n          format: delta\n          mode: append\n\n  # Step 2: Silver - Now you can transform\n  - pipeline: \"silver_customers\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_customers\"\n        read:\n          connection: bronze\n          path: customers\n\n        # \u2705 Transform in Silver\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE status != 'inactive'\"\n            - function: \"clean_text\"\n              params: { columns: [\"name\", \"email\"] }\n\n        write:\n          connection: silver\n          path: dim_customers\n          format: delta\n</code></pre>"},{"location":"patterns/anti_patterns/#the-one-exception","title":"\ud83d\udca1 The One Exception","text":"<p>You MAY add metadata columns in Bronze (they don't alter original data):</p> <pre><code># OK: Adding metadata in Bronze\ntransform:\n  steps:\n    - function: \"derive_columns\"\n      params:\n        derivations:\n          _extracted_at: \"current_timestamp\"\n          _source_file: \"'customers.csv'\"\n</code></pre>"},{"location":"patterns/anti_patterns/#2-not-deduplicating-before-scd2","title":"2. Not Deduplicating Before SCD2","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_1","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Source has duplicates, feeding directly to SCD2\nnodes:\n  - name: \"dim_customers\"\n    read:\n      connection: bronze\n      path: customers  # Contains duplicate customer_id rows!\n\n    # \u274c SCD2 without deduplication\n    transformer: \"scd2\"\n    params:\n      target: silver.dim_customers\n      keys: [\"customer_id\"]\n      track_cols: [\"name\", \"email\", \"address\"]\n      effective_time_col: \"updated_at\"\n\n    write:\n      connection: silver\n      table: dim_customers\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_1","title":"Why It's Bad","text":"<p>Your history table explodes with duplicate versions.</p> <p>If your source has: <pre><code>customer_id | name  | updated_at\n101         | Alice | 2024-01-01 10:00:00\n101         | Alice | 2024-01-01 10:00:01  &lt;- Duplicate from same extract\n101         | Alice | 2024-01-01 10:00:02  &lt;- Another duplicate\n</code></pre></p> <p>SCD2 sees three \"changes\" and creates three history rows, even though nothing actually changed:</p> <pre><code>customer_id | name  | effective_time          | end_time               | is_current\n101         | Alice | 2024-01-01 10:00:00     | 2024-01-01 10:00:01    | false\n101         | Alice | 2024-01-01 10:00:01     | 2024-01-01 10:00:02    | false  \n101         | Alice | 2024-01-01 10:00:02     | NULL                   | true\n</code></pre> <p>Your dimension table grows 3x faster than it should, wasting storage and slowing queries.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_1","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Deduplicate first, then SCD2\nnodes:\n  - name: \"dedup_customers\"\n    read:\n      connection: bronze\n      path: customers\n\n    # \u2705 Deduplicate first - keep most recent per customer\n    transformer: \"deduplicate\"\n    params:\n      keys: [\"customer_id\"]\n      order_by: \"updated_at DESC\"\n\n    write:\n      connection: staging\n      path: customers_deduped\n\n  - name: \"dim_customers\"\n    depends_on: [\"dedup_customers\"]\n\n    # \u2705 Now SCD2 sees clean data\n    transformer: \"scd2\"\n    params:\n      target: silver.dim_customers\n      keys: [\"customer_id\"]\n      track_cols: [\"name\", \"email\", \"address\"]\n      effective_time_col: \"updated_at\"\n\n    write:\n      connection: silver\n      table: dim_customers\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/anti_patterns/#3-using-scd2-for-fact-tables","title":"3. Using SCD2 for Fact Tables","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_2","title":"\u274c What NOT to Do","text":"<pre><code># BAD: SCD2 on a fact table\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: bronze\n      path: orders\n\n    # \u274c DON'T use SCD2 for facts!\n    transformer: \"scd2\"\n    params:\n      target: silver.fact_orders\n      keys: [\"order_id\"]\n      track_cols: [\"quantity\", \"total_amount\"]\n      effective_time_col: \"order_date\"\n\n    write:\n      connection: silver\n      table: fact_orders\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_2","title":"Why It's Bad","text":"<p>Facts don't change\u2014they happen.</p> <p>An order is an event. Customer 101 placed order #5001 on January 15th for $99.00. That's a historical fact. It doesn't \"change.\"</p> <p>If the source shows a different amount for the same order, that's either: 1. A correction (handle with a correction fact, not by changing history) 2. A data quality issue (should be caught by validation)</p> <p>Using SCD2 on facts: - Bloats your table unnecessarily - Creates confusing history for immutable events - Slows down analytical queries</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_2","title":"\u2705 What to Do Instead","text":"<p>For fact tables, use append mode (for new records) or merge mode (for late-arriving corrections):</p> <pre><code># GOOD: Append mode for facts\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: bronze\n      path: orders\n\n    # \u2705 Just append new orders\n    write:\n      connection: silver\n      table: fact_orders\n      format: delta\n      mode: append\n\n# OR: Merge mode if you expect corrections\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: bronze\n      path: orders\n\n    # \u2705 Merge handles late corrections\n    transformer: \"merge\"\n    params:\n      target: silver.fact_orders\n      keys: [\"order_id\"]\n      # Updates existing, inserts new\n\n    write:\n      connection: silver\n      table: fact_orders\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/anti_patterns/#when-to-use-each","title":"\ud83d\udca1 When to Use Each","text":"Scenario Pattern New orders arriving daily <code>append</code> Orders may be corrected later <code>merge</code> Customer info changes over time <code>scd2</code>"},{"location":"patterns/anti_patterns/#4-hardcoding-paths-instead-of-connections","title":"4. Hardcoding Paths Instead of Connections","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_3","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Hardcoded paths everywhere\nnodes:\n  - name: \"load_sales\"\n    read:\n      # \u274c Hardcoded path - breaks when moving to prod\n      path: \"abfss://raw@devstorageaccount.dfs.core.windows.net/sales/2024/\"\n\n    write:\n      # \u274c Another hardcoded path\n      path: \"abfss://bronze@devstorageaccount.dfs.core.windows.net/sales\"\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_3","title":"Why It's Bad","text":"<p>Your pipeline breaks when moving between environments.</p> <p>Development, staging, and production have different: - Storage account names - Credentials - Base paths</p> <p>If you hardcode paths, you need to edit the YAML for every environment. This leads to: - Copy-paste errors - Secrets accidentally committed to git - \"It works on my machine\" syndrome</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_3","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Use connections with environment variables\nconnections:\n  landing:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}  # From environment\n    container: raw\n    credential: ${STORAGE_KEY}   # Secret from Key Vault\n\n  bronze:\n    type: azure_blob\n    account: ${STORAGE_ACCOUNT}\n    container: bronze\n    credential: ${STORAGE_KEY}\n\npipelines:\n  - pipeline: \"load_sales\"\n    nodes:\n      - name: \"ingest_sales\"\n        read:\n          # \u2705 Use connection name + relative path\n          connection: landing\n          path: sales/2024/\n\n        write:\n          # \u2705 Portable across environments\n          connection: bronze\n          path: sales\n</code></pre> <p>Now the same YAML works in dev, staging, and prod\u2014just change the environment variables.</p>"},{"location":"patterns/anti_patterns/#5-not-handling-nulls-in-key-columns","title":"5. Not Handling NULLs in Key Columns","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_4","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Joining on columns that might be NULL\nnodes:\n  - name: \"enrich_orders\"\n    read:\n      connection: silver\n      path: fact_orders  # customer_id can be NULL!\n\n    # \u274c Join without NULL handling\n    transformer: \"join\"\n    params:\n      right: silver.dim_customer\n      on: [\"customer_id\"]\n      how: \"left\"\n</code></pre> <p>Source data: <pre><code>order_id | customer_id | amount\n1001     | 101         | 99.00\n1002     | NULL        | 45.00   &lt;- Guest checkout, no customer\n1003     | 102         | 150.00\n</code></pre></p>"},{"location":"patterns/anti_patterns/#why-its-bad_4","title":"Why It's Bad","text":"<p>NULL never equals NULL in SQL.</p> <p>When you join on <code>customer_id</code>: - <code>101 = 101</code> \u2705 Match - <code>NULL = NULL</code> \u274c No match! (NULL is \"unknown\", and unknown \u2260 unknown)</p> <p>Your orders with NULL customer_id get dropped or get incorrect dimension values.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_4","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Handle NULLs before joining\nnodes:\n  - name: \"prep_orders\"\n    read:\n      connection: silver\n      path: fact_orders\n\n    # \u2705 Option 1: Fill NULLs with a placeholder that maps to \"unknown\" customer\n    transform:\n      steps:\n        - function: \"fill_nulls\"\n          params:\n            columns: [\"customer_id\"]\n            value: 0  # Maps to unknown member in dim_customer\n\n    write:\n      connection: staging\n      path: orders_with_valid_keys\n\n  - name: \"enrich_orders\"\n    depends_on: [\"prep_orders\"]\n\n    transformer: \"join\"\n    params:\n      right: silver.dim_customer  # Has customer_id=0 as unknown member\n      on: [\"customer_id\"]\n      how: \"left\"\n</code></pre> <p>Or use the fact pattern with <code>orphan_handling: unknown</code>:</p> <pre><code># GOOD: Use the fact pattern for automatic NULL handling\nnodes:\n  - name: \"fact_orders\"\n    read:\n      connection: silver\n      path: orders_clean\n\n    pattern:\n      type: fact\n      params:\n        dimensions:\n          - source_column: customer_id\n            dimension_table: dim_customer\n            dimension_key: customer_id\n            surrogate_key: customer_sk\n        # \u2705 NULLs get SK=0 (unknown member)\n        orphan_handling: unknown\n</code></pre>"},{"location":"patterns/anti_patterns/#6-mixing-business-logic-across-layers","title":"6. Mixing Business Logic Across Layers","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_5","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Business logic scattered everywhere\npipelines:\n  - pipeline: \"bronze_sales\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_sales\"\n        transform:\n          steps:\n            # \u274c Business calculation in Bronze?!\n            - sql: \"SELECT *, quantity * unit_price * 0.92 as net_amount FROM df\"\n        write:\n          connection: bronze\n          path: sales\n\n  - pipeline: \"silver_sales\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_sales\"\n        transform:\n          steps:\n            # \u274c More business logic here\n            - sql: \"SELECT *, CASE WHEN net_amount &gt; 1000 THEN 'high' ELSE 'low' END as tier FROM df\"\n        write:\n          connection: silver\n          path: sales\n\n  - pipeline: \"gold_sales\"\n    layer: \"gold\"\n    nodes:\n      - name: \"report_sales\"\n        transform:\n          steps:\n            # \u274c And here too!\n            - sql: \"SELECT *, net_amount * 1.1 as projected_amount FROM df\"\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_5","title":"Why It's Bad","text":"<p>Debugging becomes a nightmare.</p> <p>When someone asks \"Why is projected_amount $1,100?\", you have to trace through: 1. Bronze: <code>quantity * unit_price * 0.92 = net_amount</code> (8% discount) 2. Silver: No change to amounts 3. Gold: <code>net_amount * 1.1 = projected_amount</code> (10% markup)</p> <p>The business logic is hidden in three different places. Any change requires editing multiple pipelines.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_5","title":"\u2705 What to Do Instead","text":"<p>Keep business logic in ONE place\u2014Silver or Gold, not both.</p> <pre><code># GOOD: Clear separation of concerns\npipelines:\n  # Bronze: Raw data only\n  - pipeline: \"bronze_sales\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_sales\"\n        read:\n          connection: landing\n          path: sales.csv\n        # \u2705 No transformations in Bronze\n        write:\n          connection: bronze\n          path: sales\n\n  # Silver: Cleaning + business logic\n  - pipeline: \"silver_sales\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_sales\"\n        read:\n          connection: bronze\n          path: sales\n        transform:\n          steps:\n            # \u2705 All business calculations in ONE place\n            - sql: |\n                SELECT\n                  *,\n                  quantity * unit_price as gross_amount,\n                  quantity * unit_price * 0.92 as net_amount,\n                  quantity * unit_price * 0.92 * 1.1 as projected_amount,\n                  CASE WHEN quantity * unit_price * 0.92 &gt; 1000 THEN 'high' ELSE 'low' END as tier\n                FROM df\n        write:\n          connection: silver\n          path: fact_sales\n\n  # Gold: Aggregation only (no new business logic)\n  - pipeline: \"gold_sales\"\n    layer: \"gold\"\n    nodes:\n      - name: \"daily_summary\"\n        read:\n          connection: silver\n          path: fact_sales\n        # \u2705 Gold just aggregates what Silver prepared\n        pattern:\n          type: aggregation\n          params:\n            grain: [sale_date, region]\n            measures:\n              - name: total_net\n                expr: \"SUM(net_amount)\"\n              - name: total_projected\n                expr: \"SUM(projected_amount)\"\n</code></pre>"},{"location":"patterns/anti_patterns/#7-skipping-the-silver-layer","title":"7. Skipping the Silver Layer","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_6","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Going directly from Bronze to Gold\npipelines:\n  - pipeline: \"bronze_orders\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: landing\n          path: orders.csv\n        write:\n          connection: bronze\n          path: orders\n\n  - pipeline: \"gold_summary\"\n    layer: \"gold\"\n    nodes:\n      - name: \"daily_sales\"\n        read:\n          connection: bronze\n          path: orders  # \u274c Reading raw Bronze directly!\n\n        # Trying to do EVERYTHING in one step\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_id IS NOT NULL\"\n            - function: \"deduplicate\"\n              params: { keys: [\"order_id\"] }\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [order_date]\n            measures:\n              - name: total_sales\n                expr: \"SUM(amount)\"\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_6","title":"Why It's Bad","text":"<p>Every downstream consumer has to repeat the cleaning.</p> <p>If you have 5 Gold tables that all read from Bronze: 1. Each one cleans duplicates (same code x5) 2. Each one handles nulls (same code x5) 3. Each one applies business rules (same code x5)</p> <p>Any cleaning bug must be fixed in 5 places. And if different teams make slightly different cleaning decisions, your reports don't match.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_6","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Silver is your \"single source of truth\"\npipelines:\n  - pipeline: \"bronze_orders\"\n    layer: \"bronze\"\n    nodes:\n      - name: \"ingest_orders\"\n        read:\n          connection: landing\n          path: orders.csv\n        write:\n          connection: bronze\n          path: orders\n\n  # \u2705 Silver: Clean ONCE, use EVERYWHERE\n  - pipeline: \"silver_orders\"\n    layer: \"silver\"\n    nodes:\n      - name: \"clean_orders\"\n        read:\n          connection: bronze\n          path: orders\n\n        # All cleaning happens here, once\n        transform:\n          steps:\n            - sql: \"SELECT * FROM df WHERE order_id IS NOT NULL\"\n            - function: \"deduplicate\"\n              params: { keys: [\"order_id\"] }\n\n        validation:\n          contracts:\n            - type: not_null\n              columns: [order_id, customer_id, amount]\n\n        write:\n          connection: silver\n          path: fact_orders\n\n  # \u2705 Gold: Just aggregate clean data\n  - pipeline: \"gold_daily\"\n    layer: \"gold\"\n    nodes:\n      - name: \"daily_sales\"\n        read:\n          connection: silver\n          path: fact_orders  # \u2705 Reading from Silver\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [order_date]\n            measures:\n              - name: total_sales\n                expr: \"SUM(amount)\"\n\n  # \u2705 Another Gold table reads the same Silver\n  - pipeline: \"gold_regional\"\n    layer: \"gold\"\n    nodes:\n      - name: \"regional_sales\"\n        read:\n          connection: silver\n          path: fact_orders  # \u2705 Same Silver source\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [region, month]\n            measures:\n              - name: total_sales\n                expr: \"SUM(amount)\"\n</code></pre>"},{"location":"patterns/anti_patterns/#8-not-adding-extracted-timestamps-in-bronze","title":"8. Not Adding Extracted Timestamps in Bronze","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_7","title":"\u274c What NOT to Do","text":"<pre><code># BAD: No extraction timestamp\nnodes:\n  - name: \"ingest_sales\"\n    read:\n      connection: landing\n      path: sales/\n\n    # \u274c No metadata about when this was loaded\n    write:\n      connection: bronze\n      path: sales\n      mode: append\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_7","title":"Why It's Bad","text":"<p>You can't debug timing issues.</p> <p>Scenario: Data looks wrong for January 15th.</p> <p>Questions you can't answer: - When was the January 15th data loaded? - Was it loaded multiple times? - Did the source file change between loads?</p> <p>Without timestamps, your Bronze table is just a pile of data with no history of how it got there.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_7","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Add extraction metadata\nnodes:\n  - name: \"ingest_sales\"\n    read:\n      connection: landing\n      path: sales/\n\n    # \u2705 Add metadata columns\n    transform:\n      steps:\n        - function: \"derive_columns\"\n          params:\n            derivations:\n              _extracted_at: \"current_timestamp\"\n              _source_file: \"input_file_name()\"\n              _batch_id: \"'${BATCH_ID}'\"  # From orchestrator\n\n    write:\n      connection: bronze\n      path: sales\n      mode: append\n</code></pre> <p>Now your Bronze data includes:</p> <pre><code>order_id | amount | _extracted_at       | _source_file        | _batch_id\n1001     | 99.00  | 2024-01-16 06:00:00 | sales_20240115.csv  | batch_42\n1002     | 45.00  | 2024-01-16 06:00:00 | sales_20240115.csv  | batch_42\n1001     | 99.00  | 2024-01-16 18:00:00 | sales_20240115.csv  | batch_43  &lt;- Aha! Loaded twice!\n</code></pre>"},{"location":"patterns/anti_patterns/#9-using-append-mode-without-deduplication","title":"9. Using Append Mode Without Deduplication","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_8","title":"\u274c What NOT to Do","text":"<pre><code># BAD: Append mode on a table that gets reprocessed\nnodes:\n  - name: \"load_daily_sales\"\n    read:\n      connection: landing\n      path: sales/${YESTERDAY}/  # Same file every rerun\n\n    # \u274c Append without deduplication\n    write:\n      connection: bronze\n      path: sales\n      mode: append\n</code></pre>"},{"location":"patterns/anti_patterns/#why-its-bad_8","title":"Why It's Bad","text":"<p>Re-running the pipeline doubles your data.</p> <ul> <li>First run: 1,000 rows appended \u2705</li> <li>Pipeline fails later, you rerun from scratch</li> <li>Second run: Same 1,000 rows appended again \u274c</li> <li>Now you have 2,000 rows (1,000 duplicates)</li> </ul> <p>Your aggregations now show 2x the real sales.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_8","title":"\u2705 What to Do Instead","text":"<p>Option 1: Use merge mode for idempotent writes</p> <pre><code># GOOD: Merge mode is idempotent\nnodes:\n  - name: \"load_daily_sales\"\n    read:\n      connection: landing\n      path: sales/${YESTERDAY}/\n\n    # \u2705 Merge inserts new, updates existing\n    transformer: \"merge\"\n    params:\n      target: bronze.sales\n      keys: [\"order_id\"]\n\n    write:\n      connection: bronze\n      table: sales\n      format: delta\n      mode: overwrite\n</code></pre> <p>Option 2: Deduplicate in Silver</p> <pre><code># GOOD: Bronze appends, Silver deduplicates\nnodes:\n  - name: \"load_sales_bronze\"\n    write:\n      connection: bronze\n      path: sales\n      mode: append  # OK because Silver deduplicates\n\n  - name: \"clean_sales_silver\"\n    read:\n      connection: bronze\n      path: sales\n\n    # \u2705 Deduplicate the appended data\n    transformer: \"deduplicate\"\n    params:\n      keys: [\"order_id\"]\n      order_by: \"_extracted_at DESC\"  # Keep most recent if duplicated\n\n    write:\n      connection: silver\n      path: fact_sales\n      mode: overwrite  # Full refresh\n</code></pre>"},{"location":"patterns/anti_patterns/#10-ignoring-schema-evolution","title":"10. Ignoring Schema Evolution","text":""},{"location":"patterns/anti_patterns/#what-not-to-do_9","title":"\u274c What NOT to Do","text":"<pre><code># BAD: No schema handling\nnodes:\n  - name: \"load_api_data\"\n    read:\n      connection: api\n      endpoint: /customers\n\n    # \u274c Just writing whatever comes from the API\n    write:\n      connection: bronze\n      path: customers\n      format: delta\n      # No schema_mode specified\n</code></pre> <p>What happens when the API adds a new field:</p> <pre><code>Day 1: {\"id\": 1, \"name\": \"Alice\"}\nDay 2: {\"id\": 2, \"name\": \"Bob\", \"loyalty_points\": 500}  &lt;- New field!\n</code></pre> <p>Pipeline fails with: <pre><code>SchemaEvolutionException: Found new column 'loyalty_points'\nnot present in the target schema\n</code></pre></p>"},{"location":"patterns/anti_patterns/#why-its-bad_9","title":"Why It's Bad","text":"<p>APIs and source systems change without warning.</p> <p>Vendors add fields, rename columns, or change types. Without explicit schema handling, your pipeline breaks whenever this happens\u2014usually at 3 AM on a weekend.</p>"},{"location":"patterns/anti_patterns/#what-to-do-instead_9","title":"\u2705 What to Do Instead","text":"<pre><code># GOOD: Explicit schema evolution handling\nnodes:\n  - name: \"load_api_data\"\n    read:\n      connection: api\n      endpoint: /customers\n\n    write:\n      connection: bronze\n      path: customers\n      format: delta\n      # \u2705 Allow schema to grow\n      delta_options:\n        mergeSchema: true\n\n    # \u2705 Or use schema_policy for fine control\n    schema_policy:\n      on_new_column: add       # Add new columns automatically\n      on_missing_column: warn  # Log warning but continue\n      on_type_mismatch: error  # Fail on type changes (dangerous!)\n</code></pre> <p>For production, consider schema contracts:</p> <pre><code># GOOD: Schema contract catches unexpected changes\ncontracts:\n  - type: schema\n    expected:\n      - column: id\n        type: integer\n        nullable: false\n      - column: name\n        type: string\n        nullable: false\n      - column: loyalty_points\n        type: integer\n        nullable: true  # We know about this field\n    on_extra_columns: warn  # New fields trigger warning, not failure\n</code></pre>"},{"location":"patterns/anti_patterns/#quick-reference-anti-patterns-cheat-sheet","title":"Quick Reference: Anti-Patterns Cheat Sheet","text":"Anti-Pattern Why It's Bad Do This Instead Transforming in Bronze Lose original data Keep Bronze raw, transform in Silver No dedup before SCD2 History table explodes Deduplicate first, then SCD2 SCD2 on fact tables Facts are immutable events Use append or merge Hardcoded paths Breaks across environments Use connections + env vars NULLs in join keys NULL \u2260 NULL, rows get dropped Handle NULLs with placeholders Business logic everywhere Can't debug, inconsistent Centralize in Silver Bronze \u2192 Gold directly Cleaning repeated everywhere Use Silver as single source of truth No _extracted_at Can't debug timing Add metadata columns Append without dedup Reruns create duplicates Use merge or deduplicate downstream Ignore schema changes Pipeline breaks on changes Use mergeSchema or schema_policy"},{"location":"patterns/anti_patterns/#next-steps","title":"Next Steps","text":"<ul> <li>SCD2 Troubleshooting</li> <li>Validation Patterns</li> <li>Best Practices Guide</li> <li>Troubleshooting Guide</li> </ul>"},{"location":"patterns/append_only_raw/","title":"Pattern: Append-Only Raw Layer (Landing \u2192 Raw)","text":"<p>Status: Core Pattern Layer: Raw (Bronze) Engine: Spark Structured Streaming or Batch Write Mode: <code>append</code> </p>"},{"location":"patterns/append_only_raw/#problem","title":"Problem","text":"<p>You have source data coming from multiple places (SQL databases, APIs, files). You need to: - Preserve the complete history of what arrived and when - Enable replay/reconstruction if downstream logic breaks - Avoid data loss from overwriting mistakes</p>"},{"location":"patterns/append_only_raw/#solution","title":"Solution","text":"<p>Append every piece of data you receive to the Raw layer without modification. Raw is the immutable audit log.</p>"},{"location":"patterns/append_only_raw/#key-principles","title":"Key Principles","text":"<ol> <li>Raw is Sacred \u2192 Never delete or overwrite Raw data</li> <li>Append-Only \u2192 Each run appends new rows (possibly duplicates)</li> <li>One-to-One Copy \u2192 Raw mirrors the source schema, nothing more</li> <li>Permanent Retention \u2192 Keep forever; storage is cheap</li> </ol>"},{"location":"patterns/append_only_raw/#pattern-flow","title":"Pattern Flow","text":"<pre><code>Source System (SQL, API, Files)\n         \u2193\n    Read Data\n         \u2193\n   Append to Raw\n    (append mode)\n         \u2193\nRaw Layer (history preserved)\n         \u2193\n[Next: Merge into Silver for dedup/cleanup]\n</code></pre>"},{"location":"patterns/append_only_raw/#example-sql-source-raw-layer","title":"Example: SQL Source \u2192 Raw Layer","text":""},{"location":"patterns/append_only_raw/#scenario","title":"Scenario","text":"<p>You have a SQL table <code>dbo.orders</code> with new/updated rows arriving daily.</p> <p>First Run (Day 1):</p> <p>Source (<code>dbo.orders</code>): <pre><code>order_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n</code></pre></p> <p>After Append to Raw: <pre><code>Raw.orders (Delta Table)\norder_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n</code></pre></p> <p>Second Run (Day 2):</p> <p>Source now has 2 new rows + 1 duplicate: <pre><code>order_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00  \u2190 Already seen\n2        | Widget B     | 5   | 2025-11-01 11:00  \u2190 Already seen\n3        | Widget A     | 2   | 2025-11-01 12:00  \u2190 Already seen\n4        | Widget C     | 8   | 2025-11-02 09:00  \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00  \u2190 NEW\n</code></pre></p> <p>After Append to Raw: <pre><code>Raw.orders (all 8 rows)\norder_id | product      | qty | created_at\n---------|--------------|-----|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00\n2        | Widget B     | 5   | 2025-11-01 11:00\n3        | Widget A     | 2   | 2025-11-01 12:00\n1        | Widget A     | 10  | 2025-11-01 10:00  \u2190 DUPLICATE (OK)\n2        | Widget B     | 5   | 2025-11-01 11:00  \u2190 DUPLICATE (OK)\n3        | Widget A     | 2   | 2025-11-01 12:00  \u2190 DUPLICATE (OK)\n4        | Widget C     | 8   | 2025-11-02 09:00\n5        | Widget B     | 12  | 2025-11-02 10:00\n</code></pre></p> <p>Note: Duplicates in Raw are OK. Silver's merge will deduplicate them.</p>"},{"location":"patterns/append_only_raw/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/append_only_raw/#using-high-water-mark-incremental","title":"Using High Water Mark (Incremental)","text":"<pre><code>pipelines:\n  - pipeline: sql_to_raw\n    layer: bronze\n    nodes:\n      - id: load_orders_raw\n        name: \"Load Orders to Raw (Bronze)\"\n        read:\n          connection: sql_prod\n          format: sql\n          table: dbo.orders\n          options:\n            query: |\n              SELECT * FROM dbo.orders\n              WHERE COALESCE(updated_at, created_at) &gt; (\n                SELECT COALESCE(MAX(COALESCE(updated_at, created_at)), '1900-01-01')\n                FROM raw.orders\n              )\n        write:\n          connection: adls_prod\n          format: delta\n          table: raw.orders\n          mode: append\n</code></pre>"},{"location":"patterns/append_only_raw/#using-scheduled-batch-full-rescan","title":"Using Scheduled Batch (Full Rescan)","text":"<p>If your source doesn't have timestamps, rescan everything and append:</p> <pre><code>      - id: load_orders_raw\n        name: \"Load Orders to Raw\"\n        read:\n          connection: sql_prod\n          format: sql\n          table: dbo.orders\n        write:\n          connection: adls_prod\n          format: delta\n          table: raw.orders\n          mode: append\n</code></pre> <p>Warning: This will append the entire table each run, creating duplicates. Use High Water Mark pattern when possible.</p>"},{"location":"patterns/append_only_raw/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/append_only_raw/#advantages","title":"Advantages","text":"<p>\u2713 Complete audit trail (when did each row arrive?) \u2713 Safe replay/reconstruction if Silver breaks \u2713 Simple logic (no deduplication, no logic) \u2713 Idempotent (rerun is safe; creates duplicates but that's OK)  </p>"},{"location":"patterns/append_only_raw/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Storage grows unbounded (mitigated by cheap cloud storage) \u2717 Duplicates must be handled downstream (Silver's job) \u2717 Full rescans can be slow without timestamp filtering  </p>"},{"location":"patterns/append_only_raw/#when-to-use","title":"When to Use","text":"<p>Always use Append-Only for Raw ingestion. No exceptions.</p>"},{"location":"patterns/append_only_raw/#when-not-to-use","title":"When NOT to Use","text":"<p>Never overwrite Raw. Never delete from Raw. Never use <code>merge</code> in Raw.</p>"},{"location":"patterns/append_only_raw/#related-patterns","title":"Related Patterns","text":"<ul> <li>High Water Mark \u2192 Efficiently filter SQL sources for incremental reads</li> <li>Merge/Upsert \u2192 Deduplicate Raw data in Silver</li> </ul>"},{"location":"patterns/date_dimension/","title":"Date Dimension Pattern","text":"<p>The <code>date_dimension</code> pattern generates a complete date dimension table with pre-calculated attributes useful for BI/reporting.</p>"},{"location":"patterns/date_dimension/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The date dimension pattern is unique - it generates data rather than transforming it. No <code>read:</code> block is needed.</p> <pre><code>project: my_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    nodes:\n      - name: dim_date\n        # No read block - pattern generates data\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2020-01-01\"\n            end_date: \"2030-12-31\"\n            fiscal_year_start_month: 7  # July fiscal year\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/date_dimension/#features","title":"Features","text":"<ul> <li>Date range generation from start_date to end_date</li> <li>Fiscal calendar support with configurable fiscal year start month</li> <li>19 pre-calculated columns for flexible analysis</li> <li>Unknown member row (date_sk=0) for orphan FK handling</li> <li>Works with both Spark and Pandas</li> </ul>"},{"location":"patterns/date_dimension/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>start_date</code> str Yes - Start date in YYYY-MM-DD format <code>end_date</code> str Yes - End date in YYYY-MM-DD format <code>fiscal_year_start_month</code> int No 1 Month when fiscal year starts (1-12) <code>unknown_member</code> bool No false Add unknown date row with date_sk=0"},{"location":"patterns/date_dimension/#generated-columns","title":"Generated Columns","text":"<p>The pattern generates 19 columns automatically:</p> Column Type Description Example <code>date_sk</code> int Surrogate key (YYYYMMDD format) 20240115 <code>full_date</code> date The actual date 2024-01-15 <code>day_of_week</code> str Day name Monday <code>day_of_week_num</code> int Day number (1=Monday, 7=Sunday) 1 <code>day_of_month</code> int Day of month (1-31) 15 <code>day_of_year</code> int Day of year (1-366) 15 <code>is_weekend</code> bool Weekend flag false <code>week_of_year</code> int ISO week number (1-53) 3 <code>month</code> int Month number (1-12) 1 <code>month_name</code> str Month name January <code>quarter</code> int Calendar quarter (1-4) 1 <code>quarter_name</code> str Quarter name Q1 <code>year</code> int Calendar year 2024 <code>fiscal_year</code> int Fiscal year 2024 <code>fiscal_quarter</code> int Fiscal quarter (1-4) 3 <code>is_month_start</code> bool First day of month false <code>is_month_end</code> bool Last day of month false <code>is_year_start</code> bool First day of year false <code>is_year_end</code> bool Last day of year false"},{"location":"patterns/date_dimension/#fiscal-calendar-configuration","title":"Fiscal Calendar Configuration","text":"<p>Configure fiscal year start month for companies with non-calendar fiscal years:</p> <pre><code>nodes:\n  - name: dim_date\n    pattern:\n      type: date_dimension\n      params:\n        start_date: \"2020-01-01\"\n        end_date: \"2030-12-31\"\n        fiscal_year_start_month: 7  # July 1st = FY start\n    write:\n      connection: warehouse\n      path: dim_date\n      mode: overwrite\n</code></pre> <p>Fiscal Year Calculation: - If <code>fiscal_year_start_month = 7</code> (July) - July 2024 \u2192 FY 2025 - June 2024 \u2192 FY 2024</p> <p>Fiscal Quarter Calculation: - Fiscal Q1: July, August, September - Fiscal Q2: October, November, December - Fiscal Q3: January, February, March - Fiscal Q4: April, May, June</p>"},{"location":"patterns/date_dimension/#unknown-member-row","title":"Unknown Member Row","text":"<p>Enable <code>unknown_member: true</code> to add a special row for orphan FK handling:</p> Column Value date_sk 0 full_date 1900-01-01 day_of_week Unknown day_of_week_num 0 month_name Unknown quarter_name Unknown year 0"},{"location":"patterns/date_dimension/#full-yaml-example","title":"Full YAML Example","text":"<p>Complete date dimension in a warehouse pipeline:</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_reference_dimensions\n    description: \"Build date and other reference dimensions\"\n    nodes:\n      - name: dim_date\n        description: \"Standard date dimension with fiscal calendar\"\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2015-01-01\"\n            end_date: \"2035-12-31\"\n            fiscal_year_start_month: 10  # October fiscal year (retail)\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n          partition_by: [year]  # Optional: partition by year\n</code></pre>"},{"location":"patterns/date_dimension/#common-fiscal-year-configurations","title":"Common Fiscal Year Configurations","text":""},{"location":"patterns/date_dimension/#retail-calendar-october-fy","title":"Retail Calendar (October FY)","text":"<pre><code>fiscal_year_start_month: 10\n</code></pre>"},{"location":"patterns/date_dimension/#government-calendar-october-fy","title":"Government Calendar (October FY)","text":"<pre><code>fiscal_year_start_month: 10\n</code></pre>"},{"location":"patterns/date_dimension/#education-calendar-july-fy","title":"Education Calendar (July FY)","text":"<pre><code>fiscal_year_start_month: 7\n</code></pre>"},{"location":"patterns/date_dimension/#standard-calendar-year","title":"Standard Calendar Year","text":"<pre><code>fiscal_year_start_month: 1  # Default\n</code></pre>"},{"location":"patterns/date_dimension/#complete-configuration-reference","title":"Complete Configuration Reference","text":""},{"location":"patterns/date_dimension/#all-parameters","title":"All Parameters","text":"Parameter Type Required Default Description <code>start_date</code> str Yes - Start date in YYYY-MM-DD format <code>end_date</code> str Yes - End date in YYYY-MM-DD format <code>fiscal_year_start_month</code> int No 1 Month when fiscal year starts (1-12) <code>unknown_member</code> bool No false Add unknown date row with date_sk=0 <code>date_format</code> str No \"%Y-%m-%d\" Format string for date parsing <code>week_start_day</code> int No 1 First day of week (1=Monday, 7=Sunday) <code>include_holidays</code> bool No false Generate is_holiday column (requires holiday_country) <code>holiday_country</code> str No \"US\" Country code for holiday calendar"},{"location":"patterns/date_dimension/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code>nodes:\n  - name: dim_date\n    pattern:\n      type: date_dimension\n      params:\n        # Required\n        start_date: \"2015-01-01\"\n        end_date: \"2035-12-31\"\n\n        # Fiscal calendar\n        fiscal_year_start_month: 7      # July fiscal year\n\n        # Unknown member\n        unknown_member: true            # Add SK=0 row for orphans\n\n        # Week configuration\n        week_start_day: 1               # Monday (ISO standard)\n\n        # Holiday support (if holidays package installed)\n        include_holidays: true\n        holiday_country: \"US\"\n    write:\n      connection: warehouse\n      path: dim_date\n      format: delta\n      mode: overwrite\n</code></pre>"},{"location":"patterns/date_dimension/#output-column-customization","title":"Output Column Customization","text":"<p>To customize output columns, add a SQL step after generation:</p> <pre><code>nodes:\n  - name: dim_date_raw\n    pattern:\n      type: date_dimension\n      params:\n        start_date: \"2020-01-01\"\n        end_date: \"2030-12-31\"\n        fiscal_year_start_month: 10\n        unknown_member: true\n\n  - name: dim_date\n    depends_on: [dim_date_raw]\n    transform:\n      steps:\n        - sql: |\n            SELECT \n              date_sk,\n              full_date,\n              day_of_week,\n              month_name,\n              quarter_name,\n              year,\n              fiscal_year,\n              fiscal_quarter,\n              is_weekend,\n              -- Custom columns\n              CONCAT(year, '-', LPAD(month, 2, '0')) AS year_month,\n              CASE WHEN month IN (11, 12) THEN true ELSE false END AS is_holiday_season\n            FROM dim_date_raw\n    write:\n      connection: warehouse\n      path: dim_date\n</code></pre>"},{"location":"patterns/date_dimension/#see-also","title":"See Also","text":"<ul> <li>Dimension Pattern - Build regular dimensions</li> <li>Fact Pattern - Build fact tables with SK lookups</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/dimension/","title":"Dimension Pattern","text":"<p>The <code>dimension</code> pattern builds complete dimension tables with automatic surrogate key generation and SCD (Slowly Changing Dimension) support.</p>"},{"location":"patterns/dimension/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>Patterns are used via the <code>pattern:</code> block in a node config. The pattern type goes in <code>pattern: type:</code> and configuration goes in <code>pattern: params:</code>.</p> <pre><code>project: my_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dimensions\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n          format: delta\n\n        # Use dimension pattern\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols: [name, email, address]\n            target: warehouse.dim_customer\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#features","title":"Features","text":"<ul> <li>Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows)</li> <li>SCD Type 0 (static - never update existing records)</li> <li>SCD Type 1 (overwrite - update in place, no history)</li> <li>SCD Type 2 (history tracking - full audit trail)</li> <li>Unknown member row (SK=0) for orphan FK handling</li> <li>Audit columns (load_timestamp, source_system)</li> </ul>"},{"location":"patterns/dimension/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>natural_key</code> str Yes - Natural/business key column name <code>surrogate_key</code> str Yes - Surrogate key column name to generate <code>scd_type</code> int No 1 0=static, 1=overwrite, 2=history tracking <code>track_cols</code> list For SCD1/2 - Columns to track for changes <code>target</code> str For SCD2 - Target table path (required to read existing history) <code>unknown_member</code> bool No false Insert a row with SK=0 for orphan FK handling <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/dimension/#audit-config","title":"Audit Config","text":"<pre><code>params:\n  # ... other params ...\n  audit:\n    load_timestamp: true      # Add load_timestamp column\n    source_system: \"pos\"      # Add source_system column with this value\n</code></pre>"},{"location":"patterns/dimension/#scd-type-0-static","title":"SCD Type 0 (Static)","text":"<p>Static dimensions never update existing records. Only new records (not matching natural key) are inserted.</p> <p>Use case: Reference data that never changes (ISO country codes, fixed lookup values).</p> <pre><code>nodes:\n  - name: dim_country\n    read:\n      connection: staging\n      path: countries\n    pattern:\n      type: dimension\n      params:\n        natural_key: country_code\n        surrogate_key: country_sk\n        scd_type: 0\n        target: warehouse.dim_country\n    write:\n      connection: warehouse\n      path: dim_country\n      mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#scd-type-1-overwrite","title":"SCD Type 1 (Overwrite)","text":"<p>Overwrite dimensions update existing records in place. No history is kept.</p> <p>Use case: Attributes where you only care about the current value (customer email, product price).</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: staging\n      path: customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id\n        surrogate_key: customer_sk\n        scd_type: 1\n        track_cols: [name, email, address]\n        target: warehouse.dim_customer\n        audit:\n          load_timestamp: true\n    write:\n      connection: warehouse\n      path: dim_customer\n      mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#scd-type-2-history-tracking","title":"SCD Type 2 (History Tracking)","text":"<p>History-tracking dimensions preserve full audit trail. Old records are closed, new versions are opened.</p> <p>Use case: Slowly changing attributes where historical accuracy matters (customer address for point-in-time reporting).</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: staging\n      path: customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id\n        surrogate_key: customer_sk\n        scd_type: 2\n        track_cols: [name, email, address, city, state]\n        target: warehouse.dim_customer\n        valid_from_col: valid_from     # Optional, default: valid_from\n        valid_to_col: valid_to         # Optional, default: valid_to\n        is_current_col: is_current     # Optional, default: is_current\n        unknown_member: true\n        audit:\n          load_timestamp: true\n          source_system: \"crm\"\n    write:\n      connection: warehouse\n      path: dim_customer\n      mode: overwrite\n</code></pre> <p>Generated Columns: - <code>valid_from</code>: Timestamp when this version became active - <code>valid_to</code>: Timestamp when this version was superseded (NULL for current) - <code>is_current</code>: Boolean flag (true for current version)</p>"},{"location":"patterns/dimension/#unknown-member-handling","title":"Unknown Member Handling","text":"<p>Enable <code>unknown_member: true</code> to automatically insert a row with SK=0. This allows fact tables to reference unknown dimensions without FK violations.</p> <p>Generated Unknown Member Row:</p> customer_sk customer_id name email valid_from is_current 0 -1 Unknown Unknown 1900-01-01 true"},{"location":"patterns/dimension/#full-star-schema-example","title":"Full Star Schema Example","text":"<p>Complete pipeline building dimensions for a star schema:</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  - pipeline: build_dimensions\n    nodes:\n      # Customer dimension with SCD2\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n          format: delta\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols:\n              - name\n              - email\n              - phone\n              - address_line_1\n              - city\n              - state\n              - postal_code\n            target: warehouse.dim_customer\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"salesforce\"\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n          mode: overwrite\n\n      # Product dimension with SCD1 (no history)\n      - name: dim_product\n        read:\n          connection: staging\n          path: products\n          format: delta\n        pattern:\n          type: dimension\n          params:\n            natural_key: product_id\n            surrogate_key: product_sk\n            scd_type: 1\n            track_cols: [name, category, price, status]\n            target: warehouse.dim_product\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_product\n          format: delta\n          mode: overwrite\n\n      # Date dimension (generated, no source read needed)\n      - name: dim_date\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2020-01-01\"\n            end_date: \"2030-12-31\"\n            fiscal_year_start_month: 7\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/dimension/#python-api","title":"Python API","text":"<pre><code>from odibi.patterns.dimension import DimensionPattern\nfrom odibi.context import EngineContext\n\n# Create pattern instance\npattern = DimensionPattern(\n    engine=my_engine,\n    config=node_config  # NodeConfig with params\n)\n\n# Or directly with params dict\nfrom odibi.patterns.dimension import DimensionPattern\n\npattern = DimensionPattern(params={\n    \"natural_key\": \"customer_id\",\n    \"surrogate_key\": \"customer_sk\",\n    \"scd_type\": 2,\n    \"track_cols\": [\"name\", \"email\", \"address\"],\n    \"target\": \"gold.dim_customer\",\n    \"unknown_member\": True,\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"crm\"\n    }\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern\ncontext = EngineContext(df=source_df, engine_type=EngineType.SPARK)\nresult_df = pattern.execute(context)\n</code></pre>"},{"location":"patterns/dimension/#see-also","title":"See Also","text":"<ul> <li>Date Dimension Pattern - Generate date dimensions</li> <li>Fact Pattern - Build fact tables with SK lookups</li> <li>Aggregation Pattern - Build aggregate tables</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/fact/","title":"Fact Pattern","text":"<p>The <code>fact</code> pattern builds fact tables with automatic surrogate key lookups from dimension tables, orphan handling, grain validation, and measure calculations.</p>"},{"location":"patterns/fact/#integration-with-odibi-yaml","title":"Integration with Odibi YAML","text":"<p>The fact pattern looks up dimension tables from context - dimensions must be registered (either by running dimension nodes in the same pipeline with <code>depends_on</code>, or by reading them from storage).</p> <pre><code>project: sales_warehouse\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_star_schema\n    nodes:\n      # First, build or load dimensions\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: delta\n        # Just loading - no transform needed\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: delta\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: delta\n\n      # Then build fact table with SK lookups\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: staging\n          path: orders\n          format: delta\n\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, line_item_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer  # References node name\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - unit_price\n              - line_total: \"quantity * unit_price\"\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: delta\n          mode: overwrite\n</code></pre>"},{"location":"patterns/fact/#features","title":"Features","text":"<ul> <li>Automatic SK lookups from dimension tables</li> <li>Orphan handling (unknown member, reject, or quarantine)</li> <li>Grain validation (detect duplicates at PK level)</li> <li>Deduplication support</li> <li>Measure calculations and renaming</li> <li>Audit columns (load_timestamp, source_system)</li> <li>SCD2 dimension support (filter to is_current=true)</li> </ul>"},{"location":"patterns/fact/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>grain</code> list No - Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No [] Dimension lookup configurations <code>orphan_handling</code> str No \"unknown\" \"unknown\", \"reject\", or \"quarantine\" <code>measures</code> list No [] Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No false Remove duplicates before insert <code>keys</code> list Required if deduplicate - Keys for deduplication <code>audit</code> dict No {} Audit column configuration"},{"location":"patterns/fact/#dimension-lookup-config","title":"Dimension Lookup Config","text":"<pre><code>params:\n  dimensions:\n    - source_column: customer_id     # Column in source data\n      dimension_table: dim_customer  # Node name in context\n      dimension_key: customer_id     # Natural key column in dimension\n      surrogate_key: customer_sk     # Surrogate key to retrieve\n      scd2: true                     # If true, filter is_current=true\n</code></pre>"},{"location":"patterns/fact/#measures-config","title":"Measures Config","text":"<pre><code>params:\n  measures:\n    - quantity                           # Passthrough\n    - revenue: total_amount              # Rename\n    - line_total: \"quantity * unit_price\" # Calculate\n</code></pre>"},{"location":"patterns/fact/#orphan-handling","title":"Orphan Handling","text":"<p>Three strategies for handling source records that don't match any dimension:</p>"},{"location":"patterns/fact/#1-unknown-default","title":"1. Unknown (Default)","text":"<p>Map orphans to the unknown member (SK=0): <pre><code>orphan_handling: unknown\n</code></pre></p>"},{"location":"patterns/fact/#2-reject","title":"2. Reject","text":"<p>Fail the pipeline if any orphans exist: <pre><code>orphan_handling: reject\n</code></pre></p>"},{"location":"patterns/fact/#3-quarantine","title":"3. Quarantine","text":"<p>Route orphans to quarantine table: <pre><code>orphan_handling: quarantine\n</code></pre></p>"},{"location":"patterns/fact/#grain-validation","title":"Grain Validation","text":"<p>Define the fact table grain to detect duplicate records:</p> <pre><code>params:\n  grain: [order_id, line_item_id]\n</code></pre> <p>If duplicates exist, the pattern raises an error with details.</p>"},{"location":"patterns/fact/#full-star-schema-example","title":"Full Star Schema Example","text":"<p>Complete pipeline building dimensions AND fact tables:</p> <pre><code>project: sales_star_schema\nengine: spark\n\nconnections:\n  staging:\n    type: delta\n    path: /mnt/staging\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  # Pipeline 1: Build dimensions\n  - pipeline: build_dimensions\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols: [name, email, region]\n            target: warehouse.dim_customer\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_customer\n          mode: overwrite\n\n      - name: dim_product\n        read:\n          connection: staging\n          path: products\n        pattern:\n          type: dimension\n          params:\n            natural_key: product_id\n            surrogate_key: product_sk\n            scd_type: 1\n            track_cols: [name, category, price]\n            target: warehouse.dim_product\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_product\n          mode: overwrite\n\n      - name: dim_date\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2020-01-01\"\n            end_date: \"2030-12-31\"\n            unknown_member: true\n        write:\n          connection: warehouse\n          path: dim_date\n          mode: overwrite\n\n  # Pipeline 2: Build fact table (depends on dimensions existing)\n  - pipeline: build_facts\n    nodes:\n      # Load dimensions into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n\n      # Build fact table\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: staging\n          path: orders\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, line_item_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - unit_price\n              - discount_amount\n              - line_total: \"quantity * unit_price\"\n              - net_amount: \"quantity * unit_price - discount_amount\"\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n        write:\n          connection: warehouse\n          path: fact_orders\n          mode: overwrite\n</code></pre>"},{"location":"patterns/fact/#see-also","title":"See Also","text":"<ul> <li>Dimension Pattern - Build dimensions with SCD support</li> <li>Date Dimension Pattern - Generate date dimensions</li> <li>Aggregation Pattern - Build aggregate tables</li> <li>FK Validation - Additional FK validation</li> <li>YAML Schema Reference - Full configuration reference</li> </ul>"},{"location":"patterns/incremental_stateful/","title":"Stateful Incremental Loading","text":"<p>Stateful Incremental Loading is the \"Auto-Pilot\" mode for ingestion. Unlike Smart Read (Rolling Window) which blindly looks back X days, Stateful Mode remembers exactly where it left off.</p> <p>It tracks the High Water Mark (HWM)\u2014the maximum value of a column (e.g., <code>updated_at</code> or <code>id</code>) seen in the previous run\u2014and only fetches records greater than that value.</p>"},{"location":"patterns/incremental_stateful/#when-to-use-this","title":"When to use this?","text":"<ul> <li>CDC-like Ingestion: You want to sync a large table and only get new rows.</li> <li>Exactness: You don't want to guess a lookback window (e.g., \"3 days just to be safe\").</li> <li>Performance: You want to query the absolute minimum data required.</li> </ul>"},{"location":"patterns/incremental_stateful/#configuration","title":"Configuration","text":"<p>Enable it by setting <code>mode: stateful</code> in the <code>incremental</code> block.</p> <pre><code>- name: \"ingest_orders\"\n  read:\n    connection: \"postgres_prod\"\n    format: \"sql\"\n    table: \"public.orders\"\n\n    incremental:\n      mode: \"stateful\"              # Enable State Tracking\n      column: \"updated_at\"      # Column to track (max value is saved)\n      fallback_column: \"created_at\" # Optional: Use this if key_column is NULL\n      watermark_lag: \"30m\"          # Safety buffer (overlaps the window)\n      state_key: \"orders_ingest\"    # Optional: Custom ID for the state file\n\n  write:\n    connection: \"bronze\"\n    format: \"delta\"\n    table: \"orders_bronze\"\n    mode: \"append\"\n</code></pre>"},{"location":"patterns/incremental_stateful/#how-it-works","title":"How It Works","text":"<ol> <li> <p>First Run (Bootstrap)</p> <ul> <li>Odibi checks the state backend (Delta table or local JSON).</li> <li>No state found? \u2192 Full Load (<code>SELECT * FROM table</code>).</li> <li>After success, it saves <code>MAX(updated_at)</code> as the HWM.</li> </ul> </li> <li> <p>Subsequent Runs (Incremental)</p> <ul> <li>Odibi retrieves the last HWM (e.g., <code>2023-10-25 10:00:00</code>).</li> <li>It subtracts the <code>watermark_lag</code> (e.g., 30 mins) \u2192 <code>09:30:00</code>.</li> <li>Generates query: <code>SELECT * FROM table WHERE updated_at &gt; '2023-10-25 09:30:00'</code>.</li> <li>After success, it updates the HWM with the new maximum from the fetched batch.</li> </ul> </li> </ol>"},{"location":"patterns/incremental_stateful/#key-features","title":"Key Features","text":""},{"location":"patterns/incremental_stateful/#watermark-lag","title":"\ud83c\udf0a Watermark Lag","text":"<p>Data often arrives late or out of order. If you run your pipeline at 10:00, you might miss a record timestamped 09:59 that gets committed at 10:01.</p> <p>The <code>watermark_lag</code> creates a safety overlap. *   Lag: \"30m\" implies: \"Give me everything since the last run, but re-read the last 30 minutes just in case.\" *   This ensures At-Least-Once delivery. *   Note: This causes duplicates in the Bronze layer. This is expected! Your Silver layer (Merge/Upsert) handles deduplication.</p>"},{"location":"patterns/incremental_stateful/#state-backends","title":"\ud83d\udee1\ufe0f State Backends","text":"<p>Odibi automatically chooses the best backend: *   Spark/Databricks: Uses a Delta table (<code>odibi_meta.state</code>) to track HWMs. This is robust and supports concurrency. *   Pandas/Local: Uses a local JSON file (<code>.odibi/state.json</code>).</p>"},{"location":"patterns/incremental_stateful/#resets","title":"\ud83d\udd04 Resets","text":"<p>To reset the state and force a full reload: 1.  Delete the target table/file. 2.  Clear the state entry (manually or via CLI - CLI command coming soon).</p>"},{"location":"patterns/incremental_stateful/#date-format-for-string-columns","title":"Date Format for String Columns","text":"<p>If your date column is stored as a string (e.g., Oracle DD-MON-YY format), use the <code>date_format</code> option:</p> <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"EVENT_TIME\"\n  date_format: \"oracle\"  # Handles '20-APR-24 07:11:01.0'\n</code></pre> <p>Supported values: <code>oracle</code>, <code>sql_server</code>, <code>us</code>, <code>eu</code>, <code>iso</code>. See Smart Read for details.</p>"},{"location":"patterns/incremental_stateful/#comparison-rolling-window-vs-stateful","title":"Comparison: Rolling Window vs. Stateful","text":"Feature Rolling Window (<code>smart_read</code>) Stateful (<code>stateful</code>) Logic <code>NOW() - lookback</code> <code>&gt; Last HWM</code> State Stateless (Time-based) Stateful (Persisted) Best For Reporting windows (\"Last 30 days\") Ingestion / Replication (\"Sync table\") Complexity Low Medium Safety Good (if lookback is large) Excellent (Exact tracking)"},{"location":"patterns/incremental_stateful/#example-cdc-ingestion-pipeline","title":"Example: CDC Ingestion Pipeline","text":"<p>Here is a robust pattern for database replication:</p> <pre><code>nodes:\n  # 1. Ingest (Bronze) - Accumulates history with duplicates\n  - name: \"ingest_users\"\n    read:\n      connection: \"db_prod\"\n      table: \"users\"\n      incremental:\n        mode: \"stateful\"\n        key_column: \"updated_at\"\n        watermark_lag: \"15m\"\n    write:\n      connection: \"lake\"\n      format: \"delta\"\n      table: \"bronze_users\"\n      mode: \"append\"\n\n  # 2. Merge (Silver) - Deduplicates and keeps current state\n  - name: \"dim_users\"\n    depends_on: [\"ingest_users\"] # Reads ONLY the new batch\n    transformer: \"merge\"\n    params:\n      keys: [\"user_id\"]\n      order_by: \"updated_at DESC\"\n    write:\n      connection: \"lake\"\n      format: \"delta\"\n      table: \"silver_users\"\n      mode: \"upsert\"\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/","title":"[Legacy] Manual High Water Mark (HWM)","text":"<p>\u26a0\ufe0f Deprecated Pattern</p> <p>This manual pattern is no longer recommended. Please use the new Stateful Incremental Loading feature which handles this automatically.</p>"},{"location":"patterns/legacy_hwm_manual/#what-is-hwm","title":"What Is HWM?","text":"<p>A pattern for incremental data loading: load all data once on the first run, then load only new/changed data on each subsequent run.</p> <p>Day 1: Load 10 years of history Day 2+: Load only today's new records</p>"},{"location":"patterns/legacy_hwm_manual/#the-pattern","title":"The Pattern","text":""},{"location":"patterns/legacy_hwm_manual/#configuration","title":"Configuration","text":"<pre><code>nodes:\n  load_orders:\n    read:\n      connection: my_sql_server\n      format: sql_server\n      query: |\n        SELECT\n          order_id,\n          customer_id,\n          amount,\n          created_at,\n          updated_at\n        FROM dbo.orders\n        WHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n\n    write:\n      connection: adls\n      format: delta\n      path: bronze/orders\n      mode: append\n      first_run_query: |\n        SELECT\n          order_id,\n          customer_id,\n          amount,\n          created_at,\n          updated_at\n        FROM dbo.orders\n      options:\n        cluster_by: [created_at]\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#how-it-works","title":"How It Works","text":"<p>Day 1 (table doesn't exist): - Odibi runs <code>first_run_query</code> - Loads ALL orders from source - Creates table with clustering - Write mode: OVERWRITE</p> <p>Day 2+ (table exists): - Odibi runs regular <code>query</code> - Loads only orders modified in last 1 day - Appends to table - Write mode: APPEND</p>"},{"location":"patterns/legacy_hwm_manual/#the-two-queries","title":"The Two Queries","text":""},{"location":"patterns/legacy_hwm_manual/#first_run_query","title":"<code>first_run_query</code>","text":"<p><pre><code>SELECT * FROM dbo.orders\n</code></pre> - Loads everything - Runs once on day 1 - Takes longer, but happens only once</p>"},{"location":"patterns/legacy_hwm_manual/#query-incremental","title":"<code>query</code> (incremental)","text":"<p><pre><code>SELECT * FROM dbo.orders\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n</code></pre> - Loads only new/updated records - Runs every day from day 2 onward - Takes seconds</p>"},{"location":"patterns/legacy_hwm_manual/#key-concepts","title":"Key Concepts","text":""},{"location":"patterns/legacy_hwm_manual/#1-track-changes","title":"1. Track Changes","text":"<p>Use <code>updated_at</code> if available. If not, use <code>created_at</code>.</p> <pre><code>-- Prefer updated_at (catches changes)\nWHERE updated_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- Fallback to created_at (new records only)\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- Use both (catches new + modified)\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#2-time-ranges","title":"2. Time Ranges","text":"<p>Use overlapping time ranges (2 days instead of 1) to catch late-arriving data:</p> <pre><code>-- 1 day: misses records that arrived late\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n\n-- 2 days: safer, catches late arrivals\nWHERE created_at &gt;= DATEADD(DAY, -2, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#3-clustering","title":"3. Clustering","text":"<p>On first run, apply clustering for query performance:</p> <pre><code>options:\n  cluster_by: [created_at]  # Applied day 1, speeds up incremental queries\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#setup-steps","title":"Setup Steps","text":"<ol> <li>Identify HWM column (<code>updated_at</code> or <code>created_at</code>)</li> <li>Write first_run_query (<code>SELECT * FROM table</code>)</li> <li>Write incremental query (<code>SELECT * WHERE timestamp &gt;= date_function</code>)</li> <li>Add options (<code>cluster_by</code> for performance)</li> <li>Deploy and let Odibi handle the rest</li> </ol>"},{"location":"patterns/legacy_hwm_manual/#example-orders-table","title":"Example: Orders Table","text":""},{"location":"patterns/legacy_hwm_manual/#source-data-dboorders","title":"Source Data (dbo.orders)","text":"<pre><code>order_id | customer_id | amount | created_at          | updated_at\n1        | 100         | 99.99  | 2025-01-20 10:00:00 | NULL\n2        | 101         | 49.99  | 2025-01-21 14:30:00 | 2025-01-21 15:00:00\n3        | 102         | 199.99 | 2025-01-23 09:00:00 | NULL\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#odibi-configuration","title":"Odibi Configuration","text":"<pre><code>nodes:\n  load_orders:\n    read:\n      connection: sql_server\n      format: sql_server\n      query: |\n        SELECT order_id, customer_id, amount, created_at, updated_at\n        FROM dbo.orders\n        WHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, CAST(GETDATE() AS DATE))\n\n    write:\n      connection: adls\n      format: delta\n      path: bronze/orders\n      mode: append\n      first_run_query: |\n        SELECT order_id, customer_id, amount, created_at, updated_at\n        FROM dbo.orders\n      options:\n        cluster_by: [created_at]\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#execution-timeline","title":"Execution Timeline","text":"<p>Day 1 (2025-01-20): <pre><code>Run: first_run_query\nLoads: All 3 orders\nWrite: OVERWRITE (creates table)\nBronze table: 3 rows\n</code></pre></p> <p>Day 2 (2025-01-21): <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-20)\nLoads: Order 1 (updated), Order 2 (new)\nWrite: APPEND\nBronze table: 5 rows (with duplicates of 1, 2)\n</code></pre></p> <p>Day 3 (2025-01-22): <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-21)\nLoads: Order 2 (updated timestamp)\nWrite: APPEND\nBronze table: 6 rows\n</code></pre></p> <p>Day 4 (2025-01-23): <pre><code>Run: Regular query (WHERE COALESCE(...) &gt;= 2025-01-22)\nLoads: Order 3 (new)\nWrite: APPEND\nBronze table: 7 rows\n</code></pre></p>"},{"location":"patterns/legacy_hwm_manual/#handling-duplicates","title":"Handling Duplicates","text":"<p>Since we append each day, you'll get duplicates in Bronze (which is fine\u2014that's what Raw/Bronze is for):</p> <pre><code>order_id | created_at          | load_date\n1        | 2025-01-20 10:00:00 | 2025-01-20  \u2190 Day 1\n1        | 2025-01-20 10:00:00 | 2025-01-21  \u2190 Day 2 (duplicate)\n2        | 2025-01-21 14:30:00 | 2025-01-21  \u2190 Day 2\n2        | 2025-01-21 14:30:00 | 2025-01-22  \u2190 Day 3 (duplicate)\n3        | 2025-01-23 09:00:00 | 2025-01-23  \u2190 Day 4\n</code></pre> <p>Silver layer (merge/upsert) deduplicates later using the merge transformer.</p>"},{"location":"patterns/legacy_hwm_manual/#sql-date-functions-by-database","title":"SQL Date Functions (by Database)","text":"Database Syntax SQL Server <code>DATEADD(DAY, -1, CAST(GETDATE() AS DATE))</code> PostgreSQL <code>CURRENT_DATE - INTERVAL '1 day'</code> MySQL <code>DATE_SUB(CURDATE(), INTERVAL 1 DAY)</code> Snowflake <code>DATEADD(day, -1, CURRENT_DATE())</code> BigQuery <code>DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)</code>"},{"location":"patterns/legacy_hwm_manual/#common-mistakes","title":"Common Mistakes","text":""},{"location":"patterns/legacy_hwm_manual/#single-query-for-everything","title":"\u274c Single query for everything","text":"<pre><code># Wrong: Won't capture first-run history\nquery: WHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#two-queries","title":"\u2705 Two queries","text":"<pre><code>first_run_query: SELECT * FROM table\nquery: WHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-instead-of","title":"\u274c Using <code>&gt;</code> instead of <code>&gt;=</code>","text":"<pre><code>-- Wrong: filters out today's data\nWHERE created_at &gt; DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-for-inclusive-range","title":"\u2705 Using <code>&gt;=</code> for inclusive range","text":"<pre><code>-- Right: includes today\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#only-using-created_at-misses-updates","title":"\u274c Only using created_at (misses updates)","text":"<pre><code>-- Wrong: updated records not captured\nWHERE created_at &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#using-coalesceupdated_at-created_at","title":"\u2705 Using COALESCE(updated_at, created_at)","text":"<pre><code>-- Right: captures new AND updated\nWHERE COALESCE(updated_at, created_at) &gt;= DATEADD(DAY, -1, GETDATE())\n</code></pre>"},{"location":"patterns/legacy_hwm_manual/#thats-it","title":"That's It","text":"<p>HWM is simple: two queries, Odibi chooses which one runs.</p> <ul> <li>First run: Odibi detects table doesn't exist, runs <code>first_run_query</code></li> <li>Subsequent runs: Odibi detects table exists, runs regular <code>query</code></li> <li>Automatic mode override: First run uses OVERWRITE, subsequent runs use APPEND</li> </ul> <p>Works great for loading from any SQL database into Bronze.</p>"},{"location":"patterns/manufacturing/","title":"Manufacturing Phase Analysis Pattern","text":""},{"location":"patterns/manufacturing/#the-problem","title":"The Problem","text":"<p>Manufacturing processes generate time-series sensor data at high frequency (every second or minute). Raw data looks like this:</p> <pre><code>timestamp           | RunID | SetupTime | ProcessTime | Status | Level | Weight\n2024-01-01 10:00:00 | R001  | 0         | 0           | 1      | 10    | 500\n2024-01-01 10:01:00 | R001  | 45        | 0           | 2      | 15    | 500\n2024-01-01 10:02:00 | R001  | 105       | 0           | 2      | 25    | 502\n... (thousands of rows per run)\n</code></pre> <p>But business users need one summary row per run:</p> <pre><code>RunID | ItemCode | SetupTime_start     | SetupTime_end       | SetupTime_max_minutes | SetupTime_active_minutes | ProcessTime_start | ...\nR001  | ITEM-A   | 2024-01-01 09:59:15 | 2024-01-01 10:15:00 | 15.75                 | 12.5                     | 2024-01-01 11:00:00 | ...\n</code></pre> <p>This pattern solves: - Phase boundary detection - When did each phase start and end? - Time-in-state analysis - How long was the equipment idle vs active? - Sequential phase chaining - Each phase starts after the previous ends</p>"},{"location":"patterns/manufacturing/#how-plc-timers-work","title":"How PLC Timers Work","text":"<p>Understanding this is key to using the transformer correctly.</p>"},{"location":"patterns/manufacturing/#the-timer-column","title":"The Timer Column","text":"<p>In manufacturing systems, a PLC (Programmable Logic Controller) tracks phase durations with timer columns. The timer:</p> <ol> <li>Starts at 0 when the phase begins</li> <li>Increments every second while the phase is running</li> <li>Stops (plateaus) when the phase completes</li> </ol> <pre><code>Real timeline:\n10:00:15 - Phase STARTS (timer = 0)\n10:01:15 - Timer = 60 seconds\n10:02:15 - Timer = 120 seconds\n10:05:15 - Phase ENDS (timer stops at 300)\n10:06:00 - Timer still shows 300 (plateau)\n</code></pre>"},{"location":"patterns/manufacturing/#the-polling-problem","title":"The Polling Problem","text":"<p>Your data system polls the PLC at intervals (e.g., every minute). You don't see the exact start - you see:</p> <pre><code>What you capture:\n10:00:00 | SetupTime = 0      \u2190 Phase hasn't started yet\n10:01:00 | SetupTime = 45     \u2190 Phase started ~45 seconds ago!\n10:02:00 | SetupTime = 105    \u2190 Running\n10:05:00 | SetupTime = 285    \u2190 Running\n10:06:00 | SetupTime = 300    \u2190 Stopped\n10:07:00 | SetupTime = 300    \u2190 Still 300 (plateau = phase ended)\n</code></pre>"},{"location":"patterns/manufacturing/#back-calculating-true-start","title":"Back-Calculating True Start","text":"<p>The transformer detects this pattern and back-calculates the true start:</p> <pre><code>First reading: timestamp = 10:01:00, timer = 45 seconds\nTrue start = 10:01:00 - 45 seconds = 10:00:15\n</code></pre>"},{"location":"patterns/manufacturing/#detecting-phase-end","title":"Detecting Phase End","text":"<p>The transformer finds the first repeated value (plateau):</p> <pre><code>10:05:00 | SetupTime = 285\n10:06:00 | SetupTime = 300\n10:07:00 | SetupTime = 300  \u2190 REPEAT! Phase ended at 10:06:00\n</code></pre>"},{"location":"patterns/manufacturing/#basic-usage","title":"Basic Usage","text":""},{"location":"patterns/manufacturing/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>transform:\n  - detect_sequential_phases:\n      group_by: RunID\n      timestamp_col: ts\n      phases:\n        - SetupTime\n        - ProcessTime\n        - CycleTime\n</code></pre> <p>Output columns: - <code>RunID</code> - <code>SetupTime_start</code>, <code>SetupTime_end</code>, <code>SetupTime_max_minutes</code> - <code>ProcessTime_start</code>, <code>ProcessTime_end</code>, <code>ProcessTime_max_minutes</code> - <code>CycleTime_start</code>, <code>CycleTime_end</code>, <code>CycleTime_max_minutes</code></p>"},{"location":"patterns/manufacturing/#with-status-tracking","title":"With Status Tracking","text":"<p>Track how long equipment spent in each state during each phase:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by: RunID\n      timestamp_col: ts\n      phases: [SetupTime, ProcessTime, CycleTime]\n      status_col: Status\n      status_mapping:\n        1: idle\n        2: active\n        3: hold\n        4: faulted\n</code></pre> <p>Additional output columns: - <code>SetupTime_idle_minutes</code>, <code>SetupTime_active_minutes</code>, <code>SetupTime_hold_minutes</code>, <code>SetupTime_faulted_minutes</code> - <code>ProcessTime_idle_minutes</code>, <code>ProcessTime_active_minutes</code>, ...</p>"},{"location":"patterns/manufacturing/#with-phase-metrics","title":"With Phase Metrics","text":"<p>Capture max/min/avg of other columns within each phase window:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by: RunID\n      timestamp_col: ts\n      phases: [SetupTime, ProcessTime]\n      phase_metrics:\n        Level: max\n        Pressure: max\n        Temperature: mean\n</code></pre> <p>Additional output columns: - <code>SetupTime_Level</code>, <code>SetupTime_Pressure</code>, <code>SetupTime_Temperature</code> - <code>ProcessTime_Level</code>, <code>ProcessTime_Pressure</code>, <code>ProcessTime_Temperature</code></p>"},{"location":"patterns/manufacturing/#with-metadata","title":"With Metadata","text":"<p>Include run-level attributes in the output:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by: RunID\n      timestamp_col: ts\n      phases: [SetupTime, ProcessTime]\n      metadata:\n        ItemCode: first_after_start  # First value after phase 1 starts\n        Weight: max                  # Max weight across run\n        Operator: first              # First value in group\n</code></pre> <p>Metadata aggregation options: | Method | Description | |--------|-------------| | <code>first</code> | First value in the group | | <code>last</code> | Last value in the group | | <code>first_after_start</code> | First value after the first phase starts (skips initialization rows) | | <code>max</code>, <code>min</code>, <code>mean</code>, <code>sum</code> | Standard aggregations |</p>"},{"location":"patterns/manufacturing/#full-example-batch-process-analysis","title":"Full Example: Batch Process Analysis","text":"<p>A complete Silver layer transformation for batch processing data:</p> <pre><code>pipelines:\n  - pipeline: silver_run_analysis\n    layer: silver\n    nodes:\n      - name: run_phase_summary\n        read:\n          connection: lakehouse\n          format: delta\n          path: \"bronze/process_telemetry\"\n\n        transform:\n          # Step 1: Detect all phases and calculate metrics\n          - detect_sequential_phases:\n              group_by: RunID\n              timestamp_col: ts\n              phases:\n                - SetupTime\n                - PrepTime\n                - ActiveTime\n                - CycleTime\n                - CompleteTime\n              start_threshold: 240\n              status_col: Status\n              status_mapping:\n                1: idle\n                2: active\n                3: hold\n                4: faulted\n              phase_metrics:\n                Level: max\n                Temperature: max\n              metadata:\n                ItemCode: first_after_start\n                Weight: max\n\n          # Step 2: Filter to complete runs only\n          - filter_rows:\n              condition: \"ActiveTime_Level &gt; 70\"\n\n          # Step 3: Sort by run start time\n          - sort:\n              by: SetupTime_start\n              ascending: true\n\n        write:\n          connection: lakehouse\n          format: delta\n          path: \"silver/run_phase_summary\"\n          mode: overwrite\n</code></pre>"},{"location":"patterns/manufacturing/#configuration-reference","title":"Configuration Reference","text":""},{"location":"patterns/manufacturing/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>group_by</code> <code>str</code> or <code>List[str]</code> Yes - Column(s) to group by. E.g., <code>\"RunID\"</code> or <code>[\"RunID\", \"AssetID\"]</code> <code>timestamp_col</code> <code>str</code> No <code>\"ts\"</code> Timestamp column for ordering <code>phases</code> <code>List[str]</code> Yes - Timer columns representing sequential phases <code>start_threshold</code> <code>int</code> No <code>240</code> Max timer value (seconds) to consider as valid start <code>status_col</code> <code>str</code> No - Column containing equipment status codes <code>status_mapping</code> <code>Dict[int, str]</code> No - Map status codes to names <code>phase_metrics</code> <code>Dict[str, str]</code> No - Columns to aggregate within each phase <code>metadata</code> <code>Dict[str, str]</code> No - Run-level columns with aggregation method <code>fill_null_minutes</code> <code>bool</code> No <code>False</code> If True, fill null numeric columns with 0. Timestamps remain null for skipped phases. <code>spark_native</code> <code>bool</code> No <code>False</code> If True, use native Spark window functions instead of applyInPandas. See Engine Support section."},{"location":"patterns/manufacturing/#output-columns","title":"Output Columns","text":"<p>For each phase (e.g., <code>SetupTime</code>):</p> Column Description <code>{phase}_start</code> Phase start timestamp <code>{phase}_end</code> Phase end timestamp <code>{phase}_max_minutes</code> Maximum timer value in minutes <code>{phase}_{status}_minutes</code> Time in each status (if status tracking enabled) <code>{phase}_{metric}</code> Aggregated metrics (if phase_metrics configured)"},{"location":"patterns/manufacturing/#edge-cases-handled","title":"Edge Cases Handled","text":"<p>The transformer handles real-world data quality issues:</p>"},{"location":"patterns/manufacturing/#late-readings","title":"Late Readings","text":"<p>If your first reading shows a timer value &gt; <code>start_threshold</code>, the transformer skips it and looks for a valid start:</p> <pre><code>start_threshold: 240  # Ignore readings where timer &gt; 4 minutes\n</code></pre> <p>Why? If you see <code>SetupTime = 500</code> on the first reading, you missed the start. The transformer finds a valid start point.</p>"},{"location":"patterns/manufacturing/#emptyskipped-phases","title":"Empty/Skipped Phases","text":"<p>If a phase never ran (timer is always 0), the columns are still generated but with null values. Use <code>fill_null_minutes: true</code> to replace null numeric values with 0:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by: RunID\n      phases: [SetupTime, ProcessTime]\n      fill_null_minutes: true  # null _max_minutes, _status_minutes \u2192 0\n</code></pre> <p>Timestamp columns (<code>_start</code>, <code>_end</code>) remain null for skipped phases.</p>"},{"location":"patterns/manufacturing/#nan-status-values","title":"NaN Status Values","text":"<p>The transformer filters out rows with null/NaN status values when calculating time-in-state.</p>"},{"location":"patterns/manufacturing/#sequential-chaining","title":"Sequential Chaining","text":"<p>Each phase automatically starts looking for data after the previous phase ended. This prevents overlap when phases share the same data window.</p>"},{"location":"patterns/manufacturing/#no-data-after-start","title":"No Data After Start","text":"<p>If the data ends mid-phase (no plateau detected), the transformer uses the last available reading as the end time.</p>"},{"location":"patterns/manufacturing/#multi-column-grouping","title":"Multi-Column Grouping","text":"<p>Group by multiple columns when the same run operates on different equipment:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by:\n        - RunID\n        - AssetID\n      timestamp_col: ts\n      phases: [SetupTime, ProcessTime]\n</code></pre> <p>Output: One row per unique <code>(RunID, AssetID)</code> combination.</p>"},{"location":"patterns/manufacturing/#use-cases","title":"Use Cases","text":"Industry Phases Status Codes Batch Processing Setup, Prep, Active, Complete idle, active, hold, faulted Assembly Line Load, Process, Inspect, Unload running, stopped, maintenance CIP Cleaning PreRinse, Wash, Rinse, Sanitize active, draining, idle Injection Molding Clamp, Inject, Cycle, Eject running, setup, fault Discrete Manufacturing Queue, Setup, Run, Teardown running, hold, changeover"},{"location":"patterns/manufacturing/#engine-support","title":"Engine Support","text":"Engine Implementation Scalability Pandas Native loops Single machine Spark <code>applyInPandas</code> (default) Parallel per group across cluster Polars Pandas fallback Single machine"},{"location":"patterns/manufacturing/#spark-performance","title":"Spark Performance","text":"<p>By default, Spark uses <code>applyInPandas</code> which: - Groups data once, then processes each run in parallel - Works well for datasets with many runs (e.g., thousands of RunIDs)</p> <p>For datasets with few large runs, you can try native Spark window functions:</p> <pre><code>transform:\n  - detect_sequential_phases:\n      group_by: RunID\n      phases: [SetupTime, ProcessTime]\n      spark_native: true  # Use window functions instead of applyInPandas\n</code></pre> <p>The native implementation uses <code>lag</code>, <code>lead</code>, and <code>row_number</code> window functions but requires multiple joins per phase, which can be slower when there are many phases or shuffles.</p>"},{"location":"patterns/manufacturing/#tips","title":"Tips","text":"<ol> <li> <p>Set <code>start_threshold</code> appropriately - Match your data polling interval. If you poll every minute, 240 seconds (4 minutes) is a safe threshold.</p> </li> <li> <p>Don't include <code>status_col</code> if you don't need it - The transformer works fine without status tracking; you just won't get time-in-state columns.</p> </li> <li> <p>Use <code>phase_metrics</code> for QA filtering - Capture <code>Level: max</code> to filter out incomplete runs:    <pre><code>phase_metrics:\n  Level: max\n# Then filter:\n- filter_rows:\n    condition: \"ProcessTime_Level &gt; 70\"\n</code></pre></p> </li> <li> <p>Chain with other transformers - The output is a normal DataFrame. Use <code>filter_rows</code>, <code>drop_columns</code>, <code>sort</code>, etc. for post-processing.</p> </li> </ol>"},{"location":"patterns/merge_upsert/","title":"Pattern: Merge/Upsert (Raw \u2192 Silver)","text":"<p>Status: Core Pattern Layer: Silver (refined/cleaned) Engine: Spark (Delta) or Pandas Strategy: Merge (MERGE INTO in Spark) Idempotent: Yes (by key)  </p>"},{"location":"patterns/merge_upsert/#problem","title":"Problem","text":"<p>Raw contains duplicates and historical versions of records. You need: - One current version per key (deduplication) - Audit columns to track when each record was created/updated - Idempotency (rerunning doesn't create duplicates or double-count)</p> <p>How do you efficiently merge new/changed raw data into Silver while maintaining a clean current state?</p>"},{"location":"patterns/merge_upsert/#solution","title":"Solution","text":"<p>Use Delta Lake's MERGE operation to upsert records by key. Odibi provides the Merge Transformer to make this configuration-driven.</p>"},{"location":"patterns/merge_upsert/#how-it-works","title":"How It Works","text":"<p>MERGE Logic: 1. Read a batch of Raw data (new/changed rows) 2. Join with Silver by key columns 3. If matched: Update the Silver row with the new data 4. If not matched: Insert the new row 5. Auto-inject audit columns (created_at, updated_at)</p>"},{"location":"patterns/merge_upsert/#step-by-step-example","title":"Step-by-Step Example","text":""},{"location":"patterns/merge_upsert/#scenario-orders-table","title":"Scenario: Orders Table","text":"<p>Raw (after 2 runs; has duplicates): <pre><code>order_id | product      | qty | created_at          | updated_at\n---------|--------------|-----|---------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL              \u2190 Duplicate\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 \u2190 Duplicate\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL\n</code></pre></p> <p>Silver (before merge): <pre><code>order_id | product      | qty | created_at          | updated_at          | _created_ts        | _updated_ts\n---------|--------------|-----|---------------------|---------------------|--------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL                | 2025-11-01 10:30   | 2025-11-01 10:30\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL                | 2025-11-01 11:30   | 2025-11-01 11:30\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 | 2025-11-01 12:30   | 2025-11-01 13:30\n</code></pre></p>"},{"location":"patterns/merge_upsert/#merge-operation","title":"Merge Operation","text":"<p>New micro-batch from Raw (after dedup by timestamp): <pre><code>order_id | product      | qty | created_at          | updated_at\n---------|--------------|-----|---------------------|-------------------\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL              \u2190 Duplicate, older timestamp\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL              \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL              \u2190 NEW\n</code></pre></p> <p>Merge By Key (<code>order_id</code>):</p> <pre><code>Row (order_id=2):\n  - Matches Silver row (order_id=2)\n  - Source timestamp: 2025-11-01 11:00\n  - Silver timestamp: 2025-11-01 13:30\n  - Source is older, skip? OR update anyway?\n  \u2192 MERGE strategy: UPDATE (keep latest by source created_at/updated_at)\n  \u2192 Actually: Insert latest version FIRST, then merge handles it\n  \u2192 Result: Silver row 2 unchanged (already has latest)\n\nRow (order_id=4):\n  - No match in Silver\n  - NEW row\n  \u2192 INSERT into Silver\n  \u2192 Set _created_ts = now(), _updated_ts = now()\n\nRow (order_id=5):\n  - No match in Silver\n  - NEW row\n  \u2192 INSERT into Silver\n  \u2192 Set _created_ts = now(), _updated_ts = now()\n</code></pre> <p>Silver (after merge): <pre><code>order_id | product      | qty | created_at          | updated_at          | _created_ts        | _updated_ts\n---------|--------------|-----|---------------------|---------------------|--------------------|-------------------\n1        | Widget A     | 10  | 2025-11-01 10:00:00 | NULL                | 2025-11-01 10:30   | 2025-11-01 10:30\n2        | Widget B     | 5   | 2025-11-01 11:00:00 | NULL                | 2025-11-01 11:30   | 2025-11-01 11:30  (unchanged)\n3        | Widget A     | 2   | 2025-11-01 12:00:00 | 2025-11-01 13:00:00 | 2025-11-01 12:30   | 2025-11-01 13:30  (unchanged)\n4        | Widget C     | 8   | 2025-11-02 09:00:00 | NULL                | 2025-11-02 09:30   | 2025-11-02 09:30  \u2190 NEW\n5        | Widget B     | 12  | 2025-11-02 10:00:00 | NULL                | 2025-11-02 10:30   | 2025-11-02 10:30  \u2190 NEW\n</code></pre></p> <p>Result: - Silver has exactly 5 unique orders (1 per key) - Duplicates from Raw are deduplicated - Audit columns track when Odibi processed each row - Idempotent: rerunning the same batch produces the same result</p>"},{"location":"patterns/merge_upsert/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/merge_upsert/#minimal-config","title":"Minimal Config","text":"<pre><code>- id: merge_orders_silver\n  name: \"Merge Orders to Silver\"\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.orders\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    strategy: upsert\n</code></pre>"},{"location":"patterns/merge_upsert/#full-config-with-audit-columns","title":"Full Config (with Audit Columns)","text":"<pre><code>- id: merge_orders_silver\n  name: \"Merge Orders: Raw \u2192 Silver\"\n  description: \"Deduplicate and upsert orders from raw layer\"\n  depends_on: [load_orders_raw]\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.orders\n  transformer: merge\n  params:\n    target: silver.orders\n    keys: [order_id]\n    strategy: upsert\n    audit_cols:\n      created_col: _created_at\n      updated_col: _updated_at\n  validation:\n    not_empty: true\n    schema:\n      order_id:\n        type: integer\n        nullable: false\n      product:\n        type: string\n        nullable: false\n      qty:\n        type: integer\n        nullable: false\n</code></pre>"},{"location":"patterns/merge_upsert/#multi-key-example-composite-key","title":"Multi-Key Example (Composite Key)","text":"<pre><code>- id: merge_inventory_silver\n  name: \"Merge Inventory to Silver\"\n  read:\n    connection: adls_prod\n    format: delta\n    table: raw.inventory\n  transformer: merge\n  params:\n    target: silver.inventory\n    keys: [store_id, material_id]  \u2190 Composite key\n    strategy: upsert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre>"},{"location":"patterns/merge_upsert/#merge-transformer-behavior","title":"Merge Transformer Behavior","text":""},{"location":"patterns/merge_upsert/#spark-delta","title":"Spark (Delta)","text":"<p>Uses native <code>DeltaTable.merge()</code>:</p> <pre><code># Pseudo-code\ndelta_table = DeltaTable.forName(\"silver.orders\")\ndelta_table.merge(\n    source_df,\n    condition=\"target.order_id = source.order_id\"\n) \\\n.whenMatchedUpdateAll() \\\n.whenNotMatchedInsertAll() \\\n.execute()\n\n# Auto-inject audit columns:\n# If insert: created_at = now(), updated_at = now()\n# If update: updated_at = now(), created_at unchanged\n</code></pre>"},{"location":"patterns/merge_upsert/#pandas","title":"Pandas","text":"<p>Loads, merges, overwrites:</p> <pre><code># Pseudo-code\ntarget = pd.read_parquet(\"silver/orders\")\nsource = df  # Input DataFrame\n\n# Merge indicator\nmerged = target.merge(source, on=['order_id'], how='outer', indicator=True)\n\n# Apply logic:\n# - Rows in target only: keep\n# - Rows in source only: insert\n# - Rows in both: update source values\n\n# Overwrite\nmerged.to_parquet(\"silver/orders\", mode=\"overwrite\")\n</code></pre>"},{"location":"patterns/merge_upsert/#strategy-options","title":"Strategy Options","text":"Strategy Behavior Best For <code>upsert</code> Insert new, update existing Standard use case (Raw \u2192 Silver) <code>append_only</code> Insert new, ignore duplicates Append-only tables (no updates) <code>delete_match</code> Delete matching rows Tombstones, soft deletes"},{"location":"patterns/merge_upsert/#audit-columns","title":"Audit Columns","text":""},{"location":"patterns/merge_upsert/#auto-injected-columns","title":"Auto-Injected Columns","text":"<p>When <code>audit_cols</code> is specified, Odibi adds two columns:</p> <pre><code># On INSERT\n_created_at = CURRENT_TIMESTAMP()\n_updated_at = CURRENT_TIMESTAMP()\n\n# On UPDATE\n_created_at = [unchanged]\n_updated_at = CURRENT_TIMESTAMP()\n</code></pre> <p>This lets you track when Odibi processed each record, separate from the source's created/updated columns.</p>"},{"location":"patterns/merge_upsert/#example","title":"Example","text":"<pre><code>audit_cols:\n  created_col: _sys_created_ts\n  updated_col: _sys_updated_ts\n</code></pre> <p>Result: <pre><code>order_id | product | _sys_created_ts        | _sys_updated_ts\n---------|---------|------------------------|-------------------\n1        | Widget  | 2025-11-01 10:30:00    | 2025-11-01 10:30:00\n2        | Gadget  | 2025-11-01 11:30:00    | 2025-11-02 14:45:00  \u2190 Updated\n</code></pre></p>"},{"location":"patterns/merge_upsert/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/merge_upsert/#advantages","title":"Advantages","text":"<p>\u2713 Deduplicates Raw data automatically \u2713 Idempotent (safe to rerun) \u2713 Tracks data lineage (audit columns) \u2713 Handles both new and changed rows efficiently \u2713 Spark merge is fast (native Delta operation)  </p>"},{"location":"patterns/merge_upsert/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Requires primary key (what makes each row unique?) \u2717 Overwrites previous values (no history of all versions) \u2717 Pandas merge is slower than Spark (pandas mode not recommended for large tables)  </p>"},{"location":"patterns/merge_upsert/#common-patterns","title":"Common Patterns","text":""},{"location":"patterns/merge_upsert/#pattern-scd-type-1-current-state-only","title":"Pattern: SCD Type 1 (Current State Only)","text":"<p>Keep only the latest version of each record. This is the default merge pattern.</p> <pre><code>transformer: merge\nparams:\n  target: silver.customers\n  keys: [customer_id]\n  strategy: upsert\n</code></pre>"},{"location":"patterns/merge_upsert/#pattern-scd-type-2-full-history","title":"Pattern: SCD Type 2 (Full History)","text":"<p>Keep all historical versions with effective dates. NOT supported by standard merge. Use a separate <code>dim_customers</code> table with: - <code>customer_id</code> - <code>effective_from</code>, <code>effective_to</code> - <code>is_current</code> flag</p> <p>Then maintain it with a separate pipeline.</p>"},{"location":"patterns/merge_upsert/#pattern-append-only-no-duplicates","title":"Pattern: Append-Only (No Duplicates)","text":"<p>If your table should never have duplicates and you want to avoid updates:</p> <pre><code>transformer: merge\nparams:\n  target: silver.events\n  keys: [event_id]\n  strategy: append_only\n</code></pre> <p>This inserts new rows but ignores duplicates instead of updating.</p>"},{"location":"patterns/merge_upsert/#pattern-connection-based-path-with-table-registration","title":"Pattern: Connection-Based Path with Table Registration","text":"<p>Use a connection to resolve ADLS paths and register the table in Unity Catalog:</p> <pre><code>transform:\n  steps:\n    - sql_file: \"sql/clean_orders.sql\"\n    - function: merge\n      params:\n        connection: adls_prod\n        path: sales/silver/orders\n        register_table: silver.orders\n        keys: [order_id]\n        strategy: upsert\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre> <p>This: 1. Resolves the full ADLS path via the <code>adls_prod</code> connection 2. Performs the merge operation 3. Registers the Delta table as <code>silver.orders</code> in the metastore</p>"},{"location":"patterns/merge_upsert/#debugging","title":"Debugging","text":""},{"location":"patterns/merge_upsert/#check-for-duplicates-in-silver","title":"Check for Duplicates in Silver","text":"<pre><code>SELECT order_id, COUNT(*) as count\nFROM silver.orders\nGROUP BY order_id\nHAVING COUNT(*) &gt; 1\n</code></pre> <p>If you see duplicates, your merge key is wrong.</p>"},{"location":"patterns/merge_upsert/#check-merge-history","title":"Check Merge History","text":"<pre><code>DESCRIBE HISTORY silver.orders\n</code></pre> <p>Shows every merge operation, versions, and row counts.</p>"},{"location":"patterns/merge_upsert/#when-to-use","title":"When to Use","text":"<ul> <li>Always for Raw \u2192 Silver refinement</li> <li>Multiple sources merging into same table</li> <li>Need to track data lineage (audit columns)</li> <li>Want idempotent transformations</li> </ul>"},{"location":"patterns/merge_upsert/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Audit tables (keep appending)</li> <li>SCD Type 2 (need version history)</li> <li>Data that should be immutable (use append instead)</li> </ul>"},{"location":"patterns/merge_upsert/#related-patterns","title":"Related Patterns","text":"<ul> <li>Append-Only Raw \u2192 Source layer (unmerged, duplicates OK)</li> <li>High Water Mark \u2192 How to efficiently feed Raw with new data</li> </ul>"},{"location":"patterns/merge_upsert/#references","title":"References","text":"<ul> <li>Databricks: Delta Lake MERGE</li> <li>Fundamentals of Data Engineering: Chapter on SCD</li> </ul>"},{"location":"patterns/scd2/","title":"SCD Type 2 (Slowly Changing Dimensions)","text":"<p>The SCD Type 2 pattern allows you to track the full history of changes for a record over time. Unlike a simple update (which overwrites the old value), SCD2 keeps the old version and adds a new version, managing effective dates for you.</p>"},{"location":"patterns/scd2/#the-time-machine-concept","title":"The \"Time Machine\" Concept","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: Each record has an \"effective window\" (<code>effective_time</code> to <code>end_time</code>) and a flag (<code>is_current</code>) indicating if it is the latest version.</p>"},{"location":"patterns/scd2/#visual-example","title":"Visual Example","text":"<p>Input (Source Update): Customer 101 moved to NY on Feb 1st.</p> customer_id address tier txn_date 101 NY Gold 2024-02-01 <p>Target Table (Before): Customer 101 lived in CA since Jan 1st.</p> customer_id address tier txn_date valid_to is_active 101 CA Gold 2024-01-01 NULL true <p>Target Table (After SCD2): Old record CLOSED (valid_to set). New record OPEN (is_active=true).</p> customer_id address tier txn_date valid_to is_active 101 CA Gold 2024-01-01 2024-02-01 false 101 NY Gold 2024-02-01 NULL true"},{"location":"patterns/scd2/#configuration","title":"Configuration","text":"<p>Use the <code>scd2</code> transformer in your pipeline node.</p>"},{"location":"patterns/scd2/#option-1-using-table-name","title":"Option 1: Using Table Name","text":"<pre><code>nodes:\n  - name: \"dim_customers\"\n    # ... (read from source) ...\n\n    transformer: \"scd2\"\n    params:\n      target: \"silver.dim_customers\"   # Registered table name\n      keys: [\"customer_id\"]            # Unique ID\n      track_cols: [\"address\", \"tier\"]  # Changes here trigger a new version\n      effective_time_col: \"txn_date\"   # When the change happened\n\n    write:\n      table: \"silver.dim_customers\"\n      format: \"delta\"\n      mode: \"overwrite\"                # Important: SCD2 returns FULL history\n</code></pre>"},{"location":"patterns/scd2/#option-2-using-connection-path-adls","title":"Option 2: Using Connection + Path (ADLS)","text":"<pre><code>nodes:\n  - name: \"dim_customers\"\n    # ... (read from source) ...\n\n    transformer: \"scd2\"\n    params:\n      connection: adls_prod            # READ existing history from here\n      path: sales/silver/dim_customers\n      keys: [\"customer_id\"]\n      track_cols: [\"address\", \"tier\"]\n      effective_time_col: \"txn_date\"\n\n    write:\n      connection: adls_prod            # WRITE result back (same location)\n      path: sales/silver/dim_customers\n      format: \"delta\"\n      mode: \"overwrite\"\n</code></pre> <p>Why specify the path twice? - <code>params.connection/path</code> \u2192 Where to read existing history (to detect changes) - <code>write.connection/path</code> \u2192 Where to write the full result</p> <p>They should typically match. SCD2 returns a DataFrame (doesn't write directly), so the write phase handles persistence separately.</p>"},{"location":"patterns/scd2/#full-configuration","title":"Full Configuration","text":"<pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"       # OR use connection + path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\", \"email\"]\n\n  # Source column for start date\n  effective_time_col: \"updated_at\"\n\n  # Target columns to manage (optional defaults shown)\n  end_time_col: \"valid_to\"\n  current_flag_col: \"is_current\"\n</code></pre>"},{"location":"patterns/scd2/#how-it-works","title":"How It Works","text":"<p>The <code>scd2</code> transformer performs a complex set of operations automatically:</p> <ol> <li>Match: Finds existing records in the <code>target</code> table using <code>keys</code>.</li> <li>Compare: Checks <code>track_cols</code> to see if any data has changed.</li> <li>Close: If a record changed, it updates the old record's <code>end_time_col</code> to equal the new record's <code>effective_time_col</code>, and sets <code>is_current = false</code>.</li> <li>Insert: It adds the new record with <code>effective_time_col</code> as the start date, <code>NULL</code> as the end date, and <code>is_current = true</code>.</li> <li>Preserve: It keeps all unchanged history records as they are.</li> </ol>"},{"location":"patterns/scd2/#important-notes","title":"Important Notes","text":"<ul> <li>Write Mode: You must use <code>mode: overwrite</code> for the write operation following this transformer. The transformer constructs the complete new state of the history table (including old closed records and new open records).</li> <li>Target Existence: If the target table doesn't exist (first run), the transformer simply prepares the source data (adds valid_to/is_current columns) and returns it.</li> <li>Engine Support: Works on both Spark (Delta Lake) and Pandas (Parquet/CSV).</li> </ul>"},{"location":"patterns/scd2/#when-to-use","title":"When to Use","text":"<ul> <li>Dimension Tables: Customer dimensions, Product dimensions where attributes change slowly over time.</li> <li>Audit Trails: When you need exact historical state reconstruction.</li> </ul>"},{"location":"patterns/scd2/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Fact Tables: Events (Transactions, Logs) are immutable; they don't change state, they just occur. Use <code>append</code> instead.</li> <li>Rapidly Changing Data: If a record changes 100 times a day, SCD2 will explode your storage size. Use a snapshot or aggregate approach instead.</li> </ul>"},{"location":"patterns/scd2/#common-errors-and-debugging","title":"Common Errors and Debugging","text":"<p>This section covers the most common SCD2 errors and how to fix them.</p>"},{"location":"patterns/scd2/#error-effective_time_col-not-found","title":"Error: <code>effective_time_col</code> Not Found","text":"<p>Error Message: <pre><code>KeyError: 'updated_at'\n# or\nAnalysisException: Column 'updated_at' does not exist\n</code></pre></p> <p>What It Means (Plain English): The <code>effective_time_col</code> you specified doesn't exist in your source DataFrame.</p> <p>Why It Happens: - The column name is misspelled - The column was renamed or dropped upstream - Case sensitivity mismatch (<code>Updated_At</code> vs <code>updated_at</code>)</p> <p>Step-by-Step Fix:</p> <ol> <li> <p>Check your source data columns: <pre><code># Add this before the SCD2 node to debug\ndf = spark.read.format(\"delta\").load(\"bronze/customers\")\nprint(df.columns)\n# Output: ['customer_id', 'name', 'UpdatedAt', 'address']\n# Aha! It's 'UpdatedAt', not 'updated_at'\n</code></pre></p> </li> <li> <p>Fix the YAML: <pre><code># BEFORE (wrong)\nparams:\n  effective_time_col: \"updated_at\"  # \u274c Doesn't exist\n\n# AFTER (correct)\nparams:\n  effective_time_col: \"UpdatedAt\"  # \u2705 Matches actual column\n</code></pre></p> </li> </ol> <p>Important: The <code>effective_time_col</code> must exist in the SOURCE data, not the target. After SCD2 processing, this column gets used to populate the history columns.</p>"},{"location":"patterns/scd2/#error-track_cols-column-mismatch","title":"Error: <code>track_cols</code> Column Mismatch","text":"<p>Error Message: <pre><code>KeyError: 'email'\n# or\nColumn 'Email' not found in schema\n</code></pre></p> <p>What It Means: One of the columns in <code>track_cols</code> doesn't exist in your source data, or there's a case mismatch.</p> <p>Why It Happens: - Column names are case-sensitive - A column was renamed in the source system - You're tracking a column that doesn't exist yet</p> <p>Step-by-Step Fix:</p> <ol> <li> <p>List actual columns: <pre><code>df = spark.read.format(\"delta\").load(\"bronze/customers\")\nprint(df.columns)\n# ['customer_id', 'Name', 'Email', 'Address']\n</code></pre></p> </li> <li> <p>Match case exactly: <pre><code># BEFORE (wrong - case doesn't match)\nparams:\n  track_cols: [\"name\", \"email\", \"address\"]\n\n# AFTER (correct - matches actual columns)\nparams:\n  track_cols: [\"Name\", \"Email\", \"Address\"]\n</code></pre></p> </li> </ol> <p>\ud83d\udca1 Pro Tip: Consider normalizing column names to lowercase in Bronze/Silver to avoid case issues: <pre><code>transform:\n  steps:\n    - function: \"rename_columns\"\n      params:\n        lowercase: true\n</code></pre></p>"},{"location":"patterns/scd2/#error-schema-evolution-issues","title":"Error: Schema Evolution Issues","text":"<p>Error Message: <pre><code>AnalysisException: A]chema mismatch detected:\n- Expected: customer_id: string, name: string, address: string, ...\n- Actual:   customer_id: string, name: string, phone: string, ...\n</code></pre></p> <p>What It Means: Your target table (from a previous run) has a different schema than the new data.</p> <p>Why It Happens: - Source added new columns (e.g., <code>phone</code>) - Source removed columns (e.g., dropped <code>address</code>) - Column types changed (e.g., <code>int</code> \u2192 <code>string</code>)</p> <p>Step-by-Step Fix:</p> <p>Option 1: Allow Schema Merging <pre><code>write:\n  connection: silver\n  table: dim_customers\n  format: delta\n  mode: overwrite\n  delta_options:\n    mergeSchema: true  # \u2705 Allows new columns\n</code></pre></p> <p>Option 2: Handle in Transform <pre><code># Add missing columns with defaults before SCD2\ntransform:\n  steps:\n    - function: \"derive_columns\"\n      params:\n        derivations:\n          phone: \"COALESCE(phone, 'unknown')\"\n</code></pre></p> <p>Option 3: Full Schema Reset (Nuclear Option) <pre><code># Delete target table and rerun from scratch\n# WARNING: Loses all history!\nspark.sql(\"DROP TABLE IF EXISTS silver.dim_customers\")\n</code></pre></p>"},{"location":"patterns/scd2/#why-did-my-row-count-explode","title":"\"Why Did My Row Count Explode?\"","text":"<p>Symptom: <pre><code>Before: dim_customers had 10,000 rows\nAfter:  dim_customers has 50,000 rows\n</code></pre></p> <p>What's Happening: SCD2 is working correctly! Every time a tracked column changes, it creates a new version. If you ran it multiple times or have duplicates, you get multiple versions per record.</p> <p>Common Causes:</p> <ol> <li> <p>Duplicate source data: <pre><code>customer_id | name  | updated_at\n101         | Alice | 2024-01-01\n101         | Alice | 2024-01-01  &lt;- Duplicate!\n101         | Alice | 2024-01-01  &lt;- Another duplicate!\n</code></pre> Fix: Deduplicate before SCD2 (see Anti-Patterns)</p> </li> <li> <p>Running SCD2 on append-mode source:    Each run sees ALL historical source data, creating versions for old changes again.    Fix: Use incremental loading or filter source to only new records.</p> </li> <li> <p>Tracking too many columns: <pre><code># Tracking every column = version explosion\ntrack_cols: [\"*\"]  # \u274c Don't do this!\n\n# Track only meaningful business changes\ntrack_cols: [\"tier\", \"status\", \"region\"]  # \u2705 Selective\n</code></pre></p> </li> </ol> <p>Debugging Query: <pre><code>-- Find customers with excessive versions\nSELECT customer_id, COUNT(*) as version_count\nFROM dim_customers\nGROUP BY customer_id\nHAVING COUNT(*) &gt; 10\nORDER BY version_count DESC\n</code></pre></p>"},{"location":"patterns/scd2/#debugging-checklist","title":"Debugging Checklist","text":"<p>Before running your SCD2 pipeline, verify these items:</p> <pre><code># \u2705 DEBUGGING CHECKLIST\n# Print this and check each box:\n\n# [ ] 1. Source Data Check\n#     - effective_time_col exists in source\n#     - All track_cols exist in source\n#     - Column names match case exactly\n\n# [ ] 2. Key Column Check  \n#     - keys columns exist in source\n#     - keys columns have no NULLs\n#     - keys columns uniquely identify records\n\n# [ ] 3. Target Table Check\n#     - Target exists (or this is first run)\n#     - Target schema is compatible\n#     - Target has end_time_col and current_flag_col\n\n# [ ] 4. Deduplication Check\n#     - Source has no duplicate keys\n#     - If duplicates exist, deduplicate BEFORE SCD2\n\n# [ ] 5. Write Mode Check\n#     - Using mode: overwrite (required for SCD2)\n</code></pre> <p>Python Debugging Script: <pre><code># Add this before your SCD2 node to validate\ndef validate_scd2_input(df, config):\n    \"\"\"Validate data before SCD2 processing.\"\"\"\n    errors = []\n\n    # Check effective_time_col exists\n    if config['effective_time_col'] not in df.columns:\n        errors.append(f\"effective_time_col '{config['effective_time_col']}' not in columns: {df.columns}\")\n\n    # Check all track_cols exist\n    for col in config['track_cols']:\n        if col not in df.columns:\n            errors.append(f\"track_col '{col}' not in columns: {df.columns}\")\n\n    # Check for duplicate keys\n    key_cols = config['keys']\n    dup_count = df.groupBy(key_cols).count().filter(\"count &gt; 1\").count()\n    if dup_count &gt; 0:\n        errors.append(f\"Found {dup_count} duplicate keys! Deduplicate first.\")\n\n    # Check for NULL keys\n    for key in key_cols:\n        null_count = df.filter(df[key].isNull()).count()\n        if null_count &gt; 0:\n            errors.append(f\"Found {null_count} NULL values in key column '{key}'\")\n\n    if errors:\n        for e in errors:\n            print(f\"\u274c {e}\")\n        raise ValueError(\"SCD2 validation failed. Fix errors above.\")\n    else:\n        print(\"\u2705 SCD2 input validation passed\")\n</code></pre></p>"},{"location":"patterns/scd2/#quick-reference-scd2-error-cheat-sheet","title":"Quick Reference: SCD2 Error Cheat Sheet","text":"Error Likely Cause Quick Fix <code>effective_time_col not found</code> Column doesn't exist or wrong name Check source columns, fix spelling/case <code>track_col X not found</code> Column name mismatch Match exact column names including case Schema mismatch Target has different columns Use <code>mergeSchema: true</code> or reset target Row count explosion Duplicates or too many runs Deduplicate source first <code>merge_key not found</code> Key column missing Verify keys exist in both source and target NULL in key columns Missing business keys Handle NULLs before SCD2"},{"location":"patterns/scd2/#next-steps","title":"Next Steps","text":"<ul> <li>Anti-Patterns Guide - What NOT to do with SCD2</li> <li>Dimension Pattern - Full dimension table management</li> <li>Troubleshooting Guide - General debugging</li> </ul>"},{"location":"patterns/skip_if_unchanged/","title":"Skip If Unchanged Pattern","text":"<p>Use Case: Avoid redundant writes for snapshot tables that may not change between pipeline runs.</p>"},{"location":"patterns/skip_if_unchanged/#the-problem","title":"The Problem","text":"<p>When ingesting snapshot data (full table extracts without timestamps), an hourly pipeline will append the same 192k rows 24 times per day if the source data doesn't change. This wastes:</p> <ul> <li>Storage: 24\u00d7 the necessary data</li> <li>Compute: Unnecessary write operations</li> <li>Query performance: More files to scan</li> </ul>"},{"location":"patterns/skip_if_unchanged/#the-solution","title":"The Solution","text":"<p>The <code>skip_if_unchanged</code> feature computes a hash of the DataFrame content before writing. If the hash matches the previous write, the write is skipped entirely.</p> <pre><code>nodes:\n  - name: bronze_data\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.MySnapshotTable\n    write:\n      connection: bronze\n      format: delta\n      table: my_snapshot_table\n      mode: append\n      skip_if_unchanged: true\n      skip_hash_sort_columns: [store_id, date_id]  # For deterministic ordering\n</code></pre>"},{"location":"patterns/skip_if_unchanged/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    A[Read source data] --&gt; B[Compute SHA256 hash of DataFrame]\n    B --&gt; C{Previous hash exists?}\n    C --&gt;|No| D[Write data]\n    D --&gt; E[Store hash in Delta metadata]\n    C --&gt;|Yes| F{Hashes match?}\n    F --&gt;|Yes| G[Skip write, log 'unchanged']\n    F --&gt;|No| D\n</code></pre> <ol> <li>Hash Computation: Before writing, the entire DataFrame is converted to CSV bytes and hashed with SHA256</li> <li>Hash Storage: The hash is stored in Delta table properties (<code>odibi.content_hash</code>)</li> <li>Comparison: On subsequent runs, the new hash is compared to the stored hash</li> <li>Skip or Write: If hashes match, the write is skipped; otherwise, data is written and hash updated</li> </ol>"},{"location":"patterns/skip_if_unchanged/#configuration-options","title":"Configuration Options","text":"Option Type Description <code>skip_if_unchanged</code> bool Enable hash-based skip detection <code>skip_hash_columns</code> list Subset of columns to hash (default: all) <code>skip_hash_sort_columns</code> list Columns to sort by before hashing (for determinism)"},{"location":"patterns/skip_if_unchanged/#when-to-use","title":"When to Use","text":"<p>\u2705 Good fit: - Snapshot tables without <code>updated_at</code> timestamps - Reference/dimension data that changes infrequently - Tables where you don't know the change frequency</p> <p>\u274c Not recommended: - Tables with reliable <code>updated_at</code> (use HWM instead) - Append-only fact tables (new data every run) - Very large tables (hash computation is expensive)</p>"},{"location":"patterns/skip_if_unchanged/#example-reference-data-sync","title":"Example: Reference Data Sync","text":"<pre><code># Pipeline runs hourly for freshness\n# But this table only changes 1-2 times per day\n\nnodes:\n  - name: bronze_reference_data\n    read:\n      connection: azure_sql\n      format: sql\n      table: dbo.vw_reference_detail\n    write:\n      connection: bronze\n      format: delta\n      table: reference_detail\n      mode: append\n      add_metadata: true\n      skip_if_unchanged: true\n      skip_hash_sort_columns: [store_id, date_id]\n</code></pre> <p>Result: - Pipeline checks every hour (data is fresh when needed) - Only writes when data actually changes (storage efficient) - Logs show \"Skipping write - content unchanged\" for skipped runs</p>"},{"location":"patterns/skip_if_unchanged/#storage-impact","title":"Storage Impact","text":"Scenario Daily Writes Annual Rows (192k/snapshot) No skip (hourly) 24 1.7 billion With skip (2 changes/day) 2 140 million Savings 92% 92%"},{"location":"patterns/skip_if_unchanged/#limitations","title":"Limitations","text":"<ol> <li>Delta only: Currently only supported for Delta format</li> <li>Full DataFrame hash: Computes hash of entire DataFrame (not row-by-row)</li> <li>Memory: DataFrame must fit in driver memory for hashing</li> <li>First run: Always writes on first run (no previous hash to compare)</li> </ol>"},{"location":"patterns/skip_if_unchanged/#related-patterns","title":"Related Patterns","text":"<ul> <li>Append-Only Raw Layer - Bronze layer best practices</li> <li>Incremental Stateful - For tables with timestamps</li> <li>Smart Read - Full load pattern</li> </ul>"},{"location":"patterns/smart_read/","title":"Smart Read (Rolling Window)","text":"<p>The \"Smart Read\" feature simplifies incremental data loading by automatically generating the correct SQL query based on time windows.</p> <p>Note: This page describes the Rolling Window mode (Stateless). For exact state tracking (HWM), see Stateful Incremental Loading.</p> <p>It eliminates the need to write complex SQL with <code>first_run_query</code> and dialect-specific date math.</p> <p>Requirement: Write Configuration</p> <p>Smart Read requires a <code>write</code> block in the same node.</p> <p>It determines whether to run a Full Load or Incremental Load by checking if the destination defined in <code>write</code> already exists.</p> <ul> <li>If you only want to read data (without writing), use the standard <code>query</code> option with explicit date filters instead.</li> <li>Ensure your <code>write</code> mode is set correctly (usually <code>append</code>) to preserve history.</li> </ul>"},{"location":"patterns/smart_read/#write-modes-for-incremental","title":"Write Modes for Incremental","text":"Mode Suitability Why? <code>append</code> \u2705 Recommended Safely adds new records to the lake. Preserves history. <code>upsert</code> \u26a0\ufe0f Advanced Use only if you are merging directly into a Silver layer table and have defined keys. <code>overwrite</code> \u274c Dangerous Do NOT use. This would replace your entire historical dataset with just the latest batch (e.g., the last 3 days)."},{"location":"patterns/smart_read/#how-it-works","title":"How It Works","text":"<p>Odibi checks if your Write target exists:</p> <ol> <li> <p>Target Missing (First Run):</p> <ul> <li>It assumes this is a historical load.</li> <li>Generates: <code>SELECT * FROM source_table</code></li> <li>Result: Loads all history.</li> </ul> </li> <li> <p>Target Exists (Subsequent Runs):</p> <ul> <li>It assumes this is an incremental load.</li> <li>Generates: <code>SELECT * FROM source_table WHERE column &gt;= [Calculated Date]</code></li> <li>Result: Loads only new/changed data.</li> </ul> </li> </ol>"},{"location":"patterns/smart_read/#the-standard-pattern-ingest-to-bronze","title":"The Standard Pattern: \"Ingest to Bronze\"","text":"<p>The most common use case for Smart Read is the Ingestion Node. This node acts as a bridge between your external source (SQL, API) and your Data Lake (Bronze Layer).</p>"},{"location":"patterns/smart_read/#why-use-this-pattern","title":"Why use this pattern?","text":"<ol> <li>State Management: The node uses the Write Target (e.g., <code>bronze_orders</code>) as its state.<ul> <li>Target Empty? \u2192 Run <code>SELECT *</code> (Full History)</li> <li>Target Exists? \u2192 Run <code>SELECT * ... WHERE date &gt; X</code> (Incremental)</li> </ul> </li> <li>Efficiency: Downstream nodes (e.g., \"clean_orders\") can simply depend on this node. They will receive the dataframe containing only the data that was just ingested (the incremental batch), allowing your entire pipeline to process only new data efficiently.</li> </ol>"},{"location":"patterns/smart_read/#example-node","title":"Example Node","text":"<pre><code>- name: \"ingest_orders\"\n  description: \"Incrementally load orders from SQL to Delta\"\n\n  # 1. READ (Source)\n  read:\n    connection: \"sql_db\"\n    format: \"sql\"\n    table: \"orders\"\n    incremental:\n      column: \"updated_at\"\n      lookback: 3\n      unit: \"day\"\n\n  # 2. WRITE (Target - Required for state tracking)\n  write:\n    connection: \"data_lake\"\n    format: \"delta\"\n    table: \"bronze_orders\"\n    mode: \"append\"  # Append new rows from the incremental batch\n</code></pre>"},{"location":"patterns/smart_read/#configuration","title":"Configuration","text":"<p>Use the <code>incremental</code> block in your <code>read</code> configuration.</p>"},{"location":"patterns/smart_read/#example-handling-updates-inserts","title":"Example: Handling Updates &amp; Inserts","text":"<p>This pattern handles both new records (<code>created_at</code>) and updates (<code>updated_at</code>).</p> <pre><code>nodes:\n  - name: \"load_orders\"\n    read:\n      connection: \"sql_server_prod\"\n      format: \"sql\"\n      table: \"dbo.orders\"\n\n      incremental:\n        column: \"updated_at\"         # Primary check\n        fallback_column: \"created_at\" # If updated_at is NULL\n        lookback: 1\n        unit: \"day\"\n\n    write:\n      connection: \"bronze\"\n      format: \"delta\"\n      table: \"orders_raw\"\n      mode: \"append\"\n</code></pre> <p>This generates: <pre><code>SELECT * FROM dbo.orders\nWHERE COALESCE(updated_at, created_at) &gt;= '2023-10-25 10:00:00'\n</code></pre></p>"},{"location":"patterns/smart_read/#example-simple-append-only","title":"Example: Simple Append-Only","text":"<p>Perfect for pipelines that run every hour but want a 4-hour safety window for late-arriving data.</p> <pre><code>    read:\n      connection: \"postgres_db\"\n      format: \"sql\"\n      table: \"public.events\"\n      incremental:\n        column: \"event_time\"\n        lookback: 4\n        unit: \"hour\"\n\n    write:\n      connection: \"bronze\"\n      format: \"delta\"\n      table: \"events_raw\"\n      mode: \"append\"\n</code></pre>"},{"location":"patterns/smart_read/#advanced-merging-directly-to-silver-upsert","title":"Advanced: Merging directly to Silver (Upsert)","text":"<p>If you are bypassing Bronze and merging directly into a Silver table, you can use <code>upsert</code>. Note: This requires defining the primary <code>keys</code> to match on.</p> <pre><code>    read:\n      connection: \"crm_db\"\n      format: \"sql\"\n      table: \"customers\"\n      incremental:\n        column: \"last_modified\"\n        lookback: 1\n        unit: \"day\"\n\n    write:\n      connection: \"silver\"\n      format: \"delta\"\n      table: \"dim_customers\"\n      mode: \"upsert\"\n      options:\n        keys: [\"customer_id\"]\n</code></pre>"},{"location":"patterns/smart_read/#supported-units","title":"Supported Units","text":"Unit Description <code>hour</code> Looks back N hours from <code>now()</code> <code>day</code> Looks back N days from <code>now()</code> <code>month</code> Looks back N * 30 days (approx) <code>year</code> Looks back N * 365 days (approx)"},{"location":"patterns/smart_read/#date-format-for-string-columns","title":"Date Format for String Columns","text":"<p>If your date column is stored as a string (not a native timestamp), you must specify the <code>date_format</code> so Odibi can generate the correct SQL conversion.</p>"},{"location":"patterns/smart_read/#supported-formats","title":"Supported Formats","text":"Format Pattern Database Example <code>oracle</code> DD-MON-YY Oracle <code>20-APR-24 07:11:01.0</code> <code>oracle_sqlserver</code> DD-MON-YY SQL Server <code>20-APR-24 07:11:01.0</code> <code>sql_server</code> CONVERT style 120 SQL Server <code>2024-04-20 07:11:01</code> <code>us</code> MM/DD/YYYY Any <code>04/20/2024 07:11:01</code> <code>eu</code> DD/MM/YYYY Any <code>20/04/2024 07:11:01</code> <code>iso</code> YYYY-MM-DDTHH:MM:SS Any <code>2024-04-20T07:11:01</code> <p>Oracle dates in SQL Server</p> <p>If your SQL Server database has date columns stored as strings in Oracle format (DD-MON-YY like <code>20-APR-24</code>), use <code>oracle_sqlserver</code> instead of <code>oracle</code>.</p>"},{"location":"patterns/smart_read/#example-oracle-date-format","title":"Example: Oracle Date Format","text":"<pre><code>read:\n  connection: \"oracle_db\"\n  format: \"sql\"\n  table: \"PRODUCTION.EVENTS\"\n  incremental:\n    column: \"EVENT_START\"\n    lookback: 3\n    unit: \"day\"\n    date_format: \"oracle\"  # Handles DD-MON-YY format\n\nwrite:\n  connection: \"bronze\"\n  format: \"delta\"\n  table: \"events_raw\"\n  mode: \"append\"\n</code></pre> <p>This generates SQL like: <pre><code>SELECT * FROM PRODUCTION.EVENTS\nWHERE TO_TIMESTAMP(EVENT_START, 'DD-MON-RR HH24:MI:SS.FF') &gt;= TO_TIMESTAMP('03-JAN-26 12:00:00', 'DD-MON-RR HH24:MI:SS')\n</code></pre></p>"},{"location":"patterns/smart_read/#comparison-with-legacy-pattern","title":"Comparison with Legacy Pattern","text":""},{"location":"patterns/smart_read/#old-way-manual","title":"\u274c Old Way (Manual)","text":"<p>You had to write two queries and know the SQL dialect.</p> <pre><code>read:\n  query: \"SELECT * FROM orders WHERE updated_at &gt;= DATEADD(DAY, -1, GETDATE())\"\nwrite:\n  first_run_query: \"SELECT * FROM orders\"\n</code></pre>"},{"location":"patterns/smart_read/#new-way-smart-read","title":"\u2705 New Way (Smart Read)","text":"<p>Configuration is declarative and dialect-agnostic.</p> <pre><code>read:\n  table: \"orders\"\n  incremental:\n    column: \"updated_at\"\n    lookback: 1\n    unit: \"day\"\n</code></pre>"},{"location":"patterns/smart_read/#faq","title":"FAQ","text":"<p>Q: What if I want to reload all history manually? A: You can simply delete the target table (or folder) in your data lake. The next run will detect it's missing and trigger the full historical load.</p> <p>Q: Does this work with <code>depends_on</code>? A: This feature is for Ingestion Nodes (Node 1) that read from external systems. Downstream nodes automatically benefit because they receive the data frame produced by Node 1.</p> <p>Q: Can I mix this with custom SQL? A: No. If you provide a <code>query</code> in the <code>read</code> section, Odibi respects your manual query and ignores the <code>incremental</code> block.</p>"},{"location":"patterns/sql_server_merge/","title":"Pattern: SQL Server Merge/Upsert","text":"<p>Status: Core Pattern (Phase 4 Complete) Target: Azure SQL Database, SQL Server Engine: Spark, Pandas, Polars Strategy: T-SQL MERGE via Staging Table Idempotent: Yes (by key)</p>"},{"location":"patterns/sql_server_merge/#what-is-this-for-beginners","title":"What is This? (For Beginners)","text":"<p>The Problem You're Solving:</p> <p>You have data in Databricks/Delta Lake (your \"data lake\") and need to sync it to Azure SQL Server (your \"reporting database\" for Power BI, apps, etc.).</p> <p>Why Not Just Overwrite?</p> <p>Imagine you have 1 million customer records. Every day, only 100 customers change their address. With <code>overwrite</code>, you'd delete all 1 million rows and re-insert them\u2014slow and wasteful. With <code>merge</code>, you only update those 100 changed rows.</p> <p>What is a MERGE?</p> <p>A MERGE (also called \"upsert\") does three things in one operation: - INSERT new rows that don't exist in the target - UPDATE existing rows that have changed - DELETE rows that should be removed (optional)</p> <p>What is a \"Staging Table\"?</p> <p>A staging table is a temporary holding area. Odibi: 1. Writes your data to <code>[staging].[your_table_staging]</code> 2. Runs a MERGE from staging \u2192 target table 3. Leaves the staging table for debugging (it gets overwritten next run)</p>"},{"location":"patterns/sql_server_merge/#problem","title":"Problem","text":"<p>Syncing data from Databricks/Delta Lake to Azure SQL Server for Power BI or operational systems. Standard JDBC writes only support basic modes (<code>overwrite</code>, <code>append</code>), forcing you to either:</p> <ol> <li>Full overwrite - Inefficient for large tables</li> <li>Manual staging tables + stored procedures - Tedious and error-prone</li> </ol>"},{"location":"patterns/sql_server_merge/#solution","title":"Solution","text":"<p>Odibi's SQL Server Merge uses a staging table pattern:</p> <ol> <li>Write source DataFrame to a staging table</li> <li>Execute T-SQL <code>MERGE</code> statement against the target</li> <li>Return insert/update/delete counts</li> </ol> <p>This provides incremental upsert with full control over conditions.</p>"},{"location":"patterns/sql_server_merge/#quick-start","title":"Quick Start","text":""},{"location":"patterns/sql_server_merge/#your-first-merge-complete-example","title":"Your First Merge (Complete Example)","text":"<p>If you're new to SQL Server merge, start here. This is a complete, working example:</p> <pre><code># 1. First, define your SQL Server connection\nconnections:\n  azure_sql:\n    type: sql_server\n    host: your-server.database.windows.net\n    database: your-database\n    username: ${SQL_USER}        # Use environment variable\n    password: ${SQL_PASSWORD}    # Never hardcode passwords!\n    driver: \"ODBC Driver 18 for SQL Server\"\n\n# 2. Then, create a node that syncs data to SQL Server\nnodes:\n  - name: sync_orders_to_sql\n    read:\n      connection: delta_lake     # Read from your data lake\n      format: delta\n      table: silver.orders\n    write:\n      connection: azure_sql      # Write to SQL Server\n      format: sql_server\n      table: dbo.orders          # Target table (schema.table)\n      mode: merge                # Use MERGE instead of overwrite\n      merge_keys: [order_id]     # Column(s) that identify each row\n      merge_options:\n        auto_create_table: true  # Create table on first run\n        audit_cols:\n          created_col: created_at\n          updated_col: updated_at\n</code></pre> <p>What this does: 1. Reads orders from your Delta Lake silver layer 2. Creates <code>dbo.orders</code> table in SQL Server (first run only) 3. Inserts new orders, updates changed orders 4. Automatically tracks when each row was created/updated</p>"},{"location":"patterns/sql_server_merge/#minimal-config","title":"Minimal Config","text":"<pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: silver.orders\n  mode: merge\n  merge_keys: [order_id]\n</code></pre>"},{"location":"patterns/sql_server_merge/#full-config","title":"Full Config","text":"<pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: sales.fact_orders\n  mode: merge\n  merge_keys: [DateId, store_id]\n  merge_options:\n    update_condition: \"source._hash_diff != target._hash_diff\"\n    delete_condition: \"source._is_deleted = 1\"\n    insert_condition: \"source.is_valid = 1\"\n    exclude_columns: [_hash_diff, _is_deleted]\n    staging_schema: staging\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n    validations:\n      check_null_keys: true\n      check_duplicate_keys: true\n      fail_on_validation_error: true\n    # Phase 4 options:\n    auto_create_schema: true\n    auto_create_table: true\n    primary_key_on_merge_keys: true  # Create PK on merge keys for performance\n    batch_size: 10000\n    schema_evolution:\n      mode: evolve\n      add_columns: true\n</code></pre>"},{"location":"patterns/sql_server_merge/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Source DataFrame  \u2502\n\u2502   (Spark/Pandas/    \u2502\n\u2502    Polars)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Validate Keys    \u2502  Check for NULL/duplicate merge keys\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2. Write to Staging \u2502  [staging].[table_staging]\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 3. Execute T-SQL    \u2502  MERGE target USING staging\n\u2502    MERGE            \u2502  ON (keys match)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 4. Return Counts    \u2502  inserted: 50, updated: 10, deleted: 2\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"patterns/sql_server_merge/#generated-t-sql","title":"Generated T-SQL","text":"<pre><code>MERGE [sales].[fact_orders] AS target\nUSING [staging].[fact_orders_staging] AS source\nON target.[DateId] = source.[DateId] AND target.[store_id] = source.[store_id]\n\nWHEN MATCHED AND source._hash_diff != target._hash_diff THEN\n    UPDATE SET\n        [value] = source.[value],\n        [updated_ts] = GETUTCDATE()\n\nWHEN MATCHED AND source._is_deleted = 1 THEN\n    DELETE\n\nWHEN NOT MATCHED BY TARGET AND source.is_valid = 1 THEN\n    INSERT ([DateId], [store_id], [value], [created_ts], [updated_ts])\n    VALUES (source.[DateId], source.[store_id], source.[value], GETUTCDATE(), GETUTCDATE())\n\nOUTPUT $action INTO @MergeActions;\n\nSELECT\n    SUM(CASE WHEN action = 'INSERT' THEN 1 ELSE 0 END) AS inserted,\n    SUM(CASE WHEN action = 'UPDATE' THEN 1 ELSE 0 END) AS updated,\n    SUM(CASE WHEN action = 'DELETE' THEN 1 ELSE 0 END) AS deleted\nFROM @MergeActions;\n</code></pre>"},{"location":"patterns/sql_server_merge/#configuration-reference","title":"Configuration Reference","text":""},{"location":"patterns/sql_server_merge/#merge_keys-required","title":"<code>merge_keys</code> (Required)","text":"<p>What are merge keys?</p> <p>Merge keys tell SQL Server how to match rows between your source data and the target table. They answer the question: \"Is this row new, or does it already exist?\"</p> <p>Example: If your table has customers identified by <code>customer_id</code>, then <code>customer_id</code> is your merge key. When Odibi sees <code>customer_id = 123</code> in the source, it checks if <code>customer_id = 123</code> exists in the target: - If yes \u2192 UPDATE that row - If no \u2192 INSERT a new row</p> <p>Single vs Composite Keys:</p> <pre><code>merge_keys: [order_id]                    # Single key - one column identifies a row\nmerge_keys: [store_id, product_id]       # Composite key - two columns together identify a row\nmerge_keys: [DateId, store_id, Shift]     # Multi-column - all three must match\n</code></pre> <p>How do I know what my merge keys are?</p> <p>Ask yourself: \"What columns make each row unique?\" This is usually your primary key or business key.</p>"},{"location":"patterns/sql_server_merge/#merge_options","title":"<code>merge_options</code>","text":"Option Type Default Description <code>update_condition</code> string None SQL condition for WHEN MATCHED UPDATE <code>delete_condition</code> string None SQL condition for WHEN MATCHED DELETE <code>insert_condition</code> string None SQL condition for WHEN NOT MATCHED INSERT <code>exclude_columns</code> list [] Columns to exclude from merge <code>staging_schema</code> string <code>staging</code> Schema for staging table <code>audit_cols</code> object None Auto-populate created/updated timestamps <code>validations</code> object None Pre-merge validation checks <code>auto_create_schema</code> bool false Auto-create schema if missing <code>auto_create_table</code> bool false Auto-create target table from DataFrame <code>primary_key_on_merge_keys</code> bool false Create clustered PK on merge keys (with auto_create_table) <code>index_on_merge_keys</code> bool false Create nonclustered index on merge keys <code>schema_evolution</code> object None Handle schema differences <code>batch_size</code> int None Chunk large writes for memory efficiency <code>incremental</code> bool false Read target hashes, compare in engine, only write changed rows to staging <code>hash_column</code> string None Pre-computed hash column for change detection (auto-detects <code>_hash_diff</code>) <code>change_detection_columns</code> list None Columns to compute hash from (defaults to all non-key columns)"},{"location":"patterns/sql_server_merge/#conditions","title":"Conditions","text":"<p>What are conditions?</p> <p>Conditions let you control WHEN to update, insert, or delete. They're optional but powerful.</p> <p>Why use <code>update_condition</code>?</p> <p>Without a condition, MERGE updates EVERY matched row\u2014even if nothing changed. This is wasteful. With <code>update_condition</code>, you only update rows that actually changed:</p> <pre><code>update_condition: \"source._hash_diff != target._hash_diff\"\n</code></pre> <p>This says: \"Only update if the hash (a fingerprint of the data) is different.\"</p> <p>Why use <code>delete_condition</code>?</p> <p>Soft deletes: Instead of removing rows from your source, you flag them with <code>_is_deleted = 1</code>. The MERGE then deletes them from the target:</p> <pre><code>delete_condition: \"source._is_deleted = 1\"\n</code></pre> <p>Why use <code>insert_condition</code>?</p> <p>Skip invalid rows: Only insert rows that meet quality criteria:</p> <pre><code>insert_condition: \"source.is_valid = 1\"\n</code></pre> <p>Important: Use <code>source.</code> and <code>target.</code> prefixes to refer to columns in your conditions.</p>"},{"location":"patterns/sql_server_merge/#audit-columns","title":"Audit Columns","text":"<p>What are audit columns?</p> <p>Audit columns automatically track WHEN rows were created or last updated. This is essential for debugging and compliance.</p> <pre><code>audit_cols:\n  created_col: created_ts   # Set to current time on INSERT only\n  updated_col: updated_ts   # Set to current time on INSERT and UPDATE\n</code></pre> <p>What happens: - When a NEW row is inserted: both <code>created_ts</code> and <code>updated_ts</code> are set to now - When an EXISTING row is updated: only <code>updated_ts</code> is set to now (created_ts stays the same)</p> <p>Do I need to add these columns to my DataFrame?</p> <p>No! Odibi automatically: 1. Adds these columns to the target table (if using <code>auto_create_table</code>) 2. Populates them with <code>GETUTCDATE()</code> during the MERGE</p>"},{"location":"patterns/sql_server_merge/#validations","title":"Validations","text":"<p>What are validations?</p> <p>Validations check your data BEFORE the merge runs, catching problems early.</p> <p>Why use them?</p> <ul> <li>NULL keys cause MERGE failures (SQL Server can't match NULL = NULL)</li> <li>Duplicate keys cause unpredictable results (which duplicate wins?)</li> </ul> <p>Check data quality before merge:</p> <pre><code>validations:\n  check_null_keys: true           # Fail if merge keys contain NULL\n  check_duplicate_keys: true      # Fail if duplicate key combinations\n  fail_on_validation_error: true  # Fail vs. warn\n</code></pre>"},{"location":"patterns/sql_server_merge/#phase-4-advanced-features","title":"Phase 4: Advanced Features","text":""},{"location":"patterns/sql_server_merge/#auto-schema-creation","title":"Auto Schema Creation","text":"<p>Create the schema if it doesn't exist:</p> <pre><code>merge_options:\n  auto_create_schema: true   # Runs: CREATE SCHEMA [staging]\n</code></pre>"},{"location":"patterns/sql_server_merge/#auto-table-creation","title":"Auto Table Creation","text":"<p>Create the target table from DataFrame schema if missing:</p> <pre><code>merge_options:\n  auto_create_table: true\n</code></pre>"},{"location":"patterns/sql_server_merge/#primary-key-on-merge-keys","title":"Primary Key on Merge Keys","text":"<p>Automatically create a clustered primary key on your merge keys when auto-creating the table. This: - Enforces uniqueness (prevents duplicate key combinations) - Improves MERGE performance (SQL Server uses the PK for the ON clause) - Creates a clustered index (physically orders data by these columns)</p> <pre><code>merge_options:\n  auto_create_table: true\n  primary_key_on_merge_keys: true  # Creates: PK_fact_orders PRIMARY KEY CLUSTERED ([DateId], [store_id])\n</code></pre>"},{"location":"patterns/sql_server_merge/#index-on-merge-keys","title":"Index on Merge Keys","text":"<p>If you already have a primary key elsewhere but want to speed up merges, create a nonclustered index instead:</p> <pre><code>merge_options:\n  index_on_merge_keys: true  # Creates: IX_fact_orders_DateId_store_id NONCLUSTERED ([DateId], [store_id])\n</code></pre> <p>Note: Use <code>primary_key_on_merge_keys</code> OR <code>index_on_merge_keys</code>, not both. Primary key takes precedence if both are set.</p> <p>Type mapping from DataFrame to SQL Server:</p> Pandas/Polars Type SQL Server Type int64 / Int64 BIGINT int32 / Int32 INT float64 / Float64 FLOAT object / Utf8 NVARCHAR(MAX) datetime64 / Datetime DATETIME2 bool / Boolean BIT"},{"location":"patterns/sql_server_merge/#schema-evolution","title":"Schema Evolution","text":"<p>Control how schema differences are handled:</p> <pre><code>merge_options:\n  schema_evolution:\n    mode: strict    # Default: fail if schemas differ\n    # mode: evolve  # Add new columns via ALTER TABLE\n    # mode: ignore  # Write only matching columns\n    add_columns: true  # With 'evolve', run ALTER TABLE ADD COLUMN\n</code></pre> Mode Behavior <code>strict</code> Fail if DataFrame has columns not in target table <code>evolve</code> Add new columns (if <code>add_columns: true</code>) <code>ignore</code> Write only columns that exist in target table"},{"location":"patterns/sql_server_merge/#batch-processing","title":"Batch Processing","text":"<p>Chunk large DataFrames for memory efficiency:</p> <pre><code>merge_options:\n  batch_size: 10000   # Write 10k rows at a time to staging\n</code></pre>"},{"location":"patterns/sql_server_merge/#incremental-merge-optimization","title":"Incremental Merge Optimization","text":"<p>What is incremental merge?</p> <p>Without <code>incremental</code>, Odibi writes ALL your source rows to the staging table, then runs MERGE. If you have 1 million rows but only 100 changed, you're still writing 1 million rows to staging\u2014wasteful!</p> <p>With <code>incremental: true</code>, Odibi: 1. Reads the existing data from your target table (just the keys and hash) 2. Compares it with your source data IN MEMORY (Spark/Pandas/Polars) 3. Filters to only the rows that are NEW or CHANGED 4. Writes only those rows to staging 5. Runs MERGE on the smaller set</p> <p>When to use it:</p> <ul> <li>Large tables (100K+ rows)</li> <li>Daily syncs where most data doesn't change</li> <li>When staging writes are slow</li> </ul> <p>When NOT to use it:</p> <ul> <li>Small tables (just write everything, it's fast)</li> <li>Full refreshes where everything changes</li> <li>First-time loads (there's nothing to compare against)</li> </ul> <pre><code>merge_options:\n  incremental: true   # Only write changed rows to staging\n</code></pre> <p>How it works: 1. Reads target table's merge keys and hash column 2. Compares in Spark/Pandas/Polars to determine which rows changed 3. Only writes changed rows to staging table 4. Runs MERGE only on the changed subset</p> <p>Performance benefit: If only 100 of 1M rows changed, staging table has 100 rows instead of 1M\u201410x faster!</p>"},{"location":"patterns/sql_server_merge/#change-detection-options","title":"Change Detection Options","text":"<p>Option 1: Use existing hash column</p> <p>If your DataFrame already has a hash column (e.g., from SCD2 transformer):</p> <pre><code>merge_options:\n  incremental: true\n  hash_column: _hash_diff   # Use pre-computed hash\n</code></pre> <p>Option 2: Auto-detect <code>_hash_diff</code></p> <p>Odibi auto-detects a column named <code>_hash_diff</code> if present:</p> <pre><code>merge_options:\n  incremental: true   # Auto-uses _hash_diff if present\n</code></pre> <p>Option 3: Specify columns for hash computation</p> <p>If no hash column exists, specify which columns to use for change detection:</p> <pre><code>merge_options:\n  incremental: true\n  change_detection_columns: [value, quantity, status]  # Only hash these columns\n</code></pre> <p>Option 4: Hash all non-key columns (default)</p> <p>If no hash column or change_detection_columns specified, computes hash from all non-key columns:</p> <pre><code>merge_options:\n  incremental: true   # Hashes all columns except merge_keys\n</code></pre>"},{"location":"patterns/sql_server_merge/#overwrite-strategies","title":"Overwrite Strategies","text":"<p>For non-merge writes, use enhanced overwrite strategies:</p> <pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: fact.summary\n  mode: overwrite\n  overwrite_options:\n    strategy: truncate_insert  # Default\n    # strategy: drop_create    # Drop and recreate table\n    # strategy: delete_insert  # DELETE FROM then INSERT\n</code></pre> Strategy Behavior Best For <code>truncate_insert</code> TRUNCATE TABLE then INSERT Fast, needs TRUNCATE permission <code>drop_create</code> DROP TABLE, CREATE, INSERT Schema refresh <code>delete_insert</code> DELETE FROM then INSERT Limited permissions"},{"location":"patterns/sql_server_merge/#engine-parity","title":"Engine Parity","text":"<p>All three engines support SQL Server Merge:</p> Feature Spark Pandas Polars Basic Merge \u2705 \u2705 \u2705 Composite Keys \u2705 \u2705 \u2705 Conditions (update/delete/insert) \u2705 \u2705 \u2705 Audit Columns \u2705 \u2705 \u2705 Validations \u2705 \u2705 \u2705 Auto Schema Creation \u2705 \u2705 \u2705 Auto Table Creation \u2705 \u2705 \u2705 Schema Evolution \u2705 \u2705 \u2705 Batch Processing \u2705 \u2705 \u2705 Enhanced Overwrite \u2705 \u2705 \u2705"},{"location":"patterns/sql_server_merge/#examples","title":"Examples","text":""},{"location":"patterns/sql_server_merge/#example-1-sales-fact-table-sync","title":"Example 1: Sales Fact Table Sync","text":"<p>Sync sales metrics data to Azure SQL for Power BI:</p> <pre><code>nodes:\n  - id: sync_sales_to_sql\n    name: \"Sync Sales to Azure SQL\"\n    read:\n      connection: delta_lake\n      format: delta\n      table: gold.fact_orders\n    write:\n      connection: azure_sql\n      format: sql_server\n      table: sales.fact_orders\n      mode: merge\n      merge_keys: [DateId, store_id]\n      merge_options:\n        update_condition: \"source._hash_diff != target._hash_diff\"\n        exclude_columns: [_hash_diff]\n        audit_cols:\n          created_col: _sys_created_at\n          updated_col: _sys_updated_at\n</code></pre>"},{"location":"patterns/sql_server_merge/#example-2-dimension-with-soft-deletes","title":"Example 2: Dimension with Soft Deletes","text":"<p>Handle soft deletes by flagging deleted records:</p> <pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: dim.customers\n  mode: merge\n  merge_keys: [customer_id]\n  merge_options:\n    delete_condition: \"source._is_deleted = 1\"\n    exclude_columns: [_is_deleted]\n</code></pre>"},{"location":"patterns/sql_server_merge/#example-3-first-load-with-auto-create","title":"Example 3: First Load with Auto-Create","text":"<p>Auto-create schema, table, and primary key on first load:</p> <pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: new_schema.new_table\n  mode: merge\n  merge_keys: [id]\n  merge_options:\n    auto_create_schema: true\n    auto_create_table: true\n    primary_key_on_merge_keys: true  # Creates PK for better performance\n</code></pre>"},{"location":"patterns/sql_server_merge/#example-4-schema-evolution","title":"Example 4: Schema Evolution","text":"<p>Add new columns automatically as your source evolves:</p> <pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: reporting.metrics\n  mode: merge\n  merge_keys: [metric_id, date]\n  merge_options:\n    schema_evolution:\n      mode: evolve\n      add_columns: true\n</code></pre>"},{"location":"patterns/sql_server_merge/#example-5-incremental-merge-optimization","title":"Example 5: Incremental Merge Optimization","text":"<p>Optimize large table syncs when only a small percentage of rows change:</p> <pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: gold.fact_orders\n  mode: merge\n  merge_keys: [DateId, store_id]\n  merge_options:\n    incremental: true                           # Only write changed rows\n    hash_column: _hash_diff                     # Use existing hash column\n    update_condition: \"source._hash_diff != target._hash_diff\"\n    exclude_columns: [_hash_diff]\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre> <p>Result: If syncing 1M rows daily but only 1K changed, staging table contains 1K rows instead of 1M\u201410x faster writes.</p>"},{"location":"patterns/sql_server_merge/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/sql_server_merge/#target-table-does-not-exist","title":"\"Target table does not exist\"","text":"<p>The table must exist for merge. Either: - Create it manually first - Use <code>auto_create_table: true</code> - Use <code>mode: overwrite</code> for initial load, then switch to <code>merge</code></p>"},{"location":"patterns/sql_server_merge/#merge-key-validation-failed-null-values","title":"\"Merge key validation failed: NULL values\"","text":"<p>Your merge keys contain NULL values. Fix your source data or disable:</p> <pre><code>validations:\n  check_null_keys: false\n</code></pre>"},{"location":"patterns/sql_server_merge/#merge-key-validation-failed-duplicates","title":"\"Merge key validation failed: duplicates\"","text":"<p>Duplicate key combinations exist in your source. Deduplicate first or disable:</p> <pre><code>validations:\n  check_duplicate_keys: false\n</code></pre>"},{"location":"patterns/sql_server_merge/#schema-evolution-mode-is-strict-but-dataframe-has-new-columns","title":"\"Schema evolution mode is 'strict' but DataFrame has new columns\"","text":"<p>Your DataFrame has columns not in the target table. Options: - Add columns to target table manually - Use <code>schema_evolution.mode: evolve</code> with <code>add_columns: true</code> - Use <code>schema_evolution.mode: ignore</code> to skip new columns</p>"},{"location":"patterns/sql_server_merge/#permission-errors","title":"Permission errors","text":"<p>Ensure your SQL Server user has permissions for: - <code>CREATE TABLE</code> (for auto_create_table) - <code>CREATE SCHEMA</code> (for auto_create_schema) - <code>ALTER TABLE</code> (for schema evolution) - <code>TRUNCATE TABLE</code> (for truncate_insert strategy)</p>"},{"location":"patterns/sql_server_merge/#connection-setup","title":"Connection Setup","text":""},{"location":"patterns/sql_server_merge/#connection-configuration","title":"Connection Configuration","text":"<pre><code>connections:\n  azure_sql:\n    type: sql_server\n    host: your-server.database.windows.net\n    database: your-database\n    username: ${SQL_USER}\n    password: ${SQL_PASSWORD}\n    driver: \"ODBC Driver 18 for SQL Server\"\n</code></pre>"},{"location":"patterns/sql_server_merge/#azure-ad-authentication","title":"Azure AD Authentication","text":"<pre><code>connections:\n  azure_sql:\n    type: sql_server\n    host: your-server.database.windows.net\n    database: your-database\n    authentication: ActiveDirectoryInteractive\n</code></pre>"},{"location":"patterns/sql_server_merge/#faq-frequently-asked-questions","title":"FAQ (Frequently Asked Questions)","text":""},{"location":"patterns/sql_server_merge/#q-should-i-use-merge-or-overwrite","title":"Q: Should I use <code>merge</code> or <code>overwrite</code>?","text":"<p>Use <code>merge</code> when: - You want to keep existing data and only add/update changes - Your table is large and only a small portion changes - You need to track created/updated timestamps</p> <p>Use <code>overwrite</code> when: - You want to replace all data every time - Your table is small (overwrite is simpler) - You're doing a full refresh/rebuild</p>"},{"location":"patterns/sql_server_merge/#q-what-happens-on-the-first-run","title":"Q: What happens on the first run?","text":"<p>If the table doesn't exist and you have <code>auto_create_table: true</code>: 1. Odibi creates the table from your DataFrame schema 2. Adds audit columns if configured 3. Creates primary key/index if configured 4. Inserts all rows (everything is \"new\")</p>"},{"location":"patterns/sql_server_merge/#q-how-do-i-know-if-my-merge-is-working","title":"Q: How do I know if my merge is working?","text":"<p>Check the logs! You'll see: <pre><code>Starting SQL Server MERGE, target_table=sales.fact_orders, merge_keys=[DateId, store_id]\nMERGE completed: inserted=50, updated=10, deleted=0\n</code></pre></p>"},{"location":"patterns/sql_server_merge/#q-why-are-my-audit-columns-null","title":"Q: Why are my audit columns NULL?","text":"<p>This was a bug fixed in v2.2.0. If you created tables before this fix, run: <pre><code>UPDATE [schema].[table] \nSET created_ts = GETUTCDATE(), updated_ts = GETUTCDATE() \nWHERE created_ts IS NULL\n</code></pre></p>"},{"location":"patterns/sql_server_merge/#q-how-do-i-handle-deletes","title":"Q: How do I handle deletes?","text":"<p>Two options:</p> <p>Soft delete (recommended): Add an <code>_is_deleted</code> flag to your source, then use: <pre><code>delete_condition: \"source._is_deleted = 1\"\n</code></pre></p> <p>Hard delete: Not supported via MERGE. Use a separate DELETE statement after the merge.</p>"},{"location":"patterns/sql_server_merge/#q-can-i-merge-to-multiple-tables","title":"Q: Can I merge to multiple tables?","text":"<p>Yes! Create multiple nodes, each with its own <code>write</code> block targeting different tables.</p>"},{"location":"patterns/sql_server_merge/#q-whats-the-difference-between-primary_key_on_merge_keys-and-index_on_merge_keys","title":"Q: What's the difference between <code>primary_key_on_merge_keys</code> and <code>index_on_merge_keys</code>?","text":"<ul> <li>Primary key: Enforces uniqueness, creates clustered index, only one per table</li> <li>Index: Speeds up queries, allows duplicates, can have many per table</li> </ul> <p>Use primary key if your merge keys ARE your primary key. Use index if you already have a different primary key.</p>"},{"location":"patterns/sql_server_merge/#related","title":"Related","text":"<ul> <li>Merge/Upsert Pattern (Delta Lake)</li> <li>Connections Guide</li> <li>Azure Setup Guide</li> </ul>"},{"location":"patterns/windowed_reprocess/","title":"Pattern: Windowed Reprocess (Silver \u2192 Gold Aggregates)","text":"<p>Status: Core Pattern Layer: Gold (aggregated/BI-ready) Engine: Spark Batch Write Mode: <code>overwrite</code> (partition-specific) Idempotent: Yes (recalculated)  </p>"},{"location":"patterns/windowed_reprocess/#problem","title":"Problem","text":"<p>You have a Gold aggregate table (e.g., daily sales summary). Late-arriving data in Silver invalidates yesterday's numbers. You need to: - Fix aggregates when new data arrives - Avoid double-counting (can't just add new rows) - Keep calculations simple (always correct, never patched)</p> <p>How do you maintain accurate aggregates without complex update logic?</p>"},{"location":"patterns/windowed_reprocess/#solution","title":"Solution","text":"<p>Instead of patching aggregates with updates (error-prone), recalculate the entire time window and replace it.</p> <p>Principle: \"Rebuild the Bucket, Don't Patch the Hole\"</p>"},{"location":"patterns/windowed_reprocess/#how-it-works","title":"How It Works","text":""},{"location":"patterns/windowed_reprocess/#the-reprocess-pattern","title":"The Reprocess Pattern","text":"<ol> <li>Identify the window (e.g., \"Last 7 days\", \"This month\")</li> <li>Read Silver filtered to that window</li> <li>Recalculate aggregate (SUM, COUNT, AVG, etc.)</li> <li>Write to Gold with Dynamic Partition Overwrite</li> </ol> <p>If late data arrives in the last 7 days, next run recalculates those days\u2014automatically fixing aggregates.</p>"},{"location":"patterns/windowed_reprocess/#step-by-step-example","title":"Step-by-Step Example","text":""},{"location":"patterns/windowed_reprocess/#scenario-daily-sales-summary","title":"Scenario: Daily Sales Summary","text":"<p>Silver Table (Orders, with timestamps):</p> <p>Day 1 (Initial load on 2025-11-01 at 10:00): <pre><code>order_id | order_date | amount | created_at\n---------|------------|--------|-------------------\n1        | 2025-11-01 | 100    | 2025-11-01 10:00\n2        | 2025-11-01 | 50     | 2025-11-01 10:30\n3        | 2025-10-31 | 200    | 2025-11-01 10:45\n</code></pre></p> <p>Run 1 (Calculate last 7 days: 2025-10-25 to 2025-11-01):</p> <pre><code>SELECT\n  DATE(order_date) as order_date,\n  SUM(amount) as total_sales,\n  COUNT(*) as order_count\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n</code></pre> <p>Gold Result (Days 25-Oct to 1-Nov): <pre><code>order_date | total_sales | order_count\n-----------|-------------|-------------\n2025-10-31 | 200         | 1\n2025-11-01 | 150         | 2\n</code></pre></p> <p>Partition written: <code>order_date=2025-10-31</code>, <code>order_date=2025-11-01</code></p>"},{"location":"patterns/windowed_reprocess/#day-2-late-data-arrives","title":"Day 2: Late Data Arrives","text":"<p>Silver Table (New data arrived at 14:00): <pre><code>order_id | order_date | amount | created_at\n---------|------------|--------|-------------------\n1        | 2025-11-01 | 100    | 2025-11-01 10:00\n2        | 2025-11-01 | 50     | 2025-11-01 10:30\n3        | 2025-10-31 | 200    | 2025-11-01 10:45\n4        | 2025-11-01 | 75     | 2025-11-02 14:00  \u2190 LATE DATA (same day, arrived late)\n</code></pre></p> <p>Run 2 (Recalculate last 7 days: 2025-10-25 to 2025-11-02):</p> <pre><code>SELECT\n  DATE(order_date) as order_date,\n  SUM(amount) as total_sales,\n  COUNT(*) as order_count\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n</code></pre> <p>Gold Result (Days 25-Oct to 2-Nov): <pre><code>order_date | total_sales | order_count\n-----------|-------------|-------------\n2025-10-31 | 200         | 1\n2025-11-01 | 225         | 3              \u2190 UPDATED (was 150, now 225)\n</code></pre></p> <p>Write Mode: Dynamic Partition Overwrite - Existing partition <code>order_date=2025-10-31</code> is untouched - Partition <code>order_date=2025-11-01</code> is replaced entirely (was 2 rows, now 3 rows)</p> <p>No double-counting: The aggregate is recalculated from scratch, not patched.</p>"},{"location":"patterns/windowed_reprocess/#why-this-works","title":"Why This Works","text":""},{"location":"patterns/windowed_reprocess/#without-windowed-reprocess-wrong","title":"Without Windowed Reprocess (WRONG)","text":"<pre><code>-- Don't do this\nUPDATE gold.sales\nSET total_sales = total_sales + 75,\n    order_count = order_count + 1\nWHERE order_date = '2025-11-01'\n</code></pre> <p>Problems: - If this query runs twice, you add 75 twice (double-counting) - If you run it out-of-order, you corrupt data - Requires tracking \"what did I update?\"</p>"},{"location":"patterns/windowed_reprocess/#with-windowed-reprocess-right","title":"With Windowed Reprocess (RIGHT)","text":"<pre><code>-- Recalculate the entire 7-day window\nSELECT\n  DATE(order_date),\n  SUM(amount),\n  COUNT(*)\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\n\n-- Write with Dynamic Partition Overwrite\n-- Entire partition is replaced\n</code></pre> <p>Advantages: - Idempotent (run 10 times = same result) - No double-counting (always fresh calculation) - Simple logic (standard SQL aggregate)</p>"},{"location":"patterns/windowed_reprocess/#odibi-yaml","title":"Odibi YAML","text":""},{"location":"patterns/windowed_reprocess/#simple-daily-aggregate","title":"Simple Daily Aggregate","text":"<pre><code>- id: gold_daily_sales\n  name: \"Daily Sales Summary (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            DATE(order_date) as order_date,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            MIN(order_date) as first_order_ts,\n            MAX(order_date) as last_order_ts\n          FROM silver.orders\n          WHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\n          GROUP BY DATE(order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.daily_sales\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#monthly-aggregate-wider-window","title":"Monthly Aggregate (Wider Window)","text":"<pre><code>- id: gold_monthly_sales\n  name: \"Monthly Sales Summary (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            DATE_TRUNC('month', order_date) as month,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            COUNT(DISTINCT customer_id) as unique_customers\n          FROM silver.orders\n          WHERE DATE_TRUNC('month', order_date) &gt;= DATE_TRUNC('month', DATE_SUB(CURRENT_DATE(), 90))\n          GROUP BY DATE_TRUNC('month', order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.monthly_sales\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#multi-grain-aggregates","title":"Multi-Grain Aggregates","text":"<pre><code>- id: gold_sales_by_region_day\n  name: \"Sales by Region &amp; Day (Gold)\"\n  depends_on: [merge_orders_silver]\n  read:\n    connection: adls_prod\n    format: delta\n    table: silver.orders\n  transform:\n    steps:\n      - sql: |\n          SELECT\n            region,\n            DATE(order_date) as order_date,\n            SUM(amount) as total_sales,\n            COUNT(*) as order_count,\n            AVG(amount) as avg_order_value\n          FROM silver.orders\n          WHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 30)\n          GROUP BY region, DATE(order_date)\n  write:\n    connection: adls_prod\n    format: delta\n    table: gold.sales_by_region_day\n    mode: overwrite\n    options:\n      partitionOverwriteMode: dynamic\n</code></pre>"},{"location":"patterns/windowed_reprocess/#window-size-strategy","title":"Window Size Strategy","text":""},{"location":"patterns/windowed_reprocess/#how-far-back-should-the-window-be","title":"How Far Back Should the Window Be?","text":"<p>Rule of Thumb: 2-3x your SLA for late data.</p> SLA Window Example Same-day delivery (next day processed) 3-7 days Daily aggregate 1-week SLA 14-21 days Weekly aggregate End-of-month close (3-5 days) 30-45 days Monthly aggregate <p>Conservative approach: Recalculate 30 days back, even if only aggregating daily. It costs minimal compute.</p>"},{"location":"patterns/windowed_reprocess/#dynamic-partition-overwrite","title":"Dynamic Partition Overwrite","text":""},{"location":"patterns/windowed_reprocess/#why-it-matters","title":"Why It Matters","text":"<p>Scenario: Your table is partitioned by <code>order_date</code>:</p> <pre><code>gold/sales/\n\u251c\u2500\u2500 order_date=2025-11-01/\n\u251c\u2500\u2500 order_date=2025-10-31/\n\u251c\u2500\u2500 order_date=2025-10-30/\n\u2514\u2500\u2500 ... (30 days of data)\n</code></pre> <p>If you use full overwrite (default): - Entire table is replaced - All 30 days are rewritten (slow) - Other columns lose their data</p> <p>If you use dynamic partition overwrite: - Only <code>order_date=2025-11-01</code> (and other affected dates) are replaced - Unaffected dates remain untouched - Much faster</p>"},{"location":"patterns/windowed_reprocess/#enabling-in-odibi","title":"Enabling in Odibi","text":"<pre><code>write:\n  connection: adls_prod\n  format: delta\n  table: gold.daily_sales\n  mode: overwrite\n  options:\n    partitionOverwriteMode: dynamic\n</code></pre> <p>This is automatically enabled by Odibi's safe defaults (per Architecture Manifesto).</p>"},{"location":"patterns/windowed_reprocess/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/windowed_reprocess/#problem-aggregate-is-still-wrong","title":"Problem: Aggregate is Still Wrong","text":"<p>Causes: 1. Window is too short \u2192 Late data arriving outside window. Increase window size. 2. Wrong grouping \u2192 Missing a dimension (e.g., region). Check Silver data. 3. Stale Silver data \u2192 No new orders merged in. Check merge pipeline.</p> <p>Debug: <pre><code>-- Check what's in Silver for the window\nSELECT DATE(order_date), COUNT(*)\nFROM silver.orders\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY DATE(order_date)\nORDER BY order_date DESC;\n\n-- Compare to Gold\nSELECT order_date, COUNT(*) as count\nFROM gold.daily_sales\nWHERE order_date &gt;= DATE_SUB(CURRENT_DATE(), 7)\nGROUP BY order_date\nORDER BY order_date DESC;\n</code></pre></p>"},{"location":"patterns/windowed_reprocess/#problem-slow-rewrites","title":"Problem: Slow Rewrites","text":"<p>Causes: 1. Window too large \u2192 Recalculating 365 days every run. Reduce window or run less frequently. 2. No partitioning \u2192 Entire table is scanned. Add <code>partition by order_date</code> to Silver.</p> <p>Solution: <pre><code># Smaller window for frequent runs\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 3)\n\n# Larger window for nightly runs\nWHERE DATE(order_date) &gt;= DATE_SUB(CURRENT_DATE(), 30)\n</code></pre></p>"},{"location":"patterns/windowed_reprocess/#trade-offs","title":"Trade-Offs","text":""},{"location":"patterns/windowed_reprocess/#advantages","title":"Advantages","text":"<p>\u2713 Always correct (fresh calculation, not patched) \u2713 Idempotent (run multiple times = same result) \u2713 Self-healing (late data automatically fixes aggregates) \u2713 Simple logic (standard SQL, no complex update logic) \u2713 Fast (recalculate 7 days vs. maintain entire history)  </p>"},{"location":"patterns/windowed_reprocess/#disadvantages","title":"Disadvantages","text":"<p>\u2717 Requires recomputation (slower than patches, but worth it) \u2717 Assumes partitioning (without partitioning, rewrites entire table) \u2717 Assumes stateless logic (can't use row-level updates)  </p>"},{"location":"patterns/windowed_reprocess/#when-to-use","title":"When to Use","text":"<ul> <li>Always for Gold aggregates (KPIs, fact tables, summaries)</li> <li>Late-arriving data possible</li> <li>Queries can be re-executed without side effects</li> <li>Need guaranteed correctness over minimal compute</li> </ul>"},{"location":"patterns/windowed_reprocess/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Audit tables (use append)</li> <li>Streaming aggregates with sub-second latency (use Structured Streaming)</li> <li>Data with complex, stateful dependencies</li> </ul>"},{"location":"patterns/windowed_reprocess/#related-patterns","title":"Related Patterns","text":"<ul> <li>Merge/Upsert \u2192 Maintains clean Silver data that aggregates read from</li> <li>Append-Only Raw \u2192 Source of truth if aggregates need replay</li> </ul>"},{"location":"patterns/windowed_reprocess/#references","title":"References","text":"<ul> <li>Databricks: Dynamic Partition Overwrite</li> <li>Fundamentals of Data Engineering: Chapter on Aggregation</li> </ul>"},{"location":"playbook/","title":"The Odibi Playbook","text":"<p>Find your problem. Get the solution.</p>"},{"location":"playbook/#most-common-flows","title":"Most Common Flows","text":"I need to... Go here Start from zero Golden Path Copy a working config Canonical Examples Load only new rows Incremental Stateful Track dimension history SCD2 Pattern Validate my data Quality Gates"},{"location":"playbook/#if-you-only-read-3-pages","title":"If You Only Read 3 Pages...","text":"<ol> <li>Golden Path \u2014 Zero to running in 10 minutes</li> <li>Patterns Overview \u2014 Common solutions to common problems</li> <li>YAML Schema \u2014 All configuration options</li> </ol>"},{"location":"playbook/#find-your-problem","title":"Find Your Problem","text":""},{"location":"playbook/#bronze-layer-ingestion","title":"Bronze Layer: Ingestion","text":"<p>\"Get data from sources into your lakehouse reliably.\"</p> Problem Pattern Docs Load all files from a folder Append-only Pattern Only process new files since last run Rolling window Pattern Track exact high-water mark Stateful HWM Pattern Fail if source is empty or stale Contracts YAML Handle malformed records Bad records path YAML Extract from SQL Server JDBC read Example"},{"location":"playbook/#silver-layer-transformation","title":"Silver Layer: Transformation","text":"<p>\"Clean, deduplicate, and model your data.\"</p> Problem Pattern Docs Remove duplicates Deduplicate transformer YAML Keep latest record per key Dedupe with ordering YAML Track dimension changes over time SCD2 Pattern Upsert into target table Merge Pattern Validate output data quality Validation tests Feature Route bad rows for review Quarantine Feature"},{"location":"playbook/#gold-layer-analytics","title":"Gold Layer: Analytics","text":"<p>\"Build fact tables, aggregations, and semantic layers.\"</p> Problem Pattern Docs Build fact table with SK lookups Fact pattern Pattern Handle orphan records Orphan handling Pattern Pre-aggregate metrics Aggregation pattern Pattern Generate date dimension Date dimension Pattern"},{"location":"playbook/#decision-trees","title":"Decision Trees","text":""},{"location":"playbook/#choose-your-engine","title":"Choose Your Engine","text":"<pre><code>Data size?\n\u251c\u2500\u25ba &lt; 1GB \u2192 engine: pandas\n\u251c\u2500\u25ba 1-10GB \u2192 engine: polars\n\u2514\u2500\u25ba &gt; 10GB or Delta Lake \u2192 engine: spark\n</code></pre>"},{"location":"playbook/#choose-your-incremental-mode","title":"Choose Your Incremental Mode","text":"<pre><code>Source has timestamps?\n\u251c\u2500\u25ba Yes \u2192 mode: stateful (exact HWM tracking)\n\u2514\u2500\u25ba No\n    \u2514\u2500\u25ba Data arrives daily? \u2192 mode: rolling_window (lookback)\n    \u2514\u2500\u25ba Unknown pattern? \u2192 write.skip_if_unchanged: true\n</code></pre>"},{"location":"playbook/#choose-your-validation-approach","title":"Choose Your Validation Approach","text":"<pre><code>When to check?\n\u251c\u2500\u25ba Before processing (source quality) \u2192 contracts:\n\u2514\u2500\u25ba After processing (output quality) \u2192 validation.tests:\n    \u2514\u2500\u25ba Need to stop pipeline? \u2192 gate.on_failure: fail\n    \u2514\u2500\u25ba Soft warning OK? \u2192 gate.on_failure: warn\n</code></pre>"},{"location":"playbook/#choose-your-scd-type","title":"Choose Your SCD Type","text":"<pre><code>Need historical state?\n\u251c\u2500\u25ba No \u2192 scd_type: 1 (overwrite)\n\u2514\u2500\u25ba Yes \u2192 scd_type: 2 (versioned)\n    \u2514\u2500\u25ba Storage concerns? \u2192 Consider snapshots instead\n</code></pre>"},{"location":"playbook/#quick-links-by-role","title":"Quick Links by Role","text":""},{"location":"playbook/#data-engineer-daily-work","title":"Data Engineer (Daily Work)","text":"<ul> <li>CLI Master Guide \u2014 Run, debug, diagnose</li> <li>Cheatsheet \u2014 Quick syntax reference</li> <li>Troubleshooting \u2014 Common errors and fixes</li> </ul>"},{"location":"playbook/#data-engineer-building-pipelines","title":"Data Engineer (Building Pipelines)","text":"<ul> <li>Canonical Examples \u2014 Copy-paste configs</li> <li>Patterns Overview \u2014 Standard solutions</li> <li>Writing Transformations \u2014 Custom logic</li> </ul>"},{"location":"playbook/#data-engineer-production","title":"Data Engineer (Production)","text":"<ul> <li>Production Deployment \u2014 Going to prod</li> <li>Alerting \u2014 Slack/email notifications</li> <li>Performance Tuning \u2014 Optimize speed</li> </ul>"},{"location":"playbook/#cli-quick-reference","title":"CLI Quick Reference","text":"Task Command Run pipeline <code>odibi run config.yaml</code> Run specific node <code>odibi run config.yaml --node name</code> Dry run (no writes) <code>odibi run config.yaml --dry-run</code> Validate config <code>odibi validate config.yaml</code> View DAG <code>odibi graph config.yaml</code> Check state <code>odibi catalog state config.yaml</code> Diagnose issues <code>odibi doctor config.yaml</code> List stories <code>odibi story list</code>"},{"location":"playbook/#all-52-transformers","title":"All 52+ Transformers","text":"Category Transformers Filtering <code>filter_rows</code>, <code>distinct</code>, <code>sample</code>, <code>limit</code> Columns <code>derive_columns</code>, <code>select_columns</code>, <code>drop_columns</code>, <code>rename_columns</code>, <code>cast_columns</code> Text <code>clean_text</code>, <code>trim_whitespace</code>, <code>regex_replace</code>, <code>split_part</code>, <code>concat_columns</code> Dates <code>extract_date_parts</code>, <code>date_add</code>, <code>date_trunc</code>, <code>date_diff</code>, <code>convert_timezone</code> Nulls <code>fill_nulls</code>, <code>coalesce_columns</code> Relational <code>join</code>, <code>union</code>, <code>aggregate</code>, <code>pivot</code>, <code>unpivot</code> Window <code>window_calculation</code> (rank, sum, lag, lead) JSON <code>parse_json</code>, <code>normalize_json</code>, <code>explode_list_column</code>, <code>unpack_struct</code> Keys <code>generate_surrogate_key</code>, <code>hash_columns</code> Patterns <code>scd2</code>, <code>merge</code>, <code>deduplicate</code>, <code>dimension</code>, <code>fact</code>, <code>aggregation</code> <p>Full reference: YAML Schema - Transformers</p>"},{"location":"playbook/#see-also","title":"See Also","text":"<ul> <li>Golden Path \u2014 Start here if new</li> <li>Canonical Examples \u2014 Working configs</li> <li>YAML Schema \u2014 Complete reference</li> </ul>"},{"location":"reference/PARITY_TABLE/","title":"Engine Parity Table","text":"<p>Comparison of Pandas, Spark, and Polars engine capabilities as of V3.</p>"},{"location":"reference/PARITY_TABLE/#write-modes","title":"Write Modes","text":"Mode Pandas Spark Polars Notes <code>overwrite</code> \u2705 \u2705 \u2705 Default mode. <code>append</code> \u2705 \u2705 \u2705 <code>error</code> \u2705 \u2705 \u2705 Fails if table exists. <code>ignore</code> \u2705 \u2705 \u2705 Skips if table exists. <code>upsert</code> \u2705 \u2705 \u2705 Requires <code>keys</code>. Spark: Delta Lake only. <code>append_once</code> \u2705 \u2705 \u2705 Requires <code>keys</code>. Spark: Delta Lake only. <code>merge</code> \u2705 \u2705 \u2705 SQL Server only. T-SQL MERGE via staging table."},{"location":"reference/PARITY_TABLE/#sql-server-features-phase-4","title":"SQL Server Features (Phase 4)","text":"Feature Pandas Spark Polars Notes SQL Server MERGE \u2705 \u2705 \u2705 Upsert via staging table pattern. Composite Merge Keys \u2705 \u2705 \u2705 Multiple columns in ON clause. Merge Conditions \u2705 \u2705 \u2705 update_condition, delete_condition, insert_condition. Audit Columns \u2705 \u2705 \u2705 Auto-populate created/updated timestamps. Key Validations \u2705 \u2705 \u2705 Null/duplicate key checks. Enhanced Overwrite \u2705 \u2705 \u2705 truncate_insert, drop_create, delete_insert. Auto Schema Creation \u2705 \u2705 \u2705 CREATE SCHEMA IF NOT EXISTS. Auto Table Creation \u2705 \u2705 \u2705 Infer schema from DataFrame. Schema Evolution \u2705 \u2705 \u2705 strict, evolve, ignore modes. Batch Processing \u2705 \u2705 \u2705 Chunk large writes for memory efficiency. Incremental Merge \u2705 \u2705 \u2705 Compare hashes, only write changed rows to staging."},{"location":"reference/PARITY_TABLE/#core-features","title":"Core Features","text":"Feature Pandas Spark Polars Notes Reading (CSV, Parquet, JSON) \u2705 \u2705 \u2705 Reading (Delta Lake) \u2705 \u2705 \u2705 Pandas/Polars require <code>deltalake</code> package. Reading (SQL Server) \u2705 \u2705 \u2705 Writing (SQL Server) \u2705 \u2705 \u2705 Including merge and enhanced overwrite. SQL Transformations \u2705 \u2705 \u2705 Pandas uses DuckDB/SQLite. Polars uses native SQL context. PII Anonymization \u2705 \u2705 \u2705 Hash, Mask, Redact. Schema Validation \u2705 \u2705 \u2705 Data Contracts \u2705 \u2705 \u2705 Incremental Loading \u2705 \u2705 \u2705 Rolling Window &amp; Stateful. Time Travel \u2705 \u2705 \u2705 Delta Lake only."},{"location":"reference/PARITY_TABLE/#execution-context","title":"Execution Context","text":"Property Pandas Spark Polars Notes <code>df</code> \u2705 \u2705 \u2705 Native DataFrame. <code>columns</code> \u2705 \u2705 \u2705 <code>schema</code> \u2705 \u2705 \u2705 Dictionary of types. <code>pii_metadata</code> \u2705 \u2705 \u2705 Active PII columns."},{"location":"reference/cheatsheet/","title":"Odibi Cheatsheet","text":""},{"location":"reference/cheatsheet/#cli-commands","title":"CLI Commands","text":"Command Description <code>odibi run odibi.yaml</code> Run the pipeline. <code>odibi run odibi.yaml --dry-run</code> Validate connections without moving data. <code>odibi doctor odibi.yaml</code> Check environment and config health. <code>odibi stress odibi.yaml</code> Run fuzz tests (random data) to find bugs. <code>odibi story view --latest</code> Open the latest run report. <code>odibi secrets init odibi.yaml</code> Create .env template for secrets. <code>odibi graph odibi.yaml</code> Visualize pipeline dependencies. <code>odibi generate-project</code> Scaffold a new project from files."},{"location":"reference/cheatsheet/#odibiyaml-structure","title":"<code>odibi.yaml</code> Structure","text":"<pre><code>version: 1\nproject: My Project\nengine: pandas          # or 'spark'\n\nconnections:\n  raw_data:\n    type: local\n    base_path: ./data\n\nstory:\n  connection: raw_data\n  path: stories/\n\npipelines:\n  - pipeline: main_etl\n    nodes:\n      # 1. Read\n      - name: load_csv\n        read:\n          connection: raw_data\n          path: input.csv\n          format: csv\n\n      # 2. Transform (SQL)\n      - name: clean_data\n        depends_on: [load_csv]\n        transform:\n          steps:\n            - \"SELECT * FROM load_csv WHERE id IS NOT NULL\"\n\n      # 3. Transform (Python)\n      - name: advanced_clean\n        depends_on: [clean_data]\n        transform:\n          steps:\n            - operation: my_custom_func  # defined in python\n              params:\n                threshold: 10\n\n      # 4. Write\n      - name: save_parquet\n        depends_on: [advanced_clean]\n        write:\n          connection: raw_data\n          path: output.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"reference/cheatsheet/#python-transformation","title":"Python Transformation","text":"<pre><code>from odibi.transformations import transformation\n\n@transformation(\"my_custom_func\")\ndef my_func(df, threshold=10):\n    \"\"\"Docstrings are required!\"\"\"\n    return df[df['val'] &gt; threshold]\n</code></pre>"},{"location":"reference/cheatsheet/#cross-pipeline-dependencies","title":"Cross-Pipeline Dependencies","text":"<p>Reference outputs from other pipelines using <code>$pipeline.node</code> syntax:</p> <pre><code>nodes:\n  - name: enriched_data\n    inputs:\n      # Cross-pipeline reference\n      events: $read_bronze.shift_events\n\n      # Explicit read config\n      calendar:\n        connection: prod\n        path: \"bronze/calendar\"\n        format: delta\n    transform:\n      steps:\n        - operation: join\n          left: events\n          right: calendar\n          on: [date_id]\n</code></pre> Syntax Example Description <code>$pipeline.node</code> <code>$read_bronze.orders</code> Reference node output from another pipeline <p>Requirements: - Referenced pipeline must have run first - Referenced node must have a <code>write</code> block - Cannot use both <code>read</code> and <code>inputs</code> in same node</p>"},{"location":"reference/cheatsheet/#sql-templates","title":"SQL Templates","text":"Variable Value <code>${source}</code> The path of the source file (if reading). <code>${SELF}</code> The name of the current node."},{"location":"reference/cheatsheet/#transformer-quick-reference","title":"Transformer Quick Reference","text":""},{"location":"reference/cheatsheet/#sql-core-column-row-operations","title":"SQL Core (Column &amp; Row Operations)","text":"Transformer Description Key Params <code>filter_rows</code> Filter rows with SQL WHERE <code>condition</code> <code>derive_columns</code> Add calculated columns <code>derivations: {col: expr}</code> <code>cast_columns</code> Change column types <code>casts: {col: type}</code> <code>clean_text</code> Trim, lowercase, remove chars <code>columns, lowercase, strip</code> <code>select_columns</code> Keep only listed columns <code>columns: [...]</code> <code>drop_columns</code> Remove columns <code>columns: [...]</code> <code>rename_columns</code> Rename columns <code>mapping: {old: new}</code> <code>fill_nulls</code> Replace nulls <code>fills: {col: value}</code> <code>case_when</code> Conditional logic <code>conditions: [{when, then}]</code> <code>sort</code> Order rows <code>order_by: \"col DESC\"</code> <code>limit</code> Take first N rows <code>n: 100</code> <code>distinct</code> Remove duplicates <code>columns: [...]</code> (optional)"},{"location":"reference/cheatsheet/#datetime-operations","title":"Date/Time Operations","text":"Transformer Description Key Params <code>extract_date_parts</code> Extract year, month, day <code>column, parts: [year, month]</code> <code>date_add</code> Add interval to date <code>column, interval, unit</code> <code>date_trunc</code> Truncate to period <code>column, unit: \"month\"</code> <code>date_diff</code> Days between dates <code>start_col, end_col</code> <code>convert_timezone</code> Convert timezones <code>column, from_tz, to_tz</code>"},{"location":"reference/cheatsheet/#relational-joins-aggregations","title":"Relational (Joins &amp; Aggregations)","text":"Transformer Description Key Params <code>join</code> Join datasets <code>right_dataset, on, how</code> <code>union</code> Stack datasets <code>datasets: [...]</code> <code>aggregate</code> Group and aggregate <code>group_by, aggregations</code> <code>pivot</code> Rows to columns <code>index, columns, values</code> <code>unpivot</code> Columns to rows <code>id_vars, value_vars</code>"},{"location":"reference/cheatsheet/#advanced-complex-transformations","title":"Advanced (Complex Transformations)","text":"Transformer Description Key Params <code>deduplicate</code> Keep first/last per key <code>keys, order_by</code> <code>hash_columns</code> Generate hash <code>columns, output_column</code> <code>window_calculation</code> Window functions <code>partition_by, order_by, expr</code> <code>explode_list_column</code> Flatten arrays <code>column, outer: true</code> <code>dict_based_mapping</code> Value mapping <code>column, mapping: {}</code> <code>regex_replace</code> Regex substitution <code>column, pattern, replacement</code> <code>parse_json</code> Extract from JSON <code>column, schema</code> <code>generate_surrogate_key</code> UUID keys <code>columns, output_column</code> <code>sessionize</code> Session detection <code>timestamp_col, gap_minutes</code>"},{"location":"reference/cheatsheet/#patterns-scd-merge-delete-detection","title":"Patterns (SCD, Merge, Delete Detection)","text":"Transformer Description Key Params <code>scd2</code> SCD Type 2 history <code>target, keys, track_cols</code> <code>merge</code> Upsert/append/delete <code>target, keys, strategy</code> <code>detect_deletes</code> Find deleted records <code>target, keys</code>"},{"location":"reference/cheatsheet/#validation","title":"Validation","text":"Transformer Description Key Params <code>cross_check</code> Compare datasets <code>inputs: [a, b], type</code> <code>validate_and_flag</code> Flag bad records <code>rules: [{col, condition}]</code>"},{"location":"reference/configuration/","title":"ODIBI Configuration System Explained","text":"<p>Last Updated: 2025-11-21 Author: Henry Odibi Purpose: Demystify how YAML configs, Python classes, and execution flow together</p>"},{"location":"reference/configuration/#table-of-contents","title":"\ud83d\udcd1 Table of Contents","text":"<ol> <li>The Big Picture</li> <li>Configuration Flow (Concept \u2192 Execution)</li> <li>Three Layers of Configuration</li> <li>Example: Tracing a Pipeline from YAML to Execution</li> <li>Key Concepts Explained</li> <li>Common Confusion Points</li> <li>Decision Trees</li> <li>Quick Reference</li> </ol>"},{"location":"reference/configuration/#the-big-picture","title":"The Big Picture","text":"<p>ODIBI has three distinct layers that work together:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 1: YAML Configuration (What You Write)              \u2502\n\u2502  - Declarative syntax (project.yaml, pipelines/)            \u2502\n\u2502  - Human-readable, version-controlled                       \u2502\n\u2502  - Defines WHAT to do, not HOW                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Parsed by\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 2: Pydantic Models (config.py)                      \u2502\n\u2502  - Validates YAML structure                                 \u2502\n\u2502  - Enforces required fields, types, constraints             \u2502\n\u2502  - Converts YAML \u2192 Python objects                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 Used by\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LAYER 3: Runtime Classes (pipeline.py, node.py, etc.)     \u2502\n\u2502  - Executes the actual work                                 \u2502\n\u2502  - Manages context, engines, connections                    \u2502\n\u2502  - Generates stories, handles errors                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Insight: You interact with Layer 1 (YAML), but under the hood, Pydantic (Layer 2) ensures correctness before Runtime (Layer 3) does the work.</p>"},{"location":"reference/configuration/#configuration-flow-concept-execution","title":"Configuration Flow (Concept \u2192 Execution)","text":"<p>Let's trace a real pipeline from file to execution:</p>"},{"location":"reference/configuration/#step-1-you-write-yaml-user","title":"Step 1: You Write YAML (User)","text":"<pre><code># examples/templates/example_local.yaml\nproject: My Pipeline\nengine: pandas\nconnections:\n  local:\n    type: local\n    base_path: ./data\npipelines:\n  - pipeline: bronze_to_silver\n    nodes:\n      - name: load_raw_sales\n        read:\n          connection: local\n          path: bronze/sales.csv\n          format: csv\n</code></pre>"},{"location":"reference/configuration/#step-2-yaml-pydantic-models-automatic","title":"Step 2: YAML \u2192 Pydantic Models (Automatic)","text":"<p>When you call <code>Pipeline.from_yaml(\"examples/templates/example_local.yaml\")</code>:</p> <pre><code># pipeline.py (line 317)\nwith open(yaml_path, \"r\") as f:\n    config = yaml.safe_load(f)  # Loads YAML as Python dict\n\n# Parse into Pydantic models (config.py)\nproject_config = ProjectConfig(**config)  # Validates project/engine/connections\npipeline_config = PipelineConfig(**config['pipelines'][0])  # Validates pipeline structure\n</code></pre> <p>What Pydantic does: - Checks that <code>project</code>, <code>engine</code>, <code>connections</code> exist - Ensures <code>engine</code> is \"pandas\", \"spark\", or \"polars\" (not \"panda\" or \"pands\") - Validates each node has required fields (<code>name</code>, at least one operation) - Converts strings to enums where needed (e.g., <code>engine: \"pandas\"</code> \u2192 <code>EngineType.PANDAS</code>)</p> <p>If validation fails: <pre><code>ValidationError: 1 validation error for PipelineConfig\nnodes.0.read.connection\n  field required (type=value_error.missing)\n</code></pre> This is Layer 2 catching errors before execution.</p>"},{"location":"reference/configuration/#step-3-create-connections-automatic","title":"Step 3: Create Connections (Automatic)","text":"<pre><code># pipeline.py (lines 342-351) - PipelineManager.from_yaml()\nconnections = {}\nfor conn_name, conn_config in config.get(\"connections\", {}).items():\n    if conn_config.get(\"type\") == \"local\":\n        connections[conn_name] = LocalConnection(\n            base_path=conn_config.get(\"base_path\", \"./data\")\n        )\n</code></pre> <p>Result: Python objects ready to use: <pre><code>connections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n</code></pre></p>"},{"location":"reference/configuration/#step-4-create-pipelinemanager-automatic","title":"Step 4: Create PipelineManager (Automatic)","text":"<pre><code># pipeline.py (lines 280-314)\nmanager = PipelineManager(\n    yaml_config=config,\n    engine=\"pandas\",\n    connections=connections,\n    story_config=config.get(\"story\", {})\n)\n</code></pre> <p>What PipelineManager does: - Creates a <code>Pipeline</code> object for each pipeline in the YAML - Stores them in <code>self._pipelines</code> dictionary keyed by pipeline name - Example: <code>manager._pipelines = {\"bronze_to_silver\": Pipeline(...), \"silver_to_gold\": Pipeline(...)}</code></p>"},{"location":"reference/configuration/#step-5-run-pipelines-user","title":"Step 5: Run Pipelines (User)","text":"<pre><code># You call:\nresults = manager.run()  # Runs ALL pipelines\n\n# Or:\nresult = manager.run('bronze_to_silver')  # Runs specific pipeline\n</code></pre> <p>What happens: 1. <code>PipelineManager.run()</code> looks up the pipeline by name 2. Calls <code>Pipeline.run()</code> (line 134) 3. <code>Pipeline.run()</code> orchestrates node execution:    - Builds dependency graph    - Topologically sorts nodes    - Executes each node in order    - Passes data via Context    - Generates story</p>"},{"location":"reference/configuration/#three-layers-of-configuration","title":"Three Layers of Configuration","text":""},{"location":"reference/configuration/#layer-1-yaml-files-declarative","title":"Layer 1: YAML Files (Declarative)","text":"<p>Purpose: Human-readable, version-controlled pipeline definitions</p> <p>Key Files: - Project-level: <code>project.yaml</code> or any YAML with <code>project</code> + <code>connections</code> + <code>pipelines</code> - Pipeline-level: Individual YAML files with specific pipelines</p> <p>What You Define: | Section | Purpose | Required | |---------|---------|----------| | <code>project</code> | Project name | \u2705 Yes | | <code>engine</code> | Execution engine (pandas/spark/polars) | \u2705 Yes | | <code>connections</code> | Where data lives | \u2705 Yes | | <code>pipelines</code> | List of pipelines | \u2705 Yes | | <code>story</code> | Story generation config | \u2705 Yes |</p> <p>Example: <pre><code>project: Sales Analytics\nengine: pandas\nconnections:\n  warehouse:\n    type: local\n    base_path: /data/warehouse\npipelines:\n  - pipeline: daily_sales\n    nodes: [...]\n</code></pre></p>"},{"location":"reference/configuration/#layer-2-pydantic-models-validation","title":"Layer 2: Pydantic Models (Validation)","text":"<p>Purpose: Type-safe, validated Python objects</p> <p>Key File: <code>odibi/config.py</code></p> <p>Main Models:</p>"},{"location":"reference/configuration/#projectconfig-line-266","title":"ProjectConfig (Line 266)","text":"<pre><code>class ProjectConfig(BaseModel):\n    project: str  # Required\n    version: str = \"1.0.0\"  # Default\n    engine: EngineType = EngineType.PANDAS  # Default\n    connections: Dict[str, Dict[str, Any]]  # Required\n    story: StoryConfig  # Required\n    pipelines: List[PipelineConfig]  # Required\n    retry: RetryConfig = RetryConfig()  # Default\n    logging: LoggingConfig = LoggingConfig()  # Default\n</code></pre> <p>Maps to YAML: <pre><code>project: My Pipeline      # \u2192 project\nversion: \"2.0.0\"          # \u2192 version\nengine: pandas            # \u2192 engine (validated as EngineType.PANDAS)\nconnections:\n  data:                   # \u2192 connections[\"data\"]\n    type: local\n    base_path: ./data\n  outputs:                # \u2192 connections[\"outputs\"]\n    type: local\n    base_path: ./outputs\n  api_source:             # \u2192 connections[\"api_source\"]\n    type: http\n    base_url: \"https://api.example.com/v1\"\n    headers:\n      Authorization: \"Bearer ${API_TOKEN}\"\nstory:\n  connection: outputs     # \u2192 story.connection (required)\n  path: stories/          # \u2192 story.path\n  auto_generate: true     # \u2192 story.auto_generate\n  max_sample_rows: 10     # \u2192 story.max_sample_rows\n  retention_days: 30      # \u2192 story.retention_days\n  retention_count: 100    # \u2192 story.retention_count\nretry:\n  enabled: true           # \u2192 retry.enabled\n  max_attempts: 3         # \u2192 retry.max_attempts\n  backoff: exponential    # \u2192 retry.backoff\nlogging:\n  level: INFO             # \u2192 logging.level\npipelines:                # \u2192 pipelines (list of pipeline configs)\n  - pipeline: example\n    nodes: [...]\n</code></pre></p>"},{"location":"reference/configuration/#pipelineconfig-line-203","title":"PipelineConfig (Line 203)","text":"<pre><code>class PipelineConfig(BaseModel):\n    pipeline: str  # Required (pipeline name)\n    description: Optional[str] = None\n    layer: Optional[str] = None\n    nodes: List[NodeConfig]  # Required (at least one node)\n</code></pre> <p>Maps to YAML: <pre><code>pipelines:\n  - pipeline: bronze_to_silver  # \u2192 pipeline\n    layer: transformation       # \u2192 layer\n    description: \"Clean data\"   # \u2192 description\n    nodes: [...]               # \u2192 nodes\n</code></pre></p>"},{"location":"reference/configuration/#nodeconfig-line-172","title":"NodeConfig (Line 172)","text":"<pre><code>class NodeConfig(BaseModel):\n    name: str  # Required (unique within pipeline)\n    depends_on: List[str] = []\n    read: Optional[ReadConfig] = None\n    inputs: Optional[Dict[str, Union[str, Dict[str, Any]]]] = None  # Cross-pipeline dependencies\n    transform: Optional[TransformConfig] = None\n    write: Optional[WriteConfig] = None\n    cache: bool = False\n    sensitive: Union[bool, List[str]] = False  # PII Masking\n</code></pre> <p>Maps to YAML: <pre><code>nodes:\n  - name: load_raw_sales         # \u2192 name\n    depends_on: [prev_node]      # \u2192 depends_on\n    sensitive: [\"email\"]         # \u2192 sensitive (Redact email in reports)\n    read:                        # \u2192 read (ReadConfig)\n      connection: local\n      path: bronze/sales.csv\n      format: csv\n    cache: true                  # \u2192 cache\n</code></pre></p> <p>Cross-Pipeline Dependencies (<code>inputs</code> block):</p> <p>For multi-input nodes that read from other pipelines, use the <code>inputs</code> block instead of <code>read</code>:</p> <pre><code>nodes:\n  - name: enriched_data\n    inputs:\n      events: $read_bronze.shift_events      # Cross-pipeline reference\n      calendar:                               # Explicit read config\n        connection: goat_prod\n        path: \"bronze/calendar\"\n        format: delta\n    transform:\n      steps:\n        - operation: join\n          left: events\n          right: calendar\n          on: [date_id]\n</code></pre> <p>Reference Syntax: <code>$pipeline_name.node_name</code> - The <code>$</code> prefix indicates a cross-pipeline reference - References are resolved via the Odibi Catalog (<code>meta_outputs</code> table) - The referenced node must have a <code>write</code> block and the pipeline must have run previously</p> <p>Validation Rules: - Node must have at least one of: <code>read</code>, <code>inputs</code>, <code>transform</code>, <code>write</code> - All node names must be unique within a pipeline - Connections referenced in <code>read.connection</code> or <code>write.connection</code> should exist (warned, not enforced) - Cannot have both <code>read</code> and <code>inputs</code> \u2014 use <code>read</code> for single-source nodes or <code>inputs</code> for multi-source cross-pipeline dependencies</p>"},{"location":"reference/configuration/#layer-3-runtime-classes-execution","title":"Layer 3: Runtime Classes (Execution)","text":"<p>Purpose: Execute the actual work</p> <p>Key Files: - <code>odibi/pipeline.py</code> - Pipeline orchestration - <code>odibi/node.py</code> - Individual node execution - <code>odibi/context.py</code> - Data passing between nodes - <code>odibi/engine/pandas_engine.py</code> - Actual read/write/transform logic</p> <p>Main Classes:</p>"},{"location":"reference/configuration/#pipelinemanager-line-280","title":"PipelineManager (Line 280)","text":"<pre><code>class PipelineManager:\n    def __init__(self, yaml_config, engine, connections, story_config):\n        self._pipelines = {}  # Dict[pipeline_name -&gt; Pipeline]\n        for pipeline_config_dict in yaml_config[\"pipelines\"]:\n            pipeline_config = PipelineConfig(**pipeline_config_dict)\n            self._pipelines[pipeline_config.pipeline] = Pipeline(...)\n\n    def run(self, pipelines=None):\n        # Run all or specific pipelines\n</code></pre> <p>Responsibilities: - Load and validate YAML - Instantiate connections - Create Pipeline objects for each pipeline - Orchestrate multi-pipeline execution - Return results</p>"},{"location":"reference/configuration/#pipeline-line-63","title":"Pipeline (Line 63)","text":"<pre><code>class Pipeline:\n    def __init__(self, pipeline_config, engine, connections, ...):\n        self.config = pipeline_config  # PipelineConfig from Layer 2\n        self.engine = PandasEngine()   # Or SparkEngine\n        self.context = create_context(engine)\n        self.graph = DependencyGraph(pipeline_config.nodes)\n\n    def run(self):\n        execution_order = self.graph.topological_sort()\n        for node_name in execution_order:\n            node = Node(...)\n            node_result = node.execute()\n</code></pre> <p>Responsibilities: - Build dependency graph - Determine execution order - Execute nodes sequentially (or parallel in future) - Manage context for data passing - Generate stories</p>"},{"location":"reference/configuration/#node-odibinodepy","title":"Node (odibi/node.py)","text":"<pre><code>class Node:\n    def execute(self):\n        # 1. Read data (if read config exists)\n        if self.config.read:\n            data = self.engine.read(...)\n            self.context.register(self.config.name, data)\n\n        # 2. Transform data (if transform config exists)\n        if self.config.transform:\n            data = self.engine.execute_transform(...)\n            self.context.register(self.config.name, data)\n\n        # 3. Write data (if write config exists)\n        if self.config.write:\n            self.engine.write(...)\n</code></pre> <p>Responsibilities: - Execute read \u2192 transform \u2192 write for a single node - Use engine for actual operations - Register results in context - Return NodeResult</p>"},{"location":"reference/configuration/#example-tracing-a-pipeline-from-yaml-to-execution","title":"Example: Tracing a Pipeline from YAML to Execution","text":"<p>Let's trace this simple YAML:</p> <pre><code>project: Simple Pipeline\nengine: pandas\nconnections:\n  local:\n    type: local\n    base_path: ./data\n  outputs:\n    type: local\n    base_path: ./outputs\nstory:\n  connection: outputs\n  path: stories/\n  enabled: true\npipelines:\n  - pipeline: example\n    nodes:\n      - name: load_data\n        read:\n          connection: local\n          path: input.csv\n          format: csv\n        cache: true\n\n      - name: clean_data\n        depends_on: [load_data]\n        transform:\n          steps:\n            - \"SELECT * FROM load_data WHERE amount &gt; 0\"\n\n      - name: save_data\n        depends_on: [clean_data]\n        write:\n          connection: local\n          path: output.parquet\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"reference/configuration/#execution-trace","title":"Execution Trace","text":"<p>1. User calls: <pre><code>from odibi.pipeline import Pipeline\nmanager = Pipeline.from_yaml(\"simple.yaml\")\n</code></pre></p> <p>2. <code>Pipeline.from_yaml()</code> delegates to <code>PipelineManager.from_yaml()</code> (line 109)</p> <p>3. <code>PipelineManager.from_yaml()</code> (line 317): <pre><code># Load YAML\nwith open(\"simple.yaml\") as f:\n    config = yaml.safe_load(f)\n# config = {\n#     \"project\": \"Simple Pipeline\",\n#     \"engine\": \"pandas\",\n#     \"connections\": {\"local\": {\"type\": \"local\", \"base_path\": \"./data\"}},\n#     \"pipelines\": [{\"pipeline\": \"example\", \"nodes\": [...]}]\n# }\n\n# Validate project config (entire YAML - single source of truth)\nproject_config = ProjectConfig(**config)\n# \u2705 Validation passed - checks:\n#    - Required fields: project, connections, story, pipelines\n#    - story.connection exists in connections\n#    - engine is valid (pandas, spark, or polars)\n\n# Create connections\nconnections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n\n# Create PipelineManager\nmanager = PipelineManager(\n    project_config=project_config,\n    engine=\"pandas\",\n    connections=connections\n)\n</code></pre></p> <p>4. <code>PipelineManager.__init__()</code> (line 283): <pre><code># Loop through pipelines in YAML\nfor pipeline_config_dict in config[\"pipelines\"]:\n    # Validate pipeline config\n    pipeline_config = PipelineConfig(\n        pipeline=\"example\",\n        nodes=[\n            NodeConfig(name=\"load_data\", read=ReadConfig(...), cache=True),\n            NodeConfig(name=\"clean_data\", depends_on=[\"load_data\"], transform=TransformConfig(...)),\n            NodeConfig(name=\"save_data\", depends_on=[\"clean_data\"], write=WriteConfig(...))\n        ]\n    )\n    # \u2705 Validation passed (all nodes have unique names, at least one operation each)\n\n    # Create Pipeline instance\n    self._pipelines[\"example\"] = Pipeline(\n        pipeline_config=pipeline_config,\n        engine=\"pandas\",\n        connections={\"local\": LocalConnection(...)},\n        story_config={}\n    )\n</code></pre></p> <p>5. User runs: <pre><code>results = manager.run()  # Run all pipelines\n</code></pre></p> <p>6. <code>PipelineManager.run()</code> (line 363): <pre><code># pipelines=None means run all\npipeline_names = list(self._pipelines.keys())  # [\"example\"]\n\n# Run each pipeline\nfor name in pipeline_names:\n    results[name] = self._pipelines[name].run()\n</code></pre></p> <p>7. <code>Pipeline.run()</code> (line 134): <pre><code># Get execution order from dependency graph\nexecution_order = self.graph.topological_sort()\n# Returns: [\"load_data\", \"clean_data\", \"save_data\"]\n\n# Execute nodes in order\nfor node_name in execution_order:  # \"load_data\"\n    node_config = self.graph.nodes[\"load_data\"]\n    node = Node(\n        config=node_config,\n        context=self.context,\n        engine=self.engine,  # PandasEngine\n        connections={\"local\": LocalConnection(...)}\n    )\n    node_result = node.execute()\n</code></pre></p> <p>8. <code>Node.execute()</code> for \"load_data\" (odibi/node.py): <pre><code># Node has read config\nif self.config.read:\n    # Get connection\n    conn = self.connections[\"local\"]  # LocalConnection(base_path=\"./data\")\n    full_path = conn.get_path(\"input.csv\")  # \"./data/input.csv\"\n\n    # Read using engine\n    data = self.engine.read(\n        path=full_path,\n        format=\"csv\",\n        options={}\n    )\n    # data = pandas.DataFrame(...)\n\n    # Register in context\n    self.context.register(\"load_data\", data)\n</code></pre></p> <p>9. <code>Node.execute()</code> for \"clean_data\": <pre><code># Node has transform config\nif self.config.transform:\n    sql = \"SELECT * FROM load_data WHERE amount &gt; 0\"\n    data = self.engine.execute_sql(sql, self.context)\n    # Engine gets \"load_data\" DataFrame from context\n    # Executes SQL using pandasql or duckdb\n    # Returns filtered DataFrame\n\n    self.context.register(\"clean_data\", data)\n</code></pre></p> <p>10. <code>Node.execute()</code> for \"save_data\": <pre><code># Node has write config\nif self.config.write:\n    data = self.context.get(\"clean_data\")  # Get from previous node\n    conn = self.connections[\"local\"]\n    full_path = conn.get_path(\"output.parquet\")  # \"./data/output.parquet\"\n\n    self.engine.write(\n        data=data,\n        path=full_path,\n        format=\"parquet\",\n        mode=\"overwrite\",\n        options={}\n    )\n    # Writes DataFrame to ./data/output.parquet\n</code></pre></p> <p>11. Story generation (if enabled): <pre><code>story_path = self.story_generator.generate(\n    node_results={...},\n    completed=[\"load_data\", \"clean_data\", \"save_data\"],\n    failed=[],\n    ...\n)\n# Generates markdown story in ./stories/\n</code></pre></p> <p>12. Return results to user: <pre><code>results = {\n    \"example\": PipelineResults(\n        pipeline_name=\"example\",\n        completed=[\"load_data\", \"clean_data\", \"save_data\"],\n        failed=[],\n        duration=2.3,\n        story_path=\"./stories/example_20251107_143025.md\"\n    )\n}\n</code></pre></p>"},{"location":"reference/configuration/#key-concepts-explained","title":"Key Concepts Explained","text":""},{"location":"reference/configuration/#1-config-vs-runtime","title":"1. Config vs Runtime","text":"<p>Config (Layer 1 + 2): - What you declare in YAML - Validated by Pydantic - Immutable once loaded - Example: <code>ReadConfig(connection=\"local\", path=\"input.csv\", format=\"csv\")</code></p> <p>Runtime (Layer 3): - What executes the work - Uses config to make decisions - Mutable state (context, results) - Example: <code>PandasEngine.read(path=\"./data/input.csv\", format=\"csv\")</code> \u2192 returns DataFrame</p> <p>Why separate? - Validation happens early (before execution) - Config is reusable (can run same config multiple times) - Easier testing (mock runtime, test config separately)</p>"},{"location":"reference/configuration/#2-connection-config-vs-object","title":"2. Connection: Config vs Object","text":"<p>Connection Config (YAML): <pre><code>connections:\n  local:\n    type: local\n    base_path: ./data\n</code></pre></p> <p>Connection Object (Python): <pre><code>connections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\n</code></pre></p> <p>What's the difference? - Config is declarative (YAML dict) - Object is executable (Python class with methods like <code>.get_path()</code>)</p> <p>Why both? - YAML is portable (version controlled, shareable) - Objects are functional (can call methods, maintain state)</p>"},{"location":"reference/configuration/#3-pipeline-vs-pipelinemanager","title":"3. Pipeline vs PipelineManager","text":"<p>Pipeline: - Represents one pipeline - Has nodes, dependencies, execution logic - Example: <code>bronze_to_silver</code> pipeline</p> <p>PipelineManager: - Manages multiple pipelines - Loads YAML, creates connections, instantiates Pipelines - Provides unified API: <code>manager.run()</code> runs all, <code>manager.run('bronze_to_silver')</code> runs one</p> <p>Why <code>Pipeline.from_yaml()</code> returns PipelineManager? - Convenience: Most YAMLs have multiple pipelines - Backward compatible: Users can still call <code>Pipeline.from_yaml()</code> - Unified API: <code>manager.run()</code> works for 1 or 10 pipelines</p>"},{"location":"reference/configuration/#4-from_yaml-the-boilerplate-eliminator","title":"4. <code>from_yaml()</code> - The Boilerplate Eliminator","text":"<p>Before (manual setup): <pre><code>import yaml\nfrom odibi.pipeline import Pipeline\nfrom odibi.config import PipelineConfig\nfrom odibi.connections import LocalConnection\n\nwith open(\"config.yaml\") as f:\n    config = yaml.safe_load(f)\n\npipeline_config = PipelineConfig(**config['pipelines'][0])\nconnections = {\n    'local': LocalConnection(base_path=config['connections']['local']['base_path'])\n}\npipeline = Pipeline(\n    pipeline_config=pipeline_config,\n    engine=\"pandas\",\n    connections=connections\n)\nresults = pipeline.run()\n</code></pre></p> <p>After (<code>from_yaml()</code>): <pre><code>from odibi.pipeline import Pipeline\n\nmanager = Pipeline.from_yaml(\"config.yaml\")\nresults = manager.run()\n</code></pre></p> <p>What <code>from_yaml()</code> does: 1. Load YAML 2. Validate with Pydantic 3. Create connection objects 4. Instantiate PipelineManager 5. Return ready-to-run manager</p> <p>Result: 2 lines instead of 15!</p>"},{"location":"reference/configuration/#5-context-the-data-bus","title":"5. Context - The Data Bus","text":"<p>Purpose: Pass data between nodes without explicit function calls</p> <p>How it works: <pre><code># Node 1: load_data\ndata = engine.read(...)\ncontext.register(\"load_data\", data)  # Store DataFrame\n\n# Node 2: clean_data (depends_on: [load_data])\ndata = context.get(\"load_data\")  # Retrieve DataFrame\ncleaned = engine.execute_sql(\"SELECT * FROM load_data WHERE ...\", context)\ncontext.register(\"clean_data\", cleaned)\n\n# Node 3: save_data (depends_on: [clean_data])\ndata = context.get(\"clean_data\")\nengine.write(data, ...)\n</code></pre></p> <p>Why not return values? - Nodes execute sequentially but independently - SQL transforms reference DataFrames by name (not variable) - Context provides unified API across Pandas and Spark</p>"},{"location":"reference/configuration/#common-confusion-points","title":"Common Confusion Points","text":""},{"location":"reference/configuration/#confusion-1-why-do-i-see-both-pipeline-and-name","title":"Confusion #1: \"Why do I see both <code>pipeline</code> and <code>name</code>?\"","text":"<p>Answer: Different levels of abstraction!</p> <pre><code>pipelines:                   # List of pipelines\n  - pipeline: bronze_to_silver  # \u2190 Pipeline NAME (identifies the pipeline)\n    nodes:                     # List of nodes in THIS pipeline\n      - name: load_data        # \u2190 Node NAME (identifies the node)\n</code></pre> <p>Analogy: - <code>pipeline</code> is like a book title (\"Harry Potter\") - <code>name</code> is like a chapter name (\"The Boy Who Lived\")</p> <p>In code: - <code>PipelineConfig.pipeline</code> \u2192 pipeline name - <code>NodeConfig.name</code> \u2192 node name</p>"},{"location":"reference/configuration/#confusion-2-whats-the-difference-between-connection-local-and-type-local","title":"Confusion #2: \"What's the difference between <code>connection: local</code> and <code>type: local</code>?\"","text":"<p>Answer: Different contexts!</p> <p>In <code>connections</code> section (defining connections): <pre><code>connections:\n  local:           # \u2190 Connection NAME (you choose this)\n    type: local    # \u2190 Connection TYPE (system type: local, azure_adls, etc.)\n    base_path: ./data\n</code></pre></p> <p>In <code>read</code>/<code>write</code> section (using connections): <pre><code>nodes:\n  - name: load_data\n    read:\n      connection: local  # \u2190 References the CONNECTION NAME from above\n      path: input.csv\n</code></pre></p> <p>Analogy: - <code>connections</code> section: \"Define a car named 'my_car' of type 'sedan'\" - <code>read.connection</code>: \"Use the car named 'my_car' to drive somewhere\"</p>"},{"location":"reference/configuration/#confusion-3-why-does-from_yaml-return-a-manager-instead-of-a-pipeline","title":"Confusion #3: \"Why does <code>from_yaml()</code> return a manager instead of a pipeline?\"","text":"<p>Answer: YAML files typically have multiple pipelines!</p> <pre><code>pipelines:\n  - pipeline: bronze_to_silver  # Pipeline 1\n    nodes: [...]\n  - pipeline: silver_to_gold    # Pipeline 2\n    nodes: [...]\n</code></pre> <p>If it returned a single Pipeline: - Which one? The first? All? - How to run specific pipelines?</p> <p>By returning PipelineManager: - Access all pipelines - Run all: <code>manager.run()</code> - Run one: <code>manager.run('bronze_to_silver')</code> - Run some: <code>manager.run(['bronze_to_silver', 'silver_to_gold'])</code></p> <p>For single pipeline YAMLs: <pre><code>result = manager.run()  # If only 1 pipeline, returns PipelineResults (not dict)\n</code></pre></p>"},{"location":"reference/configuration/#confusion-4-whats-the-difference-between-options-and-params","title":"Confusion #4: \"What's the difference between <code>options</code> and <code>params</code>?\"","text":"<p>Answer: Different operation types!</p> <p><code>options</code> (in read/write): <pre><code>read:\n  connection: local\n  path: data.csv\n  format: csv\n  options:           # \u2190 Format-specific options (passed to pandas.read_csv())\n    header: 0\n    dtype:\n      id: str\n</code></pre> Maps to: <code>pandas.read_csv(path, header=0, dtype={\"id\": str})</code></p> <p><code>params</code> (in transform): <pre><code>transform:\n  steps:\n    - function: my_custom_function\n      params:        # \u2190 Function arguments\n        threshold: 0.5\n        mode: strict\n</code></pre> Maps to: <code>my_custom_function(context, threshold=0.5, mode='strict')</code></p> <p>Key difference: - <code>options</code> \u2192 passed to engine (Pandas/Spark I/O functions) - <code>params</code> \u2192 passed to your function</p>"},{"location":"reference/configuration/#confusion-5-where-do-stories-get-written","title":"Confusion #5: \"Where do stories get written?\"","text":"<p>Answer: Stories use the connection pattern, just like data!</p> <p>Story configuration (required): <pre><code>connections:\n  outputs:\n    type: local\n    base_path: ./outputs\n\nstory:\n  connection: outputs  # \u2190 References connection name\n  path: stories/       # \u2190 Path within connection\n  enabled: true\n</code></pre></p> <p>Resolved path: <code>./outputs/stories/pipeline_name_20251107_143025.md</code></p> <p>Why this pattern? - Explicit: Clear where stories are written (no hidden defaults) - Traceable: Connection-based paths preserve truth - Consistent: Same pattern as <code>read.connection</code> and <code>write.connection</code> - Flexible: Stories can go to ADLS, DBFS, or local storage</p> <p>Before v1.1 (confusing): <pre><code># Story path was implicit - where is \"stories/\" relative to?\nconnections:\n  local:\n    type: local\n    base_path: ./data\n</code></pre></p> <p>After v1.1 (explicit): <pre><code>connections:\n  outputs:\n    type: local\n    base_path: ./outputs\n\nstory:\n  connection: outputs  # Required - must exist in connections\n  path: stories/\n</code></pre></p>"},{"location":"reference/configuration/#confusion-6-how-does-sql-find-the-dataframes","title":"Confusion #6: \"How does SQL find the DataFrames?\"","text":"<p>Answer: The engine looks them up in the context!</p> <p>Your SQL: <pre><code>SELECT * FROM load_data WHERE amount &gt; 0\n</code></pre></p> <p>What the engine does (simplified): <pre><code># PandasEngine.execute_sql()\ndef execute_sql(self, sql: str, context: Context):\n    # 1. Find all table references in SQL\n    tables = extract_table_names(sql)  # [\"load_data\"]\n\n    # 2. Get DataFrames from context\n    load_data = context.get(\"load_data\")  # The DataFrame from earlier node\n\n    # 3. Execute SQL using pandasql or duckdb\n    result = duckdb.query(sql).to_df()\n\n    return result\n</code></pre></p> <p>Key insight: Table names in SQL must match node names in the pipeline!</p>"},{"location":"reference/configuration/#decision-trees","title":"Decision Trees","text":""},{"location":"reference/configuration/#which-class-do-i-use","title":"\"Which class do I use?\"","text":"<pre><code>\u250c\u2500 Need to load and run a YAML file?\n\u2502  \u251c\u2500 YES \u2192 Use `Pipeline.from_yaml(\"config.yaml\")`\n\u2502  \u2502         Returns PipelineManager\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Are you building custom integrations?\n\u2502     \u251c\u2500 YES \u2192 Use `PipelineManager(...)` or `Pipeline(...)` directly\n\u2502     \u2514\u2500 NO \u2192 Use `Pipeline.from_yaml()` (recommended)\n</code></pre>"},{"location":"reference/configuration/#how-do-i-run-my-pipelines","title":"\"How do I run my pipelines?\"","text":"<pre><code>\u250c\u2500 How many pipelines in YAML?\n\u2502  \u251c\u2500 ONE \u2192 `manager.run()` returns PipelineResults\n\u2502  \u251c\u2500 MANY \u2192 `manager.run()` returns Dict[name -&gt; PipelineResults]\n\u2502  \u2502\n\u2502  \u2514\u2500 Want to run specific pipeline(s)?\n\u2502     \u251c\u2500 ONE \u2192 `manager.run('pipeline_name')` returns PipelineResults\n\u2502     \u2514\u2500 MULTIPLE \u2192 `manager.run(['pipe1', 'pipe2'])` returns Dict\n</code></pre>"},{"location":"reference/configuration/#where-does-my-configuration-live","title":"\"Where does my configuration live?\"","text":"<pre><code>\u250c\u2500 Is it about the OVERALL project?\n\u2502  \u251c\u2500 YES \u2192 Top level (project, engine, connections, story)\n\u2502  \u2502\n\u2502  \u2514\u2500 NO \u2192 Is it about a PIPELINE?\n\u2502     \u251c\u2500 YES \u2192 Under `pipelines:` (pipeline, layer, nodes)\n\u2502     \u2502\n\u2502     \u2514\u2500 NO \u2192 Is it about a NODE?\n\u2502        \u251c\u2500 YES \u2192 Under `nodes:` (name, read, transform, write)\n\u2502        \u2502\n\u2502        \u2514\u2500 NO \u2192 Is it about an OPERATION?\n\u2502           \u251c\u2500 READ \u2192 Under `read:` (connection, path, format, options)\n\u2502           \u251c\u2500 TRANSFORM \u2192 Under `transform:` (steps)\n\u2502           \u2514\u2500 WRITE \u2192 Under `write:` (connection, path, format, mode, options)\n</code></pre>"},{"location":"reference/configuration/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/configuration/#yaml-structure","title":"YAML Structure","text":"<pre><code># PROJECT LEVEL (required)\nproject: string               # Project name\nengine: pandas|spark|polars   # Execution engine\n\n# GLOBAL SETTINGS (optional)\nretry:\n  enabled: bool\n  max_attempts: int\n  backoff: exponential|linear|constant\nlogging:\n  level: DEBUG|INFO|WARNING|ERROR\n  structured: bool\n  metadata: dict\n\n# CONNECTIONS (required, at least one)\nconnections:\n  &lt;connection_name&gt;:          # Your choice of name\n    type: local|azure_blob|delta|sql_server|http\n    validation_mode: lazy|eager   # optional, defaults to 'lazy'\n    &lt;type-specific-config&gt;\n\n# ENVIRONMENTS (optional)\nenvironments:\n  &lt;env_name&gt;:\n    &lt;overrides&gt;: ...\n    # Or use external file: env.&lt;env_name&gt;.yaml\n\n# STORY (required)\nstory:\n  connection: string        # Name of connection to write stories\n  path: string              # Relative path under that connection\n  auto_generate: bool\n  max_sample_rows: int\n  retention_days: int (optional)\n  retention_count: int (optional)\n\n# PIPELINES (required, at least one)\npipelines:\n  - pipeline: string          # Pipeline name\n    layer: string (optional)\n    description: string (optional)\n    nodes:                    # At least one node\n      - name: string          # Unique node name\n        depends_on: [string]  # List of node names (optional)\n        cache: bool (optional)\n\n        # At least ONE of these:\n        read:\n          connection: string  # Connection name\n          path: string        # Relative to connection base_path (Required unless 'query' used)\n          table: string       # Table name (alternative to path)\n          format: csv|parquet|json|excel|avro|sql_server\n          options: dict       # Format-specific (optional)\n            query: string     # SQL query (substitutes for path/table in sql_server)\n\n        transform:\n          steps:              # List of SQL strings or function calls\n            - string (SQL)\n            - function: string\n              params: dict\n\n        write:\n          connection: string\n          path: string\n          table: string       # Table name (alternative to path)\n          register_table: string # Register file output as external table (Spark/Delta only)\n          format: csv|parquet|json|excel|avro|delta\n          mode: overwrite|append\n          options: dict (optional)\n</code></pre>"},{"location":"reference/configuration/#python-api-quick-reference","title":"Python API Quick Reference","text":"<pre><code># === RECOMMENDED: Simple Usage ===\nfrom odibi.pipeline import Pipeline\n\n# Load and run all pipelines\nmanager = Pipeline.from_yaml(\"examples/templates/template_full.yaml\")\nresults = manager.run()  # Dict[name -&gt; PipelineResults] or single PipelineResults\n\n# Run specific pipeline\nresult = manager.run('bronze_to_silver')\n\n# List available pipelines\nprint(manager.list_pipelines())  # ['bronze_to_silver', 'silver_to_gold']\n\n# === ADVANCED: Direct PipelineManager ===\nfrom odibi.pipeline import PipelineManager\n\nmanager = PipelineManager.from_yaml(\"config.yaml\")\nresults = manager.run()\n\n# Access specific pipeline\npipeline = manager.get_pipeline('bronze_to_silver')\nresult = pipeline.run()\n\n# === ADVANCED: Manual Construction ===\nfrom odibi.pipeline import Pipeline\nfrom odibi.config import PipelineConfig\nfrom odibi.connections import LocalConnection\n\npipeline_config = PipelineConfig(\n    pipeline=\"my_pipeline\",\n    nodes=[...]\n)\nconnections = {\n    \"local\": LocalConnection(base_path=\"./data\")\n}\npipeline = Pipeline(\n    pipeline_config=pipeline_config,\n    engine=\"pandas\",\n    connections=connections\n)\nresult = pipeline.run()\n</code></pre>"},{"location":"reference/configuration/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: Single pipeline in YAML <pre><code>manager = Pipeline.from_yaml(\"simple.yaml\")\nresult = manager.run()  # Returns PipelineResults (not dict)\nprint(f\"Completed: {result.completed}\")\n</code></pre></p> <p>Pattern 2: Multiple pipelines, run all <pre><code>manager = Pipeline.from_yaml(\"multi.yaml\")\nresults = manager.run()  # Returns Dict[name -&gt; PipelineResults]\nfor name, result in results.items():\n    print(f\"{name}: {len(result.completed)} nodes\")\n</code></pre></p> <p>Pattern 3: Multiple pipelines, run specific <pre><code>manager = Pipeline.from_yaml(\"multi.yaml\")\nresult = manager.run('bronze_to_silver')  # Returns PipelineResults\nprint(result.to_dict())\n</code></pre></p>"},{"location":"reference/configuration/#summary","title":"Summary","text":"<p>The Three Layers: 1. YAML (Layer 1): What you write (declarative) 2. Pydantic Models (Layer 2): Validation (automatic) 3. Runtime Classes (Layer 3): Execution (automatic)</p> <p>The Flow: <pre><code>YAML file\n  \u2192 yaml.safe_load()\n  \u2192 Pydantic validation\n  \u2192 PipelineManager/Pipeline creation\n  \u2192 manager.run()\n  \u2192 Node execution\n  \u2192 Results\n</code></pre></p> <p>Key Takeaways: - <code>Pipeline.from_yaml()</code> returns <code>PipelineManager</code> (not <code>Pipeline</code>) - <code>manager.run()</code> runs all pipelines (or specific ones by name) - Configs are validated before execution (fail fast) - Context passes data between nodes (SQL references node names) - Connections are defined once, referenced many times</p> <p>Questions? Confusion? Open an issue on GitHub or check the examples in the <code>examples/</code> directory for complete YAML references.</p> <p>This document evolves with the framework. Last updated: 2025-11-20</p>"},{"location":"reference/glossary/","title":"Quick Glossary","text":"<p>Standard terminology used across Odibi documentation.</p> <p>Looking for in-depth explanations? See the Learning Glossary for beginner-friendly definitions with examples and real-world analogies.</p>"},{"location":"reference/glossary/#data-quality-terms","title":"Data Quality Terms","text":"Term Definition YAML Key Contracts Pre-transform checks that always fail on violation. Use for input data validation. <code>contracts:</code> Validation Tests Post-transform row-level checks with configurable actions (fail/warn/quarantine). <code>validation.tests:</code> Quality Gates Batch-level thresholds (pass rate, row counts) evaluated after validation. <code>validation.gate:</code> Quarantine Routing invalid rows to a separate table for review instead of failing. <code>validation.quarantine:</code>"},{"location":"reference/glossary/#pipeline-terms","title":"Pipeline Terms","text":"Term Definition YAML Key Pipeline A collection of nodes that execute together as a logical unit. <code>pipelines:</code> Node A single unit of work: read \u2192 transform \u2192 validate \u2192 write. <code>nodes:</code> Transformer A pre-built \"app\" for major operations (scd2, merge, deduplicate). <code>transformer:</code> Transform Steps A chain of smaller operations (SQL, functions) for custom logic. <code>transform.steps:</code> Pattern A declarative dimensional modeling template (dimension, fact, aggregation). <code>pattern:</code>"},{"location":"reference/glossary/#dimensional-modeling-terms","title":"Dimensional Modeling Terms","text":"Term Definition Natural Key Business identifier from source system (e.g., <code>customer_id</code>). Surrogate Key System-generated integer key for joins (e.g., <code>customer_sk</code>). SCD Type 1 Overwrite dimension changes (no history). SCD Type 2 Track dimension changes with versioned rows (<code>is_current</code>, <code>valid_from</code>, <code>valid_to</code>). Grain The level of detail in a fact table (e.g., one row per order). Orphan A fact row with no matching dimension record."},{"location":"reference/glossary/#execution-terms","title":"Execution Terms","text":"Term Definition Story Execution report with lineage, metrics, and validation results. Connection Named data source/destination (local, Azure, Delta, SQL Server). Context Runtime environment holding registered DataFrames and engine state."},{"location":"reference/glossary/#actions-on-failure","title":"Actions on Failure","text":"Term Usage Context Behavior <code>fail</code> Contracts, Validation Stop execution immediately <code>warn</code> Validation Log warning, continue processing <code>quarantine</code> Validation Route bad rows to quarantine table <code>abort</code> Quality Gates Stop pipeline, write nothing <code>warn_and_write</code> Quality Gates Log warning, write all rows <code>write_valid_only</code> Quality Gates Write only rows that passed"},{"location":"reference/glossary/#see-also","title":"See Also","text":"<ul> <li>YAML Configuration Reference - Complete configuration options</li> <li>Validation Overview - Data quality framework</li> </ul>"},{"location":"reference/supported_formats/","title":"Supported File Formats","text":"<p>ODIBI's PandasEngine supports multiple file formats with both local and cloud storage (ADLS, S3, etc.).</p>"},{"location":"reference/supported_formats/#format-support-matrix","title":"Format Support Matrix","text":"Format Read Write ADLS S3 Local Dependencies CSV \u2705 \u2705 \u2705 \u2705 \u2705 pandas (built-in) Parquet \u2705 \u2705 \u2705 \u2705 \u2705 pyarrow or fastparquet JSON \u2705 \u2705 \u2705 \u2705 \u2705 pandas (built-in) Excel \u2705 \u2705 \u2705 \u2705 \u2705 openpyxl Avro \u2705 \u2705 \u2705 \u2705 \u2705 fastavro"},{"location":"reference/supported_formats/#format-details","title":"Format Details","text":""},{"location":"reference/supported_formats/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>Use Case: Simple tabular data, human-readable format</p> <p>Read Example: <pre><code>- name: load_csv\n  read:\n    connection: bronze\n    path: data/sales.csv\n    format: csv\n    options:\n      sep: \",\"\n      header: true\n</code></pre></p> <p>Write Example: <pre><code>- name: save_csv\n  write:\n    connection: bronze\n    path: output/results.csv\n    format: csv\n    mode: overwrite\n</code></pre></p>"},{"location":"reference/supported_formats/#parquet-apache-parquet","title":"Parquet (Apache Parquet)","text":"<p>Use Case: Data lake storage, recommended for production</p> <p>Benefits: - Columnar format (efficient for analytics) - Built-in compression - Type preservation - Fast reads for column-based queries</p> <p>Read Example: <pre><code>- name: load_parquet\n  read:\n    connection: bronze\n    path: data/sales.parquet\n    format: parquet\n</code></pre></p> <p>Write Example: <pre><code>- name: save_parquet\n  write:\n    connection: silver\n    path: output/sales.parquet\n    format: parquet\n    options:\n      compression: snappy  # or gzip, brotli\n</code></pre></p>"},{"location":"reference/supported_formats/#json-json-lines","title":"JSON (JSON Lines)","text":"<p>Use Case: Semi-structured data, nested objects</p> <p>Format: JSON lines (newline-delimited JSON objects)</p> <p>Read Example: <pre><code>- name: load_json\n  read:\n    connection: bronze\n    path: data/events.json\n    format: json\n</code></pre></p> <p>Write Example: <pre><code>- name: save_json\n  write:\n    connection: bronze\n    path: output/events.json\n    format: json\n    options:\n      orient: records\n</code></pre></p>"},{"location":"reference/supported_formats/#excel-microsoft-excel","title":"Excel (Microsoft Excel)","text":"<p>Use Case: Business reports, spreadsheets, multi-file analysis</p> <p>Supported: <code>.xlsx</code> files</p> <p>Dependencies: Requires <code>openpyxl</code> <pre><code>pip install openpyxl\n</code></pre></p> <p>Read Example (Single File): <pre><code>- name: load_excel\n  read:\n    connection: bronze\n    path: reports/sales.xlsx\n    format: excel\n    options:\n      sheet_name: \"Sheet1\"\n</code></pre></p>"},{"location":"reference/supported_formats/#reading-multiple-excel-files-glob-patterns","title":"Reading Multiple Excel Files (Glob Patterns)","text":"<p>Read all Excel files matching a pattern and combine them into a single DataFrame:</p> <pre><code>- name: load_s_curve_data\n  read:\n    connection: bronze\n    path: \"S Curve/*.xlsx\"\n    format: excel\n    options:\n      add_source_file: true  # Track which file each row came from\n</code></pre> <p>Recursive Pattern (All Subdirectories): <pre><code>- name: load_all_reports\n  read:\n    connection: bronze\n    path: \"data/**/*.xlsx\"\n    format: excel\n    options:\n      add_source_file: true\n</code></pre></p>"},{"location":"reference/supported_formats/#sheet-pattern-matching","title":"Sheet Pattern Matching","text":"<p>Filter sheets by name pattern instead of reading all sheets:</p> <pre><code>- name: load_powerbi_sheets\n  read:\n    connection: bronze\n    path: \"reports/*.xlsx\"\n    format: excel\n    options:\n      sheet_pattern:\n        - \"*powerbi*\"\n        - \"*power bi*\"\n      sheet_pattern_case_sensitive: false  # Default: case-insensitive\n      add_source_file: true\n</code></pre>"},{"location":"reference/supported_formats/#source-file-tracking","title":"Source File Tracking","text":"<p>When <code>add_source_file: true</code>, two columns are added to track data lineage: - <code>_source_file</code>: The file path (e.g., <code>S Curve/Project_Alpha.xlsx</code>) - <code>_source_sheet</code>: The sheet name (e.g., <code>Sheet1</code>)</p> <p>This is essential for debugging and auditing when combining multiple files.</p>"},{"location":"reference/supported_formats/#cloud-storage-azures3","title":"Cloud Storage (Azure/S3)","text":"<p>Excel reading works seamlessly with cloud storage:</p> <pre><code>- name: load_excel_from_azure\n  read:\n    connection: bronze  # Uses connection's storage_options\n    path: \"abfss://container@account.dfs.core.windows.net/reports/*.xlsx\"\n    format: excel\n    options:\n      sheet_pattern: [\"*data*\"]\n      add_source_file: true\n</code></pre> <p>Write Example: <pre><code>- name: save_excel\n  write:\n    connection: bronze\n    path: output/report.xlsx\n    format: excel\n</code></pre></p> <p>Note: All three engines (Pandas, Spark, Polars) support these Excel features with full parity.</p>"},{"location":"reference/supported_formats/#avro-apache-avro","title":"Avro (Apache Avro)","text":"<p>Use Case: Event streaming, schema evolution</p> <p>Benefits: - Binary format (compact) - Schema included in file - Supports schema evolution - Efficient for serialization</p> <p>Dependencies: Requires <code>fastavro</code> <pre><code>pip install fastavro\n</code></pre></p> <p>Read Example: <pre><code>- name: load_avro\n  read:\n    connection: bronze\n    path: events/stream.avro\n    format: avro\n</code></pre></p> <p>Write Example: <pre><code>- name: save_avro\n  write:\n    connection: bronze\n    path: output/events.avro\n    format: avro\n</code></pre></p> <p>Note: Avro schema is automatically inferred from DataFrame dtypes.</p>"},{"location":"reference/supported_formats/#cloud-storage-support","title":"Cloud Storage Support","text":"<p>All formats work seamlessly with cloud storage:</p> <p>ADLS (Azure Data Lake Storage): <pre><code>connections:\n  bronze:\n    type: azure_adls\n    account: mystorageaccount\n    container: bronze\n    auth_mode: key_vault\n    key_vault_name: my-vault\n    secret_name: storage-key\n\npipelines:\n  - pipeline: multi_format\n    nodes:\n      - name: read_csv_from_adls\n        read:\n          connection: bronze\n          path: data/sales.csv\n          format: csv\n\n      - name: write_avro_to_adls\n        depends_on: [read_csv_from_adls]\n        write:\n          connection: bronze\n          path: output/sales.avro\n          format: avro\n</code></pre></p> <p>All formats support: - \u2705 Multi-account connections - \u2705 Key Vault authentication - \u2705 Storage options pass-through - \u2705 Remote URI handling (<code>abfss://</code>, <code>s3://</code>)</p>"},{"location":"reference/supported_formats/#best-practices","title":"Best Practices","text":""},{"location":"reference/supported_formats/#for-data-lakes-recommended","title":"For Data Lakes (Recommended)","text":"<ol> <li>Use Parquet for production data</li> <li>Efficient storage</li> <li>Fast analytics</li> <li> <p>Type preservation</p> </li> <li> <p>Use CSV for human-readable data</p> </li> <li>Easy to inspect</li> <li>Compatible with all tools</li> <li> <p>Good for small datasets</p> </li> <li> <p>Use Avro for event streams</p> </li> <li>Schema evolution support</li> <li>Compact binary format</li> <li>Good for append-only logs</li> </ol>"},{"location":"reference/supported_formats/#performance-tips","title":"Performance Tips","text":"<p>Parquet: - Use <code>snappy</code> compression (good balance of speed/size) - Enable column pruning (read only needed columns) - Consider partitioning for large datasets (Phase 2B)</p> <p>CSV: - Use chunking for large files - Specify dtypes explicitly to avoid inference</p> <p>Avro: - Best for write-once, read-many workloads - Schema is embedded (no separate schema files needed)</p>"},{"location":"reference/supported_formats/#delta-lake-databricks-open-source","title":"Delta Lake (Databricks / Open Source)","text":"<p>Use Case: ACID transactions, time travel, data lakehouse</p> <p>Benefits: - ACID Transactions: No partial writes or corruption - Time Travel: Query previous versions of data - Schema Evolution: Safely evolve schema over time - Audit History: Track all changes to the table</p> <p>Dependencies: Requires <code>delta-spark</code> (for Spark engine) or <code>deltalake</code> (for Pandas engine).</p> <p>Read Example: <pre><code>- name: load_delta\n  read:\n    connection: bronze\n    path: data/sales.delta\n    format: delta\n    options:\n      version_as_of: 5  # Time travel!\n</code></pre></p> <p>Write Example: <pre><code>- name: save_delta\n  write:\n    connection: silver\n    path: output/sales.delta\n    format: delta\n    mode: append  # or overwrite\n</code></pre></p> <p>Delta Lake supports advanced features like VACUUM and Restore.</p>"},{"location":"reference/yaml_schema/","title":"Odibi Configuration Reference","text":"<p>This manual details the YAML configuration schema for Odibi projects. Auto-generated from Pydantic models.</p>"},{"location":"reference/yaml_schema/#project-structure","title":"Project Structure","text":""},{"location":"reference/yaml_schema/#projectconfig","title":"<code>ProjectConfig</code>","text":"<p>Complete project configuration from YAML.</p>"},{"location":"reference/yaml_schema/#enterprise-setup-guide","title":"\ud83c\udfe2 \"Enterprise Setup\" Guide","text":"<p>Business Problem: \"We need a robust production environment with alerts, retries, and proper logging.\"</p> <p>Recipe: Production Ready <pre><code>project: \"Customer360\"\nengine: \"spark\"\n\n# 1. Resilience\nretry:\n    enabled: true\n    max_attempts: 3\n    backoff: \"exponential\"\n\n# 2. Observability\nlogging:\n    level: \"INFO\"\n    structured: true  # JSON logs for Splunk/Datadog\n\n# 3. Alerting\nalerts:\n    - type: \"slack\"\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events: [\"on_failure\"]\n\n# ... connections and pipelines ...\n</code></pre></p> Field Type Required Default Description project str Yes - Project name engine EngineType No <code>EngineType.PANDAS</code> Execution engine connections Dict[str, LocalConnectionConfig | AzureBlobConnectionConfig | DeltaConnectionConfig | SQLServerConnectionConfig | HttpConnectionConfig | CustomConnectionConfig] Yes - Named connections (at least one required)Options: LocalConnectionConfig, AzureBlobConnectionConfig, DeltaConnectionConfig, SQLServerConnectionConfig, HttpConnectionConfig pipelines List[PipelineConfig] Yes - Pipeline definitions (at least one required) story StoryConfig Yes - Story generation configuration (mandatory) system SystemConfig Yes - System Catalog configuration (mandatory) lineage Optional[LineageConfig] No - OpenLineage configuration description Optional[str] No - Project description version str No <code>1.0.0</code> Project version owner Optional[str] No - Project owner/contact vars Dict[str, Any] No <code>PydanticUndefined</code> Global variables for substitution (e.g. ${vars.env}) retry RetryConfig No <code>PydanticUndefined</code> - logging LoggingConfig No <code>PydanticUndefined</code> - alerts List[AlertConfig] No <code>PydanticUndefined</code> Alert configurations performance PerformanceConfig No <code>PydanticUndefined</code> Performance tuning environments Optional[Dict[str, Dict[str, Any]]] No - Structure: same as ProjectConfig but with only overridden fields. Not yet validated strictly. semantic Optional[Dict[str, Any]] No - Semantic layer configuration. Can be inline or reference external file. Contains metrics, dimensions, and materializations for self-service analytics. Example: semantic: { config: 'semantic_config.yaml' } or inline definitions."},{"location":"reference/yaml_schema/#pipelineconfig","title":"<code>PipelineConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for a pipeline.</p> <p>Example: <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    owner: \"data-team@example.com\"\n    freshness_sla: \"6h\"\n    nodes:\n      - name: \"node1\"\n        ...\n</code></pre></p> Field Type Required Default Description pipeline str Yes - Pipeline name description Optional[str] No - Pipeline description layer Optional[str] No - Logical layer (bronze/silver/gold) owner Optional[str] No - Pipeline owner (email or name) freshness_sla Optional[str] No - Expected freshness, e.g. '6h', '1d' freshness_anchor Literal['run_completion', 'table_max_timestamp', 'watermark_state'] No <code>run_completion</code> What defines freshness. Only 'run_completion' implemented initially. nodes List[NodeConfig] Yes - List of nodes in this pipeline"},{"location":"reference/yaml_schema/#nodeconfig","title":"<code>NodeConfig</code>","text":"<p>Used in: PipelineConfig</p> <p>Configuration for a single node.</p>"},{"location":"reference/yaml_schema/#the-smart-node-pattern","title":"\ud83e\udde0 \"The Smart Node\" Pattern","text":"<p>Business Problem: \"We need complex dependencies, caching for heavy computations, and the ability to run only specific parts of the pipeline.\"</p> <p>The Solution: Nodes are the building blocks. They handle dependencies (<code>depends_on</code>), execution control (<code>tags</code>, <code>enabled</code>), and performance (<code>cache</code>).</p>"},{"location":"reference/yaml_schema/#dag-dependencies","title":"\ud83d\udd78\ufe0f DAG &amp; Dependencies","text":"<p>The Glue of the Pipeline. Nodes don't run in isolation. They form a Directed Acyclic Graph (DAG).</p> <ul> <li><code>depends_on</code>: Critical! If Node B reads from Node A (in memory), you MUST list <code>[\"Node A\"]</code>.<ul> <li>Implicit Data Flow: If a node has no <code>read</code> block, it automatically picks up the DataFrame from its first dependency.</li> </ul> </li> </ul>"},{"location":"reference/yaml_schema/#smart-read-incremental-loading","title":"\ud83e\udde0 Smart Read &amp; Incremental Loading","text":"<p>Automated History Management.</p> <p>Odibi intelligently determines whether to perform a Full Load or an Incremental Load based on the state of the target.</p> <p>The \"Smart Read\" Logic: 1.  First Run (Full Load): If the target table (defined in <code>write</code>) does not exist:     *   Incremental filtering rules are ignored.     *   The entire source dataset is read.     *   Use <code>write.first_run_query</code> (optional) to override the read query for this initial bootstrap (e.g., to backfill only 1 year of history instead of all time).</p> <ol> <li>Subsequent Runs (Incremental Load): If the target table exists:<ul> <li>Rolling Window: Filters source data where <code>column &gt;= NOW() - lookback</code>.</li> <li>Stateful: Filters source data where <code>column &gt; last_high_water_mark</code>.</li> </ul> </li> </ol> <p>This ensures you don't need separate \"init\" and \"update\" pipelines. One config handles both lifecycle states.</p>"},{"location":"reference/yaml_schema/#orchestration-tags","title":"\ud83c\udff7\ufe0f Orchestration Tags","text":"<p>Run What You Need. Tags allow you to execute slices of your pipeline. *   <code>odibi run --tag daily</code> -&gt; Runs all nodes with \"daily\" tag. *   <code>odibi run --tag critical</code> -&gt; Runs high-priority nodes.</p>"},{"location":"reference/yaml_schema/#choosing-your-logic-transformer-vs-transform","title":"\ud83e\udd16 Choosing Your Logic: Transformer vs. Transform","text":"<p>1. The \"Transformer\" (Top-Level) *   What it is: A pre-packaged, heavy-duty operation that defines the entire purpose of the node. *   When to use: When applying a standard Data Engineering pattern (e.g., SCD2, Merge, Deduplicate). *   Analogy: \"Run this App.\" *   Syntax: <code>transformer: \"scd2\"</code> + <code>params: {...}</code></p> <p>2. The \"Transform Steps\" (Process Chain) *   What it is: A sequence of smaller steps (SQL, functions, operations) executed in order. *   When to use: For custom business logic, data cleaning, or feature engineering pipelines. *   Analogy: \"Run this Script.\" *   Syntax: <code>transform: { steps: [...] }</code></p> <p>Note: You can use both! The <code>transformer</code> runs first, then <code>transform</code> steps refine the result.</p>"},{"location":"reference/yaml_schema/#chaining-operations","title":"\ud83d\udd17 Chaining Operations","text":"<p>You can mix and match! The execution order is always: 1.  Read (or Dependency Injection) 2.  Transformer (The \"App\" logic, e.g., Deduplicate) 3.  Transform Steps (The \"Script\" logic, e.g., cleanup) 4.  Validation 5.  Write</p> <p>Constraint: You must define at least one of <code>read</code>, <code>transformer</code>, <code>transform</code>, or <code>write</code>.</p>"},{"location":"reference/yaml_schema/#example-app-vs-script","title":"\u26a1 Example: App vs. Script","text":"<p>Scenario 1: The Full ETL Flow (Chained) Shows explicit Read, Transform Chain, and Write.</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]\n\n  # \"clean_text\" is a registered function from the Transformer Catalog\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Top-Level Transformer) Shows a node that applies a pattern (Deduplicate) to incoming data.</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication (From Transformer Catalog)\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner (Reporting) Shows how tags allow running specific slices (e.g., <code>odibi run --tag daily</code>).</p> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  depends_on: [\"deduped_users\"]\n\n  # Ad-hoc aggregation script\n  transform:\n    steps:\n      - sql: \"SELECT date_trunc('day', updated_at) as day, count(*) as total FROM df GROUP BY 1\"\n\n  write: { connection: \"local_data\", format: \"csv\", path: \"reports/daily_stats.csv\" }\n</code></pre> <p>Scenario 4: The \"Kitchen Sink\" (All Operations) Shows Read -&gt; Transformer -&gt; Transform -&gt; Write execution order.</p> <p>Why this works: 1.  Internal Chaining (<code>df</code>): In every step (Transformer or SQL), <code>df</code> refers to the output of the previous step. 2.  External Access (<code>depends_on</code>): If you added <code>depends_on: [\"other_node\"]</code>, you could also run <code>SELECT * FROM other_node</code> in your SQL steps!</p> <pre><code>- name: \"complex_flow\"\n  # 1. Read -&gt; Creates initial 'df'\n  read: { connection: \"bronze\", format: \"parquet\", path: \"users\" }\n\n  # 2. Transformer (The \"App\": Deduplicate first)\n  # Takes 'df' (from Read), dedups it, returns new 'df'\n  transformer: \"deduplicate\"\n  params: { keys: [\"user_id\"], order_by: \"updated_at DESC\" }\n\n  # 3. Transform Steps (The \"Script\": Filter AFTER deduplication)\n  # SQL sees the deduped data as 'df'\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n  # 4. Write -&gt; Saves the final filtered 'df'\n  write: { connection: \"silver\", format: \"delta\", table: \"active_unique_users\" }\n</code></pre>"},{"location":"reference/yaml_schema/#transformer-catalog","title":"\ud83d\udcda Transformer Catalog","text":"<p>These are the built-in functions you can use in two ways:</p> <ol> <li>As a Top-Level Transformer: <code>transformer: \"name\"</code> (Defines the node's main logic)</li> <li>As a Step in a Chain: <code>transform: { steps: [{ function: \"name\" }] }</code> (Part of a sequence)</li> </ol> <p>Note: <code>merge</code> and <code>scd2</code> are special \"Heavy Lifters\" and should generally be used as Top-Level Transformers.</p> <p>Data Engineering Patterns *   <code>merge</code>: Upsert/Merge into target (Delta/SQL). (Params) *   <code>scd2</code>: Slowly Changing Dimensions Type 2. (Params) *   <code>deduplicate</code>: Remove duplicates using window functions. (Params)</p> <p>Relational Algebra *   <code>join</code>: Join two datasets. (Params) *   <code>union</code>: Stack datasets vertically. (Params) *   <code>pivot</code>: Rotate rows to columns. (Params) *   <code>unpivot</code>: Rotate columns to rows (melt). (Params) *   <code>aggregate</code>: Group by and sum/count/avg. (Params)</p> <p>Data Quality &amp; Cleaning *   <code>validate_and_flag</code>: Check rules and flag invalid rows. (Params) *   <code>clean_text</code>: Trim and normalize case. (Params) *   <code>filter_rows</code>: SQL-based filtering. (Params) *   <code>fill_nulls</code>: Replace NULLs with defaults. (Params)</p> <p>Feature Engineering *   <code>derive_columns</code>: Create new cols via SQL expressions. (Params) *   <code>case_when</code>: Conditional logic (if-else). (Params) *   <code>generate_surrogate_key</code>: Create MD5 keys from columns. (Params) *   <code>date_diff</code>, <code>date_add</code>, <code>date_trunc</code>: Date arithmetic.</p> <p>Scenario 1: The Full ETL Flow (Show two nodes: one loader, one processor)</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]  # &lt;--- Explicit dependency\n\n  # Explicit Transformation Steps\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Transformer) (Show a node that is a Transformer, no read needed if it picks up from dependency)</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner Run only this with <code>odibi run --tag daily</code> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  # ...\n</code></pre></p> <p>Scenario 4: Pre/Post SQL Hooks Setup and cleanup with SQL statements. <pre><code>- name: \"optimize_sales\"\n  depends_on: [\"load_sales\"]\n  pre_sql:\n    - \"SET spark.sql.shuffle.partitions = 200\"\n    - \"CREATE TEMP VIEW staging AS SELECT * FROM bronze.raw_sales\"\n  transform:\n    steps:\n      - sql: \"SELECT * FROM staging WHERE amount &gt; 0\"\n  post_sql:\n    - \"OPTIMIZE gold.fact_sales ZORDER BY (customer_id)\"\n    - \"VACUUM gold.fact_sales RETAIN 168 HOURS\"\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sales\"\n</code></pre></p> <p>Scenario 5: Materialization Strategies Choose how output is persisted. <pre><code># Option 1: View (no physical storage, logical model)\n- name: \"vw_active_customers\"\n  materialized: \"view\"  # Creates SQL view instead of table\n  transform:\n    steps:\n      - sql: \"SELECT * FROM customers WHERE status = 'active'\"\n  write:\n    connection: \"gold\"\n    table: \"vw_active_customers\"\n\n# Option 2: Incremental (append to existing Delta table)\n- name: \"fact_events\"\n  materialized: \"incremental\"  # Uses APPEND mode\n  read:\n    connection: \"bronze\"\n    table: \"raw_events\"\n    incremental:\n      mode: \"stateful\"\n      column: \"event_time\"\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"fact_events\"\n\n# Option 3: Table (default - full overwrite)\n- name: \"dim_products\"\n  materialized: \"table\"  # Default behavior\n  # ...\n</code></pre></p> Field Type Required Default Description name str Yes - Unique node name description Optional[str] No - Human-readable description runbook_url Optional[str] No - URL to troubleshooting guide or runbook. Shown as 'Troubleshooting guide \u2192' link on failures. enabled bool No <code>True</code> If False, node is skipped during execution tags List[str] No <code>PydanticUndefined</code> Operational tags for selective execution (e.g., 'daily', 'critical'). Use with <code>odibi run --tag</code>. depends_on List[str] No <code>PydanticUndefined</code> List of parent nodes that must complete before this node runs. The output of these nodes is available for reading. columns Dict[str, ColumnMetadata] No <code>PydanticUndefined</code> Data Dictionary defining the output schema. Used for documentation, PII tagging, and validation. read Optional[ReadConfig] No - Input operation (Load). If missing, data is taken from the first dependency. inputs Optional[Dict[str, str | Dict[str, Any]]] No - Multi-input support for cross-pipeline dependencies. Map input names to either: (a) $pipeline.node reference (e.g., '$read_bronze.shift_events') (b) Explicit read config dict. Cannot be used with 'read'. Example: inputs: {events: '$read_bronze.events', calendar: {connection: 'goat', path: 'cal'}} transform Optional[TransformConfig] No - Chain of fine-grained transformation steps (SQL, functions). Runs after 'transformer' if both are present. write Optional[WriteConfig] No - Output operation (Save to file/table). streaming bool No <code>False</code> Enable streaming execution for this node (Spark only) transformer Optional[str] No - Name of the 'App' logic to run (e.g., 'deduplicate', 'scd2'). See Transformer Catalog for options. params Dict[str, Any] No <code>PydanticUndefined</code> Parameters for transformer pre_sql List[str] No <code>PydanticUndefined</code> List of SQL statements to execute before node runs. Use for setup: temp tables, variable initialization, grants. Example: ['SET spark.sql.shuffle.partitions=200', 'CREATE TEMP VIEW src AS SELECT * FROM raw'] post_sql List[str] No <code>PydanticUndefined</code> List of SQL statements to execute after node completes. Use for cleanup, optimization, or audit logging. Example: ['OPTIMIZE gold.fact_sales', 'VACUUM gold.fact_sales RETAIN 168 HOURS'] materialized Optional[Literal['table', 'view', 'incremental']] No - Materialization strategy. Options: 'table' (default physical write), 'view' (creates SQL view instead of table), 'incremental' (uses append mode for Delta tables). Views are useful for Gold layer logical models. cache bool No <code>False</code> Cache result for reuse log_level Optional[LogLevel] No - Override log level for this node on_error ErrorStrategy No <code>ErrorStrategy.FAIL_LATER</code> Failure handling strategy validation Optional[ValidationConfig] No - - contracts List[TestConfig] No <code>PydanticUndefined</code> Pre-condition contracts (Circuit Breakers). Runs on input data before transformation.Options: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract schema_policy Optional[SchemaPolicyConfig] No - Schema drift handling policy privacy Optional[PrivacyConfig] No - Privacy Suite: PII anonymization settings sensitive bool | List[str] No <code>False</code> If true or list of columns, masks sample data in stories source_yaml Optional[str] No - Internal: source YAML file path for sql_file resolution"},{"location":"reference/yaml_schema/#columnmetadata","title":"<code>ColumnMetadata</code>","text":"<p>Used in: NodeConfig</p> <p>Metadata for a column in the data dictionary.</p> Field Type Required Default Description description Optional[str] No - Column description pii bool No <code>False</code> Contains PII? tags List[str] No <code>PydanticUndefined</code> Tags (e.g. 'business_key', 'measure')"},{"location":"reference/yaml_schema/#systemconfig","title":"<code>SystemConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for the Odibi System Catalog (The Brain).</p> <p>Stores metadata, state, and pattern configurations. The primary connection must be a storage connection (blob/local) that supports Delta tables.</p> <p>Example: <pre><code>system:\n  connection: adls_bronze        # Primary - must be blob/local storage\n  path: _odibi_system\n  environment: dev\n</code></pre></p> <p>With sync to SQL Server (for dashboards/queries): <pre><code>system:\n  connection: adls_prod          # Primary - Delta tables\n  environment: prod\n  sync_to:\n    connection: sql_server_prod  # Secondary - SQL for visibility\n    schema_name: odibi_system\n</code></pre></p> <p>With sync to another blob (cross-region backup): <pre><code>system:\n  connection: adls_us_east\n  sync_to:\n    connection: adls_us_west\n    path: _odibi_system_replica\n</code></pre></p> Field Type Required Default Description connection str Yes - Connection for primary system tables. Must be blob storage (azure_blob) or local filesystem - NOT SQL Server. Delta tables require storage backends. path str No <code>_odibi_system</code> Path relative to connection root environment Optional[str] No - Environment tag (e.g., 'dev', 'qat', 'prod'). Written to all system table records for cross-environment querying. schema_name Optional[str] No - Deprecated. Use sync_to.schema_name for SQL Server targets. sync_to Optional[SyncToConfig] No - Secondary destination to sync system catalog data to. Use for SQL Server dashboards or cross-region Delta replication. sync_from Optional[SyncFromConfig] No - Source to sync system data from. Enables pushing local development data to centralized system tables. cost_per_compute_hour Optional[float] No - Estimated cost per compute hour (USD) for cost tracking databricks_billing_enabled bool No <code>False</code> Attempt to query Databricks billing tables for actual costs retention_days Optional[RetentionConfig] No - Retention periods for system tables"},{"location":"reference/yaml_schema/#syncfromconfig","title":"<code>SyncFromConfig</code>","text":"<p>Used in: SystemConfig</p> <p>Configuration for syncing system data from a source location.</p> <p>Used to pull system data (runs, state) from another backend into the target.</p> <p>Example: <pre><code>sync_from:\n  connection: local_parquet\n  path: .odibi/system/\n</code></pre></p> Field Type Required Default Description connection str Yes - Connection name for the source system data path Optional[str] No - Path to source system data (for file-based sources) schema_name Optional[str] No - Schema name for SQL Server source (if applicable)"},{"location":"reference/yaml_schema/#connections","title":"Connections","text":""},{"location":"reference/yaml_schema/#localconnectionconfig","title":"<code>LocalConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Local filesystem connection.</p> <p>When to Use: Development, testing, small datasets, local processing.</p> <p>See Also: AzureBlobConnectionConfig for cloud alternatives.</p> <p>Example: <pre><code>local_data:\n  type: \"local\"\n  base_path: \"./data\"\n</code></pre></p> Field Type Required Default Description type Literal['local'] No <code>ConnectionType.LOCAL</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - base_path str No <code>./data</code> Base directory path"},{"location":"reference/yaml_schema/#deltaconnectionconfig","title":"<code>DeltaConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Delta Lake connection for ACID-compliant data lakes.</p> <p>When to Use: - Production data lakes on Azure/AWS/GCP - Need time travel, ACID transactions, schema evolution - Upsert/merge operations</p> <p>See Also: WriteConfig for Delta write options</p> <p>Scenario 1: Delta via metastore <pre><code>delta_silver:\n  type: \"delta\"\n  catalog: \"spark_catalog\"\n  schema: \"silver_db\"\n</code></pre></p> <p>Scenario 2: Direct path + Node usage <pre><code>delta_local:\n  type: \"local\"\n  base_path: \"dbfs:/mnt/delta\"\n\n# In pipeline:\n# read:\n#   connection: \"delta_local\"\n#   format: \"delta\"\n#   path: \"bronze/orders\"\n</code></pre></p> Field Type Required Default Description type Literal['delta'] No <code>ConnectionType.DELTA</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - catalog str Yes - Spark catalog name (e.g. 'spark_catalog') schema_name str Yes - Database/schema name table Optional[str] No - Optional default table name for this connection (used by story/pipeline helpers)"},{"location":"reference/yaml_schema/#azureblobconnectionconfig","title":"<code>AzureBlobConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Azure Blob Storage / ADLS Gen2 connection.</p> <p>When to Use: Azure-based data lakes, landing zones, raw data storage.</p> <p>See Also: DeltaConnectionConfig for Delta-specific options</p> <p>Scenario 1: Prod with Key Vault-managed key <pre><code>adls_bronze:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"key_vault\"\n    key_vault: \"kv-data\"\n    secret: \"adls-account-key\"\n</code></pre></p> <p>Scenario 2: Local dev with inline account key <pre><code>adls_dev:\n  type: \"azure_blob\"\n  account_name: \"devaccount\"\n  container: \"sandbox\"\n  auth:\n    mode: \"account_key\"\n    account_key: \"${ADLS_ACCOUNT_KEY}\"\n</code></pre></p> <p>Scenario 3: MSI (no secrets) <pre><code>adls_msi:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"aad_msi\"\n    # optional: client_id for user-assigned identity\n    client_id: \"00000000-0000-0000-0000-000000000000\"\n</code></pre></p> Field Type Required Default Description type Literal['azure_blob'] No <code>ConnectionType.AZURE_BLOB</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - account_name str Yes - - container str Yes - - auth AzureBlobAuthConfig No <code>PydanticUndefined</code> Options: AzureBlobKeyVaultAuth, AzureBlobAccountKeyAuth, AzureBlobSasAuth, AzureBlobConnectionStringAuth, AzureBlobMsiAuth"},{"location":"reference/yaml_schema/#sqlserverconnectionconfig","title":"<code>SQLServerConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>SQL Server / Azure SQL Database connection.</p> <p>When to Use: Reading from SQL Server sources, Azure SQL DB, Azure Synapse.</p> <p>See Also: ReadConfig for query options</p> <p>Scenario 1: Managed identity (AAD MSI) <pre><code>sql_dw_msi:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"aad_msi\"\n</code></pre></p> <p>Scenario 2: SQL login <pre><code>sql_dw_login:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  port: 1433\n  driver: \"ODBC Driver 17 for SQL Server\"\n  auth:\n    mode: \"sql_login\"\n    username: \"dw_writer\"\n    password: \"${DW_PASSWORD}\"\n</code></pre></p> Field Type Required Default Description type Literal['sql_server'] No <code>ConnectionType.SQL_SERVER</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - host str Yes - - database str Yes - - port int No <code>1433</code> - driver str No <code>ODBC Driver 18 for SQL Server</code> - auth SQLServerAuthConfig No <code>PydanticUndefined</code> Options: SQLLoginAuth, SQLAadPasswordAuth, SQLMsiAuth, SQLConnectionStringAuth"},{"location":"reference/yaml_schema/#httpconnectionconfig","title":"<code>HttpConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>HTTP connection.</p> <p>Scenario: Bearer token via env var <pre><code>api_source:\n  type: \"http\"\n  base_url: \"https://api.example.com\"\n  headers:\n    User-Agent: \"odibi-pipeline\"\n  auth:\n    mode: \"bearer\"\n    token: \"${API_TOKEN}\"\n</code></pre></p> Field Type Required Default Description type Literal['http'] No <code>ConnectionType.HTTP</code> - validation_mode ValidationMode No <code>ValidationMode.LAZY</code> - base_url str Yes - - headers Dict[str, str] No <code>PydanticUndefined</code> - auth HttpAuthConfig No <code>PydanticUndefined</code> Options: HttpNoAuth, HttpBasicAuth, HttpBearerAuth, HttpApiKeyAuth"},{"location":"reference/yaml_schema/#node-operations","title":"Node Operations","text":""},{"location":"reference/yaml_schema/#readconfig","title":"<code>ReadConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for reading data into a node.</p> <p>When to Use: First node in a pipeline, or any node that reads from storage.</p> <p>Key Concepts: - <code>connection</code>: References a named connection from <code>connections:</code> section - <code>format</code>: File format (csv, parquet, delta, json, sql) - <code>incremental</code>: Enable incremental loading (only new data)</p> <p>See Also: - Incremental Loading - HWM-based loading - IncrementalConfig - Incremental loading options</p>"},{"location":"reference/yaml_schema/#universal-reader-guide","title":"\ud83d\udcd6 \"Universal Reader\" Guide","text":"<p>Business Problem: \"I need to read from files, databases, streams, and even travel back in time to see how data looked yesterday.\"</p> <p>Recipe 1: The Time Traveler (Delta/Iceberg) Reproduce a bug by seeing the data exactly as it was. <pre><code>read:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  time_travel:\n    as_of_timestamp: \"2023-10-25T14:00:00Z\"\n</code></pre></p> <p>Recipe 2: The Streamer Process data in real-time. <pre><code>read:\n  connection: \"event_hub\"\n  format: \"json\"\n  streaming: true\n</code></pre></p> <p>Recipe 3: The SQL Query Push down filtering to the source database. <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  # Use the query option to filter at source!\n  query: \"SELECT * FROM huge_table WHERE date &gt;= '2024-01-01'\"\n</code></pre></p> <p>Recipe 4: Archive Bad Records (Spark) Capture malformed records for later inspection. <pre><code>read:\n  connection: \"landing\"\n  format: \"json\"\n  path: \"events/*.json\"\n  archive_options:\n    badRecordsPath: \"/mnt/quarantine/bad_records\"\n</code></pre></p> <p>Recipe 5: Optimize JDBC Parallelism (Spark) Control partition count for SQL sources to reduce task overhead. <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  table: \"small_lookup_table\"\n  options:\n    numPartitions: 1  # Single partition for small tables\n</code></pre></p> <p>Performance Tip: For small tables (&lt;100K rows), use <code>numPartitions: 1</code> to avoid excessive Spark task scheduling overhead. For large tables, increase partitions to enable parallel reads (requires partitionColumn, lowerBound, upperBound).</p> Field Type Required Default Description connection str Yes - Connection name from project.yaml format ReadFormat | str Yes - Data format (csv, parquet, delta, etc.) table Optional[str] No - Table name for SQL/Delta path Optional[str] No - Path for file-based sources streaming bool No <code>False</code> Enable streaming read (Spark only) schema_ddl Optional[str] No - Schema for streaming reads from file sources (required for Avro, JSON, CSV). Use Spark DDL format: 'col1 STRING, col2 INT, col3 TIMESTAMP'. Not required for Delta (schema is inferred from table metadata). query Optional[str] No - SQL query to filter at source (pushdown). Mutually exclusive with table/path if supported by connector. filter Optional[str] No - SQL WHERE clause filter (pushed down to source for SQL formats). Example: \"DAY &gt; '2022-12-31'\" incremental Optional[IncrementalConfig] No - Automatic incremental loading strategy (CDC-like). If set, generates query based on target state (HWM). time_travel Optional[TimeTravelConfig] No - Time travel options (Delta only) archive_options Dict[str, Any] No <code>PydanticUndefined</code> Options for archiving bad records (e.g. badRecordsPath for Spark) options Dict[str, Any] No <code>PydanticUndefined</code> Format-specific options"},{"location":"reference/yaml_schema/#incrementalconfig","title":"<code>IncrementalConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for automatic incremental loading.</p> <p>When to Use: Load only new/changed data instead of full table scans.</p> <p>See Also: ReadConfig</p> <p>Modes: 1. Rolling Window (Default): Uses a time-based lookback from NOW().    Good for: Stateless loading where you just want \"recent\" data.    Args: <code>lookback</code>, <code>unit</code></p> <ol> <li>Stateful: Tracks the High-Water Mark (HWM) of the key column.    Good for: Exact incremental ingestion (e.g. CDC-like).    Args: <code>state_key</code> (optional), <code>watermark_lag</code> (optional)</li> </ol> <p>Generates SQL: - Rolling: <code>WHERE column &gt;= NOW() - lookback</code> - Stateful: <code>WHERE column &gt; :last_hwm</code></p> <p>Example (Rolling Window): <pre><code>incremental:\n  mode: \"rolling_window\"\n  column: \"updated_at\"\n  lookback: 3\n  unit: \"day\"\n</code></pre></p> <p>Example (Stateful HWM): <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"id\"\n  # Optional: track separate column for HWM state\n  state_key: \"last_processed_id\"\n</code></pre></p> <p>Example (Stateful with Watermark Lag): <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"updated_at\"\n  # Handle late-arriving data: look back 2 hours from HWM\n  watermark_lag: \"2h\"\n</code></pre></p> <p>Example (Oracle Date Format): <pre><code>incremental:\n  mode: \"rolling_window\"\n  column: \"EVENT_START\"\n  lookback: 3\n  unit: \"day\"\n  # For string columns with Oracle format (DD-MON-YY)\n  date_format: \"oracle\"\n</code></pre></p> <p>Supported date_format values: - <code>oracle</code>: DD-MON-YY for Oracle databases (uses TO_TIMESTAMP) - <code>oracle_sqlserver</code>: DD-MON-YY format stored in SQL Server (uses TRY_CONVERT) - <code>sql_server</code>: Uses CONVERT with style 120 - <code>us</code>: MM/DD/YYYY format - <code>eu</code>: DD/MM/YYYY format - <code>iso</code>: YYYY-MM-DDTHH:MM:SS format</p> Field Type Required Default Description mode IncrementalMode No <code>IncrementalMode.ROLLING_WINDOW</code> Incremental strategy: 'rolling_window' or 'stateful' column str Yes - Primary column to filter on (e.g., updated_at) fallback_column Optional[str] No - Backup column if primary is NULL (e.g., created_at). Generates COALESCE(col, fallback) &gt;= ... lookback Optional[int] No - Time units to look back (Rolling Window only) unit Optional[IncrementalUnit] No - Time unit for lookback (Rolling Window only). Options: 'hour', 'day', 'month', 'year' state_key Optional[str] No - Unique ID for state tracking. Defaults to node name if not provided. watermark_lag Optional[str] No - Safety buffer for late-arriving data in stateful mode. Subtracts this duration from the stored HWM when filtering. Format: '' where unit is 's', 'm', 'h', or 'd'. Examples: '2h' (2 hours), '30m' (30 minutes), '1d' (1 day). Use when source has replication lag or eventual consistency. date_format Optional[str] No - Source date format when the column is stored as a string. Options: 'oracle' (DD-MON-YY for Oracle DB), 'oracle_sqlserver' (DD-MON-YY format in SQL Server), 'sql_server' (uses CONVERT with style 120), 'us' (MM/DD/YYYY), 'eu' (DD/MM/YYYY), 'iso' (YYYY-MM-DDTHH:MM:SS). When set, SQL pushdown will use appropriate CONVERT/TO_TIMESTAMP functions."},{"location":"reference/yaml_schema/#timetravelconfig","title":"<code>TimeTravelConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for time travel reading (Delta/Iceberg).</p> <p>Example: <pre><code>time_travel:\n  as_of_version: 10\n  # OR\n  as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre></p> Field Type Required Default Description as_of_version Optional[int] No - Version number to time travel to as_of_timestamp Optional[str] No - Timestamp string to time travel to"},{"location":"reference/yaml_schema/#transformconfig","title":"<code>TransformConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for transformation steps within a node.</p> <p>When to Use: Custom business logic, data cleaning, SQL transformations.</p> <p>Key Concepts: - <code>steps</code>: Ordered list of operations (SQL, functions, or both) - Each step receives the DataFrame from the previous step - Steps execute in order: step1 \u2192 step2 \u2192 step3</p> <p>See Also: Transformer Catalog</p> <p>Transformer vs Transform: - <code>transformer</code>: Single heavy operation (scd2, merge, deduplicate) - <code>transform.steps</code>: Chain of lighter operations</p>"},{"location":"reference/yaml_schema/#transformation-pipeline-guide","title":"\ud83d\udd27 \"Transformation Pipeline\" Guide","text":"<p>Business Problem: \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"</p> <p>The Solution: Chain multiple steps together. Output of Step 1 becomes input of Step 2.</p> <p>Function Registry: The <code>function</code> step type looks up functions registered with <code>@transform</code> (or <code>@register</code>). This allows you to use the same registered functions as both top-level Transformers and steps in a chain.</p> <p>Recipe: The Mix-and-Match <pre><code>transform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n</code></pre></p> Field Type Required Default Description steps List[str | TransformStep] Yes - List of transformation steps (SQL strings or TransformStep configs)"},{"location":"reference/yaml_schema/#deletedetectionconfig","title":"<code>DeleteDetectionConfig</code>","text":"<p>Configuration for delete detection in Silver layer.</p>"},{"location":"reference/yaml_schema/#cdc-without-cdc-guide","title":"\ud83d\udd0d \"CDC Without CDC\" Guide","text":"<p>Business Problem: \"Records are deleted in our Azure SQL source, but our Silver tables still show them.\"</p> <p>The Solution: Use delete detection to identify and flag records that no longer exist in the source.</p> <p>Recipe 1: SQL Compare (Recommended for HWM) <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n</code></pre></p> <p>Recipe 2: Snapshot Diff (For Full Snapshot Sources) Use ONLY with full snapshot ingestion, NOT with HWM incremental. Requires <code>connection</code> and <code>path</code> to specify the target Delta table for comparison. <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [customer_id]\n        connection: silver_conn    # Required: connection to target Delta table\n        path: \"silver/customers\"   # Required: path to target Delta table\n</code></pre></p> <p>Recipe 3: Conservative Threshold <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: erp\n        source_table: dbo.Customers\n        max_delete_percent: 20.0\n        on_threshold_breach: error\n</code></pre></p> <p>Recipe 4: Hard Delete (Remove Rows) <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n        soft_delete_col: null  # removes rows instead of flagging\n</code></pre></p> Field Type Required Default Description mode DeleteDetectionMode No <code>DeleteDetectionMode.NONE</code> Delete detection strategy: none, snapshot_diff, sql_compare keys List[str] No <code>PydanticUndefined</code> Business key columns for comparison connection Optional[str] No - For snapshot_diff: connection name to target Delta table (required for snapshot_diff) path Optional[str] No - For snapshot_diff: path to target Delta table (required for snapshot_diff) soft_delete_col Optional[str] No <code>_is_deleted</code> Column to flag deletes (True = deleted). Set to null for hard-delete (removes rows). source_connection Optional[str] No - For sql_compare: connection name to query live source source_table Optional[str] No - For sql_compare: table to query for current keys source_query Optional[str] No - For sql_compare: custom SQL query for keys (overrides source_table) snapshot_column Optional[str] No - For snapshot_diff on non-Delta: column to identify snapshots. If None, uses Delta time travel (default). on_first_run FirstRunBehavior No <code>FirstRunBehavior.SKIP</code> Behavior when no previous version exists for snapshot_diff max_delete_percent Optional[float] No <code>50.0</code> Safety threshold: warn/error if more than X% of rows would be deleted on_threshold_breach ThresholdBreachAction No <code>ThresholdBreachAction.WARN</code> Behavior when delete percentage exceeds max_delete_percent"},{"location":"reference/yaml_schema/#validationconfig","title":"<code>ValidationConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for data validation (post-transform checks).</p> <p>When to Use: Output data quality checks that run after transformation but before writing.</p> <p>See Also: Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)</p>"},{"location":"reference/yaml_schema/#the-indestructible-pipeline-pattern","title":"\ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern","text":"<p>Business Problem: \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it before it lands.\"</p> <p>The Solution: A Quality Gate that runs after transformation but before writing.</p> <p>Recipe: The Quality Gate <pre><code>validation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges &amp; Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at &lt;= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n</code></pre></p> <p>Recipe: Quarantine + Gate <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n</code></pre></p> Field Type Required Default Description mode ValidationAction No <code>ValidationAction.FAIL</code> Execution mode: 'fail' (stop pipeline) or 'warn' (log only) on_fail OnFailAction No <code>OnFailAction.ALERT</code> Action on failure: 'alert' (send notification) or 'ignore' tests List[TestConfig] No <code>PydanticUndefined</code> List of validation testsOptions: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract quarantine Optional[QuarantineConfig] No - Quarantine configuration for failed rows gate Optional[GateConfig] No - Quality gate configuration for batch-level validation fail_fast bool No <code>False</code> Stop validation on first failure. Skips remaining tests for faster feedback. cache_df bool No <code>False</code> Cache DataFrame before validation (Spark only). Improves performance with many tests."},{"location":"reference/yaml_schema/#quarantineconfig","title":"<code>QuarantineConfig</code>","text":"<p>Used in: ValidationConfig</p> <p>Configuration for quarantine table routing.</p> <p>When to Use: Capture invalid records for review/reprocessing instead of failing the pipeline.</p> <p>See Also: Quarantine Guide, ValidationConfig</p> <p>Routes rows that fail validation tests to a quarantine table with rejection metadata for later analysis/reprocessing.</p> <p>Example: <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n    max_rows: 10000\n    sample_fraction: 0.1\n</code></pre></p> Field Type Required Default Description connection str Yes - Connection for quarantine writes path Optional[str] No - Path for quarantine data table Optional[str] No - Table name for quarantine add_columns QuarantineColumnsConfig No <code>PydanticUndefined</code> Metadata columns to add to quarantined rows retention_days Optional[int] No <code>90</code> Days to retain quarantined data (auto-cleanup) max_rows Optional[int] No - Maximum number of rows to quarantine per run. Limits storage for high-failure batches. sample_fraction Optional[float] No - Sample fraction of invalid rows to quarantine (0.0-1.0). Use for sampling large invalid sets."},{"location":"reference/yaml_schema/#quarantinecolumnsconfig","title":"<code>QuarantineColumnsConfig</code>","text":"<p>Used in: QuarantineConfig</p> <p>Columns added to quarantined rows for debugging and reprocessing.</p> <p>Example: <pre><code>quarantine:\n  connection: silver\n  path: customers_quarantine\n  add_columns:\n    _rejection_reason: true\n    _rejected_at: true\n    _source_batch_id: true\n    _failed_tests: true\n    _original_node: false\n</code></pre></p> Field Type Required Default Description rejection_reason bool No <code>True</code> Add _rejection_reason column with test failure description rejected_at bool No <code>True</code> Add _rejected_at column with UTC timestamp source_batch_id bool No <code>True</code> Add _source_batch_id column with run ID for traceability failed_tests bool No <code>True</code> Add _failed_tests column with comma-separated list of failed test names original_node bool No <code>False</code> Add _original_node column with source node name"},{"location":"reference/yaml_schema/#gateconfig","title":"<code>GateConfig</code>","text":"<p>Used in: ValidationConfig</p> <p>Quality gate configuration for batch-level validation.</p> <p>When to Use: Pipeline-level pass/fail thresholds, row count limits, change detection.</p> <p>See Also: Quality Gates, ValidationConfig</p> <p>Gates evaluate the entire batch before writing, ensuring data quality thresholds are met.</p> <p>Example: <pre><code>gate:\n  require_pass_rate: 0.95\n  on_fail: abort\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n  row_count:\n    min: 100\n    change_threshold: 0.5\n</code></pre></p> Field Type Required Default Description require_pass_rate float No <code>0.95</code> Minimum percentage of rows passing ALL tests on_fail GateOnFail No <code>GateOnFail.ABORT</code> Action when gate fails thresholds List[GateThreshold] No <code>PydanticUndefined</code> Per-test thresholds (overrides global require_pass_rate) row_count Optional[RowCountGate] No - Row count anomaly detection"},{"location":"reference/yaml_schema/#gatethreshold","title":"<code>GateThreshold</code>","text":"<p>Used in: GateConfig</p> <p>Per-test threshold configuration for quality gates.</p> <p>Allows setting different pass rate requirements for specific tests.</p> <p>Example: <pre><code>gate:\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n    - test: unique\n      min_pass_rate: 1.0\n</code></pre></p> Field Type Required Default Description test str Yes - Test name or type to apply threshold to min_pass_rate float Yes - Minimum pass rate required (0.0-1.0, e.g., 0.99 = 99%)"},{"location":"reference/yaml_schema/#rowcountgate","title":"<code>RowCountGate</code>","text":"<p>Used in: GateConfig</p> <p>Row count anomaly detection for quality gates.</p> <p>Validates that batch size falls within expected bounds and detects significant changes from previous runs.</p> <p>Example: <pre><code>gate:\n  row_count:\n    min: 100\n    max: 1000000\n    change_threshold: 0.5\n</code></pre></p> Field Type Required Default Description min Optional[int] No - Minimum expected row count max Optional[int] No - Maximum expected row count change_threshold Optional[float] No - Max allowed change vs previous run (e.g., 0.5 = 50% change triggers failure)"},{"location":"reference/yaml_schema/#writeconfig","title":"<code>WriteConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for writing data from a node.</p> <p>When to Use: Any node that persists data to storage.</p> <p>Key Concepts: - <code>mode</code>: How to handle existing data (overwrite, append, upsert) - <code>keys</code>: Required for upsert mode - columns that identify unique records - <code>partition_by</code>: Columns to partition output by (improves query performance)</p> <p>See Also: - Performance Tuning - Partitioning strategies</p>"},{"location":"reference/yaml_schema/#big-data-performance-guide","title":"\ud83d\ude80 \"Big Data Performance\" Guide","text":"<p>Business Problem: \"My dashboards are slow because the query scans terabytes of data just to find one day's sales.\"</p> <p>The Solution: Use Partitioning for coarse filtering (skipping huge chunks) and Z-Ordering for fine-grained skipping (colocating related data).</p> <p>Recipe: Lakehouse Optimized <pre><code>write:\n  connection: \"gold_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  mode: \"append\"\n\n  # 1. Partitioning: Physical folders.\n  # Use for low-cardinality columns often used in WHERE clauses.\n  # WARNING: Do NOT partition by high-cardinality cols like ID or Timestamp!\n  partition_by: [\"country_code\", \"txn_year_month\"]\n\n  # 2. Z-Ordering: Data clustering.\n  # Use for high-cardinality columns often used in JOINs or predicates.\n  zorder_by: [\"customer_id\", \"product_id\"]\n\n  # 3. Table Properties: Engine tuning.\n  table_properties:\n    \"delta.autoOptimize.optimizeWrite\": \"true\"\n    \"delta.autoOptimize.autoCompact\": \"true\"\n</code></pre></p> Field Type Required Default Description connection str Yes - Connection name from project.yaml format ReadFormat | str Yes - Output format (csv, parquet, delta, etc.) table Optional[str] No - Table name for SQL/Delta path Optional[str] No - Path for file-based outputs register_table Optional[str] No - Register file output as external table (Spark/Delta only) mode WriteMode No <code>WriteMode.OVERWRITE</code> Write mode. Options: 'overwrite', 'append', 'upsert', 'append_once' partition_by List[str] No <code>PydanticUndefined</code> List of columns to physically partition the output by (folder structure). Use for low-cardinality columns (e.g. date, country). zorder_by List[str] No <code>PydanticUndefined</code> List of columns to Z-Order by. Improves read performance for high-cardinality columns used in filters/joins (Delta only). table_properties Dict[str, str] No <code>PydanticUndefined</code> Delta table properties. Overrides global performance.delta_table_properties. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. merge_schema bool No <code>False</code> Allow schema evolution (mergeSchema option in Delta) first_run_query Optional[str] No - SQL query for full-load on first run (High Water Mark pattern). If set, uses this query when target table doesn't exist, then switches to incremental. Only applies to SQL reads. options Dict[str, Any] No <code>PydanticUndefined</code> Format-specific options auto_optimize bool | AutoOptimizeConfig No - Auto-run OPTIMIZE and VACUUM after write (Delta only) add_metadata bool | WriteMetadataConfig No - Add metadata columns for Bronze layer lineage. Set to <code>true</code> to add all applicable columns, or provide a WriteMetadataConfig for selective columns. Columns: _extracted_at, _source_file (file sources), _source_connection, _source_table (SQL sources). skip_if_unchanged bool No <code>False</code> Skip write if DataFrame content is identical to previous write. Computes SHA256 hash of entire DataFrame and compares to stored hash in Delta table metadata. Useful for snapshot tables without timestamps to avoid redundant appends. Only supported for Delta format. skip_hash_columns Optional[List[str]] No - Columns to include in hash computation for skip_if_unchanged. If None, all columns are used. Specify a subset to ignore volatile columns like timestamps. skip_hash_sort_columns Optional[List[str]] No - Columns to sort by before hashing for deterministic comparison. Required if row order may vary between runs. Typically your business key columns. streaming Optional[StreamingWriteConfig] No - Streaming write configuration for Spark Structured Streaming. When set, uses writeStream instead of batch write. Requires a streaming DataFrame from a streaming read source. merge_keys Optional[List[str]] No - Key columns for SQL Server MERGE operations. Required when mode='merge'. These columns form the ON clause of the MERGE statement. merge_options Optional[SqlServerMergeOptions] No - Options for SQL Server MERGE operations (conditions, staging, audit cols) overwrite_options Optional[SqlServerOverwriteOptions] No - Options for SQL Server overwrite operations (strategy, audit cols)"},{"location":"reference/yaml_schema/#writemetadataconfig","title":"<code>WriteMetadataConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for metadata columns added during Bronze writes.</p>"},{"location":"reference/yaml_schema/#bronze-metadata-guide","title":"\ud83d\udccb Bronze Metadata Guide","text":"<p>Business Problem: \"We need lineage tracking and debugging info for our Bronze layer data.\"</p> <p>The Solution: Add metadata columns during ingestion for traceability.</p> <p>Recipe 1: Add All Metadata (Recommended) <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n</code></pre></p> <p>Recipe 2: Selective Metadata <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: false\n    source_table: false\n</code></pre></p> <p>Available Columns: - <code>_extracted_at</code>: Pipeline execution timestamp (all sources) - <code>_source_file</code>: Source filename/path (file sources only) - <code>_source_connection</code>: Connection name used (all sources) - <code>_source_table</code>: Table or query name (SQL sources only)</p> Field Type Required Default Description extracted_at bool No <code>True</code> Add _extracted_at column with pipeline execution timestamp source_file bool No <code>True</code> Add _source_file column with source filename (file sources only) source_connection bool No <code>False</code> Add _source_connection column with connection name source_table bool No <code>False</code> Add _source_table column with table/query name (SQL sources only)"},{"location":"reference/yaml_schema/#streamingwriteconfig","title":"<code>StreamingWriteConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Spark Structured Streaming writes.</p>"},{"location":"reference/yaml_schema/#real-time-pipeline-guide","title":"\ud83d\ude80 \"Real-Time Pipeline\" Guide","text":"<p>Business Problem: \"I need to process data continuously as it arrives from Kafka/Event Hubs and write it to Delta Lake in near real-time.\"</p> <p>The Solution: Configure streaming write with checkpoint location for fault tolerance and trigger interval for processing frequency.</p> <p>Recipe: Streaming Ingestion <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_stream\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_stream\"\n    trigger:\n      processing_time: \"10 seconds\"\n</code></pre></p> <p>Recipe: One-Time Streaming (Batch-like) <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_batch\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_batch\"\n    trigger:\n      available_now: true\n</code></pre></p> Field Type Required Default Description output_mode Literal['append', 'update', 'complete'] No <code>append</code> Output mode for streaming writes. 'append' - Only new rows. 'update' - Updated rows only. 'complete' - Entire result table (requires aggregation). checkpoint_location str Yes - Path for streaming checkpoints. Required for fault tolerance. Must be a reliable storage location (e.g., cloud storage, DBFS). trigger Optional[TriggerConfig] No - Trigger configuration. If not specified, processes data as fast as possible. Use 'processing_time' for micro-batch intervals, 'once' for single batch, 'available_now' for processing all available data then stopping. query_name Optional[str] No - Name for the streaming query (useful for monitoring and debugging) await_termination Optional[bool] No <code>False</code> Wait for the streaming query to terminate. Set to True for batch-like streaming with 'once' or 'available_now' triggers. timeout_seconds Optional[int] No - Timeout in seconds when await_termination is True. If None, waits indefinitely."},{"location":"reference/yaml_schema/#triggerconfig","title":"<code>TriggerConfig</code>","text":"<p>Used in: StreamingWriteConfig</p> <p>Configuration for streaming trigger intervals.</p> <p>Specify exactly one of the trigger options.</p> <p>Example: <pre><code>trigger:\n  processing_time: \"10 seconds\"\n</code></pre></p> <p>Or for one-time processing: <pre><code>trigger:\n  once: true\n</code></pre></p> Field Type Required Default Description processing_time Optional[str] No - Trigger interval as duration string (e.g., '10 seconds', '1 minute') once Optional[bool] No - Process all available data once and stop available_now Optional[bool] No - Process all available data in multiple batches, then stop continuous Optional[str] No - Continuous processing with checkpoint interval (e.g., '1 second')"},{"location":"reference/yaml_schema/#autooptimizeconfig","title":"<code>AutoOptimizeConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Delta Lake automatic optimization.</p> <p>Example: <pre><code>auto_optimize:\n  enabled: true\n  vacuum_retention_hours: 168\n</code></pre></p> Field Type Required Default Description enabled bool No <code>True</code> Enable auto optimization vacuum_retention_hours int No <code>168</code> Hours to retain history for VACUUM (default 7 days). Set to 0 to disable VACUUM."},{"location":"reference/yaml_schema/#privacyconfig","title":"<code>PrivacyConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for PII anonymization.</p>"},{"location":"reference/yaml_schema/#privacy-pii-protection","title":"\ud83d\udd10 Privacy &amp; PII Protection","text":"<p>How It Works: 1. Mark columns as <code>pii: true</code> in the <code>columns</code> metadata 2. Configure a <code>privacy</code> block with the anonymization method 3. During node execution, all columns marked as PII (and inherited from dependencies) are anonymized 4. Upstream PII markings are inherited by downstream nodes</p> <p>Example: <pre><code>columns:\n  customer_email:\n    pii: true  # Mark as PII\n  customer_id:\n    pii: false\n\nprivacy:\n  method: hash       # hash, mask, or redact\n  salt: \"secret_key\" # Optional: makes hash unique/secure\n  declassify: []     # Remove columns from PII protection\n</code></pre></p> <p>Methods: - <code>hash</code>: SHA256 hash (length 64). With salt, prevents pre-computed rainbow tables. - <code>mask</code>: Show only last 4 chars, replace rest with <code>*</code>. Example: <code>john@email.com</code> \u2192 <code>****@email.com</code> - <code>redact</code>: Replace entire value with <code>[REDACTED]</code></p> <p>Important: - <code>pii: true</code> alone does NOTHING. You must set a <code>privacy.method</code> to actually mask data. - PII inheritance: If dependency outputs PII columns, this node inherits them unless declassified. - Salt is optional but recommended for hash to prevent attacks.</p> Field Type Required Default Description method PrivacyMethod Yes - Anonymization method: 'hash' (SHA256), 'mask' (show last 4), or 'redact' ([REDACTED]) salt Optional[str] No - Salt for hashing (optional but recommended). Appended before hashing to create unique hashes. Example: 'company_secret_key_2025' declassify List[str] No <code>PydanticUndefined</code> List of columns to remove from PII protection (stops inheritance from upstream). Example: ['customer_id']"},{"location":"reference/yaml_schema/#sqlserverauditcolsconfig","title":"<code>SqlServerAuditColsConfig</code>","text":"<p>Used in: SqlServerMergeOptions, SqlServerOverwriteOptions</p> <p>Audit column configuration for SQL Server merge operations.</p> <p>These columns are automatically populated with GETUTCDATE() during merge: - <code>created_col</code>: Set on INSERT only - <code>updated_col</code>: Set on INSERT and UPDATE</p> <p>Example: <pre><code>audit_cols:\n  created_col: created_ts\n  updated_col: updated_ts\n</code></pre></p> Field Type Required Default Description created_col Optional[str] No - Column name for creation timestamp (set on INSERT) updated_col Optional[str] No - Column name for update timestamp (set on INSERT and UPDATE)"},{"location":"reference/yaml_schema/#sqlservermergeoptions","title":"<code>SqlServerMergeOptions</code>","text":"<p>Used in: WriteConfig</p> <p>Options for SQL Server MERGE operations (Phase 1).</p> <p>Enables incremental sync from Spark to SQL Server using T-SQL MERGE. Data is written to a staging table, then merged into the target.</p>"},{"location":"reference/yaml_schema/#basic-usage","title":"Basic Usage","text":"<pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: sales.fact_orders\n  mode: merge\n  merge_keys: [DateId, store_id]\n  merge_options:\n    update_condition: \"source._hash_diff != target._hash_diff\"\n    exclude_columns: [_hash_diff]\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre>"},{"location":"reference/yaml_schema/#conditions","title":"Conditions","text":"<ul> <li><code>update_condition</code>: Only update rows matching this condition (e.g., hash diff)</li> <li><code>delete_condition</code>: Delete rows matching this condition (soft delete pattern)</li> <li><code>insert_condition</code>: Only insert rows matching this condition</li> </ul> Field Type Required Default Description update_condition Optional[str] No - SQL condition for WHEN MATCHED UPDATE. Use 'source.' and 'target.' prefixes. Example: 'source._hash_diff != target._hash_diff' delete_condition Optional[str] No - SQL condition for WHEN MATCHED DELETE. Example: 'source._is_deleted = 1' insert_condition Optional[str] No - SQL condition for WHEN NOT MATCHED INSERT. Example: 'source.is_valid = 1' exclude_columns List[str] No <code>PydanticUndefined</code> Columns to exclude from MERGE (not written to target table) staging_schema str No <code>staging</code> Schema for staging table. Table name: {staging_schema}.{table}_staging audit_cols Optional[SqlServerAuditColsConfig] No - Audit columns for created/updated timestamps validations Optional[ForwardRef('SqlServerMergeValidationConfig')] No - Validation checks before merge (null keys, duplicate keys) auto_create_schema bool No <code>False</code> Auto-create schema if it doesn't exist (Phase 4). Runs CREATE SCHEMA IF NOT EXISTS. auto_create_table bool No <code>False</code> Auto-create target table if it doesn't exist (Phase 4). Infers schema from DataFrame. schema_evolution Optional[ForwardRef('SqlServerSchemaEvolutionConfig')] No - Schema evolution configuration (Phase 4). Controls handling of schema differences. batch_size Optional[int] No - Batch size for staging table writes (Phase 4). Chunks large DataFrames for memory efficiency. primary_key_on_merge_keys bool No <code>False</code> Create a clustered primary key on merge_keys when auto-creating table. Enforces uniqueness. index_on_merge_keys bool No <code>False</code> Create a nonclustered index on merge_keys. Use if primary key already exists elsewhere. incremental bool No <code>False</code> Enable incremental merge optimization. When True, reads target table's keys and hashes to determine which rows changed, then only writes changed rows to staging. Significantly faster when few rows change between runs. hash_column Optional[str] No - Name of pre-computed hash column in DataFrame for change detection. Used when incremental=True. If not specified, will auto-detect '_hash_diff' column. change_detection_columns Optional[List[str]] No - Columns to use for computing change detection hash. Used when incremental=True and no hash_column is specified. If None, uses all non-key columns."},{"location":"reference/yaml_schema/#sqlservermergevalidationconfig","title":"<code>SqlServerMergeValidationConfig</code>","text":"<p>Used in: SqlServerMergeOptions, SqlServerOverwriteOptions</p> <p>Validation configuration for SQL Server merge/overwrite operations.</p> <p>Validates source data before writing to SQL Server.</p> <p>Example: <pre><code>merge_options:\n  validations:\n    check_null_keys: true\n    check_duplicate_keys: true\n    fail_on_validation_error: true\n</code></pre></p> Field Type Required Default Description check_null_keys bool No <code>True</code> Fail if merge_keys contain NULL values check_duplicate_keys bool No <code>True</code> Fail if merge_keys have duplicate combinations fail_on_validation_error bool No <code>True</code> If False, log warning instead of failing on validation errors"},{"location":"reference/yaml_schema/#sqlserveroverwriteoptions","title":"<code>SqlServerOverwriteOptions</code>","text":"<p>Used in: WriteConfig</p> <p>Options for SQL Server overwrite operations (Phase 2).</p> <p>Enhanced overwrite with multiple strategies for different use cases.</p>"},{"location":"reference/yaml_schema/#strategies","title":"Strategies","text":"<ul> <li><code>truncate_insert</code>: TRUNCATE TABLE then INSERT (fastest, requires TRUNCATE permission)</li> <li><code>drop_create</code>: DROP TABLE, CREATE TABLE, INSERT (refreshes schema)</li> <li><code>delete_insert</code>: DELETE FROM then INSERT (works with limited permissions)</li> </ul>"},{"location":"reference/yaml_schema/#example","title":"Example","text":"<pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: fact.combined_downtime\n  mode: overwrite\n  overwrite_options:\n    strategy: truncate_insert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre> Field Type Required Default Description strategy SqlServerOverwriteStrategy No <code>SqlServerOverwriteStrategy.TRUNCATE_INSERT</code> Overwrite strategy: truncate_insert, drop_create, delete_insert audit_cols Optional[SqlServerAuditColsConfig] No - Audit columns for created/updated timestamps validations Optional[SqlServerMergeValidationConfig] No - Validation checks before overwrite auto_create_schema bool No <code>False</code> Auto-create schema if it doesn't exist (Phase 4). Runs CREATE SCHEMA IF NOT EXISTS. auto_create_table bool No <code>False</code> Auto-create target table if it doesn't exist (Phase 4). Infers schema from DataFrame. schema_evolution Optional[SqlServerSchemaEvolutionConfig] No - Schema evolution configuration (Phase 4). Controls handling of schema differences. batch_size Optional[int] No - Batch size for writes (Phase 4). Chunks large DataFrames for memory efficiency."},{"location":"reference/yaml_schema/#sqlserverschemaevolutionconfig","title":"<code>SqlServerSchemaEvolutionConfig</code>","text":"<p>Used in: SqlServerMergeOptions, SqlServerOverwriteOptions</p> <p>Schema evolution configuration for SQL Server operations (Phase 4).</p> <p>Controls automatic schema changes when DataFrame schema differs from target table.</p> <p>Example: <pre><code>merge_options:\n  schema_evolution:\n    mode: evolve\n    add_columns: true\n</code></pre></p> Field Type Required Default Description mode SqlServerSchemaEvolutionMode No <code>SqlServerSchemaEvolutionMode.STRICT</code> Schema evolution mode: strict (fail), evolve (add columns), ignore (skip mismatched) add_columns bool No <code>False</code> If mode='evolve', automatically add new columns via ALTER TABLE ADD COLUMN"},{"location":"reference/yaml_schema/#transformstep","title":"<code>TransformStep</code>","text":"<p>Used in: TransformConfig</p> <p>Single transformation step.</p> <p>Supports four step types (exactly one required):</p> <ul> <li><code>sql</code> - Inline SQL query string</li> <li><code>sql_file</code> - Path to external .sql file (relative to the YAML file defining the node)</li> <li><code>function</code> - Registered Python function name</li> <li><code>operation</code> - Built-in operation (e.g., drop_duplicates)</li> </ul> <p>sql_file Example:</p> <p>If your project structure is: <pre><code>project.yaml              # imports pipelines/silver/silver.yaml\npipelines/\n  silver/\n    silver.yaml           # defines the node\n    sql/\n      transform.sql       # your SQL file\n</code></pre></p> <p>In <code>silver.yaml</code>, use a path relative to <code>silver.yaml</code>: <pre><code>transform:\n  steps:\n    - sql_file: sql/transform.sql   # relative to silver.yaml\n</code></pre></p> <p>Important: The path is resolved relative to the YAML file where the node is defined, NOT the project.yaml that imports it. Do NOT use absolute paths like <code>/pipelines/silver/sql/...</code>.</p> Field Type Required Default Description sql Optional[str] No - Inline SQL query. Use <code>df</code> to reference the current DataFrame. sql_file Optional[str] No - Path to external .sql file, relative to the YAML file defining the node. Example: 'sql/transform.sql' resolves relative to the node's source YAML. function Optional[str] No - Name of a registered Python function (@transform or @register). operation Optional[str] No - Built-in operation name (e.g., drop_duplicates, fill_na). params Dict[str, Any] No <code>PydanticUndefined</code> Parameters to pass to function or operation."},{"location":"reference/yaml_schema/#contracts-data-quality-gates","title":"Contracts (Data Quality Gates)","text":""},{"location":"reference/yaml_schema/#contracts-pre-transform-checks","title":"Contracts (Pre-Transform Checks)","text":"<p>Contracts are fail-fast data quality checks that run on input data before transformation. They always halt execution on failure - use them to prevent bad data from entering the pipeline.</p> <p>Contracts vs Validation vs Quality Gates:</p> Feature When it Runs On Failure Use Case Contracts Before transform Always fails Input data quality (not-null, unique keys) Validation After transform Configurable (fail/warn/quarantine) Output data quality (ranges, formats) Quality Gates After validation Configurable (abort/warn) Pipeline-level thresholds (pass rate, row counts) Quarantine With validation Routes bad rows Capture invalid records for review <p>See Also: - Validation Guide - Full validation configuration - Quarantine Guide - Quarantine setup and review - Getting Started: Validation</p> <p>Example: <pre><code>- name: \"process_orders\"\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read:\n    source: raw_orders\n</code></pre></p>"},{"location":"reference/yaml_schema/#acceptedvaluestest","title":"<code>AcceptedValuesTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures a column only contains values from an allowed list.</p> <p>When to Use: Enum-like fields, status columns, categorical data validation.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected]\n</code></pre> Field Type Required Default Description type Literal['accepted_values'] No <code>TestType.ACCEPTED_VALUES</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check values List[Any] Yes - Allowed values"},{"location":"reference/yaml_schema/#customsqltest","title":"<code>CustomSQLTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Runs a custom SQL condition and fails if too many rows violate it.</p> <pre><code>contracts:\n  - type: custom_sql\n    condition: \"amount &gt; 0\"\n    threshold: 0.01  # Allow up to 1% failures\n</code></pre> Field Type Required Default Description type Literal['custom_sql'] No <code>TestType.CUSTOM_SQL</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure condition str Yes - SQL condition that should be true for valid rows threshold float No <code>0.0</code> Failure rate threshold (0.0 = strictly no failures allowed)"},{"location":"reference/yaml_schema/#distributioncontract","title":"<code>DistributionContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if a column's statistical distribution is within expected bounds.</p> <p>When to Use: Detect data drift, anomaly detection, statistical monitoring.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \"&gt;100\"  # Mean must be &gt; 100\n    on_fail: warn\n</code></pre> Field Type Required Default Description type Literal['distribution'] No <code>TestType.DISTRIBUTION</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.WARN</code> - column str Yes - Column to analyze metric Literal['mean', 'min', 'max', 'null_percentage'] Yes - Statistical metric to check threshold str Yes - Threshold expression (e.g., '&gt;100', '&lt;0.05')"},{"location":"reference/yaml_schema/#freshnesscontract","title":"<code>FreshnessContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that data is not stale by checking a timestamp column.</p> <p>When to Use: Source systems that should update regularly, SLA monitoring.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n</code></pre> Field Type Required Default Description type Literal['freshness'] No <code>TestType.FRESHNESS</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> - column str No <code>updated_at</code> Timestamp column to check max_age str Yes - Maximum allowed age (e.g., '24h', '7d')"},{"location":"reference/yaml_schema/#notnulltest","title":"<code>NotNullTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns contain no NULL values.</p> <p>When to Use: Primary keys, required fields, foreign keys that must resolve.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n</code></pre> Field Type Required Default Description type Literal['not_null'] No <code>TestType.NOT_NULL</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure columns List[str] Yes - Columns that must not contain nulls"},{"location":"reference/yaml_schema/#rangetest","title":"<code>RangeTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values fall within a specified range.</p> <p>When to Use: Numeric bounds validation (ages, prices, quantities), date ranges.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n</code></pre> Field Type Required Default Description type Literal['range'] No <code>TestType.RANGE</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check min int | float | str No - Minimum value (inclusive) max int | float | str No - Maximum value (inclusive)"},{"location":"reference/yaml_schema/#regexmatchtest","title":"<code>RegexMatchTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values match a regex pattern.</p> <p>When to Use: Format validation (emails, phone numbers, IDs, codes).</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n</code></pre> Field Type Required Default Description type Literal['regex_match'] No <code>TestType.REGEX_MATCH</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure column str Yes - Column to check pattern str Yes - Regex pattern to match"},{"location":"reference/yaml_schema/#rowcounttest","title":"<code>RowCountTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that row count falls within expected bounds.</p> <p>When to Use: Ensure minimum data completeness, detect truncated loads, cap batch sizes.</p> <p>See Also: Contracts Overview, GateConfig</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000\n    max: 100000\n</code></pre> Field Type Required Default Description type Literal['row_count'] No <code>TestType.ROW_COUNT</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure min Optional[int] No - Minimum row count max Optional[int] No - Maximum row count"},{"location":"reference/yaml_schema/#schemacontract","title":"<code>SchemaContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that the DataFrame schema matches expected columns.</p> <p>When to Use: Enforce schema stability, detect upstream schema drift, ensure column presence.</p> <p>See Also: Contracts Overview, SchemaPolicyConfig</p> <p>Uses the <code>columns</code> metadata from NodeConfig to verify schema.</p> <pre><code>contracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n</code></pre> Field Type Required Default Description type Literal['schema'] No <code>TestType.SCHEMA</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> - strict bool No <code>True</code> If true, fail on unexpected columns"},{"location":"reference/yaml_schema/#uniquetest","title":"<code>UniqueTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns (or combination) contain unique values.</p> <p>When to Use: Primary keys, natural keys, deduplication verification.</p> <p>See Also: Contracts Overview</p> <pre><code>contracts:\n  - type: unique\n    columns: [order_id]  # Single column\n  # OR composite key:\n  - type: unique\n    columns: [customer_id, order_date]  # Composite uniqueness\n</code></pre> Field Type Required Default Description type Literal['unique'] No <code>TestType.UNIQUE</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure columns List[str] Yes - Columns that must be unique (composite key if multiple)"},{"location":"reference/yaml_schema/#volumedroptest","title":"<code>VolumeDropTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if row count dropped significantly compared to history.</p> <p>When to Use: Detect source outages, partial loads, or data pipeline issues.</p> <p>See Also: Contracts Overview, RowCountTest</p> <p>Formula: <code>(current - avg) / avg &lt; -threshold</code></p> <pre><code>contracts:\n  - type: volume_drop\n    threshold: 0.5  # Fail if &gt; 50% drop from 7-day average\n    lookback_days: 7\n</code></pre> Field Type Required Default Description type Literal['volume_drop'] No <code>TestType.VOLUME_DROP</code> - name Optional[str] No - Optional name for the check on_fail ContractSeverity No <code>ContractSeverity.FAIL</code> Action on failure threshold float No <code>0.5</code> Max allowed drop (0.5 = 50% drop) lookback_days int No <code>7</code> Days of history to average"},{"location":"reference/yaml_schema/#global-settings","title":"Global Settings","text":""},{"location":"reference/yaml_schema/#lineageconfig","title":"<code>LineageConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for OpenLineage integration.</p> <p>Example: <pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n</code></pre></p> Field Type Required Default Description url Optional[str] No - OpenLineage API URL namespace str No <code>odibi</code> Namespace for jobs api_key Optional[str] No - API Key"},{"location":"reference/yaml_schema/#alertconfig","title":"<code>AlertConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for alerts with throttling support.</p> <p>Supports Slack, Teams, and generic webhooks with event-specific payloads.</p> <p>Available Events: - <code>on_start</code> - Pipeline started - <code>on_success</code> - Pipeline completed successfully - <code>on_failure</code> - Pipeline failed - <code>on_quarantine</code> - Rows were quarantined - <code>on_gate_block</code> - Quality gate blocked the pipeline - <code>on_threshold_breach</code> - A threshold was exceeded</p> <p>Example: <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre></p> Field Type Required Default Description type AlertType Yes - - url str Yes - Webhook URL on_events List[AlertEvent] No <code>[&lt;AlertEvent.ON_FAILURE: 'on_failure'&gt;]</code> Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach metadata Dict[str, Any] No <code>PydanticUndefined</code> Extra metadata: throttle_minutes, max_per_hour, channel, etc."},{"location":"reference/yaml_schema/#loggingconfig","title":"<code>LoggingConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Logging configuration.</p> <p>Example: <pre><code>logging:\n  level: \"INFO\"\n  structured: true\n</code></pre></p> Field Type Required Default Description level LogLevel No <code>LogLevel.INFO</code> - structured bool No <code>False</code> Output JSON logs metadata Dict[str, Any] No <code>PydanticUndefined</code> Extra metadata in logs"},{"location":"reference/yaml_schema/#performanceconfig","title":"<code>PerformanceConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Performance tuning configuration.</p> <p>Example: <pre><code>performance:\n  use_arrow: true\n  spark_config:\n    \"spark.sql.shuffle.partitions\": \"200\"\n    \"spark.sql.adaptive.enabled\": \"true\"\n    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\"\n  delta_table_properties:\n    \"delta.columnMapping.mode\": \"name\"\n</code></pre></p> <p>Spark Config Notes: - Configs are applied via <code>spark.conf.set()</code> at runtime - For existing sessions (e.g., Databricks), only runtime-settable configs will take effect - Session-level configs (e.g., <code>spark.executor.memory</code>) require session restart - Common runtime-safe configs: shuffle partitions, adaptive query execution, Delta optimizations</p> Field Type Required Default Description use_arrow bool No <code>True</code> Use Apache Arrow-backed DataFrames (Pandas only). Reduces memory and speeds up I/O. spark_config Dict[str, str] No <code>PydanticUndefined</code> Spark configuration settings applied at runtime via spark.conf.set(). Example: {'spark.sql.shuffle.partitions': '200', 'spark.sql.adaptive.enabled': 'true'}. Note: Some configs require session restart and cannot be set at runtime. delta_table_properties Dict[str, str] No <code>PydanticUndefined</code> Default table properties applied to all Delta writes. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. skip_null_profiling bool No <code>False</code> Skip null profiling in metadata collection phase. Reduces execution time for large DataFrames by avoiding an additional Spark job. skip_catalog_writes bool No <code>False</code> Skip catalog metadata writes (register_asset, track_schema, log_pattern, record_lineage) after each node write. Significantly improves performance for high-throughput pipelines like Bronze layer ingestion. Set to true when catalog tracking is not needed. skip_run_logging bool No <code>False</code> Skip batch catalog writes at pipeline end (log_runs_batch, register_outputs_batch). Saves 10-20s per pipeline run. Enable when you don't need run history in the catalog. Stories are still generated and contain full execution details."},{"location":"reference/yaml_schema/#retryconfig","title":"<code>RetryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Retry configuration.</p> <p>Example: <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n</code></pre></p> Field Type Required Default Description enabled bool No <code>True</code> - max_attempts int No <code>3</code> - backoff BackoffStrategy No <code>BackoffStrategy.EXPONENTIAL</code> -"},{"location":"reference/yaml_schema/#storyconfig","title":"<code>StoryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Story generation configuration.</p> <p>Stories are ODIBI's core value - execution reports with lineage. They must use a connection for consistent, traceable output.</p> <p>Example: <pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre></p> <p>Failure Sample Settings: - <code>failure_sample_size</code>: Number of failed rows to capture per validation (default: 100) - <code>max_failure_samples</code>: Total failed rows across all validations (default: 500) - <code>max_sampled_validations</code>: After this many validations, show only counts (default: 5)</p> Field Type Required Default Description connection str Yes - Connection name for story output (uses connection's path resolution) path str Yes - Path for stories (relative to connection base_path) max_sample_rows int No <code>10</code> - auto_generate bool No <code>True</code> - retention_days Optional[int] No <code>30</code> Days to keep stories retention_count Optional[int] No <code>100</code> Max number of stories to keep failure_sample_size int No <code>100</code> Number of failed rows to capture per validation rule max_failure_samples int No <code>500</code> Maximum total failed rows across all validations max_sampled_validations int No <code>5</code> After this many validations, show only counts (no samples) async_generation bool No <code>False</code> Generate stories asynchronously (fire-and-forget). Pipeline returns immediately while story writes in background. Improves multi-pipeline performance by ~5-10s per pipeline. generate_lineage bool No <code>True</code> Generate combined lineage graph from all stories. Creates a unified view of data flow across pipelines. docs Optional[ForwardRef('DocsConfig')] No - Documentation generation settings. Generates README.md, TECHNICAL_DETAILS.md, NODE_CARDS/*.md from Story data."},{"location":"reference/yaml_schema/#transformation-reference","title":"Transformation Reference","text":""},{"location":"reference/yaml_schema/#how-to-use-transformers","title":"How to Use Transformers","text":"<p>You can use any transformer in two ways:</p> <p>1. As a Top-Level Transformer (\"The App\") Use this for major operations that define the node's purpose (e.g. Merge, SCD2). <pre><code>- name: \"my_node\"\n  transformer: \"&lt;transformer_name&gt;\"\n  params:\n    &lt;param_name&gt;: &lt;value&gt;\n</code></pre></p> <p>2. As a Step in a Chain (\"The Script\") Use this for smaller operations within a <code>transform</code> block (e.g. clean_text, filter). <pre><code>- name: \"my_node\"\n  transform:\n    steps:\n      - function: \"&lt;transformer_name&gt;\"\n         params:\n           &lt;param_name&gt;: &lt;value&gt;\n</code></pre></p> <p>Available Transformers: The models below describe the <code>params</code> required for each transformer.</p>"},{"location":"reference/yaml_schema/#common-operations","title":"\ud83d\udcc2 Common Operations","text":""},{"location":"reference/yaml_schema/#casewhencase","title":"CaseWhenCase","text":"<p>Back to Catalog</p> Field Type Required Default Description condition str Yes - - value str Yes - -"},{"location":"reference/yaml_schema/#addprefixparams","title":"<code>add_prefix</code> (AddPrefixParams)","text":"<p>Adds a prefix to column names.</p> <p>Configuration for adding a prefix to column names.</p> <p>Example - All columns: <pre><code>add_prefix:\n  prefix: \"src_\"\n</code></pre></p> <p>Example - Specific columns: <pre><code>add_prefix:\n  prefix: \"raw_\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description prefix str Yes - Prefix to add to column names columns Optional[List[str]] No - Columns to prefix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from prefixing"},{"location":"reference/yaml_schema/#addsuffixparams","title":"<code>add_suffix</code> (AddSuffixParams)","text":"<p>Adds a suffix to column names.</p> <p>Configuration for adding a suffix to column names.</p> <p>Example - All columns: <pre><code>add_suffix:\n  suffix: \"_raw\"\n</code></pre></p> <p>Example - Specific columns: <pre><code>add_suffix:\n  suffix: \"_v2\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description suffix str Yes - Suffix to add to column names columns Optional[List[str]] No - Columns to suffix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from suffixing"},{"location":"reference/yaml_schema/#casewhenparams","title":"<code>case_when</code> (CaseWhenParams)","text":"<p>Implements structured CASE WHEN logic.</p> <p>Configuration for conditional logic.</p> <p>Example: <pre><code>case_when:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description cases List[CaseWhenCase] Yes - List of conditional branches default str No <code>NULL</code> Default value if no condition met output_col str Yes - Name of the resulting column"},{"location":"reference/yaml_schema/#castcolumnsparams","title":"<code>cast_columns</code> (CastColumnsParams)","text":"<p>Casts specific columns to new types while keeping others intact.</p> <p>Configuration for column type casting.</p> <p>Example: <pre><code>cast_columns:\n  casts:\n    age: \"int\"\n    salary: \"DOUBLE\"\n    created_at: \"TIMESTAMP\"\n    tags: \"ARRAY&lt;STRING&gt;\"  # Raw SQL types allowed\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description casts Dict[str, SimpleType | str] Yes - Map of column to target SQL type"},{"location":"reference/yaml_schema/#cleantextparams","title":"<code>clean_text</code> (CleanTextParams)","text":"<p>Applies string cleaning operations (Trim/Case) via SQL.</p> <p>Configuration for text cleaning.</p> <p>Example: <pre><code>clean_text:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to clean trim bool No <code>True</code> Apply TRIM() case Literal['lower', 'upper', 'preserve'] No <code>preserve</code> Case conversion"},{"location":"reference/yaml_schema/#coalescecolumnsparams","title":"<code>coalesce_columns</code> (CoalesceColumnsParams)","text":"<p>Returns the first non-null value from a list of columns. Useful for fallback/priority scenarios.</p> <p>Configuration for coalescing columns (first non-null value).</p> <p>Example - Phone number fallback: <pre><code>coalesce_columns:\n  columns: [\"mobile_phone\", \"work_phone\", \"home_phone\"]\n  output_col: \"primary_phone\"\n</code></pre></p> <p>Example - Timestamp fallback: <pre><code>coalesce_columns:\n  columns: [\"updated_at\", \"modified_at\", \"created_at\"]\n  output_col: \"last_change_at\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to coalesce (in priority order) output_col str Yes - Name of the output column drop_source bool No <code>False</code> Drop the source columns after coalescing"},{"location":"reference/yaml_schema/#concatcolumnsparams","title":"<code>concat_columns</code> (ConcatColumnsParams)","text":"<p>Concatenates multiple columns into one string. NULLs are skipped (treated as empty string) using CONCAT_WS behavior.</p> <p>Configuration for string concatenation.</p> <p>Example: <pre><code>concat_columns:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to concatenate separator str No - Separator string output_col str Yes - Resulting column name"},{"location":"reference/yaml_schema/#converttimezoneparams","title":"<code>convert_timezone</code> (ConvertTimezoneParams)","text":"<p>Converts a timestamp from one timezone to another. Assumes the input column is a naive timestamp representing time in source_tz, or a timestamp with timezone.</p> <p>Configuration for timezone conversion.</p> <p>Example: <pre><code>convert_timezone:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - Timestamp column to convert source_tz str No <code>UTC</code> Source timezone (e.g., 'UTC', 'America/New_York') target_tz str Yes - Target timezone (e.g., 'America/Los_Angeles') output_col Optional[str] No - Name of the result column (default: {col}_{target_tz})"},{"location":"reference/yaml_schema/#dateaddparams","title":"<code>date_add</code> (DateAddParams)","text":"<p>Adds an interval to a date/timestamp column.</p> <p>Configuration for date addition.</p> <p>Example: <pre><code>date_add:\n  col: \"created_at\"\n  value: 1\n  unit: \"day\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - - value int Yes - - unit Literal['day', 'month', 'year', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema/#datediffparams","title":"<code>date_diff</code> (DateDiffParams)","text":"<p>Calculates difference between two dates/timestamps. Returns the elapsed time in the specified unit (as float for sub-day units).</p> <p>Configuration for date difference.</p> <p>Example: <pre><code>date_diff:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description start_col str Yes - - end_col str Yes - - unit Literal['day', 'hour', 'minute', 'second'] No <code>day</code> -"},{"location":"reference/yaml_schema/#datetruncparams","title":"<code>date_trunc</code> (DateTruncParams)","text":"<p>Truncates a date/timestamp to the specified precision.</p> <p>Configuration for date truncation.</p> <p>Example: <pre><code>date_trunc:\n  col: \"created_at\"\n  unit: \"month\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - - unit Literal['year', 'month', 'day', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema/#derivecolumnsparams","title":"<code>derive_columns</code> (DeriveColumnsParams)","text":"<p>Appends new columns based on SQL expressions.</p> <p>Design: - Uses projection to add fields. - Keeps all existing columns via <code>*</code>.</p> <p>Configuration for derived columns.</p> <p>Example: <pre><code>derive_columns:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre></p> <p>Note: Engine will fail if expressions reference non-existent columns.</p> <p>Back to Catalog</p> Field Type Required Default Description derivations Dict[str, str] Yes - Map of column name to SQL expression"},{"location":"reference/yaml_schema/#distinctparams","title":"<code>distinct</code> (DistinctParams)","text":"<p>Return unique rows from the dataset using SQL DISTINCT.</p>"},{"location":"reference/yaml_schema/#parameters","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to deduplicate. params : DistinctParams     Parameters specifying which columns to consider for uniqueness. If None, all columns are used.</p>"},{"location":"reference/yaml_schema/#returns","title":"Returns","text":"<p>EngineContext     The updated engine context with duplicate rows removed.</p> <p>Configuration for distinct rows.</p> <p>Example: <pre><code>distinct:\n  columns: [\"category\", \"status\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to project (if None, keeps all columns unique)"},{"location":"reference/yaml_schema/#dropcolumnsparams","title":"<code>drop_columns</code> (DropColumnsParams)","text":"<p>Removes the specified columns from the DataFrame.</p> <p>Configuration for dropping specific columns (blacklist).</p> <p>Example: <pre><code>drop_columns:\n  columns: [\"_internal_id\", \"_temp_flag\", \"_processing_date\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to drop"},{"location":"reference/yaml_schema/#extractdateparams","title":"<code>extract_date_parts</code> (ExtractDateParams)","text":"<p>Extracts date parts using ANSI SQL extract/functions.</p> <p>Configuration for extracting date parts.</p> <p>Example: <pre><code>extract_date_parts:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description source_col str Yes - - prefix Optional[str] No - - parts Literal[typing.Literal['year', 'month', 'day', 'hour']] No <code>['year', 'month', 'day']</code> -"},{"location":"reference/yaml_schema/#fillnullsparams","title":"<code>fill_nulls</code> (FillNullsParams)","text":"<p>Replaces null values with specified defaults using COALESCE.</p> <p>Configuration for filling null values.</p> <p>Example: <pre><code>fill_nulls:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description values Dict[str, str | int | float | bool] Yes - Map of column to fill value"},{"location":"reference/yaml_schema/#filterrowsparams","title":"<code>filter_rows</code> (FilterRowsParams)","text":"<p>Filters rows using a standard SQL WHERE clause.</p> <p>Design: - SQL-First: Pushes filtering to the engine's optimizer. - Zero-Copy: No data movement to Python.</p> <p>Configuration for filtering rows.</p> <p>Example: <pre><code>filter_rows:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre></p> <p>Example (Null Check): <pre><code>filter_rows:\n  condition: \"email IS NOT NULL AND email != ''\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description condition str Yes - SQL WHERE clause (e.g., 'age &gt; 18 AND status = \"active\"')"},{"location":"reference/yaml_schema/#limitparams","title":"<code>limit</code> (LimitParams)","text":"<p>Limit the number of rows returned from the dataset.</p>"},{"location":"reference/yaml_schema/#parameters_1","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to limit. params : LimitParams     Parameters specifying the number of rows to return and the offset.</p>"},{"location":"reference/yaml_schema/#returns_1","title":"Returns","text":"<p>EngineContext     The updated engine context with the limited DataFrame.</p> <p>Configuration for result limiting.</p> <p>Example: <pre><code>limit:\n  n: 100\n  offset: 0\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description n int Yes - Number of rows to return offset int No <code>0</code> Number of rows to skip"},{"location":"reference/yaml_schema/#normalizecolumnnamesparams","title":"<code>normalize_column_names</code> (NormalizeColumnNamesParams)","text":"<p>Normalizes column names to a consistent style. Useful for cleaning up messy source data with spaces, mixed case, or special characters.</p> <p>Configuration for normalizing column names.</p> <p>Example: <pre><code>normalize_column_names:\n  style: \"snake_case\"\n  lowercase: true\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description style Literal['snake_case', 'none'] No <code>snake_case</code> Naming style: 'snake_case' converts spaces/special chars to underscores lowercase bool No <code>True</code> Convert names to lowercase remove_special bool No <code>True</code> Remove special characters except underscores"},{"location":"reference/yaml_schema/#normalizeschemaparams","title":"<code>normalize_schema</code> (NormalizeSchemaParams)","text":"<p>Structural transformation to rename, drop, and reorder columns.</p> <p>Note: This is one of the few that might behave better with native API in some cases, but SQL projection handles it perfectly and is consistent.</p> <p>Configuration for schema normalization.</p> <p>Example: <pre><code>normalize_schema:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description rename Optional[Dict[str, str]] No <code>PydanticUndefined</code> old_name -&gt; new_name drop Optional[List[str]] No <code>PydanticUndefined</code> Columns to remove; ignored if not present select_order Optional[List[str]] No - Final column order; any missing columns appended after"},{"location":"reference/yaml_schema/#renamecolumnsparams","title":"<code>rename_columns</code> (RenameColumnsParams)","text":"<p>Renames columns according to the provided mapping. Columns not in the mapping are kept unchanged.</p> <p>Configuration for bulk column renaming.</p> <p>Example: <pre><code>rename_columns:\n  mapping:\n    customer_id: cust_id\n    order_date: date\n    total_amount: amount\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description mapping Dict[str, str] Yes - Map of old column name to new column name"},{"location":"reference/yaml_schema/#replacevaluesparams","title":"<code>replace_values</code> (ReplaceValuesParams)","text":"<p>Replaces values in specified columns according to the mapping. Supports replacing to NULL.</p> <p>Configuration for bulk value replacement.</p> <p>Example - Standardize nulls: <pre><code>replace_values:\n  columns: [\"status\", \"category\"]\n  mapping:\n    \"N/A\": null\n    \"\": null\n    \"Unknown\": null\n</code></pre></p> <p>Example - Code replacement: <pre><code>replace_values:\n  columns: [\"country_code\"]\n  mapping:\n    \"US\": \"USA\"\n    \"UK\": \"GBR\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to apply replacements to mapping Dict[str, Optional[str]] Yes - Map of old value to new value (use null for NULL)"},{"location":"reference/yaml_schema/#sampleparams","title":"<code>sample</code> (SampleParams)","text":"<p>Return a random sample of rows from the dataset.</p>"},{"location":"reference/yaml_schema/#parameters_2","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to sample from. params : SampleParams     Parameters specifying the fraction of rows to return and the random seed.</p>"},{"location":"reference/yaml_schema/#returns_2","title":"Returns","text":"<p>EngineContext     The updated engine context with the sampled DataFrame.</p> <p>Configuration for random sampling.</p> <p>Example: <pre><code>sample:\n  fraction: 0.1\n  seed: 42\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description fraction float Yes - Fraction of rows to return (0.0 to 1.0) seed Optional[int] No - -"},{"location":"reference/yaml_schema/#selectcolumnsparams","title":"<code>select_columns</code> (SelectColumnsParams)","text":"<p>Keeps only the specified columns, dropping all others.</p> <p>Configuration for selecting specific columns (whitelist).</p> <p>Example: <pre><code>select_columns:\n  columns: [\"id\", \"name\", \"created_at\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to keep"},{"location":"reference/yaml_schema/#sortparams","title":"<code>sort</code> (SortParams)","text":"<p>Sort the dataset by one or more columns.</p>"},{"location":"reference/yaml_schema/#parameters_3","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to sort. params : SortParams     Parameters specifying columns to sort by and sort order.</p>"},{"location":"reference/yaml_schema/#returns_3","title":"Returns","text":"<p>EngineContext     The updated engine context with the sorted DataFrame.</p> <p>Configuration for sorting.</p> <p>Example: <pre><code>sort:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description by str | List[str] Yes - Column(s) to sort by ascending bool No <code>True</code> Sort order"},{"location":"reference/yaml_schema/#splitpartparams","title":"<code>split_part</code> (SplitPartParams)","text":"<p>Extracts the Nth part of a string after splitting by a delimiter.</p> <p>Configuration for splitting strings.</p> <p>Example: <pre><code>split_part:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description col str Yes - Column to split delimiter str Yes - Delimiter to split by index int Yes - 1-based index of the token to extract"},{"location":"reference/yaml_schema/#trimwhitespaceparams","title":"<code>trim_whitespace</code> (TrimWhitespaceParams)","text":"<p>Trims leading and trailing whitespace from string columns.</p> <p>Configuration for trimming whitespace from string columns.</p> <p>Example - All string columns: <pre><code>trim_whitespace: {}\n</code></pre></p> <p>Example - Specific columns: <pre><code>trim_whitespace:\n  columns: [\"name\", \"address\", \"city\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to trim (default: all string columns detected at runtime)"},{"location":"reference/yaml_schema/#relational-algebra","title":"\ud83d\udcc2 Relational Algebra","text":""},{"location":"reference/yaml_schema/#aggregateparams","title":"<code>aggregate</code> (AggregateParams)","text":"<p>Performs grouping and aggregation via SQL.</p> <p>Configuration for aggregation.</p> <p>Example: <pre><code>aggregate:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - Columns to group by aggregations Dict[str, AggFunc] Yes - Map of column to aggregation function (sum, avg, min, max, count)"},{"location":"reference/yaml_schema/#joinparams","title":"<code>join</code> (JoinParams)","text":"<p>Joins the current dataset with another dataset from the context.</p> <p>Configuration for joining datasets.</p> <p>Scenario 1: Simple Left Join <pre><code>join:\n  right_dataset: \"customers\"\n  on: \"customer_id\"\n  how: \"left\"\n</code></pre></p> <p>Scenario 2: Join with Prefix (avoid collisions) <pre><code>join:\n  right_dataset: \"orders\"\n  on: [\"user_id\"]\n  how: \"inner\"\n  prefix: \"ord\"  # Result cols: ord_date, ord_amount...\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description right_dataset str Yes - Name of the node/dataset to join with on str | List[str] Yes - Column(s) to join on how Literal['inner', 'left', 'right', 'full', 'cross', 'anti', 'semi'] No <code>left</code> Join type prefix Optional[str] No - Prefix for columns from right dataset to avoid collisions"},{"location":"reference/yaml_schema/#pivotparams","title":"<code>pivot</code> (PivotParams)","text":"<p>Pivots row values into columns.</p> <p>Configuration for pivoting data.</p> <p>Example: <pre><code>pivot:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n</code></pre></p> <p>Example (Optimized for Spark): <pre><code>pivot:\n  group_by: [\"id\"]\n  pivot_col: \"category\"\n  values: [\"A\", \"B\", \"C\"]  # Explicit values avoid extra pass\n  agg_col: \"amount\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - - pivot_col str Yes - - agg_col str Yes - - agg_func Literal['sum', 'count', 'avg', 'max', 'min', 'first'] No <code>sum</code> - values Optional[List[str]] No - Specific values to pivot (for Spark optimization)"},{"location":"reference/yaml_schema/#unionparams","title":"<code>union</code> (UnionParams)","text":"<p>Unions current dataset with others.</p> <p>Configuration for unioning datasets.</p> <p>Example (By Name - Default): <pre><code>union:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true\n</code></pre></p> <p>Example (By Position): <pre><code>union:\n  datasets: [\"legacy_data\"]\n  by_name: false\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description datasets List[str] Yes - List of node names to union with current by_name bool No <code>True</code> Match columns by name (UNION ALL BY NAME)"},{"location":"reference/yaml_schema/#unpivotparams","title":"<code>unpivot</code> (UnpivotParams)","text":"<p>Unpivots columns into rows (Melt/Stack).</p> <p>Configuration for unpivoting (melting) data.</p> <p>Example: <pre><code>unpivot:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description id_cols List[str] Yes - - value_vars List[str] Yes - - var_name str No <code>variable</code> - value_name str No <code>value</code> -"},{"location":"reference/yaml_schema/#data-quality","title":"\ud83d\udcc2 Data Quality","text":""},{"location":"reference/yaml_schema/#crosscheckparams","title":"<code>cross_check</code> (CrossCheckParams)","text":"<p>Perform cross-node validation checks.</p> <p>Does not return a DataFrame (returns None). Raises ValidationError on failure.</p> <p>Configuration for cross-node validation checks.</p> <p>Example (Row Count Mismatch): <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre></p> <p>Example (Schema Match): <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"schema_match\"\n  inputs: [\"staging_orders\", \"prod_orders\"]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description type str Yes - Check type: 'row_count_diff', 'schema_match' inputs List[str] Yes - List of node names to compare threshold float No <code>0.0</code> Threshold for diff (0.0-1.0)"},{"location":"reference/yaml_schema/#warehousing-patterns","title":"\ud83d\udcc2 Warehousing Patterns","text":""},{"location":"reference/yaml_schema/#auditcolumnsconfig","title":"AuditColumnsConfig","text":"<p>Back to Catalog</p> Field Type Required Default Description created_col Optional[str] No - Column to set only on first insert updated_col Optional[str] No - Column to update on every merge"},{"location":"reference/yaml_schema/#mergeparams","title":"<code>merge</code> (MergeParams)","text":"<p>Merge transformer implementation. Handles Upsert, Append-Only, and Delete-Match strategies.</p> <p>Args:     context: EngineContext (preferred) or legacy PandasContext/SparkContext     params: MergeParams object (when called via function step) or DataFrame (legacy)     current: DataFrame (legacy positional arg, deprecated)     **kwargs: Parameters when not using MergeParams</p> <p>Configuration for Merge transformer (Upsert/Append).</p>"},{"location":"reference/yaml_schema/#gdpr-compliance-guide","title":"\u2696\ufe0f \"GDPR &amp; Compliance\" Guide","text":"<p>Business Problem: \"A user exercised their 'Right to be Forgotten'. We need to remove them from our Silver tables immediately.\"</p> <p>The Solution: Use the <code>delete_match</code> strategy. The source dataframe contains the IDs to be deleted, and the transformer removes them from the target.</p> <p>Recipe 1: Right to be Forgotten (Delete) <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"delete_match\"\n</code></pre></p> <p>Recipe 2: Conditional Update (SCD Type 1) \"Only update if the source record is newer than the target record.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.products\"\n  keys: [\"product_id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n</code></pre></p> <p>Recipe 3: Safe Insert (Filter Bad Records) \"Only insert records that are not marked as deleted.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.orders\"\n  keys: [\"order_id\"]\n  strategy: \"append_only\"\n  insert_condition: \"source.is_deleted = false\"\n</code></pre></p> <p>Recipe 4: Audit Columns \"Track when records were created or updated.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.users\"\n  keys: [\"user_id\"]\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre></p> <p>Recipe 5: Full Sync (Insert + Update + Delete) \"Sync target with source: insert new, update changed, and remove soft-deleted.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  # 1. Delete if source says so\n  delete_condition: \"source.is_deleted = true\"\n  # 2. Update if changed (and not deleted)\n  update_condition: \"source.hash != target.hash\"\n  # 3. Insert new (and not deleted)\n  insert_condition: \"source.is_deleted = false\"\n</code></pre></p> <p>Recipe 6: Connection-based Path Resolution (ADLS) \"Use a connection to resolve paths, just like write config.\" <pre><code>transform:\n  steps:\n    - function: merge\n      params:\n        connection: goat_prod\n        path: OEE/silver/customers\n        register_table: silver.customers\n        keys: [\"customer_id\"]\n        strategy: \"upsert\"\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre></p> <p>Strategies: *   upsert (Default): Update existing records, insert new ones. *   append_only: Ignore duplicates, only insert new keys. *   delete_match: Delete records in target that match keys in source.</p> <p>Back to Catalog</p> Field Type Required Default Description target Optional[str] No - Target table name or full path (use this OR connection+path) connection Optional[str] No - Connection name to resolve path (use with 'path' param) path Optional[str] No - Relative path within connection (e.g., 'OEE/silver/customers') register_table Optional[str] No - Register as Unity Catalog/metastore table after merge (e.g., 'silver.customers') keys List[str] Yes - List of join keys strategy MergeStrategy No <code>MergeStrategy.UPSERT</code> Merge behavior: 'upsert', 'append_only', 'delete_match' audit_cols Optional[AuditColumnsConfig] No - {'created_col': '...', 'updated_col': '...'} optimize_write bool No <code>False</code> Run OPTIMIZE after write (Spark) zorder_by Optional[List[str]] No - Columns to Z-Order by cluster_by Optional[List[str]] No - Columns to Liquid Cluster by (Delta) update_condition Optional[str] No - SQL condition for update clause (e.g. 'source.ver &gt; target.ver') insert_condition Optional[str] No - SQL condition for insert clause (e.g. 'source.status != \"deleted\"') delete_condition Optional[str] No - SQL condition for delete clause (e.g. 'source.status = \"deleted\"') table_properties Optional[dict] No - Delta table properties for initial table creation (e.g., column mapping)"},{"location":"reference/yaml_schema/#scd2params","title":"<code>scd2</code> (SCD2Params)","text":"<p>Implements SCD Type 2 Logic.</p> <p>Returns the FULL history dataset (to be written via Overwrite).</p> <p>Parameters for SCD Type 2 (Slowly Changing Dimensions) transformer.</p>"},{"location":"reference/yaml_schema/#the-time-machine-pattern","title":"\ud83d\udd70\ufe0f The \"Time Machine\" Pattern","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: SCD Type 2 tracks the full history of changes. Each record has an \"effective window\" (start/end dates) and a flag indicating if it is the current version.</p> <p>Recipe 1: Using table name <pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"   # Registered table name\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre></p> <p>Recipe 2: Using connection + path (ADLS) <pre><code>transformer: \"scd2\"\nparams:\n  connection: adls_prod            # Connection name\n  path: OEE/silver/dim_customers   # Relative path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre></p> <p>How it works: 1. Match: Finds existing records using <code>keys</code>. 2. Compare: Checks <code>track_cols</code> to see if data changed. 3. Close: If changed, updates the old record's <code>end_time_col</code> to the new <code>effective_time_col</code>. 4. Insert: Adds a new record with <code>effective_time_col</code> as start and open-ended end date.</p> <p>Note: SCD2 returns a DataFrame containing the full history. You must use a <code>write:</code> block to persist the result (typically with <code>mode: overwrite</code> to the same location as <code>target</code>).</p> <p>Back to Catalog</p> Field Type Required Default Description target Optional[str] No - Target table name or full path (use this OR connection+path) connection Optional[str] No - Connection name to resolve path (use with 'path' param) path Optional[str] No - Relative path within connection (e.g., 'OEE/silver/dim_customers') keys List[str] Yes - Natural keys to identify unique entities track_cols List[str] Yes - Columns to monitor for changes effective_time_col str Yes - Source column indicating when the change occurred. end_time_col str No <code>valid_to</code> Name of the end timestamp column current_flag_col str No <code>is_current</code> Name of the current record flag column delete_col Optional[str] No - Column indicating soft deletion (boolean)"},{"location":"reference/yaml_schema/#manufacturing-iot","title":"\ud83d\udcc2 Manufacturing &amp; IoT","text":""},{"location":"reference/yaml_schema/#phaseconfig","title":"PhaseConfig","text":"<p>Configuration for a single phase.</p> <p>Back to Catalog</p> Field Type Required Default Description timer_col str Yes - Timer column name for this phase start_threshold Optional[int] No - Override default start threshold for this phase (seconds)"},{"location":"reference/yaml_schema/#detectsequentialphasesparams","title":"<code>detect_sequential_phases</code> (DetectSequentialPhasesParams)","text":"<p>Detect and analyze sequential manufacturing phases.</p> <p>For each group (e.g., batch), this transformer: 1. Processes phases sequentially (each starts after previous ends) 2. Detects phase start by finding first valid timer reading and back-calculating 3. Detects phase end by finding first repeated (plateaued) timer value 4. Calculates time spent in each status during each phase 5. Aggregates specified metrics within each phase window 6. Outputs one summary row per group</p> <p>Output columns per phase: - {phase}start: Phase start timestamp - {phase}_end: Phase end timestamp - {phase}_max_minutes: Maximum timer value converted to minutes - {phase}minutes: Time in each status (if status_col provided) - {phase}: Aggregated metrics (if phase_metrics provided)</p> <p>Detect and analyze sequential manufacturing phases from timer columns.</p> <p>This transformer processes raw sensor/PLC data where timer columns increment during each phase. It detects phase boundaries, calculates durations, and tracks time spent in each equipment status.</p> <p>Common use cases: - Batch reactor cycle analysis - CIP (Clean-in-Place) phase timing - Food processing (cook, cool, package cycles) - Any multi-step batch process with PLC timers</p> <p>Scenario: Analyze FBR cycle times <pre><code>detect_sequential_phases:\n  group_by: BatchID\n  timestamp_col: ts\n  phases:\n    - timer_col: LoadTime\n    - timer_col: AcidTime\n    - timer_col: DryTime\n    - timer_col: CookTime\n    - timer_col: CoolTime\n    - timer_col: UnloadTime\n  start_threshold: 240\n  status_col: Status\n  status_mapping:\n    1: idle\n    2: active\n    3: hold\n    4: faulted\n  phase_metrics:\n    Level: max\n  metadata:\n    ProductCode: first_after_start\n    Weight: max\n</code></pre></p> <p>Scenario: Group by multiple columns <pre><code>detect_sequential_phases:\n  group_by:\n    - BatchID\n    - AssetID\n  phases: [LoadTime, CookTime]\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description group_by str | List[str] Yes - Column(s) to group by. Can be a single column name or list of columns. E.g., 'BatchID' or ['BatchID', 'AssetID'] timestamp_col str No <code>ts</code> Timestamp column for ordering events phases List[str | PhaseConfig] Yes - List of phase timer columns (strings) or PhaseConfig objects. Phases are processed sequentially - each phase starts after the previous ends. start_threshold int No <code>240</code> Default max timer value (seconds) to consider as valid phase start. Filters out late readings where timer already shows large elapsed time. status_col Optional[str] No - Column containing equipment status codes status_mapping Optional[Dict[int, str]] No - Mapping of status codes to names. E.g., phase_metrics Optional[Dict[str, str]] No - Columns to aggregate within each phase window. E.g., {Level: max, Pressure: max}. Outputs {Phase}_{Column} columns. metadata Optional[Dict[str, str]] No - Columns to include in output with aggregation method. Options: 'first', 'last', 'first_after_start', 'max', 'min', 'mean', 'sum'. E.g., output_time_format str No <code>%Y-%m-%d %H:%M:%S</code> Format for output timestamp columns fill_null_minutes bool No <code>False</code> If True, fill null numeric columns (_max_minutes, _status_minutes, _metrics) with 0. Timestamp columns remain null for skipped phases. spark_native bool No <code>False</code> If True, use native Spark window functions. If False (default), use applyInPandas which is often faster for datasets with many batches."},{"location":"reference/yaml_schema/#advanced-feature-engineering","title":"\ud83d\udcc2 Advanced &amp; Feature Engineering","text":""},{"location":"reference/yaml_schema/#shiftdefinition","title":"ShiftDefinition","text":"<p>Definition of a single shift.</p> <p>Back to Catalog</p> Field Type Required Default Description name str Yes - Name of the shift (e.g., 'Day', 'Night') start str Yes - Start time in HH:MM format (e.g., '06:00') end str Yes - End time in HH:MM format (e.g., '14:00')"},{"location":"reference/yaml_schema/#deduplicateparams","title":"<code>deduplicate</code> (DeduplicateParams)","text":"<p>Deduplicates data using Window functions.</p> <p>Configuration for deduplication.</p> <p>Scenario: Keep latest record <pre><code>deduplicate:\n  keys: [\"id\"]\n  order_by: \"updated_at DESC\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description keys List[str] Yes - List of columns to partition by (columns that define uniqueness) order_by Optional[str] No - SQL Order by clause (e.g. 'updated_at DESC') to determine which record to keep (first one is kept)"},{"location":"reference/yaml_schema/#dictmappingparams","title":"<code>dict_based_mapping</code> (DictMappingParams)","text":"<p>Maps values in a column using a provided dictionary.</p> <p>For each value in the specified column, replaces it with the mapped value. If 'default' is provided, uses it for values not found in the mapping. Supports Spark and Pandas engines.</p> <p>Configuration for dictionary mapping.</p> <p>Scenario: Map status codes to labels <pre><code>dict_based_mapping:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column to map values from mapping Dict[str, str | int | float | bool] Yes - Dictionary of source value -&gt; target value default str | int | float | bool No - Default value if source value is not found in mapping output_column Optional[str] No - Name of output column. If not provided, overwrites source column."},{"location":"reference/yaml_schema/#explodeparams","title":"<code>explode_list_column</code> (ExplodeParams)","text":"<p>Explodes a list/array column into multiple rows.</p> <p>For each element in the specified list column, creates a new row. If 'outer' is True, keeps rows with empty lists (like explode_outer). Supports Spark and Pandas engines.</p> <p>Configuration for exploding lists.</p> <p>Scenario: Flatten list of items per order <pre><code>explode_list_column:\n  column: \"items\"\n  outer: true  # Keep orders with empty items list\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing the list/array to explode outer bool No <code>False</code> If True, keep rows with empty lists (explode_outer behavior). If False, drops them."},{"location":"reference/yaml_schema/#numerickeyparams","title":"<code>generate_numeric_key</code> (NumericKeyParams)","text":"<p>Generates a deterministic BIGINT surrogate key from a hash of columns.</p> <p>This is useful when: - Unioning data from multiple sources - Some sources have IDs, some don't - You need stable numeric IDs for gold layer</p> <p>The key is generated by: 1. Concatenating columns with separator 2. Computing MD5 hash 3. Converting first 15 hex chars to BIGINT</p> <p>If coalesce_with is specified, keeps the existing value when not null. If output_col == coalesce_with, the original column is replaced.</p> <p>Configuration for numeric surrogate key generation.</p> <p>Generates a deterministic BIGINT key from a hash of specified columns. Useful when unioning data from multiple sources where some have IDs and others don't.</p> <p>Example: <pre><code>- function: generate_numeric_key\n  params:\n    columns: [DateID, store_id, reason_id, duration_min, notes]\n    output_col: ID\n    coalesce_with: ID  # Keep existing ID if not null\n</code></pre></p> <p>The generated key is: - Deterministic: same input data = same ID every time - BIGINT: large numeric space to avoid collisions - Stable: safe for gold layer / incremental loads</p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to combine for the key separator str No <code>|</code> Separator between values output_col str No <code>numeric_key</code> Name of the output column coalesce_with Optional[str] No - Existing column to coalesce with (keep existing value if not null)"},{"location":"reference/yaml_schema/#surrogatekeyparams","title":"<code>generate_surrogate_key</code> (SurrogateKeyParams)","text":"<p>Generates a deterministic surrogate key (MD5) from a combination of columns. Handles NULLs by treating them as empty strings to ensure consistency.</p> <p>Configuration for surrogate key generation.</p> <p>Example: <pre><code>generate_surrogate_key:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to combine for the key separator str No <code>-</code> Separator between values output_col str No <code>surrogate_key</code> Name of the output column"},{"location":"reference/yaml_schema/#geocodeparams","title":"<code>geocode</code> (GeocodeParams)","text":"<p>Geocoding Stub.</p> <p>Configuration for geocoding.</p> <p>Example: <pre><code>geocode:\n  address_col: \"full_address\"\n  output_col: \"lat_long\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description address_col str Yes - Column containing the address to geocode output_col str No <code>lat_long</code> Name of the output column for coordinates"},{"location":"reference/yaml_schema/#hashparams","title":"<code>hash_columns</code> (HashParams)","text":"<p>Hashes columns for PII/Anonymization.</p> <p>Configuration for column hashing.</p> <p>Example: <pre><code>hash_columns:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to hash algorithm HashAlgorithm No <code>HashAlgorithm.SHA256</code> Hashing algorithm. Options: 'sha256', 'md5'"},{"location":"reference/yaml_schema/#normalizejsonparams","title":"<code>normalize_json</code> (NormalizeJsonParams)","text":"<p>Flattens a nested JSON/Struct column.</p> <p>Configuration for JSON normalization.</p> <p>Example: <pre><code>normalize_json:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing nested JSON/Struct sep str No <code>_</code> Separator for nested fields (e.g., 'parent_child')"},{"location":"reference/yaml_schema/#parsejsonparams","title":"<code>parse_json</code> (ParseJsonParams)","text":"<p>Parses a JSON string column into a Struct/Map column.</p> <p>Configuration for JSON parsing.</p> <p>Example: <pre><code>parse_json:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - String column containing JSON json_schema str Yes - DDL schema string (e.g. 'a INT, b STRING') or Spark StructType DDL output_col Optional[str] No - -"},{"location":"reference/yaml_schema/#regexreplaceparams","title":"<code>regex_replace</code> (RegexReplaceParams)","text":"<p>Applies a regex replacement to a column.</p> <p>Uses SQL-based REGEXP_REPLACE to replace all matches of the pattern in the specified column with the given replacement string. Works on both Spark and DuckDB/Pandas engines.</p> <p>Configuration for regex replacement.</p> <p>Example: <pre><code>regex_replace:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Column to apply regex replacement on pattern str Yes - Regex pattern to match replacement str Yes - String to replace matches with"},{"location":"reference/yaml_schema/#sessionizeparams","title":"<code>sessionize</code> (SessionizeParams)","text":"<p>Assigns session IDs based on inactivity threshold.</p> <p>Configuration for sessionization.</p> <p>Example: <pre><code>sessionize:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description timestamp_col str Yes - Timestamp column to calculate session duration from user_col str Yes - User identifier to partition sessions by threshold_seconds int No <code>1800</code> Inactivity threshold in seconds (default: 30 minutes). If gap &gt; threshold, new session starts. session_col str No <code>session_id</code> Output column name for the generated session ID"},{"location":"reference/yaml_schema/#spliteventsbyperiodparams","title":"<code>split_events_by_period</code> (SplitEventsByPeriodParams)","text":"<p>Splits events that span multiple time periods into individual segments.</p> <p>For events spanning multiple days/hours/shifts, this creates separate rows for each period with adjusted start/end times and recalculated durations.</p> <p>Configuration for splitting events that span multiple time periods.</p> <p>Splits events that span multiple days, hours, or shifts into individual segments per period. Useful for OEE/downtime analysis, billing, and time-based aggregations.</p> <p>Example - Split by day: <pre><code>split_events_by_period:\n  start_col: \"Shutdown_Start_Time\"\n  end_col: \"Shutdown_End_Time\"\n  period: \"day\"\n  duration_col: \"Shutdown_Duration_Min\"\n</code></pre></p> <p>Example - Split by shift: <pre><code>split_events_by_period:\n  start_col: \"event_start\"\n  end_col: \"event_end\"\n  period: \"shift\"\n  duration_col: \"duration_minutes\"\n  shifts:\n    - name: \"Day\"\n      start: \"06:00\"\n      end: \"14:00\"\n    - name: \"Swing\"\n      start: \"14:00\"\n      end: \"22:00\"\n    - name: \"Night\"\n      start: \"22:00\"\n      end: \"06:00\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description start_col str Yes - Column containing the event start timestamp end_col str Yes - Column containing the event end timestamp period str No <code>day</code> Period type to split by: 'day', 'hour', or 'shift' duration_col Optional[str] No - Output column name for duration in minutes. If not set, no duration column is added. shifts Optional[List[ShiftDefinition]] No - List of shift definitions (required when period='shift') shift_col Optional[str] No <code>shift_name</code> Output column name for shift name (only used when period='shift')"},{"location":"reference/yaml_schema/#unpackstructparams","title":"<code>unpack_struct</code> (UnpackStructParams)","text":"<p>Flattens a struct/dict column into top-level columns.</p> <p>Configuration for unpacking structs.</p> <p>Example: <pre><code>unpack_struct:\n  column: \"user_info\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description column str Yes - Struct/Dictionary column to unpack/flatten into individual columns"},{"location":"reference/yaml_schema/#validateandflagparams","title":"<code>validate_and_flag</code> (ValidateAndFlagParams)","text":"<p>Validates rules and appends a column with a list/string of failed rule names.</p> <p>Configuration for validation flagging.</p> <p>Example: <pre><code>validate_and_flag:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description rules Dict[str, str] Yes - Map of rule name to SQL condition (must be TRUE) flag_col str No <code>_issues</code> Name of the column to store failed rules"},{"location":"reference/yaml_schema/#windowcalculationparams","title":"<code>window_calculation</code> (WindowCalculationParams)","text":"<p>Generic wrapper for Window functions.</p> <p>Configuration for window functions.</p> <p>Example: <pre><code>window_calculation:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre></p> <p>Back to Catalog</p> Field Type Required Default Description target_col str Yes - - function str Yes - Window function e.g. 'sum(amount)', 'rank()' partition_by List[str] No <code>PydanticUndefined</code> - order_by Optional[str] No - -"},{"location":"reference/yaml_schema/#semantic-layer","title":"Semantic Layer","text":""},{"location":"reference/yaml_schema/#semantic-layer_1","title":"Semantic Layer","text":"<p>The semantic layer provides a unified interface for defining and querying business metrics. Define metrics once, query them by name across dimensions.</p> <p>Core Components: - MetricDefinition: Define aggregation expressions (SUM, COUNT, AVG) - DimensionDefinition: Define grouping attributes with hierarchies - MaterializationConfig: Pre-compute metrics at specific grain - SemanticQuery: Execute queries like \"revenue BY region, month\" - Project: Unified API that connects pipelines and semantic layer</p> <p>Unified Project API (Recommended): <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre></p> <p>YAML Configuration: <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: gold.fact_orders    # connection.table notation\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: gold.dim_customer\n      column: region\n\nmaterializations:\n  - name: monthly_revenue\n    metrics: [revenue]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre></p> <p>The <code>source: gold.fact_orders</code> notation resolves paths automatically from connections.</p>"},{"location":"reference/yaml_schema/#dimensiondefinition","title":"<code>DimensionDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic dimension.</p> <p>A dimension represents an attribute for grouping and filtering metrics (e.g., date, product, region).</p> <p>Attributes:     name: Unique dimension identifier     label: Display name for column alias in generated views. Defaults to name.     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.dim_customer</code>         - <code>connection.path</code>: e.g., <code>gold.dim_customer</code> or <code>gold.dims/customer</code>         - <code>table_name</code>: Uses default connection     column: Column name in source (defaults to name)     expr: Custom SQL expression. If provided, overrides column and grain.         Example: \"YEAR(DATEADD(month, 6, Date))\" for fiscal year     hierarchy: Optional ordered list of columns for drill-down     description: Human-readable description     grain: Time grain transformation (day, week, month, quarter, year).         Ignored if expr is provided.</p> Field Type Required Default Description name str Yes - Unique dimension identifier label Optional[str] No - Display name for column alias (defaults to name) source Optional[str] No - Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.dim_customer), connection.path (e.g., gold.dim_customer or gold.dims/customer), or bare table_name column Optional[str] No - Column name (defaults to name) expr Optional[str] No - Custom SQL expression. Overrides column and grain. Example: YEAR(DATEADD(month, 6, Date)) for fiscal year hierarchy List[str] No <code>PydanticUndefined</code> Drill-down hierarchy description Optional[str] No - Human-readable description grain Optional[TimeGrain] No - Time grain transformation"},{"location":"reference/yaml_schema/#materializationconfig","title":"<code>MaterializationConfig</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Configuration for materializing metrics to a table.</p> <p>Materialization pre-computes aggregated metrics at a specific grain and persists them for faster querying.</p> <p>Attributes:     name: Unique materialization identifier     metrics: List of metric names to include     dimensions: List of dimension names (determines grain)     output: Output table path     schedule: Optional cron schedule for refresh     incremental: Configuration for incremental refresh</p> Field Type Required Default Description name str Yes - Unique materialization identifier metrics List[str] Yes - Metrics to materialize dimensions List[str] Yes - Dimensions for grouping output str Yes - Output table path schedule Optional[str] No - Cron schedule incremental Optional[Dict[str, Any]] No - Incremental refresh config"},{"location":"reference/yaml_schema/#metricdefinition","title":"<code>MetricDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic metric.</p> <p>A metric represents a measurable value that can be aggregated across dimensions (e.g., revenue, order_count, avg_order_value).</p> <p>Attributes:     name: Unique metric identifier     label: Display name for column alias in generated views. Defaults to name.     description: Human-readable description     expr: SQL aggregation expression (e.g., \"SUM(total_amount)\").         Optional for derived metrics.     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.fact_orders</code>         - <code>connection.path</code>: e.g., <code>gold.fact_orders</code> or <code>gold.oee/plant_a/metrics</code>         - <code>table_name</code>: Uses default connection     filters: Optional WHERE conditions to apply     type: \"simple\" (direct aggregation) or \"derived\" (references other metrics)     components: List of component metric names (required for derived metrics).         These metrics must be additive (e.g., SUM-based) for correct         recalculation at different grains.     formula: Calculation formula using component names (required for derived).         Example: \"(total_revenue - total_cost) / total_revenue\"</p> Field Type Required Default Description name str Yes - Unique metric identifier label Optional[str] No - Display name for column alias (defaults to name) description Optional[str] No - Human-readable description expr Optional[str] No - SQL aggregation expression source Optional[str] No - Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.fact_orders), connection.path (e.g., gold.fact_orders or gold.oee/plant_a/table), or bare table_name filters List[str] No <code>PydanticUndefined</code> WHERE conditions type MetricType No <code>MetricType.SIMPLE</code> Metric type components Optional[List[str]] No - Component metric names for derived metrics formula Optional[str] No - Calculation formula using component names"},{"location":"reference/yaml_schema/#semanticlayerconfig","title":"<code>SemanticLayerConfig</code>","text":"<p>Complete semantic layer configuration.</p> <p>Contains all metrics, dimensions, materializations, and views for a semantic layer deployment.</p> <p>Attributes:     metrics: List of metric definitions     dimensions: List of dimension definitions     materializations: List of materialization configurations     views: List of view configurations</p> Field Type Required Default Description metrics List[MetricDefinition] No <code>PydanticUndefined</code> Metric definitions dimensions List[DimensionDefinition] No <code>PydanticUndefined</code> Dimension definitions materializations List[MaterializationConfig] No <code>PydanticUndefined</code> Materialization configs views List[ViewConfig] No <code>PydanticUndefined</code> View configurations"},{"location":"reference/yaml_schema/#fk-validation","title":"FK Validation","text":""},{"location":"reference/yaml_schema/#fk-validation_1","title":"FK Validation","text":"<p>Declare and validate referential integrity between fact and dimension tables.</p> <p>Features: - Declare relationships in YAML - Validate FK constraints on fact load - Detect orphan records - Generate lineage from relationships</p> <p>Example: <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    on_violation: error\n</code></pre></p>"},{"location":"reference/yaml_schema/#relationshipconfig","title":"<code>RelationshipConfig</code>","text":"<p>Used in: RelationshipRegistry</p> <p>Configuration for a foreign key relationship.</p> <p>Attributes:     name: Unique relationship identifier     fact: Fact table name     dimension: Dimension table name     fact_key: Foreign key column in fact table     dimension_key: Primary/surrogate key column in dimension     nullable: Whether nulls are allowed in fact_key     on_violation: Action on violation (\"warn\", \"error\", \"quarantine\")</p> Field Type Required Default Description name str Yes - Unique relationship identifier fact str Yes - Fact table name dimension str Yes - Dimension table name fact_key str Yes - FK column in fact table dimension_key str Yes - PK/SK column in dimension nullable bool No <code>False</code> Allow nulls in fact_key on_violation str No <code>error</code> Action on violation"},{"location":"reference/yaml_schema/#relationshipregistry","title":"<code>RelationshipRegistry</code>","text":"<p>Registry of all declared relationships.</p> <p>Attributes:     relationships: List of relationship configurations</p> Field Type Required Default Description relationships List[RelationshipConfig] No <code>PydanticUndefined</code> Relationship definitions"},{"location":"reference/yaml_schema/#data-patterns","title":"Data Patterns","text":""},{"location":"reference/yaml_schema/#data-patterns_1","title":"Data Patterns","text":"<p>Declarative patterns for common data warehouse building blocks. Patterns encapsulate best practices for dimensional modeling, ensuring consistent implementation across your data warehouse.</p>"},{"location":"reference/yaml_schema/#dimensionpattern","title":"DimensionPattern","text":"<p>Build complete dimension tables with surrogate keys and SCD (Slowly Changing Dimension) support.</p> <p>When to Use: - Building dimension tables from source systems (customers, products, locations) - Need surrogate keys for star schema joins - Need to track historical changes (SCD Type 2)</p> <p>Beginner Note: Dimensions are the \"who, what, where, when\" of your data warehouse. A customer dimension has customer_id (natural key) and customer_sk (surrogate key). Fact tables join to dimensions via surrogate keys.</p> <p>See Also: FactPattern, DateDimensionPattern</p> <p>Features: - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> <p>Params:</p> Parameter Type Required Description <code>natural_key</code> str Yes Natural/business key column name <code>surrogate_key</code> str Yes Surrogate key column name to generate <code>scd_type</code> int No 0=static, 1=overwrite, 2=history (default: 1) <code>track_cols</code> list SCD1/2 Columns to track for change detection <code>target</code> str SCD2 Target table path to read existing history <code>unknown_member</code> bool No Insert row with SK=0 for orphan handling <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column with value <p>Supported Target Formats: - Spark: catalog.table, Delta paths, .parquet, .csv, .json, .orc - Pandas: .parquet, .csv, .json, .xlsx, .feather, .pickle</p> <p>Example: <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_cols: [name, email, address, city]\n    target: warehouse.dim_customer\n    unknown_member: true\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre></p>"},{"location":"reference/yaml_schema/#datedimensionpattern","title":"DateDimensionPattern","text":"<p>Generate a complete date dimension table with pre-calculated attributes for BI/reporting.</p> <p>When to Use: - Every data warehouse needs a date dimension for time-based analytics - Enable date filtering, grouping by week/month/quarter, fiscal year reporting</p> <p>Beginner Note: The date dimension is foundational for any BI/reporting system. It lets you query \"sales by month\" or \"orders in fiscal Q2\" without complex date calculations.</p> <p>See Also: DimensionPattern</p> <p>Features: - Generates all dates in a range with rich attributes - Calendar and fiscal year support - ISO week numbering - Weekend/month-end flags</p> <p>Params:</p> Parameter Type Required Description <code>start_date</code> str Yes Start date (YYYY-MM-DD) <code>end_date</code> str Yes End date (YYYY-MM-DD) <code>date_key_format</code> str No Format for date_sk (default: yyyyMMdd) <code>fiscal_year_start_month</code> int No Month fiscal year starts (1-12, default: 1) <code>unknown_member</code> bool No Add unknown date row with date_sk=0 <p>Generated Columns: <code>date_sk</code>, <code>full_date</code>, <code>day_of_week</code>, <code>day_of_week_num</code>, <code>day_of_month</code>, <code>day_of_year</code>, <code>is_weekend</code>, <code>week_of_year</code>, <code>month</code>, <code>month_name</code>, <code>quarter</code>, <code>quarter_name</code>, <code>year</code>, <code>fiscal_year</code>, <code>fiscal_quarter</code>, <code>is_month_start</code>, <code>is_month_end</code>, <code>is_year_start</code>, <code>is_year_end</code></p> <p>Example: <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    fiscal_year_start_month: 7\n    unknown_member: true\n</code></pre></p>"},{"location":"reference/yaml_schema/#factpattern","title":"FactPattern","text":"<p>Build fact tables with automatic surrogate key lookups from dimensions.</p> <p>When to Use: - Building fact tables from transactional data (orders, events, transactions) - Need to look up surrogate keys from dimension tables - Need to handle orphan records (missing dimension matches)</p> <p>Beginner Note: Facts are the \"how much, how many\" of your data warehouse. An orders fact has measures (quantity, revenue) and dimension keys (customer_sk, product_sk). The pattern automatically looks up SKs from dimensions.</p> <p>See Also: DimensionPattern, QuarantineConfig</p> <p>Features: - Automatic SK lookups from dimension tables (with SCD2 current-record filtering) - Orphan handling: unknown (SK=0), reject (error), quarantine (route to table) - Grain validation (detect duplicates) - Calculated measures and column renaming - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list No Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No Dimension lookup configurations (see below) <code>orphan_handling</code> str No \"unknown\" | \"reject\" | \"quarantine\" (default: unknown) <code>quarantine</code> dict quarantine Quarantine config (see below) <code>measures</code> list No Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No Remove duplicates before processing <code>keys</code> list dedupe Keys for deduplication <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Dimension Lookup Config: <pre><code>dimensions:\n  - source_column: customer_id      # Column in source fact\n    dimension_table: dim_customer   # Dimension in context\n    dimension_key: customer_id      # Natural key in dimension\n    surrogate_key: customer_sk      # SK to retrieve\n    scd2: true                      # Filter is_current=true\n</code></pre></p> <p>Quarantine Config (for orphan_handling: quarantine): <pre><code>quarantine:\n  connection: silver                # Required: connection name\n  path: fact_orders_orphans         # OR table: quarantine_table\n  add_columns:\n    _rejection_reason: true         # Add rejection reason\n    _rejected_at: true              # Add rejection timestamp\n    _source_dimension: true         # Add dimension name\n</code></pre></p> <p>Example: <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n        scd2: true\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n    orphan_handling: unknown\n    measures:\n      - quantity\n      - revenue: \"quantity * unit_price\"\n    audit:\n      load_timestamp: true\n      source_system: \"pos\"\n</code></pre></p>"},{"location":"reference/yaml_schema/#aggregationpattern","title":"AggregationPattern","text":"<p>Declarative aggregation with GROUP BY and optional incremental merge.</p> <p>When to Use: - Building summary/aggregate tables (daily sales, monthly metrics) - Need incremental aggregation (update existing aggregates) - Gold layer reporting tables</p> <p>Beginner Note: Aggregations summarize facts at a higher grain. Example: daily_sales aggregates orders by date with SUM(revenue).</p> <p>See Also: FactPattern</p> <p>Features: - Declare grain (GROUP BY columns) - Define measures with SQL aggregation expressions - Optional HAVING filter - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list Yes Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes Measure definitions with name and expr <code>having</code> str No HAVING clause for filtering aggregates <code>incremental.timestamp_column</code> str No Column to identify new data <code>incremental.merge_strategy</code> str No \"replace\", \"sum\", \"min\", or \"max\" <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Example: <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk, region]\n    measures:\n      - name: total_revenue\n        expr: \"SUM(total_amount)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n      - name: avg_order_value\n        expr: \"AVG(total_amount)\"\n    having: \"COUNT(*) &gt; 0\"\n    audit:\n      load_timestamp: true\n</code></pre></p>"},{"location":"reference/yaml_schema/#auditconfig","title":"<code>AuditConfig</code>","text":"<p>Configuration for audit columns.</p> Field Type Required Default Description load_timestamp bool No <code>True</code> Add load_timestamp column source_system Optional[str] No - Source system name for source_system column"},{"location":"reference/yaml_schema_v1/","title":"Odibi Configuration Reference","text":"<p>This manual details the YAML configuration schema for Odibi projects. Auto-generated from Pydantic models.</p>"},{"location":"reference/yaml_schema_v1/#project-structure","title":"Project Structure","text":""},{"location":"reference/yaml_schema_v1/#projectconfig","title":"<code>ProjectConfig</code>","text":"<p>Complete project configuration from YAML.</p>"},{"location":"reference/yaml_schema_v1/#enterprise-setup-guide","title":"\ud83c\udfe2 \"Enterprise Setup\" Guide","text":"<p>Business Problem: \"We need a robust production environment with alerts, retries, and proper logging.\"</p> <p>Recipe: Production Ready <pre><code>project: \"Customer360\"\nengine: \"spark\"\n\n# 1. Resilience\nretry:\n    enabled: true\n    max_attempts: 3\n    backoff: \"exponential\"\n\n# 2. Observability\nlogging:\n    level: \"INFO\"\n    structured: true  # JSON logs for Splunk/Datadog\n\n# 3. Alerting\nalerts:\n    - type: \"slack\"\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events: [\"on_failure\"]\n\n# ... connections and pipelines ...\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | project | str | Yes | - | Project name | | engine | EngineType | No | <code>EngineType.PANDAS</code> | Execution engine | | connections | Dict[str, LocalConnectionConfig | AzureBlobConnectionConfig | DeltaConnectionConfig | SQLServerConnectionConfig | HttpConnectionConfig | CustomConnectionConfig] | Yes | - | Named connections (at least one required)Options: LocalConnectionConfig, AzureBlobConnectionConfig, DeltaConnectionConfig, SQLServerConnectionConfig, HttpConnectionConfig | | pipelines | List[PipelineConfig] | Yes | - | Pipeline definitions (at least one required) | | story | StoryConfig | Yes | - | Story generation configuration (mandatory) | | system | SystemConfig | Yes | - | System Catalog configuration (mandatory) | | lineage | Optional[LineageConfig] | No | - | OpenLineage configuration | | description | Optional[str] | No | - | Project description | | version | str | No | <code>1.0.0</code> | Project version | | owner | Optional[str] | No | - | Project owner/contact | | vars | Dict[str, Any] | No | <code>PydanticUndefined</code> | Global variables for substitution (e.g. ${vars.env}) | | retry | RetryConfig | No | <code>PydanticUndefined</code> | - | | logging | LoggingConfig | No | <code>PydanticUndefined</code> | - | | alerts | List[AlertConfig] | No | <code>PydanticUndefined</code> | Alert configurations | | performance | PerformanceConfig | No | <code>PydanticUndefined</code> | Performance tuning | | environments | Optional[Dict[str, Dict[str, Any]]] | No | - | Structure: same as ProjectConfig but with only overridden fields. Not yet validated strictly. | | semantic | Optional[Dict[str, Any]] | No | - | Semantic layer configuration. Can be inline or reference external file. Contains metrics, dimensions, and materializations for self-service analytics. Example: semantic: { config: 'semantic_config.yaml' } or inline definitions. |</p>"},{"location":"reference/yaml_schema_v1/#pipelineconfig","title":"<code>PipelineConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for a pipeline.</p> <p>Example: <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    owner: \"data-team@example.com\"\n    freshness_sla: \"6h\"\n    nodes:\n      - name: \"node1\"\n        ...\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | pipeline | str | Yes | - | Pipeline name | | description | Optional[str] | No | - | Pipeline description | | layer | Optional[str] | No | - | Logical layer (bronze/silver/gold) | | owner | Optional[str] | No | - | Pipeline owner (email or name) | | freshness_sla | Optional[str] | No | - | Expected freshness, e.g. '6h', '1d' | | freshness_anchor | Literal['run_completion', 'table_max_timestamp', 'watermark_state'] | No | <code>run_completion</code> | What defines freshness. Only 'run_completion' implemented initially. | | nodes | List[NodeConfig] | Yes | - | List of nodes in this pipeline |</p>"},{"location":"reference/yaml_schema_v1/#nodeconfig","title":"<code>NodeConfig</code>","text":"<p>Used in: PipelineConfig</p> <p>Configuration for a single node.</p>"},{"location":"reference/yaml_schema_v1/#the-smart-node-pattern","title":"\ud83e\udde0 \"The Smart Node\" Pattern","text":"<p>Business Problem: \"We need complex dependencies, caching for heavy computations, and the ability to run only specific parts of the pipeline.\"</p> <p>The Solution: Nodes are the building blocks. They handle dependencies (<code>depends_on</code>), execution control (<code>tags</code>, <code>enabled</code>), and performance (<code>cache</code>).</p>"},{"location":"reference/yaml_schema_v1/#dag-dependencies","title":"\ud83d\udd78\ufe0f DAG &amp; Dependencies","text":"<p>The Glue of the Pipeline. Nodes don't run in isolation. They form a Directed Acyclic Graph (DAG).</p> <ul> <li><code>depends_on</code>: Critical! If Node B reads from Node A (in memory), you MUST list <code>[\"Node A\"]</code>.<ul> <li>Implicit Data Flow: If a node has no <code>read</code> block, it automatically picks up the DataFrame from its first dependency.</li> </ul> </li> </ul>"},{"location":"reference/yaml_schema_v1/#smart-read-incremental-loading","title":"\ud83e\udde0 Smart Read &amp; Incremental Loading","text":"<p>Automated History Management.</p> <p>Odibi intelligently determines whether to perform a Full Load or an Incremental Load based on the state of the target.</p> <p>The \"Smart Read\" Logic: 1.  First Run (Full Load): If the target table (defined in <code>write</code>) does not exist:     *   Incremental filtering rules are ignored.     *   The entire source dataset is read.     *   Use <code>write.first_run_query</code> (optional) to override the read query for this initial bootstrap (e.g., to backfill only 1 year of history instead of all time).</p> <ol> <li>Subsequent Runs (Incremental Load): If the target table exists:<ul> <li>Rolling Window: Filters source data where <code>column &gt;= NOW() - lookback</code>.</li> <li>Stateful: Filters source data where <code>column &gt; last_high_water_mark</code>.</li> </ul> </li> </ol> <p>This ensures you don't need separate \"init\" and \"update\" pipelines. One config handles both lifecycle states.</p>"},{"location":"reference/yaml_schema_v1/#orchestration-tags","title":"\ud83c\udff7\ufe0f Orchestration Tags","text":"<p>Run What You Need. Tags allow you to execute slices of your pipeline. *   <code>odibi run --tag daily</code> -&gt; Runs all nodes with \"daily\" tag. *   <code>odibi run --tag critical</code> -&gt; Runs high-priority nodes.</p>"},{"location":"reference/yaml_schema_v1/#choosing-your-logic-transformer-vs-transform","title":"\ud83e\udd16 Choosing Your Logic: Transformer vs. Transform","text":"<p>1. The \"Transformer\" (Top-Level) *   What it is: A pre-packaged, heavy-duty operation that defines the entire purpose of the node. *   When to use: When applying a standard Data Engineering pattern (e.g., SCD2, Merge, Deduplicate). *   Analogy: \"Run this App.\" *   Syntax: <code>transformer: \"scd2\"</code> + <code>params: {...}</code></p> <p>2. The \"Transform Steps\" (Process Chain) *   What it is: A sequence of smaller steps (SQL, functions, operations) executed in order. *   When to use: For custom business logic, data cleaning, or feature engineering pipelines. *   Analogy: \"Run this Script.\" *   Syntax: <code>transform: { steps: [...] }</code></p> <p>Note: You can use both! The <code>transformer</code> runs first, then <code>transform</code> steps refine the result.</p>"},{"location":"reference/yaml_schema_v1/#chaining-operations","title":"\ud83d\udd17 Chaining Operations","text":"<p>You can mix and match! The execution order is always: 1.  Read (or Dependency Injection) 2.  Transformer (The \"App\" logic, e.g., Deduplicate) 3.  Transform Steps (The \"Script\" logic, e.g., cleanup) 4.  Validation 5.  Write</p> <p>Constraint: You must define at least one of <code>read</code>, <code>transformer</code>, <code>transform</code>, or <code>write</code>.</p>"},{"location":"reference/yaml_schema_v1/#example-app-vs-script","title":"\u26a1 Example: App vs. Script","text":"<p>Scenario 1: The Full ETL Flow (Chained) Shows explicit Read, Transform Chain, and Write.</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]\n\n  # \"clean_text\" is a registered function from the Transformer Catalog\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Top-Level Transformer) Shows a node that applies a pattern (Deduplicate) to incoming data.</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication (From Transformer Catalog)\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner (Reporting) Shows how tags allow running specific slices (e.g., <code>odibi run --tag daily</code>).</p> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  depends_on: [\"deduped_users\"]\n\n  # Ad-hoc aggregation script\n  transform:\n    steps:\n      - sql: \"SELECT date_trunc('day', updated_at) as day, count(*) as total FROM df GROUP BY 1\"\n\n  write: { connection: \"local_data\", format: \"csv\", path: \"reports/daily_stats.csv\" }\n</code></pre> <p>Scenario 4: The \"Kitchen Sink\" (All Operations) Shows Read -&gt; Transformer -&gt; Transform -&gt; Write execution order.</p> <p>Why this works: 1.  Internal Chaining (<code>df</code>): In every step (Transformer or SQL), <code>df</code> refers to the output of the previous step. 2.  External Access (<code>depends_on</code>): If you added <code>depends_on: [\"other_node\"]</code>, you could also run <code>SELECT * FROM other_node</code> in your SQL steps!</p> <pre><code>- name: \"complex_flow\"\n  # 1. Read -&gt; Creates initial 'df'\n  read: { connection: \"bronze\", format: \"parquet\", path: \"users\" }\n\n  # 2. Transformer (The \"App\": Deduplicate first)\n  # Takes 'df' (from Read), dedups it, returns new 'df'\n  transformer: \"deduplicate\"\n  params: { keys: [\"user_id\"], order_by: \"updated_at DESC\" }\n\n  # 3. Transform Steps (The \"Script\": Filter AFTER deduplication)\n  # SQL sees the deduped data as 'df'\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n  # 4. Write -&gt; Saves the final filtered 'df'\n  write: { connection: \"silver\", format: \"delta\", table: \"active_unique_users\" }\n</code></pre>"},{"location":"reference/yaml_schema_v1/#transformer-catalog","title":"\ud83d\udcda Transformer Catalog","text":"<p>These are the built-in functions you can use in two ways:</p> <ol> <li>As a Top-Level Transformer: <code>transformer: \"name\"</code> (Defines the node's main logic)</li> <li>As a Step in a Chain: <code>transform: { steps: [{ function: \"name\" }] }</code> (Part of a sequence)</li> </ol> <p>Note: <code>merge</code> and <code>scd2</code> are special \"Heavy Lifters\" and should generally be used as Top-Level Transformers.</p> <p>Data Engineering Patterns *   <code>merge</code>: Upsert/Merge into target (Delta/SQL). (Params) *   <code>scd2</code>: Slowly Changing Dimensions Type 2. (Params) *   <code>deduplicate</code>: Remove duplicates using window functions. (Params)</p> <p>Relational Algebra *   <code>join</code>: Join two datasets. (Params) *   <code>union</code>: Stack datasets vertically. (Params) *   <code>pivot</code>: Rotate rows to columns. (Params) *   <code>unpivot</code>: Rotate columns to rows (melt). (Params) *   <code>aggregate</code>: Group by and sum/count/avg. (Params)</p> <p>Data Quality &amp; Cleaning *   <code>validate_and_flag</code>: Check rules and flag invalid rows. (Params) *   <code>clean_text</code>: Trim and normalize case. (Params) *   <code>filter_rows</code>: SQL-based filtering. (Params) *   <code>fill_nulls</code>: Replace NULLs with defaults. (Params)</p> <p>Feature Engineering *   <code>derive_columns</code>: Create new cols via SQL expressions. (Params) *   <code>case_when</code>: Conditional logic (if-else). (Params) *   <code>generate_surrogate_key</code>: Create MD5 keys from columns. (Params) *   <code>date_diff</code>, <code>date_add</code>, <code>date_trunc</code>: Date arithmetic.</p> <p>Scenario 1: The Full ETL Flow (Show two nodes: one loader, one processor)</p> <pre><code># 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]  # &lt;--- Explicit dependency\n\n  # Explicit Transformation Steps\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n</code></pre> <p>Scenario 2: The \"App\" Node (Transformer) (Show a node that is a Transformer, no read needed if it picks up from dependency)</p> <pre><code>- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n</code></pre> <p>Scenario 3: The Tagged Runner Run only this with <code>odibi run --tag daily</code> <pre><code>- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  # ...\n</code></pre></p> <p>Scenario 4: Pre/Post SQL Hooks Setup and cleanup with SQL statements. <pre><code>- name: \"optimize_sales\"\n  depends_on: [\"load_sales\"]\n  pre_sql:\n    - \"SET spark.sql.shuffle.partitions = 200\"\n    - \"CREATE TEMP VIEW staging AS SELECT * FROM bronze.raw_sales\"\n  transform:\n    steps:\n      - sql: \"SELECT * FROM staging WHERE amount &gt; 0\"\n  post_sql:\n    - \"OPTIMIZE gold.fact_sales ZORDER BY (customer_id)\"\n    - \"VACUUM gold.fact_sales RETAIN 168 HOURS\"\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sales\"\n</code></pre></p> <p>Scenario 5: Materialization Strategies Choose how output is persisted. <pre><code># Option 1: View (no physical storage, logical model)\n- name: \"vw_active_customers\"\n  materialized: \"view\"  # Creates SQL view instead of table\n  transform:\n    steps:\n      - sql: \"SELECT * FROM customers WHERE status = 'active'\"\n  write:\n    connection: \"gold\"\n    table: \"vw_active_customers\"\n\n# Option 2: Incremental (append to existing Delta table)\n- name: \"fact_events\"\n  materialized: \"incremental\"  # Uses APPEND mode\n  read:\n    connection: \"bronze\"\n    table: \"raw_events\"\n    incremental:\n      mode: \"stateful\"\n      column: \"event_time\"\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"fact_events\"\n\n# Option 3: Table (default - full overwrite)\n- name: \"dim_products\"\n  materialized: \"table\"  # Default behavior\n  # ...\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique node name | | description | Optional[str] | No | - | Human-readable description | | runbook_url | Optional[str] | No | - | URL to troubleshooting guide or runbook. Shown as 'Troubleshooting guide \u2192' link on failures. | | enabled | bool | No | <code>True</code> | If False, node is skipped during execution | | tags | List[str] | No | <code>PydanticUndefined</code> | Operational tags for selective execution (e.g., 'daily', 'critical'). Use with <code>odibi run --tag</code>. | | depends_on | List[str] | No | <code>PydanticUndefined</code> | List of parent nodes that must complete before this node runs. The output of these nodes is available for reading. | | columns | Dict[str, ColumnMetadata] | No | <code>PydanticUndefined</code> | Data Dictionary defining the output schema. Used for documentation, PII tagging, and validation. | | read | Optional[ReadConfig] | No | - | Input operation (Load). If missing, data is taken from the first dependency. | | inputs | Optional[Dict[str, str | Dict[str, Any]]] | No | - | Multi-input support for cross-pipeline dependencies. Map input names to either: (a) $pipeline.node reference (e.g., '$read_bronze.shift_events') (b) Explicit read config dict. Cannot be used with 'read'. Example: inputs: {events: '$read_bronze.events', calendar: {connection: 'goat', path: 'cal'}} | | transform | Optional[TransformConfig] | No | - | Chain of fine-grained transformation steps (SQL, functions). Runs after 'transformer' if both are present. | | write | Optional[WriteConfig] | No | - | Output operation (Save to file/table). | | streaming | bool | No | <code>False</code> | Enable streaming execution for this node (Spark only) | | transformer | Optional[str] | No | - | Name of the 'App' logic to run (e.g., 'deduplicate', 'scd2'). See Transformer Catalog for options. | | params | Dict[str, Any] | No | <code>PydanticUndefined</code> | Parameters for transformer | | pre_sql | List[str] | No | <code>PydanticUndefined</code> | List of SQL statements to execute before node runs. Use for setup: temp tables, variable initialization, grants. Example: ['SET spark.sql.shuffle.partitions=200', 'CREATE TEMP VIEW src AS SELECT * FROM raw'] | | post_sql | List[str] | No | <code>PydanticUndefined</code> | List of SQL statements to execute after node completes. Use for cleanup, optimization, or audit logging. Example: ['OPTIMIZE gold.fact_sales', 'VACUUM gold.fact_sales RETAIN 168 HOURS'] | | materialized | Optional[Literal['table', 'view', 'incremental']] | No | - | Materialization strategy. Options: 'table' (default physical write), 'view' (creates SQL view instead of table), 'incremental' (uses append mode for Delta tables). Views are useful for Gold layer logical models. | | cache | bool | No | <code>False</code> | Cache result for reuse | | log_level | Optional[LogLevel] | No | - | Override log level for this node | | on_error | ErrorStrategy | No | <code>ErrorStrategy.FAIL_LATER</code> | Failure handling strategy | | validation | Optional[ValidationConfig] | No | - | - | | contracts | List[TestConfig] | No | <code>PydanticUndefined</code> | Pre-condition contracts (Circuit Breakers). Runs on input data before transformation.Options: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract | | schema_policy | Optional[SchemaPolicyConfig] | No | - | Schema drift handling policy | | privacy | Optional[PrivacyConfig] | No | - | Privacy Suite: PII anonymization settings | | sensitive | bool | List[str] | No | <code>False</code> | If true or list of columns, masks sample data in stories | | source_yaml | Optional[str] | No | - | Internal: source YAML file path for sql_file resolution |</p>"},{"location":"reference/yaml_schema_v1/#columnmetadata","title":"<code>ColumnMetadata</code>","text":"<p>Used in: NodeConfig</p> <p>Metadata for a column in the data dictionary. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | description | Optional[str] | No | - | Column description | | pii | bool | No | <code>False</code> | Contains PII? | | tags | List[str] | No | <code>PydanticUndefined</code> | Tags (e.g. 'business_key', 'measure') |</p>"},{"location":"reference/yaml_schema_v1/#systemconfig","title":"<code>SystemConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for the Odibi System Catalog (The Brain).</p> <p>Stores metadata, state, and pattern configurations. The primary connection must be a storage connection (blob/local) that supports Delta tables.</p> <p>Example: <pre><code>system:\n  connection: adls_bronze        # Primary - must be blob/local storage\n  path: _odibi_system\n  environment: dev\n</code></pre></p> <p>With sync to SQL Server (for dashboards/queries): <pre><code>system:\n  connection: adls_prod          # Primary - Delta tables\n  environment: prod\n  sync_to:\n    connection: sql_server_prod  # Secondary - SQL for visibility\n    schema_name: odibi_system\n</code></pre></p> <p>With sync to another blob (cross-region backup): <pre><code>system:\n  connection: adls_us_east\n  sync_to:\n    connection: adls_us_west\n    path: _odibi_system_replica\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection for primary system tables. Must be blob storage (azure_blob) or local filesystem - NOT SQL Server. Delta tables require storage backends. | | path | str | No | <code>_odibi_system</code> | Path relative to connection root | | environment | Optional[str] | No | - | Environment tag (e.g., 'dev', 'qat', 'prod'). Written to all system table records for cross-environment querying. | | schema_name | Optional[str] | No | - | Deprecated. Use sync_to.schema_name for SQL Server targets. | | sync_to | Optional[SyncToConfig] | No | - | Secondary destination to sync system catalog data to. Use for SQL Server dashboards or cross-region Delta replication. | | sync_from | Optional[SyncFromConfig] | No | - | Source to sync system data from. Enables pushing local development data to centralized system tables. | | cost_per_compute_hour | Optional[float] | No | - | Estimated cost per compute hour (USD) for cost tracking | | databricks_billing_enabled | bool | No | <code>False</code> | Attempt to query Databricks billing tables for actual costs | | retention_days | Optional[RetentionConfig] | No | - | Retention periods for system tables |</p>"},{"location":"reference/yaml_schema_v1/#syncfromconfig","title":"<code>SyncFromConfig</code>","text":"<p>Used in: SystemConfig</p> <p>Configuration for syncing system data from a source location.</p> <p>Used to pull system data (runs, state) from another backend into the target.</p> <p>Example: <pre><code>sync_from:\n  connection: local_parquet\n  path: .odibi/system/\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name for the source system data | | path | Optional[str] | No | - | Path to source system data (for file-based sources) | | schema_name | Optional[str] | No | - | Schema name for SQL Server source (if applicable) |</p>"},{"location":"reference/yaml_schema_v1/#connections","title":"Connections","text":""},{"location":"reference/yaml_schema_v1/#localconnectionconfig","title":"<code>LocalConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Local filesystem connection.</p> <p>When to Use: Development, testing, small datasets, local processing.</p> <p>See Also: AzureBlobConnectionConfig for cloud alternatives.</p> <p>Example: <pre><code>local_data:\n  type: \"local\"\n  base_path: \"./data\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['local'] | No | <code>ConnectionType.LOCAL</code> | - | | validation_mode | ValidationMode | No | <code>ValidationMode.LAZY</code> | - | | base_path | str | No | <code>./data</code> | Base directory path |</p>"},{"location":"reference/yaml_schema_v1/#deltaconnectionconfig","title":"<code>DeltaConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Delta Lake connection for ACID-compliant data lakes.</p> <p>When to Use: - Production data lakes on Azure/AWS/GCP - Need time travel, ACID transactions, schema evolution - Upsert/merge operations</p> <p>See Also: WriteConfig for Delta write options</p> <p>Scenario 1: Delta via metastore <pre><code>delta_silver:\n  type: \"delta\"\n  catalog: \"spark_catalog\"\n  schema: \"silver_db\"\n</code></pre></p> <p>Scenario 2: Direct path + Node usage <pre><code>delta_local:\n  type: \"local\"\n  base_path: \"dbfs:/mnt/delta\"\n\n# In pipeline:\n# read:\n#   connection: \"delta_local\"\n#   format: \"delta\"\n#   path: \"bronze/orders\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['delta'] | No | <code>ConnectionType.DELTA</code> | - | | validation_mode | ValidationMode | No | <code>ValidationMode.LAZY</code> | - | | catalog | str | Yes | - | Spark catalog name (e.g. 'spark_catalog') | | schema_name | str | Yes | - | Database/schema name | | table | Optional[str] | No | - | Optional default table name for this connection (used by story/pipeline helpers) |</p>"},{"location":"reference/yaml_schema_v1/#azureblobconnectionconfig","title":"<code>AzureBlobConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Azure Blob Storage / ADLS Gen2 connection.</p> <p>When to Use: Azure-based data lakes, landing zones, raw data storage.</p> <p>See Also: DeltaConnectionConfig for Delta-specific options</p> <p>Scenario 1: Prod with Key Vault-managed key <pre><code>adls_bronze:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"key_vault\"\n    key_vault: \"kv-data\"\n    secret: \"adls-account-key\"\n</code></pre></p> <p>Scenario 2: Local dev with inline account key <pre><code>adls_dev:\n  type: \"azure_blob\"\n  account_name: \"devaccount\"\n  container: \"sandbox\"\n  auth:\n    mode: \"account_key\"\n    account_key: \"${ADLS_ACCOUNT_KEY}\"\n</code></pre></p> <p>Scenario 3: MSI (no secrets) <pre><code>adls_msi:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"aad_msi\"\n    # optional: client_id for user-assigned identity\n    client_id: \"00000000-0000-0000-0000-000000000000\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['azure_blob'] | No | <code>ConnectionType.AZURE_BLOB</code> | - | | validation_mode | ValidationMode | No | <code>ValidationMode.LAZY</code> | - | | account_name | str | Yes | - | - | | container | str | Yes | - | - | | auth | AzureBlobAuthConfig | No | <code>PydanticUndefined</code> | Options: AzureBlobKeyVaultAuth, AzureBlobAccountKeyAuth, AzureBlobSasAuth, AzureBlobConnectionStringAuth, AzureBlobMsiAuth |</p>"},{"location":"reference/yaml_schema_v1/#sqlserverconnectionconfig","title":"<code>SQLServerConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>SQL Server / Azure SQL Database connection.</p> <p>When to Use: Reading from SQL Server sources, Azure SQL DB, Azure Synapse.</p> <p>See Also: ReadConfig for query options</p> <p>Scenario 1: Managed identity (AAD MSI) <pre><code>sql_dw_msi:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"aad_msi\"\n</code></pre></p> <p>Scenario 2: SQL login <pre><code>sql_dw_login:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"sql_login\"\n    username: \"dw_writer\"\n    password: \"${DW_PASSWORD}\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['sql_server'] | No | <code>ConnectionType.SQL_SERVER</code> | - | | validation_mode | ValidationMode | No | <code>ValidationMode.LAZY</code> | - | | host | str | Yes | - | - | | database | str | Yes | - | - | | port | int | No | <code>1433</code> | - | | auth | SQLServerAuthConfig | No | <code>PydanticUndefined</code> | Options: SQLLoginAuth, SQLAadPasswordAuth, SQLMsiAuth, SQLConnectionStringAuth |</p>"},{"location":"reference/yaml_schema_v1/#httpconnectionconfig","title":"<code>HttpConnectionConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>HTTP connection.</p> <p>Scenario: Bearer token via env var <pre><code>api_source:\n  type: \"http\"\n  base_url: \"https://api.example.com\"\n  headers:\n    User-Agent: \"odibi-pipeline\"\n  auth:\n    mode: \"bearer\"\n    token: \"${API_TOKEN}\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['http'] | No | <code>ConnectionType.HTTP</code> | - | | validation_mode | ValidationMode | No | <code>ValidationMode.LAZY</code> | - | | base_url | str | Yes | - | - | | headers | Dict[str, str] | No | <code>PydanticUndefined</code> | - | | auth | HttpAuthConfig | No | <code>PydanticUndefined</code> | Options: HttpNoAuth, HttpBasicAuth, HttpBearerAuth, HttpApiKeyAuth |</p>"},{"location":"reference/yaml_schema_v1/#node-operations","title":"Node Operations","text":""},{"location":"reference/yaml_schema_v1/#readconfig","title":"<code>ReadConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for reading data into a node.</p> <p>When to Use: First node in a pipeline, or any node that reads from storage.</p> <p>Key Concepts: - <code>connection</code>: References a named connection from <code>connections:</code> section - <code>format</code>: File format (csv, parquet, delta, json, sql) - <code>incremental</code>: Enable incremental loading (only new data)</p> <p>See Also: - Incremental Loading - HWM-based loading - IncrementalConfig - Incremental loading options</p>"},{"location":"reference/yaml_schema_v1/#universal-reader-guide","title":"\ud83d\udcd6 \"Universal Reader\" Guide","text":"<p>Business Problem: \"I need to read from files, databases, streams, and even travel back in time to see how data looked yesterday.\"</p> <p>Recipe 1: The Time Traveler (Delta/Iceberg) Reproduce a bug by seeing the data exactly as it was. <pre><code>read:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  time_travel:\n    as_of_timestamp: \"2023-10-25T14:00:00Z\"\n</code></pre></p> <p>Recipe 2: The Streamer Process data in real-time. <pre><code>read:\n  connection: \"event_hub\"\n  format: \"json\"\n  streaming: true\n</code></pre></p> <p>Recipe 3: The SQL Query Push down filtering to the source database. <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  # Use the query option to filter at source!\n  query: \"SELECT * FROM huge_table WHERE date &gt;= '2024-01-01'\"\n</code></pre></p> <p>Recipe 4: Archive Bad Records (Spark) Capture malformed records for later inspection. <pre><code>read:\n  connection: \"landing\"\n  format: \"json\"\n  path: \"events/*.json\"\n  archive_options:\n    badRecordsPath: \"/mnt/quarantine/bad_records\"\n</code></pre></p> <p>Recipe 5: Optimize JDBC Parallelism (Spark) Control partition count for SQL sources to reduce task overhead. <pre><code>read:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  table: \"small_lookup_table\"\n  options:\n    numPartitions: 1  # Single partition for small tables\n</code></pre></p> <p>Performance Tip: For small tables (&lt;100K rows), use <code>numPartitions: 1</code> to avoid excessive Spark task scheduling overhead. For large tables, increase partitions to enable parallel reads (requires partitionColumn, lowerBound, upperBound). | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name from project.yaml | | format | ReadFormat | str | Yes | - | Data format (csv, parquet, delta, etc.) | | table | Optional[str] | No | - | Table name for SQL/Delta | | path | Optional[str] | No | - | Path for file-based sources | | streaming | bool | No | <code>False</code> | Enable streaming read (Spark only) | | schema_ddl | Optional[str] | No | - | Schema for streaming reads from file sources (required for Avro, JSON, CSV). Use Spark DDL format: 'col1 STRING, col2 INT, col3 TIMESTAMP'. Not required for Delta (schema is inferred from table metadata). | | query | Optional[str] | No | - | SQL query to filter at source (pushdown). Mutually exclusive with table/path if supported by connector. | | filter | Optional[str] | No | - | SQL WHERE clause filter (pushed down to source for SQL formats). Example: \"DAY &gt; '2022-12-31'\" | | incremental | Optional[IncrementalConfig] | No | - | Automatic incremental loading strategy (CDC-like). If set, generates query based on target state (HWM). | | time_travel | Optional[TimeTravelConfig] | No | - | Time travel options (Delta only) | | archive_options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Options for archiving bad records (e.g. badRecordsPath for Spark) | | options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Format-specific options |</p>"},{"location":"reference/yaml_schema_v1/#incrementalconfig","title":"<code>IncrementalConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for automatic incremental loading.</p> <p>When to Use: Load only new/changed data instead of full table scans.</p> <p>See Also: ReadConfig</p> <p>Modes: 1. Rolling Window (Default): Uses a time-based lookback from NOW().    Good for: Stateless loading where you just want \"recent\" data.    Args: <code>lookback</code>, <code>unit</code></p> <ol> <li>Stateful: Tracks the High-Water Mark (HWM) of the key column.    Good for: Exact incremental ingestion (e.g. CDC-like).    Args: <code>state_key</code> (optional), <code>watermark_lag</code> (optional)</li> </ol> <p>Generates SQL: - Rolling: <code>WHERE column &gt;= NOW() - lookback</code> - Stateful: <code>WHERE column &gt; :last_hwm</code></p> <p>Example (Rolling Window): <pre><code>incremental:\n  mode: \"rolling_window\"\n  column: \"updated_at\"\n  lookback: 3\n  unit: \"day\"\n</code></pre></p> <p>Example (Stateful HWM): <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"id\"\n  # Optional: track separate column for HWM state\n  state_key: \"last_processed_id\"\n</code></pre></p> <p>Example (Stateful with Watermark Lag): <pre><code>incremental:\n  mode: \"stateful\"\n  column: \"updated_at\"\n  # Handle late-arriving data: look back 2 hours from HWM\n  watermark_lag: \"2h\"\n</code></pre></p> <p>Example (Oracle Date Format): <pre><code>incremental:\n  mode: \"rolling_window\"\n  column: \"EVENT_START\"\n  lookback: 3\n  unit: \"day\"\n  # For string columns with Oracle format (DD-MON-YY)\n  date_format: \"oracle\"\n</code></pre></p> <p>Supported date_format values: - <code>oracle</code>: DD-MON-YY for Oracle databases (uses TO_TIMESTAMP) - <code>oracle_sqlserver</code>: DD-MON-YY format stored in SQL Server (uses TRY_CONVERT) - <code>sql_server</code>: Uses CONVERT with style 120 - <code>us</code>: MM/DD/YYYY format - <code>eu</code>: DD/MM/YYYY format - <code>iso</code>: YYYY-MM-DDTHH:MM:SS format | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | mode | IncrementalMode | No | <code>IncrementalMode.ROLLING_WINDOW</code> | Incremental strategy: 'rolling_window' or 'stateful' | | column | str | Yes | - | Primary column to filter on (e.g., updated_at) | | fallback_column | Optional[str] | No | - | Backup column if primary is NULL (e.g., created_at). Generates COALESCE(col, fallback) &gt;= ... | | lookback | Optional[int] | No | - | Time units to look back (Rolling Window only) | | unit | Optional[IncrementalUnit] | No | - | Time unit for lookback (Rolling Window only). Options: 'hour', 'day', 'month', 'year' | | state_key | Optional[str] | No | - | Unique ID for state tracking. Defaults to node name if not provided. | | watermark_lag | Optional[str] | No | - | Safety buffer for late-arriving data in stateful mode. Subtracts this duration from the stored HWM when filtering. Format: '' where unit is 's', 'm', 'h', or 'd'. Examples: '2h' (2 hours), '30m' (30 minutes), '1d' (1 day). Use when source has replication lag or eventual consistency. | | date_format | Optional[str] | No | - | Source date format when the column is stored as a string. Options: 'oracle' (DD-MON-YY for Oracle DB), 'oracle_sqlserver' (DD-MON-YY format in SQL Server), 'sql_server' (uses CONVERT with style 120), 'us' (MM/DD/YYYY), 'eu' (DD/MM/YYYY), 'iso' (YYYY-MM-DDTHH:MM:SS). When set, SQL pushdown will use appropriate CONVERT/TO_TIMESTAMP functions. |"},{"location":"reference/yaml_schema_v1/#timetravelconfig","title":"<code>TimeTravelConfig</code>","text":"<p>Used in: ReadConfig</p> <p>Configuration for time travel reading (Delta/Iceberg).</p> <p>Example: <pre><code>time_travel:\n  as_of_version: 10\n  # OR\n  as_of_timestamp: \"2023-10-01T12:00:00Z\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | as_of_version | Optional[int] | No | - | Version number to time travel to | | as_of_timestamp | Optional[str] | No | - | Timestamp string to time travel to |</p>"},{"location":"reference/yaml_schema_v1/#transformconfig","title":"<code>TransformConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for transformation steps within a node.</p> <p>When to Use: Custom business logic, data cleaning, SQL transformations.</p> <p>Key Concepts: - <code>steps</code>: Ordered list of operations (SQL, functions, or both) - Each step receives the DataFrame from the previous step - Steps execute in order: step1 \u2192 step2 \u2192 step3</p> <p>See Also: Transformer Catalog</p> <p>Transformer vs Transform: - <code>transformer</code>: Single heavy operation (scd2, merge, deduplicate) - <code>transform.steps</code>: Chain of lighter operations</p>"},{"location":"reference/yaml_schema_v1/#transformation-pipeline-guide","title":"\ud83d\udd27 \"Transformation Pipeline\" Guide","text":"<p>Business Problem: \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"</p> <p>The Solution: Chain multiple steps together. Output of Step 1 becomes input of Step 2.</p> <p>Function Registry: The <code>function</code> step type looks up functions registered with <code>@transform</code> (or <code>@register</code>). This allows you to use the same registered functions as both top-level Transformers and steps in a chain.</p> <p>Recipe: The Mix-and-Match <pre><code>transform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | steps | List[str | TransformStep] | Yes | - | List of transformation steps (SQL strings or TransformStep configs) |</p>"},{"location":"reference/yaml_schema_v1/#deletedetectionconfig","title":"<code>DeleteDetectionConfig</code>","text":"<p>Configuration for delete detection in Silver layer.</p>"},{"location":"reference/yaml_schema_v1/#cdc-without-cdc-guide","title":"\ud83d\udd0d \"CDC Without CDC\" Guide","text":"<p>Business Problem: \"Records are deleted in our Azure SQL source, but our Silver tables still show them.\"</p> <p>The Solution: Use delete detection to identify and flag records that no longer exist in the source.</p> <p>Recipe 1: SQL Compare (Recommended for HWM) <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n</code></pre></p> <p>Recipe 2: Snapshot Diff (For Full Snapshot Sources) Use ONLY with full snapshot ingestion, NOT with HWM incremental. Requires <code>connection</code> and <code>path</code> to specify the target Delta table for comparison. <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: snapshot_diff\n        keys: [customer_id]\n        connection: silver_conn    # Required: connection to target Delta table\n        path: \"silver/customers\"   # Required: path to target Delta table\n</code></pre></p> <p>Recipe 3: Conservative Threshold <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: erp\n        source_table: dbo.Customers\n        max_delete_percent: 20.0\n        on_threshold_breach: error\n</code></pre></p> <p>Recipe 4: Hard Delete (Remove Rows) <pre><code>transform:\n  steps:\n    - operation: detect_deletes\n      params:\n        mode: sql_compare\n        keys: [customer_id]\n        source_connection: azure_sql\n        source_table: dbo.Customers\n        soft_delete_col: null  # removes rows instead of flagging\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | mode | DeleteDetectionMode | No | <code>DeleteDetectionMode.NONE</code> | Delete detection strategy: none, snapshot_diff, sql_compare | | keys | List[str] | No | <code>PydanticUndefined</code> | Business key columns for comparison | | connection | Optional[str] | No | - | For snapshot_diff: connection name to target Delta table (required for snapshot_diff) | | path | Optional[str] | No | - | For snapshot_diff: path to target Delta table (required for snapshot_diff) | | soft_delete_col | Optional[str] | No | <code>_is_deleted</code> | Column to flag deletes (True = deleted). Set to null for hard-delete (removes rows). | | source_connection | Optional[str] | No | - | For sql_compare: connection name to query live source | | source_table | Optional[str] | No | - | For sql_compare: table to query for current keys | | source_query | Optional[str] | No | - | For sql_compare: custom SQL query for keys (overrides source_table) | | snapshot_column | Optional[str] | No | - | For snapshot_diff on non-Delta: column to identify snapshots. If None, uses Delta time travel (default). | | on_first_run | FirstRunBehavior | No | <code>FirstRunBehavior.SKIP</code> | Behavior when no previous version exists for snapshot_diff | | max_delete_percent | Optional[float] | No | <code>50.0</code> | Safety threshold: warn/error if more than X% of rows would be deleted | | on_threshold_breach | ThresholdBreachAction | No | <code>ThresholdBreachAction.WARN</code> | Behavior when delete percentage exceeds max_delete_percent |</p>"},{"location":"reference/yaml_schema_v1/#validationconfig","title":"<code>ValidationConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for data validation (post-transform checks).</p> <p>When to Use: Output data quality checks that run after transformation but before writing.</p> <p>See Also: Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)</p>"},{"location":"reference/yaml_schema_v1/#the-indestructible-pipeline-pattern","title":"\ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern","text":"<p>Business Problem: \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it before it lands.\"</p> <p>The Solution: A Quality Gate that runs after transformation but before writing.</p> <p>Recipe: The Quality Gate <pre><code>validation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges &amp; Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at &lt;= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n</code></pre></p> <p>Recipe: Quarantine + Gate <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | mode | ValidationAction | No | <code>ValidationAction.FAIL</code> | Execution mode: 'fail' (stop pipeline) or 'warn' (log only) | | on_fail | OnFailAction | No | <code>OnFailAction.ALERT</code> | Action on failure: 'alert' (send notification) or 'ignore' | | tests | List[TestConfig] | No | <code>PydanticUndefined</code> | List of validation testsOptions: NotNullTest, UniqueTest, AcceptedValuesTest, RowCountTest, CustomSQLTest, RangeTest, RegexMatchTest, VolumeDropTest, SchemaContract, DistributionContract, FreshnessContract | | quarantine | Optional[QuarantineConfig] | No | - | Quarantine configuration for failed rows | | gate | Optional[GateConfig] | No | - | Quality gate configuration for batch-level validation | | fail_fast | bool | No | <code>False</code> | Stop validation on first failure. Skips remaining tests for faster feedback. | | cache_df | bool | No | <code>False</code> | Cache DataFrame before validation (Spark only). Improves performance with many tests. |</p>"},{"location":"reference/yaml_schema_v1/#quarantineconfig","title":"<code>QuarantineConfig</code>","text":"<p>Used in: ValidationConfig</p> <p>Configuration for quarantine table routing.</p> <p>When to Use: Capture invalid records for review/reprocessing instead of failing the pipeline.</p> <p>See Also: Quarantine Guide, ValidationConfig</p> <p>Routes rows that fail validation tests to a quarantine table with rejection metadata for later analysis/reprocessing.</p> <p>Example: <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n    max_rows: 10000\n    sample_fraction: 0.1\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection for quarantine writes | | path | Optional[str] | No | - | Path for quarantine data | | table | Optional[str] | No | - | Table name for quarantine | | add_columns | QuarantineColumnsConfig | No | <code>PydanticUndefined</code> | Metadata columns to add to quarantined rows | | retention_days | Optional[int] | No | <code>90</code> | Days to retain quarantined data (auto-cleanup) | | max_rows | Optional[int] | No | - | Maximum number of rows to quarantine per run. Limits storage for high-failure batches. | | sample_fraction | Optional[float] | No | - | Sample fraction of invalid rows to quarantine (0.0-1.0). Use for sampling large invalid sets. |</p>"},{"location":"reference/yaml_schema_v1/#quarantinecolumnsconfig","title":"<code>QuarantineColumnsConfig</code>","text":"<p>Used in: QuarantineConfig</p> <p>Columns added to quarantined rows for debugging and reprocessing.</p> <p>Example: <pre><code>quarantine:\n  connection: silver\n  path: customers_quarantine\n  add_columns:\n    _rejection_reason: true\n    _rejected_at: true\n    _source_batch_id: true\n    _failed_tests: true\n    _original_node: false\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | rejection_reason | bool | No | <code>True</code> | Add _rejection_reason column with test failure description | | rejected_at | bool | No | <code>True</code> | Add _rejected_at column with UTC timestamp | | source_batch_id | bool | No | <code>True</code> | Add _source_batch_id column with run ID for traceability | | failed_tests | bool | No | <code>True</code> | Add _failed_tests column with comma-separated list of failed test names | | original_node | bool | No | <code>False</code> | Add _original_node column with source node name |</p>"},{"location":"reference/yaml_schema_v1/#gateconfig","title":"<code>GateConfig</code>","text":"<p>Used in: ValidationConfig</p> <p>Quality gate configuration for batch-level validation.</p> <p>When to Use: Pipeline-level pass/fail thresholds, row count limits, change detection.</p> <p>See Also: Quality Gates, ValidationConfig</p> <p>Gates evaluate the entire batch before writing, ensuring data quality thresholds are met.</p> <p>Example: <pre><code>gate:\n  require_pass_rate: 0.95\n  on_fail: abort\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n  row_count:\n    min: 100\n    change_threshold: 0.5\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | require_pass_rate | float | No | <code>0.95</code> | Minimum percentage of rows passing ALL tests | | on_fail | GateOnFail | No | <code>GateOnFail.ABORT</code> | Action when gate fails | | thresholds | List[GateThreshold] | No | <code>PydanticUndefined</code> | Per-test thresholds (overrides global require_pass_rate) | | row_count | Optional[RowCountGate] | No | - | Row count anomaly detection |</p>"},{"location":"reference/yaml_schema_v1/#gatethreshold","title":"<code>GateThreshold</code>","text":"<p>Used in: GateConfig</p> <p>Per-test threshold configuration for quality gates.</p> <p>Allows setting different pass rate requirements for specific tests.</p> <p>Example: <pre><code>gate:\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n    - test: unique\n      min_pass_rate: 1.0\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | test | str | Yes | - | Test name or type to apply threshold to | | min_pass_rate | float | Yes | - | Minimum pass rate required (0.0-1.0, e.g., 0.99 = 99%) |</p>"},{"location":"reference/yaml_schema_v1/#rowcountgate","title":"<code>RowCountGate</code>","text":"<p>Used in: GateConfig</p> <p>Row count anomaly detection for quality gates.</p> <p>Validates that batch size falls within expected bounds and detects significant changes from previous runs.</p> <p>Example: <pre><code>gate:\n  row_count:\n    min: 100\n    max: 1000000\n    change_threshold: 0.5\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | min | Optional[int] | No | - | Minimum expected row count | | max | Optional[int] | No | - | Maximum expected row count | | change_threshold | Optional[float] | No | - | Max allowed change vs previous run (e.g., 0.5 = 50% change triggers failure) |</p>"},{"location":"reference/yaml_schema_v1/#writeconfig","title":"<code>WriteConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for writing data from a node.</p> <p>When to Use: Any node that persists data to storage.</p> <p>Key Concepts: - <code>mode</code>: How to handle existing data (overwrite, append, upsert) - <code>keys</code>: Required for upsert mode - columns that identify unique records - <code>partition_by</code>: Columns to partition output by (improves query performance)</p> <p>See Also: - Performance Tuning - Partitioning strategies</p>"},{"location":"reference/yaml_schema_v1/#big-data-performance-guide","title":"\ud83d\ude80 \"Big Data Performance\" Guide","text":"<p>Business Problem: \"My dashboards are slow because the query scans terabytes of data just to find one day's sales.\"</p> <p>The Solution: Use Partitioning for coarse filtering (skipping huge chunks) and Z-Ordering for fine-grained skipping (colocating related data).</p> <p>Recipe: Lakehouse Optimized <pre><code>write:\n  connection: \"gold_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  mode: \"append\"\n\n  # 1. Partitioning: Physical folders.\n  # Use for low-cardinality columns often used in WHERE clauses.\n  # WARNING: Do NOT partition by high-cardinality cols like ID or Timestamp!\n  partition_by: [\"country_code\", \"txn_year_month\"]\n\n  # 2. Z-Ordering: Data clustering.\n  # Use for high-cardinality columns often used in JOINs or predicates.\n  zorder_by: [\"customer_id\", \"product_id\"]\n\n  # 3. Table Properties: Engine tuning.\n  table_properties:\n    \"delta.autoOptimize.optimizeWrite\": \"true\"\n    \"delta.autoOptimize.autoCompact\": \"true\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name from project.yaml | | format | ReadFormat | str | Yes | - | Output format (csv, parquet, delta, etc.) | | table | Optional[str] | No | - | Table name for SQL/Delta | | path | Optional[str] | No | - | Path for file-based outputs | | register_table | Optional[str] | No | - | Register file output as external table (Spark/Delta only) | | mode | WriteMode | No | <code>WriteMode.OVERWRITE</code> | Write mode. Options: 'overwrite', 'append', 'upsert', 'append_once' | | partition_by | List[str] | No | <code>PydanticUndefined</code> | List of columns to physically partition the output by (folder structure). Use for low-cardinality columns (e.g. date, country). | | zorder_by | List[str] | No | <code>PydanticUndefined</code> | List of columns to Z-Order by. Improves read performance for high-cardinality columns used in filters/joins (Delta only). | | table_properties | Dict[str, str] | No | <code>PydanticUndefined</code> | Delta table properties. Overrides global performance.delta_table_properties. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. | | merge_schema | bool | No | <code>False</code> | Allow schema evolution (mergeSchema option in Delta) | | first_run_query | Optional[str] | No | - | SQL query for full-load on first run (High Water Mark pattern). If set, uses this query when target table doesn't exist, then switches to incremental. Only applies to SQL reads. | | options | Dict[str, Any] | No | <code>PydanticUndefined</code> | Format-specific options | | auto_optimize | bool | AutoOptimizeConfig | No | - | Auto-run OPTIMIZE and VACUUM after write (Delta only) | | add_metadata | bool | WriteMetadataConfig | No | - | Add metadata columns for Bronze layer lineage. Set to <code>true</code> to add all applicable columns, or provide a WriteMetadataConfig for selective columns. Columns: _extracted_at, _source_file (file sources), _source_connection, _source_table (SQL sources). | | skip_if_unchanged | bool | No | <code>False</code> | Skip write if DataFrame content is identical to previous write. Computes SHA256 hash of entire DataFrame and compares to stored hash in Delta table metadata. Useful for snapshot tables without timestamps to avoid redundant appends. Only supported for Delta format. | | skip_hash_columns | Optional[List[str]] | No | - | Columns to include in hash computation for skip_if_unchanged. If None, all columns are used. Specify a subset to ignore volatile columns like timestamps. | | skip_hash_sort_columns | Optional[List[str]] | No | - | Columns to sort by before hashing for deterministic comparison. Required if row order may vary between runs. Typically your business key columns. | | streaming | Optional[StreamingWriteConfig] | No | - | Streaming write configuration for Spark Structured Streaming. When set, uses writeStream instead of batch write. Requires a streaming DataFrame from a streaming read source. | | merge_keys | Optional[List[str]] | No | - | Key columns for SQL Server MERGE operations. Required when mode='merge'. These columns form the ON clause of the MERGE statement. | | merge_options | Optional[SqlServerMergeOptions] | No | - | Options for SQL Server MERGE operations (conditions, staging, audit cols) | | overwrite_options | Optional[SqlServerOverwriteOptions] | No | - | Options for SQL Server overwrite operations (strategy, audit cols) |</p>"},{"location":"reference/yaml_schema_v1/#writemetadataconfig","title":"<code>WriteMetadataConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for metadata columns added during Bronze writes.</p>"},{"location":"reference/yaml_schema_v1/#bronze-metadata-guide","title":"\ud83d\udccb Bronze Metadata Guide","text":"<p>Business Problem: \"We need lineage tracking and debugging info for our Bronze layer data.\"</p> <p>The Solution: Add metadata columns during ingestion for traceability.</p> <p>Recipe 1: Add All Metadata (Recommended) <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n</code></pre></p> <p>Recipe 2: Selective Metadata <pre><code>write:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: false\n    source_table: false\n</code></pre></p> <p>Available Columns: - <code>_extracted_at</code>: Pipeline execution timestamp (all sources) - <code>_source_file</code>: Source filename/path (file sources only) - <code>_source_connection</code>: Connection name used (all sources) - <code>_source_table</code>: Table or query name (SQL sources only) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | extracted_at | bool | No | <code>True</code> | Add _extracted_at column with pipeline execution timestamp | | source_file | bool | No | <code>True</code> | Add _source_file column with source filename (file sources only) | | source_connection | bool | No | <code>False</code> | Add _source_connection column with connection name | | source_table | bool | No | <code>False</code> | Add _source_table column with table/query name (SQL sources only) |</p>"},{"location":"reference/yaml_schema_v1/#streamingwriteconfig","title":"<code>StreamingWriteConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Spark Structured Streaming writes.</p>"},{"location":"reference/yaml_schema_v1/#real-time-pipeline-guide","title":"\ud83d\ude80 \"Real-Time Pipeline\" Guide","text":"<p>Business Problem: \"I need to process data continuously as it arrives from Kafka/Event Hubs and write it to Delta Lake in near real-time.\"</p> <p>The Solution: Configure streaming write with checkpoint location for fault tolerance and trigger interval for processing frequency.</p> <p>Recipe: Streaming Ingestion <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_stream\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_stream\"\n    trigger:\n      processing_time: \"10 seconds\"\n</code></pre></p> <p>Recipe: One-Time Streaming (Batch-like) <pre><code>write:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_batch\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_batch\"\n    trigger:\n      available_now: true\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | output_mode | Literal['append', 'update', 'complete'] | No | <code>append</code> | Output mode for streaming writes. 'append' - Only new rows. 'update' - Updated rows only. 'complete' - Entire result table (requires aggregation). | | checkpoint_location | str | Yes | - | Path for streaming checkpoints. Required for fault tolerance. Must be a reliable storage location (e.g., cloud storage, DBFS). | | trigger | Optional[TriggerConfig] | No | - | Trigger configuration. If not specified, processes data as fast as possible. Use 'processing_time' for micro-batch intervals, 'once' for single batch, 'available_now' for processing all available data then stopping. | | query_name | Optional[str] | No | - | Name for the streaming query (useful for monitoring and debugging) | | await_termination | Optional[bool] | No | <code>False</code> | Wait for the streaming query to terminate. Set to True for batch-like streaming with 'once' or 'available_now' triggers. | | timeout_seconds | Optional[int] | No | - | Timeout in seconds when await_termination is True. If None, waits indefinitely. |</p>"},{"location":"reference/yaml_schema_v1/#triggerconfig","title":"<code>TriggerConfig</code>","text":"<p>Used in: StreamingWriteConfig</p> <p>Configuration for streaming trigger intervals.</p> <p>Specify exactly one of the trigger options.</p> <p>Example: <pre><code>trigger:\n  processing_time: \"10 seconds\"\n</code></pre></p> <p>Or for one-time processing: <pre><code>trigger:\n  once: true\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | processing_time | Optional[str] | No | - | Trigger interval as duration string (e.g., '10 seconds', '1 minute') | | once | Optional[bool] | No | - | Process all available data once and stop | | available_now | Optional[bool] | No | - | Process all available data in multiple batches, then stop | | continuous | Optional[str] | No | - | Continuous processing with checkpoint interval (e.g., '1 second') |</p>"},{"location":"reference/yaml_schema_v1/#autooptimizeconfig","title":"<code>AutoOptimizeConfig</code>","text":"<p>Used in: WriteConfig</p> <p>Configuration for Delta Lake automatic optimization.</p> <p>Example: <pre><code>auto_optimize:\n  enabled: true\n  vacuum_retention_hours: 168\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | enabled | bool | No | <code>True</code> | Enable auto optimization | | vacuum_retention_hours | int | No | <code>168</code> | Hours to retain history for VACUUM (default 7 days). Set to 0 to disable VACUUM. |</p>"},{"location":"reference/yaml_schema_v1/#privacyconfig","title":"<code>PrivacyConfig</code>","text":"<p>Used in: NodeConfig</p> <p>Configuration for PII anonymization.</p>"},{"location":"reference/yaml_schema_v1/#privacy-pii-protection","title":"\ud83d\udd10 Privacy &amp; PII Protection","text":"<p>How It Works: 1. Mark columns as <code>pii: true</code> in the <code>columns</code> metadata 2. Configure a <code>privacy</code> block with the anonymization method 3. During node execution, all columns marked as PII (and inherited from dependencies) are anonymized 4. Upstream PII markings are inherited by downstream nodes</p> <p>Example: <pre><code>columns:\n  customer_email:\n    pii: true  # Mark as PII\n  customer_id:\n    pii: false\n\nprivacy:\n  method: hash       # hash, mask, or redact\n  salt: \"secret_key\" # Optional: makes hash unique/secure\n  declassify: []     # Remove columns from PII protection\n</code></pre></p> <p>Methods: - <code>hash</code>: SHA256 hash (length 64). With salt, prevents pre-computed rainbow tables. - <code>mask</code>: Show only last 4 chars, replace rest with <code>*</code>. Example: <code>john@email.com</code> \u2192 <code>****@email.com</code> - <code>redact</code>: Replace entire value with <code>[REDACTED]</code></p> <p>Important: - <code>pii: true</code> alone does NOTHING. You must set a <code>privacy.method</code> to actually mask data. - PII inheritance: If dependency outputs PII columns, this node inherits them unless declassified. - Salt is optional but recommended for hash to prevent attacks. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | method | PrivacyMethod | Yes | - | Anonymization method: 'hash' (SHA256), 'mask' (show last 4), or 'redact' ([REDACTED]) | | salt | Optional[str] | No | - | Salt for hashing (optional but recommended). Appended before hashing to create unique hashes. Example: 'company_secret_key_2025' | | declassify | List[str] | No | <code>PydanticUndefined</code> | List of columns to remove from PII protection (stops inheritance from upstream). Example: ['customer_id'] |</p>"},{"location":"reference/yaml_schema_v1/#sqlserverauditcolsconfig","title":"<code>SqlServerAuditColsConfig</code>","text":"<p>Used in: SqlServerMergeOptions, SqlServerOverwriteOptions</p> <p>Audit column configuration for SQL Server merge operations.</p> <p>These columns are automatically populated with GETUTCDATE() during merge: - <code>created_col</code>: Set on INSERT only - <code>updated_col</code>: Set on INSERT and UPDATE</p> <p>Example: <pre><code>audit_cols:\n  created_col: created_ts\n  updated_col: updated_ts\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | created_col | Optional[str] | No | - | Column name for creation timestamp (set on INSERT) | | updated_col | Optional[str] | No | - | Column name for update timestamp (set on INSERT and UPDATE) |</p>"},{"location":"reference/yaml_schema_v1/#sqlservermergeoptions","title":"<code>SqlServerMergeOptions</code>","text":"<p>Used in: WriteConfig</p> <p>Options for SQL Server MERGE operations (Phase 1).</p> <p>Enables incremental sync from Spark to SQL Server using T-SQL MERGE. Data is written to a staging table, then merged into the target.</p>"},{"location":"reference/yaml_schema_v1/#basic-usage","title":"Basic Usage","text":"<pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: sales.fact_orders\n  mode: merge\n  merge_keys: [DateId, store_id]\n  merge_options:\n    update_condition: \"source._hash_diff != target._hash_diff\"\n    exclude_columns: [_hash_diff]\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre>"},{"location":"reference/yaml_schema_v1/#conditions","title":"Conditions","text":"<ul> <li><code>update_condition</code>: Only update rows matching this condition (e.g., hash diff)</li> <li><code>delete_condition</code>: Delete rows matching this condition (soft delete pattern)</li> <li><code>insert_condition</code>: Only insert rows matching this condition | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | update_condition | Optional[str] | No | - | SQL condition for WHEN MATCHED UPDATE. Use 'source.' and 'target.' prefixes. Example: 'source._hash_diff != target._hash_diff' | | delete_condition | Optional[str] | No | - | SQL condition for WHEN MATCHED DELETE. Example: 'source._is_deleted = 1' | | insert_condition | Optional[str] | No | - | SQL condition for WHEN NOT MATCHED INSERT. Example: 'source.is_valid = 1' | | exclude_columns | List[str] | No | <code>PydanticUndefined</code> | Columns to exclude from MERGE (not written to target table) | | staging_schema | str | No | <code>staging</code> | Schema for staging table. Table name: {staging_schema}.{table}_staging | | audit_cols | Optional[SqlServerAuditColsConfig] | No | - | Audit columns for created/updated timestamps | | validations | Optional[ForwardRef('SqlServerMergeValidationConfig')] | No | - | Validation checks before merge (null keys, duplicate keys) | | auto_create_schema | bool | No | <code>False</code> | Auto-create schema if it doesn't exist (Phase 4). Runs CREATE SCHEMA IF NOT EXISTS. | | auto_create_table | bool | No | <code>False</code> | Auto-create target table if it doesn't exist (Phase 4). Infers schema from DataFrame. | | schema_evolution | Optional[ForwardRef('SqlServerSchemaEvolutionConfig')] | No | - | Schema evolution configuration (Phase 4). Controls handling of schema differences. | | batch_size | Optional[int] | No | - | Batch size for staging table writes (Phase 4). Chunks large DataFrames for memory efficiency. | | primary_key_on_merge_keys | bool | No | <code>False</code> | Create a clustered primary key on merge_keys when auto-creating table. Enforces uniqueness. | | index_on_merge_keys | bool | No | <code>False</code> | Create a nonclustered index on merge_keys. Use if primary key already exists elsewhere. | | incremental | bool | No | <code>False</code> | Enable incremental merge optimization. When True, reads target table's keys and hashes to determine which rows changed, then only writes changed rows to staging. Significantly faster when few rows change between runs. | | hash_column | Optional[str] | No | - | Name of pre-computed hash column in DataFrame for change detection. Used when incremental=True. If not specified, will auto-detect '_hash_diff' column. | | change_detection_columns | Optional[List[str]] | No | - | Columns to use for computing change detection hash. Used when incremental=True and no hash_column is specified. If None, uses all non-key columns. |</li> </ul>"},{"location":"reference/yaml_schema_v1/#sqlservermergevalidationconfig","title":"<code>SqlServerMergeValidationConfig</code>","text":"<p>Used in: SqlServerMergeOptions, SqlServerOverwriteOptions</p> <p>Validation configuration for SQL Server merge/overwrite operations.</p> <p>Validates source data before writing to SQL Server.</p> <p>Example: <pre><code>merge_options:\n  validations:\n    check_null_keys: true\n    check_duplicate_keys: true\n    fail_on_validation_error: true\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | check_null_keys | bool | No | <code>True</code> | Fail if merge_keys contain NULL values | | check_duplicate_keys | bool | No | <code>True</code> | Fail if merge_keys have duplicate combinations | | fail_on_validation_error | bool | No | <code>True</code> | If False, log warning instead of failing on validation errors |</p>"},{"location":"reference/yaml_schema_v1/#sqlserveroverwriteoptions","title":"<code>SqlServerOverwriteOptions</code>","text":"<p>Used in: WriteConfig</p> <p>Options for SQL Server overwrite operations (Phase 2).</p> <p>Enhanced overwrite with multiple strategies for different use cases.</p>"},{"location":"reference/yaml_schema_v1/#strategies","title":"Strategies","text":"<ul> <li><code>truncate_insert</code>: TRUNCATE TABLE then INSERT (fastest, requires TRUNCATE permission)</li> <li><code>drop_create</code>: DROP TABLE, CREATE TABLE, INSERT (refreshes schema)</li> <li><code>delete_insert</code>: DELETE FROM then INSERT (works with limited permissions)</li> </ul>"},{"location":"reference/yaml_schema_v1/#example","title":"Example","text":"<p><pre><code>write:\n  connection: azure_sql\n  format: sql_server\n  table: fact.combined_downtime\n  mode: overwrite\n  overwrite_options:\n    strategy: truncate_insert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | strategy | SqlServerOverwriteStrategy | No | <code>SqlServerOverwriteStrategy.TRUNCATE_INSERT</code> | Overwrite strategy: truncate_insert, drop_create, delete_insert | | audit_cols | Optional[SqlServerAuditColsConfig] | No | - | Audit columns for created/updated timestamps | | validations | Optional[SqlServerMergeValidationConfig] | No | - | Validation checks before overwrite | | auto_create_schema | bool | No | <code>False</code> | Auto-create schema if it doesn't exist (Phase 4). Runs CREATE SCHEMA IF NOT EXISTS. | | auto_create_table | bool | No | <code>False</code> | Auto-create target table if it doesn't exist (Phase 4). Infers schema from DataFrame. | | schema_evolution | Optional[SqlServerSchemaEvolutionConfig] | No | - | Schema evolution configuration (Phase 4). Controls handling of schema differences. | | batch_size | Optional[int] | No | - | Batch size for writes (Phase 4). Chunks large DataFrames for memory efficiency. |</p>"},{"location":"reference/yaml_schema_v1/#sqlserverschemaevolutionconfig","title":"<code>SqlServerSchemaEvolutionConfig</code>","text":"<p>Used in: SqlServerMergeOptions, SqlServerOverwriteOptions</p> <p>Schema evolution configuration for SQL Server operations (Phase 4).</p> <p>Controls automatic schema changes when DataFrame schema differs from target table.</p> <p>Example: <pre><code>merge_options:\n  schema_evolution:\n    mode: evolve\n    add_columns: true\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | mode | SqlServerSchemaEvolutionMode | No | <code>SqlServerSchemaEvolutionMode.STRICT</code> | Schema evolution mode: strict (fail), evolve (add columns), ignore (skip mismatched) | | add_columns | bool | No | <code>False</code> | If mode='evolve', automatically add new columns via ALTER TABLE ADD COLUMN |</p>"},{"location":"reference/yaml_schema_v1/#transformstep","title":"<code>TransformStep</code>","text":"<p>Used in: TransformConfig</p> <p>Single transformation step.</p> <p>Supports four step types (exactly one required):</p> <ul> <li><code>sql</code> - Inline SQL query string</li> <li><code>sql_file</code> - Path to external .sql file (relative to the YAML file defining the node)</li> <li><code>function</code> - Registered Python function name</li> <li><code>operation</code> - Built-in operation (e.g., drop_duplicates)</li> </ul> <p>sql_file Example:</p> <p>If your project structure is: <pre><code>project.yaml              # imports pipelines/silver/silver.yaml\npipelines/\n  silver/\n    silver.yaml           # defines the node\n    sql/\n      transform.sql       # your SQL file\n</code></pre></p> <p>In <code>silver.yaml</code>, use a path relative to <code>silver.yaml</code>: <pre><code>transform:\n  steps:\n    - sql_file: sql/transform.sql   # relative to silver.yaml\n</code></pre></p> <p>Important: The path is resolved relative to the YAML file where the node is defined, NOT the project.yaml that imports it. Do NOT use absolute paths like <code>/pipelines/silver/sql/...</code>. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | sql | Optional[str] | No | - | Inline SQL query. Use <code>df</code> to reference the current DataFrame. | | sql_file | Optional[str] | No | - | Path to external .sql file, relative to the YAML file defining the node. Example: 'sql/transform.sql' resolves relative to the node's source YAML. | | function | Optional[str] | No | - | Name of a registered Python function (@transform or @register). | | operation | Optional[str] | No | - | Built-in operation name (e.g., drop_duplicates, fill_na). | | params | Dict[str, Any] | No | <code>PydanticUndefined</code> | Parameters to pass to function or operation. |</p>"},{"location":"reference/yaml_schema_v1/#contracts-data-quality-gates","title":"Contracts (Data Quality Gates)","text":""},{"location":"reference/yaml_schema_v1/#contracts-pre-transform-checks","title":"Contracts (Pre-Transform Checks)","text":"<p>Contracts are fail-fast data quality checks that run on input data before transformation. They always halt execution on failure - use them to prevent bad data from entering the pipeline.</p> <p>Contracts vs Validation vs Quality Gates:</p> Feature When it Runs On Failure Use Case Contracts Before transform Always fails Input data quality (not-null, unique keys) Validation After transform Configurable (fail/warn/quarantine) Output data quality (ranges, formats) Quality Gates After validation Configurable (abort/warn) Pipeline-level thresholds (pass rate, row counts) Quarantine With validation Routes bad rows Capture invalid records for review <p>See Also: - Validation Guide - Full validation configuration - Quarantine Guide - Quarantine setup and review - Getting Started: Validation</p> <p>Example: <pre><code>- name: \"process_orders\"\n  contracts:\n    - type: not_null\n      columns: [order_id, customer_id]\n    - type: row_count\n      min: 100\n    - type: freshness\n      column: created_at\n      max_age: \"24h\"\n  read:\n    source: raw_orders\n</code></pre></p>"},{"location":"reference/yaml_schema_v1/#acceptedvaluestest","title":"<code>AcceptedValuesTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures a column only contains values from an allowed list.</p> <p>When to Use: Enum-like fields, status columns, categorical data validation.</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected]\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['accepted_values'] | No | <code>TestType.ACCEPTED_VALUES</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | column | str | Yes | - | Column to check | | values | List[Any] | Yes | - | Allowed values |</p>"},{"location":"reference/yaml_schema_v1/#customsqltest","title":"<code>CustomSQLTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Runs a custom SQL condition and fails if too many rows violate it.</p> <p><pre><code>contracts:\n  - type: custom_sql\n    condition: \"amount &gt; 0\"\n    threshold: 0.01  # Allow up to 1% failures\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['custom_sql'] | No | <code>TestType.CUSTOM_SQL</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | condition | str | Yes | - | SQL condition that should be true for valid rows | | threshold | float | No | <code>0.0</code> | Failure rate threshold (0.0 = strictly no failures allowed) |</p>"},{"location":"reference/yaml_schema_v1/#distributioncontract","title":"<code>DistributionContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if a column's statistical distribution is within expected bounds.</p> <p>When to Use: Detect data drift, anomaly detection, statistical monitoring.</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \"&gt;100\"  # Mean must be &gt; 100\n    on_fail: warn\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['distribution'] | No | <code>TestType.DISTRIBUTION</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.WARN</code> | - | | column | str | Yes | - | Column to analyze | | metric | Literal['mean', 'min', 'max', 'null_percentage'] | Yes | - | Statistical metric to check | | threshold | str | Yes | - | Threshold expression (e.g., '&gt;100', '&lt;0.05') |</p>"},{"location":"reference/yaml_schema_v1/#freshnesscontract","title":"<code>FreshnessContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that data is not stale by checking a timestamp column.</p> <p>When to Use: Source systems that should update regularly, SLA monitoring.</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['freshness'] | No | <code>TestType.FRESHNESS</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | - | | column | str | No | <code>updated_at</code> | Timestamp column to check | | max_age | str | Yes | - | Maximum allowed age (e.g., '24h', '7d') |</p>"},{"location":"reference/yaml_schema_v1/#notnulltest","title":"<code>NotNullTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns contain no NULL values.</p> <p>When to Use: Primary keys, required fields, foreign keys that must resolve.</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['not_null'] | No | <code>TestType.NOT_NULL</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | columns | List[str] | Yes | - | Columns that must not contain nulls |</p>"},{"location":"reference/yaml_schema_v1/#rangetest","title":"<code>RangeTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values fall within a specified range.</p> <p>When to Use: Numeric bounds validation (ages, prices, quantities), date ranges.</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['range'] | No | <code>TestType.RANGE</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | column | str | Yes | - | Column to check | | min | int | float | str | No | - | Minimum value (inclusive) | | max | int | float | str | No | - | Maximum value (inclusive) |</p>"},{"location":"reference/yaml_schema_v1/#regexmatchtest","title":"<code>RegexMatchTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures column values match a regex pattern.</p> <p>When to Use: Format validation (emails, phone numbers, IDs, codes).</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['regex_match'] | No | <code>TestType.REGEX_MATCH</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | column | str | Yes | - | Column to check | | pattern | str | Yes | - | Regex pattern to match |</p>"},{"location":"reference/yaml_schema_v1/#rowcounttest","title":"<code>RowCountTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that row count falls within expected bounds.</p> <p>When to Use: Ensure minimum data completeness, detect truncated loads, cap batch sizes.</p> <p>See Also: Contracts Overview, GateConfig</p> <p><pre><code>contracts:\n  - type: row_count\n    min: 1000\n    max: 100000\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['row_count'] | No | <code>TestType.ROW_COUNT</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | min | Optional[int] | No | - | Minimum row count | | max | Optional[int] | No | - | Maximum row count |</p>"},{"location":"reference/yaml_schema_v1/#schemacontract","title":"<code>SchemaContract</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Validates that the DataFrame schema matches expected columns.</p> <p>When to Use: Enforce schema stability, detect upstream schema drift, ensure column presence.</p> <p>See Also: Contracts Overview, SchemaPolicyConfig</p> <p>Uses the <code>columns</code> metadata from NodeConfig to verify schema.</p> <p><pre><code>contracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['schema'] | No | <code>TestType.SCHEMA</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | - | | strict | bool | No | <code>True</code> | If true, fail on unexpected columns |</p>"},{"location":"reference/yaml_schema_v1/#uniquetest","title":"<code>UniqueTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Ensures specified columns (or combination) contain unique values.</p> <p>When to Use: Primary keys, natural keys, deduplication verification.</p> <p>See Also: Contracts Overview</p> <p><pre><code>contracts:\n  - type: unique\n    columns: [order_id]  # Single column\n  # OR composite key:\n  - type: unique\n    columns: [customer_id, order_date]  # Composite uniqueness\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['unique'] | No | <code>TestType.UNIQUE</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | columns | List[str] | Yes | - | Columns that must be unique (composite key if multiple) |</p>"},{"location":"reference/yaml_schema_v1/#volumedroptest","title":"<code>VolumeDropTest</code>","text":"<p>Used in: NodeConfig, ValidationConfig</p> <p>Checks if row count dropped significantly compared to history.</p> <p>When to Use: Detect source outages, partial loads, or data pipeline issues.</p> <p>See Also: Contracts Overview, RowCountTest</p> <p>Formula: <code>(current - avg) / avg &lt; -threshold</code></p> <p><pre><code>contracts:\n  - type: volume_drop\n    threshold: 0.5  # Fail if &gt; 50% drop from 7-day average\n    lookback_days: 7\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | Literal['volume_drop'] | No | <code>TestType.VOLUME_DROP</code> | - | | name | Optional[str] | No | - | Optional name for the check | | on_fail | ContractSeverity | No | <code>ContractSeverity.FAIL</code> | Action on failure | | threshold | float | No | <code>0.5</code> | Max allowed drop (0.5 = 50% drop) | | lookback_days | int | No | <code>7</code> | Days of history to average |</p>"},{"location":"reference/yaml_schema_v1/#global-settings","title":"Global Settings","text":""},{"location":"reference/yaml_schema_v1/#lineageconfig","title":"<code>LineageConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for OpenLineage integration.</p> <p>Example: <pre><code>lineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | url | Optional[str] | No | - | OpenLineage API URL | | namespace | str | No | <code>odibi</code> | Namespace for jobs | | api_key | Optional[str] | No | - | API Key |</p>"},{"location":"reference/yaml_schema_v1/#alertconfig","title":"<code>AlertConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Configuration for alerts with throttling support.</p> <p>Supports Slack, Teams, and generic webhooks with event-specific payloads.</p> <p>Available Events: - <code>on_start</code> - Pipeline started - <code>on_success</code> - Pipeline completed successfully - <code>on_failure</code> - Pipeline failed - <code>on_quarantine</code> - Rows were quarantined - <code>on_gate_block</code> - Quality gate blocked the pipeline - <code>on_threshold_breach</code> - A threshold was exceeded</p> <p>Example: <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | type | AlertType | Yes | - | - | | url | str | Yes | - | Webhook URL | | on_events | List[AlertEvent] | No | <code>[&lt;AlertEvent.ON_FAILURE: 'on_failure'&gt;]</code> | Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach | | metadata | Dict[str, Any] | No | <code>PydanticUndefined</code> | Extra metadata: throttle_minutes, max_per_hour, channel, etc. |</p>"},{"location":"reference/yaml_schema_v1/#loggingconfig","title":"<code>LoggingConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Logging configuration.</p> <p>Example: <pre><code>logging:\n  level: \"INFO\"\n  structured: true\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | level | LogLevel | No | <code>LogLevel.INFO</code> | - | | structured | bool | No | <code>False</code> | Output JSON logs | | metadata | Dict[str, Any] | No | <code>PydanticUndefined</code> | Extra metadata in logs |</p>"},{"location":"reference/yaml_schema_v1/#performanceconfig","title":"<code>PerformanceConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Performance tuning configuration.</p> <p>Example: <pre><code>performance:\n  use_arrow: true\n  spark_config:\n    \"spark.sql.shuffle.partitions\": \"200\"\n    \"spark.sql.adaptive.enabled\": \"true\"\n    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\"\n  delta_table_properties:\n    \"delta.columnMapping.mode\": \"name\"\n</code></pre></p> <p>Spark Config Notes: - Configs are applied via <code>spark.conf.set()</code> at runtime - For existing sessions (e.g., Databricks), only runtime-settable configs will take effect - Session-level configs (e.g., <code>spark.executor.memory</code>) require session restart - Common runtime-safe configs: shuffle partitions, adaptive query execution, Delta optimizations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | use_arrow | bool | No | <code>True</code> | Use Apache Arrow-backed DataFrames (Pandas only). Reduces memory and speeds up I/O. | | spark_config | Dict[str, str] | No | <code>PydanticUndefined</code> | Spark configuration settings applied at runtime via spark.conf.set(). Example: {'spark.sql.shuffle.partitions': '200', 'spark.sql.adaptive.enabled': 'true'}. Note: Some configs require session restart and cannot be set at runtime. | | delta_table_properties | Dict[str, str] | No | <code>PydanticUndefined</code> | Default table properties applied to all Delta writes. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names. | | skip_null_profiling | bool | No | <code>False</code> | Skip null profiling in metadata collection phase. Reduces execution time for large DataFrames by avoiding an additional Spark job. | | skip_catalog_writes | bool | No | <code>False</code> | Skip catalog metadata writes (register_asset, track_schema, log_pattern, record_lineage) after each node write. Significantly improves performance for high-throughput pipelines like Bronze layer ingestion. Set to true when catalog tracking is not needed. | | skip_run_logging | bool | No | <code>False</code> | Skip batch catalog writes at pipeline end (log_runs_batch, register_outputs_batch). Saves 10-20s per pipeline run. Enable when you don't need run history in the catalog. Stories are still generated and contain full execution details. |</p>"},{"location":"reference/yaml_schema_v1/#retryconfig","title":"<code>RetryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Retry configuration.</p> <p>Example: <pre><code>retry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n</code></pre> | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | enabled | bool | No | <code>True</code> | - | | max_attempts | int | No | <code>3</code> | - | | backoff | BackoffStrategy | No | <code>BackoffStrategy.EXPONENTIAL</code> | - |</p>"},{"location":"reference/yaml_schema_v1/#storyconfig","title":"<code>StoryConfig</code>","text":"<p>Used in: ProjectConfig</p> <p>Story generation configuration.</p> <p>Stories are ODIBI's core value - execution reports with lineage. They must use a connection for consistent, traceable output.</p> <p>Example: <pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre></p> <p>Failure Sample Settings: - <code>failure_sample_size</code>: Number of failed rows to capture per validation (default: 100) - <code>max_failure_samples</code>: Total failed rows across all validations (default: 500) - <code>max_sampled_validations</code>: After this many validations, show only counts (default: 5) | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | connection | str | Yes | - | Connection name for story output (uses connection's path resolution) | | path | str | Yes | - | Path for stories (relative to connection base_path) | | max_sample_rows | int | No | <code>10</code> | - | | auto_generate | bool | No | <code>True</code> | - | | retention_days | Optional[int] | No | <code>30</code> | Days to keep stories | | retention_count | Optional[int] | No | <code>100</code> | Max number of stories to keep | | failure_sample_size | int | No | <code>100</code> | Number of failed rows to capture per validation rule | | max_failure_samples | int | No | <code>500</code> | Maximum total failed rows across all validations | | max_sampled_validations | int | No | <code>5</code> | After this many validations, show only counts (no samples) | | async_generation | bool | No | <code>False</code> | Generate stories asynchronously (fire-and-forget). Pipeline returns immediately while story writes in background. Improves multi-pipeline performance by ~5-10s per pipeline. | | generate_lineage | bool | No | <code>True</code> | Generate combined lineage graph from all stories. Creates a unified view of data flow across pipelines. | | docs | Optional[ForwardRef('DocsConfig')] | No | - | Documentation generation settings. Generates README.md, TECHNICAL_DETAILS.md, NODE_CARDS/*.md from Story data. |</p>"},{"location":"reference/yaml_schema_v1/#transformation-reference","title":"Transformation Reference","text":""},{"location":"reference/yaml_schema_v1/#how-to-use-transformers","title":"How to Use Transformers","text":"<p>You can use any transformer in two ways:</p> <p>1. As a Top-Level Transformer (\"The App\") Use this for major operations that define the node's purpose (e.g. Merge, SCD2). <pre><code>- name: \"my_node\"\n  transformer: \"&lt;transformer_name&gt;\"\n  params:\n    &lt;param_name&gt;: &lt;value&gt;\n</code></pre></p> <p>2. As a Step in a Chain (\"The Script\") Use this for smaller operations within a <code>transform</code> block (e.g. clean_text, filter). <pre><code>- name: \"my_node\"\n  transform:\n    steps:\n      - function: \"&lt;transformer_name&gt;\"\n         params:\n           &lt;param_name&gt;: &lt;value&gt;\n</code></pre></p> <p>Available Transformers: The models below describe the <code>params</code> required for each transformer.</p>"},{"location":"reference/yaml_schema_v1/#common-operations","title":"\ud83d\udcc2 Common Operations","text":""},{"location":"reference/yaml_schema_v1/#casewhencase","title":"CaseWhenCase","text":"<p>Back to Catalog</p> Field Type Required Default Description condition str Yes - - value str Yes - -"},{"location":"reference/yaml_schema_v1/#add_prefix-addprefixparams","title":"<code>add_prefix</code> (AddPrefixParams)","text":"<p>Adds a prefix to column names.</p> <p>Configuration for adding a prefix to column names.</p> <p>Example - All columns: <pre><code>add_prefix:\n  prefix: \"src_\"\n</code></pre></p> <p>Example - Specific columns: <pre><code>add_prefix:\n  prefix: \"raw_\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description prefix str Yes - Prefix to add to column names columns Optional[List[str]] No - Columns to prefix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from prefixing"},{"location":"reference/yaml_schema_v1/#add_suffix-addsuffixparams","title":"<code>add_suffix</code> (AddSuffixParams)","text":"<p>Adds a suffix to column names.</p> <p>Configuration for adding a suffix to column names.</p> <p>Example - All columns: <pre><code>add_suffix:\n  suffix: \"_raw\"\n</code></pre></p> <p>Example - Specific columns: <pre><code>add_suffix:\n  suffix: \"_v2\"\n  columns: [\"id\", \"name\", \"value\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description suffix str Yes - Suffix to add to column names columns Optional[List[str]] No - Columns to suffix (default: all columns) exclude Optional[List[str]] No - Columns to exclude from suffixing"},{"location":"reference/yaml_schema_v1/#case_when-casewhenparams","title":"<code>case_when</code> (CaseWhenParams)","text":"<p>Implements structured CASE WHEN logic.</p> <p>Configuration for conditional logic.</p> <p>Example: <pre><code>case_when:\n  output_col: \"age_group\"\n  default: \"'Adult'\"\n  cases:\n    - condition: \"age &lt; 18\"\n      value: \"'Minor'\"\n    - condition: \"age &gt; 65\"\n      value: \"'Senior'\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description cases List[CaseWhenCase] Yes - List of conditional branches default str No <code>NULL</code> Default value if no condition met output_col str Yes - Name of the resulting column"},{"location":"reference/yaml_schema_v1/#cast_columns-castcolumnsparams","title":"<code>cast_columns</code> (CastColumnsParams)","text":"<p>Casts specific columns to new types while keeping others intact.</p> <p>Configuration for column type casting.</p> <p>Example: <pre><code>cast_columns:\n  casts:\n    age: \"int\"\n    salary: \"DOUBLE\"\n    created_at: \"TIMESTAMP\"\n    tags: \"ARRAY&lt;STRING&gt;\"  # Raw SQL types allowed\n</code></pre> Back to Catalog</p> Field Type Required Default Description casts Dict[str, SimpleType | str] Yes - Map of column to target SQL type"},{"location":"reference/yaml_schema_v1/#clean_text-cleantextparams","title":"<code>clean_text</code> (CleanTextParams)","text":"<p>Applies string cleaning operations (Trim/Case) via SQL.</p> <p>Configuration for text cleaning.</p> <p>Example: <pre><code>clean_text:\n  columns: [\"email\", \"username\"]\n  trim: true\n  case: \"lower\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to clean trim bool No <code>True</code> Apply TRIM() case Literal['lower', 'upper', 'preserve'] No <code>preserve</code> Case conversion"},{"location":"reference/yaml_schema_v1/#coalesce_columns-coalescecolumnsparams","title":"<code>coalesce_columns</code> (CoalesceColumnsParams)","text":"<p>Returns the first non-null value from a list of columns. Useful for fallback/priority scenarios.</p> <p>Configuration for coalescing columns (first non-null value).</p> <p>Example - Phone number fallback: <pre><code>coalesce_columns:\n  columns: [\"mobile_phone\", \"work_phone\", \"home_phone\"]\n  output_col: \"primary_phone\"\n</code></pre></p> <p>Example - Timestamp fallback: <pre><code>coalesce_columns:\n  columns: [\"updated_at\", \"modified_at\", \"created_at\"]\n  output_col: \"last_change_at\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to coalesce (in priority order) output_col str Yes - Name of the output column drop_source bool No <code>False</code> Drop the source columns after coalescing"},{"location":"reference/yaml_schema_v1/#concat_columns-concatcolumnsparams","title":"<code>concat_columns</code> (ConcatColumnsParams)","text":"<p>Concatenates multiple columns into one string. NULLs are skipped (treated as empty string) using CONCAT_WS behavior.</p> <p>Configuration for string concatenation.</p> <p>Example: <pre><code>concat_columns:\n  columns: [\"first_name\", \"last_name\"]\n  separator: \" \"\n  output_col: \"full_name\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to concatenate separator str No - Separator string output_col str Yes - Resulting column name"},{"location":"reference/yaml_schema_v1/#convert_timezone-converttimezoneparams","title":"<code>convert_timezone</code> (ConvertTimezoneParams)","text":"<p>Converts a timestamp from one timezone to another. Assumes the input column is a naive timestamp representing time in source_tz, or a timestamp with timezone.</p> <p>Configuration for timezone conversion.</p> <p>Example: <pre><code>convert_timezone:\n  col: \"utc_time\"\n  source_tz: \"UTC\"\n  target_tz: \"America/New_York\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description col str Yes - Timestamp column to convert source_tz str No <code>UTC</code> Source timezone (e.g., 'UTC', 'America/New_York') target_tz str Yes - Target timezone (e.g., 'America/Los_Angeles') output_col Optional[str] No - Name of the result column (default: {col}_{target_tz})"},{"location":"reference/yaml_schema_v1/#date_add-dateaddparams","title":"<code>date_add</code> (DateAddParams)","text":"<p>Adds an interval to a date/timestamp column.</p> <p>Configuration for date addition.</p> <p>Example: <pre><code>date_add:\n  col: \"created_at\"\n  value: 1\n  unit: \"day\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description col str Yes - - value int Yes - - unit Literal['day', 'month', 'year', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema_v1/#date_diff-datediffparams","title":"<code>date_diff</code> (DateDiffParams)","text":"<p>Calculates difference between two dates/timestamps. Returns the elapsed time in the specified unit (as float for sub-day units).</p> <p>Configuration for date difference.</p> <p>Example: <pre><code>date_diff:\n  start_col: \"created_at\"\n  end_col: \"updated_at\"\n  unit: \"day\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description start_col str Yes - - end_col str Yes - - unit Literal['day', 'hour', 'minute', 'second'] No <code>day</code> -"},{"location":"reference/yaml_schema_v1/#date_trunc-datetruncparams","title":"<code>date_trunc</code> (DateTruncParams)","text":"<p>Truncates a date/timestamp to the specified precision.</p> <p>Configuration for date truncation.</p> <p>Example: <pre><code>date_trunc:\n  col: \"created_at\"\n  unit: \"month\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description col str Yes - - unit Literal['year', 'month', 'day', 'hour', 'minute', 'second'] Yes - -"},{"location":"reference/yaml_schema_v1/#derive_columns-derivecolumnsparams","title":"<code>derive_columns</code> (DeriveColumnsParams)","text":"<p>Appends new columns based on SQL expressions.</p> <p>Design: - Uses projection to add fields. - Keeps all existing columns via <code>*</code>.</p> <p>Configuration for derived columns.</p> <p>Example: <pre><code>derive_columns:\n  derivations:\n    total_price: \"quantity * unit_price\"\n    full_name: \"concat(first_name, ' ', last_name)\"\n</code></pre></p> <p>Note: Engine will fail if expressions reference non-existent columns. Back to Catalog</p> Field Type Required Default Description derivations Dict[str, str] Yes - Map of column name to SQL expression"},{"location":"reference/yaml_schema_v1/#distinct-distinctparams","title":"<code>distinct</code> (DistinctParams)","text":"<p>Return unique rows from the dataset using SQL DISTINCT.</p>"},{"location":"reference/yaml_schema_v1/#parameters","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to deduplicate. params : DistinctParams     Parameters specifying which columns to consider for uniqueness. If None, all columns are used.</p>"},{"location":"reference/yaml_schema_v1/#returns","title":"Returns","text":"<p>EngineContext     The updated engine context with duplicate rows removed.</p> <p>Configuration for distinct rows.</p> <p>Example: <pre><code>distinct:\n  columns: [\"category\", \"status\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to project (if None, keeps all columns unique)"},{"location":"reference/yaml_schema_v1/#drop_columns-dropcolumnsparams","title":"<code>drop_columns</code> (DropColumnsParams)","text":"<p>Removes the specified columns from the DataFrame.</p> <p>Configuration for dropping specific columns (blacklist).</p> <p>Example: <pre><code>drop_columns:\n  columns: [\"_internal_id\", \"_temp_flag\", \"_processing_date\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to drop"},{"location":"reference/yaml_schema_v1/#extract_date_parts-extractdateparams","title":"<code>extract_date_parts</code> (ExtractDateParams)","text":"<p>Extracts date parts using ANSI SQL extract/functions.</p> <p>Configuration for extracting date parts.</p> <p>Example: <pre><code>extract_date_parts:\n  source_col: \"created_at\"\n  prefix: \"created\"\n  parts: [\"year\", \"month\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description source_col str Yes - - prefix Optional[str] No - - parts Literal[typing.Literal['year', 'month', 'day', 'hour']] No <code>['year', 'month', 'day']</code> -"},{"location":"reference/yaml_schema_v1/#fill_nulls-fillnullsparams","title":"<code>fill_nulls</code> (FillNullsParams)","text":"<p>Replaces null values with specified defaults using COALESCE.</p> <p>Configuration for filling null values.</p> <p>Example: <pre><code>fill_nulls:\n  values:\n    count: 0\n    description: \"N/A\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description values Dict[str, str | int | float | bool] Yes - Map of column to fill value"},{"location":"reference/yaml_schema_v1/#filter_rows-filterrowsparams","title":"<code>filter_rows</code> (FilterRowsParams)","text":"<p>Filters rows using a standard SQL WHERE clause.</p> <p>Design: - SQL-First: Pushes filtering to the engine's optimizer. - Zero-Copy: No data movement to Python.</p> <p>Configuration for filtering rows.</p> <p>Example: <pre><code>filter_rows:\n  condition: \"age &gt; 18 AND status = 'active'\"\n</code></pre></p> <p>Example (Null Check): <pre><code>filter_rows:\n  condition: \"email IS NOT NULL AND email != ''\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description condition str Yes - SQL WHERE clause (e.g., 'age &gt; 18 AND status = \"active\"')"},{"location":"reference/yaml_schema_v1/#limit-limitparams","title":"<code>limit</code> (LimitParams)","text":"<p>Limit the number of rows returned from the dataset.</p>"},{"location":"reference/yaml_schema_v1/#parameters_1","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to limit. params : LimitParams     Parameters specifying the number of rows to return and the offset.</p>"},{"location":"reference/yaml_schema_v1/#returns_1","title":"Returns","text":"<p>EngineContext     The updated engine context with the limited DataFrame.</p> <p>Configuration for result limiting.</p> <p>Example: <pre><code>limit:\n  n: 100\n  offset: 0\n</code></pre> Back to Catalog</p> Field Type Required Default Description n int Yes - Number of rows to return offset int No <code>0</code> Number of rows to skip"},{"location":"reference/yaml_schema_v1/#normalize_column_names-normalizecolumnnamesparams","title":"<code>normalize_column_names</code> (NormalizeColumnNamesParams)","text":"<p>Normalizes column names to a consistent style. Useful for cleaning up messy source data with spaces, mixed case, or special characters.</p> <p>Configuration for normalizing column names.</p> <p>Example: <pre><code>normalize_column_names:\n  style: \"snake_case\"\n  lowercase: true\n</code></pre> Back to Catalog</p> Field Type Required Default Description style Literal['snake_case', 'none'] No <code>snake_case</code> Naming style: 'snake_case' converts spaces/special chars to underscores lowercase bool No <code>True</code> Convert names to lowercase remove_special bool No <code>True</code> Remove special characters except underscores"},{"location":"reference/yaml_schema_v1/#normalize_schema-normalizeschemaparams","title":"<code>normalize_schema</code> (NormalizeSchemaParams)","text":"<p>Structural transformation to rename, drop, and reorder columns.</p> <p>Note: This is one of the few that might behave better with native API in some cases, but SQL projection handles it perfectly and is consistent.</p> <p>Configuration for schema normalization.</p> <p>Example: <pre><code>normalize_schema:\n  rename:\n    old_col: \"new_col\"\n  drop: [\"unused_col\"]\n  select_order: [\"id\", \"new_col\", \"created_at\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description rename Optional[Dict[str, str]] No <code>PydanticUndefined</code> old_name -&gt; new_name drop Optional[List[str]] No <code>PydanticUndefined</code> Columns to remove; ignored if not present select_order Optional[List[str]] No - Final column order; any missing columns appended after"},{"location":"reference/yaml_schema_v1/#rename_columns-renamecolumnsparams","title":"<code>rename_columns</code> (RenameColumnsParams)","text":"<p>Renames columns according to the provided mapping. Columns not in the mapping are kept unchanged.</p> <p>Configuration for bulk column renaming.</p> <p>Example: <pre><code>rename_columns:\n  mapping:\n    customer_id: cust_id\n    order_date: date\n    total_amount: amount\n</code></pre> Back to Catalog</p> Field Type Required Default Description mapping Dict[str, str] Yes - Map of old column name to new column name"},{"location":"reference/yaml_schema_v1/#replace_values-replacevaluesparams","title":"<code>replace_values</code> (ReplaceValuesParams)","text":"<p>Replaces values in specified columns according to the mapping. Supports replacing to NULL.</p> <p>Configuration for bulk value replacement.</p> <p>Example - Standardize nulls: <pre><code>replace_values:\n  columns: [\"status\", \"category\"]\n  mapping:\n    \"N/A\": null\n    \"\": null\n    \"Unknown\": null\n</code></pre></p> <p>Example - Code replacement: <pre><code>replace_values:\n  columns: [\"country_code\"]\n  mapping:\n    \"US\": \"USA\"\n    \"UK\": \"GBR\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to apply replacements to mapping Dict[str, Optional[str]] Yes - Map of old value to new value (use null for NULL)"},{"location":"reference/yaml_schema_v1/#sample-sampleparams","title":"<code>sample</code> (SampleParams)","text":"<p>Return a random sample of rows from the dataset.</p>"},{"location":"reference/yaml_schema_v1/#parameters_2","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to sample from. params : SampleParams     Parameters specifying the fraction of rows to return and the random seed.</p>"},{"location":"reference/yaml_schema_v1/#returns_2","title":"Returns","text":"<p>EngineContext     The updated engine context with the sampled DataFrame.</p> <p>Configuration for random sampling.</p> <p>Example: <pre><code>sample:\n  fraction: 0.1\n  seed: 42\n</code></pre> Back to Catalog</p> Field Type Required Default Description fraction float Yes - Fraction of rows to return (0.0 to 1.0) seed Optional[int] No - -"},{"location":"reference/yaml_schema_v1/#select_columns-selectcolumnsparams","title":"<code>select_columns</code> (SelectColumnsParams)","text":"<p>Keeps only the specified columns, dropping all others.</p> <p>Configuration for selecting specific columns (whitelist).</p> <p>Example: <pre><code>select_columns:\n  columns: [\"id\", \"name\", \"created_at\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of column names to keep"},{"location":"reference/yaml_schema_v1/#sort-sortparams","title":"<code>sort</code> (SortParams)","text":"<p>Sort the dataset by one or more columns.</p>"},{"location":"reference/yaml_schema_v1/#parameters_3","title":"Parameters","text":"<p>context : EngineContext     The engine context containing the DataFrame to sort. params : SortParams     Parameters specifying columns to sort by and sort order.</p>"},{"location":"reference/yaml_schema_v1/#returns_3","title":"Returns","text":"<p>EngineContext     The updated engine context with the sorted DataFrame.</p> <p>Configuration for sorting.</p> <p>Example: <pre><code>sort:\n  by: [\"created_at\", \"id\"]\n  ascending: false\n</code></pre> Back to Catalog</p> Field Type Required Default Description by str | List[str] Yes - Column(s) to sort by ascending bool No <code>True</code> Sort order"},{"location":"reference/yaml_schema_v1/#split_part-splitpartparams","title":"<code>split_part</code> (SplitPartParams)","text":"<p>Extracts the Nth part of a string after splitting by a delimiter.</p> <p>Configuration for splitting strings.</p> <p>Example: <pre><code>split_part:\n  col: \"email\"\n  delimiter: \"@\"\n  index: 2  # Extracts domain\n</code></pre> Back to Catalog</p> Field Type Required Default Description col str Yes - Column to split delimiter str Yes - Delimiter to split by index int Yes - 1-based index of the token to extract"},{"location":"reference/yaml_schema_v1/#trim_whitespace-trimwhitespaceparams","title":"<code>trim_whitespace</code> (TrimWhitespaceParams)","text":"<p>Trims leading and trailing whitespace from string columns.</p> <p>Configuration for trimming whitespace from string columns.</p> <p>Example - All string columns: <pre><code>trim_whitespace: {}\n</code></pre></p> <p>Example - Specific columns: <pre><code>trim_whitespace:\n  columns: [\"name\", \"address\", \"city\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns Optional[List[str]] No - Columns to trim (default: all string columns detected at runtime)"},{"location":"reference/yaml_schema_v1/#relational-algebra","title":"\ud83d\udcc2 Relational Algebra","text":""},{"location":"reference/yaml_schema_v1/#aggregate-aggregateparams","title":"<code>aggregate</code> (AggregateParams)","text":"<p>Performs grouping and aggregation via SQL.</p> <p>Configuration for aggregation.</p> <p>Example: <pre><code>aggregate:\n  group_by: [\"department\", \"region\"]\n  aggregations:\n    salary: \"sum\"\n    employee_id: \"count\"\n    age: \"avg\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - Columns to group by aggregations Dict[str, AggFunc] Yes - Map of column to aggregation function (sum, avg, min, max, count)"},{"location":"reference/yaml_schema_v1/#join-joinparams","title":"<code>join</code> (JoinParams)","text":"<p>Joins the current dataset with another dataset from the context.</p> <p>Configuration for joining datasets.</p> <p>Scenario 1: Simple Left Join <pre><code>join:\n  right_dataset: \"customers\"\n  on: \"customer_id\"\n  how: \"left\"\n</code></pre></p> <p>Scenario 2: Join with Prefix (avoid collisions) <pre><code>join:\n  right_dataset: \"orders\"\n  on: [\"user_id\"]\n  how: \"inner\"\n  prefix: \"ord\"  # Result cols: ord_date, ord_amount...\n</code></pre> Back to Catalog</p> Field Type Required Default Description right_dataset str Yes - Name of the node/dataset to join with on str | List[str] Yes - Column(s) to join on how Literal['inner', 'left', 'right', 'full', 'cross', 'anti', 'semi'] No <code>left</code> Join type prefix Optional[str] No - Prefix for columns from right dataset to avoid collisions"},{"location":"reference/yaml_schema_v1/#pivot-pivotparams","title":"<code>pivot</code> (PivotParams)","text":"<p>Pivots row values into columns.</p> <p>Configuration for pivoting data.</p> <p>Example: <pre><code>pivot:\n  group_by: [\"product_id\", \"region\"]\n  pivot_col: \"month\"\n  agg_col: \"sales\"\n  agg_func: \"sum\"\n</code></pre></p> <p>Example (Optimized for Spark): <pre><code>pivot:\n  group_by: [\"id\"]\n  pivot_col: \"category\"\n  values: [\"A\", \"B\", \"C\"]  # Explicit values avoid extra pass\n  agg_col: \"amount\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description group_by List[str] Yes - - pivot_col str Yes - - agg_col str Yes - - agg_func Literal['sum', 'count', 'avg', 'max', 'min', 'first'] No <code>sum</code> - values Optional[List[str]] No - Specific values to pivot (for Spark optimization)"},{"location":"reference/yaml_schema_v1/#union-unionparams","title":"<code>union</code> (UnionParams)","text":"<p>Unions current dataset with others.</p> <p>Configuration for unioning datasets.</p> <p>Example (By Name - Default): <pre><code>union:\n  datasets: [\"sales_2023\", \"sales_2024\"]\n  by_name: true\n</code></pre></p> <p>Example (By Position): <pre><code>union:\n  datasets: [\"legacy_data\"]\n  by_name: false\n</code></pre> Back to Catalog</p> Field Type Required Default Description datasets List[str] Yes - List of node names to union with current by_name bool No <code>True</code> Match columns by name (UNION ALL BY NAME)"},{"location":"reference/yaml_schema_v1/#unpivot-unpivotparams","title":"<code>unpivot</code> (UnpivotParams)","text":"<p>Unpivots columns into rows (Melt/Stack).</p> <p>Configuration for unpivoting (melting) data.</p> <p>Example: <pre><code>unpivot:\n  id_cols: [\"product_id\"]\n  value_vars: [\"jan_sales\", \"feb_sales\", \"mar_sales\"]\n  var_name: \"month\"\n  value_name: \"sales\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description id_cols List[str] Yes - - value_vars List[str] Yes - - var_name str No <code>variable</code> - value_name str No <code>value</code> -"},{"location":"reference/yaml_schema_v1/#data-quality","title":"\ud83d\udcc2 Data Quality","text":""},{"location":"reference/yaml_schema_v1/#cross_check-crosscheckparams","title":"<code>cross_check</code> (CrossCheckParams)","text":"<p>Perform cross-node validation checks.</p> <p>Does not return a DataFrame (returns None). Raises ValidationError on failure.</p> <p>Configuration for cross-node validation checks.</p> <p>Example (Row Count Mismatch): <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"row_count_diff\"\n  inputs: [\"node_a\", \"node_b\"]\n  threshold: 0.05  # Allow 5% difference\n</code></pre></p> <p>Example (Schema Match): <pre><code>transformer: \"cross_check\"\nparams:\n  type: \"schema_match\"\n  inputs: [\"staging_orders\", \"prod_orders\"]\n</code></pre> Back to Catalog</p> Field Type Required Default Description type str Yes - Check type: 'row_count_diff', 'schema_match' inputs List[str] Yes - List of node names to compare threshold float No <code>0.0</code> Threshold for diff (0.0-1.0)"},{"location":"reference/yaml_schema_v1/#warehousing-patterns","title":"\ud83d\udcc2 Warehousing Patterns","text":""},{"location":"reference/yaml_schema_v1/#auditcolumnsconfig","title":"AuditColumnsConfig","text":"<p>Back to Catalog</p> Field Type Required Default Description created_col Optional[str] No - Column to set only on first insert updated_col Optional[str] No - Column to update on every merge"},{"location":"reference/yaml_schema_v1/#merge-mergeparams","title":"<code>merge</code> (MergeParams)","text":"<p>Merge transformer implementation. Handles Upsert, Append-Only, and Delete-Match strategies.</p> <p>Args:     context: EngineContext (preferred) or legacy PandasContext/SparkContext     params: MergeParams object (when called via function step) or DataFrame (legacy)     current: DataFrame (legacy positional arg, deprecated)     **kwargs: Parameters when not using MergeParams</p> <p>Configuration for Merge transformer (Upsert/Append).</p>"},{"location":"reference/yaml_schema_v1/#gdpr-compliance-guide","title":"\u2696\ufe0f \"GDPR &amp; Compliance\" Guide","text":"<p>Business Problem: \"A user exercised their 'Right to be Forgotten'. We need to remove them from our Silver tables immediately.\"</p> <p>The Solution: Use the <code>delete_match</code> strategy. The source dataframe contains the IDs to be deleted, and the transformer removes them from the target.</p> <p>Recipe 1: Right to be Forgotten (Delete) <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"customer_id\"]\n  strategy: \"delete_match\"\n</code></pre></p> <p>Recipe 2: Conditional Update (SCD Type 1) \"Only update if the source record is newer than the target record.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.products\"\n  keys: [\"product_id\"]\n  strategy: \"upsert\"\n  update_condition: \"source.updated_at &gt; target.updated_at\"\n</code></pre></p> <p>Recipe 3: Safe Insert (Filter Bad Records) \"Only insert records that are not marked as deleted.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.orders\"\n  keys: [\"order_id\"]\n  strategy: \"append_only\"\n  insert_condition: \"source.is_deleted = false\"\n</code></pre></p> <p>Recipe 4: Audit Columns \"Track when records were created or updated.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.users\"\n  keys: [\"user_id\"]\n  audit_cols:\n    created_col: \"dw_created_at\"\n    updated_col: \"dw_updated_at\"\n</code></pre></p> <p>Recipe 5: Full Sync (Insert + Update + Delete) \"Sync target with source: insert new, update changed, and remove soft-deleted.\" <pre><code>transformer: \"merge\"\nparams:\n  target: \"silver.customers\"\n  keys: [\"id\"]\n  strategy: \"upsert\"\n  # 1. Delete if source says so\n  delete_condition: \"source.is_deleted = true\"\n  # 2. Update if changed (and not deleted)\n  update_condition: \"source.hash != target.hash\"\n  # 3. Insert new (and not deleted)\n  insert_condition: \"source.is_deleted = false\"\n</code></pre></p> <p>Recipe 6: Connection-based Path Resolution (ADLS) \"Use a connection to resolve paths, just like write config.\" <pre><code>transform:\n  steps:\n    - function: merge\n      params:\n        connection: goat_prod\n        path: OEE/silver/customers\n        register_table: silver.customers\n        keys: [\"customer_id\"]\n        strategy: \"upsert\"\n        audit_cols:\n          created_col: \"_created_at\"\n          updated_col: \"_updated_at\"\n</code></pre></p> <p>Strategies: *   upsert (Default): Update existing records, insert new ones. *   append_only: Ignore duplicates, only insert new keys. *   delete_match: Delete records in target that match keys in source. Back to Catalog</p> Field Type Required Default Description target Optional[str] No - Target table name or full path (use this OR connection+path) connection Optional[str] No - Connection name to resolve path (use with 'path' param) path Optional[str] No - Relative path within connection (e.g., 'OEE/silver/customers') register_table Optional[str] No - Register as Unity Catalog/metastore table after merge (e.g., 'silver.customers') keys List[str] Yes - List of join keys strategy MergeStrategy No <code>MergeStrategy.UPSERT</code> Merge behavior: 'upsert', 'append_only', 'delete_match' audit_cols Optional[AuditColumnsConfig] No - {'created_col': '...', 'updated_col': '...'} optimize_write bool No <code>False</code> Run OPTIMIZE after write (Spark) zorder_by Optional[List[str]] No - Columns to Z-Order by cluster_by Optional[List[str]] No - Columns to Liquid Cluster by (Delta) update_condition Optional[str] No - SQL condition for update clause (e.g. 'source.ver &gt; target.ver') insert_condition Optional[str] No - SQL condition for insert clause (e.g. 'source.status != \"deleted\"') delete_condition Optional[str] No - SQL condition for delete clause (e.g. 'source.status = \"deleted\"') table_properties Optional[dict] No - Delta table properties for initial table creation (e.g., column mapping)"},{"location":"reference/yaml_schema_v1/#scd2-scd2params","title":"<code>scd2</code> (SCD2Params)","text":"<p>Implements SCD Type 2 Logic.</p> <p>Returns the FULL history dataset (to be written via Overwrite).</p> <p>Parameters for SCD Type 2 (Slowly Changing Dimensions) transformer.</p>"},{"location":"reference/yaml_schema_v1/#the-time-machine-pattern","title":"\ud83d\udd70\ufe0f The \"Time Machine\" Pattern","text":"<p>Business Problem: \"I need to know what the customer's address was last month, not just where they live now.\"</p> <p>The Solution: SCD Type 2 tracks the full history of changes. Each record has an \"effective window\" (start/end dates) and a flag indicating if it is the current version.</p> <p>Recipe 1: Using table name <pre><code>transformer: \"scd2\"\nparams:\n  target: \"silver.dim_customers\"   # Registered table name\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre></p> <p>Recipe 2: Using connection + path (ADLS) <pre><code>transformer: \"scd2\"\nparams:\n  connection: adls_prod            # Connection name\n  path: OEE/silver/dim_customers   # Relative path\n  keys: [\"customer_id\"]\n  track_cols: [\"address\", \"tier\"]\n  effective_time_col: \"txn_date\"\n</code></pre></p> <p>How it works: 1. Match: Finds existing records using <code>keys</code>. 2. Compare: Checks <code>track_cols</code> to see if data changed. 3. Close: If changed, updates the old record's <code>end_time_col</code> to the new <code>effective_time_col</code>. 4. Insert: Adds a new record with <code>effective_time_col</code> as start and open-ended end date.</p> <p>Note: SCD2 returns a DataFrame containing the full history. You must use a <code>write:</code> block to persist the result (typically with <code>mode: overwrite</code> to the same location as <code>target</code>). Back to Catalog</p> Field Type Required Default Description target Optional[str] No - Target table name or full path (use this OR connection+path) connection Optional[str] No - Connection name to resolve path (use with 'path' param) path Optional[str] No - Relative path within connection (e.g., 'OEE/silver/dim_customers') keys List[str] Yes - Natural keys to identify unique entities track_cols List[str] Yes - Columns to monitor for changes effective_time_col str Yes - Source column indicating when the change occurred. end_time_col str No <code>valid_to</code> Name of the end timestamp column current_flag_col str No <code>is_current</code> Name of the current record flag column delete_col Optional[str] No - Column indicating soft deletion (boolean)"},{"location":"reference/yaml_schema_v1/#manufacturing-iot","title":"\ud83d\udcc2 Manufacturing &amp; IoT","text":""},{"location":"reference/yaml_schema_v1/#phaseconfig","title":"PhaseConfig","text":"<p>Configuration for a single phase. Back to Catalog</p> Field Type Required Default Description timer_col str Yes - Timer column name for this phase start_threshold Optional[int] No - Override default start threshold for this phase (seconds)"},{"location":"reference/yaml_schema_v1/#detect_sequential_phases-detectsequentialphasesparams","title":"<code>detect_sequential_phases</code> (DetectSequentialPhasesParams)","text":"<p>Detect and analyze sequential manufacturing phases.</p> <p>For each group (e.g., batch), this transformer: 1. Processes phases sequentially (each starts after previous ends) 2. Detects phase start by finding first valid timer reading and back-calculating 3. Detects phase end by finding first repeated (plateaued) timer value 4. Calculates time spent in each status during each phase 5. Aggregates specified metrics within each phase window 6. Outputs one summary row per group</p> <p>Output columns per phase: - {phase}start: Phase start timestamp - {phase}_end: Phase end timestamp - {phase}_max_minutes: Maximum timer value converted to minutes - {phase}minutes: Time in each status (if status_col provided) - {phase}: Aggregated metrics (if phase_metrics provided)</p> <p>Detect and analyze sequential manufacturing phases from timer columns.</p> <p>This transformer processes raw sensor/PLC data where timer columns increment during each phase. It detects phase boundaries, calculates durations, and tracks time spent in each equipment status.</p> <p>Common use cases: - Batch reactor cycle analysis - CIP (Clean-in-Place) phase timing - Food processing (cook, cool, package cycles) - Any multi-step batch process with PLC timers</p> <p>Scenario: Analyze FBR cycle times <pre><code>detect_sequential_phases:\n  group_by: BatchID\n  timestamp_col: ts\n  phases:\n    - timer_col: LoadTime\n    - timer_col: AcidTime\n    - timer_col: DryTime\n    - timer_col: CookTime\n    - timer_col: CoolTime\n    - timer_col: UnloadTime\n  start_threshold: 240\n  status_col: Status\n  status_mapping:\n    1: idle\n    2: active\n    3: hold\n    4: faulted\n  phase_metrics:\n    Level: max\n  metadata:\n    ProductCode: first_after_start\n    Weight: max\n</code></pre></p> <p>Scenario: Group by multiple columns <pre><code>detect_sequential_phases:\n  group_by:\n    - BatchID\n    - AssetID\n  phases: [LoadTime, CookTime]\n</code></pre> Back to Catalog</p> Field Type Required Default Description group_by str | List[str] Yes - Column(s) to group by. Can be a single column name or list of columns. E.g., 'BatchID' or ['BatchID', 'AssetID'] timestamp_col str No <code>ts</code> Timestamp column for ordering events phases List[str | PhaseConfig] Yes - List of phase timer columns (strings) or PhaseConfig objects. Phases are processed sequentially - each phase starts after the previous ends. start_threshold int No <code>240</code> Default max timer value (seconds) to consider as valid phase start. Filters out late readings where timer already shows large elapsed time. status_col Optional[str] No - Column containing equipment status codes status_mapping Optional[Dict[int, str]] No - Mapping of status codes to names. E.g., phase_metrics Optional[Dict[str, str]] No - Columns to aggregate within each phase window. E.g., {Level: max, Pressure: max}. Outputs {Phase}_{Column} columns. metadata Optional[Dict[str, str]] No - Columns to include in output with aggregation method. Options: 'first', 'last', 'first_after_start', 'max', 'min', 'mean', 'sum'. E.g., output_time_format str No <code>%Y-%m-%d %H:%M:%S</code> Format for output timestamp columns fill_null_minutes bool No <code>False</code> If True, fill null numeric columns (_max_minutes, _status_minutes, _metrics) with 0. Timestamp columns remain null for skipped phases. spark_native bool No <code>False</code> If True, use native Spark window functions. If False (default), use applyInPandas which is often faster for datasets with many batches."},{"location":"reference/yaml_schema_v1/#advanced-feature-engineering","title":"\ud83d\udcc2 Advanced &amp; Feature Engineering","text":""},{"location":"reference/yaml_schema_v1/#shiftdefinition","title":"ShiftDefinition","text":"<p>Definition of a single shift. Back to Catalog</p> Field Type Required Default Description name str Yes - Name of the shift (e.g., 'Day', 'Night') start str Yes - Start time in HH:MM format (e.g., '06:00') end str Yes - End time in HH:MM format (e.g., '14:00')"},{"location":"reference/yaml_schema_v1/#deduplicate-deduplicateparams","title":"<code>deduplicate</code> (DeduplicateParams)","text":"<p>Deduplicates data using Window functions.</p> <p>Configuration for deduplication.</p> <p>Scenario: Keep latest record <pre><code>deduplicate:\n  keys: [\"id\"]\n  order_by: \"updated_at DESC\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description keys List[str] Yes - List of columns to partition by (columns that define uniqueness) order_by Optional[str] No - SQL Order by clause (e.g. 'updated_at DESC') to determine which record to keep (first one is kept)"},{"location":"reference/yaml_schema_v1/#dict_based_mapping-dictmappingparams","title":"<code>dict_based_mapping</code> (DictMappingParams)","text":"<p>Maps values in a column using a provided dictionary.</p> <p>For each value in the specified column, replaces it with the mapped value. If 'default' is provided, uses it for values not found in the mapping. Supports Spark and Pandas engines.</p> <p>Configuration for dictionary mapping.</p> <p>Scenario: Map status codes to labels <pre><code>dict_based_mapping:\n  column: \"status_code\"\n  mapping:\n    \"1\": \"Active\"\n    \"0\": \"Inactive\"\n  default: \"Unknown\"\n  output_column: \"status_desc\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description column str Yes - Column to map values from mapping Dict[str, str | int | float | bool] Yes - Dictionary of source value -&gt; target value default str | int | float | bool No - Default value if source value is not found in mapping output_column Optional[str] No - Name of output column. If not provided, overwrites source column."},{"location":"reference/yaml_schema_v1/#explode_list_column-explodeparams","title":"<code>explode_list_column</code> (ExplodeParams)","text":"<p>Explodes a list/array column into multiple rows.</p> <p>For each element in the specified list column, creates a new row. If 'outer' is True, keeps rows with empty lists (like explode_outer). Supports Spark and Pandas engines.</p> <p>Configuration for exploding lists.</p> <p>Scenario: Flatten list of items per order <pre><code>explode_list_column:\n  column: \"items\"\n  outer: true  # Keep orders with empty items list\n</code></pre> Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing the list/array to explode outer bool No <code>False</code> If True, keep rows with empty lists (explode_outer behavior). If False, drops them."},{"location":"reference/yaml_schema_v1/#generate_numeric_key-numerickeyparams","title":"<code>generate_numeric_key</code> (NumericKeyParams)","text":"<p>Generates a deterministic BIGINT surrogate key from a hash of columns.</p> <p>This is useful when: - Unioning data from multiple sources - Some sources have IDs, some don't - You need stable numeric IDs for gold layer</p> <p>The key is generated by: 1. Concatenating columns with separator 2. Computing MD5 hash 3. Converting first 15 hex chars to BIGINT</p> <p>If coalesce_with is specified, keeps the existing value when not null. If output_col == coalesce_with, the original column is replaced.</p> <p>Configuration for numeric surrogate key generation.</p> <p>Generates a deterministic BIGINT key from a hash of specified columns. Useful when unioning data from multiple sources where some have IDs and others don't.</p> <p>Example: <pre><code>- function: generate_numeric_key\n  params:\n    columns: [DateID, store_id, reason_id, duration_min, notes]\n    output_col: ID\n    coalesce_with: ID  # Keep existing ID if not null\n</code></pre></p> <p>The generated key is: - Deterministic: same input data = same ID every time - BIGINT: large numeric space to avoid collisions - Stable: safe for gold layer / incremental loads Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to combine for the key separator str No <code>|</code> Separator between values output_col str No <code>numeric_key</code> Name of the output column coalesce_with Optional[str] No - Existing column to coalesce with (keep existing value if not null)"},{"location":"reference/yaml_schema_v1/#generate_surrogate_key-surrogatekeyparams","title":"<code>generate_surrogate_key</code> (SurrogateKeyParams)","text":"<p>Generates a deterministic surrogate key (MD5) from a combination of columns. Handles NULLs by treating them as empty strings to ensure consistency.</p> <p>Configuration for surrogate key generation.</p> <p>Example: <pre><code>generate_surrogate_key:\n  columns: [\"region\", \"product_id\"]\n  separator: \"-\"\n  output_col: \"unique_id\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - Columns to combine for the key separator str No <code>-</code> Separator between values output_col str No <code>surrogate_key</code> Name of the output column"},{"location":"reference/yaml_schema_v1/#geocode-geocodeparams","title":"<code>geocode</code> (GeocodeParams)","text":"<p>Geocoding Stub.</p> <p>Configuration for geocoding.</p> <p>Example: <pre><code>geocode:\n  address_col: \"full_address\"\n  output_col: \"lat_long\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description address_col str Yes - Column containing the address to geocode output_col str No <code>lat_long</code> Name of the output column for coordinates"},{"location":"reference/yaml_schema_v1/#hash_columns-hashparams","title":"<code>hash_columns</code> (HashParams)","text":"<p>Hashes columns for PII/Anonymization.</p> <p>Configuration for column hashing.</p> <p>Example: <pre><code>hash_columns:\n  columns: [\"email\", \"ssn\"]\n  algorithm: \"sha256\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description columns List[str] Yes - List of columns to hash algorithm HashAlgorithm No <code>HashAlgorithm.SHA256</code> Hashing algorithm. Options: 'sha256', 'md5'"},{"location":"reference/yaml_schema_v1/#normalize_json-normalizejsonparams","title":"<code>normalize_json</code> (NormalizeJsonParams)","text":"<p>Flattens a nested JSON/Struct column.</p> <p>Configuration for JSON normalization.</p> <p>Example: <pre><code>normalize_json:\n  column: \"json_data\"\n  sep: \"_\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description column str Yes - Column containing nested JSON/Struct sep str No <code>_</code> Separator for nested fields (e.g., 'parent_child')"},{"location":"reference/yaml_schema_v1/#parse_json-parsejsonparams","title":"<code>parse_json</code> (ParseJsonParams)","text":"<p>Parses a JSON string column into a Struct/Map column.</p> <p>Configuration for JSON parsing.</p> <p>Example: <pre><code>parse_json:\n  column: \"raw_json\"\n  json_schema: \"id INT, name STRING\"\n  output_col: \"parsed_struct\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description column str Yes - String column containing JSON json_schema str Yes - DDL schema string (e.g. 'a INT, b STRING') or Spark StructType DDL output_col Optional[str] No - -"},{"location":"reference/yaml_schema_v1/#regex_replace-regexreplaceparams","title":"<code>regex_replace</code> (RegexReplaceParams)","text":"<p>Applies a regex replacement to a column.</p> <p>Uses SQL-based REGEXP_REPLACE to replace all matches of the pattern in the specified column with the given replacement string. Works on both Spark and DuckDB/Pandas engines.</p> <p>Configuration for regex replacement.</p> <p>Example: <pre><code>regex_replace:\n  column: \"phone\"\n  pattern: \"[^0-9]\"\n  replacement: \"\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description column str Yes - Column to apply regex replacement on pattern str Yes - Regex pattern to match replacement str Yes - String to replace matches with"},{"location":"reference/yaml_schema_v1/#sessionize-sessionizeparams","title":"<code>sessionize</code> (SessionizeParams)","text":"<p>Assigns session IDs based on inactivity threshold.</p> <p>Configuration for sessionization.</p> <p>Example: <pre><code>sessionize:\n  timestamp_col: \"event_time\"\n  user_col: \"user_id\"\n  threshold_seconds: 1800\n</code></pre> Back to Catalog</p> Field Type Required Default Description timestamp_col str Yes - Timestamp column to calculate session duration from user_col str Yes - User identifier to partition sessions by threshold_seconds int No <code>1800</code> Inactivity threshold in seconds (default: 30 minutes). If gap &gt; threshold, new session starts. session_col str No <code>session_id</code> Output column name for the generated session ID"},{"location":"reference/yaml_schema_v1/#split_events_by_period-spliteventsbyperiodparams","title":"<code>split_events_by_period</code> (SplitEventsByPeriodParams)","text":"<p>Splits events that span multiple time periods into individual segments.</p> <p>For events spanning multiple days/hours/shifts, this creates separate rows for each period with adjusted start/end times and recalculated durations.</p> <p>Configuration for splitting events that span multiple time periods.</p> <p>Splits events that span multiple days, hours, or shifts into individual segments per period. Useful for OEE/downtime analysis, billing, and time-based aggregations.</p> <p>Example - Split by day: <pre><code>split_events_by_period:\n  start_col: \"Shutdown_Start_Time\"\n  end_col: \"Shutdown_End_Time\"\n  period: \"day\"\n  duration_col: \"Shutdown_Duration_Min\"\n</code></pre></p> <p>Example - Split by shift: <pre><code>split_events_by_period:\n  start_col: \"event_start\"\n  end_col: \"event_end\"\n  period: \"shift\"\n  duration_col: \"duration_minutes\"\n  shifts:\n    - name: \"Day\"\n      start: \"06:00\"\n      end: \"14:00\"\n    - name: \"Swing\"\n      start: \"14:00\"\n      end: \"22:00\"\n    - name: \"Night\"\n      start: \"22:00\"\n      end: \"06:00\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description start_col str Yes - Column containing the event start timestamp end_col str Yes - Column containing the event end timestamp period str No <code>day</code> Period type to split by: 'day', 'hour', or 'shift' duration_col Optional[str] No - Output column name for duration in minutes. If not set, no duration column is added. shifts Optional[List[ShiftDefinition]] No - List of shift definitions (required when period='shift') shift_col Optional[str] No <code>shift_name</code> Output column name for shift name (only used when period='shift')"},{"location":"reference/yaml_schema_v1/#unpack_struct-unpackstructparams","title":"<code>unpack_struct</code> (UnpackStructParams)","text":"<p>Flattens a struct/dict column into top-level columns.</p> <p>Configuration for unpacking structs.</p> <p>Example: <pre><code>unpack_struct:\n  column: \"user_info\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description column str Yes - Struct/Dictionary column to unpack/flatten into individual columns"},{"location":"reference/yaml_schema_v1/#validate_and_flag-validateandflagparams","title":"<code>validate_and_flag</code> (ValidateAndFlagParams)","text":"<p>Validates rules and appends a column with a list/string of failed rule names.</p> <p>Configuration for validation flagging.</p> <p>Example: <pre><code>validate_and_flag:\n  flag_col: \"data_issues\"\n  rules:\n    age_check: \"age &gt;= 0\"\n    email_format: \"email LIKE '%@%'\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description rules Dict[str, str] Yes - Map of rule name to SQL condition (must be TRUE) flag_col str No <code>_issues</code> Name of the column to store failed rules"},{"location":"reference/yaml_schema_v1/#window_calculation-windowcalculationparams","title":"<code>window_calculation</code> (WindowCalculationParams)","text":"<p>Generic wrapper for Window functions.</p> <p>Configuration for window functions.</p> <p>Example: <pre><code>window_calculation:\n  target_col: \"cumulative_sales\"\n  function: \"sum(sales)\"\n  partition_by: [\"region\"]\n  order_by: \"date ASC\"\n</code></pre> Back to Catalog</p> Field Type Required Default Description target_col str Yes - - function str Yes - Window function e.g. 'sum(amount)', 'rank()' partition_by List[str] No <code>PydanticUndefined</code> - order_by Optional[str] No - -"},{"location":"reference/yaml_schema_v1/#semantic-layer","title":"Semantic Layer","text":""},{"location":"reference/yaml_schema_v1/#semantic-layer_1","title":"Semantic Layer","text":"<p>The semantic layer provides a unified interface for defining and querying business metrics. Define metrics once, query them by name across dimensions.</p> <p>Core Components: - MetricDefinition: Define aggregation expressions (SUM, COUNT, AVG) - DimensionDefinition: Define grouping attributes with hierarchies - MaterializationConfig: Pre-compute metrics at specific grain - SemanticQuery: Execute queries like \"revenue BY region, month\" - Project: Unified API that connects pipelines and semantic layer</p> <p>Unified Project API (Recommended): <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre></p> <p>YAML Configuration: <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: gold.fact_orders    # connection.table notation\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: gold.dim_customer\n      column: region\n\nmaterializations:\n  - name: monthly_revenue\n    metrics: [revenue]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre></p> <p>The <code>source: gold.fact_orders</code> notation resolves paths automatically from connections.</p>"},{"location":"reference/yaml_schema_v1/#dimensiondefinition","title":"<code>DimensionDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic dimension.</p> <p>A dimension represents an attribute for grouping and filtering metrics (e.g., date, product, region).</p> <p>Attributes:     name: Unique dimension identifier     label: Display name for column alias in generated views. Defaults to name.     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.dim_customer</code>         - <code>connection.path</code>: e.g., <code>gold.dim_customer</code> or <code>gold.dims/customer</code>         - <code>table_name</code>: Uses default connection     column: Column name in source (defaults to name)     expr: Custom SQL expression. If provided, overrides column and grain.         Example: \"YEAR(DATEADD(month, 6, Date))\" for fiscal year     hierarchy: Optional ordered list of columns for drill-down     description: Human-readable description     grain: Time grain transformation (day, week, month, quarter, year).         Ignored if expr is provided. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique dimension identifier | | label | Optional[str] | No | - | Display name for column alias (defaults to name) | | source | Optional[str] | No | - | Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.dim_customer), connection.path (e.g., gold.dim_customer or gold.dims/customer), or bare table_name | | column | Optional[str] | No | - | Column name (defaults to name) | | expr | Optional[str] | No | - | Custom SQL expression. Overrides column and grain. Example: YEAR(DATEADD(month, 6, Date)) for fiscal year | | hierarchy | List[str] | No | <code>PydanticUndefined</code> | Drill-down hierarchy | | description | Optional[str] | No | - | Human-readable description | | grain | Optional[TimeGrain] | No | - | Time grain transformation |</p>"},{"location":"reference/yaml_schema_v1/#materializationconfig","title":"<code>MaterializationConfig</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Configuration for materializing metrics to a table.</p> <p>Materialization pre-computes aggregated metrics at a specific grain and persists them for faster querying.</p> <p>Attributes:     name: Unique materialization identifier     metrics: List of metric names to include     dimensions: List of dimension names (determines grain)     output: Output table path     schedule: Optional cron schedule for refresh     incremental: Configuration for incremental refresh | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique materialization identifier | | metrics | List[str] | Yes | - | Metrics to materialize | | dimensions | List[str] | Yes | - | Dimensions for grouping | | output | str | Yes | - | Output table path | | schedule | Optional[str] | No | - | Cron schedule | | incremental | Optional[Dict[str, Any]] | No | - | Incremental refresh config |</p>"},{"location":"reference/yaml_schema_v1/#metricdefinition","title":"<code>MetricDefinition</code>","text":"<p>Used in: SemanticLayerConfig</p> <p>Definition of a semantic metric.</p> <p>A metric represents a measurable value that can be aggregated across dimensions (e.g., revenue, order_count, avg_order_value).</p> <p>Attributes:     name: Unique metric identifier     label: Display name for column alias in generated views. Defaults to name.     description: Human-readable description     expr: SQL aggregation expression (e.g., \"SUM(total_amount)\").         Optional for derived metrics.     source: Source table reference. Supports three formats:         - <code>$pipeline.node</code> (recommended): e.g., <code>$build_warehouse.fact_orders</code>         - <code>connection.path</code>: e.g., <code>gold.fact_orders</code> or <code>gold.oee/plant_a/metrics</code>         - <code>table_name</code>: Uses default connection     filters: Optional WHERE conditions to apply     type: \"simple\" (direct aggregation) or \"derived\" (references other metrics)     components: List of component metric names (required for derived metrics).         These metrics must be additive (e.g., SUM-based) for correct         recalculation at different grains.     formula: Calculation formula using component names (required for derived).         Example: \"(total_revenue - total_cost) / total_revenue\" | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique metric identifier | | label | Optional[str] | No | - | Display name for column alias (defaults to name) | | description | Optional[str] | No | - | Human-readable description | | expr | Optional[str] | No | - | SQL aggregation expression | | source | Optional[str] | No | - | Source table reference. Formats: $pipeline.node (e.g., $build_warehouse.fact_orders), connection.path (e.g., gold.fact_orders or gold.oee/plant_a/table), or bare table_name | | filters | List[str] | No | <code>PydanticUndefined</code> | WHERE conditions | | type | MetricType | No | <code>MetricType.SIMPLE</code> | Metric type | | components | Optional[List[str]] | No | - | Component metric names for derived metrics | | formula | Optional[str] | No | - | Calculation formula using component names |</p>"},{"location":"reference/yaml_schema_v1/#semanticlayerconfig","title":"<code>SemanticLayerConfig</code>","text":"<p>Complete semantic layer configuration.</p> <p>Contains all metrics, dimensions, materializations, and views for a semantic layer deployment.</p> <p>Attributes:     metrics: List of metric definitions     dimensions: List of dimension definitions     materializations: List of materialization configurations     views: List of view configurations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | metrics | List[MetricDefinition] | No | <code>PydanticUndefined</code> | Metric definitions | | dimensions | List[DimensionDefinition] | No | <code>PydanticUndefined</code> | Dimension definitions | | materializations | List[MaterializationConfig] | No | <code>PydanticUndefined</code> | Materialization configs | | views | List[ViewConfig] | No | <code>PydanticUndefined</code> | View configurations |</p>"},{"location":"reference/yaml_schema_v1/#fk-validation","title":"FK Validation","text":""},{"location":"reference/yaml_schema_v1/#fk-validation_1","title":"FK Validation","text":"<p>Declare and validate referential integrity between fact and dimension tables.</p> <p>Features: - Declare relationships in YAML - Validate FK constraints on fact load - Detect orphan records - Generate lineage from relationships</p> <p>Example: <pre><code>relationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    on_violation: error\n</code></pre></p>"},{"location":"reference/yaml_schema_v1/#relationshipconfig","title":"<code>RelationshipConfig</code>","text":"<p>Used in: RelationshipRegistry</p> <p>Configuration for a foreign key relationship.</p> <p>Attributes:     name: Unique relationship identifier     fact: Fact table name     dimension: Dimension table name     fact_key: Foreign key column in fact table     dimension_key: Primary/surrogate key column in dimension     nullable: Whether nulls are allowed in fact_key     on_violation: Action on violation (\"warn\", \"error\", \"quarantine\") | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | name | str | Yes | - | Unique relationship identifier | | fact | str | Yes | - | Fact table name | | dimension | str | Yes | - | Dimension table name | | fact_key | str | Yes | - | FK column in fact table | | dimension_key | str | Yes | - | PK/SK column in dimension | | nullable | bool | No | <code>False</code> | Allow nulls in fact_key | | on_violation | str | No | <code>error</code> | Action on violation |</p>"},{"location":"reference/yaml_schema_v1/#relationshipregistry","title":"<code>RelationshipRegistry</code>","text":"<p>Registry of all declared relationships.</p> <p>Attributes:     relationships: List of relationship configurations | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | relationships | List[RelationshipConfig] | No | <code>PydanticUndefined</code> | Relationship definitions |</p>"},{"location":"reference/yaml_schema_v1/#data-patterns","title":"Data Patterns","text":""},{"location":"reference/yaml_schema_v1/#data-patterns_1","title":"Data Patterns","text":"<p>Declarative patterns for common data warehouse building blocks. Patterns encapsulate best practices for dimensional modeling, ensuring consistent implementation across your data warehouse.</p>"},{"location":"reference/yaml_schema_v1/#dimensionpattern","title":"DimensionPattern","text":"<p>Build complete dimension tables with surrogate keys and SCD (Slowly Changing Dimension) support.</p> <p>When to Use: - Building dimension tables from source systems (customers, products, locations) - Need surrogate keys for star schema joins - Need to track historical changes (SCD Type 2)</p> <p>Beginner Note: Dimensions are the \"who, what, where, when\" of your data warehouse. A customer dimension has customer_id (natural key) and customer_sk (surrogate key). Fact tables join to dimensions via surrogate keys.</p> <p>See Also: FactPattern, DateDimensionPattern</p> <p>Features: - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> <p>Params:</p> Parameter Type Required Description <code>natural_key</code> str Yes Natural/business key column name <code>surrogate_key</code> str Yes Surrogate key column name to generate <code>scd_type</code> int No 0=static, 1=overwrite, 2=history (default: 1) <code>track_cols</code> list SCD1/2 Columns to track for change detection <code>target</code> str SCD2 Target table path to read existing history <code>unknown_member</code> bool No Insert row with SK=0 for orphan handling <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column with value <p>Supported Target Formats: - Spark: catalog.table, Delta paths, .parquet, .csv, .json, .orc - Pandas: .parquet, .csv, .json, .xlsx, .feather, .pickle</p> <p>Example: <pre><code>pattern:\n  type: dimension\n  params:\n    natural_key: customer_id\n    surrogate_key: customer_sk\n    scd_type: 2\n    track_cols: [name, email, address, city]\n    target: warehouse.dim_customer\n    unknown_member: true\n    audit:\n      load_timestamp: true\n      source_system: \"crm\"\n</code></pre></p>"},{"location":"reference/yaml_schema_v1/#datedimensionpattern","title":"DateDimensionPattern","text":"<p>Generate a complete date dimension table with pre-calculated attributes for BI/reporting.</p> <p>When to Use: - Every data warehouse needs a date dimension for time-based analytics - Enable date filtering, grouping by week/month/quarter, fiscal year reporting</p> <p>Beginner Note: The date dimension is foundational for any BI/reporting system. It lets you query \"sales by month\" or \"orders in fiscal Q2\" without complex date calculations.</p> <p>See Also: DimensionPattern</p> <p>Features: - Generates all dates in a range with rich attributes - Calendar and fiscal year support - ISO week numbering - Weekend/month-end flags</p> <p>Params:</p> Parameter Type Required Description <code>start_date</code> str Yes Start date (YYYY-MM-DD) <code>end_date</code> str Yes End date (YYYY-MM-DD) <code>date_key_format</code> str No Format for date_sk (default: yyyyMMdd) <code>fiscal_year_start_month</code> int No Month fiscal year starts (1-12, default: 1) <code>unknown_member</code> bool No Add unknown date row with date_sk=0 <p>Generated Columns: <code>date_sk</code>, <code>full_date</code>, <code>day_of_week</code>, <code>day_of_week_num</code>, <code>day_of_month</code>, <code>day_of_year</code>, <code>is_weekend</code>, <code>week_of_year</code>, <code>month</code>, <code>month_name</code>, <code>quarter</code>, <code>quarter_name</code>, <code>year</code>, <code>fiscal_year</code>, <code>fiscal_quarter</code>, <code>is_month_start</code>, <code>is_month_end</code>, <code>is_year_start</code>, <code>is_year_end</code></p> <p>Example: <pre><code>pattern:\n  type: date_dimension\n  params:\n    start_date: \"2020-01-01\"\n    end_date: \"2030-12-31\"\n    fiscal_year_start_month: 7\n    unknown_member: true\n</code></pre></p>"},{"location":"reference/yaml_schema_v1/#factpattern","title":"FactPattern","text":"<p>Build fact tables with automatic surrogate key lookups from dimensions.</p> <p>When to Use: - Building fact tables from transactional data (orders, events, transactions) - Need to look up surrogate keys from dimension tables - Need to handle orphan records (missing dimension matches)</p> <p>Beginner Note: Facts are the \"how much, how many\" of your data warehouse. An orders fact has measures (quantity, revenue) and dimension keys (customer_sk, product_sk). The pattern automatically looks up SKs from dimensions.</p> <p>See Also: DimensionPattern, QuarantineConfig</p> <p>Features: - Automatic SK lookups from dimension tables (with SCD2 current-record filtering) - Orphan handling: unknown (SK=0), reject (error), quarantine (route to table) - Grain validation (detect duplicates) - Calculated measures and column renaming - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list No Columns defining uniqueness (validates no duplicates) <code>dimensions</code> list No Dimension lookup configurations (see below) <code>orphan_handling</code> str No \"unknown\" | \"reject\" | \"quarantine\" (default: unknown) <code>quarantine</code> dict quarantine Quarantine config (see below) <code>measures</code> list No Measure definitions (passthrough, rename, or calculated) <code>deduplicate</code> bool No Remove duplicates before processing <code>keys</code> list dedupe Keys for deduplication <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Dimension Lookup Config: <pre><code>dimensions:\n  - source_column: customer_id      # Column in source fact\n    dimension_table: dim_customer   # Dimension in context\n    dimension_key: customer_id      # Natural key in dimension\n    surrogate_key: customer_sk      # SK to retrieve\n    scd2: true                      # Filter is_current=true\n</code></pre></p> <p>Quarantine Config (for orphan_handling: quarantine): <pre><code>quarantine:\n  connection: silver                # Required: connection name\n  path: fact_orders_orphans         # OR table: quarantine_table\n  add_columns:\n    _rejection_reason: true         # Add rejection reason\n    _rejected_at: true              # Add rejection timestamp\n    _source_dimension: true         # Add dimension name\n</code></pre></p> <p>Example: <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - source_column: customer_id\n        dimension_table: dim_customer\n        dimension_key: customer_id\n        surrogate_key: customer_sk\n        scd2: true\n      - source_column: product_id\n        dimension_table: dim_product\n        dimension_key: product_id\n        surrogate_key: product_sk\n    orphan_handling: unknown\n    measures:\n      - quantity\n      - revenue: \"quantity * unit_price\"\n    audit:\n      load_timestamp: true\n      source_system: \"pos\"\n</code></pre></p>"},{"location":"reference/yaml_schema_v1/#aggregationpattern","title":"AggregationPattern","text":"<p>Declarative aggregation with GROUP BY and optional incremental merge.</p> <p>When to Use: - Building summary/aggregate tables (daily sales, monthly metrics) - Need incremental aggregation (update existing aggregates) - Gold layer reporting tables</p> <p>Beginner Note: Aggregations summarize facts at a higher grain. Example: daily_sales aggregates orders by date with SUM(revenue).</p> <p>See Also: FactPattern</p> <p>Features: - Declare grain (GROUP BY columns) - Define measures with SQL aggregation expressions - Optional HAVING filter - Audit columns</p> <p>Params:</p> Parameter Type Required Description <code>grain</code> list Yes Columns to GROUP BY (defines uniqueness) <code>measures</code> list Yes Measure definitions with name and expr <code>having</code> str No HAVING clause for filtering aggregates <code>incremental.timestamp_column</code> str No Column to identify new data <code>incremental.merge_strategy</code> str No \"replace\", \"sum\", \"min\", or \"max\" <code>audit.load_timestamp</code> bool No Add load_timestamp column <code>audit.source_system</code> str No Add source_system column <p>Example: <pre><code>pattern:\n  type: aggregation\n  params:\n    grain: [date_sk, product_sk, region]\n    measures:\n      - name: total_revenue\n        expr: \"SUM(total_amount)\"\n      - name: order_count\n        expr: \"COUNT(*)\"\n      - name: avg_order_value\n        expr: \"AVG(total_amount)\"\n    having: \"COUNT(*) &gt; 0\"\n    audit:\n      load_timestamp: true\n</code></pre></p>"},{"location":"reference/yaml_schema_v1/#auditconfig","title":"<code>AuditConfig</code>","text":"<p>Configuration for audit columns. | Field | Type | Required | Default | Description | | --- | --- | --- | --- | --- | | load_timestamp | bool | No | <code>True</code> | Add load_timestamp column | | source_system | Optional[str] | No | - | Source system name for source_system column |</p>"},{"location":"reference/api/cli/","title":"CLI API","text":""},{"location":"reference/api/cli/#odibi.cli.main","title":"<code>odibi.cli.main</code>","text":"<p>Main CLI entry point.</p>"},{"location":"reference/api/cli/#odibi.cli.main.main","title":"<code>main()</code>","text":"<p>Main CLI entry point.</p> Source code in <code>odibi\\cli\\main.py</code> <pre><code>def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    # Configure telemetry early\n    setup_telemetry()\n\n    parser = argparse.ArgumentParser(\n        description=\"Odibi Data Pipeline Framework\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nGolden Path (Quick Start):\n  odibi init my_project              Create new project from template\n  cd my_project\n  odibi validate odibi.yaml          Check configuration\n  odibi run odibi.yaml               Run pipeline\n  odibi story last                   View execution story\n\nCore Commands:\n  odibi run config.yaml              Run a pipeline\n  odibi validate config.yaml         Validate configuration\n  odibi graph config.yaml            Visualize dependencies\n  odibi doctor                       Check environment health\n\nIntrospection (for AI tools):\n  odibi list transformers            List all available transformers\n  odibi list patterns                List all available patterns\n  odibi list connections             List all connection types\n  odibi explain &lt;name&gt;               Get detailed docs for any feature\n\nDebugging:\n  odibi story show &lt;path&gt;            View a specific story\n  odibi story last --node &lt;name&gt;     Inspect a failed node\n  odibi graph config.yaml --verbose  Detailed dependency view\n\nLearn more: https://henryodibi11.github.io/Odibi/golden_path/\n        \"\"\",\n    )\n\n    # Global arguments\n    parser.add_argument(\n        \"--log-level\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n        default=\"INFO\",\n        help=\"Set logging verbosity (default: INFO)\",\n    )\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # odibi run\n    run_parser = subparsers.add_parser(\"run\", help=\"Execute pipeline\")\n    run_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    run_parser.add_argument(\n        \"--env\", default=None, help=\"Environment to apply overrides (e.g., dev, qat, prod)\"\n    )\n    run_parser.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Simulate execution without running operations\"\n    )\n    run_parser.add_argument(\n        \"--resume\", action=\"store_true\", help=\"Resume from last failure (skip successful nodes)\"\n    )\n    run_parser.add_argument(\n        \"--parallel\", action=\"store_true\", help=\"Run independent nodes in parallel\"\n    )\n    run_parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=4,\n        help=\"Number of worker threads for parallel execution (default: 4)\",\n    )\n    run_parser.add_argument(\n        \"--on-error\",\n        choices=[\"fail_fast\", \"fail_later\", \"ignore\"],\n        help=\"Override error handling strategy\",\n    )\n    run_parser.add_argument(\n        \"--tag\",\n        help=\"Filter nodes by tag (e.g., --tag daily)\",\n    )\n    run_parser.add_argument(\n        \"--pipeline\",\n        dest=\"pipeline_name\",\n        help=\"Run specific pipeline by name\",\n    )\n    run_parser.add_argument(\n        \"--node\",\n        dest=\"node_name\",\n        help=\"Run specific node by name\",\n    )\n\n    # odibi deploy\n    deploy_parser = subparsers.add_parser(\"deploy\", help=\"Deploy definitions to System Catalog\")\n    deploy_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    deploy_parser.add_argument(\n        \"--env\", default=None, help=\"Environment to apply overrides (e.g., dev, qat, prod)\"\n    )\n\n    # odibi validate\n    validate_parser = subparsers.add_parser(\"validate\", help=\"Validate config\")\n    validate_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    validate_parser.add_argument(\n        \"--env\", default=None, help=\"Environment to apply overrides (e.g., dev, qat, prod)\"\n    )\n\n    # odibi test\n    test_parser = subparsers.add_parser(\"test\", help=\"Run unit tests for transformations\")\n    test_parser.add_argument(\n        \"path\", nargs=\"?\", default=\"tests\", help=\"Path to tests directory or file (default: tests)\"\n    )\n    test_parser.add_argument(\"--snapshot\", action=\"store_true\", help=\"Update snapshots for tests\")\n\n    # odibi docs\n    subparsers.add_parser(\"docs\", help=\"Generate API documentation\")\n\n    # odibi graph\n    graph_parser = subparsers.add_parser(\"graph\", help=\"Visualize dependency graph\")\n    graph_parser.add_argument(\"config\", help=\"Path to YAML config file\")\n    graph_parser.add_argument(\"--pipeline\", help=\"Pipeline name (optional)\")\n    graph_parser.add_argument(\n        \"--env\", default=None, help=\"Environment to apply overrides (e.g., dev, qat, prod)\"\n    )\n    graph_parser.add_argument(\n        \"--format\",\n        choices=[\"ascii\", \"dot\", \"mermaid\"],\n        default=\"ascii\",\n        help=\"Output format (default: ascii)\",\n    )\n    graph_parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose output\")\n\n    # odibi story\n    add_story_parser(subparsers)\n\n    # odibi secrets\n    add_secrets_parser(subparsers)\n\n    # odibi init-pipeline (create/init)\n    add_init_parser(subparsers)\n\n    # odibi doctor\n    add_doctor_parser(subparsers)\n\n    # odibi ui\n    add_ui_parser(subparsers)\n\n    # odibi export\n    add_export_parser(subparsers)\n\n    # odibi catalog\n    add_catalog_parser(subparsers)\n\n    # odibi schema\n    add_schema_parser(subparsers)\n\n    # odibi lineage\n    add_lineage_parser(subparsers)\n\n    # odibi system\n    add_system_parser(subparsers)\n\n    # odibi list (transformers, patterns, connections)\n    add_list_parser(subparsers)\n\n    # odibi explain &lt;name&gt;\n    add_explain_parser(subparsers)\n\n    args = parser.parse_args()\n\n    # Configure logging\n    import logging\n\n    logging.basicConfig(\n        level=getattr(logging, args.log_level),\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    if args.command == \"run\":\n        return run_command(args)\n    elif args.command == \"deploy\":\n        from odibi.cli.deploy import deploy_command\n\n        return deploy_command(args)\n    elif args.command == \"docs\":\n        generate_docs()\n        return 0\n    elif args.command == \"validate\":\n        return validate_command(args)\n    elif args.command == \"test\":\n        return test_command(args)\n    elif args.command == \"graph\":\n        return graph_command(args)\n    elif args.command == \"story\":\n        return story_command(args)\n    elif args.command == \"secrets\":\n        return secrets_command(args)\n    elif args.command in [\"init-pipeline\", \"create\", \"init\", \"generate-project\"]:\n        return init_pipeline_command(args)\n    elif args.command == \"doctor\":\n        return doctor_command(args)\n    elif args.command == \"ui\":\n        return ui_command(args)\n    elif args.command == \"export\":\n        return export_command(args)\n    elif args.command == \"catalog\":\n        return catalog_command(args)\n    elif args.command == \"schema\":\n        return schema_command(args)\n    elif args.command == \"lineage\":\n        return lineage_command(args)\n    elif args.command == \"system\":\n        return system_command(args)\n    elif args.command == \"list\":\n        return list_command(args)\n    elif args.command == \"explain\":\n        return explain_command(args)\n    else:\n        parser.print_help()\n        return 1\n</code></pre>"},{"location":"reference/api/config/","title":"Configuration API","text":""},{"location":"reference/api/config/#odibi.config","title":"<code>odibi.config</code>","text":"<p>Configuration models for ODIBI framework.</p>"},{"location":"reference/api/config/#odibi.config.ConnectionConfig","title":"<code>ConnectionConfig = Union[LocalConnectionConfig, AzureBlobConnectionConfig, DeltaConnectionConfig, SQLServerConnectionConfig, HttpConnectionConfig, CustomConnectionConfig]</code>  <code>module-attribute</code>","text":""},{"location":"reference/api/config/#odibi.config.EngineType","title":"<code>EngineType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported execution engines.</p> Source code in <code>odibi\\config.py</code> <pre><code>class EngineType(str, Enum):\n    \"\"\"Supported execution engines.\"\"\"\n\n    SPARK = \"spark\"\n    PANDAS = \"pandas\"\n    POLARS = \"polars\"\n</code></pre>"},{"location":"reference/api/config/#odibi.config.ConnectionType","title":"<code>ConnectionType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported connection types.</p> Source code in <code>odibi\\config.py</code> <pre><code>class ConnectionType(str, Enum):\n    \"\"\"Supported connection types.\"\"\"\n\n    LOCAL = \"local\"\n    AZURE_BLOB = \"azure_blob\"\n    DELTA = \"delta\"\n    SQL_SERVER = \"sql_server\"\n    HTTP = \"http\"\n</code></pre>"},{"location":"reference/api/config/#odibi.config.WriteMode","title":"<code>WriteMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Write modes for output operations.</p> Source code in <code>odibi\\config.py</code> <pre><code>class WriteMode(str, Enum):\n    \"\"\"Write modes for output operations.\"\"\"\n\n    OVERWRITE = \"overwrite\"\n    APPEND = \"append\"\n    UPSERT = \"upsert\"\n    APPEND_ONCE = \"append_once\"\n    MERGE = \"merge\"  # SQL Server MERGE (staging table + T-SQL MERGE)\n</code></pre>"},{"location":"reference/api/config/#odibi.config.AlertConfig","title":"<code>AlertConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for alerts with throttling support.</p> <p>Supports Slack, Teams, and generic webhooks with event-specific payloads.</p> <p>Available Events: - <code>on_start</code> - Pipeline started - <code>on_success</code> - Pipeline completed successfully - <code>on_failure</code> - Pipeline failed - <code>on_quarantine</code> - Rows were quarantined - <code>on_gate_block</code> - Quality gate blocked the pipeline - <code>on_threshold_breach</code> - A threshold was exceeded</p> <p>Example: <pre><code>alerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n</code></pre></p> Source code in <code>odibi\\config.py</code> <pre><code>class AlertConfig(BaseModel):\n    \"\"\"\n    Configuration for alerts with throttling support.\n\n    Supports Slack, Teams, and generic webhooks with event-specific payloads.\n\n    **Available Events:**\n    - `on_start` - Pipeline started\n    - `on_success` - Pipeline completed successfully\n    - `on_failure` - Pipeline failed\n    - `on_quarantine` - Rows were quarantined\n    - `on_gate_block` - Quality gate blocked the pipeline\n    - `on_threshold_breach` - A threshold was exceeded\n\n    Example:\n    ```yaml\n    alerts:\n      - type: slack\n        url: \"${SLACK_WEBHOOK_URL}\"\n        on_events:\n          - on_failure\n          - on_quarantine\n          - on_gate_block\n        metadata:\n          throttle_minutes: 15\n          max_per_hour: 10\n          channel: \"#data-alerts\"\n    ```\n    \"\"\"\n\n    type: AlertType\n    url: str = Field(description=\"Webhook URL\")\n    on_events: List[AlertEvent] = Field(\n        default=[AlertEvent.ON_FAILURE],\n        description=\"Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach\",\n    )\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Extra metadata: throttle_minutes, max_per_hour, channel, etc.\",\n    )\n</code></pre>"},{"location":"reference/api/config/#odibi.config.TransformConfig","title":"<code>TransformConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for transformation steps within a node.</p> <p>When to Use: Custom business logic, data cleaning, SQL transformations.</p> <p>Key Concepts: - <code>steps</code>: Ordered list of operations (SQL, functions, or both) - Each step receives the DataFrame from the previous step - Steps execute in order: step1 \u2192 step2 \u2192 step3</p> <p>See Also: Transformer Catalog</p> <p>Transformer vs Transform: - <code>transformer</code>: Single heavy operation (scd2, merge, deduplicate) - <code>transform.steps</code>: Chain of lighter operations</p>"},{"location":"reference/api/config/#odibi.config.TransformConfig--transformation-pipeline-guide","title":"\ud83d\udd27 \"Transformation Pipeline\" Guide","text":"<p>Business Problem: \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"</p> <p>The Solution: Chain multiple steps together. Output of Step 1 becomes input of Step 2.</p> <p>Function Registry: The <code>function</code> step type looks up functions registered with <code>@transform</code> (or <code>@register</code>). This allows you to use the same registered functions as both top-level Transformers and steps in a chain.</p> <p>Recipe: The Mix-and-Match <pre><code>transform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n</code></pre></p> Source code in <code>odibi\\config.py</code> <pre><code>class TransformConfig(BaseModel):\n    \"\"\"\n    Configuration for transformation steps within a node.\n\n    **When to Use:** Custom business logic, data cleaning, SQL transformations.\n\n    **Key Concepts:**\n    - `steps`: Ordered list of operations (SQL, functions, or both)\n    - Each step receives the DataFrame from the previous step\n    - Steps execute in order: step1 \u2192 step2 \u2192 step3\n\n    **See Also:** [Transformer Catalog](#nodeconfig)\n\n    **Transformer vs Transform:**\n    - `transformer`: Single heavy operation (scd2, merge, deduplicate)\n    - `transform.steps`: Chain of lighter operations\n\n    ### \ud83d\udd27 \"Transformation Pipeline\" Guide\n\n    **Business Problem:**\n    \"I have complex logic that mixes SQL for speed and Python for complex calculations.\"\n\n    **The Solution:**\n    Chain multiple steps together. Output of Step 1 becomes input of Step 2.\n\n    **Function Registry:**\n    The `function` step type looks up functions registered with `@transform` (or `@register`).\n    This allows you to use the *same* registered functions as both top-level Transformers and steps in a chain.\n\n    **Recipe: The Mix-and-Match**\n    ```yaml\n    transform:\n      steps:\n        # Step 1: SQL Filter (Fast)\n        - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n        # Step 2: Custom Python Function (Complex Logic)\n        # Looks up 'calculate_lifetime_value' in the registry\n        - function: \"calculate_lifetime_value\"\n          params: { discount_rate: 0.05 }\n\n        # Step 3: Built-in Operation (Standard)\n        - operation: \"drop_duplicates\"\n          params: { subset: [\"user_id\"] }\n    ```\n    \"\"\"\n\n    steps: List[Union[str, TransformStep]] = Field(\n        description=\"List of transformation steps (SQL strings or TransformStep configs)\"\n    )\n</code></pre>"},{"location":"reference/api/config/#odibi.config.ValidationConfig","title":"<code>ValidationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for data validation (post-transform checks).</p> <p>When to Use: Output data quality checks that run after transformation but before writing.</p> <p>See Also: Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)</p>"},{"location":"reference/api/config/#odibi.config.ValidationConfig--the-indestructible-pipeline-pattern","title":"\ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern","text":"<p>Business Problem: \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it before it lands.\"</p> <p>The Solution: A Quality Gate that runs after transformation but before writing.</p> <p>Recipe: The Quality Gate <pre><code>validation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges &amp; Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at &lt;= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n</code></pre></p> <p>Recipe: Quarantine + Gate <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n</code></pre></p> Source code in <code>odibi\\config.py</code> <pre><code>class ValidationConfig(BaseModel):\n    \"\"\"\n    Configuration for data validation (post-transform checks).\n\n    **When to Use:** Output data quality checks that run after transformation but before writing.\n\n    **See Also:** Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)\n\n    ### \ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern\n\n    **Business Problem:**\n    \"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it *before* it lands.\"\n\n    **The Solution:**\n    A Quality Gate that runs *after* transformation but *before* writing.\n\n    **Recipe: The Quality Gate**\n    ```yaml\n    validation:\n      mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n      on_fail: \"alert\"      # alert or ignore\n\n      tests:\n        # 1. Completeness\n        - type: \"not_null\"\n          columns: [\"transaction_id\", \"customer_id\"]\n\n        # 2. Integrity\n        - type: \"unique\"\n          columns: [\"transaction_id\"]\n\n        - type: \"accepted_values\"\n          column: \"status\"\n          values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n        # 3. Ranges &amp; Patterns\n        - type: \"range\"\n          column: \"age\"\n          min: 18\n          max: 120\n\n        - type: \"regex_match\"\n          column: \"email\"\n          pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n\n        # 4. Business Logic (SQL)\n        - type: \"custom_sql\"\n          name: \"dates_ordered\"\n          condition: \"created_at &lt;= completed_at\"\n          threshold: 0.01   # Allow 1% failure\n    ```\n\n    **Recipe: Quarantine + Gate**\n    ```yaml\n    validation:\n      tests:\n        - type: not_null\n          columns: [customer_id]\n          on_fail: quarantine\n      quarantine:\n        connection: silver\n        path: customers_quarantine\n      gate:\n        require_pass_rate: 0.95\n        on_fail: abort\n    ```\n    \"\"\"\n\n    mode: ValidationAction = Field(\n        default=ValidationAction.FAIL,\n        description=\"Execution mode: 'fail' (stop pipeline) or 'warn' (log only)\",\n    )\n    on_fail: OnFailAction = Field(\n        default=OnFailAction.ALERT,\n        description=\"Action on failure: 'alert' (send notification) or 'ignore'\",\n    )\n    tests: List[TestConfig] = Field(default_factory=list, description=\"List of validation tests\")\n    quarantine: Optional[QuarantineConfig] = Field(\n        default=None,\n        description=\"Quarantine configuration for failed rows\",\n    )\n    gate: Optional[GateConfig] = Field(\n        default=None,\n        description=\"Quality gate configuration for batch-level validation\",\n    )\n    fail_fast: bool = Field(\n        default=False,\n        description=\"Stop validation on first failure. Skips remaining tests for faster feedback.\",\n    )\n    cache_df: bool = Field(\n        default=False,\n        description=\"Cache DataFrame before validation (Spark only). Improves performance with many tests.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_quarantine_config(self):\n        \"\"\"Warn if quarantine config exists but no tests use on_fail: quarantine.\"\"\"\n        import warnings\n\n        if self.quarantine and self.tests:\n            has_quarantine_tests = any(t.on_fail == ContractSeverity.QUARANTINE for t in self.tests)\n            if not has_quarantine_tests:\n                warnings.warn(\n                    \"Quarantine config is defined but no tests have 'on_fail: quarantine'. \"\n                    \"Quarantine will not be used. Add 'on_fail: quarantine' to tests that \"\n                    \"should route failed rows to quarantine.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n        return self\n</code></pre>"},{"location":"reference/api/config/#odibi.config.ValidationConfig.validate_quarantine_config","title":"<code>validate_quarantine_config()</code>","text":"<p>Warn if quarantine config exists but no tests use on_fail: quarantine.</p> Source code in <code>odibi\\config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_quarantine_config(self):\n    \"\"\"Warn if quarantine config exists but no tests use on_fail: quarantine.\"\"\"\n    import warnings\n\n    if self.quarantine and self.tests:\n        has_quarantine_tests = any(t.on_fail == ContractSeverity.QUARANTINE for t in self.tests)\n        if not has_quarantine_tests:\n            warnings.warn(\n                \"Quarantine config is defined but no tests have 'on_fail: quarantine'. \"\n                \"Quarantine will not be used. Add 'on_fail: quarantine' to tests that \"\n                \"should route failed rows to quarantine.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    return self\n</code></pre>"},{"location":"reference/api/config/#odibi.config.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a pipeline.</p> <p>Example: <pre><code>pipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    owner: \"data-team@example.com\"\n    freshness_sla: \"6h\"\n    nodes:\n      - name: \"node1\"\n        ...\n</code></pre></p> Source code in <code>odibi\\config.py</code> <pre><code>class PipelineConfig(BaseModel):\n    \"\"\"\n    Configuration for a pipeline.\n\n    Example:\n    ```yaml\n    pipelines:\n      - pipeline: \"user_onboarding\"\n        description: \"Ingest and process new users\"\n        layer: \"silver\"\n        owner: \"data-team@example.com\"\n        freshness_sla: \"6h\"\n        nodes:\n          - name: \"node1\"\n            ...\n    ```\n    \"\"\"\n\n    pipeline: str = Field(description=\"Pipeline name\")\n    description: Optional[str] = Field(default=None, description=\"Pipeline description\")\n    layer: Optional[str] = Field(default=None, description=\"Logical layer (bronze/silver/gold)\")\n    owner: Optional[str] = Field(\n        default=None,\n        description=\"Pipeline owner (email or name)\",\n    )\n    freshness_sla: Optional[str] = Field(\n        default=None,\n        description=\"Expected freshness, e.g. '6h', '1d'\",\n    )\n    freshness_anchor: Literal[\"run_completion\", \"table_max_timestamp\", \"watermark_state\"] = Field(\n        default=\"run_completion\",\n        description=\"What defines freshness. Only 'run_completion' implemented initially.\",\n    )\n    nodes: List[NodeConfig] = Field(description=\"List of nodes in this pipeline\")\n\n    @field_validator(\"nodes\")\n    @classmethod\n    def check_unique_node_names(cls, nodes: List[NodeConfig]) -&gt; List[NodeConfig]:\n        \"\"\"Ensure all node names are unique within the pipeline.\"\"\"\n        names = [node.name for node in nodes]\n        if len(names) != len(set(names)):\n            duplicates = [name for name in names if names.count(name) &gt; 1]\n            raise ValueError(f\"Duplicate node names found: {set(duplicates)}\")\n        return nodes\n\n    @model_validator(mode=\"after\")\n    def auto_populate_depends_on_from_inputs(self):\n        \"\"\"\n        Auto-populate depends_on for same-pipeline references in inputs.\n\n        If a node has inputs like $silver.other_node and this is the silver pipeline,\n        automatically add 'other_node' to depends_on for correct execution order.\n        \"\"\"\n        node_names = {node.name for node in self.nodes}\n\n        for node in self.nodes:\n            if not node.inputs:\n                continue\n\n            for input_name, ref in node.inputs.items():\n                if not isinstance(ref, str) or not ref.startswith(\"$\"):\n                    continue\n\n                # Parse $pipeline.node reference\n                parts = ref[1:].split(\".\", 1)\n                if len(parts) != 2:\n                    continue\n\n                ref_pipeline, ref_node = parts\n\n                # Check if reference is to same pipeline\n                if ref_pipeline == self.pipeline and ref_node in node_names:\n                    # Add to depends_on if not already there\n                    if ref_node not in node.depends_on:\n                        node.depends_on.append(ref_node)\n\n        return self\n</code></pre>"},{"location":"reference/api/config/#odibi.config.PipelineConfig.auto_populate_depends_on_from_inputs","title":"<code>auto_populate_depends_on_from_inputs()</code>","text":"<p>Auto-populate depends_on for same-pipeline references in inputs.</p> <p>If a node has inputs like $silver.other_node and this is the silver pipeline, automatically add 'other_node' to depends_on for correct execution order.</p> Source code in <code>odibi\\config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef auto_populate_depends_on_from_inputs(self):\n    \"\"\"\n    Auto-populate depends_on for same-pipeline references in inputs.\n\n    If a node has inputs like $silver.other_node and this is the silver pipeline,\n    automatically add 'other_node' to depends_on for correct execution order.\n    \"\"\"\n    node_names = {node.name for node in self.nodes}\n\n    for node in self.nodes:\n        if not node.inputs:\n            continue\n\n        for input_name, ref in node.inputs.items():\n            if not isinstance(ref, str) or not ref.startswith(\"$\"):\n                continue\n\n            # Parse $pipeline.node reference\n            parts = ref[1:].split(\".\", 1)\n            if len(parts) != 2:\n                continue\n\n            ref_pipeline, ref_node = parts\n\n            # Check if reference is to same pipeline\n            if ref_pipeline == self.pipeline and ref_node in node_names:\n                # Add to depends_on if not already there\n                if ref_node not in node.depends_on:\n                    node.depends_on.append(ref_node)\n\n    return self\n</code></pre>"},{"location":"reference/api/config/#odibi.config.PipelineConfig.check_unique_node_names","title":"<code>check_unique_node_names(nodes)</code>  <code>classmethod</code>","text":"<p>Ensure all node names are unique within the pipeline.</p> Source code in <code>odibi\\config.py</code> <pre><code>@field_validator(\"nodes\")\n@classmethod\ndef check_unique_node_names(cls, nodes: List[NodeConfig]) -&gt; List[NodeConfig]:\n    \"\"\"Ensure all node names are unique within the pipeline.\"\"\"\n    names = [node.name for node in nodes]\n    if len(names) != len(set(names)):\n        duplicates = [name for name in names if names.count(name) &gt; 1]\n        raise ValueError(f\"Duplicate node names found: {set(duplicates)}\")\n    return nodes\n</code></pre>"},{"location":"reference/api/config/#odibi.config.StoryConfig","title":"<code>StoryConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Story generation configuration.</p> <p>Stories are ODIBI's core value - execution reports with lineage. They must use a connection for consistent, traceable output.</p> <p>Example: <pre><code>story:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n</code></pre></p> <p>Failure Sample Settings: - <code>failure_sample_size</code>: Number of failed rows to capture per validation (default: 100) - <code>max_failure_samples</code>: Total failed rows across all validations (default: 500) - <code>max_sampled_validations</code>: After this many validations, show only counts (default: 5)</p> Source code in <code>odibi\\config.py</code> <pre><code>class StoryConfig(BaseModel):\n    \"\"\"\n    Story generation configuration.\n\n    Stories are ODIBI's core value - execution reports with lineage.\n    They must use a connection for consistent, traceable output.\n\n    Example:\n    ```yaml\n    story:\n      connection: \"local_data\"\n      path: \"stories/\"\n      retention_days: 30\n      failure_sample_size: 100\n      max_failure_samples: 500\n      max_sampled_validations: 5\n    ```\n\n    **Failure Sample Settings:**\n    - `failure_sample_size`: Number of failed rows to capture per validation (default: 100)\n    - `max_failure_samples`: Total failed rows across all validations (default: 500)\n    - `max_sampled_validations`: After this many validations, show only counts (default: 5)\n    \"\"\"\n\n    connection: str = Field(\n        description=\"Connection name for story output (uses connection's path resolution)\"\n    )\n    path: str = Field(description=\"Path for stories (relative to connection base_path)\")\n    max_sample_rows: int = Field(default=10, ge=0, le=100)\n    auto_generate: bool = True\n    retention_days: Optional[int] = Field(default=30, ge=1, description=\"Days to keep stories\")\n    retention_count: Optional[int] = Field(\n        default=100, ge=1, description=\"Max number of stories to keep\"\n    )\n\n    # Failure sample settings (troubleshooting)\n    failure_sample_size: int = Field(\n        default=100,\n        ge=0,\n        le=1000,\n        description=\"Number of failed rows to capture per validation rule\",\n    )\n    max_failure_samples: int = Field(\n        default=500,\n        ge=0,\n        le=5000,\n        description=\"Maximum total failed rows across all validations\",\n    )\n    max_sampled_validations: int = Field(\n        default=5,\n        ge=1,\n        le=20,\n        description=\"After this many validations, show only counts (no samples)\",\n    )\n\n    # Performance settings\n    async_generation: bool = Field(\n        default=False,\n        description=(\n            \"Generate stories asynchronously (fire-and-forget). \"\n            \"Pipeline returns immediately while story writes in background. \"\n            \"Improves multi-pipeline performance by ~5-10s per pipeline.\"\n        ),\n    )\n\n    # Lineage settings\n    generate_lineage: bool = Field(\n        default=True,\n        description=(\n            \"Generate combined lineage graph from all stories. \"\n            \"Creates a unified view of data flow across pipelines.\"\n        ),\n    )\n\n    # Documentation generation\n    docs: Optional[\"DocsConfig\"] = Field(\n        default=None,\n        description=(\n            \"Documentation generation settings. \"\n            \"Generates README.md, TECHNICAL_DETAILS.md, NODE_CARDS/*.md from Story data.\"\n        ),\n    )\n\n    @model_validator(mode=\"after\")\n    def check_retention_policy(self):\n        if self.retention_days is None and self.retention_count is None:\n            raise ValueError(\n                \"StoryConfig validation failed: No retention policy specified. \"\n                \"Provide at least one of: 'retention_days' (e.g., 30) or 'retention_count' (e.g., 100). \"\n                \"This controls how long/many story files are kept before cleanup.\"\n            )\n        return self\n</code></pre>"},{"location":"reference/api/connections/","title":"Connections API","text":""},{"location":"reference/api/connections/#odibi.connections.base","title":"<code>odibi.connections.base</code>","text":"<p>Base connection interface.</p>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection","title":"<code>BaseConnection</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for connections.</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>class BaseConnection(ABC):\n    \"\"\"Abstract base class for connections.\"\"\"\n\n    @abstractmethod\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full path for a relative path.\n\n        Args:\n            relative_path: Relative path or table name\n\n        Returns:\n            Full path to resource\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate(self) -&gt; None:\n        \"\"\"Validate connection configuration.\n\n        Raises:\n            ConnectionError: If validation fails\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection.get_path","title":"<code>get_path(relative_path)</code>  <code>abstractmethod</code>","text":"<p>Get full path for a relative path.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>Relative path or table name</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path to resource</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>@abstractmethod\ndef get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full path for a relative path.\n\n    Args:\n        relative_path: Relative path or table name\n\n    Returns:\n        Full path to resource\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.base.BaseConnection.validate","title":"<code>validate()</code>  <code>abstractmethod</code>","text":"<p>Validate connection configuration.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If validation fails</p> Source code in <code>odibi\\connections\\base.py</code> <pre><code>@abstractmethod\ndef validate(self) -&gt; None:\n    \"\"\"Validate connection configuration.\n\n    Raises:\n        ConnectionError: If validation fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local","title":"<code>odibi.connections.local</code>","text":"<p>Local filesystem connection.</p>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection","title":"<code>LocalConnection</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>class LocalConnection(BaseConnection):\n    \"\"\"Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).\"\"\"\n\n    def __init__(self, base_path: str = \"./data\"):\n        \"\"\"Initialize local connection.\n\n        Args:\n            base_path: Base directory for all paths (can be local path or URI)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"local\",\n            connection_name=\"LocalConnection\",\n            action=\"init\",\n            base_path=base_path,\n        )\n\n        self.base_path_str = base_path\n        # Detect URIs: \"://\" (standard URIs) or \"dbfs:/\" (Databricks)\n        # Windows paths like \"C:/path\" or \"D:\\path\" should NOT be treated as URIs\n        # Windows drive letters are single character followed by \":/\" which differs from \"dbfs:/\"\n        self.is_uri = \"://\" in base_path or (\n            \":/\" in base_path and len(base_path.split(\":/\")[0]) &gt; 1\n        )\n\n        if not self.is_uri:\n            self.base_path = Path(base_path)\n            ctx.debug(\n                \"LocalConnection initialized with filesystem path\",\n                base_path=base_path,\n                is_uri=False,\n            )\n        else:\n            self.base_path = None  # Not used for URIs\n            ctx.debug(\n                \"LocalConnection initialized with URI path\",\n                base_path=base_path,\n                is_uri=True,\n            )\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full path for a relative path.\n\n        Args:\n            relative_path: Relative path from base\n\n        Returns:\n            Full absolute path or URI\n        \"\"\"\n        ctx = get_logging_context()\n\n        if self.is_uri:\n            # Use os.path for simple string joining, handling slashes manually for consistency\n            # Strip leading slash from relative to avoid root replacement\n            clean_rel = relative_path.lstrip(\"/\").lstrip(\"\\\\\")\n            # Handle cases where base_path might not have trailing slash\n            if self.base_path_str.endswith(\"/\") or self.base_path_str.endswith(\"\\\\\"):\n                full_path = f\"{self.base_path_str}{clean_rel}\"\n            else:\n                # Use forward slash for URIs\n                full_path = f\"{self.base_path_str}/{clean_rel}\"\n\n            ctx.debug(\n                \"Resolved URI path\",\n                relative_path=relative_path,\n                full_path=full_path,\n            )\n            return full_path\n        else:\n            # Standard local path logic\n            full_path = self.base_path / relative_path\n            resolved = str(full_path.absolute())\n\n            ctx.debug(\n                \"Resolved local path\",\n                relative_path=relative_path,\n                full_path=resolved,\n            )\n            return resolved\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate that base path exists or can be created.\n\n        Raises:\n            ConnectionError: If validation fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating LocalConnection\",\n            base_path=self.base_path_str,\n            is_uri=self.is_uri,\n        )\n\n        if self.is_uri:\n            # Cannot validate/create URIs with local os module\n            # Assume valid or handled by engine\n            ctx.debug(\n                \"Skipping URI validation (handled by engine)\",\n                base_path=self.base_path_str,\n            )\n        else:\n            # Create base directory if it doesn't exist\n            try:\n                self.base_path.mkdir(parents=True, exist_ok=True)\n                ctx.info(\n                    \"LocalConnection validated successfully\",\n                    base_path=str(self.base_path.absolute()),\n                    created=not self.base_path.exists(),\n                )\n            except Exception as e:\n                ctx.error(\n                    \"LocalConnection validation failed\",\n                    base_path=self.base_path_str,\n                    error=str(e),\n                )\n                raise\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.__init__","title":"<code>__init__(base_path='./data')</code>","text":"<p>Initialize local connection.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Base directory for all paths (can be local path or URI)</p> <code>'./data'</code> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def __init__(self, base_path: str = \"./data\"):\n    \"\"\"Initialize local connection.\n\n    Args:\n        base_path: Base directory for all paths (can be local path or URI)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"local\",\n        connection_name=\"LocalConnection\",\n        action=\"init\",\n        base_path=base_path,\n    )\n\n    self.base_path_str = base_path\n    # Detect URIs: \"://\" (standard URIs) or \"dbfs:/\" (Databricks)\n    # Windows paths like \"C:/path\" or \"D:\\path\" should NOT be treated as URIs\n    # Windows drive letters are single character followed by \":/\" which differs from \"dbfs:/\"\n    self.is_uri = \"://\" in base_path or (\n        \":/\" in base_path and len(base_path.split(\":/\")[0]) &gt; 1\n    )\n\n    if not self.is_uri:\n        self.base_path = Path(base_path)\n        ctx.debug(\n            \"LocalConnection initialized with filesystem path\",\n            base_path=base_path,\n            is_uri=False,\n        )\n    else:\n        self.base_path = None  # Not used for URIs\n        ctx.debug(\n            \"LocalConnection initialized with URI path\",\n            base_path=base_path,\n            is_uri=True,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get full path for a relative path.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>Relative path from base</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full absolute path or URI</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full path for a relative path.\n\n    Args:\n        relative_path: Relative path from base\n\n    Returns:\n        Full absolute path or URI\n    \"\"\"\n    ctx = get_logging_context()\n\n    if self.is_uri:\n        # Use os.path for simple string joining, handling slashes manually for consistency\n        # Strip leading slash from relative to avoid root replacement\n        clean_rel = relative_path.lstrip(\"/\").lstrip(\"\\\\\")\n        # Handle cases where base_path might not have trailing slash\n        if self.base_path_str.endswith(\"/\") or self.base_path_str.endswith(\"\\\\\"):\n            full_path = f\"{self.base_path_str}{clean_rel}\"\n        else:\n            # Use forward slash for URIs\n            full_path = f\"{self.base_path_str}/{clean_rel}\"\n\n        ctx.debug(\n            \"Resolved URI path\",\n            relative_path=relative_path,\n            full_path=full_path,\n        )\n        return full_path\n    else:\n        # Standard local path logic\n        full_path = self.base_path / relative_path\n        resolved = str(full_path.absolute())\n\n        ctx.debug(\n            \"Resolved local path\",\n            relative_path=relative_path,\n            full_path=resolved,\n        )\n        return resolved\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.local.LocalConnection.validate","title":"<code>validate()</code>","text":"<p>Validate that base path exists or can be created.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If validation fails</p> Source code in <code>odibi\\connections\\local.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate that base path exists or can be created.\n\n    Raises:\n        ConnectionError: If validation fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating LocalConnection\",\n        base_path=self.base_path_str,\n        is_uri=self.is_uri,\n    )\n\n    if self.is_uri:\n        # Cannot validate/create URIs with local os module\n        # Assume valid or handled by engine\n        ctx.debug(\n            \"Skipping URI validation (handled by engine)\",\n            base_path=self.base_path_str,\n        )\n    else:\n        # Create base directory if it doesn't exist\n        try:\n            self.base_path.mkdir(parents=True, exist_ok=True)\n            ctx.info(\n                \"LocalConnection validated successfully\",\n                base_path=str(self.base_path.absolute()),\n                created=not self.base_path.exists(),\n            )\n        except Exception as e:\n            ctx.error(\n                \"LocalConnection validation failed\",\n                base_path=self.base_path_str,\n                error=str(e),\n            )\n            raise\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls","title":"<code>odibi.connections.azure_adls</code>","text":"<p>Azure Data Lake Storage Gen2 connection (Phase 2A: Multi-mode authentication).</p>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS","title":"<code>AzureADLS</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Azure Data Lake Storage Gen2 connection.</p> <p>Phase 2A: Multi-mode authentication + multi-account support Supports key_vault (recommended), direct_key, service_principal, and managed_identity.</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>class AzureADLS(BaseConnection):\n    \"\"\"Azure Data Lake Storage Gen2 connection.\n\n    Phase 2A: Multi-mode authentication + multi-account support\n    Supports key_vault (recommended), direct_key, service_principal, and managed_identity.\n    \"\"\"\n\n    def __init__(\n        self,\n        account: str,\n        container: str,\n        path_prefix: str = \"\",\n        auth_mode: str = \"key_vault\",\n        key_vault_name: Optional[str] = None,\n        secret_name: Optional[str] = None,\n        account_key: Optional[str] = None,\n        sas_token: Optional[str] = None,\n        tenant_id: Optional[str] = None,\n        client_id: Optional[str] = None,\n        client_secret: Optional[str] = None,\n        validate: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Initialize ADLS connection.\n\n        Args:\n            account: Storage account name (e.g., 'mystorageaccount')\n            container: Container/filesystem name\n            path_prefix: Optional prefix for all paths\n            auth_mode: Authentication mode\n                ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')\n            key_vault_name: Azure Key Vault name (required for key_vault mode)\n            secret_name: Secret name in Key Vault (required for key_vault mode)\n            account_key: Storage account key (required for direct_key mode)\n            sas_token: Shared Access Signature token (required for sas_token mode)\n            tenant_id: Azure Tenant ID (required for service_principal)\n            client_id: Service Principal Client ID (required for service_principal)\n            client_secret: Service Principal Client Secret (required for service_principal)\n            validate: Validate configuration on init\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"azure_adls\",\n            connection_name=f\"{account}/{container}\",\n            action=\"init\",\n            account=account,\n            container=container,\n            auth_mode=auth_mode,\n            path_prefix=path_prefix or \"(none)\",\n        )\n\n        self.account = account\n        self.container = container\n        self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n        self.auth_mode = auth_mode\n        self.key_vault_name = key_vault_name\n        self.secret_name = secret_name\n        self.account_key = account_key\n        self.sas_token = sas_token\n        self.tenant_id = tenant_id\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n        self._cached_key: Optional[str] = None\n        self._cache_lock = threading.Lock()\n\n        if validate:\n            self.validate()\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate ADLS connection configuration.\n\n        Raises:\n            ValueError: If required fields are missing for the selected auth_mode\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating AzureADLS connection\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        if not self.account:\n            ctx.error(\"ADLS connection validation failed: missing 'account'\")\n            raise ValueError(\n                \"ADLS connection requires 'account'. \"\n                \"Provide the storage account name (e.g., account: 'mystorageaccount').\"\n            )\n        if not self.container:\n            ctx.error(\n                \"ADLS connection validation failed: missing 'container'\",\n                account=self.account,\n            )\n            raise ValueError(\n                f\"ADLS connection requires 'container' for account '{self.account}'. \"\n                \"Provide the container/filesystem name.\"\n            )\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"ADLS key_vault mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                    key_vault_name=self.key_vault_name or \"(missing)\",\n                    secret_name=self.secret_name or \"(missing)\",\n                )\n                raise ValueError(\n                    f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"direct_key\":\n            if not self.account_key:\n                ctx.error(\n                    \"ADLS direct_key mode validation failed: missing account_key\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"direct_key mode requires 'account_key' \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n\n            # Warn in production\n            if os.getenv(\"ODIBI_ENV\") == \"production\":\n                ctx.warning(\n                    \"Using direct_key in production is not recommended\",\n                    account=self.account,\n                    container=self.container,\n                )\n                warnings.warn(\n                    f\"\u26a0\ufe0f  Using direct_key in production is not recommended. \"\n                    f\"Use auth_mode: key_vault. Connection: {self.account}/{self.container}\",\n                    UserWarning,\n                )\n        elif self.auth_mode == \"sas_token\":\n            if not self.sas_token and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"ADLS sas_token mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"sas_token mode requires 'sas_token' (or key_vault_name/secret_name) \"\n                    f\"for connection to {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"service_principal\":\n            if not self.tenant_id or not self.client_id:\n                ctx.error(\n                    \"ADLS service_principal mode validation failed\",\n                    account=self.account,\n                    container=self.container,\n                    missing=\"tenant_id and/or client_id\",\n                )\n                raise ValueError(\n                    f\"service_principal mode requires 'tenant_id' and 'client_id' \"\n                    f\"for connection to {self.account}/{self.container}. \"\n                    f\"Got tenant_id={self.tenant_id or '(missing)'}, \"\n                    f\"client_id={self.client_id or '(missing)'}.\"\n                )\n\n            if not self.client_secret and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"ADLS service_principal mode validation failed: missing client_secret\",\n                    account=self.account,\n                    container=self.container,\n                )\n                raise ValueError(\n                    f\"service_principal mode requires 'client_secret' \"\n                    f\"(or key_vault_name/secret_name) for {self.account}/{self.container}\"\n                )\n        elif self.auth_mode == \"managed_identity\":\n            # No specific config required, but we might check if environment supports it\n            ctx.debug(\n                \"Using managed_identity auth mode\",\n                account=self.account,\n                container=self.container,\n            )\n        else:\n            ctx.error(\n                \"ADLS validation failed: unsupported auth_mode\",\n                account=self.account,\n                container=self.container,\n                auth_mode=self.auth_mode,\n            )\n            raise ValueError(\n                f\"Unsupported auth_mode: '{self.auth_mode}'. \"\n                f\"Use 'key_vault', 'direct_key', 'service_principal', or 'managed_identity'.\"\n            )\n\n        ctx.info(\n            \"AzureADLS connection validated successfully\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n    def get_storage_key(self, timeout: float = 30.0) -&gt; Optional[str]:\n        \"\"\"Get storage account key (cached).\n\n        Only relevant for 'key_vault' and 'direct_key' modes.\n\n        Args:\n            timeout: Timeout for Key Vault operations in seconds (default: 30.0)\n\n        Returns:\n            Storage account key or None if not applicable for auth_mode\n\n        Raises:\n            ImportError: If azure libraries not installed (key_vault mode)\n            TimeoutError: If Key Vault fetch exceeds timeout\n            Exception: If Key Vault access fails\n        \"\"\"\n        ctx = get_logging_context()\n\n        with self._cache_lock:\n            # Return cached key if available (double-check inside lock)\n            if self._cached_key:\n                ctx.debug(\n                    \"Using cached storage key\",\n                    account=self.account,\n                    container=self.container,\n                )\n                return self._cached_key\n\n            if self.auth_mode == \"key_vault\":\n                ctx.debug(\n                    \"Fetching storage key from Key Vault\",\n                    account=self.account,\n                    key_vault_name=self.key_vault_name,\n                    secret_name=self.secret_name,\n                    timeout=timeout,\n                )\n\n                try:\n                    import concurrent.futures\n\n                    from azure.identity import DefaultAzureCredential\n                    from azure.keyvault.secrets import SecretClient\n                except ImportError as e:\n                    ctx.error(\n                        \"Key Vault authentication failed: missing azure libraries\",\n                        account=self.account,\n                        error=str(e),\n                    )\n                    raise ImportError(\n                        \"Key Vault authentication requires 'azure-identity' and \"\n                        \"'azure-keyvault-secrets'. Install with: pip install odibi[azure]\"\n                    ) from e\n\n                # Create Key Vault client\n                credential = DefaultAzureCredential()\n                kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n                client = SecretClient(vault_url=kv_uri, credential=credential)\n\n                ctx.debug(\n                    \"Connecting to Key Vault\",\n                    key_vault_uri=kv_uri,\n                    secret_name=self.secret_name,\n                )\n\n                # Fetch secret with timeout protection\n                def _fetch():\n                    secret = client.get_secret(self.secret_name)\n                    return secret.value\n\n                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                    future = executor.submit(_fetch)\n                    try:\n                        self._cached_key = future.result(timeout=timeout)\n                        logger.register_secret(self._cached_key)\n                        ctx.info(\n                            \"Successfully fetched storage key from Key Vault\",\n                            account=self.account,\n                            key_vault_name=self.key_vault_name,\n                        )\n                        return self._cached_key\n                    except concurrent.futures.TimeoutError:\n                        ctx.error(\n                            \"Key Vault fetch timed out\",\n                            account=self.account,\n                            key_vault_name=self.key_vault_name,\n                            secret_name=self.secret_name,\n                            timeout=timeout,\n                        )\n                        raise TimeoutError(\n                            f\"Key Vault fetch timed out after {timeout}s for \"\n                            f\"vault '{self.key_vault_name}', secret '{self.secret_name}'\"\n                        )\n\n            elif self.auth_mode == \"direct_key\":\n                ctx.debug(\n                    \"Using direct account key\",\n                    account=self.account,\n                )\n                return self.account_key\n\n            elif self.auth_mode == \"sas_token\":\n                # Return cached key (fetched from KV) if available, else sas_token arg\n                ctx.debug(\n                    \"Using SAS token\",\n                    account=self.account,\n                    from_cache=bool(self._cached_key),\n                )\n                return self._cached_key or self.sas_token\n\n            # For other modes (SP, MI), we don't use an account key\n            ctx.debug(\n                \"No storage key required for auth_mode\",\n                account=self.account,\n                auth_mode=self.auth_mode,\n            )\n            return None\n\n    def get_client_secret(self) -&gt; Optional[str]:\n        \"\"\"Get Service Principal client secret (cached or literal).\"\"\"\n        return self._cached_key or self.client_secret\n\n    def pandas_storage_options(self) -&gt; Dict[str, Any]:\n        \"\"\"Get storage options for pandas/fsspec.\n\n        Returns:\n            Dictionary with appropriate authentication parameters for fsspec\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Building pandas storage options\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        base_options = {\"account_name\": self.account}\n\n        if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n            return {**base_options, \"account_key\": self.get_storage_key()}\n\n        elif self.auth_mode == \"sas_token\":\n            # Use get_storage_key() which handles KV fallback for SAS\n            return {**base_options, \"sas_token\": self.get_storage_key()}\n\n        elif self.auth_mode == \"service_principal\":\n            return {\n                **base_options,\n                \"tenant_id\": self.tenant_id,\n                \"client_id\": self.client_id,\n                \"client_secret\": self.get_client_secret(),\n            }\n\n        elif self.auth_mode == \"managed_identity\":\n            # adlfs supports using DefaultAzureCredential implicitly if anon=False\n            # and no other creds provided, assuming azure.identity is installed\n            return {**base_options, \"anon\": False}\n\n        return base_options\n\n    def configure_spark(self, spark: \"Any\") -&gt; None:\n        \"\"\"Configure Spark session with storage credentials.\n\n        Args:\n            spark: SparkSession instance\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Configuring Spark for AzureADLS\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n\n        if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n            config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(config_key, self.get_storage_key())\n            ctx.debug(\n                \"Set Spark config for account key\",\n                config_key=config_key,\n            )\n\n        elif self.auth_mode == \"sas_token\":\n            # SAS Token Configuration\n            # fs.azure.sas.token.provider.type -&gt; FixedSASTokenProvider\n            # fs.azure.sas.fixed.token -&gt; &lt;token&gt;\n            provider_key = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(provider_key, \"SAS\")\n\n            sas_provider_key = (\n                f\"fs.azure.sas.token.provider.type.{self.account}.dfs.core.windows.net\"\n            )\n            spark.conf.set(\n                sas_provider_key, \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n            )\n\n            sas_token = self.get_storage_key()\n\n            sas_token_key = f\"fs.azure.sas.fixed.token.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(sas_token_key, sas_token)\n\n            ctx.debug(\n                \"Set Spark config for SAS token\",\n                auth_type_key=provider_key,\n                provider_key=sas_provider_key,\n            )\n\n        elif self.auth_mode == \"service_principal\":\n            # Configure OAuth for ADLS Gen2\n            # Ref: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html\n            prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"OAuth\")\n\n            prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n            prefix = f\"fs.azure.account.oauth2.client.id.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, self.client_id)\n\n            prefix = f\"fs.azure.account.oauth2.client.secret.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, self.get_client_secret())\n\n            prefix = f\"fs.azure.account.oauth2.client.endpoint.{self.account}.dfs.core.windows.net\"\n            endpoint = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n            spark.conf.set(prefix, endpoint)\n\n            ctx.debug(\n                \"Set Spark config for service principal OAuth\",\n                tenant_id=self.tenant_id,\n                client_id=self.client_id,\n            )\n\n        elif self.auth_mode == \"managed_identity\":\n            prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"OAuth\")\n\n            prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n            spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n\n            ctx.debug(\n                \"Set Spark config for managed identity\",\n                account=self.account,\n            )\n\n        ctx.info(\n            \"Spark configuration complete\",\n            account=self.account,\n            auth_mode=self.auth_mode,\n        )\n\n    def uri(self, path: str) -&gt; str:\n        \"\"\"Build abfss:// URI for given path.\n\n        Args:\n            path: Relative path within container\n\n        Returns:\n            Full abfss:// URI\n\n        Example:\n            &gt;&gt;&gt; conn = AzureADLS(\n            ...     account=\"myaccount\", container=\"data\",\n            ...     auth_mode=\"direct_key\", account_key=\"key123\"\n            ... )\n            &gt;&gt;&gt; conn.uri(\"folder/file.csv\")\n            'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'\n        \"\"\"\n        if self.path_prefix:\n            full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n        else:\n            full_path = path.lstrip(\"/\")\n\n        return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get full abfss:// URI for relative path.\"\"\"\n        ctx = get_logging_context()\n        full_uri = self.uri(relative_path)\n\n        ctx.debug(\n            \"Resolved ADLS path\",\n            account=self.account,\n            container=self.container,\n            relative_path=relative_path,\n            full_uri=full_uri,\n        )\n\n        return full_uri\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.__init__","title":"<code>__init__(account, container, path_prefix='', auth_mode='key_vault', key_vault_name=None, secret_name=None, account_key=None, sas_token=None, tenant_id=None, client_id=None, client_secret=None, validate=True, **kwargs)</code>","text":"<p>Initialize ADLS connection.</p> <p>Parameters:</p> Name Type Description Default <code>account</code> <code>str</code> <p>Storage account name (e.g., 'mystorageaccount')</p> required <code>container</code> <code>str</code> <p>Container/filesystem name</p> required <code>path_prefix</code> <code>str</code> <p>Optional prefix for all paths</p> <code>''</code> <code>auth_mode</code> <code>str</code> <p>Authentication mode ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')</p> <code>'key_vault'</code> <code>key_vault_name</code> <code>Optional[str]</code> <p>Azure Key Vault name (required for key_vault mode)</p> <code>None</code> <code>secret_name</code> <code>Optional[str]</code> <p>Secret name in Key Vault (required for key_vault mode)</p> <code>None</code> <code>account_key</code> <code>Optional[str]</code> <p>Storage account key (required for direct_key mode)</p> <code>None</code> <code>sas_token</code> <code>Optional[str]</code> <p>Shared Access Signature token (required for sas_token mode)</p> <code>None</code> <code>tenant_id</code> <code>Optional[str]</code> <p>Azure Tenant ID (required for service_principal)</p> <code>None</code> <code>client_id</code> <code>Optional[str]</code> <p>Service Principal Client ID (required for service_principal)</p> <code>None</code> <code>client_secret</code> <code>Optional[str]</code> <p>Service Principal Client Secret (required for service_principal)</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Validate configuration on init</p> <code>True</code> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def __init__(\n    self,\n    account: str,\n    container: str,\n    path_prefix: str = \"\",\n    auth_mode: str = \"key_vault\",\n    key_vault_name: Optional[str] = None,\n    secret_name: Optional[str] = None,\n    account_key: Optional[str] = None,\n    sas_token: Optional[str] = None,\n    tenant_id: Optional[str] = None,\n    client_id: Optional[str] = None,\n    client_secret: Optional[str] = None,\n    validate: bool = True,\n    **kwargs,\n):\n    \"\"\"Initialize ADLS connection.\n\n    Args:\n        account: Storage account name (e.g., 'mystorageaccount')\n        container: Container/filesystem name\n        path_prefix: Optional prefix for all paths\n        auth_mode: Authentication mode\n            ('key_vault', 'direct_key', 'sas_token', 'service_principal', 'managed_identity')\n        key_vault_name: Azure Key Vault name (required for key_vault mode)\n        secret_name: Secret name in Key Vault (required for key_vault mode)\n        account_key: Storage account key (required for direct_key mode)\n        sas_token: Shared Access Signature token (required for sas_token mode)\n        tenant_id: Azure Tenant ID (required for service_principal)\n        client_id: Service Principal Client ID (required for service_principal)\n        client_secret: Service Principal Client Secret (required for service_principal)\n        validate: Validate configuration on init\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"azure_adls\",\n        connection_name=f\"{account}/{container}\",\n        action=\"init\",\n        account=account,\n        container=container,\n        auth_mode=auth_mode,\n        path_prefix=path_prefix or \"(none)\",\n    )\n\n    self.account = account\n    self.container = container\n    self.path_prefix = path_prefix.strip(\"/\") if path_prefix else \"\"\n    self.auth_mode = auth_mode\n    self.key_vault_name = key_vault_name\n    self.secret_name = secret_name\n    self.account_key = account_key\n    self.sas_token = sas_token\n    self.tenant_id = tenant_id\n    self.client_id = client_id\n    self.client_secret = client_secret\n\n    self._cached_key: Optional[str] = None\n    self._cache_lock = threading.Lock()\n\n    if validate:\n        self.validate()\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.configure_spark","title":"<code>configure_spark(spark)</code>","text":"<p>Configure Spark session with storage credentials.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>Any</code> <p>SparkSession instance</p> required Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def configure_spark(self, spark: \"Any\") -&gt; None:\n    \"\"\"Configure Spark session with storage credentials.\n\n    Args:\n        spark: SparkSession instance\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Configuring Spark for AzureADLS\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n        config_key = f\"fs.azure.account.key.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(config_key, self.get_storage_key())\n        ctx.debug(\n            \"Set Spark config for account key\",\n            config_key=config_key,\n        )\n\n    elif self.auth_mode == \"sas_token\":\n        # SAS Token Configuration\n        # fs.azure.sas.token.provider.type -&gt; FixedSASTokenProvider\n        # fs.azure.sas.fixed.token -&gt; &lt;token&gt;\n        provider_key = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(provider_key, \"SAS\")\n\n        sas_provider_key = (\n            f\"fs.azure.sas.token.provider.type.{self.account}.dfs.core.windows.net\"\n        )\n        spark.conf.set(\n            sas_provider_key, \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n        )\n\n        sas_token = self.get_storage_key()\n\n        sas_token_key = f\"fs.azure.sas.fixed.token.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(sas_token_key, sas_token)\n\n        ctx.debug(\n            \"Set Spark config for SAS token\",\n            auth_type_key=provider_key,\n            provider_key=sas_provider_key,\n        )\n\n    elif self.auth_mode == \"service_principal\":\n        # Configure OAuth for ADLS Gen2\n        # Ref: https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html\n        prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"OAuth\")\n\n        prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n\n        prefix = f\"fs.azure.account.oauth2.client.id.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, self.client_id)\n\n        prefix = f\"fs.azure.account.oauth2.client.secret.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, self.get_client_secret())\n\n        prefix = f\"fs.azure.account.oauth2.client.endpoint.{self.account}.dfs.core.windows.net\"\n        endpoint = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n        spark.conf.set(prefix, endpoint)\n\n        ctx.debug(\n            \"Set Spark config for service principal OAuth\",\n            tenant_id=self.tenant_id,\n            client_id=self.client_id,\n        )\n\n    elif self.auth_mode == \"managed_identity\":\n        prefix = f\"fs.azure.account.auth.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"OAuth\")\n\n        prefix = f\"fs.azure.account.oauth.provider.type.{self.account}.dfs.core.windows.net\"\n        spark.conf.set(prefix, \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\")\n\n        ctx.debug(\n            \"Set Spark config for managed identity\",\n            account=self.account,\n        )\n\n    ctx.info(\n        \"Spark configuration complete\",\n        account=self.account,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_client_secret","title":"<code>get_client_secret()</code>","text":"<p>Get Service Principal client secret (cached or literal).</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_client_secret(self) -&gt; Optional[str]:\n    \"\"\"Get Service Principal client secret (cached or literal).\"\"\"\n    return self._cached_key or self.client_secret\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get full abfss:// URI for relative path.</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get full abfss:// URI for relative path.\"\"\"\n    ctx = get_logging_context()\n    full_uri = self.uri(relative_path)\n\n    ctx.debug(\n        \"Resolved ADLS path\",\n        account=self.account,\n        container=self.container,\n        relative_path=relative_path,\n        full_uri=full_uri,\n    )\n\n    return full_uri\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.get_storage_key","title":"<code>get_storage_key(timeout=30.0)</code>","text":"<p>Get storage account key (cached).</p> <p>Only relevant for 'key_vault' and 'direct_key' modes.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Timeout for Key Vault operations in seconds (default: 30.0)</p> <code>30.0</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Storage account key or None if not applicable for auth_mode</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If azure libraries not installed (key_vault mode)</p> <code>TimeoutError</code> <p>If Key Vault fetch exceeds timeout</p> <code>Exception</code> <p>If Key Vault access fails</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def get_storage_key(self, timeout: float = 30.0) -&gt; Optional[str]:\n    \"\"\"Get storage account key (cached).\n\n    Only relevant for 'key_vault' and 'direct_key' modes.\n\n    Args:\n        timeout: Timeout for Key Vault operations in seconds (default: 30.0)\n\n    Returns:\n        Storage account key or None if not applicable for auth_mode\n\n    Raises:\n        ImportError: If azure libraries not installed (key_vault mode)\n        TimeoutError: If Key Vault fetch exceeds timeout\n        Exception: If Key Vault access fails\n    \"\"\"\n    ctx = get_logging_context()\n\n    with self._cache_lock:\n        # Return cached key if available (double-check inside lock)\n        if self._cached_key:\n            ctx.debug(\n                \"Using cached storage key\",\n                account=self.account,\n                container=self.container,\n            )\n            return self._cached_key\n\n        if self.auth_mode == \"key_vault\":\n            ctx.debug(\n                \"Fetching storage key from Key Vault\",\n                account=self.account,\n                key_vault_name=self.key_vault_name,\n                secret_name=self.secret_name,\n                timeout=timeout,\n            )\n\n            try:\n                import concurrent.futures\n\n                from azure.identity import DefaultAzureCredential\n                from azure.keyvault.secrets import SecretClient\n            except ImportError as e:\n                ctx.error(\n                    \"Key Vault authentication failed: missing azure libraries\",\n                    account=self.account,\n                    error=str(e),\n                )\n                raise ImportError(\n                    \"Key Vault authentication requires 'azure-identity' and \"\n                    \"'azure-keyvault-secrets'. Install with: pip install odibi[azure]\"\n                ) from e\n\n            # Create Key Vault client\n            credential = DefaultAzureCredential()\n            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n            client = SecretClient(vault_url=kv_uri, credential=credential)\n\n            ctx.debug(\n                \"Connecting to Key Vault\",\n                key_vault_uri=kv_uri,\n                secret_name=self.secret_name,\n            )\n\n            # Fetch secret with timeout protection\n            def _fetch():\n                secret = client.get_secret(self.secret_name)\n                return secret.value\n\n            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n                future = executor.submit(_fetch)\n                try:\n                    self._cached_key = future.result(timeout=timeout)\n                    logger.register_secret(self._cached_key)\n                    ctx.info(\n                        \"Successfully fetched storage key from Key Vault\",\n                        account=self.account,\n                        key_vault_name=self.key_vault_name,\n                    )\n                    return self._cached_key\n                except concurrent.futures.TimeoutError:\n                    ctx.error(\n                        \"Key Vault fetch timed out\",\n                        account=self.account,\n                        key_vault_name=self.key_vault_name,\n                        secret_name=self.secret_name,\n                        timeout=timeout,\n                    )\n                    raise TimeoutError(\n                        f\"Key Vault fetch timed out after {timeout}s for \"\n                        f\"vault '{self.key_vault_name}', secret '{self.secret_name}'\"\n                    )\n\n        elif self.auth_mode == \"direct_key\":\n            ctx.debug(\n                \"Using direct account key\",\n                account=self.account,\n            )\n            return self.account_key\n\n        elif self.auth_mode == \"sas_token\":\n            # Return cached key (fetched from KV) if available, else sas_token arg\n            ctx.debug(\n                \"Using SAS token\",\n                account=self.account,\n                from_cache=bool(self._cached_key),\n            )\n            return self._cached_key or self.sas_token\n\n        # For other modes (SP, MI), we don't use an account key\n        ctx.debug(\n            \"No storage key required for auth_mode\",\n            account=self.account,\n            auth_mode=self.auth_mode,\n        )\n        return None\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.pandas_storage_options","title":"<code>pandas_storage_options()</code>","text":"<p>Get storage options for pandas/fsspec.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with appropriate authentication parameters for fsspec</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def pandas_storage_options(self) -&gt; Dict[str, Any]:\n    \"\"\"Get storage options for pandas/fsspec.\n\n    Returns:\n        Dictionary with appropriate authentication parameters for fsspec\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Building pandas storage options\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    base_options = {\"account_name\": self.account}\n\n    if self.auth_mode in [\"key_vault\", \"direct_key\"]:\n        return {**base_options, \"account_key\": self.get_storage_key()}\n\n    elif self.auth_mode == \"sas_token\":\n        # Use get_storage_key() which handles KV fallback for SAS\n        return {**base_options, \"sas_token\": self.get_storage_key()}\n\n    elif self.auth_mode == \"service_principal\":\n        return {\n            **base_options,\n            \"tenant_id\": self.tenant_id,\n            \"client_id\": self.client_id,\n            \"client_secret\": self.get_client_secret(),\n        }\n\n    elif self.auth_mode == \"managed_identity\":\n        # adlfs supports using DefaultAzureCredential implicitly if anon=False\n        # and no other creds provided, assuming azure.identity is installed\n        return {**base_options, \"anon\": False}\n\n    return base_options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.uri","title":"<code>uri(path)</code>","text":"<p>Build abfss:// URI for given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path within container</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full abfss:// URI</p> Example <p>conn = AzureADLS( ...     account=\"myaccount\", container=\"data\", ...     auth_mode=\"direct_key\", account_key=\"key123\" ... ) conn.uri(\"folder/file.csv\") 'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def uri(self, path: str) -&gt; str:\n    \"\"\"Build abfss:// URI for given path.\n\n    Args:\n        path: Relative path within container\n\n    Returns:\n        Full abfss:// URI\n\n    Example:\n        &gt;&gt;&gt; conn = AzureADLS(\n        ...     account=\"myaccount\", container=\"data\",\n        ...     auth_mode=\"direct_key\", account_key=\"key123\"\n        ... )\n        &gt;&gt;&gt; conn.uri(\"folder/file.csv\")\n        'abfss://data@myaccount.dfs.core.windows.net/folder/file.csv'\n    \"\"\"\n    if self.path_prefix:\n        full_path = posixpath.join(self.path_prefix, path.lstrip(\"/\"))\n    else:\n        full_path = path.lstrip(\"/\")\n\n    return f\"abfss://{self.container}@{self.account}.dfs.core.windows.net/{full_path}\"\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_adls.AzureADLS.validate","title":"<code>validate()</code>","text":"<p>Validate ADLS connection configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing for the selected auth_mode</p> Source code in <code>odibi\\connections\\azure_adls.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate ADLS connection configuration.\n\n    Raises:\n        ValueError: If required fields are missing for the selected auth_mode\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating AzureADLS connection\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n\n    if not self.account:\n        ctx.error(\"ADLS connection validation failed: missing 'account'\")\n        raise ValueError(\n            \"ADLS connection requires 'account'. \"\n            \"Provide the storage account name (e.g., account: 'mystorageaccount').\"\n        )\n    if not self.container:\n        ctx.error(\n            \"ADLS connection validation failed: missing 'container'\",\n            account=self.account,\n        )\n        raise ValueError(\n            f\"ADLS connection requires 'container' for account '{self.account}'. \"\n            \"Provide the container/filesystem name.\"\n        )\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"ADLS key_vault mode validation failed\",\n                account=self.account,\n                container=self.container,\n                key_vault_name=self.key_vault_name or \"(missing)\",\n                secret_name=self.secret_name or \"(missing)\",\n            )\n            raise ValueError(\n                f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"direct_key\":\n        if not self.account_key:\n            ctx.error(\n                \"ADLS direct_key mode validation failed: missing account_key\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"direct_key mode requires 'account_key' \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n\n        # Warn in production\n        if os.getenv(\"ODIBI_ENV\") == \"production\":\n            ctx.warning(\n                \"Using direct_key in production is not recommended\",\n                account=self.account,\n                container=self.container,\n            )\n            warnings.warn(\n                f\"\u26a0\ufe0f  Using direct_key in production is not recommended. \"\n                f\"Use auth_mode: key_vault. Connection: {self.account}/{self.container}\",\n                UserWarning,\n            )\n    elif self.auth_mode == \"sas_token\":\n        if not self.sas_token and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"ADLS sas_token mode validation failed\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"sas_token mode requires 'sas_token' (or key_vault_name/secret_name) \"\n                f\"for connection to {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"service_principal\":\n        if not self.tenant_id or not self.client_id:\n            ctx.error(\n                \"ADLS service_principal mode validation failed\",\n                account=self.account,\n                container=self.container,\n                missing=\"tenant_id and/or client_id\",\n            )\n            raise ValueError(\n                f\"service_principal mode requires 'tenant_id' and 'client_id' \"\n                f\"for connection to {self.account}/{self.container}. \"\n                f\"Got tenant_id={self.tenant_id or '(missing)'}, \"\n                f\"client_id={self.client_id or '(missing)'}.\"\n            )\n\n        if not self.client_secret and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"ADLS service_principal mode validation failed: missing client_secret\",\n                account=self.account,\n                container=self.container,\n            )\n            raise ValueError(\n                f\"service_principal mode requires 'client_secret' \"\n                f\"(or key_vault_name/secret_name) for {self.account}/{self.container}\"\n            )\n    elif self.auth_mode == \"managed_identity\":\n        # No specific config required, but we might check if environment supports it\n        ctx.debug(\n            \"Using managed_identity auth mode\",\n            account=self.account,\n            container=self.container,\n        )\n    else:\n        ctx.error(\n            \"ADLS validation failed: unsupported auth_mode\",\n            account=self.account,\n            container=self.container,\n            auth_mode=self.auth_mode,\n        )\n        raise ValueError(\n            f\"Unsupported auth_mode: '{self.auth_mode}'. \"\n            f\"Use 'key_vault', 'direct_key', 'service_principal', or 'managed_identity'.\"\n        )\n\n    ctx.info(\n        \"AzureADLS connection validated successfully\",\n        account=self.account,\n        container=self.container,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql","title":"<code>odibi.connections.azure_sql</code>","text":""},{"location":"reference/api/connections/#odibi.connections.azure_sql--azure-sql-database-connection","title":"Azure SQL Database Connection","text":"<p>Provides connectivity to Azure SQL databases with authentication support.</p>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL","title":"<code>AzureSQL</code>","text":"<p>               Bases: <code>BaseConnection</code></p> <p>Azure SQL Database connection.</p> <p>Supports: - SQL authentication (username/password) - Azure Active Directory Managed Identity - Connection pooling - Read/write operations via SQLAlchemy</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>class AzureSQL(BaseConnection):\n    \"\"\"\n    Azure SQL Database connection.\n\n    Supports:\n    - SQL authentication (username/password)\n    - Azure Active Directory Managed Identity\n    - Connection pooling\n    - Read/write operations via SQLAlchemy\n    \"\"\"\n\n    def __init__(\n        self,\n        server: str,\n        database: str,\n        driver: str = \"ODBC Driver 18 for SQL Server\",\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        auth_mode: str = \"aad_msi\",  # \"aad_msi\", \"sql\", \"key_vault\"\n        key_vault_name: Optional[str] = None,\n        secret_name: Optional[str] = None,\n        port: int = 1433,\n        timeout: int = 30,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize Azure SQL connection.\n\n        Args:\n            server: SQL server hostname (e.g., 'myserver.database.windows.net')\n            database: Database name\n            driver: ODBC driver name (default: ODBC Driver 18 for SQL Server)\n            username: SQL auth username (required if auth_mode='sql')\n            password: SQL auth password (required if auth_mode='sql')\n            auth_mode: Authentication mode ('aad_msi', 'sql', 'key_vault')\n            key_vault_name: Key Vault name (required if auth_mode='key_vault')\n            secret_name: Secret name containing password (required if auth_mode='key_vault')\n            port: SQL Server port (default: 1433)\n            timeout: Connection timeout in seconds (default: 30)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.log_connection(\n            connection_type=\"azure_sql\",\n            connection_name=f\"{server}/{database}\",\n            action=\"init\",\n            server=server,\n            database=database,\n            auth_mode=auth_mode,\n            port=port,\n        )\n\n        self.server = server\n        self.database = database\n        self.driver = driver\n        self.username = username\n        self.password = password\n        self.auth_mode = auth_mode\n        self.key_vault_name = key_vault_name\n        self.secret_name = secret_name\n        self.port = port\n        self.timeout = timeout\n        self._engine = None\n        self._cached_key = None  # For consistency with ADLS / parallel fetch\n\n        ctx.debug(\n            \"AzureSQL connection initialized\",\n            server=server,\n            database=database,\n            auth_mode=auth_mode,\n            driver=driver,\n        )\n\n    def get_password(self) -&gt; Optional[str]:\n        \"\"\"Get password (cached).\"\"\"\n        ctx = get_logging_context()\n\n        if self.password:\n            ctx.debug(\n                \"Using provided password\",\n                server=self.server,\n                database=self.database,\n            )\n            return self.password\n\n        if self._cached_key:\n            ctx.debug(\n                \"Using cached password\",\n                server=self.server,\n                database=self.database,\n            )\n            return self._cached_key\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"Key Vault mode requires key_vault_name and secret_name\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                    f\"for connection to {self.server}/{self.database}. \"\n                    f\"Got key_vault_name={self.key_vault_name or '(missing)'}, \"\n                    f\"secret_name={self.secret_name or '(missing)'}.\"\n                )\n\n            ctx.debug(\n                \"Fetching password from Key Vault\",\n                server=self.server,\n                key_vault_name=self.key_vault_name,\n                secret_name=self.secret_name,\n            )\n\n            try:\n                from azure.identity import DefaultAzureCredential\n                from azure.keyvault.secrets import SecretClient\n\n                credential = DefaultAzureCredential()\n                kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n                client = SecretClient(vault_url=kv_uri, credential=credential)\n                secret = client.get_secret(self.secret_name)\n                self._cached_key = secret.value\n                logger.register_secret(self._cached_key)\n\n                ctx.info(\n                    \"Successfully fetched password from Key Vault\",\n                    server=self.server,\n                    key_vault_name=self.key_vault_name,\n                )\n                return self._cached_key\n            except ImportError as e:\n                ctx.error(\n                    \"Key Vault support requires azure libraries\",\n                    server=self.server,\n                    error=str(e),\n                )\n                raise ImportError(\n                    \"Key Vault support requires 'azure-identity' and 'azure-keyvault-secrets'. \"\n                    \"Install with: pip install odibi[azure]\"\n                )\n\n        ctx.debug(\n            \"No password required for auth_mode\",\n            server=self.server,\n            auth_mode=self.auth_mode,\n        )\n        return None\n\n    def odbc_dsn(self) -&gt; str:\n        \"\"\"Build ODBC connection string.\n\n        Returns:\n            ODBC DSN string\n\n        Example:\n            &gt;&gt;&gt; conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\")\n            &gt;&gt;&gt; conn.odbc_dsn()\n            'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Building ODBC connection string\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        dsn = (\n            f\"Driver={{{self.driver}}};\"\n            f\"Server=tcp:{self.server},{self.port};\"\n            f\"Database={self.database};\"\n            f\"Encrypt=yes;\"\n            f\"TrustServerCertificate=yes;\"\n            f\"Connection Timeout={self.timeout};\"\n        )\n\n        pwd = self.get_password()\n        if self.username and pwd:\n            dsn += f\"UID={self.username};PWD={pwd};\"\n            ctx.debug(\n                \"Using SQL authentication\",\n                server=self.server,\n                username=self.username,\n            )\n        elif self.auth_mode == \"aad_msi\":\n            dsn += \"Authentication=ActiveDirectoryMsi;\"\n            ctx.debug(\n                \"Using AAD Managed Identity authentication\",\n                server=self.server,\n            )\n        elif self.auth_mode == \"aad_service_principal\":\n            # Not fully supported via ODBC string simply without token usually\n            ctx.debug(\n                \"Using AAD Service Principal authentication\",\n                server=self.server,\n            )\n\n        return dsn\n\n    def get_path(self, relative_path: str) -&gt; str:\n        \"\"\"Get table reference for relative path.\"\"\"\n        return relative_path\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate Azure SQL connection configuration.\"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Validating AzureSQL connection\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        if not self.server:\n            ctx.error(\"AzureSQL validation failed: missing 'server'\")\n            raise ValueError(\n                \"Azure SQL connection requires 'server'. \"\n                \"Provide the SQL server hostname (e.g., server: 'myserver.database.windows.net').\"\n            )\n        if not self.database:\n            ctx.error(\n                \"AzureSQL validation failed: missing 'database'\",\n                server=self.server,\n            )\n            raise ValueError(\n                f\"Azure SQL connection requires 'database' for server '{self.server}'.\"\n            )\n\n        if self.auth_mode == \"sql\":\n            if not self.username:\n                ctx.error(\n                    \"AzureSQL validation failed: SQL auth requires username\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    f\"Azure SQL with auth_mode='sql' requires 'username' \"\n                    f\"for connection to {self.server}/{self.database}.\"\n                )\n            if not self.password and not (self.key_vault_name and self.secret_name):\n                ctx.error(\n                    \"AzureSQL validation failed: SQL auth requires password\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    \"Azure SQL with auth_mode='sql' requires password \"\n                    \"(or key_vault_name/secret_name)\"\n                )\n\n        if self.auth_mode == \"key_vault\":\n            if not self.key_vault_name or not self.secret_name:\n                ctx.error(\n                    \"AzureSQL validation failed: key_vault mode missing config\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\n                    \"Azure SQL with auth_mode='key_vault' requires key_vault_name and secret_name\"\n                )\n            if not self.username:\n                ctx.error(\n                    \"AzureSQL validation failed: key_vault mode requires username\",\n                    server=self.server,\n                    database=self.database,\n                )\n                raise ValueError(\"Azure SQL with auth_mode='key_vault' requires username\")\n\n        ctx.info(\n            \"AzureSQL connection validated successfully\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n    def get_engine(self) -&gt; Any:\n        \"\"\"\n        Get or create SQLAlchemy engine.\n\n        Returns:\n            SQLAlchemy engine instance\n\n        Raises:\n            ConnectionError: If connection fails or drivers missing\n        \"\"\"\n        ctx = get_logging_context()\n\n        if self._engine is not None:\n            ctx.debug(\n                \"Using cached SQLAlchemy engine\",\n                server=self.server,\n                database=self.database,\n            )\n            return self._engine\n\n        ctx.debug(\n            \"Creating SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n        )\n\n        try:\n            from urllib.parse import quote_plus\n\n            from sqlalchemy import create_engine\n        except ImportError as e:\n            ctx.error(\n                \"SQLAlchemy import failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=\"Required packages 'sqlalchemy' or 'pyodbc' not found.\",\n                suggestions=[\n                    \"Install required packages: pip install sqlalchemy pyodbc\",\n                    \"Or install odibi with azure extras: pip install 'odibi[azure]'\",\n                ],\n            )\n\n        try:\n            # Build connection string\n            conn_str = self.odbc_dsn()\n            connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}\"\n\n            ctx.debug(\n                \"Creating SQLAlchemy engine with connection pooling\",\n                server=self.server,\n                database=self.database,\n            )\n\n            # Create engine with connection pooling\n            self._engine = create_engine(\n                connection_url,\n                pool_pre_ping=True,  # Verify connections before use\n                pool_recycle=3600,  # Recycle connections after 1 hour\n                echo=False,\n            )\n\n            # Test connection\n            with self._engine.connect():\n                pass\n\n            ctx.info(\n                \"SQLAlchemy engine created successfully\",\n                server=self.server,\n                database=self.database,\n            )\n\n            return self._engine\n\n        except Exception as e:\n            suggestions = self._get_error_suggestions(str(e))\n            ctx.error(\n                \"Failed to create SQLAlchemy engine\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n                suggestions=suggestions,\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Failed to create engine: {str(e)}\",\n                suggestions=suggestions,\n            )\n\n    def read_sql(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Execute SQL query and return results as DataFrame.\n\n        Args:\n            query: SQL query string\n            params: Optional query parameters for parameterized queries\n\n        Returns:\n            Query results as pandas DataFrame\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Executing SQL query\",\n            server=self.server,\n            database=self.database,\n            query_length=len(query),\n        )\n\n        try:\n            engine = self.get_engine()\n            # Use SQLAlchemy connection directly (preferred by pandas)\n            with engine.connect() as conn:\n                result = pd.read_sql(query, conn, params=params)\n\n            ctx.info(\n                \"SQL query executed successfully\",\n                server=self.server,\n                database=self.database,\n                rows_returned=len(result),\n            )\n            return result\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"SQL query execution failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Query execution failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def read_table(self, table_name: str, schema: Optional[str] = \"dbo\") -&gt; pd.DataFrame:\n        \"\"\"\n        Read entire table into DataFrame.\n\n        Args:\n            table_name: Name of the table\n            schema: Schema name (default: dbo)\n\n        Returns:\n            Table contents as pandas DataFrame\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Reading table\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            schema=schema,\n        )\n\n        if schema:\n            query = f\"SELECT * FROM [{schema}].[{table_name}]\"\n        else:\n            query = f\"SELECT * FROM [{table_name}]\"\n\n        return self.read_sql(query)\n\n    def read_sql_query(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Execute a SQL query and return results as DataFrame.\n\n        Use this for custom SELECT queries (e.g., to exclude unsupported columns).\n\n        Args:\n            query: SQL SELECT query\n            params: Optional parameters for parameterized query\n\n        Returns:\n            Query results as pandas DataFrame\n        \"\"\"\n        return self.read_sql(query, params)\n\n    def write_table(\n        self,\n        df: pd.DataFrame,\n        table_name: str,\n        schema: Optional[str] = \"dbo\",\n        if_exists: str = \"replace\",\n        index: bool = False,\n        chunksize: Optional[int] = 1000,\n    ) -&gt; int:\n        \"\"\"\n        Write DataFrame to SQL table.\n\n        Args:\n            df: DataFrame to write\n            table_name: Name of the table\n            schema: Schema name (default: dbo)\n            if_exists: How to behave if table exists ('fail', 'replace', 'append')\n            index: Whether to write DataFrame index as column\n            chunksize: Number of rows to write in each batch (default: 1000)\n\n        Returns:\n            Number of rows written\n\n        Raises:\n            ConnectionError: If write fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Writing DataFrame to table\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            schema=schema,\n            rows=len(df),\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n\n        try:\n            engine = self.get_engine()\n\n            rows_written = df.to_sql(\n                name=table_name,\n                con=engine,\n                schema=schema,\n                if_exists=if_exists,\n                index=index,\n                chunksize=chunksize,\n                method=\"multi\",  # Use multi-row INSERT for better performance\n            )\n\n            result_rows = rows_written if rows_written is not None else len(df)\n            ctx.info(\n                \"Table write completed successfully\",\n                server=self.server,\n                database=self.database,\n                table_name=table_name,\n                rows_written=result_rows,\n            )\n            return result_rows\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"Table write failed\",\n                server=self.server,\n                database=self.database,\n                table_name=table_name,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Write operation failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def execute_sql(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n        \"\"\"\n        Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n        Alias for execute() - used by SqlServerMergeWriter.\n\n        Args:\n            sql: SQL statement\n            params: Optional parameters for parameterized query\n\n        Returns:\n            Result from execution\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        return self.execute(sql, params)\n\n    def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n        \"\"\"\n        Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n        Args:\n            sql: SQL statement\n            params: Optional parameters for parameterized query\n\n        Returns:\n            Result from execution\n\n        Raises:\n            ConnectionError: If execution fails\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Executing SQL statement\",\n            server=self.server,\n            database=self.database,\n            statement_length=len(sql),\n        )\n\n        try:\n            engine = self.get_engine()\n            from sqlalchemy import text\n\n            # Use begin() for proper transaction handling in SQLAlchemy 1.4+\n            with engine.begin() as conn:\n                result = conn.execute(text(sql), params or {})\n                # Fetch all results before transaction ends\n                if result.returns_rows:\n                    rows = result.fetchall()\n                else:\n                    rows = None\n                # Transaction auto-commits on exit from begin() context\n\n                ctx.info(\n                    \"SQL statement executed successfully\",\n                    server=self.server,\n                    database=self.database,\n                )\n                return rows\n        except Exception as e:\n            if isinstance(e, ConnectionError):\n                raise\n            ctx.error(\n                \"SQL statement execution failed\",\n                server=self.server,\n                database=self.database,\n                error=str(e),\n            )\n            raise ConnectionError(\n                connection_name=f\"AzureSQL({self.server})\",\n                reason=f\"Statement execution failed: {str(e)}\",\n                suggestions=self._get_error_suggestions(str(e)),\n            )\n\n    def close(self):\n        \"\"\"Close database connection and dispose of engine.\"\"\"\n        ctx = get_logging_context()\n        ctx.debug(\n            \"Closing AzureSQL connection\",\n            server=self.server,\n            database=self.database,\n        )\n\n        if self._engine:\n            self._engine.dispose()\n            self._engine = None\n            ctx.info(\n                \"AzureSQL connection closed\",\n                server=self.server,\n                database=self.database,\n            )\n\n    def _get_error_suggestions(self, error_msg: str) -&gt; List[str]:\n        \"\"\"Generate suggestions using centralized error suggestion engine.\"\"\"\n        try:\n            error = Exception(error_msg)\n            return get_suggestions_for_connection(\n                error=error,\n                connection_name=self.name if hasattr(self, \"name\") else \"azure_sql\",\n                connection_type=\"azure_sql\",\n                auth_mode=self.auth_mode,\n            )\n        except Exception:\n            return []\n\n    def get_spark_options(self) -&gt; Dict[str, str]:\n        \"\"\"Get Spark JDBC options.\n\n        Returns:\n            Dictionary of Spark JDBC options (url, user, password, etc.)\n        \"\"\"\n        ctx = get_logging_context()\n        ctx.info(\n            \"Building Spark JDBC options\",\n            server=self.server,\n            database=self.database,\n            auth_mode=self.auth_mode,\n        )\n\n        jdbc_url = (\n            f\"jdbc:sqlserver://{self.server}:{self.port};\"\n            f\"databaseName={self.database};encrypt=true;trustServerCertificate=true;\"\n        )\n\n        if self.auth_mode == \"aad_msi\":\n            jdbc_url += (\n                \"hostNameInCertificate=*.database.windows.net;\"\n                \"loginTimeout=30;authentication=ActiveDirectoryMsi;\"\n            )\n            ctx.debug(\n                \"Configured JDBC URL for AAD MSI\",\n                server=self.server,\n            )\n        elif self.auth_mode == \"aad_service_principal\":\n            # Not fully implemented in init yet, but placeholder\n            ctx.debug(\n                \"Configured JDBC URL for AAD Service Principal\",\n                server=self.server,\n            )\n\n        options = {\n            \"url\": jdbc_url,\n            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n        }\n\n        if self.auth_mode == \"sql\" or self.auth_mode == \"key_vault\":\n            if self.username:\n                options[\"user\"] = self.username\n\n            pwd = self.get_password()\n            if pwd:\n                options[\"password\"] = pwd\n\n            ctx.debug(\n                \"Added SQL authentication to Spark options\",\n                server=self.server,\n                username=self.username,\n            )\n\n        ctx.info(\n            \"Spark JDBC options built successfully\",\n            server=self.server,\n            database=self.database,\n        )\n\n        return options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.__init__","title":"<code>__init__(server, database, driver='ODBC Driver 18 for SQL Server', username=None, password=None, auth_mode='aad_msi', key_vault_name=None, secret_name=None, port=1433, timeout=30, **kwargs)</code>","text":"<p>Initialize Azure SQL connection.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>SQL server hostname (e.g., 'myserver.database.windows.net')</p> required <code>database</code> <code>str</code> <p>Database name</p> required <code>driver</code> <code>str</code> <p>ODBC driver name (default: ODBC Driver 18 for SQL Server)</p> <code>'ODBC Driver 18 for SQL Server'</code> <code>username</code> <code>Optional[str]</code> <p>SQL auth username (required if auth_mode='sql')</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>SQL auth password (required if auth_mode='sql')</p> <code>None</code> <code>auth_mode</code> <code>str</code> <p>Authentication mode ('aad_msi', 'sql', 'key_vault')</p> <code>'aad_msi'</code> <code>key_vault_name</code> <code>Optional[str]</code> <p>Key Vault name (required if auth_mode='key_vault')</p> <code>None</code> <code>secret_name</code> <code>Optional[str]</code> <p>Secret name containing password (required if auth_mode='key_vault')</p> <code>None</code> <code>port</code> <code>int</code> <p>SQL Server port (default: 1433)</p> <code>1433</code> <code>timeout</code> <code>int</code> <p>Connection timeout in seconds (default: 30)</p> <code>30</code> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def __init__(\n    self,\n    server: str,\n    database: str,\n    driver: str = \"ODBC Driver 18 for SQL Server\",\n    username: Optional[str] = None,\n    password: Optional[str] = None,\n    auth_mode: str = \"aad_msi\",  # \"aad_msi\", \"sql\", \"key_vault\"\n    key_vault_name: Optional[str] = None,\n    secret_name: Optional[str] = None,\n    port: int = 1433,\n    timeout: int = 30,\n    **kwargs,\n):\n    \"\"\"\n    Initialize Azure SQL connection.\n\n    Args:\n        server: SQL server hostname (e.g., 'myserver.database.windows.net')\n        database: Database name\n        driver: ODBC driver name (default: ODBC Driver 18 for SQL Server)\n        username: SQL auth username (required if auth_mode='sql')\n        password: SQL auth password (required if auth_mode='sql')\n        auth_mode: Authentication mode ('aad_msi', 'sql', 'key_vault')\n        key_vault_name: Key Vault name (required if auth_mode='key_vault')\n        secret_name: Secret name containing password (required if auth_mode='key_vault')\n        port: SQL Server port (default: 1433)\n        timeout: Connection timeout in seconds (default: 30)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.log_connection(\n        connection_type=\"azure_sql\",\n        connection_name=f\"{server}/{database}\",\n        action=\"init\",\n        server=server,\n        database=database,\n        auth_mode=auth_mode,\n        port=port,\n    )\n\n    self.server = server\n    self.database = database\n    self.driver = driver\n    self.username = username\n    self.password = password\n    self.auth_mode = auth_mode\n    self.key_vault_name = key_vault_name\n    self.secret_name = secret_name\n    self.port = port\n    self.timeout = timeout\n    self._engine = None\n    self._cached_key = None  # For consistency with ADLS / parallel fetch\n\n    ctx.debug(\n        \"AzureSQL connection initialized\",\n        server=server,\n        database=database,\n        auth_mode=auth_mode,\n        driver=driver,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.close","title":"<code>close()</code>","text":"<p>Close database connection and dispose of engine.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def close(self):\n    \"\"\"Close database connection and dispose of engine.\"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Closing AzureSQL connection\",\n        server=self.server,\n        database=self.database,\n    )\n\n    if self._engine:\n        self._engine.dispose()\n        self._engine = None\n        ctx.info(\n            \"AzureSQL connection closed\",\n            server=self.server,\n            database=self.database,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.execute","title":"<code>execute(sql, params=None)</code>","text":"<p>Execute SQL statement (INSERT, UPDATE, DELETE, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL statement</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for parameterized query</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result from execution</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def execute(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n    Args:\n        sql: SQL statement\n        params: Optional parameters for parameterized query\n\n    Returns:\n        Result from execution\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Executing SQL statement\",\n        server=self.server,\n        database=self.database,\n        statement_length=len(sql),\n    )\n\n    try:\n        engine = self.get_engine()\n        from sqlalchemy import text\n\n        # Use begin() for proper transaction handling in SQLAlchemy 1.4+\n        with engine.begin() as conn:\n            result = conn.execute(text(sql), params or {})\n            # Fetch all results before transaction ends\n            if result.returns_rows:\n                rows = result.fetchall()\n            else:\n                rows = None\n            # Transaction auto-commits on exit from begin() context\n\n            ctx.info(\n                \"SQL statement executed successfully\",\n                server=self.server,\n                database=self.database,\n            )\n            return rows\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"SQL statement execution failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Statement execution failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.execute_sql","title":"<code>execute_sql(sql, params=None)</code>","text":"<p>Execute SQL statement (INSERT, UPDATE, DELETE, etc.).</p> <p>Alias for execute() - used by SqlServerMergeWriter.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL statement</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for parameterized query</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result from execution</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def execute_sql(self, sql: str, params: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Execute SQL statement (INSERT, UPDATE, DELETE, etc.).\n\n    Alias for execute() - used by SqlServerMergeWriter.\n\n    Args:\n        sql: SQL statement\n        params: Optional parameters for parameterized query\n\n    Returns:\n        Result from execution\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    return self.execute(sql, params)\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_engine","title":"<code>get_engine()</code>","text":"<p>Get or create SQLAlchemy engine.</p> <p>Returns:</p> Type Description <code>Any</code> <p>SQLAlchemy engine instance</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection fails or drivers missing</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_engine(self) -&gt; Any:\n    \"\"\"\n    Get or create SQLAlchemy engine.\n\n    Returns:\n        SQLAlchemy engine instance\n\n    Raises:\n        ConnectionError: If connection fails or drivers missing\n    \"\"\"\n    ctx = get_logging_context()\n\n    if self._engine is not None:\n        ctx.debug(\n            \"Using cached SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n        )\n        return self._engine\n\n    ctx.debug(\n        \"Creating SQLAlchemy engine\",\n        server=self.server,\n        database=self.database,\n    )\n\n    try:\n        from urllib.parse import quote_plus\n\n        from sqlalchemy import create_engine\n    except ImportError as e:\n        ctx.error(\n            \"SQLAlchemy import failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=\"Required packages 'sqlalchemy' or 'pyodbc' not found.\",\n            suggestions=[\n                \"Install required packages: pip install sqlalchemy pyodbc\",\n                \"Or install odibi with azure extras: pip install 'odibi[azure]'\",\n            ],\n        )\n\n    try:\n        # Build connection string\n        conn_str = self.odbc_dsn()\n        connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(conn_str)}\"\n\n        ctx.debug(\n            \"Creating SQLAlchemy engine with connection pooling\",\n            server=self.server,\n            database=self.database,\n        )\n\n        # Create engine with connection pooling\n        self._engine = create_engine(\n            connection_url,\n            pool_pre_ping=True,  # Verify connections before use\n            pool_recycle=3600,  # Recycle connections after 1 hour\n            echo=False,\n        )\n\n        # Test connection\n        with self._engine.connect():\n            pass\n\n        ctx.info(\n            \"SQLAlchemy engine created successfully\",\n            server=self.server,\n            database=self.database,\n        )\n\n        return self._engine\n\n    except Exception as e:\n        suggestions = self._get_error_suggestions(str(e))\n        ctx.error(\n            \"Failed to create SQLAlchemy engine\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n            suggestions=suggestions,\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Failed to create engine: {str(e)}\",\n            suggestions=suggestions,\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_password","title":"<code>get_password()</code>","text":"<p>Get password (cached).</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_password(self) -&gt; Optional[str]:\n    \"\"\"Get password (cached).\"\"\"\n    ctx = get_logging_context()\n\n    if self.password:\n        ctx.debug(\n            \"Using provided password\",\n            server=self.server,\n            database=self.database,\n        )\n        return self.password\n\n    if self._cached_key:\n        ctx.debug(\n            \"Using cached password\",\n            server=self.server,\n            database=self.database,\n        )\n        return self._cached_key\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"Key Vault mode requires key_vault_name and secret_name\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                f\"key_vault mode requires 'key_vault_name' and 'secret_name' \"\n                f\"for connection to {self.server}/{self.database}. \"\n                f\"Got key_vault_name={self.key_vault_name or '(missing)'}, \"\n                f\"secret_name={self.secret_name or '(missing)'}.\"\n            )\n\n        ctx.debug(\n            \"Fetching password from Key Vault\",\n            server=self.server,\n            key_vault_name=self.key_vault_name,\n            secret_name=self.secret_name,\n        )\n\n        try:\n            from azure.identity import DefaultAzureCredential\n            from azure.keyvault.secrets import SecretClient\n\n            credential = DefaultAzureCredential()\n            kv_uri = f\"https://{self.key_vault_name}.vault.azure.net\"\n            client = SecretClient(vault_url=kv_uri, credential=credential)\n            secret = client.get_secret(self.secret_name)\n            self._cached_key = secret.value\n            logger.register_secret(self._cached_key)\n\n            ctx.info(\n                \"Successfully fetched password from Key Vault\",\n                server=self.server,\n                key_vault_name=self.key_vault_name,\n            )\n            return self._cached_key\n        except ImportError as e:\n            ctx.error(\n                \"Key Vault support requires azure libraries\",\n                server=self.server,\n                error=str(e),\n            )\n            raise ImportError(\n                \"Key Vault support requires 'azure-identity' and 'azure-keyvault-secrets'. \"\n                \"Install with: pip install odibi[azure]\"\n            )\n\n    ctx.debug(\n        \"No password required for auth_mode\",\n        server=self.server,\n        auth_mode=self.auth_mode,\n    )\n    return None\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_path","title":"<code>get_path(relative_path)</code>","text":"<p>Get table reference for relative path.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_path(self, relative_path: str) -&gt; str:\n    \"\"\"Get table reference for relative path.\"\"\"\n    return relative_path\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.get_spark_options","title":"<code>get_spark_options()</code>","text":"<p>Get Spark JDBC options.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary of Spark JDBC options (url, user, password, etc.)</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def get_spark_options(self) -&gt; Dict[str, str]:\n    \"\"\"Get Spark JDBC options.\n\n    Returns:\n        Dictionary of Spark JDBC options (url, user, password, etc.)\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Building Spark JDBC options\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    jdbc_url = (\n        f\"jdbc:sqlserver://{self.server}:{self.port};\"\n        f\"databaseName={self.database};encrypt=true;trustServerCertificate=true;\"\n    )\n\n    if self.auth_mode == \"aad_msi\":\n        jdbc_url += (\n            \"hostNameInCertificate=*.database.windows.net;\"\n            \"loginTimeout=30;authentication=ActiveDirectoryMsi;\"\n        )\n        ctx.debug(\n            \"Configured JDBC URL for AAD MSI\",\n            server=self.server,\n        )\n    elif self.auth_mode == \"aad_service_principal\":\n        # Not fully implemented in init yet, but placeholder\n        ctx.debug(\n            \"Configured JDBC URL for AAD Service Principal\",\n            server=self.server,\n        )\n\n    options = {\n        \"url\": jdbc_url,\n        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n    }\n\n    if self.auth_mode == \"sql\" or self.auth_mode == \"key_vault\":\n        if self.username:\n            options[\"user\"] = self.username\n\n        pwd = self.get_password()\n        if pwd:\n            options[\"password\"] = pwd\n\n        ctx.debug(\n            \"Added SQL authentication to Spark options\",\n            server=self.server,\n            username=self.username,\n        )\n\n    ctx.info(\n        \"Spark JDBC options built successfully\",\n        server=self.server,\n        database=self.database,\n    )\n\n    return options\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.odbc_dsn","title":"<code>odbc_dsn()</code>","text":"<p>Build ODBC connection string.</p> <p>Returns:</p> Type Description <code>str</code> <p>ODBC DSN string</p> Example <p>conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\") conn.odbc_dsn() 'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def odbc_dsn(self) -&gt; str:\n    \"\"\"Build ODBC connection string.\n\n    Returns:\n        ODBC DSN string\n\n    Example:\n        &gt;&gt;&gt; conn = AzureSQL(server=\"myserver.database.windows.net\", database=\"mydb\")\n        &gt;&gt;&gt; conn.odbc_dsn()\n        'Driver={ODBC Driver 18 for SQL Server};Server=tcp:myserver...'\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Building ODBC connection string\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    dsn = (\n        f\"Driver={{{self.driver}}};\"\n        f\"Server=tcp:{self.server},{self.port};\"\n        f\"Database={self.database};\"\n        f\"Encrypt=yes;\"\n        f\"TrustServerCertificate=yes;\"\n        f\"Connection Timeout={self.timeout};\"\n    )\n\n    pwd = self.get_password()\n    if self.username and pwd:\n        dsn += f\"UID={self.username};PWD={pwd};\"\n        ctx.debug(\n            \"Using SQL authentication\",\n            server=self.server,\n            username=self.username,\n        )\n    elif self.auth_mode == \"aad_msi\":\n        dsn += \"Authentication=ActiveDirectoryMsi;\"\n        ctx.debug(\n            \"Using AAD Managed Identity authentication\",\n            server=self.server,\n        )\n    elif self.auth_mode == \"aad_service_principal\":\n        # Not fully supported via ODBC string simply without token usually\n        ctx.debug(\n            \"Using AAD Service Principal authentication\",\n            server=self.server,\n        )\n\n    return dsn\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_sql","title":"<code>read_sql(query, params=None)</code>","text":"<p>Execute SQL query and return results as DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional query parameters for parameterized queries</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Query results as pandas DataFrame</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If execution fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_sql(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute SQL query and return results as DataFrame.\n\n    Args:\n        query: SQL query string\n        params: Optional query parameters for parameterized queries\n\n    Returns:\n        Query results as pandas DataFrame\n\n    Raises:\n        ConnectionError: If execution fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Executing SQL query\",\n        server=self.server,\n        database=self.database,\n        query_length=len(query),\n    )\n\n    try:\n        engine = self.get_engine()\n        # Use SQLAlchemy connection directly (preferred by pandas)\n        with engine.connect() as conn:\n            result = pd.read_sql(query, conn, params=params)\n\n        ctx.info(\n            \"SQL query executed successfully\",\n            server=self.server,\n            database=self.database,\n            rows_returned=len(result),\n        )\n        return result\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"SQL query execution failed\",\n            server=self.server,\n            database=self.database,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Query execution failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_sql_query","title":"<code>read_sql_query(query, params=None)</code>","text":"<p>Execute a SQL query and return results as DataFrame.</p> <p>Use this for custom SELECT queries (e.g., to exclude unsupported columns).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL SELECT query</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for parameterized query</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Query results as pandas DataFrame</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_sql_query(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute a SQL query and return results as DataFrame.\n\n    Use this for custom SELECT queries (e.g., to exclude unsupported columns).\n\n    Args:\n        query: SQL SELECT query\n        params: Optional parameters for parameterized query\n\n    Returns:\n        Query results as pandas DataFrame\n    \"\"\"\n    return self.read_sql(query, params)\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.read_table","title":"<code>read_table(table_name, schema='dbo')</code>","text":"<p>Read entire table into DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>schema</code> <code>Optional[str]</code> <p>Schema name (default: dbo)</p> <code>'dbo'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table contents as pandas DataFrame</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def read_table(self, table_name: str, schema: Optional[str] = \"dbo\") -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire table into DataFrame.\n\n    Args:\n        table_name: Name of the table\n        schema: Schema name (default: dbo)\n\n    Returns:\n        Table contents as pandas DataFrame\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Reading table\",\n        server=self.server,\n        database=self.database,\n        table_name=table_name,\n        schema=schema,\n    )\n\n    if schema:\n        query = f\"SELECT * FROM [{schema}].[{table_name}]\"\n    else:\n        query = f\"SELECT * FROM [{table_name}]\"\n\n    return self.read_sql(query)\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.validate","title":"<code>validate()</code>","text":"<p>Validate Azure SQL connection configuration.</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate Azure SQL connection configuration.\"\"\"\n    ctx = get_logging_context()\n    ctx.debug(\n        \"Validating AzureSQL connection\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n\n    if not self.server:\n        ctx.error(\"AzureSQL validation failed: missing 'server'\")\n        raise ValueError(\n            \"Azure SQL connection requires 'server'. \"\n            \"Provide the SQL server hostname (e.g., server: 'myserver.database.windows.net').\"\n        )\n    if not self.database:\n        ctx.error(\n            \"AzureSQL validation failed: missing 'database'\",\n            server=self.server,\n        )\n        raise ValueError(\n            f\"Azure SQL connection requires 'database' for server '{self.server}'.\"\n        )\n\n    if self.auth_mode == \"sql\":\n        if not self.username:\n            ctx.error(\n                \"AzureSQL validation failed: SQL auth requires username\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                f\"Azure SQL with auth_mode='sql' requires 'username' \"\n                f\"for connection to {self.server}/{self.database}.\"\n            )\n        if not self.password and not (self.key_vault_name and self.secret_name):\n            ctx.error(\n                \"AzureSQL validation failed: SQL auth requires password\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                \"Azure SQL with auth_mode='sql' requires password \"\n                \"(or key_vault_name/secret_name)\"\n            )\n\n    if self.auth_mode == \"key_vault\":\n        if not self.key_vault_name or not self.secret_name:\n            ctx.error(\n                \"AzureSQL validation failed: key_vault mode missing config\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\n                \"Azure SQL with auth_mode='key_vault' requires key_vault_name and secret_name\"\n            )\n        if not self.username:\n            ctx.error(\n                \"AzureSQL validation failed: key_vault mode requires username\",\n                server=self.server,\n                database=self.database,\n            )\n            raise ValueError(\"Azure SQL with auth_mode='key_vault' requires username\")\n\n    ctx.info(\n        \"AzureSQL connection validated successfully\",\n        server=self.server,\n        database=self.database,\n        auth_mode=self.auth_mode,\n    )\n</code></pre>"},{"location":"reference/api/connections/#odibi.connections.azure_sql.AzureSQL.write_table","title":"<code>write_table(df, table_name, schema='dbo', if_exists='replace', index=False, chunksize=1000)</code>","text":"<p>Write DataFrame to SQL table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>table_name</code> <code>str</code> <p>Name of the table</p> required <code>schema</code> <code>Optional[str]</code> <p>Schema name (default: dbo)</p> <code>'dbo'</code> <code>if_exists</code> <code>str</code> <p>How to behave if table exists ('fail', 'replace', 'append')</p> <code>'replace'</code> <code>index</code> <code>bool</code> <p>Whether to write DataFrame index as column</p> <code>False</code> <code>chunksize</code> <code>Optional[int]</code> <p>Number of rows to write in each batch (default: 1000)</p> <code>1000</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows written</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If write fails</p> Source code in <code>odibi\\connections\\azure_sql.py</code> <pre><code>def write_table(\n    self,\n    df: pd.DataFrame,\n    table_name: str,\n    schema: Optional[str] = \"dbo\",\n    if_exists: str = \"replace\",\n    index: bool = False,\n    chunksize: Optional[int] = 1000,\n) -&gt; int:\n    \"\"\"\n    Write DataFrame to SQL table.\n\n    Args:\n        df: DataFrame to write\n        table_name: Name of the table\n        schema: Schema name (default: dbo)\n        if_exists: How to behave if table exists ('fail', 'replace', 'append')\n        index: Whether to write DataFrame index as column\n        chunksize: Number of rows to write in each batch (default: 1000)\n\n    Returns:\n        Number of rows written\n\n    Raises:\n        ConnectionError: If write fails\n    \"\"\"\n    ctx = get_logging_context()\n    ctx.info(\n        \"Writing DataFrame to table\",\n        server=self.server,\n        database=self.database,\n        table_name=table_name,\n        schema=schema,\n        rows=len(df),\n        if_exists=if_exists,\n        chunksize=chunksize,\n    )\n\n    try:\n        engine = self.get_engine()\n\n        rows_written = df.to_sql(\n            name=table_name,\n            con=engine,\n            schema=schema,\n            if_exists=if_exists,\n            index=index,\n            chunksize=chunksize,\n            method=\"multi\",  # Use multi-row INSERT for better performance\n        )\n\n        result_rows = rows_written if rows_written is not None else len(df)\n        ctx.info(\n            \"Table write completed successfully\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            rows_written=result_rows,\n        )\n        return result_rows\n    except Exception as e:\n        if isinstance(e, ConnectionError):\n            raise\n        ctx.error(\n            \"Table write failed\",\n            server=self.server,\n            database=self.database,\n            table_name=table_name,\n            error=str(e),\n        )\n        raise ConnectionError(\n            connection_name=f\"AzureSQL({self.server})\",\n            reason=f\"Write operation failed: {str(e)}\",\n            suggestions=self._get_error_suggestions(str(e)),\n        )\n</code></pre>"},{"location":"reference/api/engine/","title":"Engine API","text":""},{"location":"reference/api/engine/#odibi.engine.base","title":"<code>odibi.engine.base</code>","text":"<p>Base engine interface.</p>"},{"location":"reference/api/engine/#odibi.engine.base.Engine","title":"<code>Engine</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for execution engines.</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>class Engine(ABC):\n    \"\"\"Abstract base class for execution engines.\"\"\"\n\n    # Custom format registry\n    _custom_readers: Dict[str, Any] = {}\n    _custom_writers: Dict[str, Any] = {}\n\n    @classmethod\n    def register_format(cls, fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None):\n        \"\"\"Register custom format reader/writer.\n\n        Args:\n            fmt: Format name (e.g. 'netcdf')\n            reader: Function(path, **options) -&gt; DataFrame\n            writer: Function(df, path, **options) -&gt; None\n        \"\"\"\n        if reader:\n            cls._custom_readers[fmt] = reader\n        if writer:\n            cls._custom_writers[fmt] = writer\n\n    @abstractmethod\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n    ) -&gt; Any:\n        \"\"\"Read data from source.\n\n        Args:\n            connection: Connection object\n            format: Data format (csv, parquet, delta, etc.)\n            table: Table name (for SQL/Delta)\n            path: File path (for file-based sources)\n            options: Format-specific options\n\n        Returns:\n            DataFrame (engine-specific type)\n        \"\"\"\n        pass\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Materialized DataFrame\n        \"\"\"\n        return df\n\n    @abstractmethod\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Write data to destination.\n\n        Args:\n            df: DataFrame to write\n            connection: Connection object\n            format: Output format\n            table: Table name (for SQL/Delta)\n            path: File path (for file-based outputs)\n            mode: Write mode (overwrite/append)\n            options: Format-specific options\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_sql(self, sql: str, context: Context) -&gt; Any:\n        \"\"\"Execute SQL query.\n\n        Args:\n            sql: SQL query string\n            context: Execution context with registered DataFrames\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n        \"\"\"Execute built-in operation (pivot, etc.).\n\n        Args:\n            operation: Operation name\n            params: Operation parameters\n            df: Input DataFrame\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_schema(self, df: Any) -&gt; Any:\n        \"\"\"Get DataFrame schema.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dict[str, str] mapping column names to types, or List[str] of names (deprecated)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            (rows, columns)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Row count\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\n\n        Args:\n            df: DataFrame\n            columns: Columns to check\n\n        Returns:\n            Dictionary of column -&gt; null count\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\n\n        Args:\n            df: DataFrame\n            schema_rules: Validation rules\n\n        Returns:\n            List of validation failures (empty if valid)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate data against rules.\n\n        Args:\n            df: DataFrame to validate\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages (empty if valid)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\n\n        Args:\n            df: DataFrame\n            n: Number of rows to return\n\n        Returns:\n            List of row dictionaries\n        \"\"\"\n        pass\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            List of file paths (or empty list if not applicable/supported)\n        \"\"\"\n        return []\n\n    def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dictionary of {column_name: null_percentage} (0.0 to 1.0)\n        \"\"\"\n        return {}\n\n    @abstractmethod\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Args:\n            connection: Connection object\n            table: Table name (for catalog tables)\n            path: File path (for path-based tables)\n\n        Returns:\n            True if table/location exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\n\n        Args:\n            df: Input DataFrame\n            target_schema: Target schema (column name -&gt; type)\n            policy: SchemaPolicyConfig object\n\n        Returns:\n            Harmonized DataFrame\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; Any:\n        \"\"\"Anonymize specified columns.\n\n        Args:\n            df: DataFrame to anonymize\n            columns: List of columns to anonymize\n            method: Method ('hash', 'mask', 'redact')\n            salt: Optional salt for hashing\n\n        Returns:\n            Anonymized DataFrame\n        \"\"\"\n        pass\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\n\n        Args:\n            connection: Connection object\n            table: Table name\n            path: File path\n            format: Data format (optional, helps with file-based sources)\n\n        Returns:\n            Schema dict or None if table doesn't exist or schema fetch fails.\n        \"\"\"\n        return None\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\n\n        Args:\n            connection: Connection object\n            format: Table format\n            table: Table name\n            path: Table path\n            config: AutoOptimizeConfig object\n        \"\"\"\n        pass\n\n    def add_write_metadata(\n        self,\n        df: Any,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; Any:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added (or unchanged if metadata_config is None/False)\n        \"\"\"\n        return df  # Default: no-op\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with metadata columns added (or unchanged if metadata_config is None/False)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: Any,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; Any:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added (or unchanged if metadata_config is None/False)\n    \"\"\"\n    return df  # Default: no-op\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>  <code>abstractmethod</code>","text":"<p>Anonymize specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to anonymize</p> required <code>columns</code> <code>List[str]</code> <p>List of columns to anonymize</p> required <code>method</code> <code>str</code> <p>Method ('hash', 'mask', 'redact')</p> required <code>salt</code> <code>Optional[str]</code> <p>Optional salt for hashing</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Anonymized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; Any:\n    \"\"\"Anonymize specified columns.\n\n    Args:\n        df: DataFrame to anonymize\n        columns: List of columns to anonymize\n        method: Method ('hash', 'mask', 'redact')\n        salt: Optional salt for hashing\n\n    Returns:\n        Anonymized DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.count_nulls","title":"<code>count_nulls(df, columns)</code>  <code>abstractmethod</code>","text":"<p>Count nulls in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>columns</code> <code>List[str]</code> <p>Columns to check</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary of column -&gt; null count</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\n\n    Args:\n        df: DataFrame\n        columns: Columns to check\n\n    Returns:\n        Dictionary of column -&gt; null count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.count_rows","title":"<code>count_rows(df)</code>  <code>abstractmethod</code>","text":"<p>Count rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>int</code> <p>Row count</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Row count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>  <code>abstractmethod</code>","text":"<p>Execute built-in operation (pivot, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Operation name</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Operation parameters</p> required <code>df</code> <code>Any</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n    \"\"\"Execute built-in operation (pivot, etc.).\n\n    Args:\n        operation: Operation name\n        params: Operation parameters\n        df: Input DataFrame\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.execute_sql","title":"<code>execute_sql(sql, context)</code>  <code>abstractmethod</code>","text":"<p>Execute SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context with registered DataFrames</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef execute_sql(self, sql: str, context: Context) -&gt; Any:\n    \"\"\"Execute SQL query.\n\n    Args:\n        sql: SQL query string\n        context: Execution context with registered DataFrames\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_sample","title":"<code>get_sample(df, n=10)</code>  <code>abstractmethod</code>","text":"<p>Get sample rows as list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>n</code> <code>int</code> <p>Number of rows to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of row dictionaries</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\n\n    Args:\n        df: DataFrame\n        n: Number of rows to return\n\n    Returns:\n        List of row dictionaries\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_schema","title":"<code>get_schema(df)</code>  <code>abstractmethod</code>","text":"<p>Get DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dict[str, str] mapping column names to types, or List[str] of names (deprecated)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_schema(self, df: Any) -&gt; Any:\n    \"\"\"Get DataFrame schema.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dict[str, str] mapping column names to types, or List[str] of names (deprecated)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_shape","title":"<code>get_shape(df)</code>  <code>abstractmethod</code>","text":"<p>Get DataFrame shape.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(rows, columns)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        (rows, columns)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths (or empty list if not applicable/supported)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        List of file paths (or empty list if not applicable/supported)\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Data format (optional, helps with file-based sources)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, str]]</code> <p>Schema dict or None if table doesn't exist or schema fetch fails.</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\n\n    Args:\n        connection: Connection object\n        table: Table name\n        path: File path\n        format: Data format (optional, helps with file-based sources)\n\n    Returns:\n        Schema dict or None if table doesn't exist or schema fetch fails.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>  <code>abstractmethod</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Input DataFrame</p> required <code>target_schema</code> <code>Dict[str, str]</code> <p>Target schema (column name -&gt; type)</p> required <code>policy</code> <code>Any</code> <p>SchemaPolicyConfig object</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Harmonized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\n\n    Args:\n        df: Input DataFrame\n        target_schema: Target schema (column name -&gt; type)\n        policy: SchemaPolicyConfig object\n\n    Returns:\n        Harmonized DataFrame\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Table format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>Table path</p> <code>None</code> <code>config</code> <code>Optional[Any]</code> <p>AutoOptimizeConfig object</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\n\n    Args:\n        connection: Connection object\n        format: Table format\n        table: Table name\n        path: Table path\n        config: AutoOptimizeConfig object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset into memory (DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Materialized DataFrame</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Materialized DataFrame\n    \"\"\"\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of {column_name: null_percentage} (0.0 to 1.0)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dictionary of {column_name: null_percentage} (0.0 to 1.0)\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.read","title":"<code>read(connection, format, table=None, path=None, options=None)</code>  <code>abstractmethod</code>","text":"<p>Read data from source.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Data format (csv, parquet, delta, etc.)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for SQL/Delta)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for file-based sources)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame (engine-specific type)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n) -&gt; Any:\n    \"\"\"Read data from source.\n\n    Args:\n        connection: Connection object\n        format: Data format (csv, parquet, delta, etc.)\n        table: Table name (for SQL/Delta)\n        path: File path (for file-based sources)\n        options: Format-specific options\n\n    Returns:\n        DataFrame (engine-specific type)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.register_format","title":"<code>register_format(fmt, reader=None, writer=None)</code>  <code>classmethod</code>","text":"<p>Register custom format reader/writer.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>str</code> <p>Format name (e.g. 'netcdf')</p> required <code>reader</code> <code>Optional[Any]</code> <p>Function(path, **options) -&gt; DataFrame</p> <code>None</code> <code>writer</code> <code>Optional[Any]</code> <p>Function(df, path, **options) -&gt; None</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@classmethod\ndef register_format(cls, fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None):\n    \"\"\"Register custom format reader/writer.\n\n    Args:\n        fmt: Format name (e.g. 'netcdf')\n        reader: Function(path, **options) -&gt; DataFrame\n        writer: Function(df, path, **options) -&gt; None\n    \"\"\"\n    if reader:\n        cls._custom_readers[fmt] = reader\n    if writer:\n        cls._custom_writers[fmt] = writer\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>  <code>abstractmethod</code>","text":"<p>Check if table or location exists.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for catalog tables)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for path-based tables)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if table/location exists, False otherwise</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Args:\n        connection: Connection object\n        table: Table name (for catalog tables)\n        path: File path (for path-based tables)\n\n    Returns:\n        True if table/location exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.validate_data","title":"<code>validate_data(df, validation_config)</code>  <code>abstractmethod</code>","text":"<p>Validate data against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to validate</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages (empty if valid)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate data against rules.\n\n    Args:\n        df: DataFrame to validate\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages (empty if valid)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>  <code>abstractmethod</code>","text":"<p>Validate DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame</p> required <code>schema_rules</code> <code>Dict[str, Any]</code> <p>Validation rules</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failures (empty if valid)</p> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\n\n    Args:\n        df: DataFrame\n        schema_rules: Validation rules\n\n    Returns:\n        List of validation failures (empty if valid)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.base.Engine.write","title":"<code>write(df, connection, format, table=None, path=None, mode='overwrite', options=None, streaming_config=None)</code>  <code>abstractmethod</code>","text":"<p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to write</p> required <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Output format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (for SQL/Delta)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path (for file-based outputs)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Write mode (overwrite/append)</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options</p> <code>None</code> Source code in <code>odibi\\engine\\base.py</code> <pre><code>@abstractmethod\ndef write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Write data to destination.\n\n    Args:\n        df: DataFrame to write\n        connection: Connection object\n        format: Output format\n        table: Table name (for SQL/Delta)\n        path: File path (for file-based outputs)\n        mode: Write mode (overwrite/append)\n        options: Format-specific options\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine","title":"<code>odibi.engine.pandas_engine</code>","text":"<p>Pandas engine implementation.</p>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.LazyDataset","title":"<code>LazyDataset</code>  <code>dataclass</code>","text":"<p>Lazy representation of a dataset (file) for out-of-core processing.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>@dataclass\nclass LazyDataset:\n    \"\"\"Lazy representation of a dataset (file) for out-of-core processing.\"\"\"\n\n    path: Union[str, List[str]]\n    format: str\n    options: Dict[str, Any]\n    connection: Optional[Any] = None  # To resolve path/credentials if needed\n\n    def __repr__(self):\n        return f\"LazyDataset(path={self.path}, format={self.format})\"\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine","title":"<code>PandasEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Pandas-based execution engine.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>class PandasEngine(Engine):\n    \"\"\"Pandas-based execution engine.\"\"\"\n\n    name = \"pandas\"\n    engine_type = EngineType.PANDAS\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Pandas engine.\n\n        Args:\n            connections: Dictionary of connection objects\n            config: Engine configuration (optional)\n        \"\"\"\n        self.connections = connections or {}\n        self.config = config or {}\n\n        # Suppress noisy delta-rs transaction conflict warnings (handled by retry)\n        if \"RUST_LOG\" not in os.environ:\n            os.environ[\"RUST_LOG\"] = \"deltalake_core::kernel::transaction=error\"\n\n        # Check for performance flags\n        performance = self.config.get(\"performance\", {})\n\n        # Determine desired state\n        if hasattr(performance, \"use_arrow\"):\n            desired_use_arrow = performance.use_arrow\n        elif isinstance(performance, dict):\n            desired_use_arrow = performance.get(\"use_arrow\", True)\n        else:\n            desired_use_arrow = True\n\n        # Verify availability\n        if desired_use_arrow:\n            try:\n                import pyarrow  # noqa: F401\n\n                self.use_arrow = True\n            except ImportError:\n                import logging\n\n                logger = logging.getLogger(__name__)\n                logger.warning(\n                    \"Apache Arrow not found. Disabling Arrow optimizations. \"\n                    \"Install 'pyarrow' to enable.\"\n                )\n                self.use_arrow = False\n        else:\n            self.use_arrow = False\n\n        # Check for DuckDB\n        self.use_duckdb = False\n        # Default to False to ensure stability with existing tests (Lazy Loading is opt-in)\n        if self.config.get(\"performance\", {}).get(\"use_duckdb\", False):\n            try:\n                import duckdb  # noqa: F401\n\n                self.use_duckdb = True\n            except ImportError:\n                pass\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset.\"\"\"\n        if isinstance(df, LazyDataset):\n            # Re-invoke read but force materialization (by bypassing Lazy check)\n            # We pass the resolved path directly\n            # Note: We need to handle the case where path was resolved.\n            # LazyDataset.path should be the FULL path.\n            return self._read_file(\n                full_path=df.path, format=df.format, options=df.options, connection=df.connection\n            )\n        return df\n\n    def _process_df(\n        self, df: Union[pd.DataFrame, Iterator[pd.DataFrame]], query: Optional[str]\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Apply post-read processing (filtering).\"\"\"\n        if query and df is not None:\n            # Handle Iterator\n            from collections.abc import Iterator\n\n            if isinstance(df, Iterator):\n                # Filter each chunk\n                return (chunk.query(query) for chunk in df)\n\n            if not df.empty:\n                try:\n                    return df.query(query)\n                except Exception as e:\n                    import logging\n\n                    logger = logging.getLogger(__name__)\n                    logger.warning(f\"Failed to apply query '{query}': {e}\")\n        return df\n\n    _CLOUD_URI_PREFIXES = (\n        \"abfss://\",\n        \"abfs://\",\n        \"wasbs://\",\n        \"wasb://\",\n        \"az://\",\n        \"s3://\",\n        \"gs://\",\n        \"https://\",\n    )\n\n    def _retry_delta_operation(self, func, max_retries: int = 5, base_delay: float = 0.2):\n        \"\"\"Retry Delta operations with exponential backoff for concurrent conflicts.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                return func()\n            except Exception as e:\n                error_str = str(e).lower()\n                is_conflict = \"conflict\" in error_str or \"concurrent\" in error_str\n                if attempt == max_retries - 1 or not is_conflict:\n                    raise\n                delay = base_delay * (2**attempt) + random.uniform(0, 0.1)\n                time.sleep(delay)\n\n    def _read_delta_with_duckdb(\n        self,\n        path: str,\n        storage_options: dict,\n        version: Optional[int],\n        timestamp: Optional[str],\n        post_read_query: Optional[str],\n        ctx,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Read Delta table using DuckDB (supports DeletionVectors and ColumnMapping).\n\n        DuckDB's delta extension supports advanced Delta features that the Python\n        deltalake library doesn't yet support. This is used as a fallback when\n        deltalake fails with unsupported reader features.\n\n        Args:\n            path: Path to Delta table\n            storage_options: Storage authentication options (for ADLS, S3, etc.)\n            version: Specific version to read (time travel)\n            timestamp: Specific timestamp to read (time travel)\n            post_read_query: Optional pandas query to apply after reading\n            ctx: Logging context\n\n        Returns:\n            DataFrame with Delta table contents\n        \"\"\"\n        try:\n            import duckdb\n        except ImportError:\n            raise ImportError(\n                \"DuckDB is required to read Delta tables with DeletionVectors or ColumnMapping. \"\n                \"Install with 'pip install duckdb&gt;=1.0.0'.\"\n            )\n\n        ctx.debug(\"Reading Delta table with DuckDB\", path=path)\n\n        conn = duckdb.connect(\":memory:\")\n\n        # Install and load the delta extension\n        conn.execute(\"INSTALL delta\")\n        conn.execute(\"LOAD delta\")\n\n        # Configure storage options for cloud paths\n        if storage_options:\n            # Azure ADLS configuration\n            if \"account_name\" in storage_options:\n                account = storage_options[\"account_name\"]\n                if \"account_key\" in storage_options:\n                    conn.execute(\n                        f\"SET azure_storage_account_name = '{account}';\"\n                        f\"SET azure_storage_account_key = '{storage_options['account_key']}';\"\n                    )\n                elif \"sas_token\" in storage_options:\n                    sas = storage_options[\"sas_token\"]\n                    if sas.startswith(\"?\"):\n                        sas = sas[1:]\n                    conn.execute(\n                        f\"SET azure_storage_account_name = '{account}';\"\n                        f\"SET azure_storage_sas_token = '{sas}';\"\n                    )\n                # For Azure Identity (DefaultCredential), DuckDB uses env vars automatically\n\n            # AWS S3 configuration\n            if \"AWS_ACCESS_KEY_ID\" in storage_options:\n                conn.execute(\n                    f\"SET s3_access_key_id = '{storage_options['AWS_ACCESS_KEY_ID']}';\"\n                    f\"SET s3_secret_access_key = '{storage_options['AWS_SECRET_ACCESS_KEY']}';\"\n                )\n                if \"AWS_REGION\" in storage_options:\n                    conn.execute(f\"SET s3_region = '{storage_options['AWS_REGION']}';\")\n\n        # Build the delta_scan query\n        # Note: DuckDB delta_scan doesn't support version/timestamp time travel yet\n        if version is not None or timestamp is not None:\n            ctx.warning(\n                \"DuckDB delta_scan does not support time travel yet. \"\n                \"Reading latest version instead.\",\n                requested_version=version,\n                requested_timestamp=timestamp,\n            )\n\n        query = f\"SELECT * FROM delta_scan('{path}')\"\n        ctx.debug(\"Executing DuckDB query\", query=query)\n\n        df = conn.execute(query).fetchdf()\n        conn.close()\n\n        ctx.debug(\"Delta table read via DuckDB\", rows=len(df), columns=len(df.columns))\n\n        return self._process_df(df, post_read_query)\n\n    def _resolve_path(self, path: Optional[str], connection: Any) -&gt; str:\n        \"\"\"Resolve path to full URI, avoiding double-prefixing for cloud URIs.\n\n        Args:\n            path: Relative or absolute path\n            connection: Connection object (may have get_path method)\n\n        Returns:\n            Full resolved path\n        \"\"\"\n        if not path:\n            raise ValueError(\n                \"Failed to resolve path: path argument is required but was empty or None. \"\n                \"Provide a valid file path or use 'table' parameter with a connection.\"\n            )\n        if path.startswith(self._CLOUD_URI_PREFIXES):\n            return path\n        if connection:\n            return connection.get_path(path)\n        return path\n\n    def _merge_storage_options(\n        self, connection: Any, options: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Merge connection storage options with user options.\n\n        Args:\n            connection: Connection object (may have pandas_storage_options method)\n            options: User-provided options\n\n        Returns:\n            Merged options dictionary\n        \"\"\"\n        options = options or {}\n\n        # If connection provides storage_options (e.g., AzureADLS), merge them\n        if hasattr(connection, \"pandas_storage_options\"):\n            conn_storage_opts = connection.pandas_storage_options()\n            user_storage_opts = options.get(\"storage_options\", {})\n\n            # User options override connection options\n            merged_storage_opts = {**conn_storage_opts, **user_storage_opts}\n\n            # Return options with merged storage_options\n            return {**options, \"storage_options\": merged_storage_opts}\n\n        return options\n\n    def _is_remote_uri(self, path: Union[str, Path]) -&gt; bool:\n        \"\"\"Check if a path is a remote URI (cloud storage).\n\n        Args:\n            path: File path or URI\n\n        Returns:\n            True if path is a remote URI requiring fsspec\n        \"\"\"\n        if isinstance(path, Path):\n            path = str(path)\n\n        parsed = urlparse(path)\n\n        # Windows drive letter check (e.g., C:\\foo.xlsx parses as scheme='c')\n        if os.name == \"nt\" and len(parsed.scheme) == 1 and parsed.scheme.isalpha():\n            return False\n\n        # Cloud schemes\n        if parsed.scheme and parsed.scheme not in (\"file\", \"\"):\n            return True\n\n        # Fallback prefix check\n        return path.startswith(self._CLOUD_URI_PREFIXES)\n\n    def _expand_remote_glob(\n        self,\n        pattern: str,\n        storage_options: Optional[Dict[str, Any]] = None,\n        ctx: Any = None,\n    ) -&gt; List[str]:\n        \"\"\"Expand glob pattern for remote paths using fsspec.\n\n        Args:\n            pattern: Glob pattern (e.g., \"abfss://container@account.dfs.core.windows.net/path/*.xlsx\")\n            storage_options: Authentication options for cloud storage\n            ctx: Logging context\n\n        Returns:\n            List of matching file URIs\n        \"\"\"\n        if ctx is None:\n            ctx = get_logging_context()\n\n        try:\n            import fsspec\n        except ImportError:\n            ctx.error(\"fsspec required for remote glob expansion\")\n            raise ImportError(\n                \"Remote glob patterns require 'fsspec'. Install with: pip install fsspec adlfs\"\n            )\n\n        # Parse protocol and path\n        if \"://\" in pattern:\n            protocol, _, path_part = pattern.partition(\"://\")\n        else:\n            protocol = \"file\"\n            path_part = pattern\n\n        ctx.debug(\n            \"Expanding remote glob pattern\",\n            protocol=protocol,\n            pattern=path_part,\n        )\n\n        # Create filesystem with storage options\n        fs = fsspec.filesystem(protocol, **(storage_options or {}))\n\n        # Glob and reconstruct full URIs\n        matched = fs.glob(path_part)\n\n        if not matched:\n            ctx.warning(\"No files matched remote glob pattern\", pattern=pattern)\n            return []\n\n        # Reconstruct full URIs\n        result = [f\"{protocol}://{m}\" for m in matched]\n\n        ctx.info(\n            \"Remote glob pattern expanded\",\n            pattern=pattern,\n            matched_files=len(result),\n        )\n\n        return result\n\n    def _open_remote_file(\n        self,\n        path: str,\n        storage_options: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Open a remote file using fsspec.\n\n        Args:\n            path: Remote file URI\n            storage_options: Authentication options for cloud storage\n\n        Returns:\n            Context manager yielding a binary file-like object\n        \"\"\"\n        import fsspec\n\n        return fsspec.open(path, \"rb\", **(storage_options or {}))\n\n    def _read_parallel(self, read_func: Any, paths: List[str], **kwargs) -&gt; pd.DataFrame:\n        \"\"\"Read multiple files in parallel using threads.\n\n        Args:\n            read_func: Pandas read function (e.g. pd.read_csv)\n            paths: List of file paths\n            kwargs: Arguments to pass to read_func\n\n        Returns:\n            Concatenated DataFrame\n        \"\"\"\n        # Conservative worker count to avoid OOM on large files\n        max_workers = min(8, os.cpu_count() or 4)\n\n        dfs = []\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # map preserves order\n            results = executor.map(lambda p: read_func(p, **kwargs), paths)\n            dfs = list(results)\n\n        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n\n    def _read_excel_with_patterns(\n        self,\n        paths: Union[str, Path, List[str]],\n        sheet_pattern: Union[str, List[str], None] = None,\n        sheet_pattern_case_sensitive: bool = False,\n        add_source_file: bool = False,\n        is_glob: bool = False,\n        ctx: Any = None,\n        storage_options: Optional[Dict[str, Any]] = None,\n        **read_kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Read Excel files with glob and sheet pattern support.\n\n        Supports both local files and remote cloud storage (Azure Blob, S3, etc.)\n        via fsspec integration.\n\n        Args:\n            paths: Single path, glob-expanded list, or list of paths.\n                   Can be local paths or cloud URIs (abfss://, s3://, etc.)\n            sheet_pattern: Pattern(s) to match sheet names (e.g., \"*powerbi*\").\n                          Supports single string or list of patterns.\n            sheet_pattern_case_sensitive: Whether pattern matching is case-sensitive.\n            add_source_file: If True, adds '_source_file' column with filename.\n            is_glob: Whether paths contains a glob pattern to expand.\n            ctx: Logging context.\n            storage_options: Authentication options for cloud storage (from connection).\n            **read_kwargs: Additional kwargs for pd.read_excel.\n\n        Returns:\n            Concatenated DataFrame from all matching files/sheets.\n        \"\"\"\n        if ctx is None:\n            ctx = get_logging_context()\n\n        # Normalize patterns to list\n        if sheet_pattern is None:\n            patterns = None\n        elif isinstance(sheet_pattern, str):\n            patterns = [sheet_pattern]\n        else:\n            patterns = list(sheet_pattern)\n\n        def match_sheet(sheet_name: str) -&gt; bool:\n            \"\"\"Check if sheet name matches any pattern.\"\"\"\n            if patterns is None:\n                return True  # No pattern = match all\n\n            check_name = sheet_name if sheet_pattern_case_sensitive else sheet_name.lower()\n            for pattern in patterns:\n                check_pattern = pattern if sheet_pattern_case_sensitive else pattern.lower()\n                if fnmatch.fnmatch(check_name, check_pattern):\n                    return True\n            return False\n\n        def get_file_name(file_path: str) -&gt; str:\n            \"\"\"Extract filename from path (works for both local and remote URIs).\"\"\"\n            if \"://\" in file_path:\n                # Remote URI - extract filename from path part\n                path_part = file_path.split(\"://\", 1)[1]\n                return path_part.rsplit(\"/\", 1)[-1] if \"/\" in path_part else path_part\n            else:\n                return Path(file_path).name\n\n        def read_single_excel(file_path: str) -&gt; pd.DataFrame:\n            \"\"\"Read a single Excel file, matching sheets by pattern.\n\n            Handles both local and remote files transparently.\n            \"\"\"\n            file_name = get_file_name(file_path)\n            is_remote = self._is_remote_uri(file_path)\n\n            # For remote files, remove storage_options - we handle it via fsspec\n            # For local files, keep storage_options for consistency (pandas ignores it)\n            if is_remote:\n                excel_kwargs = {k: v for k, v in read_kwargs.items() if k != \"storage_options\"}\n            else:\n                excel_kwargs = read_kwargs.copy()\n                # Add storage_options back if provided (for test compatibility)\n                if storage_options and \"storage_options\" not in excel_kwargs:\n                    excel_kwargs[\"storage_options\"] = storage_options\n\n            try:\n                if is_remote:\n                    # Remote file - use fsspec to open and pass file handle\n                    ctx.debug(\n                        \"Reading remote Excel file\",\n                        file=file_name,\n                        uri=file_path,\n                    )\n                    with self._open_remote_file(file_path, storage_options) as fh:\n                        return _read_excel_from_source(fh, file_name, excel_kwargs)\n                else:\n                    # Local file - pass path directly to pandas (supports mocking)\n                    ctx.debug(\"Reading local Excel file\", file=file_name)\n                    return _read_excel_from_source(file_path, file_name, excel_kwargs)\n\n            except Exception as e:\n                ctx.warning(\n                    \"Failed to read Excel file\",\n                    file=file_name,\n                    path=file_path,\n                    error=str(e),\n                )\n                raise\n\n        def _read_excel_from_source(source, file_name: str, excel_kwargs: dict) -&gt; pd.DataFrame:\n            \"\"\"Read Excel data from a path or file handle with sheet pattern matching.\n\n            Args:\n                source: File path (str) or file handle for pandas to read from\n                file_name: Display name for the file (used in _source_file column)\n                excel_kwargs: Keyword arguments to pass to pd.read_excel\n            \"\"\"\n            # If no sheet pattern, use simple read_excel\n            if patterns is None:\n                df = pd.read_excel(source, **excel_kwargs)\n                if add_source_file:\n                    df[\"_source_file\"] = file_name\n                    df[\"_source_sheet\"] = excel_kwargs.get(\"sheet_name\", 0)\n                return df\n\n            # With sheet pattern, need to inspect sheets first\n            xls = pd.ExcelFile(source)\n            matching_sheets = [s for s in xls.sheet_names if match_sheet(s)]\n\n            if not matching_sheets:\n                ctx.debug(\n                    \"No matching sheets in Excel file\",\n                    file=file_name,\n                    patterns=patterns,\n                    available_sheets=xls.sheet_names,\n                )\n                return pd.DataFrame()\n\n            ctx.debug(\n                \"Reading Excel sheets\",\n                file=file_name,\n                matching_sheets=matching_sheets,\n            )\n\n            dfs = []\n            for sheet in matching_sheets:\n                df = pd.read_excel(xls, sheet_name=sheet, **excel_kwargs)\n                if add_source_file:\n                    df[\"_source_file\"] = file_name\n                    df[\"_source_sheet\"] = sheet\n                dfs.append(df)\n\n            return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n\n        # Expand glob patterns if needed\n        file_list: List[str] = []\n\n        if isinstance(paths, (str, Path)):\n            path_str = str(paths)\n\n            # Check if this is a glob pattern that needs expansion\n            has_glob = \"*\" in path_str or \"?\" in path_str or \"[\" in path_str\n\n            if has_glob:\n                if self._is_remote_uri(path_str):\n                    # Remote glob - use fsspec\n                    file_list = self._expand_remote_glob(path_str, storage_options, ctx)\n                else:\n                    # Local glob\n                    file_list = glob.glob(path_str)\n                    if not file_list:\n                        raise FileNotFoundError(f\"No files matched pattern: {path_str}\")\n                    ctx.info(\n                        \"Local glob pattern expanded\",\n                        pattern=path_str,\n                        matched_files=len(file_list),\n                    )\n            else:\n                # Single file (no glob)\n                file_list = [path_str]\n\n        elif isinstance(paths, list):\n            # Already a list of paths (pre-expanded)\n            file_list = [str(p) for p in paths]\n\n        if not file_list:\n            raise FileNotFoundError(f\"No Excel files found for: {paths}\")\n\n        # Read all files\n        ctx.info(\n            \"Reading Excel files\",\n            file_count=len(file_list),\n            sheet_patterns=patterns,\n            is_remote=self._is_remote_uri(file_list[0]) if file_list else False,\n        )\n\n        all_dfs = []\n        source_files = []\n\n        for file_path in file_list:\n            df = read_single_excel(file_path)\n            if not df.empty:\n                all_dfs.append(df)\n                source_files.append(file_path)\n\n        if not all_dfs:\n            ctx.warning(\"No data read from Excel files\", file_count=len(file_list))\n            return pd.DataFrame()\n\n        result = pd.concat(all_dfs, ignore_index=True)\n        if hasattr(result, \"attrs\"):\n            result.attrs[\"odibi_source_files\"] = source_files\n\n        return result\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        as_of_version: Optional[int] = None,\n        as_of_timestamp: Optional[str] = None,\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Read data using Pandas (or LazyDataset).\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        source = path or table\n        ctx.debug(\n            \"Starting read operation\",\n            format=format,\n            path=source,\n            streaming=streaming,\n            use_arrow=self.use_arrow,\n        )\n\n        if streaming:\n            ctx.error(\n                \"Streaming not supported in Pandas engine\",\n                format=format,\n                path=source,\n            )\n            raise ValueError(\n                \"Streaming is not supported in the Pandas engine. \"\n                \"Please use 'engine: spark' for streaming pipelines.\"\n            )\n\n        options = options or {}\n\n        # Resolve full path from connection\n        try:\n            full_path = self._resolve_path(path or table, connection)\n        except ValueError:\n            if table and not connection:\n                ctx.error(\"Connection required when specifying 'table'\", table=table)\n                raise ValueError(\n                    f\"Cannot read table '{table}': connection is required when using 'table' parameter. \"\n                    \"Provide a valid connection object or use 'path' for file-based reads.\"\n                )\n            ctx.error(\"Neither path nor table provided for read operation\")\n            raise ValueError(\n                \"Read operation failed: neither 'path' nor 'table' was provided. \"\n                \"Specify a file path or table name in your configuration.\"\n            )\n\n        # Merge storage options for cloud connections\n        merged_options = self._merge_storage_options(connection, options)\n\n        # Sanitize options for pandas compatibility\n        if \"header\" in merged_options:\n            if merged_options[\"header\"] is True:\n                merged_options[\"header\"] = 0\n            elif merged_options[\"header\"] is False:\n                merged_options[\"header\"] = None\n\n        # Handle Time Travel options\n        if as_of_version is not None:\n            merged_options[\"versionAsOf\"] = as_of_version\n            ctx.debug(\"Time travel enabled\", version=as_of_version)\n        if as_of_timestamp is not None:\n            merged_options[\"timestampAsOf\"] = as_of_timestamp\n            ctx.debug(\"Time travel enabled\", timestamp=as_of_timestamp)\n\n        # Check for Lazy/DuckDB optimization\n        can_lazy_load = False\n\n        if can_lazy_load:\n            ctx.debug(\"Using lazy loading via DuckDB\", path=str(full_path))\n            if isinstance(full_path, (str, Path)):\n                return LazyDataset(\n                    path=str(full_path),\n                    format=format,\n                    options=merged_options,\n                    connection=connection,\n                )\n            elif isinstance(full_path, list):\n                return LazyDataset(\n                    path=full_path, format=format, options=merged_options, connection=connection\n                )\n\n        result = self._read_file(full_path, format, merged_options, connection)\n\n        # Log metrics for materialized DataFrames\n        elapsed = (time.time() - start) * 1000\n        if isinstance(result, pd.DataFrame):\n            row_count = len(result)\n            memory_mb = result.memory_usage(deep=True).sum() / (1024 * 1024)\n\n            ctx.log_file_io(\n                path=str(full_path) if not isinstance(full_path, list) else str(full_path[0]),\n                format=format,\n                mode=\"read\",\n                rows=row_count,\n            )\n            ctx.log_pandas_metrics(\n                memory_mb=memory_mb,\n                dtypes={col: str(dtype) for col, dtype in result.dtypes.items()},\n            )\n            ctx.info(\n                \"Read completed\",\n                format=format,\n                rows=row_count,\n                elapsed_ms=round(elapsed, 2),\n                memory_mb=round(memory_mb, 2),\n            )\n\n        return result\n\n    def _read_file(\n        self,\n        full_path: Union[str, List[str], Any],\n        format: str,\n        options: Dict[str, Any],\n        connection: Any = None,\n    ) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n        \"\"\"Internal file reading logic.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        ctx.debug(\n            \"Reading file\",\n            path=str(full_path) if not isinstance(full_path, list) else f\"{len(full_path)} files\",\n            format=format,\n        )\n\n        # Custom Readers\n        if format in self._custom_readers:\n            ctx.debug(f\"Using custom reader for format: {format}\")\n            return self._custom_readers[format](full_path, **options)\n\n        # Handle glob patterns for local files\n        is_glob = False\n        if isinstance(full_path, (str, Path)) and (\n            \"*\" in str(full_path) or \"?\" in str(full_path) or \"[\" in str(full_path)\n        ):\n            parsed = urlparse(str(full_path))\n            # Only expand for local files (no scheme, file://, or drive letter)\n            is_local = (\n                not parsed.scheme\n                or parsed.scheme == \"file\"\n                or (len(parsed.scheme) == 1 and parsed.scheme.isalpha())\n            )\n\n            if is_local:\n                glob_path = str(full_path)\n                if glob_path.startswith(\"file:///\"):\n                    glob_path = glob_path[8:]\n                elif glob_path.startswith(\"file://\"):\n                    glob_path = glob_path[7:]\n\n                matched_files = glob.glob(glob_path)\n                if not matched_files:\n                    ctx.error(\n                        \"No files matched glob pattern\",\n                        pattern=glob_path,\n                    )\n                    raise FileNotFoundError(f\"No files matched pattern: {glob_path}\")\n\n                ctx.info(\n                    \"Glob pattern expanded\",\n                    pattern=glob_path,\n                    matched_files=len(matched_files),\n                )\n                full_path = matched_files\n                is_glob = True\n\n        # Prepare read options (options already includes storage_options from caller)\n        read_kwargs = options.copy()\n\n        # Filter out Spark-specific options that don't apply to Pandas\n        spark_only_options = {\n            \"inferSchema\",\n            \"multiLine\",\n            \"mode\",\n            \"columnNameOfCorruptRecord\",\n            \"dateFormat\",\n            \"timestampFormat\",\n            \"nullValue\",\n            \"nanValue\",\n            \"positiveInf\",\n            \"negativeInf\",\n            \"escape\",\n            \"charToEscapeQuoteEscaping\",\n            \"ignoreLeadingWhiteSpace\",\n            \"ignoreTrailingWhiteSpace\",\n            \"maxColumns\",\n            \"maxCharsPerColumn\",\n            \"unescapedQuoteHandling\",\n            \"enforceSchema\",\n            \"samplingRatio\",\n            \"emptyValue\",\n            \"locale\",\n            \"lineSep\",\n            \"pathGlobFilter\",\n            \"recursiveFileLookup\",\n            \"modifiedBefore\",\n            \"modifiedAfter\",\n        }\n        for opt in spark_only_options:\n            read_kwargs.pop(opt, None)\n\n        # Extract 'query' or 'filter' option for post-read filtering\n        post_read_query = read_kwargs.pop(\"query\", None) or read_kwargs.pop(\"filter\", None)\n\n        if self.use_arrow:\n            read_kwargs[\"dtype_backend\"] = \"pyarrow\"\n\n        # Read based on format\n        if format == \"csv\":\n            try:\n                if is_glob and isinstance(full_path, list):\n                    ctx.debug(\n                        \"Parallel CSV read\",\n                        file_count=len(full_path),\n                    )\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n            except UnicodeDecodeError:\n                ctx.warning(\n                    \"UnicodeDecodeError, retrying with latin1 encoding\",\n                    path=str(full_path),\n                )\n                read_kwargs[\"encoding\"] = \"latin1\"\n                if is_glob and isinstance(full_path, list):\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n            except pd.errors.ParserError:\n                ctx.warning(\n                    \"ParserError, retrying with on_bad_lines='skip'\",\n                    path=str(full_path),\n                )\n                read_kwargs[\"on_bad_lines\"] = \"skip\"\n                if is_glob and isinstance(full_path, list):\n                    df = self._read_parallel(pd.read_csv, full_path, **read_kwargs)\n                    df.attrs[\"odibi_source_files\"] = full_path\n                    return self._process_df(df, post_read_query)\n\n                df = pd.read_csv(full_path, **read_kwargs)\n                if hasattr(df, \"attrs\"):\n                    df.attrs[\"odibi_source_files\"] = [str(full_path)]\n                return self._process_df(df, post_read_query)\n        elif format == \"parquet\":\n            ctx.debug(\"Reading parquet\", path=str(full_path))\n            df = pd.read_parquet(full_path, **read_kwargs)\n            if isinstance(full_path, list):\n                df.attrs[\"odibi_source_files\"] = full_path\n            else:\n                df.attrs[\"odibi_source_files\"] = [str(full_path)]\n            return self._process_df(df, post_read_query)\n        elif format == \"json\":\n            if is_glob and isinstance(full_path, list):\n                ctx.debug(\n                    \"Parallel JSON read\",\n                    file_count=len(full_path),\n                )\n                df = self._read_parallel(pd.read_json, full_path, **read_kwargs)\n                df.attrs[\"odibi_source_files\"] = full_path\n                return self._process_df(df, post_read_query)\n\n            df = pd.read_json(full_path, **read_kwargs)\n            if hasattr(df, \"attrs\"):\n                df.attrs[\"odibi_source_files\"] = [str(full_path)]\n            return self._process_df(df, post_read_query)\n        elif format == \"excel\":\n            ctx.debug(\"Reading Excel file\", path=str(full_path))\n            read_kwargs.pop(\"dtype_backend\", None)\n\n            # Extract excel-specific options\n            sheet_pattern = read_kwargs.pop(\"sheet_pattern\", None)\n            sheet_pattern_case_sensitive = read_kwargs.pop(\"sheet_pattern_case_sensitive\", False)\n            add_source_file = read_kwargs.pop(\"add_source_file\", False)\n\n            # Extract storage_options for cloud storage authentication\n            storage_options = read_kwargs.pop(\"storage_options\", None)\n\n            df = self._read_excel_with_patterns(\n                full_path,\n                sheet_pattern=sheet_pattern,\n                sheet_pattern_case_sensitive=sheet_pattern_case_sensitive,\n                add_source_file=add_source_file,\n                is_glob=is_glob,\n                ctx=ctx,\n                storage_options=storage_options,\n                **read_kwargs,\n            )\n            return self._process_df(df, post_read_query)\n        elif format == \"delta\":\n            ctx.debug(\"Reading Delta table\", path=str(full_path))\n            try:\n                from deltalake import DeltaTable\n            except ImportError:\n                ctx.error(\n                    \"Delta Lake library not installed\",\n                    path=str(full_path),\n                )\n                raise ImportError(\n                    \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                    \"or 'pip install deltalake'. See README.md for installation instructions.\"\n                )\n\n            storage_opts = options.get(\"storage_options\", {})\n            version = options.get(\"versionAsOf\")\n            timestamp = options.get(\"timestampAsOf\")\n\n            # Try deltalake first, fall back to DuckDB for unsupported features\n            # (e.g., DeletionVectors, ColumnMapping)\n            try:\n                if timestamp is not None:\n                    from datetime import datetime as dt_module\n\n                    if isinstance(timestamp, str):\n                        ts = dt_module.fromisoformat(timestamp.replace(\"Z\", \"+00:00\"))\n                    else:\n                        ts = timestamp\n                    dt = DeltaTable(full_path, storage_options=storage_opts)\n                    dt.load_with_datetime(ts)\n                    ctx.debug(\"Delta table loaded with timestamp\", timestamp=str(ts))\n                elif version is not None:\n                    dt = DeltaTable(full_path, storage_options=storage_opts, version=version)\n                    ctx.debug(\"Delta table loaded with version\", version=version)\n                else:\n                    dt = DeltaTable(full_path, storage_options=storage_opts)\n                    ctx.debug(\"Delta table loaded (latest version)\")\n\n                if self.use_arrow:\n                    import inspect\n\n                    sig = inspect.signature(dt.to_pandas)\n\n                    if \"arrow_options\" in sig.parameters:\n                        return self._process_df(\n                            dt.to_pandas(\n                                partitions=None, arrow_options={\"types_mapper\": pd.ArrowDtype}\n                            ),\n                            post_read_query,\n                        )\n                    else:\n                        return self._process_df(\n                            dt.to_pyarrow_table().to_pandas(types_mapper=pd.ArrowDtype),\n                            post_read_query,\n                        )\n                else:\n                    return self._process_df(dt.to_pandas(), post_read_query)\n\n            except Exception as e:\n                # Check if this is a DeltaProtocolError for unsupported features\n                error_msg = str(e).lower()\n                unsupported_features = (\n                    \"deletionvectors\" in error_msg\n                    or \"columnmapping\" in error_msg\n                    or \"reader features\" in error_msg\n                    or \"not yet supported\" in error_msg\n                )\n\n                if unsupported_features:\n                    ctx.warning(\n                        \"Delta table uses features not supported by deltalake library, \"\n                        \"falling back to DuckDB\",\n                        path=str(full_path),\n                        error=str(e),\n                    )\n                    return self._read_delta_with_duckdb(\n                        full_path, storage_opts, version, timestamp, post_read_query, ctx\n                    )\n                else:\n                    raise\n        elif format == \"avro\":\n            ctx.debug(\"Reading Avro file\", path=str(full_path))\n            try:\n                import fastavro\n            except ImportError:\n                ctx.error(\n                    \"fastavro library not installed\",\n                    path=str(full_path),\n                )\n                raise ImportError(\n                    \"Avro support requires 'pip install odibi[pandas]' \"\n                    \"or 'pip install fastavro'. See README.md for installation instructions.\"\n                )\n\n            parsed = urlparse(full_path)\n            if parsed.scheme and parsed.scheme not in [\"file\", \"\"]:\n                import fsspec\n\n                storage_opts = options.get(\"storage_options\", {})\n                with fsspec.open(full_path, \"rb\", **storage_opts) as f:\n                    reader = fastavro.reader(f)\n                    records = [record for record in reader]\n                return pd.DataFrame(records)\n            else:\n                with open(full_path, \"rb\") as f:\n                    reader = fastavro.reader(f)\n                    records = [record for record in reader]\n                return self._process_df(pd.DataFrame(records), post_read_query)\n        elif format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            ctx.debug(\"Reading SQL table\", table=str(full_path), format=format)\n\n            if not hasattr(connection, \"read_table\") and not hasattr(connection, \"read_sql_query\"):\n                ctx.error(\n                    \"Connection does not support SQL operations\",\n                    connection_type=type(connection).__name__,\n                )\n                raise ValueError(\n                    f\"Cannot read SQL table '{full_path}': connection type '{type(connection).__name__}' \"\n                    \"does not support SQL operations. Use a SQL-compatible connection \"\n                    \"(e.g., SqlServerConnection, AzureSqlConnection).\"\n                )\n\n            table_name = str(full_path)\n            if \".\" in table_name:\n                schema, tbl = table_name.split(\".\", 1)\n            else:\n                schema, tbl = \"dbo\", table_name\n\n            # Build SQL query with optional WHERE clause for incremental pushdown\n            # post_read_query contains the SQL filter condition (e.g., \"[DateInserted] &gt; '2025-01-01'\")\n            if schema:\n                base_query = f\"SELECT * FROM [{schema}].[{tbl}]\"\n            else:\n                base_query = f\"SELECT * FROM [{tbl}]\"\n\n            if post_read_query and hasattr(connection, \"read_sql_query\"):\n                # Treat post_read_query as a WHERE clause condition for SQL pushdown\n                full_query = f\"{base_query} WHERE {post_read_query}\"\n                ctx.debug(\n                    \"Executing SQL query with incremental filter\",\n                    query=full_query,\n                )\n                return connection.read_sql_query(full_query)\n\n            ctx.debug(\"Executing SQL read\", schema=schema, table=tbl)\n            return connection.read_table(table_name=tbl, schema=schema)\n        else:\n            ctx.error(\"Unsupported format\", format=format)\n            raise ValueError(\n                f\"Unsupported format for Pandas engine: '{format}'. \"\n                \"Supported formats: csv, parquet, json, excel, delta, sql, sql_server, azure_sql.\"\n            )\n\n    def write(\n        self,\n        df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Pandas.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        destination = path or table\n        ctx.debug(\n            \"Starting write operation\",\n            format=format,\n            destination=destination,\n            mode=mode,\n        )\n\n        # Ensure materialization if LazyDataset\n        df = self.materialize(df)\n\n        options = options or {}\n\n        # Handle iterator/generator input\n        from collections.abc import Iterator\n\n        if isinstance(df, Iterator):\n            ctx.debug(\"Writing iterator/generator input\")\n            return self._write_iterator(df, connection, format, table, path, mode, options)\n\n        row_count = len(df)\n        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n\n        ctx.log_pandas_metrics(\n            memory_mb=memory_mb,\n            dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n        )\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            ctx.debug(\"Writing to SQL\", table=table, mode=mode)\n            return self._write_sql(df, connection, table, mode, options)\n\n        # Resolve full path from connection\n        try:\n            full_path = self._resolve_path(path or table, connection)\n        except ValueError:\n            if table and not connection:\n                ctx.error(\"Connection required when specifying 'table'\", table=table)\n                raise ValueError(\"Connection is required when specifying 'table'.\")\n            ctx.error(\"Neither path nor table provided for write operation\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Merge storage options for cloud connections\n        merged_options = self._merge_storage_options(connection, options)\n\n        # Custom Writers\n        if format in self._custom_writers:\n            ctx.debug(f\"Using custom writer for format: {format}\")\n            writer_options = merged_options.copy()\n            writer_options.pop(\"keys\", None)\n            self._custom_writers[format](df, full_path, mode=mode, **writer_options)\n            return None\n\n        # Ensure directory exists (local only)\n        self._ensure_directory(full_path)\n\n        # Warn about partitioning\n        self._check_partitioning(merged_options)\n\n        # Delta Lake Write\n        if format == \"delta\":\n            ctx.debug(\"Writing Delta table\", path=str(full_path), mode=mode)\n            result = self._write_delta(df, full_path, mode, merged_options)\n            elapsed = (time.time() - start) * 1000\n            ctx.log_file_io(\n                path=str(full_path),\n                format=format,\n                mode=mode,\n                rows=row_count,\n            )\n            ctx.info(\n                \"Write completed\",\n                format=format,\n                rows=row_count,\n                elapsed_ms=round(elapsed, 2),\n            )\n            return result\n\n        # Handle Generic Upsert/Append-Once for non-Delta\n        if mode in [\"upsert\", \"append_once\"]:\n            ctx.debug(f\"Handling {mode} mode for non-Delta format\")\n            df, mode = self._handle_generic_upsert(df, full_path, format, mode, merged_options)\n            row_count = len(df)\n\n        # Standard File Write\n        result = self._write_file(df, full_path, format, mode, merged_options)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.log_file_io(\n            path=str(full_path),\n            format=format,\n            mode=mode,\n            rows=row_count,\n        )\n        ctx.info(\n            \"Write completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return result\n\n    def _write_iterator(\n        self,\n        df_iter: Iterator[pd.DataFrame],\n        connection: Any,\n        format: str,\n        table: Optional[str],\n        path: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle writing of iterator/generator.\"\"\"\n        first_chunk = True\n        for chunk in df_iter:\n            # Determine mode for this chunk\n            current_mode = mode if first_chunk else \"append\"\n            current_options = options.copy()\n\n            # Handle CSV header for chunks\n            if not first_chunk and format == \"csv\":\n                if current_options.get(\"header\") is not False:\n                    current_options[\"header\"] = False\n\n            self.write(\n                chunk,\n                connection,\n                format,\n                table,\n                path,\n                mode=current_mode,\n                options=current_options,\n            )\n            first_chunk = False\n        return None\n\n    def _write_sql(\n        self,\n        df: pd.DataFrame,\n        connection: Any,\n        table: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Handle SQL writing including merge and enhanced overwrite.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        if not hasattr(connection, \"write_table\"):\n            raise ValueError(\n                f\"Connection type '{type(connection).__name__}' does not support SQL operations\"\n            )\n\n        if not table:\n            raise ValueError(\"SQL format requires 'table' config\")\n\n        # Handle MERGE mode for SQL Server\n        if mode == \"merge\":\n            merge_keys = options.get(\"merge_keys\")\n            merge_options = options.get(\"merge_options\")\n\n            if not merge_keys:\n                raise ValueError(\n                    \"MERGE mode requires 'merge_keys' in options. \"\n                    \"Specify the key columns for the MERGE ON clause.\"\n                )\n\n            from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n            writer = SqlServerMergeWriter(connection)\n            ctx.debug(\n                \"Executing SQL Server MERGE (Pandas)\",\n                target=table,\n                merge_keys=merge_keys,\n            )\n\n            result = writer.merge_pandas(\n                df=df,\n                target_table=table,\n                merge_keys=merge_keys,\n                options=merge_options,\n            )\n\n            ctx.info(\n                \"SQL Server MERGE completed (Pandas)\",\n                target=table,\n                inserted=result.inserted,\n                updated=result.updated,\n                deleted=result.deleted,\n            )\n\n            return {\n                \"mode\": \"merge\",\n                \"inserted\": result.inserted,\n                \"updated\": result.updated,\n                \"deleted\": result.deleted,\n                \"total_affected\": result.total_affected,\n            }\n\n        # Handle enhanced overwrite with strategies\n        if mode == \"overwrite\" and options.get(\"overwrite_options\"):\n            from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n            overwrite_options = options.get(\"overwrite_options\")\n            writer = SqlServerMergeWriter(connection)\n\n            ctx.debug(\n                \"Executing SQL Server enhanced overwrite (Pandas)\",\n                target=table,\n                strategy=(\n                    overwrite_options.strategy.value\n                    if hasattr(overwrite_options, \"strategy\")\n                    else \"truncate_insert\"\n                ),\n            )\n\n            result = writer.overwrite_pandas(\n                df=df,\n                target_table=table,\n                options=overwrite_options,\n            )\n\n            ctx.info(\n                \"SQL Server enhanced overwrite completed (Pandas)\",\n                target=table,\n                strategy=result.strategy,\n                rows_written=result.rows_written,\n            )\n\n            return {\n                \"mode\": \"overwrite\",\n                \"strategy\": result.strategy,\n                \"rows_written\": result.rows_written,\n            }\n\n        # Extract schema from table name if present\n        if \".\" in table:\n            schema, table_name = table.split(\".\", 1)\n        else:\n            schema, table_name = \"dbo\", table\n\n        # Map mode to if_exists\n        if_exists = \"replace\"  # overwrite\n        if mode == \"append\":\n            if_exists = \"append\"\n        elif mode == \"fail\":\n            if_exists = \"fail\"\n\n        chunksize = options.get(\"chunksize\", 1000)\n\n        connection.write_table(\n            df=df,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n        return None\n\n    def _ensure_directory(self, full_path: str) -&gt; None:\n        \"\"\"Ensure parent directory exists for local files.\"\"\"\n        parsed = urlparse(str(full_path))\n        is_windows_drive = (\n            len(parsed.scheme) == 1 and parsed.scheme.isalpha() if parsed.scheme else False\n        )\n\n        if not parsed.scheme or parsed.scheme == \"file\" or is_windows_drive:\n            Path(full_path).parent.mkdir(parents=True, exist_ok=True)\n\n    def _check_partitioning(self, options: Dict[str, Any]) -&gt; None:\n        \"\"\"Warn about potential partitioning issues.\"\"\"\n        partition_by = options.get(\"partition_by\") or options.get(\"partitionBy\")\n        if partition_by:\n            import warnings\n\n            warnings.warn(\n                \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n                \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n                \"and ensure each partition has &gt; 1000 rows.\",\n                UserWarning,\n            )\n\n    def _write_delta(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        mode: str,\n        merged_options: Dict[str, Any],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Handle Delta Lake writing.\"\"\"\n        try:\n            from deltalake import DeltaTable, write_deltalake\n        except ImportError:\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' or 'pip install deltalake'. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        storage_opts = merged_options.get(\"storage_options\", {})\n\n        # Handle null-only columns: Delta Lake doesn't support Null dtype\n        # Cast columns with all-null values to string to avoid schema errors\n        for col in df.columns:\n            if df[col].isna().all():\n                df[col] = df[col].astype(\"string\")\n\n        # Map modes\n        delta_mode = \"overwrite\"\n        if mode == \"append\":\n            delta_mode = \"append\"\n        elif mode == \"error\" or mode == \"fail\":\n            delta_mode = \"error\"\n        elif mode == \"ignore\":\n            delta_mode = \"ignore\"\n\n        # Handle upsert/append_once logic\n        if mode == \"upsert\":\n            keys = merged_options.get(\"keys\")\n            if not keys:\n                raise ValueError(\"Upsert requires 'keys' in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            def do_upsert():\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                (\n                    dt.merge(\n                        source=df,\n                        predicate=\" AND \".join([f\"s.{k} = t.{k}\" for k in keys]),\n                        source_alias=\"s\",\n                        target_alias=\"t\",\n                    )\n                    .when_matched_update_all()\n                    .when_not_matched_insert_all()\n                    .execute()\n                )\n\n            self._retry_delta_operation(do_upsert)\n        elif mode == \"append_once\":\n            keys = merged_options.get(\"keys\")\n            if not keys:\n                raise ValueError(\"Append_once requires 'keys' in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            def do_append_once():\n                dt = DeltaTable(full_path, storage_options=storage_opts)\n                (\n                    dt.merge(\n                        source=df,\n                        predicate=\" AND \".join([f\"s.{k} = t.{k}\" for k in keys]),\n                        source_alias=\"s\",\n                        target_alias=\"t\",\n                    )\n                    .when_not_matched_insert_all()\n                    .execute()\n                )\n\n            self._retry_delta_operation(do_append_once)\n        else:\n            # Filter options supported by write_deltalake\n            write_kwargs = {\n                k: v\n                for k, v in merged_options.items()\n                if k\n                in [\n                    \"partition_by\",\n                    \"mode\",\n                    \"overwrite_schema\",\n                    \"schema_mode\",\n                    \"name\",\n                    \"description\",\n                    \"configuration\",\n                    \"writer_properties\",\n                ]\n            }\n\n            def do_write():\n                write_deltalake(\n                    full_path,\n                    df,\n                    mode=delta_mode,\n                    storage_options=storage_opts,\n                    engine=\"rust\",\n                    **write_kwargs,\n                )\n\n            self._retry_delta_operation(do_write)\n\n        # Return commit info\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=1)\n        latest = history[0]\n\n        return {\n            \"version\": dt.version(),\n            \"timestamp\": datetime.fromtimestamp(latest.get(\"timestamp\", 0) / 1000),\n            \"operation\": latest.get(\"operation\"),\n            \"operation_metrics\": latest.get(\"operationMetrics\", {}),\n            \"read_version\": latest.get(\"readVersion\"),\n        }\n\n    def _handle_generic_upsert(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        format: str,\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; tuple[pd.DataFrame, str]:\n        \"\"\"Handle upsert/append_once for standard files by merging with existing data.\"\"\"\n        if \"keys\" not in options:\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        keys = options[\"keys\"]\n        if isinstance(keys, str):\n            keys = [keys]\n\n        # Try to read existing file\n        existing_df = None\n        try:\n            read_opts = options.copy()\n            read_opts.pop(\"keys\", None)\n\n            if format == \"csv\":\n                existing_df = pd.read_csv(full_path, **read_opts)\n            elif format == \"parquet\":\n                existing_df = pd.read_parquet(full_path, **read_opts)\n            elif format == \"json\":\n                existing_df = pd.read_json(full_path, **read_opts)\n            elif format == \"excel\":\n                existing_df = pd.read_excel(full_path, **read_opts)\n        except Exception:\n            # File doesn't exist or can't be read\n            return df, \"overwrite\"  # Treat as new write\n\n        if existing_df is None:\n            return df, \"overwrite\"\n\n        if mode == \"append_once\":\n            # Check if keys exist\n            missing_keys = set(keys) - set(df.columns)\n            if missing_keys:\n                raise KeyError(f\"Keys {missing_keys} not found in input data\")\n\n            # Identify new rows\n            merged = df.merge(existing_df[keys], on=keys, how=\"left\", indicator=True)\n            new_rows = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n            if format in [\"csv\", \"json\"]:\n                return new_rows, \"append\"\n            else:\n                # Rewrite everything\n                return pd.concat([existing_df, new_rows], ignore_index=True), \"overwrite\"\n\n        elif mode == \"upsert\":\n            # Check if keys exist\n            missing_keys = set(keys) - set(df.columns)\n            if missing_keys:\n                raise KeyError(f\"Keys {missing_keys} not found in input data\")\n\n            # 1. Remove rows from existing that are in input\n            merged_indicator = existing_df.merge(df[keys], on=keys, how=\"left\", indicator=True)\n            rows_to_keep = existing_df[merged_indicator[\"_merge\"] == \"left_only\"]\n\n            # 2. Concat rows_to_keep + input df\n            # 3. Write mode becomes overwrite\n            return pd.concat([rows_to_keep, df], ignore_index=True), \"overwrite\"\n\n        return df, mode\n\n    def _write_file(\n        self,\n        df: pd.DataFrame,\n        full_path: str,\n        format: str,\n        mode: str,\n        merged_options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Handle standard file writing (CSV, Parquet, etc.).\"\"\"\n        writer_options = merged_options.copy()\n        writer_options.pop(\"keys\", None)\n\n        # Remove storage_options for local pandas writers usually?\n        # Some pandas writers accept storage_options (parquet, csv with fsspec)\n\n        if format == \"csv\":\n            mode_param = \"w\"\n            if mode == \"append\":\n                mode_param = \"a\"\n                if not os.path.exists(full_path):\n                    # If file doesn't exist, include header\n                    writer_options[\"header\"] = True\n                else:\n                    # If appending, don't write header unless explicit\n                    if \"header\" not in writer_options:\n                        writer_options[\"header\"] = False\n\n            df.to_csv(full_path, index=False, mode=mode_param, **writer_options)\n\n        elif format == \"parquet\":\n            if mode == \"append\":\n                # Pandas read_parquet doesn't support append directly usually.\n                # We implement simple read-concat-write for local files\n                if os.path.exists(full_path):\n                    existing = pd.read_parquet(full_path, **merged_options)\n                    df = pd.concat([existing, df], ignore_index=True)\n\n            df.to_parquet(full_path, index=False, **writer_options)\n\n        elif format == \"json\":\n            if mode == \"append\":\n                writer_options[\"mode\"] = \"a\"\n\n            # Default to records if not specified\n            if \"orient\" not in writer_options:\n                writer_options[\"orient\"] = \"records\"\n\n            # Include storage_options for cloud storage (ADLS, S3, GCS)\n            if \"storage_options\" in merged_options:\n                writer_options[\"storage_options\"] = merged_options[\"storage_options\"]\n\n            df.to_json(full_path, **writer_options)\n\n        elif format == \"excel\":\n            if mode == \"append\":\n                # Simple append for excel\n                if os.path.exists(full_path):\n                    with pd.ExcelWriter(full_path, mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n                        df.to_excel(writer, index=False, **writer_options)\n                    return\n\n            df.to_excel(full_path, index=False, **writer_options)\n\n        elif format == \"avro\":\n            try:\n                import fastavro\n            except ImportError:\n                raise ImportError(\"Avro support requires 'pip install fastavro'\")\n\n            # Convert datetime columns to microseconds for Avro timestamp-micros\n            df_avro = df.copy()\n            for col in df_avro.columns:\n                if pd.api.types.is_datetime64_any_dtype(df_avro[col].dtype):\n                    df_avro[col] = df_avro[col].apply(\n                        lambda x: int(x.timestamp() * 1_000_000) if pd.notna(x) else None\n                    )\n\n            records = df_avro.to_dict(\"records\")\n            schema = self._infer_avro_schema(df)\n\n            # Use fsspec for remote URIs (abfss://, s3://, etc.)\n            parsed = urlparse(full_path)\n            if parsed.scheme and parsed.scheme not in [\"file\", \"\"]:\n                # Remote file - use fsspec\n                import fsspec\n\n                storage_opts = merged_options.get(\"storage_options\", {})\n                write_mode = \"wb\" if mode == \"overwrite\" else \"ab\"\n                with fsspec.open(full_path, write_mode, **storage_opts) as f:\n                    fastavro.writer(f, schema, records)\n            else:\n                # Local file - use standard open\n                open_mode = \"wb\"\n                if mode == \"append\" and os.path.exists(full_path):\n                    open_mode = \"a+b\"\n\n                with open(full_path, open_mode) as f:\n                    fastavro.writer(f, schema, records)\n        else:\n            raise ValueError(f\"Unsupported format for Pandas engine: {format}\")\n\n    def add_write_metadata(\n        self,\n        df: pd.DataFrame,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: Pandas DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added\n        \"\"\"\n        from odibi.config import WriteMetadataConfig\n\n        # Normalize config: True -&gt; all defaults\n        if metadata_config is True:\n            config = WriteMetadataConfig()\n        elif isinstance(metadata_config, WriteMetadataConfig):\n            config = metadata_config\n        else:\n            return df  # None or invalid -&gt; no metadata\n\n        # Work on a copy to avoid modifying original\n        df = df.copy()\n\n        # _extracted_at: always applicable\n        if config.extracted_at:\n            df[\"_extracted_at\"] = pd.Timestamp.now()\n\n        # _source_file: only for file sources\n        if config.source_file and is_file_source and source_path:\n            df[\"_source_file\"] = source_path\n\n        # _source_connection: all sources\n        if config.source_connection and source_connection:\n            df[\"_source_connection\"] = source_connection\n\n        # _source_table: SQL sources only\n        if config.source_table and source_table:\n            df[\"_source_table\"] = source_table\n\n        return df\n\n    def _register_lazy_view_unused(self, conn, name: str, df: Any) -&gt; None:\n        \"\"\"Register a LazyDataset as a DuckDB view.\"\"\"\n        duck_fmt = df.format\n        if duck_fmt == \"json\":\n            duck_fmt = \"json_auto\"\n\n        if isinstance(df.path, list):\n            paths = \", \".join([f\"'{p}'\" for p in df.path])\n            conn.execute(\n                f\"CREATE OR REPLACE VIEW {name} AS SELECT * FROM read_{duck_fmt}([{paths}])\"\n            )\n        else:\n            conn.execute(\n                f\"CREATE OR REPLACE VIEW {name} AS SELECT * FROM read_{duck_fmt}('{df.path}')\"\n            )\n\n    def execute_sql(self, sql: str, context: Context) -&gt; pd.DataFrame:\n        \"\"\"Execute SQL query using DuckDB (if available) or pandasql.\n\n        Args:\n            sql: SQL query string\n            context: Execution context\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        if not isinstance(context, PandasContext):\n            raise TypeError(\"PandasEngine requires PandasContext\")\n\n        # Try to use DuckDB for SQL\n        try:\n            import duckdb\n\n            # Create in-memory database\n            conn = duckdb.connect(\":memory:\")\n\n            # Register all DataFrames from context\n            for name in context.list_names():\n                dataset_obj = context.get(name)\n\n                # Debug check\n                # print(f\"DEBUG: Registering {name} type={type(dataset_obj)} LazyDataset={LazyDataset}\")\n\n                # Handle LazyDataset (DuckDB optimization)\n                # if isinstance(dataset_obj, LazyDataset):\n                #     self._register_lazy_view(conn, name, dataset_obj)\n                #     # Log that we used DuckDB on file\n                #     # logger.info(f\"Executing SQL via DuckDB on lazy file: {dataset_obj.path}\")\n                #     continue\n\n                # Handle chunked data (Iterator)\n                from collections.abc import Iterator\n\n                if isinstance(dataset_obj, Iterator):\n                    # Warning: Materializing iterator for SQL execution\n                    # Note: DuckDB doesn't support streaming from iterator yet\n                    dataset_obj = pd.concat(dataset_obj, ignore_index=True)\n\n                conn.register(name, dataset_obj)\n\n            # Execute query\n            result = conn.execute(sql).df()\n            conn.close()\n\n            return result\n\n        except ImportError:\n            # Fallback: try pandasql\n            try:\n                from pandasql import sqldf\n\n                # Build local namespace with DataFrames\n                locals_dict = {}\n                for name in context.list_names():\n                    df = context.get(name)\n\n                    # Handle chunked data (Iterator)\n                    from collections.abc import Iterator\n\n                    if isinstance(df, Iterator):\n                        df = pd.concat(df, ignore_index=True)\n\n                    locals_dict[name] = df\n\n                return sqldf(sql, locals_dict)\n\n            except ImportError:\n                raise TransformError(\n                    \"SQL execution requires 'duckdb' or 'pandasql'. \"\n                    \"Install with: pip install duckdb\"\n                )\n\n    def execute_operation(\n        self,\n        operation: str,\n        params: Dict[str, Any],\n        df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute built-in operation.\n\n        Args:\n            operation: Operation name\n            params: Operation parameters\n            df: Input DataFrame or Iterator\n\n        Returns:\n            Result DataFrame\n        \"\"\"\n        # Materialize LazyDataset\n        df = self.materialize(df)\n\n        # Handle chunked data (Iterator)\n        from collections.abc import Iterator\n\n        if isinstance(df, Iterator):\n            # Warning: Materializing iterator for operation execution\n            df = pd.concat(df, ignore_index=True)\n\n        if operation == \"pivot\":\n            return self._pivot(df, params)\n        elif operation == \"drop_duplicates\":\n            return df.drop_duplicates(**params)\n        elif operation == \"fillna\":\n            return df.fillna(**params)\n        elif operation == \"drop\":\n            return df.drop(**params)\n        elif operation == \"rename\":\n            return df.rename(**params)\n        elif operation == \"sort\":\n            return df.sort_values(**params)\n        elif operation == \"sample\":\n            return df.sample(**params)\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext, PandasContext\n            from odibi.registry import FunctionRegistry\n\n            if FunctionRegistry.has_function(operation):\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df\n                engine_ctx = EngineContext(\n                    context=PandasContext(),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n            raise ValueError(f\"Unsupported operation: {operation}\")\n\n    def _pivot(self, df: pd.DataFrame, params: Dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"Execute pivot operation.\n\n        Args:\n            df: Input DataFrame\n            params: Pivot parameters\n\n        Returns:\n            Pivoted DataFrame\n        \"\"\"\n        group_by = params.get(\"group_by\", [])\n        pivot_column = params[\"pivot_column\"]\n        value_column = params[\"value_column\"]\n        agg_func = params.get(\"agg_func\", \"first\")\n\n        # Validate columns exist\n        required_columns = set()\n        if isinstance(group_by, list):\n            required_columns.update(group_by)\n        elif isinstance(group_by, str):\n            required_columns.add(group_by)\n            group_by = [group_by]\n\n        required_columns.add(pivot_column)\n        required_columns.add(value_column)\n\n        missing = required_columns - set(df.columns)\n        if missing:\n            raise KeyError(\n                f\"Columns not found in DataFrame for pivot operation: {missing}. \"\n                f\"Available: {list(df.columns)}\"\n            )\n\n        result = df.pivot_table(\n            index=group_by, columns=pivot_column, values=value_column, aggfunc=agg_func\n        ).reset_index()\n\n        # Flatten column names if multi-level\n        if isinstance(result.columns, pd.MultiIndex):\n            result.columns = [\"_\".join(col).strip(\"_\") for col in result.columns.values]\n\n        return result\n\n    def harmonize_schema(\n        self, df: pd.DataFrame, target_schema: Dict[str, str], policy: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        target_cols = list(target_schema.keys())\n        current_cols = df.columns.tolist()\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        # 1. Check Validations\n        if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # 2. Apply Transformations\n        if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n            # Evolve: Add missing columns, Keep new columns\n            for col in missing:\n                df[col] = None\n        else:\n            # Enforce / Ignore New: Project to target schema (Drops new, Adds missing)\n            # Note: reindex adds NaN for missing columns\n            df = df.reindex(columns=target_cols)\n\n        return df\n\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Anonymize specified columns.\"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        res = df.copy()\n\n        for col in columns:\n            if col not in res.columns:\n                continue\n\n            if method == \"hash\":\n                # Vectorized Hashing (via map/apply)\n                # Note: True vectorization requires C-level support (e.g. pyarrow.compute)\n                # Standard Pandas apply is the fallback but we can optimize string handling\n\n                # Convert to string, handling nulls\n                # s_col = res[col].astype(str)\n                # Nulls become 'nan'/'None' string, we want to preserve them or hash them consistently?\n                # Typically nulls should remain null.\n\n                mask_nulls = res[col].isna()\n\n                def _hash_val(val):\n                    to_hash = val\n                    if salt:\n                        to_hash += salt\n                    return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n                # Apply only to non-nulls\n                res.loc[~mask_nulls, col] = res.loc[~mask_nulls, col].astype(str).apply(_hash_val)\n\n            elif method == \"mask\":\n                # Vectorized Masking\n                # Mask all but last 4 characters\n\n                mask_nulls = res[col].isna()\n                s_valid = res.loc[~mask_nulls, col].astype(str)\n\n                # Use vectorized regex replacement\n                # Replace any character that is followed by 4 characters with '*'\n                res.loc[~mask_nulls, col] = s_valid.str.replace(r\".(?=.{4})\", \"*\", regex=True)\n\n            elif method == \"redact\":\n                res[col] = \"[REDACTED]\"\n\n        return res\n\n    def get_schema(self, df: Any) -&gt; Dict[str, str]:\n        \"\"\"Get DataFrame schema with types.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Dict[str, str]: Column name -&gt; Type string\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res = conn.execute(\"DESCRIBE SELECT * FROM df\").df()\n                    return dict(zip(res[\"column_name\"], res[\"column_type\"]))\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return {col: str(df[col].dtype) for col in df.columns}\n\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            (rows, columns)\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            cols = len(self.get_schema(df))\n            rows = self.count_rows(df)\n            return (rows, cols)\n        return df.shape\n\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            Row count\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res = conn.execute(\"SELECT count(*) FROM df\").fetchone()\n                    return res[0] if res else 0\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return len(df)\n\n    def count_nulls(self, df: pd.DataFrame, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\n\n        Args:\n            df: DataFrame\n            columns: Columns to check\n\n        Returns:\n            Dictionary of column -&gt; null count\n        \"\"\"\n        null_counts = {}\n        for col in columns:\n            if col in df.columns:\n                null_counts[col] = int(df[col].isna().sum())\n            else:\n                raise ValueError(\n                    f\"Column '{col}' not found in DataFrame. Available columns: {list(df.columns)}\"\n                )\n        return null_counts\n\n    def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\n\n        Args:\n            df: DataFrame\n            schema_rules: Validation rules\n\n        Returns:\n            List of validation failures\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        failures = []\n\n        # Check required columns\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(df.columns)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        # Check column types\n        if \"types\" in schema_rules:\n            type_map = {\n                \"int\": [\"int64\", \"int32\", \"int16\", \"int8\"],\n                \"float\": [\"float64\", \"float32\"],\n                \"str\": [\"object\", \"string\"],\n                \"bool\": [\"bool\"],\n            }\n\n            for col, expected_type in schema_rules[\"types\"].items():\n                if col not in df.columns:\n                    failures.append(f\"Column '{col}' not found for type validation\")\n                    continue\n\n                actual_type = str(df[col].dtype)\n                # Handle pyarrow types (e.g. int64[pyarrow])\n                if \"[\" in actual_type and \"pyarrow\" in actual_type:\n                    actual_type = actual_type.split(\"[\")[0]\n\n                expected_dtypes = type_map.get(expected_type, [expected_type])\n\n                if actual_type not in expected_dtypes:\n                    failures.append(\n                        f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def _infer_avro_schema(self, df: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Infer Avro schema from pandas DataFrame.\n\n        Args:\n            df: DataFrame to infer schema from\n\n        Returns:\n            Avro schema dictionary\n        \"\"\"\n        type_mapping = {\n            \"int64\": \"long\",\n            \"int32\": \"int\",\n            \"float64\": \"double\",\n            \"float32\": \"float\",\n            \"bool\": \"boolean\",\n            \"object\": \"string\",\n            \"string\": \"string\",\n        }\n\n        fields = []\n        for col in df.columns:\n            dtype = df[col].dtype\n            dtype_str = str(dtype)\n\n            # Handle datetime types with Avro logical types\n            if pd.api.types.is_datetime64_any_dtype(dtype):\n                avro_type = {\n                    \"type\": \"long\",\n                    \"logicalType\": \"timestamp-micros\",\n                }\n            elif dtype_str == \"date\" or (hasattr(dtype, \"name\") and \"date\" in dtype.name.lower()):\n                avro_type = {\n                    \"type\": \"int\",\n                    \"logicalType\": \"date\",\n                }\n            elif pd.api.types.is_timedelta64_dtype(dtype):\n                avro_type = {\n                    \"type\": \"long\",\n                    \"logicalType\": \"time-micros\",\n                }\n            else:\n                avro_type = type_mapping.get(dtype_str, \"string\")\n\n            # Handle nullable columns\n            if df[col].isnull().any():\n                avro_type = [\"null\", avro_type]\n\n            fields.append({\"name\": col, \"type\": avro_type})\n\n        return {\"type\": \"record\", \"name\": \"DataFrame\", \"fields\": fields}\n\n    def validate_data(self, df: pd.DataFrame, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate DataFrame against rules.\n\n        Args:\n            df: DataFrame\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        failures = []\n\n        # Check not empty\n        if validation_config.not_empty:\n            if len(df) == 0:\n                failures.append(\"DataFrame is empty\")\n\n        # Check for nulls in specified columns\n        if validation_config.no_nulls:\n            null_counts = self.count_nulls(df, validation_config.no_nulls)\n            for col, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col}' has {count} null values\")\n\n        # Schema validation\n        if validation_config.schema_validation:\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        # Range validation\n        if validation_config.ranges:\n            for col, bounds in validation_config.ranges.items():\n                if col in df.columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    if min_val is not None:\n                        min_violations = df[df[col] &lt; min_val]\n                        if len(min_violations) &gt; 0:\n                            failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        max_violations = df[df[col] &gt; max_val]\n                        if len(max_violations) &gt; 0:\n                            failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col}' not found for range validation\")\n\n        # Allowed values validation\n        if validation_config.allowed_values:\n            for col, allowed in validation_config.allowed_values.items():\n                if col in df.columns:\n                    # Check for values not in allowed list\n                    invalid = df[~df[col].isin(allowed)]\n                    if len(invalid) &gt; 0:\n                        failures.append(f\"Column '{col}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n        return failures\n\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\n\n        Args:\n            df: DataFrame or LazyDataset\n            n: Number of rows to return\n\n        Returns:\n            List of row dictionaries\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if self.use_duckdb:\n                try:\n                    import duckdb\n\n                    conn = duckdb.connect(\":memory:\")\n                    self._register_lazy_view(conn, \"df\", df)\n                    res_df = conn.execute(f\"SELECT * FROM df LIMIT {n}\").df()\n                    return res_df.to_dict(\"records\")\n                except Exception:\n                    pass\n            df = self.materialize(df)\n\n        return df.head(n).to_dict(\"records\")\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Args:\n            connection: Connection object\n            table: Table name (not used in Pandas\u2014no catalog)\n            path: File path\n\n        Returns:\n            True if file/directory exists, False otherwise\n        \"\"\"\n        if path:\n            full_path = connection.get_path(path)\n            return os.path.exists(full_path)\n        return False\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\"\"\"\n        try:\n            if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n                # SQL Server: Read empty result\n                query = f\"SELECT TOP 0 * FROM {table}\"\n                df = connection.read_sql(query)\n                return self.get_schema(df)\n\n            if path:\n                full_path = connection.get_path(path)\n                if not os.path.exists(full_path):\n                    return None\n\n                if format == \"delta\":\n                    from deltalake import DeltaTable\n\n                    dt = DeltaTable(full_path)\n                    # Use pyarrow schema to pandas schema to avoid reading data\n                    arrow_schema = dt.schema().to_pyarrow()\n                    empty_df = arrow_schema.empty_table().to_pandas()\n                    return self.get_schema(empty_df)\n\n                elif format == \"parquet\":\n                    import pyarrow.parquet as pq\n\n                    target_path = full_path\n                    if os.path.isdir(full_path):\n                        # Find first parquet file\n                        files = glob.glob(os.path.join(full_path, \"*.parquet\"))\n                        if not files:\n                            return None\n                        target_path = files[0]\n\n                    schema = pq.read_schema(target_path)\n                    empty_df = schema.empty_table().to_pandas()\n                    return self.get_schema(empty_df)\n\n                elif format == \"csv\":\n                    df = pd.read_csv(full_path, nrows=0)\n                    return self.get_schema(df)\n\n        except (FileNotFoundError, PermissionError):\n            return None\n        except ImportError as e:\n            # Log missing optional dependency\n            import logging\n\n            logging.getLogger(__name__).warning(\n                f\"Could not infer schema due to missing dependency: {e}\"\n            )\n            return None\n        except Exception as e:\n            import logging\n\n            logging.getLogger(__name__).warning(f\"Failed to infer schema for {table or path}: {e}\")\n            return None\n        return None\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n        dry_run: bool = False,\n        enforce_retention_duration: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"VACUUM a Delta table to remove old files.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            retention_hours: Retention period (default 168 = 7 days)\n            dry_run: If True, only show files to be deleted\n            enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n        Returns:\n            Dictionary with files_deleted count\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n        )\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        deleted_files = dt.vacuum(\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n            enforce_retention_duration=enforce_retention_duration,\n        )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=str(full_path),\n            files_deleted=len(deleted_files),\n            dry_run=dry_run,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return {\"files_deleted\": len(deleted_files)}\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            limit: Maximum number of versions to return\n\n        Returns:\n            List of version metadata dictionaries\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=limit)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=str(full_path),\n            versions_returned=len(history) if history else 0,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n\n    def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n        \"\"\"Restore Delta table to a specific version.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            version: Version number to restore to\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n        start = time.time()\n\n        ctx.info(\"Starting Delta table restore\", path=path, target_version=version)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[pandas]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        dt.restore(version)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta table restored\",\n            path=str(full_path),\n            restored_to_version=version,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n        ctx = get_logging_context().with_context(engine=\"pandas\")\n\n        if format != \"delta\" or not config or not config.enabled:\n            return\n\n        if not path and not table:\n            return\n\n        full_path = connection.get_path(path if path else table)\n        start = time.time()\n\n        ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.warning(\n                \"Auto-optimize skipped: 'deltalake' library not installed\",\n                path=str(full_path),\n            )\n            return\n\n        try:\n            storage_opts = {}\n            if hasattr(connection, \"pandas_storage_options\"):\n                storage_opts = connection.pandas_storage_options()\n\n            dt = DeltaTable(full_path, storage_options=storage_opts)\n\n            ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n            dt.optimize.compact()\n\n            retention = config.vacuum_retention_hours\n            if retention is not None and retention &gt; 0:\n                ctx.info(\n                    \"Running Delta VACUUM\",\n                    path=str(full_path),\n                    retention_hours=retention,\n                )\n                dt.vacuum(\n                    retention_hours=retention,\n                    enforce_retention_duration=True,\n                    dry_run=False,\n                )\n\n            elapsed = (time.time() - start) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                path=str(full_path),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            ctx.warning(\n                \"Auto-optimize failed\",\n                path=str(full_path),\n                error=str(e),\n            )\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        Args:\n            df: DataFrame or LazyDataset\n\n        Returns:\n            List of file paths\n        \"\"\"\n        if isinstance(df, LazyDataset):\n            if isinstance(df.path, list):\n                return df.path\n            return [str(df.path)]\n\n        if hasattr(df, \"attrs\"):\n            return df.attrs.get(\"odibi_source_files\", [])\n        return []\n\n    def profile_nulls(self, df: pd.DataFrame) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\n\n        Args:\n            df: DataFrame\n\n        Returns:\n            Dictionary of {column_name: null_percentage}\n        \"\"\"\n        # Ensure materialization\n        df = self.materialize(df)\n\n        # mean() of boolean DataFrame gives the percentage of True values\n        return df.isna().mean().to_dict()\n\n    def filter_greater_than(self, df: pd.DataFrame, column: str, value: Any) -&gt; pd.DataFrame:\n        \"\"\"Filter DataFrame where column &gt; value.\n\n        Automatically casts string columns to datetime for proper comparison.\n        \"\"\"\n        if column not in df.columns:\n            raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n        try:\n            col_series = df[column]\n\n            if pd.api.types.is_string_dtype(col_series):\n                col_series = pd.to_datetime(col_series, errors=\"coerce\")\n            elif pd.api.types.is_datetime64_any_dtype(col_series) and isinstance(value, str):\n                value = pd.to_datetime(value)\n\n            return df[col_series &gt; value]\n        except Exception as e:\n            raise ValueError(f\"Failed to filter {column} &gt; {value}: {e}\")\n\n    def filter_coalesce(\n        self, df: pd.DataFrame, col1: str, col2: str, op: str, value: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"Filter using COALESCE(col1, col2) op value.\n\n        Automatically casts string columns to datetime for proper comparison.\n        \"\"\"\n        if col1 not in df.columns:\n            raise ValueError(f\"Column '{col1}' not found\")\n\n        def _to_datetime_if_string(series: pd.Series) -&gt; pd.Series:\n            if pd.api.types.is_string_dtype(series):\n                return pd.to_datetime(series, errors=\"coerce\")\n            return series\n\n        s1 = _to_datetime_if_string(df[col1])\n\n        if col2 not in df.columns:\n            s = s1\n        else:\n            s2 = _to_datetime_if_string(df[col2])\n            s = s1.combine_first(s2)\n\n        try:\n            if pd.api.types.is_datetime64_any_dtype(s) and isinstance(value, str):\n                value = pd.to_datetime(value)\n\n            if op == \"&gt;=\":\n                return df[s &gt;= value]\n            elif op == \"&gt;\":\n                return df[s &gt; value]\n            elif op == \"&lt;=\":\n                return df[s &lt;= value]\n            elif op == \"&lt;\":\n                return df[s &lt; value]\n            elif op == \"==\" or op == \"=\":\n                return df[s == value]\n            else:\n                raise ValueError(f\"Unsupported operator: {op}\")\n        except Exception as e:\n            raise ValueError(f\"Failed to filter COALESCE({col1}, {col2}) {op} {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.__init__","title":"<code>__init__(connections=None, config=None)</code>","text":"<p>Initialize Pandas engine.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Pandas engine.\n\n    Args:\n        connections: Dictionary of connection objects\n        config: Engine configuration (optional)\n    \"\"\"\n    self.connections = connections or {}\n    self.config = config or {}\n\n    # Suppress noisy delta-rs transaction conflict warnings (handled by retry)\n    if \"RUST_LOG\" not in os.environ:\n        os.environ[\"RUST_LOG\"] = \"deltalake_core::kernel::transaction=error\"\n\n    # Check for performance flags\n    performance = self.config.get(\"performance\", {})\n\n    # Determine desired state\n    if hasattr(performance, \"use_arrow\"):\n        desired_use_arrow = performance.use_arrow\n    elif isinstance(performance, dict):\n        desired_use_arrow = performance.get(\"use_arrow\", True)\n    else:\n        desired_use_arrow = True\n\n    # Verify availability\n    if desired_use_arrow:\n        try:\n            import pyarrow  # noqa: F401\n\n            self.use_arrow = True\n        except ImportError:\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"Apache Arrow not found. Disabling Arrow optimizations. \"\n                \"Install 'pyarrow' to enable.\"\n            )\n            self.use_arrow = False\n    else:\n        self.use_arrow = False\n\n    # Check for DuckDB\n    self.use_duckdb = False\n    # Default to False to ensure stability with existing tests (Lazy Loading is opt-in)\n    if self.config.get(\"performance\", {}).get(\"use_duckdb\", False):\n        try:\n            import duckdb  # noqa: F401\n\n            self.use_duckdb = True\n        except ImportError:\n            pass\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with metadata columns added</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: pd.DataFrame,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: Pandas DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added\n    \"\"\"\n    from odibi.config import WriteMetadataConfig\n\n    # Normalize config: True -&gt; all defaults\n    if metadata_config is True:\n        config = WriteMetadataConfig()\n    elif isinstance(metadata_config, WriteMetadataConfig):\n        config = metadata_config\n    else:\n        return df  # None or invalid -&gt; no metadata\n\n    # Work on a copy to avoid modifying original\n    df = df.copy()\n\n    # _extracted_at: always applicable\n    if config.extracted_at:\n        df[\"_extracted_at\"] = pd.Timestamp.now()\n\n    # _source_file: only for file sources\n    if config.source_file and is_file_source and source_path:\n        df[\"_source_file\"] = source_path\n\n    # _source_connection: all sources\n    if config.source_connection and source_connection:\n        df[\"_source_connection\"] = source_connection\n\n    # _source_table: SQL sources only\n    if config.source_table and source_table:\n        df[\"_source_table\"] = source_table\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize specified columns.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Anonymize specified columns.\"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    res = df.copy()\n\n    for col in columns:\n        if col not in res.columns:\n            continue\n\n        if method == \"hash\":\n            # Vectorized Hashing (via map/apply)\n            # Note: True vectorization requires C-level support (e.g. pyarrow.compute)\n            # Standard Pandas apply is the fallback but we can optimize string handling\n\n            # Convert to string, handling nulls\n            # s_col = res[col].astype(str)\n            # Nulls become 'nan'/'None' string, we want to preserve them or hash them consistently?\n            # Typically nulls should remain null.\n\n            mask_nulls = res[col].isna()\n\n            def _hash_val(val):\n                to_hash = val\n                if salt:\n                    to_hash += salt\n                return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n            # Apply only to non-nulls\n            res.loc[~mask_nulls, col] = res.loc[~mask_nulls, col].astype(str).apply(_hash_val)\n\n        elif method == \"mask\":\n            # Vectorized Masking\n            # Mask all but last 4 characters\n\n            mask_nulls = res[col].isna()\n            s_valid = res.loc[~mask_nulls, col].astype(str)\n\n            # Use vectorized regex replacement\n            # Replace any character that is followed by 4 characters with '*'\n            res.loc[~mask_nulls, col] = s_valid.str.replace(r\".(?=.{4})\", \"*\", regex=True)\n\n        elif method == \"redact\":\n            res[col] = \"[REDACTED]\"\n\n    return res\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>columns</code> <code>List[str]</code> <p>Columns to check</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary of column -&gt; null count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def count_nulls(self, df: pd.DataFrame, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\n\n    Args:\n        df: DataFrame\n        columns: Columns to check\n\n    Returns:\n        Dictionary of column -&gt; null count\n    \"\"\"\n    null_counts = {}\n    for col in columns:\n        if col in df.columns:\n            null_counts[col] = int(df[col].isna().sum())\n        else:\n            raise ValueError(\n                f\"Column '{col}' not found in DataFrame. Available columns: {list(df.columns)}\"\n            )\n    return null_counts\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>int</code> <p>Row count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Row count\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res = conn.execute(\"SELECT count(*) FROM df\").fetchone()\n                return res[0] if res else 0\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return len(df)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>Operation name</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Operation parameters</p> required <code>df</code> <code>Union[DataFrame, Iterator[DataFrame]]</code> <p>Input DataFrame or Iterator</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def execute_operation(\n    self,\n    operation: str,\n    params: Dict[str, Any],\n    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n) -&gt; pd.DataFrame:\n    \"\"\"Execute built-in operation.\n\n    Args:\n        operation: Operation name\n        params: Operation parameters\n        df: Input DataFrame or Iterator\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    # Materialize LazyDataset\n    df = self.materialize(df)\n\n    # Handle chunked data (Iterator)\n    from collections.abc import Iterator\n\n    if isinstance(df, Iterator):\n        # Warning: Materializing iterator for operation execution\n        df = pd.concat(df, ignore_index=True)\n\n    if operation == \"pivot\":\n        return self._pivot(df, params)\n    elif operation == \"drop_duplicates\":\n        return df.drop_duplicates(**params)\n    elif operation == \"fillna\":\n        return df.fillna(**params)\n    elif operation == \"drop\":\n        return df.drop(**params)\n    elif operation == \"rename\":\n        return df.rename(**params)\n    elif operation == \"sort\":\n        return df.sort_values(**params)\n    elif operation == \"sample\":\n        return df.sample(**params)\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext, PandasContext\n        from odibi.registry import FunctionRegistry\n\n        if FunctionRegistry.has_function(operation):\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df\n            engine_ctx = EngineContext(\n                context=PandasContext(),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n        raise ValueError(f\"Unsupported operation: {operation}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.execute_sql","title":"<code>execute_sql(sql, context)</code>","text":"<p>Execute SQL query using DuckDB (if available) or pandasql.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Context) -&gt; pd.DataFrame:\n    \"\"\"Execute SQL query using DuckDB (if available) or pandasql.\n\n    Args:\n        sql: SQL query string\n        context: Execution context\n\n    Returns:\n        Result DataFrame\n    \"\"\"\n    if not isinstance(context, PandasContext):\n        raise TypeError(\"PandasEngine requires PandasContext\")\n\n    # Try to use DuckDB for SQL\n    try:\n        import duckdb\n\n        # Create in-memory database\n        conn = duckdb.connect(\":memory:\")\n\n        # Register all DataFrames from context\n        for name in context.list_names():\n            dataset_obj = context.get(name)\n\n            # Debug check\n            # print(f\"DEBUG: Registering {name} type={type(dataset_obj)} LazyDataset={LazyDataset}\")\n\n            # Handle LazyDataset (DuckDB optimization)\n            # if isinstance(dataset_obj, LazyDataset):\n            #     self._register_lazy_view(conn, name, dataset_obj)\n            #     # Log that we used DuckDB on file\n            #     # logger.info(f\"Executing SQL via DuckDB on lazy file: {dataset_obj.path}\")\n            #     continue\n\n            # Handle chunked data (Iterator)\n            from collections.abc import Iterator\n\n            if isinstance(dataset_obj, Iterator):\n                # Warning: Materializing iterator for SQL execution\n                # Note: DuckDB doesn't support streaming from iterator yet\n                dataset_obj = pd.concat(dataset_obj, ignore_index=True)\n\n            conn.register(name, dataset_obj)\n\n        # Execute query\n        result = conn.execute(sql).df()\n        conn.close()\n\n        return result\n\n    except ImportError:\n        # Fallback: try pandasql\n        try:\n            from pandasql import sqldf\n\n            # Build local namespace with DataFrames\n            locals_dict = {}\n            for name in context.list_names():\n                df = context.get(name)\n\n                # Handle chunked data (Iterator)\n                from collections.abc import Iterator\n\n                if isinstance(df, Iterator):\n                    df = pd.concat(df, ignore_index=True)\n\n                locals_dict[name] = df\n\n            return sqldf(sql, locals_dict)\n\n        except ImportError:\n            raise TransformError(\n                \"SQL execution requires 'duckdb' or 'pandasql'. \"\n                \"Install with: pip install duckdb\"\n            )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.filter_coalesce","title":"<code>filter_coalesce(df, col1, col2, op, value)</code>","text":"<p>Filter using COALESCE(col1, col2) op value.</p> <p>Automatically casts string columns to datetime for proper comparison.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def filter_coalesce(\n    self, df: pd.DataFrame, col1: str, col2: str, op: str, value: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Filter using COALESCE(col1, col2) op value.\n\n    Automatically casts string columns to datetime for proper comparison.\n    \"\"\"\n    if col1 not in df.columns:\n        raise ValueError(f\"Column '{col1}' not found\")\n\n    def _to_datetime_if_string(series: pd.Series) -&gt; pd.Series:\n        if pd.api.types.is_string_dtype(series):\n            return pd.to_datetime(series, errors=\"coerce\")\n        return series\n\n    s1 = _to_datetime_if_string(df[col1])\n\n    if col2 not in df.columns:\n        s = s1\n    else:\n        s2 = _to_datetime_if_string(df[col2])\n        s = s1.combine_first(s2)\n\n    try:\n        if pd.api.types.is_datetime64_any_dtype(s) and isinstance(value, str):\n            value = pd.to_datetime(value)\n\n        if op == \"&gt;=\":\n            return df[s &gt;= value]\n        elif op == \"&gt;\":\n            return df[s &gt; value]\n        elif op == \"&lt;=\":\n            return df[s &lt;= value]\n        elif op == \"&lt;\":\n            return df[s &lt; value]\n        elif op == \"==\" or op == \"=\":\n            return df[s == value]\n        else:\n            raise ValueError(f\"Unsupported operator: {op}\")\n    except Exception as e:\n        raise ValueError(f\"Failed to filter COALESCE({col1}, {col2}) {op} {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.filter_greater_than","title":"<code>filter_greater_than(df, column, value)</code>","text":"<p>Filter DataFrame where column &gt; value.</p> <p>Automatically casts string columns to datetime for proper comparison.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def filter_greater_than(self, df: pd.DataFrame, column: str, value: Any) -&gt; pd.DataFrame:\n    \"\"\"Filter DataFrame where column &gt; value.\n\n    Automatically casts string columns to datetime for proper comparison.\n    \"\"\"\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n    try:\n        col_series = df[column]\n\n        if pd.api.types.is_string_dtype(col_series):\n            col_series = pd.to_datetime(col_series, errors=\"coerce\")\n        elif pd.api.types.is_datetime64_any_dtype(col_series) and isinstance(value, str):\n            value = pd.to_datetime(value)\n\n        return df[col_series &gt; value]\n    except Exception as e:\n        raise ValueError(f\"Failed to filter {column} &gt; {value}: {e}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>limit</code> <code>Optional[int]</code> <p>Maximum number of versions to return</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of version metadata dictionaries</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        limit: Maximum number of versions to return\n\n    Returns:\n        List of version metadata dictionaries\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    history = dt.history(limit=limit)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta history retrieved\",\n        path=str(full_path),\n        versions_returned=len(history) if history else 0,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return history\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <code>n</code> <code>int</code> <p>Number of rows to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of row dictionaries</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\n\n    Args:\n        df: DataFrame or LazyDataset\n        n: Number of rows to return\n\n    Returns:\n        List of row dictionaries\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res_df = conn.execute(f\"SELECT * FROM df LIMIT {n}\").df()\n                return res_df.to_dict(\"records\")\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return df.head(n).to_dict(\"records\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema with types.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Column name -&gt; Type string</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_schema(self, df: Any) -&gt; Dict[str, str]:\n    \"\"\"Get DataFrame schema with types.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        Dict[str, str]: Column name -&gt; Type string\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if self.use_duckdb:\n            try:\n                import duckdb\n\n                conn = duckdb.connect(\":memory:\")\n                self._register_lazy_view(conn, \"df\", df)\n                res = conn.execute(\"DESCRIBE SELECT * FROM df\").df()\n                return dict(zip(res[\"column_name\"], res[\"column_type\"]))\n            except Exception:\n                pass\n        df = self.materialize(df)\n\n    return {col: str(df[col].dtype) for col in df.columns}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(rows, columns)</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        (rows, columns)\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        cols = len(self.get_schema(df))\n        rows = self.count_rows(df)\n        return (rows, cols)\n    return df.shape\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyDataset</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    Args:\n        df: DataFrame or LazyDataset\n\n    Returns:\n        List of file paths\n    \"\"\"\n    if isinstance(df, LazyDataset):\n        if isinstance(df.path, list):\n            return df.path\n        return [str(df.path)]\n\n    if hasattr(df, \"attrs\"):\n        return df.attrs.get(\"odibi_source_files\", [])\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\"\"\"\n    try:\n        if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            # SQL Server: Read empty result\n            query = f\"SELECT TOP 0 * FROM {table}\"\n            df = connection.read_sql(query)\n            return self.get_schema(df)\n\n        if path:\n            full_path = connection.get_path(path)\n            if not os.path.exists(full_path):\n                return None\n\n            if format == \"delta\":\n                from deltalake import DeltaTable\n\n                dt = DeltaTable(full_path)\n                # Use pyarrow schema to pandas schema to avoid reading data\n                arrow_schema = dt.schema().to_pyarrow()\n                empty_df = arrow_schema.empty_table().to_pandas()\n                return self.get_schema(empty_df)\n\n            elif format == \"parquet\":\n                import pyarrow.parquet as pq\n\n                target_path = full_path\n                if os.path.isdir(full_path):\n                    # Find first parquet file\n                    files = glob.glob(os.path.join(full_path, \"*.parquet\"))\n                    if not files:\n                        return None\n                    target_path = files[0]\n\n                schema = pq.read_schema(target_path)\n                empty_df = schema.empty_table().to_pandas()\n                return self.get_schema(empty_df)\n\n            elif format == \"csv\":\n                df = pd.read_csv(full_path, nrows=0)\n                return self.get_schema(df)\n\n    except (FileNotFoundError, PermissionError):\n        return None\n    except ImportError as e:\n        # Log missing optional dependency\n        import logging\n\n        logging.getLogger(__name__).warning(\n            f\"Could not infer schema due to missing dependency: {e}\"\n        )\n        return None\n    except Exception as e:\n        import logging\n\n        logging.getLogger(__name__).warning(f\"Failed to infer schema for {table or path}: {e}\")\n        return None\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def harmonize_schema(\n    self, df: pd.DataFrame, target_schema: Dict[str, str], policy: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    target_cols = list(target_schema.keys())\n    current_cols = df.columns.tolist()\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    # 1. Check Validations\n    if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # 2. Apply Transformations\n    if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n        # Evolve: Add missing columns, Keep new columns\n        for col in missing:\n            df[col] = None\n    else:\n        # Enforce / Ignore New: Project to target schema (Drops new, Adds missing)\n        # Note: reindex adds NaN for missing columns\n        df = df.reindex(columns=target_cols)\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n\n    if format != \"delta\" or not config or not config.enabled:\n        return\n\n    if not path and not table:\n        return\n\n    full_path = connection.get_path(path if path else table)\n    start = time.time()\n\n    ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.warning(\n            \"Auto-optimize skipped: 'deltalake' library not installed\",\n            path=str(full_path),\n        )\n        return\n\n    try:\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n\n        ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n        dt.optimize.compact()\n\n        retention = config.vacuum_retention_hours\n        if retention is not None and retention &gt; 0:\n            ctx.info(\n                \"Running Delta VACUUM\",\n                path=str(full_path),\n                retention_hours=retention,\n            )\n            dt.vacuum(\n                retention_hours=retention,\n                enforce_retention_duration=True,\n                dry_run=False,\n            )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            path=str(full_path),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        ctx.warning(\n            \"Auto-optimize failed\",\n            path=str(full_path),\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset.\"\"\"\n    if isinstance(df, LazyDataset):\n        # Re-invoke read but force materialization (by bypassing Lazy check)\n        # We pass the resolved path directly\n        # Note: We need to handle the case where path was resolved.\n        # LazyDataset.path should be the FULL path.\n        return self._read_file(\n            full_path=df.path, format=df.format, options=df.options, connection=df.connection\n        )\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of {column_name: null_percentage}</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def profile_nulls(self, df: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        Dictionary of {column_name: null_percentage}\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    # mean() of boolean DataFrame gives the percentage of True values\n    return df.isna().mean().to_dict()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, as_of_version=None, as_of_timestamp=None)</code>","text":"<p>Read data using Pandas (or LazyDataset).</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    as_of_version: Optional[int] = None,\n    as_of_timestamp: Optional[str] = None,\n) -&gt; Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Read data using Pandas (or LazyDataset).\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    source = path or table\n    ctx.debug(\n        \"Starting read operation\",\n        format=format,\n        path=source,\n        streaming=streaming,\n        use_arrow=self.use_arrow,\n    )\n\n    if streaming:\n        ctx.error(\n            \"Streaming not supported in Pandas engine\",\n            format=format,\n            path=source,\n        )\n        raise ValueError(\n            \"Streaming is not supported in the Pandas engine. \"\n            \"Please use 'engine: spark' for streaming pipelines.\"\n        )\n\n    options = options or {}\n\n    # Resolve full path from connection\n    try:\n        full_path = self._resolve_path(path or table, connection)\n    except ValueError:\n        if table and not connection:\n            ctx.error(\"Connection required when specifying 'table'\", table=table)\n            raise ValueError(\n                f\"Cannot read table '{table}': connection is required when using 'table' parameter. \"\n                \"Provide a valid connection object or use 'path' for file-based reads.\"\n            )\n        ctx.error(\"Neither path nor table provided for read operation\")\n        raise ValueError(\n            \"Read operation failed: neither 'path' nor 'table' was provided. \"\n            \"Specify a file path or table name in your configuration.\"\n        )\n\n    # Merge storage options for cloud connections\n    merged_options = self._merge_storage_options(connection, options)\n\n    # Sanitize options for pandas compatibility\n    if \"header\" in merged_options:\n        if merged_options[\"header\"] is True:\n            merged_options[\"header\"] = 0\n        elif merged_options[\"header\"] is False:\n            merged_options[\"header\"] = None\n\n    # Handle Time Travel options\n    if as_of_version is not None:\n        merged_options[\"versionAsOf\"] = as_of_version\n        ctx.debug(\"Time travel enabled\", version=as_of_version)\n    if as_of_timestamp is not None:\n        merged_options[\"timestampAsOf\"] = as_of_timestamp\n        ctx.debug(\"Time travel enabled\", timestamp=as_of_timestamp)\n\n    # Check for Lazy/DuckDB optimization\n    can_lazy_load = False\n\n    if can_lazy_load:\n        ctx.debug(\"Using lazy loading via DuckDB\", path=str(full_path))\n        if isinstance(full_path, (str, Path)):\n            return LazyDataset(\n                path=str(full_path),\n                format=format,\n                options=merged_options,\n                connection=connection,\n            )\n        elif isinstance(full_path, list):\n            return LazyDataset(\n                path=full_path, format=format, options=merged_options, connection=connection\n            )\n\n    result = self._read_file(full_path, format, merged_options, connection)\n\n    # Log metrics for materialized DataFrames\n    elapsed = (time.time() - start) * 1000\n    if isinstance(result, pd.DataFrame):\n        row_count = len(result)\n        memory_mb = result.memory_usage(deep=True).sum() / (1024 * 1024)\n\n        ctx.log_file_io(\n            path=str(full_path) if not isinstance(full_path, list) else str(full_path[0]),\n            format=format,\n            mode=\"read\",\n            rows=row_count,\n        )\n        ctx.log_pandas_metrics(\n            memory_mb=memory_mb,\n            dtypes={col: str(dtype) for col, dtype in result.dtypes.items()},\n        )\n        ctx.info(\n            \"Read completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n            memory_mb=round(memory_mb, 2),\n        )\n\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.restore_delta","title":"<code>restore_delta(connection, path, version)</code>","text":"<p>Restore Delta table to a specific version.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>version</code> <code>int</code> <p>Version number to restore to</p> required Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n    \"\"\"Restore Delta table to a specific version.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        version: Version number to restore to\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.info(\"Starting Delta table restore\", path=path, target_version=version)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    dt.restore(version)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta table restored\",\n        path=str(full_path),\n        restored_to_version=version,\n        elapsed_ms=round(elapsed, 2),\n    )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name (not used in Pandas\u2014no catalog)</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if file/directory exists, False otherwise</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Args:\n        connection: Connection object\n        table: Table name (not used in Pandas\u2014no catalog)\n        path: File path\n\n    Returns:\n        True if file/directory exists, False otherwise\n    \"\"\"\n    if path:\n        full_path = connection.get_path(path)\n        return os.path.exists(full_path)\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168, dry_run=False, enforce_retention_duration=True)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>retention_hours</code> <code>int</code> <p>Retention period (default 168 = 7 days)</p> <code>168</code> <code>dry_run</code> <code>bool</code> <p>If True, only show files to be deleted</p> <code>False</code> <code>enforce_retention_duration</code> <code>bool</code> <p>If False, allows retention &lt; 168 hours (testing only)</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with files_deleted count</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n    dry_run: bool = False,\n    enforce_retention_duration: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"VACUUM a Delta table to remove old files.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        retention_hours: Retention period (default 168 = 7 days)\n        dry_run: If True, only show files to be deleted\n        enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n    Returns:\n        Dictionary with files_deleted count\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n    )\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[pandas]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    deleted_files = dt.vacuum(\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n        enforce_retention_duration=enforce_retention_duration,\n    )\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta VACUUM completed\",\n        path=str(full_path),\n        files_deleted=len(deleted_files),\n        dry_run=dry_run,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return {\"files_deleted\": len(deleted_files)}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate DataFrame against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def validate_data(self, df: pd.DataFrame, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate DataFrame against rules.\n\n    Args:\n        df: DataFrame\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    failures = []\n\n    # Check not empty\n    if validation_config.not_empty:\n        if len(df) == 0:\n            failures.append(\"DataFrame is empty\")\n\n    # Check for nulls in specified columns\n    if validation_config.no_nulls:\n        null_counts = self.count_nulls(df, validation_config.no_nulls)\n        for col, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col}' has {count} null values\")\n\n    # Schema validation\n    if validation_config.schema_validation:\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    # Range validation\n    if validation_config.ranges:\n        for col, bounds in validation_config.ranges.items():\n            if col in df.columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                if min_val is not None:\n                    min_violations = df[df[col] &lt; min_val]\n                    if len(min_violations) &gt; 0:\n                        failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    max_violations = df[df[col] &gt; max_val]\n                    if len(max_violations) &gt; 0:\n                        failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col}' not found for range validation\")\n\n    # Allowed values validation\n    if validation_config.allowed_values:\n        for col, allowed in validation_config.allowed_values.items():\n            if col in df.columns:\n                # Check for values not in allowed list\n                invalid = df[~df[col].isin(allowed)]\n                if len(invalid) &gt; 0:\n                    failures.append(f\"Column '{col}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame</p> required <code>schema_rules</code> <code>Dict[str, Any]</code> <p>Validation rules</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failures</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def validate_schema(self, df: pd.DataFrame, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\n\n    Args:\n        df: DataFrame\n        schema_rules: Validation rules\n\n    Returns:\n        List of validation failures\n    \"\"\"\n    # Ensure materialization\n    df = self.materialize(df)\n\n    failures = []\n\n    # Check required columns\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(df.columns)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    # Check column types\n    if \"types\" in schema_rules:\n        type_map = {\n            \"int\": [\"int64\", \"int32\", \"int16\", \"int8\"],\n            \"float\": [\"float64\", \"float32\"],\n            \"str\": [\"object\", \"string\"],\n            \"bool\": [\"bool\"],\n        }\n\n        for col, expected_type in schema_rules[\"types\"].items():\n            if col not in df.columns:\n                failures.append(f\"Column '{col}' not found for type validation\")\n                continue\n\n            actual_type = str(df[col].dtype)\n            # Handle pyarrow types (e.g. int64[pyarrow])\n            if \"[\" in actual_type and \"pyarrow\" in actual_type:\n                actual_type = actual_type.split(\"[\")[0]\n\n            expected_dtypes = type_map.get(expected_type, [expected_type])\n\n            if actual_type not in expected_dtypes:\n                failures.append(\n                    f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.pandas_engine.PandasEngine.write","title":"<code>write(df, connection, format, table=None, path=None, register_table=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Pandas.</p> Source code in <code>odibi\\engine\\pandas_engine.py</code> <pre><code>def write(\n    self,\n    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    register_table: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Pandas.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"pandas\")\n    start = time.time()\n\n    destination = path or table\n    ctx.debug(\n        \"Starting write operation\",\n        format=format,\n        destination=destination,\n        mode=mode,\n    )\n\n    # Ensure materialization if LazyDataset\n    df = self.materialize(df)\n\n    options = options or {}\n\n    # Handle iterator/generator input\n    from collections.abc import Iterator\n\n    if isinstance(df, Iterator):\n        ctx.debug(\"Writing iterator/generator input\")\n        return self._write_iterator(df, connection, format, table, path, mode, options)\n\n    row_count = len(df)\n    memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n\n    ctx.log_pandas_metrics(\n        memory_mb=memory_mb,\n        dtypes={col: str(dtype) for col, dtype in df.dtypes.items()},\n    )\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        ctx.debug(\"Writing to SQL\", table=table, mode=mode)\n        return self._write_sql(df, connection, table, mode, options)\n\n    # Resolve full path from connection\n    try:\n        full_path = self._resolve_path(path or table, connection)\n    except ValueError:\n        if table and not connection:\n            ctx.error(\"Connection required when specifying 'table'\", table=table)\n            raise ValueError(\"Connection is required when specifying 'table'.\")\n        ctx.error(\"Neither path nor table provided for write operation\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Merge storage options for cloud connections\n    merged_options = self._merge_storage_options(connection, options)\n\n    # Custom Writers\n    if format in self._custom_writers:\n        ctx.debug(f\"Using custom writer for format: {format}\")\n        writer_options = merged_options.copy()\n        writer_options.pop(\"keys\", None)\n        self._custom_writers[format](df, full_path, mode=mode, **writer_options)\n        return None\n\n    # Ensure directory exists (local only)\n    self._ensure_directory(full_path)\n\n    # Warn about partitioning\n    self._check_partitioning(merged_options)\n\n    # Delta Lake Write\n    if format == \"delta\":\n        ctx.debug(\"Writing Delta table\", path=str(full_path), mode=mode)\n        result = self._write_delta(df, full_path, mode, merged_options)\n        elapsed = (time.time() - start) * 1000\n        ctx.log_file_io(\n            path=str(full_path),\n            format=format,\n            mode=mode,\n            rows=row_count,\n        )\n        ctx.info(\n            \"Write completed\",\n            format=format,\n            rows=row_count,\n            elapsed_ms=round(elapsed, 2),\n        )\n        return result\n\n    # Handle Generic Upsert/Append-Once for non-Delta\n    if mode in [\"upsert\", \"append_once\"]:\n        ctx.debug(f\"Handling {mode} mode for non-Delta format\")\n        df, mode = self._handle_generic_upsert(df, full_path, format, mode, merged_options)\n        row_count = len(df)\n\n    # Standard File Write\n    result = self._write_file(df, full_path, format, mode, merged_options)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.log_file_io(\n        path=str(full_path),\n        format=format,\n        mode=mode,\n        rows=row_count,\n    )\n    ctx.info(\n        \"Write completed\",\n        format=format,\n        rows=row_count,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine","title":"<code>odibi.engine.spark_engine</code>","text":"<p>Spark execution engine (Phase 2B: Delta Lake support).</p> <p>Status: Phase 2B implemented - Delta Lake read/write, VACUUM, history, restore</p>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine","title":"<code>SparkEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Spark execution engine with PySpark backend.</p> <p>Phase 2A: Basic read/write + ADLS multi-account support Phase 2B: Delta Lake support</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>class SparkEngine(Engine):\n    \"\"\"Spark execution engine with PySpark backend.\n\n    Phase 2A: Basic read/write + ADLS multi-account support\n    Phase 2B: Delta Lake support\n    \"\"\"\n\n    name = \"spark\"\n    engine_type = EngineType.SPARK\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        spark_session: Any = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Spark engine with import guard.\n\n        Args:\n            connections: Dictionary of connection objects (for multi-account config)\n            spark_session: Existing SparkSession (optional, creates new if None)\n            config: Engine configuration (optional)\n\n        Raises:\n            ImportError: If pyspark not installed\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        ctx.debug(\"Initializing SparkEngine\", connections_count=len(connections or {}))\n\n        try:\n            from pyspark.sql import SparkSession\n        except ImportError as e:\n            ctx.error(\n                \"PySpark not installed\",\n                error_type=\"ImportError\",\n                suggestion=\"pip install odibi[spark]\",\n            )\n            raise ImportError(\n                \"Spark support requires 'pip install odibi[spark]'. \"\n                \"See docs/setup_databricks.md for setup instructions.\"\n            ) from e\n\n        start_time = time.time()\n\n        # Configure Delta Lake support\n        try:\n            from delta import configure_spark_with_delta_pip\n\n            builder = SparkSession.builder.appName(\"odibi\").config(\n                \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n            )\n\n            # Performance Optimizations\n            builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n            builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n            # Reduce Verbosity\n            builder = builder.config(\n                \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n            builder = builder.config(\n                \"spark.executor.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n\n            self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n            self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n            ctx.debug(\"Delta Lake support enabled\")\n\n        except ImportError:\n            ctx.debug(\"Delta Lake not available, using standard Spark\")\n            builder = SparkSession.builder.appName(\"odibi\").config(\n                \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n            )\n\n            # Performance Optimizations\n            builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n            builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n            # Reduce Verbosity\n            builder = builder.config(\n                \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n            )\n\n            self.spark = spark_session or builder.getOrCreate()\n            self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n        self.config = config or {}\n        self.connections = connections or {}\n\n        # Configure all ADLS connections upfront\n        self._configure_all_connections()\n\n        # Apply user-defined Spark configs from performance settings\n        self._apply_spark_config()\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"SparkEngine initialized\",\n            elapsed_ms=round(elapsed, 2),\n            app_name=self.spark.sparkContext.appName,\n            spark_version=self.spark.version,\n            connections_configured=len(self.connections),\n            using_existing_session=spark_session is not None,\n        )\n\n    def _configure_all_connections(self) -&gt; None:\n        \"\"\"Configure Spark with all ADLS connection credentials.\n\n        This sets all storage account keys upfront so Spark can access\n        multiple accounts. Keys are scoped by account name, so no conflicts.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        for conn_name, connection in self.connections.items():\n            if hasattr(connection, \"configure_spark\"):\n                ctx.log_connection(\n                    connection_type=type(connection).__name__,\n                    connection_name=conn_name,\n                    action=\"configure_spark\",\n                )\n                try:\n                    connection.configure_spark(self.spark)\n                    ctx.debug(f\"Configured ADLS connection: {conn_name}\")\n                except Exception as e:\n                    ctx.error(\n                        f\"Failed to configure ADLS connection: {conn_name}\",\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                    )\n                    raise\n\n    def _apply_spark_config(self) -&gt; None:\n        \"\"\"Apply user-defined Spark configurations from performance settings.\n\n        Applies configs via spark.conf.set() for runtime-settable options.\n        For existing sessions (e.g., Databricks), only modifiable configs take effect.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        performance = self.config.get(\"performance\", {})\n        spark_config = performance.get(\"spark_config\", {})\n\n        if not spark_config:\n            return\n\n        ctx.debug(\"Applying Spark configuration\", config_count=len(spark_config))\n\n        for key, value in spark_config.items():\n            try:\n                self.spark.conf.set(key, value)\n                ctx.debug(\n                    f\"Applied Spark config: {key}={value}\", config_key=key, config_value=value\n                )\n            except Exception as e:\n                ctx.warning(\n                    f\"Failed to set Spark config '{key}'\",\n                    config_key=key,\n                    error_message=str(e),\n                    suggestion=\"This config may require session restart\",\n                )\n\n    def _apply_table_properties(\n        self, target: str, properties: Dict[str, str], is_table: bool = False\n    ) -&gt; None:\n        \"\"\"Apply table properties to a Delta table.\n\n        Performance: Batches all properties into a single ALTER TABLE statement\n        to avoid multiple round-trips to the catalog.\n        \"\"\"\n        if not properties:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            table_ref = target if is_table else f\"delta.`{target}`\"\n            ctx.debug(\n                f\"Applying table properties to {target}\",\n                properties_count=len(properties),\n                is_table=is_table,\n            )\n\n            props_list = [f\"'{k}' = '{v}'\" for k, v in properties.items()]\n            props_str = \", \".join(props_list)\n            sql = f\"ALTER TABLE {table_ref} SET TBLPROPERTIES ({props_str})\"\n            self.spark.sql(sql)\n            ctx.debug(f\"Set {len(properties)} table properties in single statement\")\n\n        except Exception as e:\n            ctx.warning(\n                f\"Failed to set table properties on {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n            )\n\n    def _optimize_delta_write(\n        self, target: str, options: Dict[str, Any], is_table: bool = False\n    ) -&gt; None:\n        \"\"\"Run Delta Lake optimization (OPTIMIZE / ZORDER).\"\"\"\n        should_optimize = options.get(\"optimize_write\", False)\n        zorder_by = options.get(\"zorder_by\")\n\n        if not should_optimize and not zorder_by:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        try:\n            if is_table:\n                sql = f\"OPTIMIZE {target}\"\n            else:\n                sql = f\"OPTIMIZE delta.`{target}`\"\n\n            if zorder_by:\n                if isinstance(zorder_by, str):\n                    zorder_by = [zorder_by]\n                cols = \", \".join(zorder_by)\n                sql += f\" ZORDER BY ({cols})\"\n\n            ctx.debug(\"Running Delta optimization\", sql=sql, target=target)\n            self.spark.sql(sql)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta optimization completed\",\n                target=target,\n                zorder_by=zorder_by,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.warning(\n                f\"Optimization failed for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n    def _get_last_delta_commit_info(\n        self, target: str, is_table: bool = False\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for the most recent Delta commit.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            from delta.tables import DeltaTable\n\n            if is_table:\n                dt = DeltaTable.forName(self.spark, target)\n            else:\n                dt = DeltaTable.forPath(self.spark, target)\n\n            last_commit = dt.history(1).collect()[0]\n\n            def safe_get(row, field):\n                if hasattr(row, field):\n                    return getattr(row, field)\n                if hasattr(row, \"__getitem__\"):\n                    try:\n                        return row[field]\n                    except (KeyError, ValueError):\n                        return None\n                return None\n\n            commit_info = {\n                \"version\": safe_get(last_commit, \"version\"),\n                \"timestamp\": safe_get(last_commit, \"timestamp\"),\n                \"operation\": safe_get(last_commit, \"operation\"),\n                \"operation_metrics\": safe_get(last_commit, \"operationMetrics\"),\n                \"read_version\": safe_get(last_commit, \"readVersion\"),\n            }\n\n            ctx.debug(\n                \"Delta commit metadata retrieved\",\n                target=target,\n                version=commit_info.get(\"version\"),\n                operation=commit_info.get(\"operation\"),\n            )\n\n            return commit_info\n\n        except Exception as e:\n            ctx.warning(\n                f\"Failed to fetch Delta commit info for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n            )\n            return None\n\n    def harmonize_schema(self, df, target_schema: Dict[str, str], policy: Any):\n        \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n        from pyspark.sql.functions import col, lit\n\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        target_cols = list(target_schema.keys())\n        current_cols = df.columns\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        ctx.debug(\n            \"Schema harmonization\",\n            target_columns=len(target_cols),\n            current_columns=len(current_cols),\n            missing_columns=list(missing) if missing else None,\n            new_columns=list(new_cols) if new_cols else None,\n            policy_mode=policy.mode.value if hasattr(policy.mode, \"value\") else str(policy.mode),\n        )\n\n        # Check Validations\n        if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n            ctx.error(\n                f\"Schema Policy Violation: Missing columns {missing}\",\n                missing_columns=list(missing),\n            )\n            raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n        if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n            ctx.error(\n                f\"Schema Policy Violation: New columns {new_cols}\",\n                new_columns=list(new_cols),\n            )\n            raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n        # Apply Transformations\n        if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n            res = df\n            for c in missing:\n                res = res.withColumn(c, lit(None))\n            ctx.debug(\"Schema evolved: added missing columns as null\")\n            return res\n        else:\n            select_exprs = []\n            for c in target_cols:\n                if c in current_cols:\n                    # Escape column names with backticks to handle special characters\n                    select_exprs.append(col(f\"`{c}`\"))\n                else:\n                    select_exprs.append(lit(None).alias(c))\n\n            ctx.debug(\"Schema enforced: projected to target schema\")\n            return df.select(*select_exprs)\n\n    def anonymize(self, df, columns: List[str], method: str, salt: Optional[str] = None):\n        \"\"\"Anonymize columns using Spark functions.\"\"\"\n        from pyspark.sql.functions import col, concat, lit, regexp_replace, sha2\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        ctx.debug(\n            \"Anonymizing columns\",\n            columns=columns,\n            method=method,\n            has_salt=salt is not None,\n        )\n\n        res = df\n        for c in columns:\n            if c not in df.columns:\n                ctx.warning(f\"Column '{c}' not found for anonymization, skipping\", column=c)\n                continue\n\n            # Escape column names with backticks to handle special characters\n            escaped_col = f\"`{c}`\"\n            if method == \"hash\":\n                if salt:\n                    res = res.withColumn(c, sha2(concat(col(escaped_col), lit(salt)), 256))\n                else:\n                    res = res.withColumn(c, sha2(col(escaped_col), 256))\n\n            elif method == \"mask\":\n                res = res.withColumn(c, regexp_replace(col(escaped_col), \".(?=.{4})\", \"*\"))\n\n            elif method == \"redact\":\n                res = res.withColumn(c, lit(\"[REDACTED]\"))\n\n        ctx.debug(f\"Anonymization completed using {method}\")\n        return res\n\n    def get_schema(self, df) -&gt; Dict[str, str]:\n        \"\"\"Get DataFrame schema with types.\"\"\"\n        return {f.name: f.dataType.simpleString() for f in df.schema}\n\n    def get_shape(self, df) -&gt; Tuple[int, int]:\n        \"\"\"Get DataFrame shape as (rows, columns).\"\"\"\n        return (df.count(), len(df.columns))\n\n    def count_rows(self, df) -&gt; int:\n        \"\"\"Count rows in DataFrame.\"\"\"\n        return df.count()\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        as_of_version: Optional[int] = None,\n        as_of_timestamp: Optional[str] = None,\n    ) -&gt; Any:\n        \"\"\"Read data using Spark.\n\n        Args:\n            connection: Connection object (with get_path method)\n            format: Data format (csv, parquet, json, delta, sql_server)\n            table: Table name\n            path: File path\n            streaming: Whether to read as a stream (readStream)\n            schema: Schema string in DDL format (required for streaming file sources)\n            options: Format-specific options (including versionAsOf for Delta time travel)\n            as_of_version: Time travel version\n            as_of_timestamp: Time travel timestamp\n\n        Returns:\n            Spark DataFrame (or Streaming DataFrame)\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        source_identifier = table or path or \"unknown\"\n        ctx.debug(\n            \"Starting Spark read\",\n            format=format,\n            source=source_identifier,\n            streaming=streaming,\n            as_of_version=as_of_version,\n            as_of_timestamp=as_of_timestamp,\n        )\n\n        # Handle Time Travel options\n        if as_of_version is not None:\n            options[\"versionAsOf\"] = as_of_version\n            ctx.debug(f\"Time travel enabled: version {as_of_version}\")\n        if as_of_timestamp is not None:\n            options[\"timestampAsOf\"] = as_of_timestamp\n            ctx.debug(f\"Time travel enabled: timestamp {as_of_timestamp}\")\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if streaming:\n                ctx.error(\"Streaming not supported for SQL Server / Azure SQL\")\n                raise ValueError(\"Streaming not supported for SQL Server / Azure SQL yet.\")\n\n            if not hasattr(connection, \"get_spark_options\"):\n                conn_type = type(connection).__name__\n                msg = f\"Connection type '{conn_type}' does not support Spark SQL read\"\n                ctx.error(msg, connection_type=conn_type)\n                raise ValueError(msg)\n\n            jdbc_options = connection.get_spark_options()\n            merged_options = {**jdbc_options, **options}\n\n            # Extract filter for SQL pushdown\n            sql_filter = merged_options.pop(\"filter\", None)\n\n            if \"query\" in merged_options:\n                merged_options.pop(\"dbtable\", None)\n                # If filter provided with query, append to WHERE clause\n                if sql_filter:\n                    existing_query = merged_options[\"query\"]\n                    # Wrap existing query and add filter\n                    if \"WHERE\" in existing_query.upper():\n                        merged_options[\"query\"] = f\"({existing_query}) AND ({sql_filter})\"\n                    else:\n                        subquery = f\"SELECT * FROM ({existing_query}) AS _subq WHERE {sql_filter}\"\n                        merged_options[\"query\"] = subquery\n                    ctx.debug(f\"Applied SQL pushdown filter to query: {sql_filter}\")\n            elif table:\n                # Build query with filter pushdown instead of using dbtable\n                if sql_filter:\n                    merged_options.pop(\"dbtable\", None)\n                    merged_options[\"query\"] = f\"SELECT * FROM {table} WHERE {sql_filter}\"\n                    ctx.debug(f\"Applied SQL pushdown filter: {sql_filter}\")\n                else:\n                    merged_options[\"dbtable\"] = table\n            elif \"dbtable\" not in merged_options:\n                ctx.error(\"SQL format requires 'table' config or 'query' option\")\n                raise ValueError(\"SQL format requires 'table' config or 'query' option\")\n\n            ctx.debug(\"Executing JDBC read\", has_query=\"query\" in merged_options)\n\n            try:\n                df = self.spark.read.format(\"jdbc\").options(**merged_options).load()\n                elapsed = (time.time() - start_time) * 1000\n                partition_count = df.rdd.getNumPartitions()\n\n                ctx.log_file_io(path=source_identifier, format=format, mode=\"read\")\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.info(\n                    \"JDBC read completed\",\n                    source=source_identifier,\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                )\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"JDBC read failed\",\n                    source=source_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Read based on format\n        if table:\n            # Managed/External Table (Catalog)\n            ctx.debug(f\"Reading from catalog table: {table}\")\n\n            if streaming:\n                reader = self.spark.readStream.format(format)\n            else:\n                reader = self.spark.read.format(format)\n\n            for key, value in options.items():\n                reader = reader.option(key, value)\n\n            try:\n                df = reader.table(table)\n\n                if \"filter\" in options:\n                    df = df.filter(options[\"filter\"])\n                    ctx.debug(f\"Applied filter: {options['filter']}\")\n\n                elapsed = (time.time() - start_time) * 1000\n\n                if not streaming:\n                    partition_count = df.rdd.getNumPartitions()\n                    ctx.log_spark_metrics(partition_count=partition_count)\n                    ctx.log_file_io(path=table, format=format, mode=\"read\")\n                    ctx.info(\n                        f\"Table read completed: {table}\",\n                        elapsed_ms=round(elapsed, 2),\n                        partitions=partition_count,\n                    )\n                else:\n                    ctx.info(f\"Streaming read started: {table}\", elapsed_ms=round(elapsed, 2))\n\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"Table read failed: {table}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        elif path:\n            # File Path\n            full_path = connection.get_path(path)\n            ctx.debug(f\"Reading from path: {full_path}\")\n\n            # Excel format: delegate to Pandas, convert to Spark\n            if format == \"excel\":\n                if streaming:\n                    ctx.error(\"Streaming not supported for Excel format\")\n                    raise ValueError(\"Streaming not supported for Excel format\")\n\n                ctx.debug(\"Reading Excel via Pandas engine (best Excel support)\")\n                from odibi.engine.pandas_engine import PandasEngine\n\n                # Get storage_options from connection for Azure/cloud authentication\n                storage_options = None\n                if hasattr(connection, \"pandas_storage_options\"):\n                    storage_options = connection.pandas_storage_options()\n\n                # Use Pandas engine for Excel reading\n                pandas_engine = PandasEngine()\n                pdf = pandas_engine._read_excel_with_patterns(\n                    full_path,\n                    sheet_pattern=options.pop(\"sheet_pattern\", None),\n                    sheet_pattern_case_sensitive=options.pop(\"sheet_pattern_case_sensitive\", False),\n                    add_source_file=options.pop(\"add_source_file\", False),\n                    is_glob=\"*\" in str(full_path) or \"?\" in str(full_path),\n                    ctx=ctx,\n                    storage_options=storage_options,\n                    **options,\n                )\n\n                # Convert Pandas DataFrame to Spark DataFrame\n                df = self.spark.createDataFrame(pdf)\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    f\"Excel read completed (via Pandas): {path}\",\n                    elapsed_ms=round(elapsed, 2),\n                    row_count=len(pdf),\n                )\n                return df\n\n            # Auto-detect encoding for CSV (Batch only)\n            if not streaming and format == \"csv\" and options.get(\"auto_encoding\"):\n                options = options.copy()\n                options.pop(\"auto_encoding\")\n\n                if \"encoding\" not in options:\n                    try:\n                        from odibi.utils.encoding import detect_encoding\n\n                        detected = detect_encoding(connection, path)\n                        if detected:\n                            options[\"encoding\"] = detected\n                            ctx.debug(f\"Detected encoding: {detected}\", path=path)\n                    except ImportError:\n                        pass\n                    except Exception as e:\n                        ctx.warning(\n                            f\"Encoding detection failed for {path}\",\n                            error_message=str(e),\n                        )\n\n            # Resolve cloudFiles.schemaLocation through read connection if still relative\n            # (Node level resolves via write connection first if available)\n            if \"cloudFiles.schemaLocation\" in options:\n                schema_location = options[\"cloudFiles.schemaLocation\"]\n                if not schema_location.startswith(\n                    (\"abfss://\", \"s3://\", \"gs://\", \"dbfs://\", \"hdfs://\", \"wasbs://\")\n                ):\n                    options = options.copy()\n                    options[\"cloudFiles.schemaLocation\"] = connection.get_path(schema_location)\n                    ctx.debug(\n                        \"Resolved cloudFiles.schemaLocation through read connection (fallback)\",\n                        original=schema_location,\n                        resolved=options[\"cloudFiles.schemaLocation\"],\n                    )\n\n            if streaming:\n                reader = self.spark.readStream.format(format)\n                if schema:\n                    reader = reader.schema(schema)\n                    ctx.debug(f\"Applied schema for streaming read: {schema[:100]}...\")\n                else:\n                    # Determine if we should warn about missing schema\n                    # Formats that can infer schema: delta, parquet, avro (embedded schema)\n                    # cloudFiles with schemaLocation or self-describing formats (avro, parquet) are fine\n                    should_warn = True\n\n                    if format in [\"delta\", \"parquet\"]:\n                        should_warn = False\n                    elif format == \"cloudFiles\":\n                        cloud_format = options.get(\"cloudFiles.format\", \"\")\n                        has_schema_location = \"cloudFiles.schemaLocation\" in options\n                        # avro and parquet have embedded schemas\n                        if cloud_format in [\"avro\", \"parquet\"] or has_schema_location:\n                            should_warn = False\n\n                    if should_warn:\n                        ctx.warning(\n                            f\"Streaming read from '{format}' format without schema. \"\n                            \"Schema inference is not supported for streaming sources. \"\n                            \"Consider adding 'schema' to your read config.\"\n                        )\n            else:\n                reader = self.spark.read.format(format)\n                if schema:\n                    reader = reader.schema(schema)\n\n            for key, value in options.items():\n                if key == \"header\" and isinstance(value, bool):\n                    value = str(value).lower()\n                reader = reader.option(key, value)\n\n            try:\n                df = reader.load(full_path)\n\n                if \"filter\" in options:\n                    df = df.filter(options[\"filter\"])\n                    ctx.debug(f\"Applied filter: {options['filter']}\")\n\n                elapsed = (time.time() - start_time) * 1000\n\n                if not streaming:\n                    partition_count = df.rdd.getNumPartitions()\n                    ctx.log_spark_metrics(partition_count=partition_count)\n                    ctx.log_file_io(path=path, format=format, mode=\"read\")\n                    ctx.info(\n                        f\"File read completed: {path}\",\n                        elapsed_ms=round(elapsed, 2),\n                        partitions=partition_count,\n                        format=format,\n                    )\n                else:\n                    ctx.info(f\"Streaming read started: {path}\", elapsed_ms=round(elapsed, 2))\n\n                return df\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"File read failed: {path}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                    format=format,\n                )\n                raise\n        else:\n            ctx.error(\"Either path or table must be provided\")\n            raise ValueError(\"Either path or table must be provided\")\n\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Spark.\n\n        Args:\n            df: Spark DataFrame to write\n            connection: Connection object\n            format: Output format (csv, parquet, json, delta)\n            table: Table name\n            path: File path\n            register_table: Name to register as external table (if path is used)\n            mode: Write mode (overwrite, append, error, ignore, upsert, append_once)\n            options: Format-specific options (including partition_by for partitioning)\n            streaming_config: StreamingWriteConfig for streaming DataFrames\n\n        Returns:\n            Optional dictionary containing Delta commit metadata (if format=delta),\n            or streaming query info (if streaming)\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        if getattr(df, \"isStreaming\", False) is True:\n            return self._write_streaming(\n                df=df,\n                connection=connection,\n                format=format,\n                table=table,\n                path=path,\n                register_table=register_table,\n                options=options,\n                streaming_config=streaming_config,\n            )\n\n        target_identifier = table or path or \"unknown\"\n        try:\n            partition_count = df.rdd.getNumPartitions()\n        except Exception:\n            partition_count = 1  # Fallback for mocks or unsupported DataFrames\n\n        # Auto-coalesce DataFrames for Delta writes to reduce file overhead\n        # Use coalesce_partitions option to explicitly set target partitions\n        # NOTE: We avoid df.count() here as it would trigger double-evaluation of lazy DataFrames\n        coalesce_partitions = options.pop(\"coalesce_partitions\", None)\n        if (\n            coalesce_partitions\n            and isinstance(partition_count, int)\n            and partition_count &gt; coalesce_partitions\n        ):\n            df = df.coalesce(coalesce_partitions)\n            ctx.debug(\n                f\"Coalesced DataFrame to {coalesce_partitions} partition(s)\",\n                original_partitions=partition_count,\n            )\n            partition_count = coalesce_partitions\n\n        ctx.debug(\n            \"Starting Spark write\",\n            format=format,\n            target=target_identifier,\n            mode=mode,\n            partitions=partition_count,\n        )\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if not hasattr(connection, \"get_spark_options\"):\n                conn_type = type(connection).__name__\n                msg = f\"Connection type '{conn_type}' does not support Spark SQL write\"\n                ctx.error(msg, connection_type=conn_type)\n                raise ValueError(msg)\n\n            jdbc_options = connection.get_spark_options()\n            merged_options = {**jdbc_options, **options}\n\n            if table:\n                merged_options[\"dbtable\"] = table\n            elif \"dbtable\" not in merged_options:\n                ctx.error(\"SQL format requires 'table' config or 'dbtable' option\")\n                raise ValueError(\"SQL format requires 'table' config or 'dbtable' option\")\n\n            # Handle MERGE mode for SQL Server\n            if mode == \"merge\":\n                merge_keys = options.get(\"merge_keys\")\n                merge_options = options.get(\"merge_options\")\n\n                if not merge_keys:\n                    ctx.error(\"MERGE mode requires 'merge_keys' in options\")\n                    raise ValueError(\n                        \"MERGE mode requires 'merge_keys' in options. \"\n                        \"Specify the key columns for the MERGE ON clause.\"\n                    )\n\n                from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n                writer = SqlServerMergeWriter(connection)\n                ctx.debug(\n                    \"Executing SQL Server MERGE\",\n                    target=table,\n                    merge_keys=merge_keys,\n                )\n\n                try:\n                    result = writer.merge(\n                        df=df,\n                        spark_engine=self,\n                        target_table=table,\n                        merge_keys=merge_keys,\n                        options=merge_options,\n                        jdbc_options=jdbc_options,\n                    )\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                    ctx.info(\n                        \"SQL Server MERGE completed\",\n                        target=target_identifier,\n                        mode=mode,\n                        inserted=result.inserted,\n                        updated=result.updated,\n                        deleted=result.deleted,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    return {\n                        \"mode\": \"merge\",\n                        \"inserted\": result.inserted,\n                        \"updated\": result.updated,\n                        \"deleted\": result.deleted,\n                        \"total_affected\": result.total_affected,\n                    }\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"SQL Server MERGE failed\",\n                        target=target_identifier,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n            # Handle enhanced overwrite with strategies\n            if mode == \"overwrite\" and options.get(\"overwrite_options\"):\n                from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n                overwrite_options = options.get(\"overwrite_options\")\n                writer = SqlServerMergeWriter(connection)\n\n                ctx.debug(\n                    \"Executing SQL Server enhanced overwrite\",\n                    target=table,\n                    strategy=(\n                        overwrite_options.strategy.value\n                        if hasattr(overwrite_options, \"strategy\")\n                        else \"truncate_insert\"\n                    ),\n                )\n\n                try:\n                    result = writer.overwrite_spark(\n                        df=df,\n                        target_table=table,\n                        options=overwrite_options,\n                        jdbc_options=jdbc_options,\n                    )\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                    ctx.info(\n                        \"SQL Server enhanced overwrite completed\",\n                        target=target_identifier,\n                        strategy=result.strategy,\n                        rows_written=result.rows_written,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    return {\n                        \"mode\": \"overwrite\",\n                        \"strategy\": result.strategy,\n                        \"rows_written\": result.rows_written,\n                    }\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"SQL Server enhanced overwrite failed\",\n                        target=target_identifier,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n            if mode not in [\"overwrite\", \"append\", \"ignore\", \"error\"]:\n                if mode == \"fail\":\n                    mode = \"error\"\n                else:\n                    ctx.error(f\"Write mode '{mode}' not supported for Spark SQL write\")\n                    raise ValueError(f\"Write mode '{mode}' not supported for Spark SQL write\")\n\n            ctx.debug(\"Executing JDBC write\", target=table or merged_options.get(\"dbtable\"))\n\n            try:\n                df.write.format(\"jdbc\").options(**merged_options).mode(mode).save()\n                elapsed = (time.time() - start_time) * 1000\n                ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                ctx.info(\n                    \"JDBC write completed\",\n                    target=target_identifier,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"JDBC write failed\",\n                    target=target_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Handle Upsert/AppendOnce (Delta Only)\n        if mode in [\"upsert\", \"append_once\"]:\n            if format != \"delta\":\n                ctx.error(f\"Mode '{mode}' only supported for Delta format\")\n                raise NotImplementedError(\n                    f\"Mode '{mode}' only supported for Delta format in Spark engine.\"\n                )\n\n            keys = options.get(\"keys\")\n            if not keys:\n                ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n                raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n            if isinstance(keys, str):\n                keys = [keys]\n\n            exists = self.table_exists(connection, table, path)\n            ctx.debug(\"Table existence check for merge\", target=target_identifier, exists=exists)\n\n            if not exists:\n                mode = \"overwrite\"\n                ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n            else:\n                from delta.tables import DeltaTable\n\n                target_dt = None\n                target_name = \"\"\n                is_table_target = False\n\n                if table:\n                    target_dt = DeltaTable.forName(self.spark, table)\n                    target_name = table\n                    is_table_target = True\n                elif path:\n                    full_path = connection.get_path(path)\n                    target_dt = DeltaTable.forPath(self.spark, full_path)\n                    target_name = full_path\n                    is_table_target = False\n\n                condition = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in keys])\n                ctx.debug(\"Executing Delta merge\", merge_mode=mode, keys=keys, condition=condition)\n\n                merge_builder = target_dt.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n                try:\n                    if mode == \"upsert\":\n                        merge_builder.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                    elif mode == \"append_once\":\n                        merge_builder.whenNotMatchedInsertAll().execute()\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Delta merge completed\",\n                        target=target_name,\n                        mode=mode,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    self._optimize_delta_write(target_name, options, is_table=is_table_target)\n                    commit_info = self._get_last_delta_commit_info(\n                        target_name, is_table=is_table_target\n                    )\n\n                    if commit_info:\n                        ctx.debug(\n                            \"Delta commit info\",\n                            version=commit_info.get(\"version\"),\n                            operation=commit_info.get(\"operation\"),\n                        )\n\n                    return commit_info\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Delta merge failed\",\n                        target=target_name,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n        # Get output location\n        if table:\n            # Managed/External Table (Catalog)\n            ctx.debug(f\"Writing to catalog table: {table}\")\n            writer = df.write.format(format).mode(mode)\n\n            partition_by = options.get(\"partition_by\")\n            if partition_by:\n                if isinstance(partition_by, str):\n                    partition_by = [partition_by]\n                writer = writer.partitionBy(*partition_by)\n                ctx.debug(f\"Partitioning by: {partition_by}\")\n\n            for key, value in options.items():\n                writer = writer.option(key, value)\n\n            try:\n                writer.saveAsTable(table)\n                elapsed = (time.time() - start_time) * 1000\n\n                ctx.log_file_io(\n                    path=table,\n                    format=format,\n                    mode=mode,\n                    partitions=partition_by,\n                )\n                ctx.info(\n                    f\"Table write completed: {table}\",\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if format == \"delta\":\n                    self._optimize_delta_write(table, options, is_table=True)\n                    return self._get_last_delta_commit_info(table, is_table=True)\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    f\"Table write failed: {table}\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        elif path:\n            full_path = connection.get_path(path)\n        else:\n            ctx.error(\"Either path or table must be provided\")\n            raise ValueError(\"Either path or table must be provided\")\n\n        # Extract partition_by option\n        partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n\n        # Extract cluster_by option (Liquid Clustering)\n        cluster_by = options.pop(\"cluster_by\", None)\n\n        # Warn about partitioning anti-patterns\n        if partition_by and cluster_by:\n            import warnings\n\n            ctx.warning(\n                \"Conflict: Both 'partition_by' and 'cluster_by' are set\",\n                partition_by=partition_by,\n                cluster_by=cluster_by,\n            )\n            warnings.warn(\n                \"\u26a0\ufe0f  Conflict: Both 'partition_by' and 'cluster_by' (Liquid Clustering) are set. \"\n                \"Liquid Clustering supersedes partitioning. 'partition_by' will be ignored \"\n                \"if the table is being created now.\",\n                UserWarning,\n            )\n\n        elif partition_by:\n            import warnings\n\n            ctx.warning(\n                \"Partitioning warning: ensure low-cardinality columns\",\n                partition_by=partition_by,\n            )\n            warnings.warn(\n                \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n                \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n                \"and ensure each partition has &gt; 1000 rows.\",\n                UserWarning,\n            )\n\n        # Handle Upsert/Append-Once for Delta Lake (Path-based only for now)\n        if format == \"delta\" and mode in [\"upsert\", \"append_once\"]:\n            try:\n                from delta.tables import DeltaTable\n            except ImportError:\n                ctx.error(\"Delta Lake support requires 'delta-spark'\")\n                raise ImportError(\"Delta Lake support requires 'delta-spark'\")\n\n            if \"keys\" not in options:\n                ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n                raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n            if DeltaTable.isDeltaTable(self.spark, full_path):\n                ctx.debug(f\"Performing Delta merge at path: {full_path}\")\n                delta_table = DeltaTable.forPath(self.spark, full_path)\n                keys = options[\"keys\"]\n                if isinstance(keys, str):\n                    keys = [keys]\n\n                condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in keys])\n                merger = delta_table.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n                try:\n                    if mode == \"upsert\":\n                        merger.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                    else:\n                        merger.whenNotMatchedInsertAll().execute()\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Delta merge completed at path\",\n                        path=path,\n                        mode=mode,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    if register_table:\n                        try:\n                            table_in_catalog = self.spark.catalog.tableExists(register_table)\n                            needs_registration = not table_in_catalog\n\n                            # Handle orphan catalog entries (only for path-not-found errors)\n                            if table_in_catalog:\n                                try:\n                                    # Use limit(1) not limit(0) - limit(0) can succeed from metadata alone\n                                    self.spark.table(register_table).limit(1).collect()\n                                    ctx.debug(\n                                        f\"Table '{register_table}' already registered and valid\"\n                                    )\n                                except Exception as verify_err:\n                                    error_str = str(verify_err)\n                                    is_orphan = (\n                                        \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                                        or \"Path does not exist\" in error_str\n                                        or \"FileNotFoundException\" in error_str\n                                    )\n                                    if is_orphan:\n                                        ctx.warning(\n                                            f\"Table '{register_table}' is orphan, re-registering\"\n                                        )\n                                        try:\n                                            self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                                        except Exception:\n                                            pass\n                                        needs_registration = True\n                                    else:\n                                        ctx.debug(\n                                            f\"Table '{register_table}' verify failed, \"\n                                            \"skipping registration\"\n                                        )\n\n                            if needs_registration:\n                                create_sql = (\n                                    f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                    f\"USING DELTA LOCATION '{full_path}'\"\n                                )\n                                self.spark.sql(create_sql)\n                                ctx.info(f\"Registered table: {register_table}\", path=full_path)\n                        except Exception as e:\n                            ctx.error(\n                                f\"Failed to register external table '{register_table}'\",\n                                error_message=str(e),\n                            )\n\n                    self._optimize_delta_write(full_path, options, is_table=False)\n                    return self._get_last_delta_commit_info(full_path, is_table=False)\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Delta merge failed at path\",\n                        path=path,\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n            else:\n                mode = \"overwrite\"\n                ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n\n        # Write based on format (Path-based)\n        ctx.debug(f\"Writing to path: {full_path}\")\n\n        # Handle Liquid Clustering (New Table Creation via SQL)\n        if format == \"delta\" and cluster_by:\n            should_create = False\n            target_name = None\n\n            if table:\n                target_name = table\n                if mode == \"overwrite\":\n                    should_create = True\n                elif mode == \"append\":\n                    if not self.spark.catalog.tableExists(table):\n                        should_create = True\n            elif path:\n                full_path = connection.get_path(path)\n                target_name = f\"delta.`{full_path}`\"\n                if mode == \"overwrite\":\n                    should_create = True\n                elif mode == \"append\":\n                    try:\n                        from delta.tables import DeltaTable\n\n                        if not DeltaTable.isDeltaTable(self.spark, full_path):\n                            should_create = True\n                    except ImportError:\n                        pass\n\n            if should_create:\n                if isinstance(cluster_by, str):\n                    cluster_by = [cluster_by]\n\n                cols = \", \".join(cluster_by)\n                temp_view = f\"odibi_temp_writer_{abs(hash(str(target_name)))}\"\n                df.createOrReplaceTempView(temp_view)\n\n                create_cmd = (\n                    \"CREATE OR REPLACE TABLE\"\n                    if mode == \"overwrite\"\n                    else \"CREATE TABLE IF NOT EXISTS\"\n                )\n\n                sql = (\n                    f\"{create_cmd} {target_name} USING DELTA CLUSTER BY ({cols}) \"\n                    f\"AS SELECT * FROM {temp_view}\"\n                )\n\n                ctx.debug(\"Creating clustered Delta table\", sql=sql, cluster_by=cluster_by)\n\n                try:\n                    self.spark.sql(sql)\n                    self.spark.catalog.dropTempView(temp_view)\n\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.info(\n                        \"Clustered Delta table created\",\n                        target=target_name,\n                        cluster_by=cluster_by,\n                        elapsed_ms=round(elapsed, 2),\n                    )\n\n                    if register_table and path:\n                        try:\n                            reg_sql = (\n                                f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                f\"USING DELTA LOCATION '{full_path}'\"\n                            )\n                            self.spark.sql(reg_sql)\n                            ctx.info(f\"Registered table: {register_table}\")\n                        except Exception:\n                            pass\n\n                    if format == \"delta\":\n                        self._optimize_delta_write(\n                            target_name if table else full_path, options, is_table=bool(table)\n                        )\n                        return self._get_last_delta_commit_info(\n                            target_name if table else full_path, is_table=bool(table)\n                        )\n                    return None\n\n                except Exception as e:\n                    elapsed = (time.time() - start_time) * 1000\n                    ctx.error(\n                        \"Failed to create clustered Delta table\",\n                        error_type=type(e).__name__,\n                        error_message=str(e),\n                        elapsed_ms=round(elapsed, 2),\n                    )\n                    raise\n\n        # Extract table_properties from options\n        table_properties = options.pop(\"table_properties\", None)\n\n        # For column mapping and other properties that must be set BEFORE write\n        original_configs = {}\n        if table_properties and format == \"delta\":\n            for prop_name, prop_value in table_properties.items():\n                spark_conf_key = (\n                    f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n                )\n                try:\n                    original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n                except Exception:\n                    original_configs[spark_conf_key] = None\n                self.spark.conf.set(spark_conf_key, prop_value)\n            ctx.debug(\n                \"Applied table properties as session defaults\",\n                properties=list(table_properties.keys()),\n            )\n\n        writer = df.write.format(format).mode(mode)\n\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            writer.save(full_path)\n            elapsed = (time.time() - start_time) * 1000\n\n            ctx.log_file_io(\n                path=path,\n                format=format,\n                mode=mode,\n                partitions=partition_by,\n            )\n            ctx.info(\n                f\"File write completed: {path}\",\n                format=format,\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"File write failed: {path}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n        finally:\n            for conf_key, original_value in original_configs.items():\n                if original_value is None:\n                    self.spark.conf.unset(conf_key)\n                else:\n                    self.spark.conf.set(conf_key, original_value)\n\n        if format == \"delta\":\n            self._optimize_delta_write(full_path, options, is_table=False)\n\n        if register_table and format == \"delta\":\n            try:\n                table_in_catalog = self.spark.catalog.tableExists(register_table)\n                needs_registration = not table_in_catalog\n\n                # Handle orphan catalog entries: table exists but points to deleted path\n                # Only treat as orphan if it's specifically a DELTA_PATH_DOES_NOT_EXIST error\n                if table_in_catalog:\n                    try:\n                        # Use limit(1) not limit(0) - limit(0) can succeed from metadata alone\n                        self.spark.table(register_table).limit(1).collect()\n                        ctx.debug(\n                            f\"Table '{register_table}' already registered and valid, \"\n                            \"skipping registration\"\n                        )\n                    except Exception as verify_err:\n                        error_str = str(verify_err)\n                        is_orphan = (\n                            \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                            or \"Path does not exist\" in error_str\n                            or \"FileNotFoundException\" in error_str\n                        )\n\n                        if is_orphan:\n                            # Orphan entry - table in catalog but path was deleted\n                            ctx.warning(\n                                f\"Table '{register_table}' is orphan (path deleted), \"\n                                \"dropping and re-registering\",\n                                error_message=error_str[:200],\n                            )\n                            try:\n                                self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                            except Exception:\n                                pass  # Best effort cleanup\n                            needs_registration = True\n                        else:\n                            # Other error (auth, network, etc.) - don't drop, just log\n                            ctx.debug(\n                                f\"Table '{register_table}' exists but verify failed \"\n                                \"(not orphan), skipping registration\",\n                                error_message=error_str[:200],\n                            )\n\n                if needs_registration:\n                    ctx.debug(f\"Registering table '{register_table}' at '{full_path}'\")\n                    reg_sql = (\n                        f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                        f\"USING DELTA LOCATION '{full_path}'\"\n                    )\n                    self.spark.sql(reg_sql)\n                    ctx.info(f\"Registered table: {register_table}\", path=full_path)\n            except Exception as e:\n                ctx.error(\n                    f\"Failed to register table '{register_table}'\",\n                    error_message=str(e),\n                )\n                raise RuntimeError(\n                    f\"Failed to register external table '{register_table}': {e}\"\n                ) from e\n\n        if format == \"delta\":\n            return self._get_last_delta_commit_info(full_path, is_table=False)\n\n        return None\n\n    def _write_streaming(\n        self,\n        df,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        register_table: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Write streaming DataFrame using Spark Structured Streaming.\n\n        Args:\n            df: Streaming Spark DataFrame\n            connection: Connection object\n            format: Output format (delta, kafka, etc.)\n            table: Table name\n            path: File path\n            register_table: Name to register as external table (if path is used)\n            options: Format-specific options\n            streaming_config: StreamingWriteConfig with streaming parameters\n\n        Returns:\n            Dictionary with streaming query information\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n        options = options or {}\n\n        if streaming_config is None:\n            ctx.error(\"Streaming DataFrame requires streaming_config\")\n            raise ValueError(\n                \"Streaming DataFrame detected but no streaming_config provided. \"\n                \"Add a 'streaming' section to your write config with at least \"\n                \"'checkpoint_location' specified.\"\n            )\n\n        target_identifier = table or path or \"unknown\"\n\n        checkpoint_location = streaming_config.checkpoint_location\n        if checkpoint_location and connection:\n            if not checkpoint_location.startswith(\n                (\"abfss://\", \"s3://\", \"gs://\", \"dbfs://\", \"hdfs://\", \"wasbs://\")\n            ):\n                checkpoint_location = connection.get_path(checkpoint_location)\n                ctx.debug(\n                    \"Resolved checkpoint location through connection\",\n                    original=streaming_config.checkpoint_location,\n                    resolved=checkpoint_location,\n                )\n\n        # Extract table_properties from options for Delta column mapping etc.\n        table_properties = options.pop(\"table_properties\", None)\n\n        # For column mapping and other properties that must be set BEFORE write\n        original_configs = {}\n        if table_properties and format == \"delta\":\n            for prop_name, prop_value in table_properties.items():\n                spark_conf_key = (\n                    f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n                )\n                try:\n                    original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n                except Exception:\n                    original_configs[spark_conf_key] = None\n                self.spark.conf.set(spark_conf_key, prop_value)\n            ctx.debug(\n                \"Applied table properties as session defaults for streaming\",\n                properties=list(table_properties.keys()),\n            )\n\n        ctx.debug(\n            \"Starting streaming write\",\n            format=format,\n            target=target_identifier,\n            output_mode=streaming_config.output_mode,\n            checkpoint=checkpoint_location,\n        )\n\n        writer = df.writeStream.format(format)\n        writer = writer.outputMode(streaming_config.output_mode)\n        writer = writer.option(\"checkpointLocation\", checkpoint_location)\n\n        if streaming_config.query_name:\n            writer = writer.queryName(streaming_config.query_name)\n\n        if streaming_config.trigger:\n            trigger = streaming_config.trigger\n            if trigger.once:\n                writer = writer.trigger(once=True)\n            elif trigger.available_now:\n                writer = writer.trigger(availableNow=True)\n            elif trigger.processing_time:\n                writer = writer.trigger(processingTime=trigger.processing_time)\n            elif trigger.continuous:\n                writer = writer.trigger(continuous=trigger.continuous)\n\n        partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        # Capture Delta version before streaming to detect if new data was written\n        version_before = None\n        if path and format == \"delta\":\n            try:\n                full_path = connection.get_path(path)\n                history_df = self.spark.sql(f\"DESCRIBE HISTORY delta.`{full_path}` LIMIT 1\")\n                rows = history_df.collect()\n                if rows:\n                    version_before = (\n                        rows[0].version if hasattr(rows[0], \"version\") else rows[0][\"version\"]\n                    )\n            except Exception:\n                pass  # Table may not exist yet\n\n        try:\n            if table:\n                query = writer.toTable(table)\n                ctx.info(\n                    f\"Streaming query started: writing to table {table}\",\n                    query_id=str(query.id),\n                    query_name=query.name,\n                )\n            elif path:\n                full_path = connection.get_path(path)\n                query = writer.start(full_path)\n                ctx.info(\n                    f\"Streaming query started: writing to path {path}\",\n                    query_id=str(query.id),\n                    query_name=query.name,\n                )\n            else:\n                ctx.error(\"Either path or table must be provided for streaming write\")\n                raise ValueError(\n                    \"Streaming write operation failed: neither 'path' nor 'table' was provided. \"\n                    \"Specify a file path or table name in your streaming configuration.\"\n                )\n\n            elapsed = (time.time() - start_time) * 1000\n\n            result = {\n                \"streaming\": True,\n                \"query_id\": str(query.id),\n                \"query_name\": query.name,\n                \"status\": \"running\",\n                \"target\": target_identifier,\n                \"output_mode\": streaming_config.output_mode,\n                \"checkpoint_location\": streaming_config.checkpoint_location,\n                \"elapsed_ms\": round(elapsed, 2),\n            }\n\n            should_wait = streaming_config.await_termination\n            if streaming_config.trigger:\n                trigger = streaming_config.trigger\n                if trigger.once or trigger.available_now:\n                    should_wait = True\n\n            if should_wait:\n                ctx.info(\n                    \"Awaiting streaming query termination\",\n                    timeout_seconds=streaming_config.timeout_seconds,\n                )\n                query.awaitTermination(streaming_config.timeout_seconds)\n                result[\"status\"] = \"terminated\"\n                elapsed = (time.time() - start_time) * 1000\n                result[\"elapsed_ms\"] = round(elapsed, 2)\n\n                # Get rows written from streaming query progress\n                rows_written = None\n\n                # Method 1: Sum all micro-batch progress (most accurate for streaming)\n                # For autoloader, numInputRows = rows read from source files in this run\n                try:\n                    recent_progress = query.recentProgress\n                    if recent_progress:\n                        total_rows = 0\n                        for prog in recent_progress:\n                            if prog:\n                                # Use numInputRows - this is rows processed from source\n                                # NOT sink.numOutputRows which can be cumulative\n                                input_rows = prog.get(\"numInputRows\")\n                                if input_rows is not None:\n                                    total_rows += input_rows\n                        if total_rows &gt; 0:\n                            rows_written = total_rows\n                except Exception:\n                    pass\n\n                # Method 1b: Try lastProgress if recentProgress is empty\n                if rows_written is None:\n                    try:\n                        last_progress = query.lastProgress\n                        if last_progress:\n                            input_rows = last_progress.get(\"numInputRows\")\n                            if input_rows is not None and input_rows &gt; 0:\n                                rows_written = input_rows\n                    except Exception:\n                        pass\n\n                # Method 2: Check Delta table history for rows written in THIS run only\n                # Compare version before/after to detect if new data was written\n                if rows_written is None and path and format == \"delta\":\n                    try:\n                        full_path = connection.get_path(path)\n                        history_df = self.spark.sql(f\"DESCRIBE HISTORY delta.`{full_path}` LIMIT 1\")\n                        rows = history_df.collect()\n                        if rows:\n                            row = rows[0]\n                            version_after = (\n                                row.version if hasattr(row, \"version\") else row[\"version\"]\n                            )\n\n                            # If version hasn't changed, no new data was written\n                            if version_before is not None and version_after == version_before:\n                                rows_written = 0\n                            else:\n                                # New version created - get the row count from metrics\n                                metrics = (\n                                    row.operationMetrics\n                                    if hasattr(row, \"operationMetrics\")\n                                    else row[\"operationMetrics\"]\n                                )\n                                if metrics:\n                                    # For streaming: numAddedRows is rows added in this batch\n                                    added_rows = metrics.get(\"numAddedRows\")\n                                    if added_rows is not None:\n                                        rows_written = int(added_rows)\n                                    else:\n                                        # Fallback to numOutputRows\n                                        output_rows = metrics.get(\"numOutputRows\")\n                                        if output_rows is not None:\n                                            rows_written = int(output_rows)\n                    except Exception:\n                        pass\n\n                if rows_written is not None:\n                    result[\"_cached_row_count\"] = rows_written\n\n                ctx.info(\n                    \"Streaming query terminated\",\n                    query_id=str(query.id),\n                    elapsed_ms=round(elapsed, 2),\n                    rows_written=rows_written,\n                )\n\n                if register_table and path and format == \"delta\":\n                    full_path = connection.get_path(path)\n                    try:\n                        self.spark.sql(\n                            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                            f\"USING DELTA LOCATION '{full_path}'\"\n                        )\n                        ctx.info(\n                            f\"Registered external table: {register_table}\",\n                            path=full_path,\n                        )\n                        result[\"registered_table\"] = register_table\n                    except Exception as reg_err:\n                        ctx.warning(\n                            f\"Failed to register external table '{register_table}'\",\n                            error=str(reg_err),\n                        )\n            else:\n                result[\"streaming_query\"] = query\n                if register_table:\n                    ctx.warning(\n                        \"register_table ignored for continuous streaming. \"\n                        \"Table will be registered after query terminates or manually.\"\n                    )\n\n            return result\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Streaming write failed\",\n                target=target_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n        finally:\n            # Restore original session configs\n            for conf_key, original_value in original_configs.items():\n                try:\n                    if original_value is None:\n                        self.spark.conf.unset(conf_key)\n                    else:\n                        self.spark.conf.set(conf_key, original_value)\n                except Exception:\n                    pass  # Best effort cleanup\n\n    def execute_sql(self, sql: str, context: Any = None) -&gt; Any:\n        \"\"\"Execute SQL query in Spark.\n\n        Args:\n            sql: SQL query string\n            context: Execution context (optional, not used for Spark)\n\n        Returns:\n            Spark DataFrame with query results\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Executing Spark SQL\", query_preview=sql[:200] if len(sql) &gt; 200 else sql)\n\n        try:\n            result = self.spark.sql(sql)\n            elapsed = (time.time() - start_time) * 1000\n            partition_count = result.rdd.getNumPartitions()\n\n            ctx.log_spark_metrics(partition_count=partition_count)\n            ctx.info(\n                \"Spark SQL executed\",\n                elapsed_ms=round(elapsed, 2),\n                partitions=partition_count,\n            )\n\n            return result\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            error_type = type(e).__name__\n            clean_message = _extract_spark_error_message(e)\n\n            if \"AnalysisException\" in error_type:\n                ctx.error(\n                    \"Spark SQL Analysis Error\",\n                    error_type=error_type,\n                    error_message=clean_message,\n                    query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise TransformError(f\"Spark SQL Analysis Error: {clean_message}\")\n\n            if \"ParseException\" in error_type:\n                ctx.error(\n                    \"Spark SQL Parse Error\",\n                    error_type=error_type,\n                    error_message=clean_message,\n                    query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise TransformError(f\"Spark SQL Parse Error: {clean_message}\")\n\n            ctx.error(\n                \"Spark SQL execution failed\",\n                error_type=error_type,\n                error_message=clean_message,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Error: {clean_message}\")\n\n    def execute_transform(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"SparkEngine.execute_transform() will be implemented in Phase 2B. \"\n            \"See PHASES.md for implementation plan.\"\n        )\n\n    def execute_operation(self, operation: str, params: Dict[str, Any], df) -&gt; Any:\n        \"\"\"Execute built-in operation on Spark DataFrame.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        params = params or {}\n\n        ctx.debug(f\"Executing operation: {operation}\", params=list(params.keys()))\n\n        if operation == \"pivot\":\n            group_by = params.get(\"group_by\", [])\n            pivot_column = params.get(\"pivot_column\")\n            value_column = params.get(\"value_column\")\n            agg_func = params.get(\"agg_func\", \"first\")\n\n            if not pivot_column or not value_column:\n                ctx.error(\"Pivot requires 'pivot_column' and 'value_column'\")\n                raise ValueError(\"Pivot requires 'pivot_column' and 'value_column'\")\n\n            if isinstance(group_by, str):\n                group_by = [group_by]\n\n            agg_expr = {value_column: agg_func}\n            return df.groupBy(*group_by).pivot(pivot_column).agg(agg_expr)\n\n        elif operation == \"drop_duplicates\":\n            subset = params.get(\"subset\")\n            if subset:\n                if isinstance(subset, str):\n                    subset = [subset]\n                return df.dropDuplicates(subset=subset)\n            return df.dropDuplicates()\n\n        elif operation == \"fillna\":\n            value = params.get(\"value\")\n            subset = params.get(\"subset\")\n            return df.fillna(value, subset=subset)\n\n        elif operation == \"drop\":\n            columns = params.get(\"columns\")\n            if not columns:\n                return df\n            if isinstance(columns, str):\n                columns = [columns]\n            return df.drop(*columns)\n\n        elif operation == \"rename\":\n            columns = params.get(\"columns\")\n            if not columns:\n                return df\n\n            res = df\n            for old_name, new_name in columns.items():\n                res = res.withColumnRenamed(old_name, new_name)\n            return res\n\n        elif operation == \"sort\":\n            by = params.get(\"by\")\n            ascending = params.get(\"ascending\", True)\n\n            if not by:\n                return df\n\n            if isinstance(by, str):\n                by = [by]\n\n            if not ascending:\n                from pyspark.sql.functions import desc\n\n                sort_cols = [desc(c) for c in by]\n                return df.orderBy(*sort_cols)\n\n            return df.orderBy(*by)\n\n        elif operation == \"sample\":\n            fraction = params.get(\"frac\", 0.1)\n            seed = params.get(\"random_state\")\n            with_replacement = params.get(\"replace\", False)\n            return df.sample(withReplacement=with_replacement, fraction=fraction, seed=seed)\n\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext\n            from odibi.registry import FunctionRegistry\n\n            ctx.debug(\n                f\"Checking registry for operation: {operation}\",\n                registered_functions=list(FunctionRegistry._functions.keys())[:10],\n                has_function=FunctionRegistry.has_function(operation),\n            )\n\n            if FunctionRegistry.has_function(operation):\n                ctx.debug(f\"Executing registered transformer as operation: {operation}\")\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df\n                from odibi.context import SparkContext\n\n                engine_ctx = EngineContext(\n                    context=SparkContext(self.spark),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n            ctx.error(f\"Unsupported operation for Spark engine: {operation}\")\n            raise ValueError(f\"Unsupported operation for Spark engine: {operation}\")\n\n    def count_nulls(self, df, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\"\"\"\n        from pyspark.sql.functions import col, count, when\n\n        missing = set(columns) - set(df.columns)\n        if missing:\n            raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing)}\")\n\n        # Escape column names with backticks to handle special characters (., ,, spaces)\n        aggs = [count(when(col(f\"`{c}`\").isNull(), c)).alias(c) for c in columns]\n        result = df.select(*aggs).collect()[0].asDict()\n        return result\n\n    def validate_schema(self, df, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\"\"\"\n        failures = []\n\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(df.columns)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        if \"types\" in schema_rules:\n            type_map = {\n                \"int\": [\"integer\", \"long\", \"short\", \"byte\", \"bigint\"],\n                \"float\": [\"double\", \"float\"],\n                \"str\": [\"string\"],\n                \"bool\": [\"boolean\"],\n            }\n\n            for col_name, expected_type in schema_rules[\"types\"].items():\n                if col_name not in df.columns:\n                    failures.append(f\"Column '{col_name}' not found for type validation\")\n                    continue\n\n                actual_type = dict(df.dtypes)[col_name]\n                expected_dtypes = type_map.get(expected_type, [expected_type])\n\n                if actual_type not in expected_dtypes:\n                    failures.append(\n                        f\"Column '{col_name}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def validate_data(self, df, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate DataFrame against rules.\"\"\"\n        from pyspark.sql.functions import col\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        failures = []\n\n        if validation_config.not_empty:\n            if df.isEmpty():\n                failures.append(\"DataFrame is empty\")\n\n        if validation_config.no_nulls:\n            null_counts = self.count_nulls(df, validation_config.no_nulls)\n            for col_name, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col_name}' has {count} null values\")\n\n        if validation_config.schema_validation:\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        if validation_config.ranges:\n            for col_name, bounds in validation_config.ranges.items():\n                if col_name in df.columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    # Escape column names with backticks to handle special characters\n                    escaped_col = f\"`{col_name}`\"\n                    if min_val is not None:\n                        count = df.filter(col(escaped_col) &lt; min_val).count()\n                        if count &gt; 0:\n                            failures.append(f\"Column '{col_name}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        count = df.filter(col(escaped_col) &gt; max_val).count()\n                        if count &gt; 0:\n                            failures.append(f\"Column '{col_name}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col_name}' not found for range validation\")\n\n        if validation_config.allowed_values:\n            for col_name, allowed in validation_config.allowed_values.items():\n                if col_name in df.columns:\n                    # Escape column names with backticks to handle special characters\n                    escaped_col = f\"`{col_name}`\"\n                    count = df.filter(~col(escaped_col).isin(allowed)).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col_name}' not found for allowed values validation\")\n\n        ctx.log_validation_result(\n            passed=len(failures) == 0,\n            rule_name=\"data_validation\",\n            failures=failures if failures else None,\n        )\n\n        return failures\n\n    def get_sample(self, df, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\"\"\"\n        return [row.asDict() for row in df.limit(n).collect()]\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\n\n        Handles orphan catalog entries where the table is registered but\n        the underlying Delta path no longer exists.\n        \"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        if table:\n            try:\n                if not self.spark.catalog.tableExists(table):\n                    ctx.debug(f\"Table does not exist: {table}\")\n                    return False\n                # Table exists in catalog - verify it's actually readable\n                # This catches orphan entries where path was deleted\n                # Use limit(1) not limit(0) - limit(0) can succeed from metadata alone\n                self.spark.table(table).limit(1).collect()\n                ctx.debug(f\"Table existence check: {table}\", exists=True)\n                return True\n            except Exception as e:\n                # Table exists in catalog but underlying data is gone (orphan entry)\n                # This is expected during first-run detection - log at debug level\n                ctx.debug(\n                    f\"Table {table} exists in catalog but is not accessible (treating as first run)\",\n                    error_message=str(e),\n                )\n                return False\n        elif path:\n            try:\n                from delta.tables import DeltaTable\n\n                full_path = connection.get_path(path)\n                exists = DeltaTable.isDeltaTable(self.spark, full_path)\n                ctx.debug(f\"Delta table existence check: {path}\", exists=exists)\n                return exists\n            except ImportError:\n                try:\n                    full_path = connection.get_path(path)\n                    exists = (\n                        self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(\n                            self.spark.sparkContext._jsc.hadoopConfiguration()\n                        ).exists(\n                            self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path(\n                                full_path\n                            )\n                        )\n                    )\n                    ctx.debug(f\"Path existence check: {path}\", exists=exists)\n                    return exists\n                except Exception as e:\n                    ctx.warning(f\"Path existence check failed: {path}\", error_message=str(e))\n                    return False\n            except Exception as e:\n                ctx.warning(f\"Table existence check failed: {path}\", error_message=str(e))\n                return False\n        return False\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n\n        try:\n            if table:\n                if self.spark.catalog.tableExists(table):\n                    schema = self.get_schema(self.spark.table(table))\n                    ctx.debug(f\"Retrieved schema for table: {table}\", columns=len(schema))\n                    return schema\n            elif path:\n                full_path = connection.get_path(path)\n                if format == \"delta\":\n                    from delta.tables import DeltaTable\n\n                    if DeltaTable.isDeltaTable(self.spark, full_path):\n                        schema = self.get_schema(DeltaTable.forPath(self.spark, full_path).toDF())\n                        ctx.debug(f\"Retrieved Delta schema: {path}\", columns=len(schema))\n                        return schema\n                elif format == \"parquet\":\n                    schema = self.get_schema(self.spark.read.parquet(full_path))\n                    ctx.debug(f\"Retrieved Parquet schema: {path}\", columns=len(schema))\n                    return schema\n                elif format:\n                    schema = self.get_schema(self.spark.read.format(format).load(full_path))\n                    ctx.debug(f\"Retrieved schema: {path}\", format=format, columns=len(schema))\n                    return schema\n        except Exception as e:\n            ctx.warning(\n                \"Failed to get schema\",\n                table=table,\n                path=path,\n                error_message=str(e),\n            )\n        return None\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n    ) -&gt; None:\n        \"\"\"VACUUM a Delta table to remove old files.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n        )\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            delta_table.vacuum(retention_hours / 24.0)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta VACUUM completed\",\n                path=path,\n                retention_hours=retention_hours,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Delta VACUUM failed\",\n                path=path,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Fetching Delta history\", path=path, limit=limit)\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            history_df = delta_table.history(limit) if limit else delta_table.history()\n            history = [row.asDict() for row in history_df.collect()]\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta history retrieved\",\n                path=path,\n                versions_returned=len(history),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n            return history\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Failed to get Delta history\",\n                path=path,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n        \"\"\"Restore Delta table to a specific version.\"\"\"\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        ctx.debug(\"Restoring Delta table\", path=path, version=version)\n\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[spark]' \"\n                \"with delta-spark. \"\n                \"See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path)\n\n        try:\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            delta_table.restoreToVersion(version)\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Delta table restored\",\n                path=path,\n                version=version,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"Delta restore failed\",\n                path=path,\n                version=version,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n        if format != \"delta\" or not config or not config.enabled:\n            return\n\n        ctx = get_logging_context().with_context(engine=\"spark\")\n        start_time = time.time()\n\n        if table:\n            target = table\n        elif path:\n            full_path = connection.get_path(path)\n            target = f\"delta.`{full_path}`\"\n        else:\n            return\n\n        ctx.debug(\"Starting table maintenance\", target=target)\n\n        try:\n            ctx.debug(f\"Running OPTIMIZE on {target}\")\n            self.spark.sql(f\"OPTIMIZE {target}\")\n\n            retention = config.vacuum_retention_hours\n            if retention is not None and retention &gt; 0:\n                ctx.debug(f\"Running VACUUM on {target}\", retention_hours=retention)\n                self.spark.sql(f\"VACUUM {target} RETAIN {retention} HOURS\")\n\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                target=target,\n                vacuum_retention_hours=retention,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.warning(\n                f\"Auto-optimize failed for {target}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n    def get_source_files(self, df) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\"\"\"\n        try:\n            return df.inputFiles()\n        except Exception:\n            return []\n\n    def profile_nulls(self, df) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\"\"\"\n        from pyspark.sql.functions import col, mean, when\n\n        aggs = []\n        for c in df.columns:\n            # Escape column names with backticks to handle special characters (., ,, spaces)\n            escaped_col = f\"`{c}`\"\n            aggs.append(mean(when(col(escaped_col).isNull(), 1).otherwise(0)).alias(c))\n\n        if not aggs:\n            return {}\n\n        try:\n            result = df.select(*aggs).collect()[0].asDict()\n            return result\n        except Exception:\n            return {}\n\n    def filter_greater_than(self, df, column: str, value: Any) -&gt; Any:\n        \"\"\"Filter DataFrame where column &gt; value.\n\n        Automatically casts string columns to timestamp for proper comparison.\n        Tries multiple date formats including Oracle-style (DD-MON-YY).\n        \"\"\"\n        from pyspark.sql import functions as F\n        from pyspark.sql.types import StringType\n\n        # Escape column names with backticks to handle special characters\n        escaped_col = f\"`{column}`\"\n        col_type = df.schema[column].dataType\n        if isinstance(col_type, StringType):\n            ts_col = self._parse_string_to_timestamp(F.col(escaped_col))\n            return df.filter(ts_col &gt; value)\n        return df.filter(F.col(escaped_col) &gt; value)\n\n    def _parse_string_to_timestamp(self, col):\n        \"\"\"Parse string column to timestamp, trying multiple formats.\n\n        Supports:\n        - ISO format: 2024-04-20 07:11:01\n        - Oracle format: 20-APR-24 07:11:01.0 (handles uppercase months)\n        \"\"\"\n        from pyspark.sql import functions as F\n\n        result = F.to_timestamp(col)\n\n        result = F.coalesce(result, F.to_timestamp(col, \"yyyy-MM-dd HH:mm:ss\"))\n        result = F.coalesce(result, F.to_timestamp(col, \"yyyy-MM-dd'T'HH:mm:ss\"))\n        result = F.coalesce(result, F.to_timestamp(col, \"MM/dd/yyyy HH:mm:ss\"))\n\n        col_oracle = F.concat(\n            F.substring(col, 1, 3),\n            F.upper(F.substring(col, 4, 1)),\n            F.lower(F.substring(col, 5, 2)),\n            F.substring(col, 7, 100),\n        )\n        result = F.coalesce(result, F.to_timestamp(col_oracle, \"dd-MMM-yy HH:mm:ss.S\"))\n        result = F.coalesce(result, F.to_timestamp(col_oracle, \"dd-MMM-yy HH:mm:ss\"))\n\n        return result\n\n    def filter_coalesce(self, df, col1: str, col2: str, op: str, value: Any) -&gt; Any:\n        \"\"\"Filter using COALESCE(col1, col2) op value.\n\n        Automatically casts string columns to timestamp for proper comparison.\n        Tries multiple date formats including Oracle-style (DD-MON-YY).\n        \"\"\"\n        from pyspark.sql import functions as F\n        from pyspark.sql.types import StringType\n\n        # Escape column names with backticks to handle special characters\n        escaped_col1 = f\"`{col1}`\"\n        escaped_col2 = f\"`{col2}`\"\n        col1_type = df.schema[col1].dataType\n        col2_type = df.schema[col2].dataType\n\n        if isinstance(col1_type, StringType):\n            c1 = self._parse_string_to_timestamp(F.col(escaped_col1))\n        else:\n            c1 = F.col(escaped_col1)\n\n        if isinstance(col2_type, StringType):\n            c2 = self._parse_string_to_timestamp(F.col(escaped_col2))\n        else:\n            c2 = F.col(escaped_col2)\n\n        coalesced = F.coalesce(c1, c2)\n\n        if op == \"&gt;\":\n            return df.filter(coalesced &gt; value)\n        elif op == \"&gt;=\":\n            return df.filter(coalesced &gt;= value)\n        elif op == \"&lt;\":\n            return df.filter(coalesced &lt; value)\n        elif op == \"&lt;=\":\n            return df.filter(coalesced &lt;= value)\n        elif op == \"=\":\n            return df.filter(coalesced == value)\n        else:\n            return df.filter(f\"COALESCE({col1}, {col2}) {op} '{value}'\")\n\n    def add_write_metadata(\n        self,\n        df: Any,\n        metadata_config: Any,\n        source_connection: Optional[str] = None,\n        source_table: Optional[str] = None,\n        source_path: Optional[str] = None,\n        is_file_source: bool = False,\n    ) -&gt; Any:\n        \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n        Args:\n            df: Spark DataFrame\n            metadata_config: WriteMetadataConfig or True (for all defaults)\n            source_connection: Name of the source connection\n            source_table: Name of the source table (SQL sources)\n            source_path: Path of the source file (file sources)\n            is_file_source: True if source is a file-based read\n\n        Returns:\n            DataFrame with metadata columns added\n        \"\"\"\n        from pyspark.sql import functions as F\n\n        from odibi.config import WriteMetadataConfig\n\n        if metadata_config is True:\n            config = WriteMetadataConfig()\n        elif isinstance(metadata_config, WriteMetadataConfig):\n            config = metadata_config\n        else:\n            return df\n\n        if config.extracted_at:\n            df = df.withColumn(\"_extracted_at\", F.current_timestamp())\n\n        if config.source_file and is_file_source and source_path:\n            df = df.withColumn(\"_source_file\", F.lit(source_path))\n\n        if config.source_connection and source_connection:\n            df = df.withColumn(\"_source_connection\", F.lit(source_connection))\n\n        if config.source_table and source_table:\n            df = df.withColumn(\"_source_table\", F.lit(source_table))\n\n        return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.__init__","title":"<code>__init__(connections=None, spark_session=None, config=None)</code>","text":"<p>Initialize Spark engine with import guard.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects (for multi-account config)</p> <code>None</code> <code>spark_session</code> <code>Any</code> <p>Existing SparkSession (optional, creates new if None)</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pyspark not installed</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    spark_session: Any = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Spark engine with import guard.\n\n    Args:\n        connections: Dictionary of connection objects (for multi-account config)\n        spark_session: Existing SparkSession (optional, creates new if None)\n        config: Engine configuration (optional)\n\n    Raises:\n        ImportError: If pyspark not installed\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    ctx.debug(\"Initializing SparkEngine\", connections_count=len(connections or {}))\n\n    try:\n        from pyspark.sql import SparkSession\n    except ImportError as e:\n        ctx.error(\n            \"PySpark not installed\",\n            error_type=\"ImportError\",\n            suggestion=\"pip install odibi[spark]\",\n        )\n        raise ImportError(\n            \"Spark support requires 'pip install odibi[spark]'. \"\n            \"See docs/setup_databricks.md for setup instructions.\"\n        ) from e\n\n    start_time = time.time()\n\n    # Configure Delta Lake support\n    try:\n        from delta import configure_spark_with_delta_pip\n\n        builder = SparkSession.builder.appName(\"odibi\").config(\n            \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n        )\n\n        # Performance Optimizations\n        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n        # Reduce Verbosity\n        builder = builder.config(\n            \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n        builder = builder.config(\n            \"spark.executor.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n\n        self.spark = spark_session or configure_spark_with_delta_pip(builder).getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n        ctx.debug(\"Delta Lake support enabled\")\n\n    except ImportError:\n        ctx.debug(\"Delta Lake not available, using standard Spark\")\n        builder = SparkSession.builder.appName(\"odibi\").config(\n            \"spark.sql.sources.partitionOverwriteMode\", \"dynamic\"\n        )\n\n        # Performance Optimizations\n        builder = builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        builder = builder.config(\"spark.sql.adaptive.enabled\", \"true\")\n\n        # Reduce Verbosity\n        builder = builder.config(\n            \"spark.driver.extraJavaOptions\", \"-Dlog4j.rootCategory=ERROR, console\"\n        )\n\n        self.spark = spark_session or builder.getOrCreate()\n        self.spark.sparkContext.setLogLevel(\"ERROR\")\n\n    self.config = config or {}\n    self.connections = connections or {}\n\n    # Configure all ADLS connections upfront\n    self._configure_all_connections()\n\n    # Apply user-defined Spark configs from performance settings\n    self._apply_spark_config()\n\n    elapsed = (time.time() - start_time) * 1000\n    ctx.info(\n        \"SparkEngine initialized\",\n        elapsed_ms=round(elapsed, 2),\n        app_name=self.spark.sparkContext.appName,\n        spark_version=self.spark.version,\n        connections_configured=len(self.connections),\n        using_existing_session=spark_session is not None,\n    )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.add_write_metadata","title":"<code>add_write_metadata(df, metadata_config, source_connection=None, source_table=None, source_path=None, is_file_source=False)</code>","text":"<p>Add metadata columns to DataFrame before writing (Bronze layer lineage).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Spark DataFrame</p> required <code>metadata_config</code> <code>Any</code> <p>WriteMetadataConfig or True (for all defaults)</p> required <code>source_connection</code> <code>Optional[str]</code> <p>Name of the source connection</p> <code>None</code> <code>source_table</code> <code>Optional[str]</code> <p>Name of the source table (SQL sources)</p> <code>None</code> <code>source_path</code> <code>Optional[str]</code> <p>Path of the source file (file sources)</p> <code>None</code> <code>is_file_source</code> <code>bool</code> <p>True if source is a file-based read</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with metadata columns added</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def add_write_metadata(\n    self,\n    df: Any,\n    metadata_config: Any,\n    source_connection: Optional[str] = None,\n    source_table: Optional[str] = None,\n    source_path: Optional[str] = None,\n    is_file_source: bool = False,\n) -&gt; Any:\n    \"\"\"Add metadata columns to DataFrame before writing (Bronze layer lineage).\n\n    Args:\n        df: Spark DataFrame\n        metadata_config: WriteMetadataConfig or True (for all defaults)\n        source_connection: Name of the source connection\n        source_table: Name of the source table (SQL sources)\n        source_path: Path of the source file (file sources)\n        is_file_source: True if source is a file-based read\n\n    Returns:\n        DataFrame with metadata columns added\n    \"\"\"\n    from pyspark.sql import functions as F\n\n    from odibi.config import WriteMetadataConfig\n\n    if metadata_config is True:\n        config = WriteMetadataConfig()\n    elif isinstance(metadata_config, WriteMetadataConfig):\n        config = metadata_config\n    else:\n        return df\n\n    if config.extracted_at:\n        df = df.withColumn(\"_extracted_at\", F.current_timestamp())\n\n    if config.source_file and is_file_source and source_path:\n        df = df.withColumn(\"_source_file\", F.lit(source_path))\n\n    if config.source_connection and source_connection:\n        df = df.withColumn(\"_source_connection\", F.lit(source_connection))\n\n    if config.source_table and source_table:\n        df = df.withColumn(\"_source_table\", F.lit(source_table))\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize columns using Spark functions.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def anonymize(self, df, columns: List[str], method: str, salt: Optional[str] = None):\n    \"\"\"Anonymize columns using Spark functions.\"\"\"\n    from pyspark.sql.functions import col, concat, lit, regexp_replace, sha2\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    ctx.debug(\n        \"Anonymizing columns\",\n        columns=columns,\n        method=method,\n        has_salt=salt is not None,\n    )\n\n    res = df\n    for c in columns:\n        if c not in df.columns:\n            ctx.warning(f\"Column '{c}' not found for anonymization, skipping\", column=c)\n            continue\n\n        # Escape column names with backticks to handle special characters\n        escaped_col = f\"`{c}`\"\n        if method == \"hash\":\n            if salt:\n                res = res.withColumn(c, sha2(concat(col(escaped_col), lit(salt)), 256))\n            else:\n                res = res.withColumn(c, sha2(col(escaped_col), 256))\n\n        elif method == \"mask\":\n            res = res.withColumn(c, regexp_replace(col(escaped_col), \".(?=.{4})\", \"*\"))\n\n        elif method == \"redact\":\n            res = res.withColumn(c, lit(\"[REDACTED]\"))\n\n    ctx.debug(f\"Anonymization completed using {method}\")\n    return res\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def count_nulls(self, df, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\"\"\"\n    from pyspark.sql.functions import col, count, when\n\n    missing = set(columns) - set(df.columns)\n    if missing:\n        raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing)}\")\n\n    # Escape column names with backticks to handle special characters (., ,, spaces)\n    aggs = [count(when(col(f\"`{c}`\").isNull(), c)).alias(c) for c in columns]\n    result = df.select(*aggs).collect()[0].asDict()\n    return result\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def count_rows(self, df) -&gt; int:\n    \"\"\"Count rows in DataFrame.\"\"\"\n    return df.count()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation on Spark DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def execute_operation(self, operation: str, params: Dict[str, Any], df) -&gt; Any:\n    \"\"\"Execute built-in operation on Spark DataFrame.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    params = params or {}\n\n    ctx.debug(f\"Executing operation: {operation}\", params=list(params.keys()))\n\n    if operation == \"pivot\":\n        group_by = params.get(\"group_by\", [])\n        pivot_column = params.get(\"pivot_column\")\n        value_column = params.get(\"value_column\")\n        agg_func = params.get(\"agg_func\", \"first\")\n\n        if not pivot_column or not value_column:\n            ctx.error(\"Pivot requires 'pivot_column' and 'value_column'\")\n            raise ValueError(\"Pivot requires 'pivot_column' and 'value_column'\")\n\n        if isinstance(group_by, str):\n            group_by = [group_by]\n\n        agg_expr = {value_column: agg_func}\n        return df.groupBy(*group_by).pivot(pivot_column).agg(agg_expr)\n\n    elif operation == \"drop_duplicates\":\n        subset = params.get(\"subset\")\n        if subset:\n            if isinstance(subset, str):\n                subset = [subset]\n            return df.dropDuplicates(subset=subset)\n        return df.dropDuplicates()\n\n    elif operation == \"fillna\":\n        value = params.get(\"value\")\n        subset = params.get(\"subset\")\n        return df.fillna(value, subset=subset)\n\n    elif operation == \"drop\":\n        columns = params.get(\"columns\")\n        if not columns:\n            return df\n        if isinstance(columns, str):\n            columns = [columns]\n        return df.drop(*columns)\n\n    elif operation == \"rename\":\n        columns = params.get(\"columns\")\n        if not columns:\n            return df\n\n        res = df\n        for old_name, new_name in columns.items():\n            res = res.withColumnRenamed(old_name, new_name)\n        return res\n\n    elif operation == \"sort\":\n        by = params.get(\"by\")\n        ascending = params.get(\"ascending\", True)\n\n        if not by:\n            return df\n\n        if isinstance(by, str):\n            by = [by]\n\n        if not ascending:\n            from pyspark.sql.functions import desc\n\n            sort_cols = [desc(c) for c in by]\n            return df.orderBy(*sort_cols)\n\n        return df.orderBy(*by)\n\n    elif operation == \"sample\":\n        fraction = params.get(\"frac\", 0.1)\n        seed = params.get(\"random_state\")\n        with_replacement = params.get(\"replace\", False)\n        return df.sample(withReplacement=with_replacement, fraction=fraction, seed=seed)\n\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext\n        from odibi.registry import FunctionRegistry\n\n        ctx.debug(\n            f\"Checking registry for operation: {operation}\",\n            registered_functions=list(FunctionRegistry._functions.keys())[:10],\n            has_function=FunctionRegistry.has_function(operation),\n        )\n\n        if FunctionRegistry.has_function(operation):\n            ctx.debug(f\"Executing registered transformer as operation: {operation}\")\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df\n            from odibi.context import SparkContext\n\n            engine_ctx = EngineContext(\n                context=SparkContext(self.spark),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n        ctx.error(f\"Unsupported operation for Spark engine: {operation}\")\n        raise ValueError(f\"Unsupported operation for Spark engine: {operation}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.execute_sql","title":"<code>execute_sql(sql, context=None)</code>","text":"<p>Execute SQL query in Spark.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Any</code> <p>Execution context (optional, not used for Spark)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Spark DataFrame with query results</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Any = None) -&gt; Any:\n    \"\"\"Execute SQL query in Spark.\n\n    Args:\n        sql: SQL query string\n        context: Execution context (optional, not used for Spark)\n\n    Returns:\n        Spark DataFrame with query results\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Executing Spark SQL\", query_preview=sql[:200] if len(sql) &gt; 200 else sql)\n\n    try:\n        result = self.spark.sql(sql)\n        elapsed = (time.time() - start_time) * 1000\n        partition_count = result.rdd.getNumPartitions()\n\n        ctx.log_spark_metrics(partition_count=partition_count)\n        ctx.info(\n            \"Spark SQL executed\",\n            elapsed_ms=round(elapsed, 2),\n            partitions=partition_count,\n        )\n\n        return result\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        error_type = type(e).__name__\n        clean_message = _extract_spark_error_message(e)\n\n        if \"AnalysisException\" in error_type:\n            ctx.error(\n                \"Spark SQL Analysis Error\",\n                error_type=error_type,\n                error_message=clean_message,\n                query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Analysis Error: {clean_message}\")\n\n        if \"ParseException\" in error_type:\n            ctx.error(\n                \"Spark SQL Parse Error\",\n                error_type=error_type,\n                error_message=clean_message,\n                query_preview=sql[:200] if len(sql) &gt; 200 else sql,\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise TransformError(f\"Spark SQL Parse Error: {clean_message}\")\n\n        ctx.error(\n            \"Spark SQL execution failed\",\n            error_type=error_type,\n            error_message=clean_message,\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise TransformError(f\"Spark SQL Error: {clean_message}\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.filter_coalesce","title":"<code>filter_coalesce(df, col1, col2, op, value)</code>","text":"<p>Filter using COALESCE(col1, col2) op value.</p> <p>Automatically casts string columns to timestamp for proper comparison. Tries multiple date formats including Oracle-style (DD-MON-YY).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def filter_coalesce(self, df, col1: str, col2: str, op: str, value: Any) -&gt; Any:\n    \"\"\"Filter using COALESCE(col1, col2) op value.\n\n    Automatically casts string columns to timestamp for proper comparison.\n    Tries multiple date formats including Oracle-style (DD-MON-YY).\n    \"\"\"\n    from pyspark.sql import functions as F\n    from pyspark.sql.types import StringType\n\n    # Escape column names with backticks to handle special characters\n    escaped_col1 = f\"`{col1}`\"\n    escaped_col2 = f\"`{col2}`\"\n    col1_type = df.schema[col1].dataType\n    col2_type = df.schema[col2].dataType\n\n    if isinstance(col1_type, StringType):\n        c1 = self._parse_string_to_timestamp(F.col(escaped_col1))\n    else:\n        c1 = F.col(escaped_col1)\n\n    if isinstance(col2_type, StringType):\n        c2 = self._parse_string_to_timestamp(F.col(escaped_col2))\n    else:\n        c2 = F.col(escaped_col2)\n\n    coalesced = F.coalesce(c1, c2)\n\n    if op == \"&gt;\":\n        return df.filter(coalesced &gt; value)\n    elif op == \"&gt;=\":\n        return df.filter(coalesced &gt;= value)\n    elif op == \"&lt;\":\n        return df.filter(coalesced &lt; value)\n    elif op == \"&lt;=\":\n        return df.filter(coalesced &lt;= value)\n    elif op == \"=\":\n        return df.filter(coalesced == value)\n    else:\n        return df.filter(f\"COALESCE({col1}, {col2}) {op} '{value}'\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.filter_greater_than","title":"<code>filter_greater_than(df, column, value)</code>","text":"<p>Filter DataFrame where column &gt; value.</p> <p>Automatically casts string columns to timestamp for proper comparison. Tries multiple date formats including Oracle-style (DD-MON-YY).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def filter_greater_than(self, df, column: str, value: Any) -&gt; Any:\n    \"\"\"Filter DataFrame where column &gt; value.\n\n    Automatically casts string columns to timestamp for proper comparison.\n    Tries multiple date formats including Oracle-style (DD-MON-YY).\n    \"\"\"\n    from pyspark.sql import functions as F\n    from pyspark.sql.types import StringType\n\n    # Escape column names with backticks to handle special characters\n    escaped_col = f\"`{column}`\"\n    col_type = df.schema[column].dataType\n    if isinstance(col_type, StringType):\n        ts_col = self._parse_string_to_timestamp(F.col(escaped_col))\n        return df.filter(ts_col &gt; value)\n    return df.filter(F.col(escaped_col) &gt; value)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Fetching Delta history\", path=path, limit=limit)\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        history_df = delta_table.history(limit) if limit else delta_table.history()\n        history = [row.asDict() for row in history_df.collect()]\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=path,\n            versions_returned=len(history),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Failed to get Delta history\",\n            path=path,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_sample(self, df, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\"\"\"\n    return [row.asDict() for row in df.limit(n).collect()]\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema with types.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_schema(self, df) -&gt; Dict[str, str]:\n    \"\"\"Get DataFrame schema with types.\"\"\"\n    return {f.name: f.dataType.simpleString() for f in df.schema}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape as (rows, columns).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_shape(self, df) -&gt; Tuple[int, int]:\n    \"\"\"Get DataFrame shape as (rows, columns).\"\"\"\n    return (df.count(), len(df.columns))\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_source_files(self, df) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\"\"\"\n    try:\n        return df.inputFiles()\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    try:\n        if table:\n            if self.spark.catalog.tableExists(table):\n                schema = self.get_schema(self.spark.table(table))\n                ctx.debug(f\"Retrieved schema for table: {table}\", columns=len(schema))\n                return schema\n        elif path:\n            full_path = connection.get_path(path)\n            if format == \"delta\":\n                from delta.tables import DeltaTable\n\n                if DeltaTable.isDeltaTable(self.spark, full_path):\n                    schema = self.get_schema(DeltaTable.forPath(self.spark, full_path).toDF())\n                    ctx.debug(f\"Retrieved Delta schema: {path}\", columns=len(schema))\n                    return schema\n            elif format == \"parquet\":\n                schema = self.get_schema(self.spark.read.parquet(full_path))\n                ctx.debug(f\"Retrieved Parquet schema: {path}\", columns=len(schema))\n                return schema\n            elif format:\n                schema = self.get_schema(self.spark.read.format(format).load(full_path))\n                ctx.debug(f\"Retrieved schema: {path}\", format=format, columns=len(schema))\n                return schema\n    except Exception as e:\n        ctx.warning(\n            \"Failed to get schema\",\n            table=table,\n            path=path,\n            error_message=str(e),\n        )\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema with target schema according to policy.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def harmonize_schema(self, df, target_schema: Dict[str, str], policy: Any):\n    \"\"\"Harmonize DataFrame schema with target schema according to policy.\"\"\"\n    from pyspark.sql.functions import col, lit\n\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    target_cols = list(target_schema.keys())\n    current_cols = df.columns\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    ctx.debug(\n        \"Schema harmonization\",\n        target_columns=len(target_cols),\n        current_columns=len(current_cols),\n        missing_columns=list(missing) if missing else None,\n        new_columns=list(new_cols) if new_cols else None,\n        policy_mode=policy.mode.value if hasattr(policy.mode, \"value\") else str(policy.mode),\n    )\n\n    # Check Validations\n    if missing and policy.on_missing_columns == OnMissingColumns.FAIL:\n        ctx.error(\n            f\"Schema Policy Violation: Missing columns {missing}\",\n            missing_columns=list(missing),\n        )\n        raise ValueError(f\"Schema Policy Violation: Missing columns {missing}\")\n\n    if new_cols and policy.on_new_columns == OnNewColumns.FAIL:\n        ctx.error(\n            f\"Schema Policy Violation: New columns {new_cols}\",\n            new_columns=list(new_cols),\n        )\n        raise ValueError(f\"Schema Policy Violation: New columns {new_cols}\")\n\n    # Apply Transformations\n    if policy.mode == SchemaMode.EVOLVE and policy.on_new_columns == OnNewColumns.ADD_NULLABLE:\n        res = df\n        for c in missing:\n            res = res.withColumn(c, lit(None))\n        ctx.debug(\"Schema evolved: added missing columns as null\")\n        return res\n    else:\n        select_exprs = []\n        for c in target_cols:\n            if c in current_cols:\n                # Escape column names with backticks to handle special characters\n                select_exprs.append(col(f\"`{c}`\"))\n            else:\n                select_exprs.append(lit(None).alias(c))\n\n        ctx.debug(\"Schema enforced: projected to target schema\")\n        return df.select(*select_exprs)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum).</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum).\"\"\"\n    if format != \"delta\" or not config or not config.enabled:\n        return\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    if table:\n        target = table\n    elif path:\n        full_path = connection.get_path(path)\n        target = f\"delta.`{full_path}`\"\n    else:\n        return\n\n    ctx.debug(\"Starting table maintenance\", target=target)\n\n    try:\n        ctx.debug(f\"Running OPTIMIZE on {target}\")\n        self.spark.sql(f\"OPTIMIZE {target}\")\n\n        retention = config.vacuum_retention_hours\n        if retention is not None and retention &gt; 0:\n            ctx.debug(f\"Running VACUUM on {target}\", retention_hours=retention)\n            self.spark.sql(f\"VACUUM {target} RETAIN {retention} HOURS\")\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            target=target,\n            vacuum_retention_hours=retention,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.warning(\n            f\"Auto-optimize failed for {target}\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def profile_nulls(self, df) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\"\"\"\n    from pyspark.sql.functions import col, mean, when\n\n    aggs = []\n    for c in df.columns:\n        # Escape column names with backticks to handle special characters (., ,, spaces)\n        escaped_col = f\"`{c}`\"\n        aggs.append(mean(when(col(escaped_col).isNull(), 1).otherwise(0)).alias(c))\n\n    if not aggs:\n        return {}\n\n    try:\n        result = df.select(*aggs).collect()[0].asDict()\n        return result\n    except Exception:\n        return {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, as_of_version=None, as_of_timestamp=None)</code>","text":"<p>Read data using Spark.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object (with get_path method)</p> required <code>format</code> <code>str</code> <p>Data format (csv, parquet, json, delta, sql_server)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Whether to read as a stream (readStream)</p> <code>False</code> <code>schema</code> <code>Optional[str]</code> <p>Schema string in DDL format (required for streaming file sources)</p> <code>None</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options (including versionAsOf for Delta time travel)</p> <code>None</code> <code>as_of_version</code> <code>Optional[int]</code> <p>Time travel version</p> <code>None</code> <code>as_of_timestamp</code> <code>Optional[str]</code> <p>Time travel timestamp</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Spark DataFrame (or Streaming DataFrame)</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    as_of_version: Optional[int] = None,\n    as_of_timestamp: Optional[str] = None,\n) -&gt; Any:\n    \"\"\"Read data using Spark.\n\n    Args:\n        connection: Connection object (with get_path method)\n        format: Data format (csv, parquet, json, delta, sql_server)\n        table: Table name\n        path: File path\n        streaming: Whether to read as a stream (readStream)\n        schema: Schema string in DDL format (required for streaming file sources)\n        options: Format-specific options (including versionAsOf for Delta time travel)\n        as_of_version: Time travel version\n        as_of_timestamp: Time travel timestamp\n\n    Returns:\n        Spark DataFrame (or Streaming DataFrame)\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n    options = options or {}\n\n    source_identifier = table or path or \"unknown\"\n    ctx.debug(\n        \"Starting Spark read\",\n        format=format,\n        source=source_identifier,\n        streaming=streaming,\n        as_of_version=as_of_version,\n        as_of_timestamp=as_of_timestamp,\n    )\n\n    # Handle Time Travel options\n    if as_of_version is not None:\n        options[\"versionAsOf\"] = as_of_version\n        ctx.debug(f\"Time travel enabled: version {as_of_version}\")\n    if as_of_timestamp is not None:\n        options[\"timestampAsOf\"] = as_of_timestamp\n        ctx.debug(f\"Time travel enabled: timestamp {as_of_timestamp}\")\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if streaming:\n            ctx.error(\"Streaming not supported for SQL Server / Azure SQL\")\n            raise ValueError(\"Streaming not supported for SQL Server / Azure SQL yet.\")\n\n        if not hasattr(connection, \"get_spark_options\"):\n            conn_type = type(connection).__name__\n            msg = f\"Connection type '{conn_type}' does not support Spark SQL read\"\n            ctx.error(msg, connection_type=conn_type)\n            raise ValueError(msg)\n\n        jdbc_options = connection.get_spark_options()\n        merged_options = {**jdbc_options, **options}\n\n        # Extract filter for SQL pushdown\n        sql_filter = merged_options.pop(\"filter\", None)\n\n        if \"query\" in merged_options:\n            merged_options.pop(\"dbtable\", None)\n            # If filter provided with query, append to WHERE clause\n            if sql_filter:\n                existing_query = merged_options[\"query\"]\n                # Wrap existing query and add filter\n                if \"WHERE\" in existing_query.upper():\n                    merged_options[\"query\"] = f\"({existing_query}) AND ({sql_filter})\"\n                else:\n                    subquery = f\"SELECT * FROM ({existing_query}) AS _subq WHERE {sql_filter}\"\n                    merged_options[\"query\"] = subquery\n                ctx.debug(f\"Applied SQL pushdown filter to query: {sql_filter}\")\n        elif table:\n            # Build query with filter pushdown instead of using dbtable\n            if sql_filter:\n                merged_options.pop(\"dbtable\", None)\n                merged_options[\"query\"] = f\"SELECT * FROM {table} WHERE {sql_filter}\"\n                ctx.debug(f\"Applied SQL pushdown filter: {sql_filter}\")\n            else:\n                merged_options[\"dbtable\"] = table\n        elif \"dbtable\" not in merged_options:\n            ctx.error(\"SQL format requires 'table' config or 'query' option\")\n            raise ValueError(\"SQL format requires 'table' config or 'query' option\")\n\n        ctx.debug(\"Executing JDBC read\", has_query=\"query\" in merged_options)\n\n        try:\n            df = self.spark.read.format(\"jdbc\").options(**merged_options).load()\n            elapsed = (time.time() - start_time) * 1000\n            partition_count = df.rdd.getNumPartitions()\n\n            ctx.log_file_io(path=source_identifier, format=format, mode=\"read\")\n            ctx.log_spark_metrics(partition_count=partition_count)\n            ctx.info(\n                \"JDBC read completed\",\n                source=source_identifier,\n                elapsed_ms=round(elapsed, 2),\n                partitions=partition_count,\n            )\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"JDBC read failed\",\n                source=source_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    # Read based on format\n    if table:\n        # Managed/External Table (Catalog)\n        ctx.debug(f\"Reading from catalog table: {table}\")\n\n        if streaming:\n            reader = self.spark.readStream.format(format)\n        else:\n            reader = self.spark.read.format(format)\n\n        for key, value in options.items():\n            reader = reader.option(key, value)\n\n        try:\n            df = reader.table(table)\n\n            if \"filter\" in options:\n                df = df.filter(options[\"filter\"])\n                ctx.debug(f\"Applied filter: {options['filter']}\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            if not streaming:\n                partition_count = df.rdd.getNumPartitions()\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.log_file_io(path=table, format=format, mode=\"read\")\n                ctx.info(\n                    f\"Table read completed: {table}\",\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                )\n            else:\n                ctx.info(f\"Streaming read started: {table}\", elapsed_ms=round(elapsed, 2))\n\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Table read failed: {table}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    elif path:\n        # File Path\n        full_path = connection.get_path(path)\n        ctx.debug(f\"Reading from path: {full_path}\")\n\n        # Excel format: delegate to Pandas, convert to Spark\n        if format == \"excel\":\n            if streaming:\n                ctx.error(\"Streaming not supported for Excel format\")\n                raise ValueError(\"Streaming not supported for Excel format\")\n\n            ctx.debug(\"Reading Excel via Pandas engine (best Excel support)\")\n            from odibi.engine.pandas_engine import PandasEngine\n\n            # Get storage_options from connection for Azure/cloud authentication\n            storage_options = None\n            if hasattr(connection, \"pandas_storage_options\"):\n                storage_options = connection.pandas_storage_options()\n\n            # Use Pandas engine for Excel reading\n            pandas_engine = PandasEngine()\n            pdf = pandas_engine._read_excel_with_patterns(\n                full_path,\n                sheet_pattern=options.pop(\"sheet_pattern\", None),\n                sheet_pattern_case_sensitive=options.pop(\"sheet_pattern_case_sensitive\", False),\n                add_source_file=options.pop(\"add_source_file\", False),\n                is_glob=\"*\" in str(full_path) or \"?\" in str(full_path),\n                ctx=ctx,\n                storage_options=storage_options,\n                **options,\n            )\n\n            # Convert Pandas DataFrame to Spark DataFrame\n            df = self.spark.createDataFrame(pdf)\n            elapsed = (time.time() - start_time) * 1000\n            ctx.info(\n                f\"Excel read completed (via Pandas): {path}\",\n                elapsed_ms=round(elapsed, 2),\n                row_count=len(pdf),\n            )\n            return df\n\n        # Auto-detect encoding for CSV (Batch only)\n        if not streaming and format == \"csv\" and options.get(\"auto_encoding\"):\n            options = options.copy()\n            options.pop(\"auto_encoding\")\n\n            if \"encoding\" not in options:\n                try:\n                    from odibi.utils.encoding import detect_encoding\n\n                    detected = detect_encoding(connection, path)\n                    if detected:\n                        options[\"encoding\"] = detected\n                        ctx.debug(f\"Detected encoding: {detected}\", path=path)\n                except ImportError:\n                    pass\n                except Exception as e:\n                    ctx.warning(\n                        f\"Encoding detection failed for {path}\",\n                        error_message=str(e),\n                    )\n\n        # Resolve cloudFiles.schemaLocation through read connection if still relative\n        # (Node level resolves via write connection first if available)\n        if \"cloudFiles.schemaLocation\" in options:\n            schema_location = options[\"cloudFiles.schemaLocation\"]\n            if not schema_location.startswith(\n                (\"abfss://\", \"s3://\", \"gs://\", \"dbfs://\", \"hdfs://\", \"wasbs://\")\n            ):\n                options = options.copy()\n                options[\"cloudFiles.schemaLocation\"] = connection.get_path(schema_location)\n                ctx.debug(\n                    \"Resolved cloudFiles.schemaLocation through read connection (fallback)\",\n                    original=schema_location,\n                    resolved=options[\"cloudFiles.schemaLocation\"],\n                )\n\n        if streaming:\n            reader = self.spark.readStream.format(format)\n            if schema:\n                reader = reader.schema(schema)\n                ctx.debug(f\"Applied schema for streaming read: {schema[:100]}...\")\n            else:\n                # Determine if we should warn about missing schema\n                # Formats that can infer schema: delta, parquet, avro (embedded schema)\n                # cloudFiles with schemaLocation or self-describing formats (avro, parquet) are fine\n                should_warn = True\n\n                if format in [\"delta\", \"parquet\"]:\n                    should_warn = False\n                elif format == \"cloudFiles\":\n                    cloud_format = options.get(\"cloudFiles.format\", \"\")\n                    has_schema_location = \"cloudFiles.schemaLocation\" in options\n                    # avro and parquet have embedded schemas\n                    if cloud_format in [\"avro\", \"parquet\"] or has_schema_location:\n                        should_warn = False\n\n                if should_warn:\n                    ctx.warning(\n                        f\"Streaming read from '{format}' format without schema. \"\n                        \"Schema inference is not supported for streaming sources. \"\n                        \"Consider adding 'schema' to your read config.\"\n                    )\n        else:\n            reader = self.spark.read.format(format)\n            if schema:\n                reader = reader.schema(schema)\n\n        for key, value in options.items():\n            if key == \"header\" and isinstance(value, bool):\n                value = str(value).lower()\n            reader = reader.option(key, value)\n\n        try:\n            df = reader.load(full_path)\n\n            if \"filter\" in options:\n                df = df.filter(options[\"filter\"])\n                ctx.debug(f\"Applied filter: {options['filter']}\")\n\n            elapsed = (time.time() - start_time) * 1000\n\n            if not streaming:\n                partition_count = df.rdd.getNumPartitions()\n                ctx.log_spark_metrics(partition_count=partition_count)\n                ctx.log_file_io(path=path, format=format, mode=\"read\")\n                ctx.info(\n                    f\"File read completed: {path}\",\n                    elapsed_ms=round(elapsed, 2),\n                    partitions=partition_count,\n                    format=format,\n                )\n            else:\n                ctx.info(f\"Streaming read started: {path}\", elapsed_ms=round(elapsed, 2))\n\n            return df\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"File read failed: {path}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n                format=format,\n            )\n            raise\n    else:\n        ctx.error(\"Either path or table must be provided\")\n        raise ValueError(\"Either path or table must be provided\")\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.restore_delta","title":"<code>restore_delta(connection, path, version)</code>","text":"<p>Restore Delta table to a specific version.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def restore_delta(self, connection: Any, path: str, version: int) -&gt; None:\n    \"\"\"Restore Delta table to a specific version.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\"Restoring Delta table\", path=path, version=version)\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        delta_table.restoreToVersion(version)\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta table restored\",\n            path=path,\n            version=version,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Delta restore failed\",\n            path=path,\n            version=version,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> <p>Handles orphan catalog entries where the table is registered but the underlying Delta path no longer exists.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\n\n    Handles orphan catalog entries where the table is registered but\n    the underlying Delta path no longer exists.\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n\n    if table:\n        try:\n            if not self.spark.catalog.tableExists(table):\n                ctx.debug(f\"Table does not exist: {table}\")\n                return False\n            # Table exists in catalog - verify it's actually readable\n            # This catches orphan entries where path was deleted\n            # Use limit(1) not limit(0) - limit(0) can succeed from metadata alone\n            self.spark.table(table).limit(1).collect()\n            ctx.debug(f\"Table existence check: {table}\", exists=True)\n            return True\n        except Exception as e:\n            # Table exists in catalog but underlying data is gone (orphan entry)\n            # This is expected during first-run detection - log at debug level\n            ctx.debug(\n                f\"Table {table} exists in catalog but is not accessible (treating as first run)\",\n                error_message=str(e),\n            )\n            return False\n    elif path:\n        try:\n            from delta.tables import DeltaTable\n\n            full_path = connection.get_path(path)\n            exists = DeltaTable.isDeltaTable(self.spark, full_path)\n            ctx.debug(f\"Delta table existence check: {path}\", exists=exists)\n            return exists\n        except ImportError:\n            try:\n                full_path = connection.get_path(path)\n                exists = (\n                    self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem.get(\n                        self.spark.sparkContext._jsc.hadoopConfiguration()\n                    ).exists(\n                        self.spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path(\n                            full_path\n                        )\n                    )\n                )\n                ctx.debug(f\"Path existence check: {path}\", exists=exists)\n                return exists\n            except Exception as e:\n                ctx.warning(f\"Path existence check failed: {path}\", error_message=str(e))\n                return False\n        except Exception as e:\n            ctx.warning(f\"Table existence check failed: {path}\", error_message=str(e))\n            return False\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n) -&gt; None:\n    \"\"\"VACUUM a Delta table to remove old files.\"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n    )\n\n    try:\n        from delta.tables import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake support requires 'delta-spark'\")\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[spark]' \"\n            \"with delta-spark. \"\n            \"See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path)\n\n    try:\n        delta_table = DeltaTable.forPath(self.spark, full_path)\n        delta_table.vacuum(retention_hours / 24.0)\n\n        elapsed = (time.time() - start_time) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=path,\n            retention_hours=retention_hours,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            \"Delta VACUUM failed\",\n            path=path,\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate DataFrame against rules.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def validate_data(self, df, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate DataFrame against rules.\"\"\"\n    from pyspark.sql.functions import col\n\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    failures = []\n\n    if validation_config.not_empty:\n        if df.isEmpty():\n            failures.append(\"DataFrame is empty\")\n\n    if validation_config.no_nulls:\n        null_counts = self.count_nulls(df, validation_config.no_nulls)\n        for col_name, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col_name}' has {count} null values\")\n\n    if validation_config.schema_validation:\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    if validation_config.ranges:\n        for col_name, bounds in validation_config.ranges.items():\n            if col_name in df.columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                # Escape column names with backticks to handle special characters\n                escaped_col = f\"`{col_name}`\"\n                if min_val is not None:\n                    count = df.filter(col(escaped_col) &lt; min_val).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    count = df.filter(col(escaped_col) &gt; max_val).count()\n                    if count &gt; 0:\n                        failures.append(f\"Column '{col_name}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col_name}' not found for range validation\")\n\n    if validation_config.allowed_values:\n        for col_name, allowed in validation_config.allowed_values.items():\n            if col_name in df.columns:\n                # Escape column names with backticks to handle special characters\n                escaped_col = f\"`{col_name}`\"\n                count = df.filter(~col(escaped_col).isin(allowed)).count()\n                if count &gt; 0:\n                    failures.append(f\"Column '{col_name}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col_name}' not found for allowed values validation\")\n\n    ctx.log_validation_result(\n        passed=len(failures) == 0,\n        rule_name=\"data_validation\",\n        failures=failures if failures else None,\n    )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def validate_schema(self, df, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\"\"\"\n    failures = []\n\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(df.columns)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    if \"types\" in schema_rules:\n        type_map = {\n            \"int\": [\"integer\", \"long\", \"short\", \"byte\", \"bigint\"],\n            \"float\": [\"double\", \"float\"],\n            \"str\": [\"string\"],\n            \"bool\": [\"boolean\"],\n        }\n\n        for col_name, expected_type in schema_rules[\"types\"].items():\n            if col_name not in df.columns:\n                failures.append(f\"Column '{col_name}' not found for type validation\")\n                continue\n\n            actual_type = dict(df.dtypes)[col_name]\n            expected_dtypes = type_map.get(expected_type, [expected_type])\n\n            if actual_type not in expected_dtypes:\n                failures.append(\n                    f\"Column '{col_name}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.spark_engine.SparkEngine.write","title":"<code>write(df, connection, format, table=None, path=None, register_table=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Spark.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Spark DataFrame to write</p> required <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Output format (csv, parquet, json, delta)</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>register_table</code> <code>Optional[str]</code> <p>Name to register as external table (if path is used)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Write mode (overwrite, append, error, ignore, upsert, append_once)</p> <code>'overwrite'</code> <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Format-specific options (including partition_by for partitioning)</p> <code>None</code> <code>streaming_config</code> <code>Optional[Any]</code> <p>StreamingWriteConfig for streaming DataFrames</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary containing Delta commit metadata (if format=delta),</p> <code>Optional[Dict[str, Any]]</code> <p>or streaming query info (if streaming)</p> Source code in <code>odibi\\engine\\spark_engine.py</code> <pre><code>def write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    register_table: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Spark.\n\n    Args:\n        df: Spark DataFrame to write\n        connection: Connection object\n        format: Output format (csv, parquet, json, delta)\n        table: Table name\n        path: File path\n        register_table: Name to register as external table (if path is used)\n        mode: Write mode (overwrite, append, error, ignore, upsert, append_once)\n        options: Format-specific options (including partition_by for partitioning)\n        streaming_config: StreamingWriteConfig for streaming DataFrames\n\n    Returns:\n        Optional dictionary containing Delta commit metadata (if format=delta),\n        or streaming query info (if streaming)\n    \"\"\"\n    ctx = get_logging_context().with_context(engine=\"spark\")\n    start_time = time.time()\n    options = options or {}\n\n    if getattr(df, \"isStreaming\", False) is True:\n        return self._write_streaming(\n            df=df,\n            connection=connection,\n            format=format,\n            table=table,\n            path=path,\n            register_table=register_table,\n            options=options,\n            streaming_config=streaming_config,\n        )\n\n    target_identifier = table or path or \"unknown\"\n    try:\n        partition_count = df.rdd.getNumPartitions()\n    except Exception:\n        partition_count = 1  # Fallback for mocks or unsupported DataFrames\n\n    # Auto-coalesce DataFrames for Delta writes to reduce file overhead\n    # Use coalesce_partitions option to explicitly set target partitions\n    # NOTE: We avoid df.count() here as it would trigger double-evaluation of lazy DataFrames\n    coalesce_partitions = options.pop(\"coalesce_partitions\", None)\n    if (\n        coalesce_partitions\n        and isinstance(partition_count, int)\n        and partition_count &gt; coalesce_partitions\n    ):\n        df = df.coalesce(coalesce_partitions)\n        ctx.debug(\n            f\"Coalesced DataFrame to {coalesce_partitions} partition(s)\",\n            original_partitions=partition_count,\n        )\n        partition_count = coalesce_partitions\n\n    ctx.debug(\n        \"Starting Spark write\",\n        format=format,\n        target=target_identifier,\n        mode=mode,\n        partitions=partition_count,\n    )\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if not hasattr(connection, \"get_spark_options\"):\n            conn_type = type(connection).__name__\n            msg = f\"Connection type '{conn_type}' does not support Spark SQL write\"\n            ctx.error(msg, connection_type=conn_type)\n            raise ValueError(msg)\n\n        jdbc_options = connection.get_spark_options()\n        merged_options = {**jdbc_options, **options}\n\n        if table:\n            merged_options[\"dbtable\"] = table\n        elif \"dbtable\" not in merged_options:\n            ctx.error(\"SQL format requires 'table' config or 'dbtable' option\")\n            raise ValueError(\"SQL format requires 'table' config or 'dbtable' option\")\n\n        # Handle MERGE mode for SQL Server\n        if mode == \"merge\":\n            merge_keys = options.get(\"merge_keys\")\n            merge_options = options.get(\"merge_options\")\n\n            if not merge_keys:\n                ctx.error(\"MERGE mode requires 'merge_keys' in options\")\n                raise ValueError(\n                    \"MERGE mode requires 'merge_keys' in options. \"\n                    \"Specify the key columns for the MERGE ON clause.\"\n                )\n\n            from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n            writer = SqlServerMergeWriter(connection)\n            ctx.debug(\n                \"Executing SQL Server MERGE\",\n                target=table,\n                merge_keys=merge_keys,\n            )\n\n            try:\n                result = writer.merge(\n                    df=df,\n                    spark_engine=self,\n                    target_table=table,\n                    merge_keys=merge_keys,\n                    options=merge_options,\n                    jdbc_options=jdbc_options,\n                )\n                elapsed = (time.time() - start_time) * 1000\n                ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                ctx.info(\n                    \"SQL Server MERGE completed\",\n                    target=target_identifier,\n                    mode=mode,\n                    inserted=result.inserted,\n                    updated=result.updated,\n                    deleted=result.deleted,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                return {\n                    \"mode\": \"merge\",\n                    \"inserted\": result.inserted,\n                    \"updated\": result.updated,\n                    \"deleted\": result.deleted,\n                    \"total_affected\": result.total_affected,\n                }\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"SQL Server MERGE failed\",\n                    target=target_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        # Handle enhanced overwrite with strategies\n        if mode == \"overwrite\" and options.get(\"overwrite_options\"):\n            from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n            overwrite_options = options.get(\"overwrite_options\")\n            writer = SqlServerMergeWriter(connection)\n\n            ctx.debug(\n                \"Executing SQL Server enhanced overwrite\",\n                target=table,\n                strategy=(\n                    overwrite_options.strategy.value\n                    if hasattr(overwrite_options, \"strategy\")\n                    else \"truncate_insert\"\n                ),\n            )\n\n            try:\n                result = writer.overwrite_spark(\n                    df=df,\n                    target_table=table,\n                    options=overwrite_options,\n                    jdbc_options=jdbc_options,\n                )\n                elapsed = (time.time() - start_time) * 1000\n                ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n                ctx.info(\n                    \"SQL Server enhanced overwrite completed\",\n                    target=target_identifier,\n                    strategy=result.strategy,\n                    rows_written=result.rows_written,\n                    elapsed_ms=round(elapsed, 2),\n                )\n                return {\n                    \"mode\": \"overwrite\",\n                    \"strategy\": result.strategy,\n                    \"rows_written\": result.rows_written,\n                }\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"SQL Server enhanced overwrite failed\",\n                    target=target_identifier,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n        if mode not in [\"overwrite\", \"append\", \"ignore\", \"error\"]:\n            if mode == \"fail\":\n                mode = \"error\"\n            else:\n                ctx.error(f\"Write mode '{mode}' not supported for Spark SQL write\")\n                raise ValueError(f\"Write mode '{mode}' not supported for Spark SQL write\")\n\n        ctx.debug(\"Executing JDBC write\", target=table or merged_options.get(\"dbtable\"))\n\n        try:\n            df.write.format(\"jdbc\").options(**merged_options).mode(mode).save()\n            elapsed = (time.time() - start_time) * 1000\n            ctx.log_file_io(path=target_identifier, format=format, mode=\"write\")\n            ctx.info(\n                \"JDBC write completed\",\n                target=target_identifier,\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n            return None\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                \"JDBC write failed\",\n                target=target_identifier,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    # Handle Upsert/AppendOnce (Delta Only)\n    if mode in [\"upsert\", \"append_once\"]:\n        if format != \"delta\":\n            ctx.error(f\"Mode '{mode}' only supported for Delta format\")\n            raise NotImplementedError(\n                f\"Mode '{mode}' only supported for Delta format in Spark engine.\"\n            )\n\n        keys = options.get(\"keys\")\n        if not keys:\n            ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        exists = self.table_exists(connection, table, path)\n        ctx.debug(\"Table existence check for merge\", target=target_identifier, exists=exists)\n\n        if not exists:\n            mode = \"overwrite\"\n            ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n        else:\n            from delta.tables import DeltaTable\n\n            target_dt = None\n            target_name = \"\"\n            is_table_target = False\n\n            if table:\n                target_dt = DeltaTable.forName(self.spark, table)\n                target_name = table\n                is_table_target = True\n            elif path:\n                full_path = connection.get_path(path)\n                target_dt = DeltaTable.forPath(self.spark, full_path)\n                target_name = full_path\n                is_table_target = False\n\n            condition = \" AND \".join([f\"target.`{k}` = source.`{k}`\" for k in keys])\n            ctx.debug(\"Executing Delta merge\", merge_mode=mode, keys=keys, condition=condition)\n\n            merge_builder = target_dt.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n            try:\n                if mode == \"upsert\":\n                    merge_builder.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                elif mode == \"append_once\":\n                    merge_builder.whenNotMatchedInsertAll().execute()\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Delta merge completed\",\n                    target=target_name,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                self._optimize_delta_write(target_name, options, is_table=is_table_target)\n                commit_info = self._get_last_delta_commit_info(\n                    target_name, is_table=is_table_target\n                )\n\n                if commit_info:\n                    ctx.debug(\n                        \"Delta commit info\",\n                        version=commit_info.get(\"version\"),\n                        operation=commit_info.get(\"operation\"),\n                    )\n\n                return commit_info\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Delta merge failed\",\n                    target=target_name,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n    # Get output location\n    if table:\n        # Managed/External Table (Catalog)\n        ctx.debug(f\"Writing to catalog table: {table}\")\n        writer = df.write.format(format).mode(mode)\n\n        partition_by = options.get(\"partition_by\")\n        if partition_by:\n            if isinstance(partition_by, str):\n                partition_by = [partition_by]\n            writer = writer.partitionBy(*partition_by)\n            ctx.debug(f\"Partitioning by: {partition_by}\")\n\n        for key, value in options.items():\n            writer = writer.option(key, value)\n\n        try:\n            writer.saveAsTable(table)\n            elapsed = (time.time() - start_time) * 1000\n\n            ctx.log_file_io(\n                path=table,\n                format=format,\n                mode=mode,\n                partitions=partition_by,\n            )\n            ctx.info(\n                f\"Table write completed: {table}\",\n                mode=mode,\n                elapsed_ms=round(elapsed, 2),\n            )\n\n            if format == \"delta\":\n                self._optimize_delta_write(table, options, is_table=True)\n                return self._get_last_delta_commit_info(table, is_table=True)\n            return None\n\n        except Exception as e:\n            elapsed = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Table write failed: {table}\",\n                error_type=type(e).__name__,\n                error_message=str(e),\n                elapsed_ms=round(elapsed, 2),\n            )\n            raise\n\n    elif path:\n        full_path = connection.get_path(path)\n    else:\n        ctx.error(\"Either path or table must be provided\")\n        raise ValueError(\"Either path or table must be provided\")\n\n    # Extract partition_by option\n    partition_by = options.pop(\"partition_by\", None) or options.pop(\"partitionBy\", None)\n\n    # Extract cluster_by option (Liquid Clustering)\n    cluster_by = options.pop(\"cluster_by\", None)\n\n    # Warn about partitioning anti-patterns\n    if partition_by and cluster_by:\n        import warnings\n\n        ctx.warning(\n            \"Conflict: Both 'partition_by' and 'cluster_by' are set\",\n            partition_by=partition_by,\n            cluster_by=cluster_by,\n        )\n        warnings.warn(\n            \"\u26a0\ufe0f  Conflict: Both 'partition_by' and 'cluster_by' (Liquid Clustering) are set. \"\n            \"Liquid Clustering supersedes partitioning. 'partition_by' will be ignored \"\n            \"if the table is being created now.\",\n            UserWarning,\n        )\n\n    elif partition_by:\n        import warnings\n\n        ctx.warning(\n            \"Partitioning warning: ensure low-cardinality columns\",\n            partition_by=partition_by,\n        )\n        warnings.warn(\n            \"\u26a0\ufe0f  Partitioning can cause performance issues if misused. \"\n            \"Only partition on low-cardinality columns (&lt; 1000 unique values) \"\n            \"and ensure each partition has &gt; 1000 rows.\",\n            UserWarning,\n        )\n\n    # Handle Upsert/Append-Once for Delta Lake (Path-based only for now)\n    if format == \"delta\" and mode in [\"upsert\", \"append_once\"]:\n        try:\n            from delta.tables import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake support requires 'delta-spark'\")\n            raise ImportError(\"Delta Lake support requires 'delta-spark'\")\n\n        if \"keys\" not in options:\n            ctx.error(f\"Mode '{mode}' requires 'keys' list in options\")\n            raise ValueError(f\"Mode '{mode}' requires 'keys' list in options\")\n\n        if DeltaTable.isDeltaTable(self.spark, full_path):\n            ctx.debug(f\"Performing Delta merge at path: {full_path}\")\n            delta_table = DeltaTable.forPath(self.spark, full_path)\n            keys = options[\"keys\"]\n            if isinstance(keys, str):\n                keys = [keys]\n\n            condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in keys])\n            merger = delta_table.alias(\"target\").merge(df.alias(\"source\"), condition)\n\n            try:\n                if mode == \"upsert\":\n                    merger.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n                else:\n                    merger.whenNotMatchedInsertAll().execute()\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Delta merge completed at path\",\n                    path=path,\n                    mode=mode,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table:\n                    try:\n                        table_in_catalog = self.spark.catalog.tableExists(register_table)\n                        needs_registration = not table_in_catalog\n\n                        # Handle orphan catalog entries (only for path-not-found errors)\n                        if table_in_catalog:\n                            try:\n                                # Use limit(1) not limit(0) - limit(0) can succeed from metadata alone\n                                self.spark.table(register_table).limit(1).collect()\n                                ctx.debug(\n                                    f\"Table '{register_table}' already registered and valid\"\n                                )\n                            except Exception as verify_err:\n                                error_str = str(verify_err)\n                                is_orphan = (\n                                    \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                                    or \"Path does not exist\" in error_str\n                                    or \"FileNotFoundException\" in error_str\n                                )\n                                if is_orphan:\n                                    ctx.warning(\n                                        f\"Table '{register_table}' is orphan, re-registering\"\n                                    )\n                                    try:\n                                        self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                                    except Exception:\n                                        pass\n                                    needs_registration = True\n                                else:\n                                    ctx.debug(\n                                        f\"Table '{register_table}' verify failed, \"\n                                        \"skipping registration\"\n                                    )\n\n                        if needs_registration:\n                            create_sql = (\n                                f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                                f\"USING DELTA LOCATION '{full_path}'\"\n                            )\n                            self.spark.sql(create_sql)\n                            ctx.info(f\"Registered table: {register_table}\", path=full_path)\n                    except Exception as e:\n                        ctx.error(\n                            f\"Failed to register external table '{register_table}'\",\n                            error_message=str(e),\n                        )\n\n                self._optimize_delta_write(full_path, options, is_table=False)\n                return self._get_last_delta_commit_info(full_path, is_table=False)\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Delta merge failed at path\",\n                    path=path,\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n        else:\n            mode = \"overwrite\"\n            ctx.debug(\"Target does not exist, falling back to overwrite mode\")\n\n    # Write based on format (Path-based)\n    ctx.debug(f\"Writing to path: {full_path}\")\n\n    # Handle Liquid Clustering (New Table Creation via SQL)\n    if format == \"delta\" and cluster_by:\n        should_create = False\n        target_name = None\n\n        if table:\n            target_name = table\n            if mode == \"overwrite\":\n                should_create = True\n            elif mode == \"append\":\n                if not self.spark.catalog.tableExists(table):\n                    should_create = True\n        elif path:\n            full_path = connection.get_path(path)\n            target_name = f\"delta.`{full_path}`\"\n            if mode == \"overwrite\":\n                should_create = True\n            elif mode == \"append\":\n                try:\n                    from delta.tables import DeltaTable\n\n                    if not DeltaTable.isDeltaTable(self.spark, full_path):\n                        should_create = True\n                except ImportError:\n                    pass\n\n        if should_create:\n            if isinstance(cluster_by, str):\n                cluster_by = [cluster_by]\n\n            cols = \", \".join(cluster_by)\n            temp_view = f\"odibi_temp_writer_{abs(hash(str(target_name)))}\"\n            df.createOrReplaceTempView(temp_view)\n\n            create_cmd = (\n                \"CREATE OR REPLACE TABLE\"\n                if mode == \"overwrite\"\n                else \"CREATE TABLE IF NOT EXISTS\"\n            )\n\n            sql = (\n                f\"{create_cmd} {target_name} USING DELTA CLUSTER BY ({cols}) \"\n                f\"AS SELECT * FROM {temp_view}\"\n            )\n\n            ctx.debug(\"Creating clustered Delta table\", sql=sql, cluster_by=cluster_by)\n\n            try:\n                self.spark.sql(sql)\n                self.spark.catalog.dropTempView(temp_view)\n\n                elapsed = (time.time() - start_time) * 1000\n                ctx.info(\n                    \"Clustered Delta table created\",\n                    target=target_name,\n                    cluster_by=cluster_by,\n                    elapsed_ms=round(elapsed, 2),\n                )\n\n                if register_table and path:\n                    try:\n                        reg_sql = (\n                            f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                            f\"USING DELTA LOCATION '{full_path}'\"\n                        )\n                        self.spark.sql(reg_sql)\n                        ctx.info(f\"Registered table: {register_table}\")\n                    except Exception:\n                        pass\n\n                if format == \"delta\":\n                    self._optimize_delta_write(\n                        target_name if table else full_path, options, is_table=bool(table)\n                    )\n                    return self._get_last_delta_commit_info(\n                        target_name if table else full_path, is_table=bool(table)\n                    )\n                return None\n\n            except Exception as e:\n                elapsed = (time.time() - start_time) * 1000\n                ctx.error(\n                    \"Failed to create clustered Delta table\",\n                    error_type=type(e).__name__,\n                    error_message=str(e),\n                    elapsed_ms=round(elapsed, 2),\n                )\n                raise\n\n    # Extract table_properties from options\n    table_properties = options.pop(\"table_properties\", None)\n\n    # For column mapping and other properties that must be set BEFORE write\n    original_configs = {}\n    if table_properties and format == \"delta\":\n        for prop_name, prop_value in table_properties.items():\n            spark_conf_key = (\n                f\"spark.databricks.delta.properties.defaults.{prop_name.replace('delta.', '')}\"\n            )\n            try:\n                original_configs[spark_conf_key] = self.spark.conf.get(spark_conf_key, None)\n            except Exception:\n                original_configs[spark_conf_key] = None\n            self.spark.conf.set(spark_conf_key, prop_value)\n        ctx.debug(\n            \"Applied table properties as session defaults\",\n            properties=list(table_properties.keys()),\n        )\n\n    writer = df.write.format(format).mode(mode)\n\n    if partition_by:\n        if isinstance(partition_by, str):\n            partition_by = [partition_by]\n        writer = writer.partitionBy(*partition_by)\n        ctx.debug(f\"Partitioning by: {partition_by}\")\n\n    for key, value in options.items():\n        writer = writer.option(key, value)\n\n    try:\n        writer.save(full_path)\n        elapsed = (time.time() - start_time) * 1000\n\n        ctx.log_file_io(\n            path=path,\n            format=format,\n            mode=mode,\n            partitions=partition_by,\n        )\n        ctx.info(\n            f\"File write completed: {path}\",\n            format=format,\n            mode=mode,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        elapsed = (time.time() - start_time) * 1000\n        ctx.error(\n            f\"File write failed: {path}\",\n            error_type=type(e).__name__,\n            error_message=str(e),\n            elapsed_ms=round(elapsed, 2),\n        )\n        raise\n    finally:\n        for conf_key, original_value in original_configs.items():\n            if original_value is None:\n                self.spark.conf.unset(conf_key)\n            else:\n                self.spark.conf.set(conf_key, original_value)\n\n    if format == \"delta\":\n        self._optimize_delta_write(full_path, options, is_table=False)\n\n    if register_table and format == \"delta\":\n        try:\n            table_in_catalog = self.spark.catalog.tableExists(register_table)\n            needs_registration = not table_in_catalog\n\n            # Handle orphan catalog entries: table exists but points to deleted path\n            # Only treat as orphan if it's specifically a DELTA_PATH_DOES_NOT_EXIST error\n            if table_in_catalog:\n                try:\n                    # Use limit(1) not limit(0) - limit(0) can succeed from metadata alone\n                    self.spark.table(register_table).limit(1).collect()\n                    ctx.debug(\n                        f\"Table '{register_table}' already registered and valid, \"\n                        \"skipping registration\"\n                    )\n                except Exception as verify_err:\n                    error_str = str(verify_err)\n                    is_orphan = (\n                        \"DELTA_PATH_DOES_NOT_EXIST\" in error_str\n                        or \"Path does not exist\" in error_str\n                        or \"FileNotFoundException\" in error_str\n                    )\n\n                    if is_orphan:\n                        # Orphan entry - table in catalog but path was deleted\n                        ctx.warning(\n                            f\"Table '{register_table}' is orphan (path deleted), \"\n                            \"dropping and re-registering\",\n                            error_message=error_str[:200],\n                        )\n                        try:\n                            self.spark.sql(f\"DROP TABLE IF EXISTS {register_table}\")\n                        except Exception:\n                            pass  # Best effort cleanup\n                        needs_registration = True\n                    else:\n                        # Other error (auth, network, etc.) - don't drop, just log\n                        ctx.debug(\n                            f\"Table '{register_table}' exists but verify failed \"\n                            \"(not orphan), skipping registration\",\n                            error_message=error_str[:200],\n                        )\n\n            if needs_registration:\n                ctx.debug(f\"Registering table '{register_table}' at '{full_path}'\")\n                reg_sql = (\n                    f\"CREATE TABLE IF NOT EXISTS {register_table} \"\n                    f\"USING DELTA LOCATION '{full_path}'\"\n                )\n                self.spark.sql(reg_sql)\n                ctx.info(f\"Registered table: {register_table}\", path=full_path)\n        except Exception as e:\n            ctx.error(\n                f\"Failed to register table '{register_table}'\",\n                error_message=str(e),\n            )\n            raise RuntimeError(\n                f\"Failed to register external table '{register_table}': {e}\"\n            ) from e\n\n    if format == \"delta\":\n        return self._get_last_delta_commit_info(full_path, is_table=False)\n\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine","title":"<code>odibi.engine.polars_engine</code>","text":"<p>Polars engine implementation.</p>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine","title":"<code>PolarsEngine</code>","text":"<p>               Bases: <code>Engine</code></p> <p>Polars-based execution engine (High Performance).</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>class PolarsEngine(Engine):\n    \"\"\"Polars-based execution engine (High Performance).\"\"\"\n\n    name = \"polars\"\n    engine_type = EngineType.POLARS\n\n    def __init__(\n        self,\n        connections: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize Polars engine.\n\n        Args:\n            connections: Dictionary of connection objects\n            config: Engine configuration (optional)\n        \"\"\"\n        if pl is None:\n            raise ImportError(\"Polars not installed. Run 'pip install polars'.\")\n\n        self.connections = connections or {}\n        self.config = config or {}\n\n    def materialize(self, df: Any) -&gt; Any:\n        \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n        Args:\n            df: LazyFrame or DataFrame\n\n        Returns:\n            Materialized DataFrame (pl.DataFrame)\n        \"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return df.collect()\n        return df\n\n    def read(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        streaming: bool = False,\n        schema: Optional[str] = None,\n        options: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"Read data using Polars (Lazy by default).\n\n        Returns:\n            pl.LazyFrame or pl.DataFrame\n        \"\"\"\n        options = options or {}\n\n        # SQL Server / Azure SQL Support\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            if not hasattr(connection, \"read_table\") and not hasattr(connection, \"read_sql_query\"):\n                raise ValueError(\n                    f\"Cannot read SQL table: connection type '{type(connection).__name__}' \"\n                    \"does not support SQL operations. Use a SQL-compatible connection.\"\n                )\n            table_name = table or path\n            if not table_name:\n                raise ValueError(\"SQL read requires 'table' or 'path' to specify the table name.\")\n            if \".\" in table_name:\n                schema_name, tbl = table_name.split(\".\", 1)\n            else:\n                schema_name, tbl = \"dbo\", table_name\n\n            # Check for incremental SQL filter in options\n            sql_filter = options.get(\"filter\")\n\n            if sql_filter and hasattr(connection, \"read_sql_query\"):\n                # Build query with WHERE clause for SQL pushdown\n                if schema_name:\n                    base_query = f\"SELECT * FROM [{schema_name}].[{tbl}]\"\n                else:\n                    base_query = f\"SELECT * FROM [{tbl}]\"\n                full_query = f\"{base_query} WHERE {sql_filter}\"\n                pdf = connection.read_sql_query(full_query)\n            else:\n                # read_table returns Pandas DataFrame\n                pdf = connection.read_table(table_name=tbl, schema=schema_name)\n\n            return pl.from_pandas(pdf).lazy()\n\n        # Get full path\n        if path:\n            if connection:\n                full_path = connection.get_path(path)\n            else:\n                full_path = path\n        elif table:\n            if connection:\n                full_path = connection.get_path(table)\n            else:\n                raise ValueError(\n                    f\"Cannot read table '{table}': connection is required when using 'table' parameter. \"\n                    \"Provide a valid connection object or use 'path' for file-based reads.\"\n                )\n        else:\n            raise ValueError(\n                \"Read operation failed: neither 'path' nor 'table' was provided. \"\n                \"Specify a file path or table name in your configuration.\"\n            )\n\n        # Handle glob patterns/lists\n        # Polars scan methods often support glob strings directly.\n\n        try:\n            if format == \"csv\":\n                # scan_csv supports glob patterns\n                return pl.scan_csv(full_path, **options)\n\n            elif format == \"parquet\":\n                return pl.scan_parquet(full_path, **options)\n\n            elif format == \"json\":\n                # scan_ndjson for newline delimited json, read_json for standard\n                # Assuming ndjson/jsonl for big data usually\n                if options.get(\"json_lines\", True):  # Default to ndjson scan\n                    return pl.scan_ndjson(full_path, **options)\n                else:\n                    # Standard JSON doesn't support lazy scan well in all versions, fallback to read\n                    return pl.read_json(full_path, **options).lazy()\n\n            elif format == \"delta\":\n                # scan_delta requires 'deltalake' extra usually or feature\n                storage_options = options.get(\"storage_options\", None)\n                version = options.get(\"versionAsOf\", None)\n\n                # scan_delta is available in recent polars\n                # It might accept storage_options in recent versions\n                delta_opts = {}\n                if storage_options:\n                    delta_opts[\"storage_options\"] = storage_options\n                if version is not None:\n                    delta_opts[\"version\"] = version\n\n                return pl.scan_delta(full_path, **delta_opts)\n\n            elif format == \"excel\":\n                # Excel format: delegate to Pandas (best Excel support), convert to Polars\n                from odibi.engine.pandas_engine import PandasEngine\n                from odibi.context import get_logging_context\n\n                ctx = get_logging_context().with_context(engine=\"polars\")\n                ctx.debug(\"Reading Excel via Pandas engine (best Excel support)\")\n\n                # Get storage_options from connection for Azure/cloud authentication\n                excel_storage_options = None\n                if hasattr(connection, \"pandas_storage_options\"):\n                    excel_storage_options = connection.pandas_storage_options()\n\n                # Use Pandas engine for Excel reading\n                pandas_engine = PandasEngine()\n                pdf = pandas_engine._read_excel_with_patterns(\n                    full_path,\n                    sheet_pattern=options.pop(\"sheet_pattern\", None),\n                    sheet_pattern_case_sensitive=options.pop(\"sheet_pattern_case_sensitive\", False),\n                    add_source_file=options.pop(\"add_source_file\", False),\n                    is_glob=\"*\" in str(full_path) or \"?\" in str(full_path),\n                    ctx=ctx,\n                    storage_options=excel_storage_options,\n                    **options,\n                )\n\n                # Convert Pandas DataFrame to Polars LazyFrame\n                ctx.info(f\"Excel read completed (via Pandas): {path}\", row_count=len(pdf))\n                return pl.from_pandas(pdf).lazy()\n\n            else:\n                raise ValueError(\n                    f\"Unsupported format for Polars engine: '{format}'. \"\n                    \"Supported formats: csv, parquet, json, delta, excel, sql, sql_server, azure_sql.\"\n                )\n\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to read {format} from '{full_path}': {e}. \"\n                \"Check that the file exists, the format is correct, and you have read permissions.\"\n            )\n\n    def write(\n        self,\n        df: Any,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        mode: str = \"overwrite\",\n        options: Optional[Dict[str, Any]] = None,\n        streaming_config: Optional[Any] = None,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Write data using Polars.\"\"\"\n        options = options or {}\n\n        if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            return self._write_sql(df, connection, table, mode, options)\n\n        if path:\n            if connection:\n                full_path = connection.get_path(path)\n            else:\n                full_path = path\n        elif table:\n            if connection:\n                full_path = connection.get_path(table)\n            else:\n                raise ValueError(\n                    f\"Cannot write to table '{table}': connection is required when using 'table' parameter. \"\n                    \"Provide a valid connection object or use 'path' for file-based writes.\"\n                )\n        else:\n            raise ValueError(\n                \"Write operation failed: neither 'path' nor 'table' was provided. \"\n                \"Specify a file path or table name in your configuration.\"\n            )\n\n        is_lazy = isinstance(df, pl.LazyFrame)\n\n        parent_dir = os.path.dirname(full_path)\n        if parent_dir:\n            os.makedirs(parent_dir, exist_ok=True)\n\n        if format == \"parquet\":\n            if is_lazy:\n                df.sink_parquet(full_path, **options)\n            else:\n                df.write_parquet(full_path, **options)\n\n        elif format == \"csv\":\n            if is_lazy:\n                df.sink_csv(full_path, **options)\n            else:\n                df.write_csv(full_path, **options)\n\n        elif format == \"json\":\n            if is_lazy:\n                df.sink_ndjson(full_path, **options)\n            else:\n                df.write_ndjson(full_path, **options)\n\n        elif format == \"delta\":\n            if is_lazy:\n                df = df.collect()\n\n            storage_options = options.get(\"storage_options\", None)\n            delta_write_options = options.copy()\n            if \"storage_options\" in delta_write_options:\n                del delta_write_options[\"storage_options\"]\n\n            df.write_delta(\n                full_path, mode=mode, storage_options=storage_options, **delta_write_options\n            )\n\n        else:\n            raise ValueError(\n                f\"Unsupported write format for Polars engine: '{format}'. \"\n                \"Supported formats: csv, parquet, json, delta.\"\n            )\n\n        return None\n\n    def _write_sql(\n        self,\n        df: Any,\n        connection: Any,\n        table: Optional[str],\n        mode: str,\n        options: Dict[str, Any],\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Handle SQL writing including merge and enhanced overwrite for Polars (Phase 4).\"\"\"\n        from odibi.utils.logging_context import get_logging_context\n\n        ctx = get_logging_context().with_context(engine=\"polars\")\n\n        if not hasattr(connection, \"write_table\"):\n            raise ValueError(\n                f\"Connection type '{type(connection).__name__}' does not support SQL operations\"\n            )\n\n        if not table:\n            raise ValueError(\n                \"SQL write operation failed: 'table' parameter is required but was not provided. \"\n                \"Specify the target table name in your configuration.\"\n            )\n\n        if mode == \"merge\":\n            merge_keys = options.get(\"merge_keys\")\n            merge_options = options.get(\"merge_options\")\n\n            if not merge_keys:\n                raise ValueError(\n                    \"MERGE mode requires 'merge_keys' in options. \"\n                    \"Specify the key columns for the MERGE ON clause.\"\n                )\n\n            from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n            writer = SqlServerMergeWriter(connection)\n            ctx.debug(\n                \"Executing SQL Server MERGE (Polars)\",\n                target=table,\n                merge_keys=merge_keys,\n            )\n\n            result = writer.merge_polars(\n                df=df,\n                target_table=table,\n                merge_keys=merge_keys,\n                options=merge_options,\n            )\n\n            ctx.info(\n                \"SQL Server MERGE completed (Polars)\",\n                target=table,\n                inserted=result.inserted,\n                updated=result.updated,\n                deleted=result.deleted,\n            )\n\n            return {\n                \"mode\": \"merge\",\n                \"inserted\": result.inserted,\n                \"updated\": result.updated,\n                \"deleted\": result.deleted,\n                \"total_affected\": result.total_affected,\n            }\n\n        if mode == \"overwrite\" and options.get(\"overwrite_options\"):\n            from odibi.writers.sql_server_writer import SqlServerMergeWriter\n\n            overwrite_options = options.get(\"overwrite_options\")\n            writer = SqlServerMergeWriter(connection)\n\n            ctx.debug(\n                \"Executing SQL Server enhanced overwrite (Polars)\",\n                target=table,\n                strategy=(\n                    overwrite_options.strategy.value\n                    if hasattr(overwrite_options, \"strategy\")\n                    else \"truncate_insert\"\n                ),\n            )\n\n            result = writer.overwrite_polars(\n                df=df,\n                target_table=table,\n                options=overwrite_options,\n            )\n\n            ctx.info(\n                \"SQL Server enhanced overwrite completed (Polars)\",\n                target=table,\n                strategy=result.strategy,\n                rows_written=result.rows_written,\n            )\n\n            return {\n                \"mode\": \"overwrite\",\n                \"strategy\": result.strategy,\n                \"rows_written\": result.rows_written,\n            }\n\n        if isinstance(df, pl.LazyFrame):\n            df = df.collect()\n\n        if \".\" in table:\n            schema, table_name = table.split(\".\", 1)\n        else:\n            schema, table_name = \"dbo\", table\n\n        if_exists = \"replace\"\n        if mode == \"append\":\n            if_exists = \"append\"\n        elif mode == \"fail\":\n            if_exists = \"fail\"\n\n        df_pandas = df.to_pandas()\n        chunksize = options.get(\"chunksize\", 1000)\n\n        connection.write_table(\n            df=df_pandas,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            chunksize=chunksize,\n        )\n        return None\n\n    def execute_sql(self, sql: str, context: Context) -&gt; Any:\n        \"\"\"Execute SQL query using Polars SQLContext.\n\n        Args:\n            sql: SQL query string\n            context: Execution context with registered DataFrames\n\n        Returns:\n            pl.LazyFrame\n        \"\"\"\n        ctx = pl.SQLContext()\n\n        # Register datasets from context\n        # We iterate over all registered names in the context\n        try:\n            names = context.list_names()\n            for name in names:\n                df = context.get(name)\n                # Register LazyFrame or DataFrame\n                # Polars SQLContext supports registering LazyFrame, DataFrame, and some others\n                # We might need to convert if it's not a Polars object, but we assume Polars engine uses Polars objects\n                ctx.register(name, df)\n        except Exception:\n            # If context doesn't support listing or getting, we proceed with empty context\n            # (e.g. if context is not fully compatible or empty)\n            pass\n\n        return ctx.execute(sql, eager=False)\n\n    def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n        \"\"\"Execute built-in operation.\"\"\"\n        # Ensure LazyFrame for consistency if possible, but operations work on both usually.\n        # If DataFrame, some operations might need different methods.\n\n        if operation == \"pivot\":\n            # Pivot requires materialization usually in other engines, but Polars LazyFrame has 'collect' or similar constraints?\n            # Polars lazy pivot is not fully supported in older versions without collect, but check recent.\n            # Pivot changes shape drastically.\n            # params: pivot_column, value_column, group_by, agg_func\n\n            # If lazy, we might need to collect for pivot if lazy pivot isn't supported or experimental.\n            # But let's try to keep it lazy if possible.\n            # As of recent Polars, pivot is available on DataFrame, experimental on LazyFrame?\n            # Actually, 'unstack' or 'pivot' on LazyFrame is limited.\n            # Safe bet: materialize if needed, or use lazy pivot if available.\n\n            # Let's collect if input is lazy, because pivot usually implies strict schema change hard to predict.\n            if isinstance(df, pl.LazyFrame):\n                df = df.collect()\n\n            return df.pivot(\n                index=params.get(\"group_by\"),\n                on=params[\"pivot_column\"],\n                values=params[\"value_column\"],\n                aggregate_function=params.get(\"agg_func\", \"first\"),\n            )  # Returns DataFrame\n\n        elif operation == \"drop_duplicates\":\n            subset = params.get(\"subset\")\n            if isinstance(df, pl.LazyFrame):\n                return df.unique(subset=subset)\n            return df.unique(subset=subset)\n\n        elif operation == \"fillna\":\n            value = params.get(\"value\")\n            # Polars uses fill_null\n            if isinstance(value, dict):\n                # Fill specific columns\n                # value = {'col1': 0, 'col2': 'unknown'}\n                # We need to chain with_columns\n                exprs = []\n                for col, val in value.items():\n                    exprs.append(pl.col(col).fill_null(val))\n                return df.with_columns(exprs)\n            else:\n                # Fill all columns? Polars fill_null requires specifying columns or using all()\n                return df.fill_null(value)\n\n        elif operation == \"drop\":\n            columns = params.get(\"columns\") or params.get(\"labels\")\n            return df.drop(columns)\n\n        elif operation == \"rename\":\n            columns = params.get(\"columns\") or params.get(\"mapper\")\n            return df.rename(columns)\n\n        elif operation == \"sort\":\n            by = params.get(\"by\")\n            descending = not params.get(\"ascending\", True)\n            if isinstance(df, pl.LazyFrame):\n                return df.sort(by, descending=descending)\n            return df.sort(by, descending=descending)\n\n        elif operation == \"sample\":\n            # Sample n or frac\n            n = params.get(\"n\")\n            frac = params.get(\"frac\")\n            seed = params.get(\"random_state\")\n\n            # Lazy sample supported\n            if n is not None:\n                # Note: Polars Lazy sample might be approximate or require 'collect' depending on version/backend?\n                # But usually supported.\n                if isinstance(df, pl.LazyFrame):\n                    # LazyFrame.sample takes n (int) or fraction.\n                    # But polars 0.19+ changed sample signature?\n                    # It's generally `sample(n=..., fraction=..., seed=...)`\n                    return (\n                        df.collect().sample(n=n, seed=seed).lazy()\n                    )  # Collecting for exact sample n on lazy might be needed if not supported?\n                    # Actually, fetch(n) is head. Sample is random.\n                    # Let's materialize for safety with sample as it's often for checks.\n                    pass\n                return df.sample(n=n, seed=seed)\n            elif frac is not None:\n                if isinstance(df, pl.LazyFrame):\n                    # Lazy sampling by fraction is supported\n                    pass  # fall through\n                return df.sample(fraction=frac, seed=seed)\n\n        elif operation == \"filter\":\n            # Legacy or simple filter\n            pass\n\n        else:\n            # Fallback: check if operation is a registered transformer\n            from odibi.context import EngineContext, PandasContext\n            from odibi.registry import FunctionRegistry\n\n            if FunctionRegistry.has_function(operation):\n                func = FunctionRegistry.get_function(operation)\n                param_model = FunctionRegistry.get_param_model(operation)\n\n                # Create EngineContext from current df (use PandasContext as placeholder)\n                engine_ctx = EngineContext(\n                    context=PandasContext(),\n                    df=df,\n                    engine=self,\n                    engine_type=self.engine_type,\n                )\n\n                # Validate and instantiate params\n                if param_model:\n                    validated_params = param_model(**params)\n                    result_ctx = func(engine_ctx, validated_params)\n                else:\n                    result_ctx = func(engine_ctx, **params)\n\n                return result_ctx.df\n\n        return df\n\n    def get_schema(self, df: Any) -&gt; Any:\n        \"\"\"Get DataFrame schema.\"\"\"\n        # Polars schema is a dict {name: DataType}\n        # We can return a dict of strings for compatibility\n        schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n        return {name: str(dtype) for name, dtype in schema.items()}\n\n    def get_shape(self, df: Any) -&gt; tuple:\n        \"\"\"Get DataFrame shape.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            # Expensive to count rows in LazyFrame without scan\n            # But usually shape implies (rows, cols)\n            # columns is cheap. rows requires partial scan or metadata.\n            # Fetching 1 row might give columns.\n            # For exact row count, we need collect(count)\n            cols = len(df.collect_schema().names())\n            rows = df.select(pl.len()).collect().item()\n            return (rows, cols)\n        return df.shape\n\n    def count_rows(self, df: Any) -&gt; int:\n        \"\"\"Count rows in DataFrame.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return df.select(pl.len()).collect().item()\n        return len(df)\n\n    def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Count nulls in specified columns.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            # efficient null count\n            return df.select([pl.col(c).null_count() for c in columns]).collect().to_dicts()[0]\n\n        return df.select([pl.col(c).null_count() for c in columns]).to_dicts()[0]\n\n    def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate DataFrame schema.\"\"\"\n        failures = []\n\n        # Schema is dict-like in Polars\n        current_schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n        current_cols = current_schema.keys()\n\n        if \"required_columns\" in schema_rules:\n            required = schema_rules[\"required_columns\"]\n            missing = set(required) - set(current_cols)\n            if missing:\n                failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n        if \"types\" in schema_rules:\n            for col, expected_type in schema_rules[\"types\"].items():\n                if col not in current_cols:\n                    failures.append(f\"Column '{col}' not found for type validation\")\n                    continue\n\n                actual_type = str(current_schema[col])\n                # Basic type check - simplistic string matching\n                if expected_type.lower() not in actual_type.lower():\n                    failures.append(\n                        f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                    )\n\n        return failures\n\n    def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n        \"\"\"Validate data against rules.\n\n        Args:\n            df: DataFrame or LazyFrame\n            validation_config: ValidationConfig object\n\n        Returns:\n            List of validation failure messages\n        \"\"\"\n        failures = []\n\n        if isinstance(df, pl.LazyFrame):\n            schema = df.collect_schema()\n            columns = schema.names()\n        else:\n            columns = df.columns\n\n        if getattr(validation_config, \"not_empty\", False):\n            count = self.count_rows(df)\n            if count == 0:\n                failures.append(\"DataFrame is empty\")\n\n        if getattr(validation_config, \"no_nulls\", None):\n            cols = validation_config.no_nulls\n            null_counts = self.count_nulls(df, cols)\n            for col, count in null_counts.items():\n                if count &gt; 0:\n                    failures.append(f\"Column '{col}' has {count} null values\")\n\n        if getattr(validation_config, \"schema_validation\", None):\n            schema_failures = self.validate_schema(df, validation_config.schema_validation)\n            failures.extend(schema_failures)\n\n        if getattr(validation_config, \"ranges\", None):\n            for col, bounds in validation_config.ranges.items():\n                if col in columns:\n                    min_val = bounds.get(\"min\")\n                    max_val = bounds.get(\"max\")\n\n                    if min_val is not None:\n                        if isinstance(df, pl.LazyFrame):\n                            min_violations = (\n                                df.filter(pl.col(col) &lt; min_val).select(pl.len()).collect().item()\n                            )\n                        else:\n                            min_violations = len(df.filter(pl.col(col) &lt; min_val))\n                        if min_violations &gt; 0:\n                            failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                    if max_val is not None:\n                        if isinstance(df, pl.LazyFrame):\n                            max_violations = (\n                                df.filter(pl.col(col) &gt; max_val).select(pl.len()).collect().item()\n                            )\n                        else:\n                            max_violations = len(df.filter(pl.col(col) &gt; max_val))\n                        if max_violations &gt; 0:\n                            failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n                else:\n                    failures.append(f\"Column '{col}' not found for range validation\")\n\n        if getattr(validation_config, \"allowed_values\", None):\n            for col, allowed in validation_config.allowed_values.items():\n                if col in columns:\n                    if isinstance(df, pl.LazyFrame):\n                        invalid_count = (\n                            df.filter(~pl.col(col).is_in(allowed)).select(pl.len()).collect().item()\n                        )\n                    else:\n                        invalid_count = len(df.filter(~pl.col(col).is_in(allowed)))\n                    if invalid_count &gt; 0:\n                        failures.append(f\"Column '{col}' has invalid values\")\n                else:\n                    failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n        return failures\n\n    def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get sample rows as list of dictionaries.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return df.limit(n).collect().to_dicts()\n        return df.head(n).to_dicts()\n\n    def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n        \"\"\"Calculate null percentage for each column.\"\"\"\n        if isinstance(df, pl.LazyFrame):\n            # null_count() / count()\n            # We can do this in one expression\n            total_count = df.select(pl.len()).collect().item()\n            if total_count == 0:\n                return {col: 0.0 for col in df.collect_schema().names()}\n\n            cols = df.collect_schema().names()\n            null_counts = df.select([pl.col(c).null_count().alias(c) for c in cols]).collect()\n            return {col: null_counts[col][0] / total_count for col in cols}\n\n        total_count = len(df)\n        if total_count == 0:\n            return {col: 0.0 for col in df.columns}\n\n        null_counts = df.null_count()\n        return {col: null_counts[col][0] / total_count for col in df.columns}\n\n    def table_exists(\n        self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n    ) -&gt; bool:\n        \"\"\"Check if table or location exists.\"\"\"\n        if path:\n            full_path = connection.get_path(path)\n            return os.path.exists(full_path)\n        return False\n\n    def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n        \"\"\"Harmonize DataFrame schema.\"\"\"\n        # policy: SchemaPolicyConfig\n        from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n        # Helper to get current columns/schema\n        if isinstance(df, pl.LazyFrame):\n            current_schema = df.collect_schema()\n        else:\n            current_schema = df.schema\n\n        current_cols = current_schema.names()\n        target_cols = list(target_schema.keys())\n\n        missing = set(target_cols) - set(current_cols)\n        new_cols = set(current_cols) - set(target_cols)\n\n        # 1. Validation\n        if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FAIL:\n            raise ValueError(\n                f\"Schema Policy Violation: DataFrame is missing required columns {missing}. \"\n                f\"Available columns: {current_cols}. Add missing columns or set on_missing_columns policy.\"\n            )\n\n        if new_cols and getattr(policy, \"on_new_columns\", None) == OnNewColumns.FAIL:\n            raise ValueError(\n                f\"Schema Policy Violation: DataFrame contains unexpected columns {new_cols}. \"\n                f\"Expected columns: {target_cols}. Remove extra columns or set on_new_columns policy.\"\n            )\n\n        # 2. Transformations\n        exprs = []\n\n        # Handle Missing (Add nulls)\n        # Evolve means we keep new columns, Enforce means we select only target\n        mode = getattr(policy, \"mode\", SchemaMode.ENFORCE)\n\n        if (\n            mode == SchemaMode.EVOLVE\n            and getattr(policy, \"on_new_columns\", None) == OnNewColumns.ADD_NULLABLE\n        ):\n            # Add missing (if missing cols exist, we fill them with nulls)\n            # on_missing_columns controls what to do with missing target cols.\n            # If mode is EVOLVE, we typically keep everything?\n            # But harmonize_schema is about matching a TARGET schema.\n            # If target has cols that df doesn't:\n            # If on_missing_columns == FILL_NULL -&gt; Add them as null.\n            pass\n\n        # We should respect on_missing_columns regardless of mode?\n        if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FILL_NULL:\n            for col in missing:\n                exprs.append(pl.lit(None).alias(col))\n\n        if exprs:\n            df = df.with_columns(exprs)\n\n        # Now Select\n        if mode == SchemaMode.ENFORCE:\n            # Select only target columns.\n            # Missing columns were added above if configured.\n            # New columns (not in target) are dropped implicitly by selecting target_cols.\n            # But wait, we added exprs to df (lazy).\n\n            final_cols = []\n            for col in target_cols:\n                final_cols.append(pl.col(col))\n\n            df = df.select(final_cols)\n\n        elif mode == SchemaMode.EVOLVE:\n            # We keep new columns.\n            # If target has columns that were missing in df, we added them above (if FILL_NULL).\n            # If df has columns not in target (new_cols), we keep them.\n            pass\n\n        return df\n\n    def anonymize(\n        self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n    ) -&gt; Any:\n        \"\"\"Anonymize specified columns.\"\"\"\n        if method == \"mask\":\n            # Mask all but last 4 characters: '******1234'\n            # Regex look-around not supported in some envs.\n            # Manual approach:\n            # If len &gt; 4: repeat('*', len-4) + suffix(4)\n            # Else: keep original (or mask all? Pandas engine masked all but last 4, which implies keeping small strings?)\n            # Pandas: .str.replace(r\".(?=.{4})\", \"*\") -&gt; replaces chars that are followed by 4 chars.\n            # If str is \"123\", no char is followed by 4 chars -&gt; \"123\".\n            # If str is \"12345\", '1' is followed by '2345' (4 chars) -&gt; \"*2345\".\n\n            return df.with_columns(\n                [\n                    pl.when(pl.col(c).cast(pl.Utf8).str.len_chars() &gt; 4)\n                    .then(\n                        pl.concat_str(\n                            [\n                                pl.lit(\"*\").repeat_by(pl.col(c).str.len_chars() - 4).list.join(\"\"),\n                                pl.col(c).str.slice(-4),\n                            ]\n                        )\n                    )\n                    .otherwise(pl.col(c).cast(pl.Utf8))\n                    .alias(c)\n                    for c in columns\n                ]\n            )\n\n        elif method == \"hash\":\n            # Polars hash() is non-cryptographic usually (xxHash).\n            # For cryptographic hash (sha256), we might need map_elements (slow) or plugin.\n            # Requirement is just 'hash', often consistent for analytics.\n            # Gap Analysis mentions \"salt\".\n            # PandasEngine used sha256 with salt.\n            # Polars `hash` is fast 64-bit hash.\n            # If we need SHA256, we must use map_elements (python UDF) or custom.\n            # For \"High Performance\", map_elements is bad.\n            # However, without native plugin, we have no choice for SHA256.\n            # Let's implement SHA256 via map_elements for compatibility,\n            # OR use Polars internal hash if user accepts non-crypto.\n            # But \"salt\" implies security/crypto usage.\n\n            def _hash_val(val):\n                if val is None:\n                    return None\n                to_hash = str(val)\n                if salt:\n                    to_hash += salt\n                return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n            # Apply to each column. Warning: Slow path.\n            # But Polars UDFs are still faster than Pandas apply often due to no GIL? No, Python UDF has GIL.\n            return df.with_columns(\n                [pl.col(c).map_elements(_hash_val, return_dtype=pl.Utf8).alias(c) for c in columns]\n            )\n\n        elif method == \"redact\":\n            return df.with_columns([pl.lit(\"[REDACTED]\").alias(c) for c in columns])\n\n        return df\n\n    def get_table_schema(\n        self,\n        connection: Any,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        format: Optional[str] = None,\n    ) -&gt; Optional[Dict[str, str]]:\n        \"\"\"Get schema of an existing table/file.\n\n        Args:\n            connection: Connection object\n            table: Table name\n            path: File path\n            format: Data format (optional, helps with file-based sources)\n\n        Returns:\n            Schema dict or None if table doesn't exist or schema fetch fails.\n        \"\"\"\n        from odibi.utils.logging_context import get_logging_context\n\n        ctx = get_logging_context().with_context(engine=\"polars\")\n\n        try:\n            if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n                query = f\"SELECT TOP 0 * FROM {table}\"\n                df = connection.read_sql(query)\n                return {col: str(dtype) for col, dtype in zip(df.columns, df.dtypes)}\n\n            if path:\n                full_path = connection.get_path(path) if connection else path\n                if not os.path.exists(full_path):\n                    return None\n\n                if format == \"delta\":\n                    try:\n                        from deltalake import DeltaTable\n\n                        dt = DeltaTable(full_path)\n                        arrow_schema = dt.schema().to_pyarrow()\n                        return {field.name: str(field.type) for field in arrow_schema}\n                    except ImportError:\n                        ctx.warning(\n                            \"deltalake library not installed for schema introspection\",\n                            path=full_path,\n                        )\n                        return None\n\n                elif format == \"parquet\":\n                    try:\n                        import pyarrow.parquet as pq\n                        import glob as glob_mod\n\n                        target_path = full_path\n                        if os.path.isdir(full_path):\n                            files = glob_mod.glob(os.path.join(full_path, \"*.parquet\"))\n                            if not files:\n                                return None\n                            target_path = files[0]\n\n                        schema = pq.read_schema(target_path)\n                        return {field.name: str(field.type) for field in schema}\n                    except ImportError:\n                        lf = pl.scan_parquet(full_path)\n                        schema = lf.collect_schema()\n                        return {name: str(dtype) for name, dtype in schema.items()}\n\n                elif format == \"csv\":\n                    lf = pl.scan_csv(full_path)\n                    schema = lf.collect_schema()\n                    return {name: str(dtype) for name, dtype in schema.items()}\n\n        except (FileNotFoundError, PermissionError):\n            return None\n        except Exception as e:\n            ctx.warning(f\"Failed to infer schema for {table or path}: {e}\")\n            return None\n\n        return None\n\n    def maintain_table(\n        self,\n        connection: Any,\n        format: str,\n        table: Optional[str] = None,\n        path: Optional[str] = None,\n        config: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Run table maintenance operations (optimize, vacuum) for Delta tables.\n\n        Args:\n            connection: Connection object\n            format: Table format\n            table: Table name\n            path: Table path\n            config: AutoOptimizeConfig object\n        \"\"\"\n        from odibi.utils.logging_context import get_logging_context\n\n        ctx = get_logging_context().with_context(engine=\"polars\")\n\n        if format != \"delta\" or not config or not getattr(config, \"enabled\", False):\n            return\n\n        if not path and not table:\n            return\n\n        full_path = connection.get_path(path if path else table) if connection else (path or table)\n\n        ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.warning(\n                \"Auto-optimize skipped: 'deltalake' library not installed\",\n                path=str(full_path),\n            )\n            return\n\n        try:\n            import time\n\n            start = time.time()\n\n            storage_opts = {}\n            if hasattr(connection, \"pandas_storage_options\"):\n                storage_opts = connection.pandas_storage_options()\n\n            dt = DeltaTable(full_path, storage_options=storage_opts)\n\n            ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n            dt.optimize.compact()\n\n            retention = getattr(config, \"vacuum_retention_hours\", None)\n            if retention is not None and retention &gt; 0:\n                ctx.info(\n                    \"Running Delta VACUUM\",\n                    path=str(full_path),\n                    retention_hours=retention,\n                )\n                dt.vacuum(\n                    retention_hours=retention,\n                    enforce_retention_duration=True,\n                    dry_run=False,\n                )\n\n            elapsed = (time.time() - start) * 1000\n            ctx.info(\n                \"Table maintenance completed\",\n                path=str(full_path),\n                elapsed_ms=round(elapsed, 2),\n            )\n\n        except Exception as e:\n            ctx.warning(\n                \"Auto-optimize failed\",\n                path=str(full_path),\n                error=str(e),\n            )\n\n    def get_source_files(self, df: Any) -&gt; List[str]:\n        \"\"\"Get list of source files that generated this DataFrame.\n\n        For Polars, this checks if source file info was stored\n        in the DataFrame's metadata during read.\n\n        Args:\n            df: DataFrame or LazyFrame\n\n        Returns:\n            List of file paths (or empty list if not applicable/supported)\n        \"\"\"\n        if isinstance(df, pl.LazyFrame):\n            return []\n\n        if hasattr(df, \"attrs\"):\n            return df.attrs.get(\"odibi_source_files\", [])\n\n        return []\n\n    def vacuum_delta(\n        self,\n        connection: Any,\n        path: str,\n        retention_hours: int = 168,\n        dry_run: bool = False,\n        enforce_retention_duration: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"VACUUM a Delta table to remove old files.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            retention_hours: Retention period (default 168 = 7 days)\n            dry_run: If True, only show files to be deleted\n            enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n        Returns:\n            Dictionary with files_deleted count\n        \"\"\"\n        from odibi.utils.logging_context import get_logging_context\n        import time\n\n        ctx = get_logging_context().with_context(engine=\"polars\")\n        start = time.time()\n\n        ctx.debug(\n            \"Starting Delta VACUUM\",\n            path=path,\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n        )\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[polars]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path) if connection else path\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        deleted_files = dt.vacuum(\n            retention_hours=retention_hours,\n            dry_run=dry_run,\n            enforce_retention_duration=enforce_retention_duration,\n        )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta VACUUM completed\",\n            path=str(full_path),\n            files_deleted=len(deleted_files),\n            dry_run=dry_run,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return {\"files_deleted\": len(deleted_files)}\n\n    def get_delta_history(\n        self, connection: Any, path: str, limit: Optional[int] = None\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get Delta table history.\n\n        Args:\n            connection: Connection object\n            path: Delta table path\n            limit: Maximum number of versions to return\n\n        Returns:\n            List of version metadata dictionaries\n        \"\"\"\n        from odibi.utils.logging_context import get_logging_context\n        import time\n\n        ctx = get_logging_context().with_context(engine=\"polars\")\n        start = time.time()\n\n        ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n        try:\n            from deltalake import DeltaTable\n        except ImportError:\n            ctx.error(\"Delta Lake library not installed\", path=path)\n            raise ImportError(\n                \"Delta Lake support requires 'pip install odibi[polars]' \"\n                \"or 'pip install deltalake'. See README.md for installation instructions.\"\n            )\n\n        full_path = connection.get_path(path) if connection else path\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n        history = dt.history(limit=limit)\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Delta history retrieved\",\n            path=str(full_path),\n            versions_returned=len(history) if history else 0,\n            elapsed_ms=round(elapsed, 2),\n        )\n\n        return history\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.__init__","title":"<code>__init__(connections=None, config=None)</code>","text":"<p>Initialize Polars engine.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of connection objects</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Engine configuration (optional)</p> <code>None</code> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def __init__(\n    self,\n    connections: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize Polars engine.\n\n    Args:\n        connections: Dictionary of connection objects\n        config: Engine configuration (optional)\n    \"\"\"\n    if pl is None:\n        raise ImportError(\"Polars not installed. Run 'pip install polars'.\")\n\n    self.connections = connections or {}\n    self.config = config or {}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.anonymize","title":"<code>anonymize(df, columns, method, salt=None)</code>","text":"<p>Anonymize specified columns.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def anonymize(\n    self, df: Any, columns: List[str], method: str, salt: Optional[str] = None\n) -&gt; Any:\n    \"\"\"Anonymize specified columns.\"\"\"\n    if method == \"mask\":\n        # Mask all but last 4 characters: '******1234'\n        # Regex look-around not supported in some envs.\n        # Manual approach:\n        # If len &gt; 4: repeat('*', len-4) + suffix(4)\n        # Else: keep original (or mask all? Pandas engine masked all but last 4, which implies keeping small strings?)\n        # Pandas: .str.replace(r\".(?=.{4})\", \"*\") -&gt; replaces chars that are followed by 4 chars.\n        # If str is \"123\", no char is followed by 4 chars -&gt; \"123\".\n        # If str is \"12345\", '1' is followed by '2345' (4 chars) -&gt; \"*2345\".\n\n        return df.with_columns(\n            [\n                pl.when(pl.col(c).cast(pl.Utf8).str.len_chars() &gt; 4)\n                .then(\n                    pl.concat_str(\n                        [\n                            pl.lit(\"*\").repeat_by(pl.col(c).str.len_chars() - 4).list.join(\"\"),\n                            pl.col(c).str.slice(-4),\n                        ]\n                    )\n                )\n                .otherwise(pl.col(c).cast(pl.Utf8))\n                .alias(c)\n                for c in columns\n            ]\n        )\n\n    elif method == \"hash\":\n        # Polars hash() is non-cryptographic usually (xxHash).\n        # For cryptographic hash (sha256), we might need map_elements (slow) or plugin.\n        # Requirement is just 'hash', often consistent for analytics.\n        # Gap Analysis mentions \"salt\".\n        # PandasEngine used sha256 with salt.\n        # Polars `hash` is fast 64-bit hash.\n        # If we need SHA256, we must use map_elements (python UDF) or custom.\n        # For \"High Performance\", map_elements is bad.\n        # However, without native plugin, we have no choice for SHA256.\n        # Let's implement SHA256 via map_elements for compatibility,\n        # OR use Polars internal hash if user accepts non-crypto.\n        # But \"salt\" implies security/crypto usage.\n\n        def _hash_val(val):\n            if val is None:\n                return None\n            to_hash = str(val)\n            if salt:\n                to_hash += salt\n            return hashlib.sha256(to_hash.encode(\"utf-8\")).hexdigest()\n\n        # Apply to each column. Warning: Slow path.\n        # But Polars UDFs are still faster than Pandas apply often due to no GIL? No, Python UDF has GIL.\n        return df.with_columns(\n            [pl.col(c).map_elements(_hash_val, return_dtype=pl.Utf8).alias(c) for c in columns]\n        )\n\n    elif method == \"redact\":\n        return df.with_columns([pl.lit(\"[REDACTED]\").alias(c) for c in columns])\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.count_nulls","title":"<code>count_nulls(df, columns)</code>","text":"<p>Count nulls in specified columns.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def count_nulls(self, df: Any, columns: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count nulls in specified columns.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        # efficient null count\n        return df.select([pl.col(c).null_count() for c in columns]).collect().to_dicts()[0]\n\n    return df.select([pl.col(c).null_count() for c in columns]).to_dicts()[0]\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.count_rows","title":"<code>count_rows(df)</code>","text":"<p>Count rows in DataFrame.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def count_rows(self, df: Any) -&gt; int:\n    \"\"\"Count rows in DataFrame.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return df.select(pl.len()).collect().item()\n    return len(df)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.execute_operation","title":"<code>execute_operation(operation, params, df)</code>","text":"<p>Execute built-in operation.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def execute_operation(self, operation: str, params: Dict[str, Any], df: Any) -&gt; Any:\n    \"\"\"Execute built-in operation.\"\"\"\n    # Ensure LazyFrame for consistency if possible, but operations work on both usually.\n    # If DataFrame, some operations might need different methods.\n\n    if operation == \"pivot\":\n        # Pivot requires materialization usually in other engines, but Polars LazyFrame has 'collect' or similar constraints?\n        # Polars lazy pivot is not fully supported in older versions without collect, but check recent.\n        # Pivot changes shape drastically.\n        # params: pivot_column, value_column, group_by, agg_func\n\n        # If lazy, we might need to collect for pivot if lazy pivot isn't supported or experimental.\n        # But let's try to keep it lazy if possible.\n        # As of recent Polars, pivot is available on DataFrame, experimental on LazyFrame?\n        # Actually, 'unstack' or 'pivot' on LazyFrame is limited.\n        # Safe bet: materialize if needed, or use lazy pivot if available.\n\n        # Let's collect if input is lazy, because pivot usually implies strict schema change hard to predict.\n        if isinstance(df, pl.LazyFrame):\n            df = df.collect()\n\n        return df.pivot(\n            index=params.get(\"group_by\"),\n            on=params[\"pivot_column\"],\n            values=params[\"value_column\"],\n            aggregate_function=params.get(\"agg_func\", \"first\"),\n        )  # Returns DataFrame\n\n    elif operation == \"drop_duplicates\":\n        subset = params.get(\"subset\")\n        if isinstance(df, pl.LazyFrame):\n            return df.unique(subset=subset)\n        return df.unique(subset=subset)\n\n    elif operation == \"fillna\":\n        value = params.get(\"value\")\n        # Polars uses fill_null\n        if isinstance(value, dict):\n            # Fill specific columns\n            # value = {'col1': 0, 'col2': 'unknown'}\n            # We need to chain with_columns\n            exprs = []\n            for col, val in value.items():\n                exprs.append(pl.col(col).fill_null(val))\n            return df.with_columns(exprs)\n        else:\n            # Fill all columns? Polars fill_null requires specifying columns or using all()\n            return df.fill_null(value)\n\n    elif operation == \"drop\":\n        columns = params.get(\"columns\") or params.get(\"labels\")\n        return df.drop(columns)\n\n    elif operation == \"rename\":\n        columns = params.get(\"columns\") or params.get(\"mapper\")\n        return df.rename(columns)\n\n    elif operation == \"sort\":\n        by = params.get(\"by\")\n        descending = not params.get(\"ascending\", True)\n        if isinstance(df, pl.LazyFrame):\n            return df.sort(by, descending=descending)\n        return df.sort(by, descending=descending)\n\n    elif operation == \"sample\":\n        # Sample n or frac\n        n = params.get(\"n\")\n        frac = params.get(\"frac\")\n        seed = params.get(\"random_state\")\n\n        # Lazy sample supported\n        if n is not None:\n            # Note: Polars Lazy sample might be approximate or require 'collect' depending on version/backend?\n            # But usually supported.\n            if isinstance(df, pl.LazyFrame):\n                # LazyFrame.sample takes n (int) or fraction.\n                # But polars 0.19+ changed sample signature?\n                # It's generally `sample(n=..., fraction=..., seed=...)`\n                return (\n                    df.collect().sample(n=n, seed=seed).lazy()\n                )  # Collecting for exact sample n on lazy might be needed if not supported?\n                # Actually, fetch(n) is head. Sample is random.\n                # Let's materialize for safety with sample as it's often for checks.\n                pass\n            return df.sample(n=n, seed=seed)\n        elif frac is not None:\n            if isinstance(df, pl.LazyFrame):\n                # Lazy sampling by fraction is supported\n                pass  # fall through\n            return df.sample(fraction=frac, seed=seed)\n\n    elif operation == \"filter\":\n        # Legacy or simple filter\n        pass\n\n    else:\n        # Fallback: check if operation is a registered transformer\n        from odibi.context import EngineContext, PandasContext\n        from odibi.registry import FunctionRegistry\n\n        if FunctionRegistry.has_function(operation):\n            func = FunctionRegistry.get_function(operation)\n            param_model = FunctionRegistry.get_param_model(operation)\n\n            # Create EngineContext from current df (use PandasContext as placeholder)\n            engine_ctx = EngineContext(\n                context=PandasContext(),\n                df=df,\n                engine=self,\n                engine_type=self.engine_type,\n            )\n\n            # Validate and instantiate params\n            if param_model:\n                validated_params = param_model(**params)\n                result_ctx = func(engine_ctx, validated_params)\n            else:\n                result_ctx = func(engine_ctx, **params)\n\n            return result_ctx.df\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.execute_sql","title":"<code>execute_sql(sql, context)</code>","text":"<p>Execute SQL query using Polars SQLContext.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <code>context</code> <code>Context</code> <p>Execution context with registered DataFrames</p> required <p>Returns:</p> Type Description <code>Any</code> <p>pl.LazyFrame</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def execute_sql(self, sql: str, context: Context) -&gt; Any:\n    \"\"\"Execute SQL query using Polars SQLContext.\n\n    Args:\n        sql: SQL query string\n        context: Execution context with registered DataFrames\n\n    Returns:\n        pl.LazyFrame\n    \"\"\"\n    ctx = pl.SQLContext()\n\n    # Register datasets from context\n    # We iterate over all registered names in the context\n    try:\n        names = context.list_names()\n        for name in names:\n            df = context.get(name)\n            # Register LazyFrame or DataFrame\n            # Polars SQLContext supports registering LazyFrame, DataFrame, and some others\n            # We might need to convert if it's not a Polars object, but we assume Polars engine uses Polars objects\n            ctx.register(name, df)\n    except Exception:\n        # If context doesn't support listing or getting, we proceed with empty context\n        # (e.g. if context is not fully compatible or empty)\n        pass\n\n    return ctx.execute(sql, eager=False)\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_delta_history","title":"<code>get_delta_history(connection, path, limit=None)</code>","text":"<p>Get Delta table history.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>limit</code> <code>Optional[int]</code> <p>Maximum number of versions to return</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of version metadata dictionaries</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_delta_history(\n    self, connection: Any, path: str, limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get Delta table history.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        limit: Maximum number of versions to return\n\n    Returns:\n        List of version metadata dictionaries\n    \"\"\"\n    from odibi.utils.logging_context import get_logging_context\n    import time\n\n    ctx = get_logging_context().with_context(engine=\"polars\")\n    start = time.time()\n\n    ctx.debug(\"Getting Delta table history\", path=path, limit=limit)\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[polars]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path) if connection else path\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    history = dt.history(limit=limit)\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta history retrieved\",\n        path=str(full_path),\n        versions_returned=len(history) if history else 0,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return history\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_sample","title":"<code>get_sample(df, n=10)</code>","text":"<p>Get sample rows as list of dictionaries.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_sample(self, df: Any, n: int = 10) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get sample rows as list of dictionaries.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return df.limit(n).collect().to_dicts()\n    return df.head(n).to_dicts()\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_schema","title":"<code>get_schema(df)</code>","text":"<p>Get DataFrame schema.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_schema(self, df: Any) -&gt; Any:\n    \"\"\"Get DataFrame schema.\"\"\"\n    # Polars schema is a dict {name: DataType}\n    # We can return a dict of strings for compatibility\n    schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n    return {name: str(dtype) for name, dtype in schema.items()}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_shape","title":"<code>get_shape(df)</code>","text":"<p>Get DataFrame shape.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_shape(self, df: Any) -&gt; tuple:\n    \"\"\"Get DataFrame shape.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        # Expensive to count rows in LazyFrame without scan\n        # But usually shape implies (rows, cols)\n        # columns is cheap. rows requires partial scan or metadata.\n        # Fetching 1 row might give columns.\n        # For exact row count, we need collect(count)\n        cols = len(df.collect_schema().names())\n        rows = df.select(pl.len()).collect().item()\n        return (rows, cols)\n    return df.shape\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_source_files","title":"<code>get_source_files(df)</code>","text":"<p>Get list of source files that generated this DataFrame.</p> <p>For Polars, this checks if source file info was stored in the DataFrame's metadata during read.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyFrame</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths (or empty list if not applicable/supported)</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_source_files(self, df: Any) -&gt; List[str]:\n    \"\"\"Get list of source files that generated this DataFrame.\n\n    For Polars, this checks if source file info was stored\n    in the DataFrame's metadata during read.\n\n    Args:\n        df: DataFrame or LazyFrame\n\n    Returns:\n        List of file paths (or empty list if not applicable/supported)\n    \"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return []\n\n    if hasattr(df, \"attrs\"):\n        return df.attrs.get(\"odibi_source_files\", [])\n\n    return []\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.get_table_schema","title":"<code>get_table_schema(connection, table=None, path=None, format=None)</code>","text":"<p>Get schema of an existing table/file.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>File path</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Data format (optional, helps with file-based sources)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, str]]</code> <p>Schema dict or None if table doesn't exist or schema fetch fails.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def get_table_schema(\n    self,\n    connection: Any,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n) -&gt; Optional[Dict[str, str]]:\n    \"\"\"Get schema of an existing table/file.\n\n    Args:\n        connection: Connection object\n        table: Table name\n        path: File path\n        format: Data format (optional, helps with file-based sources)\n\n    Returns:\n        Schema dict or None if table doesn't exist or schema fetch fails.\n    \"\"\"\n    from odibi.utils.logging_context import get_logging_context\n\n    ctx = get_logging_context().with_context(engine=\"polars\")\n\n    try:\n        if table and format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n            query = f\"SELECT TOP 0 * FROM {table}\"\n            df = connection.read_sql(query)\n            return {col: str(dtype) for col, dtype in zip(df.columns, df.dtypes)}\n\n        if path:\n            full_path = connection.get_path(path) if connection else path\n            if not os.path.exists(full_path):\n                return None\n\n            if format == \"delta\":\n                try:\n                    from deltalake import DeltaTable\n\n                    dt = DeltaTable(full_path)\n                    arrow_schema = dt.schema().to_pyarrow()\n                    return {field.name: str(field.type) for field in arrow_schema}\n                except ImportError:\n                    ctx.warning(\n                        \"deltalake library not installed for schema introspection\",\n                        path=full_path,\n                    )\n                    return None\n\n            elif format == \"parquet\":\n                try:\n                    import pyarrow.parquet as pq\n                    import glob as glob_mod\n\n                    target_path = full_path\n                    if os.path.isdir(full_path):\n                        files = glob_mod.glob(os.path.join(full_path, \"*.parquet\"))\n                        if not files:\n                            return None\n                        target_path = files[0]\n\n                    schema = pq.read_schema(target_path)\n                    return {field.name: str(field.type) for field in schema}\n                except ImportError:\n                    lf = pl.scan_parquet(full_path)\n                    schema = lf.collect_schema()\n                    return {name: str(dtype) for name, dtype in schema.items()}\n\n            elif format == \"csv\":\n                lf = pl.scan_csv(full_path)\n                schema = lf.collect_schema()\n                return {name: str(dtype) for name, dtype in schema.items()}\n\n    except (FileNotFoundError, PermissionError):\n        return None\n    except Exception as e:\n        ctx.warning(f\"Failed to infer schema for {table or path}: {e}\")\n        return None\n\n    return None\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.harmonize_schema","title":"<code>harmonize_schema(df, target_schema, policy)</code>","text":"<p>Harmonize DataFrame schema.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def harmonize_schema(self, df: Any, target_schema: Dict[str, str], policy: Any) -&gt; Any:\n    \"\"\"Harmonize DataFrame schema.\"\"\"\n    # policy: SchemaPolicyConfig\n    from odibi.config import OnMissingColumns, OnNewColumns, SchemaMode\n\n    # Helper to get current columns/schema\n    if isinstance(df, pl.LazyFrame):\n        current_schema = df.collect_schema()\n    else:\n        current_schema = df.schema\n\n    current_cols = current_schema.names()\n    target_cols = list(target_schema.keys())\n\n    missing = set(target_cols) - set(current_cols)\n    new_cols = set(current_cols) - set(target_cols)\n\n    # 1. Validation\n    if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FAIL:\n        raise ValueError(\n            f\"Schema Policy Violation: DataFrame is missing required columns {missing}. \"\n            f\"Available columns: {current_cols}. Add missing columns or set on_missing_columns policy.\"\n        )\n\n    if new_cols and getattr(policy, \"on_new_columns\", None) == OnNewColumns.FAIL:\n        raise ValueError(\n            f\"Schema Policy Violation: DataFrame contains unexpected columns {new_cols}. \"\n            f\"Expected columns: {target_cols}. Remove extra columns or set on_new_columns policy.\"\n        )\n\n    # 2. Transformations\n    exprs = []\n\n    # Handle Missing (Add nulls)\n    # Evolve means we keep new columns, Enforce means we select only target\n    mode = getattr(policy, \"mode\", SchemaMode.ENFORCE)\n\n    if (\n        mode == SchemaMode.EVOLVE\n        and getattr(policy, \"on_new_columns\", None) == OnNewColumns.ADD_NULLABLE\n    ):\n        # Add missing (if missing cols exist, we fill them with nulls)\n        # on_missing_columns controls what to do with missing target cols.\n        # If mode is EVOLVE, we typically keep everything?\n        # But harmonize_schema is about matching a TARGET schema.\n        # If target has cols that df doesn't:\n        # If on_missing_columns == FILL_NULL -&gt; Add them as null.\n        pass\n\n    # We should respect on_missing_columns regardless of mode?\n    if missing and getattr(policy, \"on_missing_columns\", None) == OnMissingColumns.FILL_NULL:\n        for col in missing:\n            exprs.append(pl.lit(None).alias(col))\n\n    if exprs:\n        df = df.with_columns(exprs)\n\n    # Now Select\n    if mode == SchemaMode.ENFORCE:\n        # Select only target columns.\n        # Missing columns were added above if configured.\n        # New columns (not in target) are dropped implicitly by selecting target_cols.\n        # But wait, we added exprs to df (lazy).\n\n        final_cols = []\n        for col in target_cols:\n            final_cols.append(pl.col(col))\n\n        df = df.select(final_cols)\n\n    elif mode == SchemaMode.EVOLVE:\n        # We keep new columns.\n        # If target has columns that were missing in df, we added them above (if FILL_NULL).\n        # If df has columns not in target (new_cols), we keep them.\n        pass\n\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.maintain_table","title":"<code>maintain_table(connection, format, table=None, path=None, config=None)</code>","text":"<p>Run table maintenance operations (optimize, vacuum) for Delta tables.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>format</code> <code>str</code> <p>Table format</p> required <code>table</code> <code>Optional[str]</code> <p>Table name</p> <code>None</code> <code>path</code> <code>Optional[str]</code> <p>Table path</p> <code>None</code> <code>config</code> <code>Optional[Any]</code> <p>AutoOptimizeConfig object</p> <code>None</code> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def maintain_table(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    config: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Run table maintenance operations (optimize, vacuum) for Delta tables.\n\n    Args:\n        connection: Connection object\n        format: Table format\n        table: Table name\n        path: Table path\n        config: AutoOptimizeConfig object\n    \"\"\"\n    from odibi.utils.logging_context import get_logging_context\n\n    ctx = get_logging_context().with_context(engine=\"polars\")\n\n    if format != \"delta\" or not config or not getattr(config, \"enabled\", False):\n        return\n\n    if not path and not table:\n        return\n\n    full_path = connection.get_path(path if path else table) if connection else (path or table)\n\n    ctx.info(\"Starting table maintenance\", path=str(full_path))\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.warning(\n            \"Auto-optimize skipped: 'deltalake' library not installed\",\n            path=str(full_path),\n        )\n        return\n\n    try:\n        import time\n\n        start = time.time()\n\n        storage_opts = {}\n        if hasattr(connection, \"pandas_storage_options\"):\n            storage_opts = connection.pandas_storage_options()\n\n        dt = DeltaTable(full_path, storage_options=storage_opts)\n\n        ctx.info(\"Running Delta OPTIMIZE (compaction)\", path=str(full_path))\n        dt.optimize.compact()\n\n        retention = getattr(config, \"vacuum_retention_hours\", None)\n        if retention is not None and retention &gt; 0:\n            ctx.info(\n                \"Running Delta VACUUM\",\n                path=str(full_path),\n                retention_hours=retention,\n            )\n            dt.vacuum(\n                retention_hours=retention,\n                enforce_retention_duration=True,\n                dry_run=False,\n            )\n\n        elapsed = (time.time() - start) * 1000\n        ctx.info(\n            \"Table maintenance completed\",\n            path=str(full_path),\n            elapsed_ms=round(elapsed, 2),\n        )\n\n    except Exception as e:\n        ctx.warning(\n            \"Auto-optimize failed\",\n            path=str(full_path),\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.materialize","title":"<code>materialize(df)</code>","text":"<p>Materialize lazy dataset into memory (DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>LazyFrame or DataFrame</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Materialized DataFrame (pl.DataFrame)</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def materialize(self, df: Any) -&gt; Any:\n    \"\"\"Materialize lazy dataset into memory (DataFrame).\n\n    Args:\n        df: LazyFrame or DataFrame\n\n    Returns:\n        Materialized DataFrame (pl.DataFrame)\n    \"\"\"\n    if isinstance(df, pl.LazyFrame):\n        return df.collect()\n    return df\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.profile_nulls","title":"<code>profile_nulls(df)</code>","text":"<p>Calculate null percentage for each column.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def profile_nulls(self, df: Any) -&gt; Dict[str, float]:\n    \"\"\"Calculate null percentage for each column.\"\"\"\n    if isinstance(df, pl.LazyFrame):\n        # null_count() / count()\n        # We can do this in one expression\n        total_count = df.select(pl.len()).collect().item()\n        if total_count == 0:\n            return {col: 0.0 for col in df.collect_schema().names()}\n\n        cols = df.collect_schema().names()\n        null_counts = df.select([pl.col(c).null_count().alias(c) for c in cols]).collect()\n        return {col: null_counts[col][0] / total_count for col in cols}\n\n    total_count = len(df)\n    if total_count == 0:\n        return {col: 0.0 for col in df.columns}\n\n    null_counts = df.null_count()\n    return {col: null_counts[col][0] / total_count for col in df.columns}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.read","title":"<code>read(connection, format, table=None, path=None, streaming=False, schema=None, options=None, **kwargs)</code>","text":"<p>Read data using Polars (Lazy by default).</p> <p>Returns:</p> Type Description <code>Any</code> <p>pl.LazyFrame or pl.DataFrame</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def read(\n    self,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    streaming: bool = False,\n    schema: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Read data using Polars (Lazy by default).\n\n    Returns:\n        pl.LazyFrame or pl.DataFrame\n    \"\"\"\n    options = options or {}\n\n    # SQL Server / Azure SQL Support\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        if not hasattr(connection, \"read_table\") and not hasattr(connection, \"read_sql_query\"):\n            raise ValueError(\n                f\"Cannot read SQL table: connection type '{type(connection).__name__}' \"\n                \"does not support SQL operations. Use a SQL-compatible connection.\"\n            )\n        table_name = table or path\n        if not table_name:\n            raise ValueError(\"SQL read requires 'table' or 'path' to specify the table name.\")\n        if \".\" in table_name:\n            schema_name, tbl = table_name.split(\".\", 1)\n        else:\n            schema_name, tbl = \"dbo\", table_name\n\n        # Check for incremental SQL filter in options\n        sql_filter = options.get(\"filter\")\n\n        if sql_filter and hasattr(connection, \"read_sql_query\"):\n            # Build query with WHERE clause for SQL pushdown\n            if schema_name:\n                base_query = f\"SELECT * FROM [{schema_name}].[{tbl}]\"\n            else:\n                base_query = f\"SELECT * FROM [{tbl}]\"\n            full_query = f\"{base_query} WHERE {sql_filter}\"\n            pdf = connection.read_sql_query(full_query)\n        else:\n            # read_table returns Pandas DataFrame\n            pdf = connection.read_table(table_name=tbl, schema=schema_name)\n\n        return pl.from_pandas(pdf).lazy()\n\n    # Get full path\n    if path:\n        if connection:\n            full_path = connection.get_path(path)\n        else:\n            full_path = path\n    elif table:\n        if connection:\n            full_path = connection.get_path(table)\n        else:\n            raise ValueError(\n                f\"Cannot read table '{table}': connection is required when using 'table' parameter. \"\n                \"Provide a valid connection object or use 'path' for file-based reads.\"\n            )\n    else:\n        raise ValueError(\n            \"Read operation failed: neither 'path' nor 'table' was provided. \"\n            \"Specify a file path or table name in your configuration.\"\n        )\n\n    # Handle glob patterns/lists\n    # Polars scan methods often support glob strings directly.\n\n    try:\n        if format == \"csv\":\n            # scan_csv supports glob patterns\n            return pl.scan_csv(full_path, **options)\n\n        elif format == \"parquet\":\n            return pl.scan_parquet(full_path, **options)\n\n        elif format == \"json\":\n            # scan_ndjson for newline delimited json, read_json for standard\n            # Assuming ndjson/jsonl for big data usually\n            if options.get(\"json_lines\", True):  # Default to ndjson scan\n                return pl.scan_ndjson(full_path, **options)\n            else:\n                # Standard JSON doesn't support lazy scan well in all versions, fallback to read\n                return pl.read_json(full_path, **options).lazy()\n\n        elif format == \"delta\":\n            # scan_delta requires 'deltalake' extra usually or feature\n            storage_options = options.get(\"storage_options\", None)\n            version = options.get(\"versionAsOf\", None)\n\n            # scan_delta is available in recent polars\n            # It might accept storage_options in recent versions\n            delta_opts = {}\n            if storage_options:\n                delta_opts[\"storage_options\"] = storage_options\n            if version is not None:\n                delta_opts[\"version\"] = version\n\n            return pl.scan_delta(full_path, **delta_opts)\n\n        elif format == \"excel\":\n            # Excel format: delegate to Pandas (best Excel support), convert to Polars\n            from odibi.engine.pandas_engine import PandasEngine\n            from odibi.context import get_logging_context\n\n            ctx = get_logging_context().with_context(engine=\"polars\")\n            ctx.debug(\"Reading Excel via Pandas engine (best Excel support)\")\n\n            # Get storage_options from connection for Azure/cloud authentication\n            excel_storage_options = None\n            if hasattr(connection, \"pandas_storage_options\"):\n                excel_storage_options = connection.pandas_storage_options()\n\n            # Use Pandas engine for Excel reading\n            pandas_engine = PandasEngine()\n            pdf = pandas_engine._read_excel_with_patterns(\n                full_path,\n                sheet_pattern=options.pop(\"sheet_pattern\", None),\n                sheet_pattern_case_sensitive=options.pop(\"sheet_pattern_case_sensitive\", False),\n                add_source_file=options.pop(\"add_source_file\", False),\n                is_glob=\"*\" in str(full_path) or \"?\" in str(full_path),\n                ctx=ctx,\n                storage_options=excel_storage_options,\n                **options,\n            )\n\n            # Convert Pandas DataFrame to Polars LazyFrame\n            ctx.info(f\"Excel read completed (via Pandas): {path}\", row_count=len(pdf))\n            return pl.from_pandas(pdf).lazy()\n\n        else:\n            raise ValueError(\n                f\"Unsupported format for Polars engine: '{format}'. \"\n                \"Supported formats: csv, parquet, json, delta, excel, sql, sql_server, azure_sql.\"\n            )\n\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to read {format} from '{full_path}': {e}. \"\n            \"Check that the file exists, the format is correct, and you have read permissions.\"\n        )\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.table_exists","title":"<code>table_exists(connection, table=None, path=None)</code>","text":"<p>Check if table or location exists.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def table_exists(\n    self, connection: Any, table: Optional[str] = None, path: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Check if table or location exists.\"\"\"\n    if path:\n        full_path = connection.get_path(path)\n        return os.path.exists(full_path)\n    return False\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.vacuum_delta","title":"<code>vacuum_delta(connection, path, retention_hours=168, dry_run=False, enforce_retention_duration=True)</code>","text":"<p>VACUUM a Delta table to remove old files.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Any</code> <p>Connection object</p> required <code>path</code> <code>str</code> <p>Delta table path</p> required <code>retention_hours</code> <code>int</code> <p>Retention period (default 168 = 7 days)</p> <code>168</code> <code>dry_run</code> <code>bool</code> <p>If True, only show files to be deleted</p> <code>False</code> <code>enforce_retention_duration</code> <code>bool</code> <p>If False, allows retention &lt; 168 hours (testing only)</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with files_deleted count</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def vacuum_delta(\n    self,\n    connection: Any,\n    path: str,\n    retention_hours: int = 168,\n    dry_run: bool = False,\n    enforce_retention_duration: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"VACUUM a Delta table to remove old files.\n\n    Args:\n        connection: Connection object\n        path: Delta table path\n        retention_hours: Retention period (default 168 = 7 days)\n        dry_run: If True, only show files to be deleted\n        enforce_retention_duration: If False, allows retention &lt; 168 hours (testing only)\n\n    Returns:\n        Dictionary with files_deleted count\n    \"\"\"\n    from odibi.utils.logging_context import get_logging_context\n    import time\n\n    ctx = get_logging_context().with_context(engine=\"polars\")\n    start = time.time()\n\n    ctx.debug(\n        \"Starting Delta VACUUM\",\n        path=path,\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n    )\n\n    try:\n        from deltalake import DeltaTable\n    except ImportError:\n        ctx.error(\"Delta Lake library not installed\", path=path)\n        raise ImportError(\n            \"Delta Lake support requires 'pip install odibi[polars]' \"\n            \"or 'pip install deltalake'. See README.md for installation instructions.\"\n        )\n\n    full_path = connection.get_path(path) if connection else path\n\n    storage_opts = {}\n    if hasattr(connection, \"pandas_storage_options\"):\n        storage_opts = connection.pandas_storage_options()\n\n    dt = DeltaTable(full_path, storage_options=storage_opts)\n    deleted_files = dt.vacuum(\n        retention_hours=retention_hours,\n        dry_run=dry_run,\n        enforce_retention_duration=enforce_retention_duration,\n    )\n\n    elapsed = (time.time() - start) * 1000\n    ctx.info(\n        \"Delta VACUUM completed\",\n        path=str(full_path),\n        files_deleted=len(deleted_files),\n        dry_run=dry_run,\n        elapsed_ms=round(elapsed, 2),\n    )\n\n    return {\"files_deleted\": len(deleted_files)}\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.validate_data","title":"<code>validate_data(df, validation_config)</code>","text":"<p>Validate data against rules.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame or LazyFrame</p> required <code>validation_config</code> <code>Any</code> <p>ValidationConfig object</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of validation failure messages</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def validate_data(self, df: Any, validation_config: Any) -&gt; List[str]:\n    \"\"\"Validate data against rules.\n\n    Args:\n        df: DataFrame or LazyFrame\n        validation_config: ValidationConfig object\n\n    Returns:\n        List of validation failure messages\n    \"\"\"\n    failures = []\n\n    if isinstance(df, pl.LazyFrame):\n        schema = df.collect_schema()\n        columns = schema.names()\n    else:\n        columns = df.columns\n\n    if getattr(validation_config, \"not_empty\", False):\n        count = self.count_rows(df)\n        if count == 0:\n            failures.append(\"DataFrame is empty\")\n\n    if getattr(validation_config, \"no_nulls\", None):\n        cols = validation_config.no_nulls\n        null_counts = self.count_nulls(df, cols)\n        for col, count in null_counts.items():\n            if count &gt; 0:\n                failures.append(f\"Column '{col}' has {count} null values\")\n\n    if getattr(validation_config, \"schema_validation\", None):\n        schema_failures = self.validate_schema(df, validation_config.schema_validation)\n        failures.extend(schema_failures)\n\n    if getattr(validation_config, \"ranges\", None):\n        for col, bounds in validation_config.ranges.items():\n            if col in columns:\n                min_val = bounds.get(\"min\")\n                max_val = bounds.get(\"max\")\n\n                if min_val is not None:\n                    if isinstance(df, pl.LazyFrame):\n                        min_violations = (\n                            df.filter(pl.col(col) &lt; min_val).select(pl.len()).collect().item()\n                        )\n                    else:\n                        min_violations = len(df.filter(pl.col(col) &lt; min_val))\n                    if min_violations &gt; 0:\n                        failures.append(f\"Column '{col}' has values &lt; {min_val}\")\n\n                if max_val is not None:\n                    if isinstance(df, pl.LazyFrame):\n                        max_violations = (\n                            df.filter(pl.col(col) &gt; max_val).select(pl.len()).collect().item()\n                        )\n                    else:\n                        max_violations = len(df.filter(pl.col(col) &gt; max_val))\n                    if max_violations &gt; 0:\n                        failures.append(f\"Column '{col}' has values &gt; {max_val}\")\n            else:\n                failures.append(f\"Column '{col}' not found for range validation\")\n\n    if getattr(validation_config, \"allowed_values\", None):\n        for col, allowed in validation_config.allowed_values.items():\n            if col in columns:\n                if isinstance(df, pl.LazyFrame):\n                    invalid_count = (\n                        df.filter(~pl.col(col).is_in(allowed)).select(pl.len()).collect().item()\n                    )\n                else:\n                    invalid_count = len(df.filter(~pl.col(col).is_in(allowed)))\n                if invalid_count &gt; 0:\n                    failures.append(f\"Column '{col}' has invalid values\")\n            else:\n                failures.append(f\"Column '{col}' not found for allowed values validation\")\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.validate_schema","title":"<code>validate_schema(df, schema_rules)</code>","text":"<p>Validate DataFrame schema.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def validate_schema(self, df: Any, schema_rules: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Validate DataFrame schema.\"\"\"\n    failures = []\n\n    # Schema is dict-like in Polars\n    current_schema = df.collect_schema() if isinstance(df, pl.LazyFrame) else df.schema\n    current_cols = current_schema.keys()\n\n    if \"required_columns\" in schema_rules:\n        required = schema_rules[\"required_columns\"]\n        missing = set(required) - set(current_cols)\n        if missing:\n            failures.append(f\"Missing required columns: {', '.join(missing)}\")\n\n    if \"types\" in schema_rules:\n        for col, expected_type in schema_rules[\"types\"].items():\n            if col not in current_cols:\n                failures.append(f\"Column '{col}' not found for type validation\")\n                continue\n\n            actual_type = str(current_schema[col])\n            # Basic type check - simplistic string matching\n            if expected_type.lower() not in actual_type.lower():\n                failures.append(\n                    f\"Column '{col}' has type '{actual_type}', expected '{expected_type}'\"\n                )\n\n    return failures\n</code></pre>"},{"location":"reference/api/engine/#odibi.engine.polars_engine.PolarsEngine.write","title":"<code>write(df, connection, format, table=None, path=None, mode='overwrite', options=None, streaming_config=None)</code>","text":"<p>Write data using Polars.</p> Source code in <code>odibi\\engine\\polars_engine.py</code> <pre><code>def write(\n    self,\n    df: Any,\n    connection: Any,\n    format: str,\n    table: Optional[str] = None,\n    path: Optional[str] = None,\n    mode: str = \"overwrite\",\n    options: Optional[Dict[str, Any]] = None,\n    streaming_config: Optional[Any] = None,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Write data using Polars.\"\"\"\n    options = options or {}\n\n    if format in [\"sql\", \"sql_server\", \"azure_sql\"]:\n        return self._write_sql(df, connection, table, mode, options)\n\n    if path:\n        if connection:\n            full_path = connection.get_path(path)\n        else:\n            full_path = path\n    elif table:\n        if connection:\n            full_path = connection.get_path(table)\n        else:\n            raise ValueError(\n                f\"Cannot write to table '{table}': connection is required when using 'table' parameter. \"\n                \"Provide a valid connection object or use 'path' for file-based writes.\"\n            )\n    else:\n        raise ValueError(\n            \"Write operation failed: neither 'path' nor 'table' was provided. \"\n            \"Specify a file path or table name in your configuration.\"\n        )\n\n    is_lazy = isinstance(df, pl.LazyFrame)\n\n    parent_dir = os.path.dirname(full_path)\n    if parent_dir:\n        os.makedirs(parent_dir, exist_ok=True)\n\n    if format == \"parquet\":\n        if is_lazy:\n            df.sink_parquet(full_path, **options)\n        else:\n            df.write_parquet(full_path, **options)\n\n    elif format == \"csv\":\n        if is_lazy:\n            df.sink_csv(full_path, **options)\n        else:\n            df.write_csv(full_path, **options)\n\n    elif format == \"json\":\n        if is_lazy:\n            df.sink_ndjson(full_path, **options)\n        else:\n            df.write_ndjson(full_path, **options)\n\n    elif format == \"delta\":\n        if is_lazy:\n            df = df.collect()\n\n        storage_options = options.get(\"storage_options\", None)\n        delta_write_options = options.copy()\n        if \"storage_options\" in delta_write_options:\n            del delta_write_options[\"storage_options\"]\n\n        df.write_delta(\n            full_path, mode=mode, storage_options=storage_options, **delta_write_options\n        )\n\n    else:\n        raise ValueError(\n            f\"Unsupported write format for Polars engine: '{format}'. \"\n            \"Supported formats: csv, parquet, json, delta.\"\n        )\n\n    return None\n</code></pre>"},{"location":"reference/api/patterns/","title":"Patterns API","text":""},{"location":"reference/api/patterns/#odibi.patterns.base","title":"<code>odibi.patterns.base</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.scd2","title":"<code>odibi.patterns.scd2</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.scd2.SCD2Pattern","title":"<code>SCD2Pattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>SCD2 Pattern: Slowly Changing Dimension Type 2.</p> <p>Tracks history by creating new rows for updates.</p> <p>Configuration Options (via params dict):     - keys (list): Business keys.     - time_col (str): Timestamp column for versioning (default: current time).     - valid_from_col (str): Name of start date column (default: valid_from).     - valid_to_col (str): Name of end date column (default: valid_to).     - is_current_col (str): Name of current flag column (default: is_current).</p> Source code in <code>odibi\\patterns\\scd2.py</code> <pre><code>class SCD2Pattern(Pattern):\n    \"\"\"\n    SCD2 Pattern: Slowly Changing Dimension Type 2.\n\n    Tracks history by creating new rows for updates.\n\n    Configuration Options (via params dict):\n        - **keys** (list): Business keys.\n        - **time_col** (str): Timestamp column for versioning (default: current time).\n        - **valid_from_col** (str): Name of start date column (default: valid_from).\n        - **valid_to_col** (str): Name of end date column (default: valid_to).\n        - **is_current_col** (str): Name of current flag column (default: is_current).\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"SCD2Pattern validation starting\",\n            pattern=\"SCD2Pattern\",\n            keys=self.params.get(\"keys\"),\n            target=self.params.get(\"target\"),\n        )\n\n        if not self.params.get(\"keys\"):\n            ctx.error(\n                \"SCD2Pattern validation failed: 'keys' parameter is required\",\n                pattern=\"SCD2Pattern\",\n            )\n            raise ValueError(\n                \"SCD2Pattern: 'keys' parameter is required. \"\n                f\"Expected a list of business key column names, but got: {self.params.get('keys')!r}. \"\n                f\"Available params: {list(self.params.keys())}. \"\n                \"Fix: Provide 'keys' as a list, e.g., keys=['customer_id'].\"\n            )\n        if not self.params.get(\"target\"):\n            ctx.error(\n                \"SCD2Pattern validation failed: 'target' parameter is required\",\n                pattern=\"SCD2Pattern\",\n            )\n            raise ValueError(\n                \"SCD2Pattern: 'target' parameter is required. \"\n                f\"Expected a table name or path string, but got: {self.params.get('target')!r}. \"\n                \"Fix: Provide 'target' as a string, e.g., target='dim_customer'.\"\n            )\n\n        ctx.debug(\n            \"SCD2Pattern validation passed\",\n            pattern=\"SCD2Pattern\",\n            keys=self.params.get(\"keys\"),\n            target=self.params.get(\"target\"),\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        keys = self.params.get(\"keys\")\n        target = self.params.get(\"target\")\n        valid_from_col = self.params.get(\"valid_from_col\", \"valid_from\")\n        valid_to_col = self.params.get(\"valid_to_col\", \"valid_to\")\n        is_current_col = self.params.get(\"is_current_col\", \"is_current\")\n        track_cols = self.params.get(\"track_cols\")\n\n        ctx.debug(\n            \"SCD2 pattern starting\",\n            pattern=\"SCD2Pattern\",\n            keys=keys,\n            target=target,\n            valid_from_col=valid_from_col,\n            valid_to_col=valid_to_col,\n            is_current_col=is_current_col,\n            track_cols=track_cols,\n        )\n\n        source_count = None\n        try:\n            if context.engine_type == \"spark\":\n                source_count = context.df.count()\n            else:\n                source_count = len(context.df)\n            ctx.debug(\"SCD2 source data loaded\", pattern=\"SCD2Pattern\", source_rows=source_count)\n        except Exception:\n            ctx.debug(\"SCD2 could not determine source row count\", pattern=\"SCD2Pattern\")\n\n        valid_keys = SCD2Params.model_fields.keys()\n        filtered_params = {k: v for k, v in self.params.items() if k in valid_keys}\n\n        try:\n            scd_params = SCD2Params(**filtered_params)\n        except Exception as e:\n            ctx.error(\n                f\"SCD2 invalid parameters: {e}\",\n                pattern=\"SCD2Pattern\",\n                error_type=type(e).__name__,\n                params=filtered_params,\n            )\n            raise ValueError(\n                f\"Invalid SCD2 parameters: {e}. \"\n                f\"Provided params: {filtered_params}. \"\n                f\"Valid param names: {list(valid_keys)}.\"\n            )\n\n        try:\n            result_ctx = scd2(context, scd_params)\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"SCD2 pattern execution failed: {e}\",\n                pattern=\"SCD2Pattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n        result_df = result_ctx.df\n        elapsed_ms = (time.time() - start_time) * 1000\n\n        result_count = None\n        try:\n            if context.engine_type == \"spark\":\n                result_count = result_df.count()\n            else:\n                result_count = len(result_df)\n        except Exception:\n            pass\n\n        ctx.info(\n            \"SCD2 pattern completed\",\n            pattern=\"SCD2Pattern\",\n            elapsed_ms=round(elapsed_ms, 2),\n            source_rows=source_count,\n            result_rows=result_count,\n            keys=keys,\n            target=target,\n            valid_from_col=valid_from_col,\n            valid_to_col=valid_to_col,\n        )\n\n        return result_df\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.merge","title":"<code>odibi.patterns.merge</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.merge.MergePattern","title":"<code>MergePattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Merge Pattern: Upsert/Merge logic.</p> <p>Configuration Options (via params dict):     - target (str): Target table/path.     - keys (list): Join keys.     - strategy (str): 'upsert', 'append_only', 'delete_match'.</p> Source code in <code>odibi\\patterns\\merge.py</code> <pre><code>class MergePattern(Pattern):\n    \"\"\"\n    Merge Pattern: Upsert/Merge logic.\n\n    Configuration Options (via params dict):\n        - **target** (str): Target table/path.\n        - **keys** (list): Join keys.\n        - **strategy** (str): 'upsert', 'append_only', 'delete_match'.\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n\n        # Support both 'target' and 'path' for compatibility with merge transformer\n        target = self.params.get(\"target\") or self.params.get(\"path\")\n\n        ctx.debug(\n            \"MergePattern validation starting\",\n            pattern=\"MergePattern\",\n            target=target,\n            keys=self.params.get(\"keys\"),\n            strategy=self.params.get(\"strategy\"),\n        )\n\n        if not target:\n            ctx.error(\n                \"MergePattern validation failed: 'target' or 'path' is required\",\n                pattern=\"MergePattern\",\n            )\n            provided_params = {k: v for k, v in self.params.items() if v is not None}\n            raise ValueError(\n                f\"MergePattern: 'target' or 'path' is required. \"\n                f\"Expected: A target table path string. \"\n                f\"Provided params: {list(provided_params.keys())}. \"\n                f\"Fix: Add 'target' or 'path' to your pattern configuration.\"\n            )\n        if not self.params.get(\"keys\"):\n            ctx.error(\n                \"MergePattern validation failed: 'keys' is required\",\n                pattern=\"MergePattern\",\n            )\n            source_columns = list(self.source.columns) if hasattr(self.source, \"columns\") else []\n            raise ValueError(\n                f\"MergePattern: 'keys' is required. \"\n                f\"Expected: A list of column names to match source and target rows for merge. \"\n                f\"Available source columns: {source_columns}. \"\n                f\"Fix: Add 'keys' with columns that uniquely identify rows (e.g., keys=['id']).\"\n            )\n\n        ctx.debug(\n            \"MergePattern validation passed\",\n            pattern=\"MergePattern\",\n            target=self.params.get(\"target\"),\n            keys=self.params.get(\"keys\"),\n            strategy=self.params.get(\"strategy\", \"upsert\"),\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        # Support both 'target' and 'path' for compatibility\n        target = self.params.get(\"target\") or self.params.get(\"path\")\n        keys = self.params.get(\"keys\")\n        strategy = self.params.get(\"strategy\", \"upsert\")\n\n        ctx.debug(\n            \"Merge pattern starting\",\n            pattern=\"MergePattern\",\n            target=target,\n            keys=keys,\n            strategy=strategy,\n        )\n\n        source_count = None\n        try:\n            if context.engine_type == \"spark\":\n                source_count = context.df.count()\n            else:\n                source_count = len(context.df)\n            ctx.debug(\n                \"Merge source data loaded\",\n                pattern=\"MergePattern\",\n                source_rows=source_count,\n            )\n        except Exception:\n            ctx.debug(\"Merge could not determine source row count\", pattern=\"MergePattern\")\n\n        valid_keys = MergeParams.model_fields.keys()\n        filtered_params = {k: v for k, v in self.params.items() if k in valid_keys}\n\n        try:\n            merge(context, context.df, **filtered_params)\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"Merge pattern execution failed: {e}\",\n                pattern=\"MergePattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n                target=target,\n                keys=keys,\n                strategy=strategy,\n            )\n            raise\n\n        elapsed_ms = (time.time() - start_time) * 1000\n\n        ctx.info(\n            \"Merge pattern completed\",\n            pattern=\"MergePattern\",\n            elapsed_ms=round(elapsed_ms, 2),\n            source_rows=source_count,\n            target=target,\n            keys=keys,\n            strategy=strategy,\n        )\n\n        return context.df\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.dimension","title":"<code>odibi.patterns.dimension</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.dimension.AuditConfig","title":"<code>AuditConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for audit columns.</p> Source code in <code>odibi\\patterns\\dimension.py</code> <pre><code>class AuditConfig(BaseModel):\n    \"\"\"Configuration for audit columns.\"\"\"\n\n    load_timestamp: bool = Field(default=True, description=\"Add load_timestamp column\")\n    source_system: Optional[str] = Field(\n        default=None, description=\"Source system name for source_system column\"\n    )\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.dimension.DimensionPattern","title":"<code>DimensionPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Dimension Pattern: Builds complete dimension tables with surrogate keys and SCD support.</p> <p>Features: - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows) - SCD Type 0 (static), 1 (overwrite), 2 (history tracking) - Optional unknown member row (SK=0) for orphan FK handling - Audit columns (load_timestamp, source_system)</p> <p>Configuration Options (via params dict):     - natural_key (str): Natural/business key column name     - surrogate_key (str): Surrogate key column name to generate     - scd_type (int): 0=static, 1=overwrite, 2=history tracking (default: 1)     - track_cols (list): Columns to track for SCD1/2 changes     - target (str): Target table path (required for SCD2 to read existing history)     - unknown_member (bool): If true, insert a row with SK=0 for orphan FK handling     - audit (dict): Audit configuration with load_timestamp and source_system</p> Supported target formats <p>Spark:     - Catalog tables: catalog.schema.table, warehouse.dim_customer     - Delta paths: /path/to/delta (no extension)     - Parquet: /path/to/file.parquet     - CSV: /path/to/file.csv     - JSON: /path/to/file.json     - ORC: /path/to/file.orc Pandas:     - Parquet: path/to/file.parquet (or directory)     - CSV: path/to/file.csv     - JSON: path/to/file.json     - Excel: path/to/file.xlsx, path/to/file.xls     - Feather/Arrow: path/to/file.feather, path/to/file.arrow     - Pickle: path/to/file.pickle, path/to/file.pkl     - Connection-prefixed: warehouse.dim_customer</p> Source code in <code>odibi\\patterns\\dimension.py</code> <pre><code>class DimensionPattern(Pattern):\n    \"\"\"\n    Dimension Pattern: Builds complete dimension tables with surrogate keys and SCD support.\n\n    Features:\n    - Auto-generate integer surrogate keys (MAX(existing) + ROW_NUMBER for new rows)\n    - SCD Type 0 (static), 1 (overwrite), 2 (history tracking)\n    - Optional unknown member row (SK=0) for orphan FK handling\n    - Audit columns (load_timestamp, source_system)\n\n    Configuration Options (via params dict):\n        - **natural_key** (str): Natural/business key column name\n        - **surrogate_key** (str): Surrogate key column name to generate\n        - **scd_type** (int): 0=static, 1=overwrite, 2=history tracking (default: 1)\n        - **track_cols** (list): Columns to track for SCD1/2 changes\n        - **target** (str): Target table path (required for SCD2 to read existing history)\n        - **unknown_member** (bool): If true, insert a row with SK=0 for orphan FK handling\n        - **audit** (dict): Audit configuration with load_timestamp and source_system\n\n    Supported target formats:\n        Spark:\n            - Catalog tables: catalog.schema.table, warehouse.dim_customer\n            - Delta paths: /path/to/delta (no extension)\n            - Parquet: /path/to/file.parquet\n            - CSV: /path/to/file.csv\n            - JSON: /path/to/file.json\n            - ORC: /path/to/file.orc\n        Pandas:\n            - Parquet: path/to/file.parquet (or directory)\n            - CSV: path/to/file.csv\n            - JSON: path/to/file.json\n            - Excel: path/to/file.xlsx, path/to/file.xls\n            - Feather/Arrow: path/to/file.feather, path/to/file.arrow\n            - Pickle: path/to/file.pickle, path/to/file.pkl\n            - Connection-prefixed: warehouse.dim_customer\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"DimensionPattern validation starting\",\n            pattern=\"DimensionPattern\",\n            params=self.params,\n        )\n\n        if not self.params.get(\"natural_key\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'natural_key' is required\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\n                \"DimensionPattern: 'natural_key' parameter is required. \"\n                \"The natural_key identifies the business key column(s) that uniquely identify \"\n                \"each dimension record in the source system. \"\n                \"Provide natural_key as a string (single column) or list of strings (composite key).\"\n            )\n\n        if not self.params.get(\"surrogate_key\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'surrogate_key' is required\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\n                \"DimensionPattern: 'surrogate_key' parameter is required. \"\n                \"The surrogate_key is the auto-generated primary key column for the dimension table, \"\n                \"used to join with fact tables instead of the natural key. \"\n                \"Provide surrogate_key as a string specifying the column name (e.g., 'customer_sk').\"\n            )\n\n        scd_type = self.params.get(\"scd_type\", 1)\n        if scd_type not in (0, 1, 2):\n            ctx.error(\n                f\"DimensionPattern validation failed: invalid scd_type {scd_type}\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\n                f\"DimensionPattern: 'scd_type' must be 0, 1, or 2. Got: {scd_type}. \"\n                \"SCD Type 0: No changes tracked (static dimension). \"\n                \"SCD Type 1: Overwrite changes (no history). \"\n                \"SCD Type 2: Track full history with valid_from/valid_to dates.\"\n            )\n\n        if scd_type == 2 and not self.params.get(\"target\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'target' required for SCD2\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\n                \"DimensionPattern: 'target' parameter is required for scd_type=2. \"\n                \"SCD Type 2 compares incoming data against existing records to detect changes, \"\n                \"so a target DataFrame containing current dimension data must be provided. \"\n                \"Pass the existing dimension table as the 'target' parameter.\"\n            )\n\n        if scd_type in (1, 2) and not self.params.get(\"track_cols\"):\n            ctx.error(\n                \"DimensionPattern validation failed: 'track_cols' required for SCD1/2\",\n                pattern=\"DimensionPattern\",\n            )\n            raise ValueError(\n                \"DimensionPattern: 'track_cols' parameter is required for scd_type 1 or 2. \"\n                \"The track_cols specifies which columns to monitor for changes. \"\n                \"When these columns change, SCD1 overwrites values or SCD2 creates new history records. \"\n                \"Provide track_cols as a list of column names (e.g., ['address', 'phone', 'email']).\"\n            )\n\n        ctx.debug(\n            \"DimensionPattern validation passed\",\n            pattern=\"DimensionPattern\",\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        natural_key = self.params.get(\"natural_key\")\n        surrogate_key = self.params.get(\"surrogate_key\")\n        scd_type = self.params.get(\"scd_type\", 1)\n        track_cols = self.params.get(\"track_cols\", [])\n        target = self.params.get(\"target\")\n        unknown_member = self.params.get(\"unknown_member\", False)\n        audit_config = self.params.get(\"audit\", {})\n\n        ctx.debug(\n            \"DimensionPattern starting\",\n            pattern=\"DimensionPattern\",\n            natural_key=natural_key,\n            surrogate_key=surrogate_key,\n            scd_type=scd_type,\n            track_cols=track_cols,\n            target=target,\n            unknown_member=unknown_member,\n        )\n\n        source_count = self._get_row_count(context.df, context.engine_type)\n        ctx.debug(\"Dimension source loaded\", pattern=\"DimensionPattern\", source_rows=source_count)\n\n        try:\n            if scd_type == 0:\n                result_df = self._execute_scd0(context, natural_key, surrogate_key, target)\n            elif scd_type == 1:\n                result_df = self._execute_scd1(\n                    context, natural_key, surrogate_key, track_cols, target\n                )\n            else:\n                result_df = self._execute_scd2(\n                    context, natural_key, surrogate_key, track_cols, target\n                )\n\n            result_df = self._add_audit_columns(context, result_df, audit_config)\n\n            if unknown_member:\n                result_df = self._ensure_unknown_member(\n                    context, result_df, natural_key, surrogate_key, audit_config\n                )\n\n            result_count = self._get_row_count(result_df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"DimensionPattern completed\",\n                pattern=\"DimensionPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                source_rows=source_count,\n                result_rows=result_count,\n                scd_type=scd_type,\n            )\n\n            return result_df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"DimensionPattern failed: {e}\",\n                pattern=\"DimensionPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _load_existing_target(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table if it exists.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._load_existing_spark(context, target)\n        else:\n            return self._load_existing_pandas(context, target)\n\n    def _load_existing_spark(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table from Spark with multi-format support.\"\"\"\n        ctx = get_logging_context()\n        spark = context.spark\n\n        # Try catalog table first\n        try:\n            return spark.table(target)\n        except Exception:\n            pass\n\n        # Check file extension for format detection\n        target_lower = target.lower()\n\n        try:\n            if target_lower.endswith(\".parquet\"):\n                return spark.read.parquet(target)\n            elif target_lower.endswith(\".csv\"):\n                return spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(target)\n            elif target_lower.endswith(\".json\"):\n                return spark.read.json(target)\n            elif target_lower.endswith(\".orc\"):\n                return spark.read.orc(target)\n            else:\n                # Try Delta format as fallback (for paths without extension)\n                return spark.read.format(\"delta\").load(target)\n        except Exception as e:\n            ctx.warning(\n                f\"Could not load existing target '{target}': {e}. Treating as initial load.\",\n                pattern=\"DimensionPattern\",\n                target=target,\n            )\n            return None\n\n    def _load_existing_pandas(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table from Pandas with multi-format support.\"\"\"\n        import os\n\n        import pandas as pd\n\n        ctx = get_logging_context()\n        path = target\n\n        # Handle connection-prefixed paths\n        if hasattr(context, \"engine\") and context.engine:\n            if \".\" in path:\n                parts = path.split(\".\", 1)\n                conn_name = parts[0]\n                rel_path = parts[1]\n                if conn_name in context.engine.connections:\n                    try:\n                        path = context.engine.connections[conn_name].get_path(rel_path)\n                    except Exception:\n                        pass\n\n        if not os.path.exists(path):\n            return None\n\n        path_lower = str(path).lower()\n\n        try:\n            # Parquet (file or directory)\n            if path_lower.endswith(\".parquet\") or os.path.isdir(path):\n                return pd.read_parquet(path)\n            # CSV\n            elif path_lower.endswith(\".csv\"):\n                return pd.read_csv(path)\n            # JSON\n            elif path_lower.endswith(\".json\"):\n                return pd.read_json(path)\n            # Excel\n            elif path_lower.endswith(\".xlsx\") or path_lower.endswith(\".xls\"):\n                return pd.read_excel(path)\n            # Feather / Arrow IPC\n            elif path_lower.endswith(\".feather\") or path_lower.endswith(\".arrow\"):\n                return pd.read_feather(path)\n            # Pickle\n            elif path_lower.endswith(\".pickle\") or path_lower.endswith(\".pkl\"):\n                return pd.read_pickle(path)\n            else:\n                ctx.warning(\n                    f\"Unrecognized file format for target '{target}'. \"\n                    \"Supported formats: parquet, csv, json, xlsx, xls, feather, arrow, pickle. \"\n                    \"Treating as initial load.\",\n                    pattern=\"DimensionPattern\",\n                    target=target,\n                )\n                return None\n        except Exception as e:\n            ctx.warning(\n                f\"Could not load existing target '{target}': {e}. Treating as initial load.\",\n                pattern=\"DimensionPattern\",\n                target=target,\n            )\n            return None\n\n    def _get_max_sk(self, df, surrogate_key: str, engine_type) -&gt; int:\n        \"\"\"Get the maximum surrogate key value from existing data.\"\"\"\n        if df is None:\n            return 0\n        try:\n            if engine_type == EngineType.SPARK:\n                from pyspark.sql import functions as F\n\n                max_row = df.agg(F.max(surrogate_key)).collect()[0]\n                max_val = max_row[0]\n                return max_val if max_val is not None else 0\n            else:\n                if surrogate_key not in df.columns:\n                    return 0\n                max_val = df[surrogate_key].max()\n                return int(max_val) if max_val is not None and not (max_val != max_val) else 0\n        except Exception:\n            return 0\n\n    def _generate_surrogate_keys(\n        self,\n        context: EngineContext,\n        df,\n        natural_key: str,\n        surrogate_key: str,\n        start_sk: int,\n    ):\n        \"\"\"Generate surrogate keys starting from start_sk + 1.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n            from pyspark.sql.window import Window\n\n            window = Window.orderBy(natural_key)\n            df = df.withColumn(\n                surrogate_key, (F.row_number().over(window) + F.lit(start_sk)).cast(\"int\")\n            )\n            return df\n        else:\n            df = df.copy()\n            df = df.sort_values(by=natural_key).reset_index(drop=True)\n            df[surrogate_key] = range(start_sk + 1, start_sk + 1 + len(df))\n            df[surrogate_key] = df[surrogate_key].astype(\"int64\")\n            return df\n\n    def _execute_scd0(\n        self,\n        context: EngineContext,\n        natural_key: str,\n        surrogate_key: str,\n        target: Optional[str],\n    ):\n        \"\"\"\n        SCD Type 0: Static dimension - never update existing records.\n        Only insert new records that don't exist in target.\n        \"\"\"\n        existing_df = self._load_existing_target(context, target) if target else None\n        source_df = context.df\n\n        if existing_df is None:\n            return self._generate_surrogate_keys(\n                context, source_df, natural_key, surrogate_key, start_sk=0\n            )\n\n        max_sk = self._get_max_sk(existing_df, surrogate_key, context.engine_type)\n\n        if context.engine_type == EngineType.SPARK:\n            existing_keys = existing_df.select(natural_key).distinct()\n            new_records = source_df.join(existing_keys, on=natural_key, how=\"left_anti\")\n        else:\n            existing_keys = set(existing_df[natural_key].unique())\n            new_records = source_df[~source_df[natural_key].isin(existing_keys)].copy()\n\n        if self._get_row_count(new_records, context.engine_type) == 0:\n            return existing_df\n\n        new_with_sk = self._generate_surrogate_keys(\n            context, new_records, natural_key, surrogate_key, start_sk=max_sk\n        )\n\n        if context.engine_type == EngineType.SPARK:\n            return existing_df.unionByName(new_with_sk, allowMissingColumns=True)\n        else:\n            import pandas as pd\n\n            return pd.concat([existing_df, new_with_sk], ignore_index=True)\n\n    def _execute_scd1(\n        self,\n        context: EngineContext,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        target: Optional[str],\n    ):\n        \"\"\"\n        SCD Type 1: Overwrite changes - no history tracking.\n        Update existing records in place, insert new records.\n        \"\"\"\n        existing_df = self._load_existing_target(context, target) if target else None\n        source_df = context.df\n\n        if existing_df is None:\n            return self._generate_surrogate_keys(\n                context, source_df, natural_key, surrogate_key, start_sk=0\n            )\n\n        max_sk = self._get_max_sk(existing_df, surrogate_key, context.engine_type)\n\n        if context.engine_type == EngineType.SPARK:\n            return self._execute_scd1_spark(\n                context, source_df, existing_df, natural_key, surrogate_key, track_cols, max_sk\n            )\n        else:\n            return self._execute_scd1_pandas(\n                context, source_df, existing_df, natural_key, surrogate_key, track_cols, max_sk\n            )\n\n    def _execute_scd1_spark(\n        self,\n        context: EngineContext,\n        source_df,\n        existing_df,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        max_sk: int,\n    ):\n        from pyspark.sql import functions as F\n\n        t_prefix = \"__existing_\"\n        renamed_existing = existing_df\n        for c in existing_df.columns:\n            renamed_existing = renamed_existing.withColumnRenamed(c, f\"{t_prefix}{c}\")\n\n        joined = source_df.join(\n            renamed_existing,\n            source_df[natural_key] == renamed_existing[f\"{t_prefix}{natural_key}\"],\n            \"left\",\n        )\n\n        new_records = joined.filter(F.col(f\"{t_prefix}{natural_key}\").isNull()).select(\n            source_df.columns\n        )\n\n        update_records = joined.filter(F.col(f\"{t_prefix}{natural_key}\").isNotNull())\n        update_cols = [F.col(f\"{t_prefix}{surrogate_key}\").alias(surrogate_key)] + [\n            F.col(c) for c in source_df.columns\n        ]\n        updated_records = update_records.select(update_cols)\n\n        unchanged_keys = update_records.select(F.col(f\"{t_prefix}{natural_key}\").alias(natural_key))\n        unchanged = existing_df.join(unchanged_keys, on=natural_key, how=\"left_anti\")\n\n        new_with_sk = self._generate_surrogate_keys(\n            context, new_records, natural_key, surrogate_key, start_sk=max_sk\n        )\n\n        result = unchanged.unionByName(updated_records, allowMissingColumns=True).unionByName(\n            new_with_sk, allowMissingColumns=True\n        )\n        return result\n\n    def _execute_scd1_pandas(\n        self,\n        context: EngineContext,\n        source_df,\n        existing_df,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        max_sk: int,\n    ):\n        import pandas as pd\n\n        merged = pd.merge(\n            source_df,\n            existing_df[[natural_key, surrogate_key]],\n            on=natural_key,\n            how=\"left\",\n            suffixes=(\"\", \"_existing\"),\n        )\n\n        has_existing_sk = f\"{surrogate_key}_existing\" in merged.columns\n        if has_existing_sk:\n            merged[surrogate_key] = merged[f\"{surrogate_key}_existing\"]\n            merged = merged.drop(columns=[f\"{surrogate_key}_existing\"])\n\n        new_mask = merged[surrogate_key].isna()\n        new_records = merged[new_mask].drop(columns=[surrogate_key])\n        existing_records = merged[~new_mask]\n\n        if len(new_records) &gt; 0:\n            new_with_sk = self._generate_surrogate_keys(\n                context, new_records, natural_key, surrogate_key, start_sk=max_sk\n            )\n        else:\n            new_with_sk = pd.DataFrame()\n\n        unchanged = existing_df[~existing_df[natural_key].isin(source_df[natural_key])]\n\n        result = pd.concat([unchanged, existing_records, new_with_sk], ignore_index=True)\n        return result\n\n    def _execute_scd2(\n        self,\n        context: EngineContext,\n        natural_key: str,\n        surrogate_key: str,\n        track_cols: List[str],\n        target: str,\n    ):\n        \"\"\"\n        SCD Type 2: History tracking - reuse existing scd2 transformer.\n        Surrogate keys are generated for new/changed records.\n        \"\"\"\n        existing_df = self._load_existing_target(context, target)\n\n        valid_from_col = self.params.get(\"valid_from_col\", \"valid_from\")\n        valid_to_col = self.params.get(\"valid_to_col\", \"valid_to\")\n        is_current_col = self.params.get(\"is_current_col\", \"is_current\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            source_with_time = context.df.withColumn(valid_from_col, F.current_timestamp())\n        else:\n            source_df = context.df.copy()\n            # Use timezone-aware timestamp for Delta Lake compatibility\n            from datetime import timezone\n\n            source_df[valid_from_col] = datetime.now(timezone.utc)\n            source_with_time = source_df\n\n        temp_context = context.with_df(source_with_time)\n\n        scd_params = SCD2Params(\n            target=target,\n            keys=[natural_key],\n            track_cols=track_cols,\n            effective_time_col=valid_from_col,\n            end_time_col=valid_to_col,\n            current_flag_col=is_current_col,\n        )\n\n        result_context = scd2(temp_context, scd_params)\n        result_df = result_context.df\n\n        max_sk = self._get_max_sk(existing_df, surrogate_key, context.engine_type)\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n            from pyspark.sql.window import Window\n\n            if surrogate_key not in result_df.columns:\n                window = Window.orderBy(natural_key, valid_from_col)\n                result_df = result_df.withColumn(\n                    surrogate_key, (F.row_number().over(window) + F.lit(max_sk)).cast(\"int\")\n                )\n            else:\n                null_sk_df = result_df.filter(F.col(surrogate_key).isNull())\n                has_sk_df = result_df.filter(F.col(surrogate_key).isNotNull())\n\n                if null_sk_df.count() &gt; 0:\n                    window = Window.orderBy(natural_key, valid_from_col)\n                    null_sk_df = null_sk_df.withColumn(\n                        surrogate_key, (F.row_number().over(window) + F.lit(max_sk)).cast(\"int\")\n                    )\n                    result_df = has_sk_df.unionByName(null_sk_df)\n        else:\n            import pandas as pd\n\n            if surrogate_key not in result_df.columns:\n                result_df = result_df.sort_values([natural_key, valid_from_col]).reset_index(\n                    drop=True\n                )\n                result_df[surrogate_key] = range(max_sk + 1, max_sk + 1 + len(result_df))\n            else:\n                null_mask = result_df[surrogate_key].isna()\n                if null_mask.any():\n                    null_df = result_df[null_mask].copy()\n                    null_df = null_df.sort_values([natural_key, valid_from_col]).reset_index(\n                        drop=True\n                    )\n                    null_df[surrogate_key] = range(max_sk + 1, max_sk + 1 + len(null_df))\n                    result_df = pd.concat([result_df[~null_mask], null_df], ignore_index=True)\n\n        return result_df\n\n    def _add_audit_columns(self, context: EngineContext, df, audit_config: dict):\n        \"\"\"Add audit columns (load_timestamp, source_system) to the dataframe.\"\"\"\n        load_timestamp = audit_config.get(\"load_timestamp\", True)\n        source_system = audit_config.get(\"source_system\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            if load_timestamp:\n                df = df.withColumn(\"load_timestamp\", F.current_timestamp())\n            if source_system:\n                df = df.withColumn(\"source_system\", F.lit(source_system))\n        else:\n            df = df.copy()\n            if load_timestamp:\n                # Use timezone-aware timestamp for Delta Lake compatibility\n                from datetime import timezone\n\n                df[\"load_timestamp\"] = datetime.now(timezone.utc)\n            if source_system:\n                df[\"source_system\"] = source_system\n\n        return df\n\n    def _ensure_unknown_member(\n        self,\n        context: EngineContext,\n        df,\n        natural_key: str,\n        surrogate_key: str,\n        audit_config: dict,\n    ):\n        \"\"\"Ensure unknown member row exists with SK=0.\"\"\"\n        valid_from_col = self.params.get(\"valid_from_col\", \"valid_from\")\n        valid_to_col = self.params.get(\"valid_to_col\", \"valid_to\")\n        is_current_col = self.params.get(\"is_current_col\", \"is_current\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            existing_unknown = df.filter(F.col(surrogate_key) == 0)\n            if existing_unknown.count() &gt; 0:\n                return df\n\n            columns = df.columns\n            unknown_values = []\n            for col in columns:\n                if col == surrogate_key:\n                    unknown_values.append(0)\n                elif col == natural_key:\n                    unknown_values.append(\"-1\")\n                elif col == valid_from_col:\n                    unknown_values.append(datetime(1900, 1, 1))\n                elif col == valid_to_col:\n                    unknown_values.append(None)\n                elif col == is_current_col:\n                    unknown_values.append(True)\n                elif col == \"load_timestamp\":\n                    # Use timezone-aware timestamp for Delta Lake compatibility\n                    from datetime import timezone\n\n                    unknown_values.append(datetime.now(timezone.utc))\n                elif col == \"source_system\":\n                    unknown_values.append(audit_config.get(\"source_system\", \"Unknown\"))\n                else:\n                    unknown_values.append(\"Unknown\")\n\n            unknown_row = context.spark.createDataFrame([unknown_values], columns)\n            return unknown_row.unionByName(df)\n        else:\n            import pandas as pd\n\n            if (df[surrogate_key] == 0).any():\n                return df\n\n            unknown_row = {}\n            for col in df.columns:\n                if col == surrogate_key:\n                    unknown_row[col] = 0\n                elif col == natural_key:\n                    unknown_row[col] = \"-1\"\n                elif col == valid_from_col:\n                    unknown_row[col] = datetime(1900, 1, 1)\n                elif col == valid_to_col:\n                    unknown_row[col] = None\n                elif col == is_current_col:\n                    unknown_row[col] = True\n                elif col == \"load_timestamp\":\n                    # Use timezone-aware timestamp for Delta Lake compatibility\n                    from datetime import timezone\n\n                    unknown_row[col] = datetime.now(timezone.utc)\n                elif col == \"source_system\":\n                    unknown_row[col] = audit_config.get(\"source_system\", \"Unknown\")\n                else:\n                    dtype = df[col].dtype\n                    if pd.api.types.is_numeric_dtype(dtype):\n                        unknown_row[col] = 0\n                    else:\n                        unknown_row[col] = \"Unknown\"\n\n            unknown_df = pd.DataFrame([unknown_row])\n            for col in unknown_df.columns:\n                if col in df.columns:\n                    unknown_df[col] = unknown_df[col].astype(df[col].dtype)\n            return pd.concat([unknown_df, df], ignore_index=True)\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.date_dimension","title":"<code>odibi.patterns.date_dimension</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.date_dimension.DateDimensionPattern","title":"<code>DateDimensionPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Date Dimension Pattern: Generates a complete date dimension table.</p> <p>Creates a date dimension with pre-calculated attributes useful for BI/reporting including day of week, quarter, fiscal year, etc.</p> <p>Configuration Options (via params dict):     - start_date (str): Start date in YYYY-MM-DD format     - end_date (str): End date in YYYY-MM-DD format     - date_key_format (str): Format for date_sk (default: \"yyyyMMdd\" -&gt; 20240115)     - fiscal_year_start_month (int): Month when fiscal year starts (1-12, default: 1)     - include_time (bool): If true, generate time dimension (not implemented yet)     - unknown_member (bool): If true, add unknown date row with date_sk=0</p> Generated Columns <ul> <li>date_sk: Integer surrogate key (YYYYMMDD format)</li> <li>full_date: The actual date</li> <li>day_of_week: Day name (Monday, Tuesday, etc.)</li> <li>day_of_week_num: Day number (1=Monday, 7=Sunday)</li> <li>day_of_month: Day of month (1-31)</li> <li>day_of_year: Day of year (1-366)</li> <li>is_weekend: Boolean flag</li> <li>week_of_year: ISO week number (1-53)</li> <li>month: Month number (1-12)</li> <li>month_name: Month name (January, February, etc.)</li> <li>quarter: Calendar quarter (1-4)</li> <li>quarter_name: Q1, Q2, Q3, Q4</li> <li>year: Calendar year</li> <li>fiscal_year: Fiscal year</li> <li>fiscal_quarter: Fiscal quarter (1-4)</li> <li>is_month_start: First day of month</li> <li>is_month_end: Last day of month</li> <li>is_year_start: First day of year</li> <li>is_year_end: Last day of year</li> </ul> Source code in <code>odibi\\patterns\\date_dimension.py</code> <pre><code>class DateDimensionPattern(Pattern):\n    \"\"\"\n    Date Dimension Pattern: Generates a complete date dimension table.\n\n    Creates a date dimension with pre-calculated attributes useful for\n    BI/reporting including day of week, quarter, fiscal year, etc.\n\n    Configuration Options (via params dict):\n        - **start_date** (str): Start date in YYYY-MM-DD format\n        - **end_date** (str): End date in YYYY-MM-DD format\n        - **date_key_format** (str): Format for date_sk (default: \"yyyyMMdd\" -&gt; 20240115)\n        - **fiscal_year_start_month** (int): Month when fiscal year starts (1-12, default: 1)\n        - **include_time** (bool): If true, generate time dimension (not implemented yet)\n        - **unknown_member** (bool): If true, add unknown date row with date_sk=0\n\n    Generated Columns:\n        - date_sk: Integer surrogate key (YYYYMMDD format)\n        - full_date: The actual date\n        - day_of_week: Day name (Monday, Tuesday, etc.)\n        - day_of_week_num: Day number (1=Monday, 7=Sunday)\n        - day_of_month: Day of month (1-31)\n        - day_of_year: Day of year (1-366)\n        - is_weekend: Boolean flag\n        - week_of_year: ISO week number (1-53)\n        - month: Month number (1-12)\n        - month_name: Month name (January, February, etc.)\n        - quarter: Calendar quarter (1-4)\n        - quarter_name: Q1, Q2, Q3, Q4\n        - year: Calendar year\n        - fiscal_year: Fiscal year\n        - fiscal_quarter: Fiscal quarter (1-4)\n        - is_month_start: First day of month\n        - is_month_end: Last day of month\n        - is_year_start: First day of year\n        - is_year_end: Last day of year\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        ctx.debug(\n            \"DateDimensionPattern validation starting\",\n            pattern=\"DateDimensionPattern\",\n            params=self.params,\n        )\n\n        if not self.params.get(\"start_date\"):\n            ctx.error(\n                \"DateDimensionPattern validation failed: 'start_date' is required\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\n                \"DateDimensionPattern: 'start_date' parameter is required. \"\n                \"Expected format: 'YYYY-MM-DD' (e.g., '2024-01-01'). \"\n                \"Provide a valid start_date in params.\"\n            )\n\n        if not self.params.get(\"end_date\"):\n            ctx.error(\n                \"DateDimensionPattern validation failed: 'end_date' is required\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\n                \"DateDimensionPattern: 'end_date' parameter is required. \"\n                \"Expected format: 'YYYY-MM-DD' (e.g., '2024-12-31'). \"\n                \"Provide a valid end_date in params.\"\n            )\n\n        try:\n            start = self._parse_date(self.params[\"start_date\"])\n            end = self._parse_date(self.params[\"end_date\"])\n            if start &gt; end:\n                raise ValueError(\n                    f\"start_date must be before or equal to end_date. \"\n                    f\"Provided: start_date='{self.params['start_date']}', \"\n                    f\"end_date='{self.params['end_date']}'. \"\n                    f\"Swap the values or adjust the date range.\"\n                )\n        except Exception as e:\n            ctx.error(\n                f\"DateDimensionPattern validation failed: {e}\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\n                f\"DateDimensionPattern: Invalid date parameters. {e} \"\n                f\"Provided: start_date='{self.params.get('start_date')}', \"\n                f\"end_date='{self.params.get('end_date')}'. \"\n                f\"Expected format: 'YYYY-MM-DD'.\"\n            )\n\n        fiscal_month = self.params.get(\"fiscal_year_start_month\", 1)\n        if not isinstance(fiscal_month, int) or fiscal_month &lt; 1 or fiscal_month &gt; 12:\n            ctx.error(\n                \"DateDimensionPattern validation failed: invalid fiscal_year_start_month\",\n                pattern=\"DateDimensionPattern\",\n            )\n            raise ValueError(\n                f\"DateDimensionPattern: 'fiscal_year_start_month' must be an integer 1-12. \"\n                f\"Provided: {fiscal_month!r} (type: {type(fiscal_month).__name__}). \"\n                f\"Use an integer like 1 for January or 7 for July.\"\n            )\n\n        ctx.debug(\n            \"DateDimensionPattern validation passed\",\n            pattern=\"DateDimensionPattern\",\n        )\n\n    def _parse_date(self, date_str: str) -&gt; date:\n        \"\"\"Parse a date string in YYYY-MM-DD format.\"\"\"\n        if isinstance(date_str, (date, datetime)):\n            return date_str if isinstance(date_str, date) else date_str.date()\n        return datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        start_date = self._parse_date(self.params[\"start_date\"])\n        end_date = self._parse_date(self.params[\"end_date\"])\n        fiscal_year_start_month = self.params.get(\"fiscal_year_start_month\", 1)\n        unknown_member = self.params.get(\"unknown_member\", False)\n\n        ctx.debug(\n            \"DateDimensionPattern starting\",\n            pattern=\"DateDimensionPattern\",\n            start_date=str(start_date),\n            end_date=str(end_date),\n            fiscal_year_start_month=fiscal_year_start_month,\n        )\n\n        try:\n            if context.engine_type == EngineType.SPARK:\n                result_df = self._generate_spark(\n                    context, start_date, end_date, fiscal_year_start_month\n                )\n            else:\n                result_df = self._generate_pandas(start_date, end_date, fiscal_year_start_month)\n\n            if unknown_member:\n                result_df = self._add_unknown_member(context, result_df)\n\n            row_count = self._get_row_count(result_df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"DateDimensionPattern completed\",\n                pattern=\"DateDimensionPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                rows_generated=row_count,\n                start_date=str(start_date),\n                end_date=str(end_date),\n            )\n\n            return result_df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"DateDimensionPattern failed: {e}\",\n                pattern=\"DateDimensionPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _generate_pandas(\n        self, start_date: date, end_date: date, fiscal_year_start_month: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate date dimension using Pandas.\"\"\"\n        dates = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n        df = pd.DataFrame({\"full_date\": dates})\n\n        df[\"date_sk\"] = df[\"full_date\"].dt.strftime(\"%Y%m%d\").astype(int)\n\n        df[\"day_of_week\"] = df[\"full_date\"].dt.day_name()\n        df[\"day_of_week_num\"] = df[\"full_date\"].dt.dayofweek + 1\n        df[\"day_of_month\"] = df[\"full_date\"].dt.day\n        df[\"day_of_year\"] = df[\"full_date\"].dt.dayofyear\n\n        df[\"is_weekend\"] = df[\"day_of_week_num\"].isin([6, 7])\n\n        df[\"week_of_year\"] = df[\"full_date\"].dt.isocalendar().week.astype(int)\n\n        df[\"month\"] = df[\"full_date\"].dt.month\n        df[\"month_name\"] = df[\"full_date\"].dt.month_name()\n\n        df[\"quarter\"] = df[\"full_date\"].dt.quarter\n        df[\"quarter_name\"] = \"Q\" + df[\"quarter\"].astype(str)\n\n        df[\"year\"] = df[\"full_date\"].dt.year\n\n        df[\"fiscal_year\"] = df.apply(\n            lambda row: self._calc_fiscal_year(row[\"full_date\"], fiscal_year_start_month),\n            axis=1,\n        )\n        df[\"fiscal_quarter\"] = df.apply(\n            lambda row: self._calc_fiscal_quarter(row[\"full_date\"], fiscal_year_start_month),\n            axis=1,\n        )\n\n        df[\"is_month_start\"] = df[\"full_date\"].dt.is_month_start\n        df[\"is_month_end\"] = df[\"full_date\"].dt.is_month_end\n        df[\"is_year_start\"] = (df[\"month\"] == 1) &amp; (df[\"day_of_month\"] == 1)\n        df[\"is_year_end\"] = (df[\"month\"] == 12) &amp; (df[\"day_of_month\"] == 31)\n\n        df[\"full_date\"] = df[\"full_date\"].dt.date\n\n        column_order = [\n            \"date_sk\",\n            \"full_date\",\n            \"day_of_week\",\n            \"day_of_week_num\",\n            \"day_of_month\",\n            \"day_of_year\",\n            \"is_weekend\",\n            \"week_of_year\",\n            \"month\",\n            \"month_name\",\n            \"quarter\",\n            \"quarter_name\",\n            \"year\",\n            \"fiscal_year\",\n            \"fiscal_quarter\",\n            \"is_month_start\",\n            \"is_month_end\",\n            \"is_year_start\",\n            \"is_year_end\",\n        ]\n        return df[column_order]\n\n    def _calc_fiscal_year(self, dt, fiscal_start_month: int) -&gt; int:\n        \"\"\"Calculate fiscal year based on fiscal start month.\"\"\"\n        if isinstance(dt, pd.Timestamp):\n            month = dt.month\n            year = dt.year\n        else:\n            month = dt.month\n            year = dt.year\n\n        if fiscal_start_month == 1:\n            return year\n        if month &gt;= fiscal_start_month:\n            return year + 1\n        return year\n\n    def _calc_fiscal_quarter(self, dt, fiscal_start_month: int) -&gt; int:\n        \"\"\"Calculate fiscal quarter based on fiscal start month.\"\"\"\n        if isinstance(dt, pd.Timestamp):\n            month = dt.month\n        else:\n            month = dt.month\n\n        adjusted_month = (month - fiscal_start_month) % 12\n        return (adjusted_month // 3) + 1\n\n    def _generate_spark(\n        self, context: EngineContext, start_date: date, end_date: date, fiscal_year_start_month: int\n    ):\n        \"\"\"Generate date dimension using Spark.\"\"\"\n        from pyspark.sql import functions as F\n        from pyspark.sql.types import IntegerType\n\n        spark = context.spark\n\n        num_days = (end_date - start_date).days + 1\n        start_date_str = start_date.strftime(\"%Y-%m-%d\")\n\n        df = spark.range(num_days).select(\n            F.date_add(F.lit(start_date_str), F.col(\"id\").cast(IntegerType())).alias(\"full_date\")\n        )\n\n        df = df.withColumn(\"date_sk\", F.date_format(\"full_date\", \"yyyyMMdd\").cast(IntegerType()))\n\n        df = df.withColumn(\"day_of_week\", F.date_format(\"full_date\", \"EEEE\"))\n        df = df.withColumn(\"day_of_week_num\", F.dayofweek(\"full_date\"))\n        df = df.withColumn(\n            \"day_of_week_num\",\n            F.when(F.col(\"day_of_week_num\") == 1, 7).otherwise(F.col(\"day_of_week_num\") - 1),\n        )\n        df = df.withColumn(\"day_of_month\", F.dayofmonth(\"full_date\"))\n        df = df.withColumn(\"day_of_year\", F.dayofyear(\"full_date\"))\n\n        df = df.withColumn(\"is_weekend\", F.col(\"day_of_week_num\").isin([6, 7]))\n\n        df = df.withColumn(\"week_of_year\", F.weekofyear(\"full_date\"))\n\n        df = df.withColumn(\"month\", F.month(\"full_date\"))\n        df = df.withColumn(\"month_name\", F.date_format(\"full_date\", \"MMMM\"))\n\n        df = df.withColumn(\"quarter\", F.quarter(\"full_date\"))\n        df = df.withColumn(\"quarter_name\", F.concat(F.lit(\"Q\"), F.col(\"quarter\")))\n\n        df = df.withColumn(\"year\", F.year(\"full_date\"))\n\n        if fiscal_year_start_month == 1:\n            df = df.withColumn(\"fiscal_year\", F.col(\"year\"))\n            df = df.withColumn(\"fiscal_quarter\", F.col(\"quarter\"))\n        else:\n            df = df.withColumn(\n                \"fiscal_year\",\n                F.when(F.col(\"month\") &gt;= fiscal_year_start_month, F.col(\"year\") + 1).otherwise(\n                    F.col(\"year\")\n                ),\n            )\n            adjusted_month = (F.col(\"month\") - fiscal_year_start_month + 12) % 12\n            df = df.withColumn(\"fiscal_quarter\", (adjusted_month / 3).cast(IntegerType()) + 1)\n\n        df = df.withColumn(\n            \"is_month_start\",\n            F.col(\"day_of_month\") == 1,\n        )\n        df = df.withColumn(\n            \"is_month_end\",\n            F.col(\"full_date\") == F.last_day(\"full_date\"),\n        )\n        df = df.withColumn(\n            \"is_year_start\",\n            (F.col(\"month\") == 1) &amp; (F.col(\"day_of_month\") == 1),\n        )\n        df = df.withColumn(\n            \"is_year_end\",\n            (F.col(\"month\") == 12) &amp; (F.col(\"day_of_month\") == 31),\n        )\n\n        column_order = [\n            \"date_sk\",\n            \"full_date\",\n            \"day_of_week\",\n            \"day_of_week_num\",\n            \"day_of_month\",\n            \"day_of_year\",\n            \"is_weekend\",\n            \"week_of_year\",\n            \"month\",\n            \"month_name\",\n            \"quarter\",\n            \"quarter_name\",\n            \"year\",\n            \"fiscal_year\",\n            \"fiscal_quarter\",\n            \"is_month_start\",\n            \"is_month_end\",\n            \"is_year_start\",\n            \"is_year_end\",\n        ]\n        return df.select(column_order)\n\n    def _add_unknown_member(self, context: EngineContext, df):\n        \"\"\"Add unknown member row with date_sk=0.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import Row\n\n            unknown_data = {\n                \"date_sk\": 0,\n                \"full_date\": date(1900, 1, 1),\n                \"day_of_week\": \"Unknown\",\n                \"day_of_week_num\": 0,\n                \"day_of_month\": 0,\n                \"day_of_year\": 0,\n                \"is_weekend\": False,\n                \"week_of_year\": 0,\n                \"month\": 0,\n                \"month_name\": \"Unknown\",\n                \"quarter\": 0,\n                \"quarter_name\": \"Unknown\",\n                \"year\": 0,\n                \"fiscal_year\": 0,\n                \"fiscal_quarter\": 0,\n                \"is_month_start\": False,\n                \"is_month_end\": False,\n                \"is_year_start\": False,\n                \"is_year_end\": False,\n            }\n            unknown_row = context.spark.createDataFrame([Row(**unknown_data)])\n            return unknown_row.unionByName(df)\n        else:\n            unknown_row = pd.DataFrame(\n                [\n                    {\n                        \"date_sk\": 0,\n                        \"full_date\": date(1900, 1, 1),\n                        \"day_of_week\": \"Unknown\",\n                        \"day_of_week_num\": 0,\n                        \"day_of_month\": 0,\n                        \"day_of_year\": 0,\n                        \"is_weekend\": False,\n                        \"week_of_year\": 0,\n                        \"month\": 0,\n                        \"month_name\": \"Unknown\",\n                        \"quarter\": 0,\n                        \"quarter_name\": \"Unknown\",\n                        \"year\": 0,\n                        \"fiscal_year\": 0,\n                        \"fiscal_quarter\": 0,\n                        \"is_month_start\": False,\n                        \"is_month_end\": False,\n                        \"is_year_start\": False,\n                        \"is_year_end\": False,\n                    }\n                ]\n            )\n            return pd.concat([unknown_row, df], ignore_index=True)\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.fact","title":"<code>odibi.patterns.fact</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.fact.FactPattern","title":"<code>FactPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Enhanced Fact Pattern: Builds fact tables with automatic SK lookups.</p> <p>Features: - Automatic surrogate key lookups from dimension tables - Orphan handling (unknown member, reject, or quarantine) - Grain validation (detect duplicates at PK level) - Audit columns (load_timestamp, source_system) - Deduplication support - Measure calculations and renaming</p> <p>Basic Params (backward compatible):     deduplicate (bool): If true, removes duplicates before insert.     keys (list): Keys for deduplication.</p> Enhanced Params <p>grain (list): Columns that define uniqueness (validates no duplicates) dimensions (list): Dimension lookup configurations     - source_column: Column in source data     - dimension_table: Name of dimension in context     - dimension_key: Natural key column in dimension     - surrogate_key: Surrogate key to retrieve     - scd2 (bool): If true, filter is_current=true orphan_handling (str): \"unknown\" | \"reject\" | \"quarantine\" quarantine (dict): Quarantine configuration (required if orphan_handling=quarantine)     - connection: Connection name for quarantine writes     - path: Path for quarantine data (or use 'table')     - table: Table name for quarantine (or use 'path')     - add_columns (dict): Metadata columns to add         - _rejection_reason (bool): Add rejection reason column         - _rejected_at (bool): Add rejection timestamp column         - _source_dimension (bool): Add source dimension name column measures (list): Measure definitions (passthrough, rename, or calculated) audit (dict): Audit column configuration     - load_timestamp (bool)     - source_system (str)</p> Example Config <p>pattern:   type: fact   params:     grain: [order_id]     dimensions:       - source_column: customer_id         dimension_table: dim_customer         dimension_key: customer_id         surrogate_key: customer_sk         scd2: true     orphan_handling: unknown     measures:       - quantity       - total_amount: \"quantity * price\"     audit:       load_timestamp: true       source_system: \"pos\"</p> Example with Quarantine <p>pattern:   type: fact   params:     dimensions:       - source_column: customer_id         dimension_table: dim_customer         dimension_key: customer_id         surrogate_key: customer_sk     orphan_handling: quarantine     quarantine:       connection: silver       path: fact_orders_orphans       add_columns:         _rejection_reason: true         _rejected_at: true         _source_dimension: true</p> Source code in <code>odibi\\patterns\\fact.py</code> <pre><code>class FactPattern(Pattern):\n    \"\"\"\n    Enhanced Fact Pattern: Builds fact tables with automatic SK lookups.\n\n    Features:\n    - Automatic surrogate key lookups from dimension tables\n    - Orphan handling (unknown member, reject, or quarantine)\n    - Grain validation (detect duplicates at PK level)\n    - Audit columns (load_timestamp, source_system)\n    - Deduplication support\n    - Measure calculations and renaming\n\n    Basic Params (backward compatible):\n        deduplicate (bool): If true, removes duplicates before insert.\n        keys (list): Keys for deduplication.\n\n    Enhanced Params:\n        grain (list): Columns that define uniqueness (validates no duplicates)\n        dimensions (list): Dimension lookup configurations\n            - source_column: Column in source data\n            - dimension_table: Name of dimension in context\n            - dimension_key: Natural key column in dimension\n            - surrogate_key: Surrogate key to retrieve\n            - scd2 (bool): If true, filter is_current=true\n        orphan_handling (str): \"unknown\" | \"reject\" | \"quarantine\"\n        quarantine (dict): Quarantine configuration (required if orphan_handling=quarantine)\n            - connection: Connection name for quarantine writes\n            - path: Path for quarantine data (or use 'table')\n            - table: Table name for quarantine (or use 'path')\n            - add_columns (dict): Metadata columns to add\n                - _rejection_reason (bool): Add rejection reason column\n                - _rejected_at (bool): Add rejection timestamp column\n                - _source_dimension (bool): Add source dimension name column\n        measures (list): Measure definitions (passthrough, rename, or calculated)\n        audit (dict): Audit column configuration\n            - load_timestamp (bool)\n            - source_system (str)\n\n    Example Config:\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - total_amount: \"quantity * price\"\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n\n    Example with Quarantine:\n        pattern:\n          type: fact\n          params:\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n            orphan_handling: quarantine\n            quarantine:\n              connection: silver\n              path: fact_orders_orphans\n              add_columns:\n                _rejection_reason: true\n                _rejected_at: true\n                _source_dimension: true\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        deduplicate = self.params.get(\"deduplicate\")\n        keys = self.params.get(\"keys\")\n        grain = self.params.get(\"grain\")\n        dimensions = self.params.get(\"dimensions\", [])\n        orphan_handling = self.params.get(\"orphan_handling\", \"unknown\")\n\n        ctx.debug(\n            \"FactPattern validation starting\",\n            pattern=\"FactPattern\",\n            deduplicate=deduplicate,\n            keys=keys,\n            grain=grain,\n            dimensions_count=len(dimensions),\n        )\n\n        if deduplicate and not keys:\n            ctx.error(\n                \"FactPattern validation failed: 'keys' required when 'deduplicate' is True\",\n                pattern=\"FactPattern\",\n            )\n            raise ValueError(\n                \"FactPattern: 'keys' required when 'deduplicate' is True. \"\n                \"Keys define which columns uniquely identify a fact row for deduplication. \"\n                \"Provide keys=['col1', 'col2'] to specify the deduplication columns.\"\n            )\n\n        if orphan_handling not in (\"unknown\", \"reject\", \"quarantine\"):\n            ctx.error(\n                f\"FactPattern validation failed: invalid orphan_handling '{orphan_handling}'\",\n                pattern=\"FactPattern\",\n            )\n            raise ValueError(\n                f\"FactPattern: 'orphan_handling' must be 'unknown', 'reject', or 'quarantine'. \"\n                f\"Got: {orphan_handling}\"\n            )\n\n        if orphan_handling == \"quarantine\":\n            quarantine_config = self.params.get(\"quarantine\")\n            if not quarantine_config:\n                ctx.error(\n                    \"FactPattern validation failed: 'quarantine' config required \"\n                    \"when orphan_handling='quarantine'\",\n                    pattern=\"FactPattern\",\n                )\n                raise ValueError(\n                    \"FactPattern: 'quarantine' configuration is required when \"\n                    \"orphan_handling='quarantine'.\"\n                )\n            if not quarantine_config.get(\"connection\"):\n                ctx.error(\n                    \"FactPattern validation failed: quarantine.connection is required\",\n                    pattern=\"FactPattern\",\n                )\n                raise ValueError(\n                    \"FactPattern: 'quarantine.connection' is required. \"\n                    \"The connection specifies where to write quarantined orphan records \"\n                    \"(e.g., a Spark session or database connection). \"\n                    \"Add 'connection' to your quarantine config.\"\n                )\n            if not quarantine_config.get(\"path\") and not quarantine_config.get(\"table\"):\n                ctx.error(\n                    \"FactPattern validation failed: quarantine requires 'path' or 'table'\",\n                    pattern=\"FactPattern\",\n                )\n                raise ValueError(\n                    f\"FactPattern: 'quarantine' requires either 'path' or 'table'. \"\n                    f\"Got config: {quarantine_config}. \"\n                    \"Add 'path' for file storage or 'table' for database storage.\"\n                )\n\n        for i, dim in enumerate(dimensions):\n            required_keys = [\"source_column\", \"dimension_table\", \"dimension_key\", \"surrogate_key\"]\n            for key in required_keys:\n                if key not in dim:\n                    ctx.error(\n                        f\"FactPattern validation failed: dimension[{i}] missing '{key}'\",\n                        pattern=\"FactPattern\",\n                    )\n                    raise ValueError(\n                        f\"FactPattern: dimension[{i}] missing required key '{key}'. \"\n                        f\"Required keys: {required_keys}. \"\n                        f\"Got: {dim}. \"\n                        f\"Ensure all required keys are provided in the dimension config.\"\n                    )\n\n        ctx.debug(\n            \"FactPattern validation passed\",\n            pattern=\"FactPattern\",\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        deduplicate = self.params.get(\"deduplicate\")\n        keys = self.params.get(\"keys\")\n        grain = self.params.get(\"grain\")\n        dimensions = self.params.get(\"dimensions\", [])\n        orphan_handling = self.params.get(\"orphan_handling\", \"unknown\")\n        quarantine_config = self.params.get(\"quarantine\", {})\n        measures = self.params.get(\"measures\", [])\n        audit_config = self.params.get(\"audit\", {})\n\n        ctx.debug(\n            \"FactPattern starting\",\n            pattern=\"FactPattern\",\n            deduplicate=deduplicate,\n            keys=keys,\n            grain=grain,\n            dimensions_count=len(dimensions),\n            orphan_handling=orphan_handling,\n        )\n\n        df = context.df\n        source_count = self._get_row_count(df, context.engine_type)\n        ctx.debug(\"Fact source loaded\", pattern=\"FactPattern\", source_rows=source_count)\n\n        try:\n            if deduplicate and keys:\n                df = self._deduplicate(context, df, keys)\n                ctx.debug(\n                    \"Fact deduplication complete\",\n                    pattern=\"FactPattern\",\n                    rows_after=self._get_row_count(df, context.engine_type),\n                )\n\n            if dimensions:\n                df, orphan_count, quarantined_df = self._lookup_dimensions(\n                    context, df, dimensions, orphan_handling, quarantine_config\n                )\n                ctx.debug(\n                    \"Fact dimension lookups complete\",\n                    pattern=\"FactPattern\",\n                    orphan_count=orphan_count,\n                )\n\n                if orphan_handling == \"quarantine\" and quarantined_df is not None:\n                    self._write_quarantine(context, quarantined_df, quarantine_config)\n                    ctx.info(\n                        f\"Quarantined {orphan_count} orphan records\",\n                        pattern=\"FactPattern\",\n                        quarantine_path=quarantine_config.get(\"path\")\n                        or quarantine_config.get(\"table\"),\n                    )\n\n            if measures:\n                df = self._apply_measures(context, df, measures)\n\n            if grain:\n                self._validate_grain(context, df, grain)\n\n            df = self._add_audit_columns(context, df, audit_config)\n\n            result_count = self._get_row_count(df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"FactPattern completed\",\n                pattern=\"FactPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                source_rows=source_count,\n                result_rows=result_count,\n            )\n\n            return df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"FactPattern failed: {e}\",\n                pattern=\"FactPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _deduplicate(self, context: EngineContext, df, keys: List[str]):\n        \"\"\"Remove duplicates based on keys.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return df.dropDuplicates(keys)\n        else:\n            return df.drop_duplicates(subset=keys)\n\n    def _lookup_dimensions(\n        self,\n        context: EngineContext,\n        df,\n        dimensions: List[Dict],\n        orphan_handling: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"\n        Perform surrogate key lookups from dimension tables.\n\n        Returns:\n            Tuple of (result_df, orphan_count, quarantined_df)\n        \"\"\"\n        total_orphans = 0\n        all_quarantined = []\n\n        for dim_config in dimensions:\n            source_col = dim_config[\"source_column\"]\n            dim_table = dim_config[\"dimension_table\"]\n            dim_key = dim_config[\"dimension_key\"]\n            sk_col = dim_config[\"surrogate_key\"]\n            is_scd2 = dim_config.get(\"scd2\", False)\n\n            dim_df = self._get_dimension_df(context, dim_table, is_scd2)\n            if dim_df is None:\n                raise ValueError(\n                    f\"FactPattern: Dimension table '{dim_table}' not found in context.\"\n                )\n\n            df, orphan_count, quarantined = self._join_dimension(\n                context,\n                df,\n                dim_df,\n                source_col,\n                dim_key,\n                sk_col,\n                orphan_handling,\n                dim_table,\n                quarantine_config,\n            )\n            total_orphans += orphan_count\n            if quarantined is not None:\n                all_quarantined.append(quarantined)\n\n        quarantined_df = None\n        if all_quarantined:\n            quarantined_df = self._union_dataframes(context, all_quarantined)\n\n        return df, total_orphans, quarantined_df\n\n    def _union_dataframes(self, context: EngineContext, dfs: List):\n        \"\"\"Union multiple DataFrames together.\"\"\"\n        if not dfs:\n            return None\n        if context.engine_type == EngineType.SPARK:\n            result = dfs[0]\n            for df in dfs[1:]:\n                result = result.unionByName(df, allowMissingColumns=True)\n            return result\n        else:\n            import pandas as pd\n\n            return pd.concat(dfs, ignore_index=True)\n\n    def _get_dimension_df(self, context: EngineContext, dim_table: str, is_scd2: bool):\n        \"\"\"Get dimension DataFrame from context, optionally filtering for current records.\"\"\"\n        try:\n            dim_df = context.get(dim_table)\n        except KeyError:\n            return None\n\n        if is_scd2:\n            is_current_col = \"is_current\"\n            if context.engine_type == EngineType.SPARK:\n                from pyspark.sql import functions as F\n\n                if is_current_col in dim_df.columns:\n                    dim_df = dim_df.filter(F.col(is_current_col) == True)  # noqa: E712\n            else:\n                if is_current_col in dim_df.columns:\n                    dim_df = dim_df[dim_df[is_current_col] == True].copy()  # noqa: E712\n\n        return dim_df\n\n    def _join_dimension(\n        self,\n        context: EngineContext,\n        fact_df,\n        dim_df,\n        source_col: str,\n        dim_key: str,\n        sk_col: str,\n        orphan_handling: str,\n        dim_table: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"\n        Join fact to dimension and retrieve surrogate key.\n\n        Returns:\n            Tuple of (result_df, orphan_count, quarantined_df)\n        \"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._join_dimension_spark(\n                context,\n                fact_df,\n                dim_df,\n                source_col,\n                dim_key,\n                sk_col,\n                orphan_handling,\n                dim_table,\n                quarantine_config,\n            )\n        else:\n            return self._join_dimension_pandas(\n                fact_df,\n                dim_df,\n                source_col,\n                dim_key,\n                sk_col,\n                orphan_handling,\n                dim_table,\n                quarantine_config,\n            )\n\n    def _join_dimension_spark(\n        self,\n        context: EngineContext,\n        fact_df,\n        dim_df,\n        source_col: str,\n        dim_key: str,\n        sk_col: str,\n        orphan_handling: str,\n        dim_table: str,\n        quarantine_config: Dict,\n    ):\n        from pyspark.sql import functions as F\n\n        dim_subset = dim_df.select(\n            F.col(dim_key).alias(f\"_dim_{dim_key}\"),\n            F.col(sk_col).alias(sk_col),\n        )\n\n        joined = fact_df.join(\n            dim_subset,\n            fact_df[source_col] == dim_subset[f\"_dim_{dim_key}\"],\n            \"left\",\n        )\n\n        orphan_mask = F.col(sk_col).isNull()\n        orphan_count = joined.filter(orphan_mask).count()\n        quarantined_df = None\n\n        if orphan_handling == \"reject\" and orphan_count &gt; 0:\n            raise ValueError(\n                f\"FactPattern: {orphan_count} orphan records found for dimension \"\n                f\"lookup on '{source_col}'. Orphan handling is set to 'reject'.\"\n            )\n\n        if orphan_handling == \"unknown\":\n            joined = joined.withColumn(sk_col, F.coalesce(F.col(sk_col), F.lit(0)))\n\n        if orphan_handling == \"quarantine\" and orphan_count &gt; 0:\n            orphan_rows = joined.filter(orphan_mask).drop(f\"_dim_{dim_key}\")\n            orphan_rows = self._add_quarantine_metadata_spark(\n                orphan_rows, dim_table, source_col, quarantine_config\n            )\n            quarantined_df = orphan_rows\n            joined = joined.filter(~orphan_mask)\n\n        result = joined.drop(f\"_dim_{dim_key}\")\n\n        return result, orphan_count, quarantined_df\n\n    def _join_dimension_pandas(\n        self,\n        fact_df,\n        dim_df,\n        source_col: str,\n        dim_key: str,\n        sk_col: str,\n        orphan_handling: str,\n        dim_table: str,\n        quarantine_config: Dict,\n    ):\n        import pandas as pd\n\n        dim_subset = dim_df[[dim_key, sk_col]].copy()\n        dim_subset = dim_subset.rename(columns={dim_key: f\"_dim_{dim_key}\"})\n\n        merged = pd.merge(\n            fact_df,\n            dim_subset,\n            left_on=source_col,\n            right_on=f\"_dim_{dim_key}\",\n            how=\"left\",\n        )\n\n        orphan_mask = merged[sk_col].isna()\n        orphan_count = orphan_mask.sum()\n        quarantined_df = None\n\n        if orphan_handling == \"reject\" and orphan_count &gt; 0:\n            raise ValueError(\n                f\"FactPattern: {orphan_count} orphan records found for dimension \"\n                f\"lookup on '{source_col}'. Orphan handling is set to 'reject'.\"\n            )\n\n        if orphan_handling == \"unknown\":\n            merged[sk_col] = merged[sk_col].fillna(0).infer_objects(copy=False).astype(int)\n\n        if orphan_handling == \"quarantine\" and orphan_count &gt; 0:\n            orphan_rows = merged[orphan_mask].drop(columns=[f\"_dim_{dim_key}\"]).copy()\n            orphan_rows = self._add_quarantine_metadata_pandas(\n                orphan_rows, dim_table, source_col, quarantine_config\n            )\n            quarantined_df = orphan_rows\n            merged = merged[~orphan_mask].copy()\n\n        result = merged.drop(columns=[f\"_dim_{dim_key}\"])\n\n        return result, int(orphan_count), quarantined_df\n\n    def _apply_measures(self, context: EngineContext, df, measures: List):\n        \"\"\"\n        Apply measure transformations.\n\n        Measures can be:\n        - String: passthrough column name\n        - Dict with single key-value: rename or calculate\n          - {\"new_name\": \"old_name\"} -&gt; rename\n          - {\"new_name\": \"expr\"} -&gt; calculate (if expr contains operators)\n        \"\"\"\n        for measure in measures:\n            if isinstance(measure, str):\n                continue\n            elif isinstance(measure, dict):\n                for new_name, expr in measure.items():\n                    if self._is_expression(expr):\n                        df = self._add_calculated_measure(context, df, new_name, expr)\n                    else:\n                        df = self._rename_column(context, df, expr, new_name)\n\n        return df\n\n    def _is_expression(self, expr: str) -&gt; bool:\n        \"\"\"Check if string is a calculation expression.\"\"\"\n        operators = [\"+\", \"-\", \"*\", \"/\", \"(\", \")\"]\n        return any(op in expr for op in operators)\n\n    def _add_calculated_measure(self, context: EngineContext, df, name: str, expr: str):\n        \"\"\"Add a calculated measure column.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            return df.withColumn(name, F.expr(expr))\n        else:\n            df = df.copy()\n            df[name] = df.eval(expr)\n            return df\n\n    def _rename_column(self, context: EngineContext, df, old_name: str, new_name: str):\n        \"\"\"Rename a column.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return df.withColumnRenamed(old_name, new_name)\n        else:\n            return df.rename(columns={old_name: new_name})\n\n    def _validate_grain(self, context: EngineContext, df, grain: List[str]):\n        \"\"\"\n        Validate that no duplicate rows exist at the grain level.\n\n        Raises ValueError if duplicates are found.\n        \"\"\"\n        ctx = get_logging_context()\n\n        if context.engine_type == EngineType.SPARK:\n            total_count = df.count()\n            distinct_count = df.select(grain).distinct().count()\n        else:\n            total_count = len(df)\n            distinct_count = len(df.drop_duplicates(subset=grain))\n\n        if total_count != distinct_count:\n            duplicate_count = total_count - distinct_count\n            ctx.error(\n                f\"FactPattern grain validation failed: {duplicate_count} duplicate rows\",\n                pattern=\"FactPattern\",\n                grain=grain,\n                total_rows=total_count,\n                distinct_rows=distinct_count,\n            )\n            raise ValueError(\n                f\"FactPattern: Grain validation failed. Found {duplicate_count} duplicate \"\n                f\"rows at grain level {grain}. Total rows: {total_count}, \"\n                f\"Distinct rows: {distinct_count}.\"\n            )\n\n        ctx.debug(\n            \"FactPattern grain validation passed\",\n            pattern=\"FactPattern\",\n            grain=grain,\n            total_rows=total_count,\n        )\n\n    def _add_audit_columns(self, context: EngineContext, df, audit_config: Dict):\n        \"\"\"Add audit columns (load_timestamp, source_system).\"\"\"\n        load_timestamp = audit_config.get(\"load_timestamp\", False)\n        source_system = audit_config.get(\"source_system\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            if load_timestamp:\n                df = df.withColumn(\"load_timestamp\", F.current_timestamp())\n            if source_system:\n                df = df.withColumn(\"source_system\", F.lit(source_system))\n        else:\n            if load_timestamp or source_system:\n                df = df.copy()\n            if load_timestamp:\n                # Use timezone-aware timestamp for Delta Lake compatibility\n                from datetime import timezone\n\n                df[\"load_timestamp\"] = datetime.now(timezone.utc)\n            if source_system:\n                df[\"source_system\"] = source_system\n\n        return df\n\n    def _add_quarantine_metadata_spark(\n        self,\n        df,\n        dim_table: str,\n        source_col: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"Add metadata columns to quarantined Spark DataFrame.\"\"\"\n        from pyspark.sql import functions as F\n\n        add_columns = quarantine_config.get(\"add_columns\", {})\n\n        if add_columns.get(\"_rejection_reason\", False):\n            reason = f\"Orphan record: no match in dimension '{dim_table}' on column '{source_col}'\"\n            df = df.withColumn(\"_rejection_reason\", F.lit(reason))\n\n        if add_columns.get(\"_rejected_at\", False):\n            df = df.withColumn(\"_rejected_at\", F.current_timestamp())\n\n        if add_columns.get(\"_source_dimension\", False):\n            df = df.withColumn(\"_source_dimension\", F.lit(dim_table))\n\n        return df\n\n    def _add_quarantine_metadata_pandas(\n        self,\n        df,\n        dim_table: str,\n        source_col: str,\n        quarantine_config: Dict,\n    ):\n        \"\"\"Add metadata columns to quarantined Pandas DataFrame.\"\"\"\n        add_columns = quarantine_config.get(\"add_columns\", {})\n\n        if add_columns.get(\"_rejection_reason\", False):\n            reason = f\"Orphan record: no match in dimension '{dim_table}' on column '{source_col}'\"\n            df[\"_rejection_reason\"] = reason\n\n        if add_columns.get(\"_rejected_at\", False):\n            # Use timezone-aware timestamp for Delta Lake compatibility\n            from datetime import timezone\n\n            df[\"_rejected_at\"] = datetime.now(timezone.utc)\n\n        if add_columns.get(\"_source_dimension\", False):\n            df[\"_source_dimension\"] = dim_table\n\n        return df\n\n    def _write_quarantine(\n        self,\n        context: EngineContext,\n        quarantined_df,\n        quarantine_config: Dict,\n    ):\n        \"\"\"Write quarantined records to the configured destination.\"\"\"\n        ctx = get_logging_context()\n        connection = quarantine_config.get(\"connection\")\n        path = quarantine_config.get(\"path\")\n        table = quarantine_config.get(\"table\")\n\n        if context.engine_type == EngineType.SPARK:\n            self._write_quarantine_spark(context, quarantined_df, connection, path, table)\n        else:\n            self._write_quarantine_pandas(context, quarantined_df, connection, path, table)\n\n        ctx.debug(\n            \"Quarantine data written\",\n            pattern=\"FactPattern\",\n            connection=connection,\n            destination=path or table,\n        )\n\n    def _write_quarantine_spark(\n        self,\n        context: EngineContext,\n        df,\n        connection: str,\n        path: Optional[str],\n        table: Optional[str],\n    ):\n        \"\"\"Write quarantine data using Spark.\"\"\"\n        if table:\n            full_table = f\"{connection}.{table}\" if connection else table\n            df.write.format(\"delta\").mode(\"append\").saveAsTable(full_table)\n        elif path:\n            full_path = path\n            if hasattr(context, \"engine\") and context.engine:\n                if connection in getattr(context.engine, \"connections\", {}):\n                    try:\n                        full_path = context.engine.connections[connection].get_path(path)\n                    except Exception:\n                        pass\n            df.write.format(\"delta\").mode(\"append\").save(full_path)\n\n    def _write_quarantine_pandas(\n        self,\n        context: EngineContext,\n        df,\n        connection: str,\n        path: Optional[str],\n        table: Optional[str],\n    ):\n        \"\"\"Write quarantine data using Pandas.\"\"\"\n        import os\n\n        destination = path or table\n        full_path = destination\n\n        if hasattr(context, \"engine\") and context.engine:\n            if connection in getattr(context.engine, \"connections\", {}):\n                try:\n                    full_path = context.engine.connections[connection].get_path(destination)\n                except Exception:\n                    pass\n\n        path_lower = str(full_path).lower()\n\n        if path_lower.endswith(\".csv\"):\n            if os.path.exists(full_path):\n                df.to_csv(full_path, mode=\"a\", header=False, index=False)\n            else:\n                df.to_csv(full_path, index=False)\n        elif path_lower.endswith(\".json\"):\n            if os.path.exists(full_path):\n                import pandas as pd\n\n                existing = pd.read_json(full_path)\n                combined = pd.concat([existing, df], ignore_index=True)\n                combined.to_json(full_path, orient=\"records\")\n            else:\n                df.to_json(full_path, orient=\"records\")\n        else:\n            if os.path.exists(full_path):\n                import pandas as pd\n\n                existing = pd.read_parquet(full_path)\n                combined = pd.concat([existing, df], ignore_index=True)\n                combined.to_parquet(full_path, index=False)\n            else:\n                df.to_parquet(full_path, index=False)\n</code></pre>"},{"location":"reference/api/patterns/#odibi.patterns.aggregation","title":"<code>odibi.patterns.aggregation</code>","text":""},{"location":"reference/api/patterns/#odibi.patterns.aggregation.AggregationPattern","title":"<code>AggregationPattern</code>","text":"<p>               Bases: <code>Pattern</code></p> <p>Aggregation Pattern: Declarative aggregation with time-grain rollups.</p> <p>Features: - Declare grain (GROUP BY columns) - Declare measures with aggregation functions - Incremental aggregation (merge new data with existing) - Time rollups (generate multiple grain levels) - Audit columns</p> <p>Configuration Options (via params dict):     - grain (list): Columns to GROUP BY (defines uniqueness)     - measures (list): Measure definitions with name and aggregation expr         - name: Output column name         - expr: SQL aggregation expression (e.g., \"SUM(amount)\")     - incremental (dict): Incremental merge configuration (optional)         - timestamp_column: Column to identify new data         - merge_strategy: \"replace\", \"sum\", \"min\", or \"max\"     - having (str): Optional HAVING clause for filtering aggregates     - audit (dict): Audit column configuration</p> Example Config <p>pattern:   type: aggregation   params:     grain: [date_sk, product_sk]     measures:       - name: total_revenue         expr: \"SUM(total_amount)\"       - name: order_count         expr: \"COUNT()\"       - name: avg_order_value         expr: \"AVG(total_amount)\"     having: \"COUNT() &gt; 0\"     audit:       load_timestamp: true</p> Source code in <code>odibi\\patterns\\aggregation.py</code> <pre><code>class AggregationPattern(Pattern):\n    \"\"\"\n    Aggregation Pattern: Declarative aggregation with time-grain rollups.\n\n    Features:\n    - Declare grain (GROUP BY columns)\n    - Declare measures with aggregation functions\n    - Incremental aggregation (merge new data with existing)\n    - Time rollups (generate multiple grain levels)\n    - Audit columns\n\n    Configuration Options (via params dict):\n        - **grain** (list): Columns to GROUP BY (defines uniqueness)\n        - **measures** (list): Measure definitions with name and aggregation expr\n            - name: Output column name\n            - expr: SQL aggregation expression (e.g., \"SUM(amount)\")\n        - **incremental** (dict): Incremental merge configuration (optional)\n            - timestamp_column: Column to identify new data\n            - merge_strategy: \"replace\", \"sum\", \"min\", or \"max\"\n        - **having** (str): Optional HAVING clause for filtering aggregates\n        - **audit** (dict): Audit column configuration\n\n    Example Config:\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(total_amount)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: avg_order_value\n                expr: \"AVG(total_amount)\"\n            having: \"COUNT(*) &gt; 0\"\n            audit:\n              load_timestamp: true\n    \"\"\"\n\n    def validate(self) -&gt; None:\n        ctx = get_logging_context()\n        grain = self.params.get(\"grain\")\n        measures = self.params.get(\"measures\", [])\n\n        ctx.debug(\n            \"AggregationPattern validation starting\",\n            pattern=\"AggregationPattern\",\n            grain=grain,\n            measures_count=len(measures),\n        )\n\n        if not grain:\n            ctx.error(\n                \"AggregationPattern validation failed: 'grain' is required\",\n                pattern=\"AggregationPattern\",\n            )\n            raise ValueError(\n                \"AggregationPattern: 'grain' parameter is required. \"\n                \"Grain defines the grouping columns for aggregation (e.g., ['date', 'region']). \"\n                \"Provide a list of column names to group by.\"\n            )\n\n        if not measures:\n            ctx.error(\n                \"AggregationPattern validation failed: 'measures' is required\",\n                pattern=\"AggregationPattern\",\n            )\n            raise ValueError(\n                \"AggregationPattern: 'measures' parameter is required. \"\n                \"Measures define the aggregations to compute (e.g., [{'name': 'total_sales', 'expr': 'sum(amount)'}]). \"\n                \"Provide a list of dicts, each with 'name' and 'expr' keys.\"\n            )\n\n        for i, measure in enumerate(measures):\n            if not isinstance(measure, dict):\n                ctx.error(\n                    f\"AggregationPattern validation failed: measure[{i}] must be a dict\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    f\"AggregationPattern: measure[{i}] must be a dict with 'name' and 'expr'. \"\n                    f\"Got {type(measure).__name__}: {measure!r}. \"\n                    \"Example: {'name': 'total_sales', 'expr': 'sum(amount)'}\"\n                )\n            if \"name\" not in measure:\n                ctx.error(\n                    f\"AggregationPattern validation failed: measure[{i}] missing 'name'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    f\"AggregationPattern: measure[{i}] missing 'name'. \"\n                    f\"Got: {measure!r}. Add a 'name' key for the output column name.\"\n                )\n            if \"expr\" not in measure:\n                ctx.error(\n                    f\"AggregationPattern validation failed: measure[{i}] missing 'expr'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    f\"AggregationPattern: measure[{i}] missing 'expr'. \"\n                    f\"Got: {measure!r}. Add an 'expr' key with the aggregation expression (e.g., 'sum(amount)').\"\n                )\n\n        incremental = self.params.get(\"incremental\")\n        if incremental:\n            if \"timestamp_column\" not in incremental:\n                ctx.error(\n                    \"AggregationPattern validation failed: incremental missing 'timestamp_column'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    \"AggregationPattern: incremental config requires 'timestamp_column'. \"\n                    f\"Got: {incremental!r}. \"\n                    \"Add 'timestamp_column' to specify which column tracks record timestamps.\"\n                )\n            merge_strategy = incremental.get(\"merge_strategy\", \"replace\")\n            if merge_strategy not in (\"replace\", \"sum\", \"min\", \"max\"):\n                ctx.error(\n                    f\"AggregationPattern validation failed: invalid merge_strategy '{merge_strategy}'\",\n                    pattern=\"AggregationPattern\",\n                )\n                raise ValueError(\n                    f\"AggregationPattern: 'merge_strategy' must be 'replace', 'sum', 'min', or 'max'. \"\n                    f\"Got: {merge_strategy}\"\n                )\n\n        ctx.debug(\n            \"AggregationPattern validation passed\",\n            pattern=\"AggregationPattern\",\n        )\n\n    def execute(self, context: EngineContext) -&gt; Any:\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        grain = self.params.get(\"grain\")\n        measures = self.params.get(\"measures\", [])\n        having = self.params.get(\"having\")\n        incremental = self.params.get(\"incremental\")\n        audit_config = self.params.get(\"audit\", {})\n        target = self.params.get(\"target\")\n\n        ctx.debug(\n            \"AggregationPattern starting\",\n            pattern=\"AggregationPattern\",\n            grain=grain,\n            measures_count=len(measures),\n            incremental=incremental is not None,\n        )\n\n        df = context.df\n        source_count = self._get_row_count(df, context.engine_type)\n        ctx.debug(\n            \"Aggregation source loaded\",\n            pattern=\"AggregationPattern\",\n            source_rows=source_count,\n        )\n\n        try:\n            result_df = self._aggregate(context, df, grain, measures, having)\n\n            if incremental and target:\n                result_df = self._apply_incremental(\n                    context, result_df, grain, measures, incremental, target\n                )\n\n            result_df = self._add_audit_columns(context, result_df, audit_config)\n\n            result_count = self._get_row_count(result_df, context.engine_type)\n            elapsed_ms = (time.time() - start_time) * 1000\n\n            ctx.info(\n                \"AggregationPattern completed\",\n                pattern=\"AggregationPattern\",\n                elapsed_ms=round(elapsed_ms, 2),\n                source_rows=source_count,\n                result_rows=result_count,\n                grain=grain,\n            )\n\n            return result_df\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"AggregationPattern failed: {e}\",\n                pattern=\"AggregationPattern\",\n                error_type=type(e).__name__,\n                elapsed_ms=round(elapsed_ms, 2),\n            )\n            raise\n\n    def _get_row_count(self, df, engine_type) -&gt; Optional[int]:\n        try:\n            if engine_type == EngineType.SPARK:\n                return df.count()\n            else:\n                return len(df)\n        except Exception:\n            return None\n\n    def _aggregate(\n        self,\n        context: EngineContext,\n        df,\n        grain: List[str],\n        measures: List[Dict],\n        having: Optional[str],\n    ):\n        \"\"\"Perform the aggregation using SQL.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._aggregate_spark(context, df, grain, measures, having)\n        else:\n            return self._aggregate_pandas(context, df, grain, measures, having)\n\n    def _aggregate_spark(\n        self,\n        context: EngineContext,\n        df,\n        grain: List[str],\n        measures: List[Dict],\n        having: Optional[str],\n    ):\n        \"\"\"Aggregate using Spark SQL.\"\"\"\n        from pyspark.sql import functions as F\n\n        grain_cols = [F.col(c) for c in grain]\n\n        agg_exprs = []\n        for measure in measures:\n            name = measure[\"name\"]\n            expr = measure[\"expr\"]\n            agg_exprs.append(F.expr(expr).alias(name))\n\n        result = df.groupBy(*grain_cols).agg(*agg_exprs)\n\n        if having:\n            result = result.filter(F.expr(having))\n\n        return result\n\n    def _aggregate_pandas(\n        self,\n        context: EngineContext,\n        df,\n        grain: List[str],\n        measures: List[Dict],\n        having: Optional[str],\n    ):\n        \"\"\"Aggregate using DuckDB SQL via context.sql().\"\"\"\n        grain_str = \", \".join(grain)\n\n        measure_exprs = []\n        for measure in measures:\n            name = measure[\"name\"]\n            expr = measure[\"expr\"]\n            measure_exprs.append(f\"{expr} AS {name}\")\n        measures_str = \", \".join(measure_exprs)\n\n        sql = f\"SELECT {grain_str}, {measures_str} FROM df GROUP BY {grain_str}\"\n\n        if having:\n            sql += f\" HAVING {having}\"\n\n        temp_context = context.with_df(df)\n        result_context = temp_context.sql(sql)\n        return result_context.df\n\n    def _apply_incremental(\n        self,\n        context: EngineContext,\n        new_agg_df,\n        grain: List[str],\n        measures: List[Dict],\n        incremental: Dict,\n        target: str,\n    ):\n        \"\"\"Apply incremental merge with existing aggregations.\"\"\"\n        merge_strategy = incremental.get(\"merge_strategy\", \"replace\")\n\n        existing_df = self._load_existing_target(context, target)\n        if existing_df is None:\n            return new_agg_df\n\n        if merge_strategy == \"replace\":\n            return self._merge_replace(context, existing_df, new_agg_df, grain)\n        elif merge_strategy == \"sum\":\n            return self._merge_sum(context, existing_df, new_agg_df, grain, measures)\n        elif merge_strategy == \"min\":\n            return self._merge_min(context, existing_df, new_agg_df, grain, measures)\n        else:  # max\n            return self._merge_max(context, existing_df, new_agg_df, grain, measures)\n\n    def _load_existing_target(self, context: EngineContext, target: str):\n        \"\"\"Load existing target table if it exists.\"\"\"\n        if context.engine_type == EngineType.SPARK:\n            return self._load_existing_spark(context, target)\n        else:\n            return self._load_existing_pandas(context, target)\n\n    def _load_existing_spark(self, context: EngineContext, target: str):\n        spark = context.spark\n        try:\n            return spark.table(target)\n        except Exception:\n            try:\n                return spark.read.format(\"delta\").load(target)\n            except Exception:\n                return None\n\n    def _load_existing_pandas(self, context: EngineContext, target: str):\n        import os\n\n        import pandas as pd\n\n        path = target\n        if hasattr(context, \"engine\") and context.engine:\n            if \".\" in path:\n                parts = path.split(\".\", 1)\n                conn_name = parts[0]\n                rel_path = parts[1]\n                if conn_name in context.engine.connections:\n                    try:\n                        path = context.engine.connections[conn_name].get_path(rel_path)\n                    except Exception:\n                        pass\n\n        if not os.path.exists(path):\n            return None\n\n        try:\n            if str(path).endswith(\".parquet\") or os.path.isdir(path):\n                return pd.read_parquet(path)\n            elif str(path).endswith(\".csv\"):\n                return pd.read_csv(path)\n        except Exception:\n            return None\n\n        return None\n\n    def _merge_replace(self, context: EngineContext, existing_df, new_df, grain: List[str]):\n        \"\"\"\n        Replace strategy: New aggregates overwrite existing for matching grain keys.\n        \"\"\"\n        if context.engine_type == EngineType.SPARK:\n            new_keys = new_df.select(grain).distinct()\n\n            unchanged = existing_df.join(new_keys, on=grain, how=\"left_anti\")\n\n            return unchanged.unionByName(new_df, allowMissingColumns=True)\n        else:\n            import pandas as pd\n\n            new_keys = new_df[grain].drop_duplicates()\n\n            merged = pd.merge(existing_df, new_keys, on=grain, how=\"left\", indicator=True)\n            unchanged = merged[merged[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])\n\n            return pd.concat([unchanged, new_df], ignore_index=True)\n\n    def _merge_sum(\n        self,\n        context: EngineContext,\n        existing_df,\n        new_df,\n        grain: List[str],\n        measures: List[Dict],\n    ):\n        \"\"\"\n        Sum strategy: Add new measure values to existing for matching grain keys.\n        \"\"\"\n        measure_names = [m[\"name\"] for m in measures]\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            joined = existing_df.alias(\"e\").join(new_df.alias(\"n\"), on=grain, how=\"full_outer\")\n\n            select_cols = []\n            for col in grain:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            for name in measure_names:\n                select_cols.append(\n                    (\n                        F.coalesce(F.col(f\"e.{name}\"), F.lit(0))\n                        + F.coalesce(F.col(f\"n.{name}\"), F.lit(0))\n                    ).alias(name)\n                )\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            return joined.select(select_cols)\n        else:\n            import pandas as pd\n\n            merged = pd.merge(existing_df, new_df, on=grain, how=\"outer\", suffixes=(\"_e\", \"_n\"))\n\n            result = merged[grain].copy()\n\n            for name in measure_names:\n                e_col = f\"{name}_e\" if f\"{name}_e\" in merged.columns else name\n                n_col = f\"{name}_n\" if f\"{name}_n\" in merged.columns else name\n\n                if e_col in merged.columns and n_col in merged.columns:\n                    result[name] = merged[e_col].fillna(0).infer_objects(copy=False) + merged[\n                        n_col\n                    ].fillna(0).infer_objects(copy=False)\n                elif e_col in merged.columns:\n                    result[name] = merged[e_col].fillna(0).infer_objects(copy=False)\n                elif n_col in merged.columns:\n                    result[name] = merged[n_col].fillna(0).infer_objects(copy=False)\n                else:\n                    result[name] = 0\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                e_col = f\"{col}_e\" if f\"{col}_e\" in merged.columns else col\n                n_col = f\"{col}_n\" if f\"{col}_n\" in merged.columns else col\n                if e_col in merged.columns:\n                    result[col] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[col] = merged[n_col]\n\n            return result\n\n    def _merge_min(\n        self,\n        context: EngineContext,\n        existing_df,\n        new_df,\n        grain: List[str],\n        measures: List[Dict],\n    ):\n        \"\"\"\n        Min strategy: Keep the minimum value for each measure across existing and new.\n        \"\"\"\n        measure_names = [m[\"name\"] for m in measures]\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            joined = existing_df.alias(\"e\").join(new_df.alias(\"n\"), on=grain, how=\"full_outer\")\n\n            select_cols = []\n            for col in grain:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            for name in measure_names:\n                select_cols.append(\n                    F.least(\n                        F.coalesce(F.col(f\"e.{name}\"), F.col(f\"n.{name}\")),\n                        F.coalesce(F.col(f\"n.{name}\"), F.col(f\"e.{name}\")),\n                    ).alias(name)\n                )\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            return joined.select(select_cols)\n        else:\n            import pandas as pd\n\n            merged = pd.merge(existing_df, new_df, on=grain, how=\"outer\", suffixes=(\"_e\", \"_n\"))\n\n            result = merged[grain].copy()\n\n            for name in measure_names:\n                e_col = f\"{name}_e\" if f\"{name}_e\" in merged.columns else name\n                n_col = f\"{name}_n\" if f\"{name}_n\" in merged.columns else name\n\n                if e_col in merged.columns and n_col in merged.columns:\n                    result[name] = merged[[e_col, n_col]].min(axis=1)\n                elif e_col in merged.columns:\n                    result[name] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[name] = merged[n_col]\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                e_col = f\"{col}_e\" if f\"{col}_e\" in merged.columns else col\n                n_col = f\"{col}_n\" if f\"{col}_n\" in merged.columns else col\n                if e_col in merged.columns:\n                    result[col] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[col] = merged[n_col]\n\n            return result\n\n    def _merge_max(\n        self,\n        context: EngineContext,\n        existing_df,\n        new_df,\n        grain: List[str],\n        measures: List[Dict],\n    ):\n        \"\"\"\n        Max strategy: Keep the maximum value for each measure across existing and new.\n        \"\"\"\n        measure_names = [m[\"name\"] for m in measures]\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            joined = existing_df.alias(\"e\").join(new_df.alias(\"n\"), on=grain, how=\"full_outer\")\n\n            select_cols = []\n            for col in grain:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            for name in measure_names:\n                select_cols.append(\n                    F.greatest(\n                        F.coalesce(F.col(f\"e.{name}\"), F.col(f\"n.{name}\")),\n                        F.coalesce(F.col(f\"n.{name}\"), F.col(f\"e.{name}\")),\n                    ).alias(name)\n                )\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                select_cols.append(F.coalesce(F.col(f\"e.{col}\"), F.col(f\"n.{col}\")).alias(col))\n\n            return joined.select(select_cols)\n        else:\n            import pandas as pd\n\n            merged = pd.merge(existing_df, new_df, on=grain, how=\"outer\", suffixes=(\"_e\", \"_n\"))\n\n            result = merged[grain].copy()\n\n            for name in measure_names:\n                e_col = f\"{name}_e\" if f\"{name}_e\" in merged.columns else name\n                n_col = f\"{name}_n\" if f\"{name}_n\" in merged.columns else name\n\n                if e_col in merged.columns and n_col in merged.columns:\n                    result[name] = merged[[e_col, n_col]].max(axis=1)\n                elif e_col in merged.columns:\n                    result[name] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[name] = merged[n_col]\n\n            other_cols = [\n                c for c in existing_df.columns if c not in grain and c not in measure_names\n            ]\n            for col in other_cols:\n                e_col = f\"{col}_e\" if f\"{col}_e\" in merged.columns else col\n                n_col = f\"{col}_n\" if f\"{col}_n\" in merged.columns else col\n                if e_col in merged.columns:\n                    result[col] = merged[e_col]\n                elif n_col in merged.columns:\n                    result[col] = merged[n_col]\n\n            return result\n\n    def _add_audit_columns(self, context: EngineContext, df, audit_config: Dict):\n        \"\"\"Add audit columns (load_timestamp, source_system).\"\"\"\n        load_timestamp = audit_config.get(\"load_timestamp\", False)\n        source_system = audit_config.get(\"source_system\")\n\n        if context.engine_type == EngineType.SPARK:\n            from pyspark.sql import functions as F\n\n            if load_timestamp:\n                df = df.withColumn(\"load_timestamp\", F.current_timestamp())\n            if source_system:\n                df = df.withColumn(\"source_system\", F.lit(source_system))\n        else:\n            if load_timestamp or source_system:\n                df = df.copy()\n            if load_timestamp:\n                # Use timezone-aware timestamp for Delta Lake compatibility\n                from datetime import timezone\n\n                df[\"load_timestamp\"] = datetime.now(timezone.utc)\n            if source_system:\n                df[\"source_system\"] = source_system\n\n        return df\n</code></pre>"},{"location":"reference/api/pipeline/","title":"Pipeline API","text":""},{"location":"reference/api/pipeline/#odibi.pipeline","title":"<code>odibi.pipeline</code>","text":"<p>Pipeline executor and orchestration.</p>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager","title":"<code>PipelineManager</code>","text":"<p>Manages multiple pipelines from a YAML configuration.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>class PipelineManager:\n    \"\"\"Manages multiple pipelines from a YAML configuration.\"\"\"\n\n    def __init__(\n        self,\n        project_config: ProjectConfig,\n        connections: Dict[str, Any],\n    ):\n        \"\"\"Initialize pipeline manager.\n\n        Args:\n            project_config: Validated project configuration\n            connections: Connection objects (already instantiated)\n        \"\"\"\n        self.project_config = project_config\n        self.connections = connections\n        self._pipelines: Dict[str, Pipeline] = {}\n        self.catalog_manager = None\n        self.lineage_adapter = None\n\n        # Configure logging\n        configure_logging(\n            structured=project_config.logging.structured, level=project_config.logging.level.value\n        )\n\n        # Create manager-level logging context\n        self._ctx = create_logging_context(engine=project_config.engine)\n\n        self._ctx.info(\n            \"Initializing PipelineManager\",\n            project=project_config.project,\n            engine=project_config.engine,\n            pipeline_count=len(project_config.pipelines),\n            connection_count=len(connections),\n        )\n\n        # Initialize Lineage Adapter\n        self.lineage_adapter = OpenLineageAdapter(project_config.lineage)\n\n        # Initialize CatalogManager if configured\n        if project_config.system:\n            from odibi.catalog import CatalogManager\n\n            spark = None\n            engine_instance = None\n\n            if project_config.engine == \"spark\":\n                try:\n                    from odibi.engine.spark_engine import SparkEngine\n\n                    temp_engine = SparkEngine(connections=connections, config={})\n                    spark = temp_engine.spark\n                    self._ctx.debug(\"Spark session initialized for System Catalog\")\n                except Exception as e:\n                    self._ctx.warning(\n                        f\"Failed to initialize Spark for System Catalog: {e}\",\n                        suggestion=\"Check Spark configuration\",\n                    )\n\n            sys_conn = connections.get(project_config.system.connection)\n            if sys_conn:\n                base_path = sys_conn.get_path(project_config.system.path)\n\n                if not spark:\n                    try:\n                        from odibi.engine.pandas_engine import PandasEngine\n\n                        engine_instance = PandasEngine(config={})\n                        self._ctx.debug(\"PandasEngine initialized for System Catalog\")\n                    except Exception as e:\n                        self._ctx.warning(\n                            f\"Failed to initialize PandasEngine for System Catalog: {e}\"\n                        )\n\n                if spark or engine_instance:\n                    self.catalog_manager = CatalogManager(\n                        spark=spark,\n                        config=project_config.system,\n                        base_path=base_path,\n                        engine=engine_instance,\n                        connection=sys_conn,\n                    )\n                    # Set project name for tagging all catalog records\n                    self.catalog_manager.project = project_config.project\n\n                    # Skip bootstrap if catalog writes are disabled\n                    skip_catalog = (\n                        getattr(project_config.performance, \"skip_catalog_writes\", False)\n                        if project_config.performance\n                        else False\n                    )\n                    if not skip_catalog:\n                        self.catalog_manager.bootstrap()\n                        self._ctx.info(\n                            \"System Catalog initialized\",\n                            path=base_path,\n                            project=project_config.project,\n                        )\n                    else:\n                        self._ctx.debug(\n                            \"System Catalog bootstrap skipped (skip_catalog_writes=true)\"\n                        )\n            else:\n                self._ctx.warning(\n                    f\"System connection '{project_config.system.connection}' not found\",\n                    suggestion=\"Configure the system connection in your config\",\n                )\n\n        # Get story configuration\n        story_config = self._get_story_config()\n\n        # Create all pipeline instances\n        self._ctx.debug(\n            \"Creating pipeline instances\",\n            pipelines=[p.pipeline for p in project_config.pipelines],\n        )\n        for pipeline_config in project_config.pipelines:\n            pipeline_name = pipeline_config.pipeline\n\n            self._pipelines[pipeline_name] = Pipeline(\n                pipeline_config=pipeline_config,\n                engine=project_config.engine,\n                connections=connections,\n                generate_story=story_config.get(\"auto_generate\", True),\n                story_config=story_config,\n                retry_config=project_config.retry,\n                alerts=project_config.alerts,\n                performance_config=project_config.performance,\n                catalog_manager=self.catalog_manager,\n                lineage_adapter=self.lineage_adapter,\n            )\n            self._pipelines[pipeline_name].project_config = project_config\n\n        self._ctx.info(\n            \"PipelineManager ready\",\n            pipelines=list(self._pipelines.keys()),\n        )\n\n    def _get_story_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Build story config from project_config.story.\n\n        Resolves story output path using connection.\n\n        Returns:\n            Dictionary for StoryGenerator initialization\n        \"\"\"\n        story_cfg = self.project_config.story\n\n        # Resolve story path using connection\n        story_conn = self.connections[story_cfg.connection]\n        output_path = story_conn.get_path(story_cfg.path)\n\n        # Get storage options (e.g., credentials) from connection if available\n        storage_options = {}\n        if hasattr(story_conn, \"pandas_storage_options\"):\n            storage_options = story_conn.pandas_storage_options()\n\n        # Build docs config dict if present\n        docs_config = None\n        if story_cfg.docs:\n            docs_config = story_cfg.docs.model_dump()\n\n        return {\n            \"auto_generate\": story_cfg.auto_generate,\n            \"max_sample_rows\": story_cfg.max_sample_rows,\n            \"output_path\": output_path,\n            \"storage_options\": storage_options,\n            \"async_generation\": story_cfg.async_generation,\n            \"docs\": docs_config,\n        }\n\n    @classmethod\n    def from_yaml(cls, yaml_path: str, env: str = None) -&gt; \"PipelineManager\":\n        \"\"\"Create PipelineManager from YAML file.\n\n        Args:\n            yaml_path: Path to YAML configuration file\n            env: Environment name to apply overrides (e.g. 'prod')\n\n        Returns:\n            PipelineManager instance ready to run pipelines\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n            &gt;&gt;&gt; results = manager.run()  # Run all pipelines\n        \"\"\"\n        logger.info(f\"Loading configuration from: {yaml_path}\")\n\n        register_standard_library()\n\n        yaml_path_obj = Path(yaml_path)\n        config_dir = yaml_path_obj.parent.absolute()\n\n        import importlib.util\n        import os\n        import sys\n\n        # Load .env file from config directory if it exists\n        env_file = config_dir / \".env\"\n        if env_file.exists():\n            try:\n                from dotenv import load_dotenv\n\n                load_dotenv(env_file, override=True)\n                logger.debug(f\"Loaded environment from: {env_file}\")\n            except ImportError:\n                logger.warning(\"python-dotenv not installed, skipping .env file\")\n\n        def load_transforms_module(path):\n            if os.path.exists(path):\n                try:\n                    spec = importlib.util.spec_from_file_location(\"transforms_autodiscovered\", path)\n                    if spec and spec.loader:\n                        module = importlib.util.module_from_spec(spec)\n                        sys.modules[\"transforms_autodiscovered\"] = module\n                        spec.loader.exec_module(module)\n                        logger.info(f\"Auto-loaded transforms from: {path}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to auto-load transforms from {path}: {e}\")\n\n        load_transforms_module(os.path.join(config_dir, \"transforms.py\"))\n\n        cwd = os.getcwd()\n        if os.path.abspath(cwd) != str(config_dir):\n            load_transforms_module(os.path.join(cwd, \"transforms.py\"))\n\n        try:\n            config = load_yaml_with_env(str(yaml_path_obj), env=env)\n            logger.debug(\"Configuration loaded successfully\")\n        except FileNotFoundError:\n            logger.error(f\"YAML file not found: {yaml_path}\")\n            raise FileNotFoundError(\n                f\"YAML file not found: {yaml_path}. \"\n                f\"Verify the file exists and consider using an absolute path.\"\n            )\n\n        project_config = ProjectConfig(**config)\n        logger.debug(\n            \"Project config validated\",\n            project=project_config.project,\n            pipelines=len(project_config.pipelines),\n        )\n\n        connections = cls._build_connections(project_config.connections)\n\n        return cls(\n            project_config=project_config,\n            connections=connections,\n        )\n\n    @staticmethod\n    def _build_connections(conn_configs: Dict[str, Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Convert connection configs to connection objects.\n\n        Args:\n            conn_configs: Connection configurations from ProjectConfig\n\n        Returns:\n            Dictionary of connection name -&gt; connection object\n\n        Raises:\n            ValueError: If connection type is not supported\n        \"\"\"\n        from odibi.connections.factory import register_builtins\n\n        logger.debug(f\"Building {len(conn_configs)} connections\")\n\n        connections = {}\n\n        register_builtins()\n        load_plugins()\n\n        for conn_name, conn_config in conn_configs.items():\n            if hasattr(conn_config, \"model_dump\"):\n                conn_config = conn_config.model_dump()\n            elif hasattr(conn_config, \"dict\"):\n                conn_config = conn_config.model_dump()\n\n            conn_type = conn_config.get(\"type\", \"local\")\n\n            factory = get_connection_factory(conn_type)\n            if factory:\n                try:\n                    connections[conn_name] = factory(conn_name, conn_config)\n                    logger.debug(\n                        f\"Connection created: {conn_name}\",\n                        type=conn_type,\n                    )\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to create connection '{conn_name}'\",\n                        type=conn_type,\n                        error=str(e),\n                    )\n                    raise ValueError(\n                        f\"Failed to create connection '{conn_name}' (type={conn_type}): {e}\"\n                    ) from e\n            else:\n                logger.error(\n                    f\"Unsupported connection type: {conn_type}\",\n                    connection=conn_name,\n                    suggestion=\"Check supported connection types in docs\",\n                )\n                raise ValueError(\n                    f\"Unsupported connection type: {conn_type}. \"\n                    f\"Supported types: local, azure_adls, azure_sql, delta, etc. \"\n                    f\"See docs for connection setup.\"\n                )\n\n        try:\n            from odibi.utils import configure_connections_parallel\n\n            connections, errors = configure_connections_parallel(connections, verbose=False)\n            if errors:\n                for error in errors:\n                    logger.warning(error)\n        except ImportError:\n            pass\n\n        logger.info(f\"Built {len(connections)} connections successfully\")\n\n        return connections\n\n    def register_outputs(\n        self,\n        pipelines: Optional[Union[str, List[str]]] = None,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        Pre-register node outputs from pipeline configs without running them.\n\n        Scans pipeline nodes for output locations (write blocks, merge/scd2 params)\n        and registers them to meta_outputs. This enables cross-pipeline references\n        without requiring the source pipelines to have run first.\n\n        Args:\n            pipelines: Pipeline name(s) to register. If None, registers all pipelines.\n\n        Returns:\n            Dict mapping pipeline name to number of outputs registered\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"pipelines.yaml\")\n            &gt;&gt;&gt; counts = manager.register_outputs(\"silver\")  # Register just silver\n            &gt;&gt;&gt; counts = manager.register_outputs()  # Register all pipelines\n        \"\"\"\n        if pipelines is None:\n            pipeline_names = list(self._pipelines.keys())\n        elif isinstance(pipelines, str):\n            pipeline_names = [pipelines]\n        else:\n            pipeline_names = pipelines\n\n        results = {}\n        for name in pipeline_names:\n            if name not in self._pipelines:\n                self._ctx.warning(f\"Pipeline not found: {name}\")\n                continue\n\n            pipeline = self._pipelines[name]\n            count = pipeline.register_outputs()\n            results[name] = count\n\n        total = sum(results.values())\n        self._ctx.info(f\"Pre-registered {total} outputs from {len(results)} pipelines\")\n        return results\n\n    def run(\n        self,\n        pipelines: Optional[Union[str, List[str]]] = None,\n        dry_run: bool = False,\n        resume_from_failure: bool = False,\n        parallel: bool = False,\n        max_workers: int = 4,\n        on_error: Optional[str] = None,\n        tag: Optional[str] = None,\n        node: Optional[Union[str, List[str]]] = None,\n        console: bool = False,\n    ) -&gt; Union[PipelineResults, Dict[str, PipelineResults]]:\n        \"\"\"Run one, multiple, or all pipelines.\n\n        Args:\n            pipelines: Pipeline name(s) to run.\n            dry_run: Whether to simulate execution.\n            resume_from_failure: Whether to skip successfully completed nodes from last run.\n            parallel: Whether to run nodes in parallel.\n            max_workers: Maximum number of worker threads for parallel execution.\n            on_error: Override error handling strategy (fail_fast, fail_later, ignore).\n            tag: Filter nodes by tag (only nodes with this tag will run).\n            node: Run only specific node(s) by name - can be a string or list of strings.\n            console: Whether to show rich console output with progress.\n\n        Returns:\n            PipelineResults or Dict of results\n        \"\"\"\n        if pipelines is None:\n            pipeline_names = list(self._pipelines.keys())\n        elif isinstance(pipelines, str):\n            pipeline_names = [pipelines]\n        else:\n            pipeline_names = pipelines\n\n        for name in pipeline_names:\n            if name not in self._pipelines:\n                available = \", \".join(self._pipelines.keys())\n                self._ctx.error(\n                    f\"Pipeline not found: {name}\",\n                    available=list(self._pipelines.keys()),\n                )\n                raise ValueError(f\"Pipeline '{name}' not found. Available pipelines: {available}\")\n\n        # Phase 2: Auto-register pipelines and nodes before execution\n        if self.catalog_manager:\n            self._auto_register_pipelines(pipeline_names)\n\n        self._ctx.info(\n            f\"Running {len(pipeline_names)} pipeline(s)\",\n            pipelines=pipeline_names,\n            dry_run=dry_run,\n            parallel=parallel,\n        )\n\n        results = {}\n        for idx, name in enumerate(pipeline_names):\n            # Invalidate cache before each pipeline so it sees latest outputs\n            if self.catalog_manager:\n                self.catalog_manager.invalidate_cache()\n\n            self._ctx.info(\n                f\"Executing pipeline {idx + 1}/{len(pipeline_names)}: {name}\",\n                pipeline=name,\n                order=idx + 1,\n            )\n\n            results[name] = self._pipelines[name].run(\n                dry_run=dry_run,\n                resume_from_failure=resume_from_failure,\n                parallel=parallel,\n                max_workers=max_workers,\n                on_error=on_error,\n                tag=tag,\n                node=node,\n                console=console,\n            )\n\n            result = results[name]\n            status = \"SUCCESS\" if not result.failed else \"FAILED\"\n            self._ctx.info(\n                f\"Pipeline {status}: {name}\",\n                status=status,\n                duration_s=round(result.duration, 2),\n                completed=len(result.completed),\n                failed=len(result.failed),\n            )\n\n            if result.story_path:\n                self._ctx.debug(f\"Story generated: {result.story_path}\")\n\n        # Generate combined lineage if configured\n        has_story = hasattr(self.project_config, \"story\") and self.project_config.story\n        generate_lineage_enabled = has_story and self.project_config.story.generate_lineage\n\n        self._ctx.debug(\n            \"Lineage check\",\n            has_story=has_story,\n            generate_lineage_enabled=generate_lineage_enabled,\n        )\n\n        if generate_lineage_enabled:\n            # Flush any pending async story writes before generating lineage\n            self._ctx.info(\"Generating combined lineage...\")\n            self.flush_stories()\n\n            try:\n                lineage_result = generate_lineage(self.project_config)\n                if lineage_result:\n                    self._ctx.info(\n                        \"Combined lineage generated\",\n                        nodes=len(lineage_result.nodes),\n                        edges=len(lineage_result.edges),\n                        json_path=lineage_result.json_path,\n                    )\n                else:\n                    self._ctx.warning(\"Lineage generation returned None\")\n            except Exception as e:\n                self._ctx.warning(f\"Failed to generate combined lineage: {e}\")\n\n        if len(pipeline_names) == 1:\n            return results[pipeline_names[0]]\n        else:\n            return results\n\n    def list_pipelines(self) -&gt; List[str]:\n        \"\"\"Get list of available pipeline names.\n\n        Returns:\n            List of pipeline names\n        \"\"\"\n        return list(self._pipelines.keys())\n\n    def flush_stories(self, timeout: float = 60.0) -&gt; Dict[str, Optional[str]]:\n        \"\"\"Wait for all pending async story generation to complete.\n\n        Call this before operations that need story files to be written,\n        such as lineage generation with SemanticLayerRunner.\n\n        Args:\n            timeout: Maximum seconds to wait per pipeline\n\n        Returns:\n            Dict mapping pipeline name to story path (or None if no pending story)\n\n        Example:\n            &gt;&gt;&gt; manager.run(pipelines=['bronze', 'silver', 'gold'])\n            &gt;&gt;&gt; manager.flush_stories()  # Wait for all stories to be written\n            &gt;&gt;&gt; semantic_runner.run()    # Now lineage can read the stories\n        \"\"\"\n        results = {}\n        for name, pipeline in self._pipelines.items():\n            story_path = pipeline.flush_stories(timeout=timeout)\n            if story_path:\n                results[name] = story_path\n                self._ctx.debug(f\"Story flushed for {name}\", path=story_path)\n        if results:\n            self._ctx.info(f\"Flushed {len(results)} pending story writes\")\n        return results\n\n    def get_pipeline(self, name: str) -&gt; Pipeline:\n        \"\"\"Get a specific pipeline instance.\n\n        Args:\n            name: Pipeline name\n\n        Returns:\n            Pipeline instance\n\n        Raises:\n            ValueError: If pipeline not found\n        \"\"\"\n        if name not in self._pipelines:\n            available = \", \".join(self._pipelines.keys())\n            raise ValueError(f\"Pipeline '{name}' not found. Available: {available}\")\n        return self._pipelines[name]\n\n    def deploy(self, pipelines: Optional[Union[str, List[str]]] = None) -&gt; bool:\n        \"\"\"Deploy pipeline definitions to the System Catalog.\n\n        This registers pipeline and node configurations in the catalog,\n        enabling drift detection and governance features.\n\n        Args:\n            pipelines: Optional pipeline name(s) to deploy. If None, deploys all.\n\n        Returns:\n            True if deployment succeeded, False otherwise.\n\n        Example:\n            &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"odibi.yaml\")\n            &gt;&gt;&gt; manager.deploy()  # Deploy all pipelines\n            &gt;&gt;&gt; manager.deploy(\"sales_daily\")  # Deploy specific pipeline\n        \"\"\"\n        if not self.catalog_manager:\n            self._ctx.warning(\n                \"System Catalog not configured. Cannot deploy.\",\n                suggestion=\"Configure system catalog in your YAML config\",\n            )\n            return False\n\n        if pipelines is None:\n            to_deploy = self.project_config.pipelines\n        elif isinstance(pipelines, str):\n            to_deploy = [p for p in self.project_config.pipelines if p.pipeline == pipelines]\n        else:\n            to_deploy = [p for p in self.project_config.pipelines if p.pipeline in pipelines]\n\n        if not to_deploy:\n            self._ctx.warning(\"No matching pipelines found to deploy.\")\n            return False\n\n        self._ctx.info(\n            f\"Deploying {len(to_deploy)} pipeline(s) to System Catalog\",\n            pipelines=[p.pipeline for p in to_deploy],\n        )\n\n        try:\n            # Skip bootstrap if catalog writes are disabled\n            skip_catalog = (\n                getattr(self.project_config.performance, \"skip_catalog_writes\", False)\n                if self.project_config.performance\n                else False\n            )\n            if not skip_catalog:\n                self.catalog_manager.bootstrap()\n\n            for pipeline_config in to_deploy:\n                self._ctx.debug(\n                    f\"Deploying pipeline: {pipeline_config.pipeline}\",\n                    node_count=len(pipeline_config.nodes),\n                )\n                self.catalog_manager.register_pipeline(pipeline_config, self.project_config)\n\n                for node in pipeline_config.nodes:\n                    self.catalog_manager.register_node(pipeline_config.pipeline, node)\n\n            self._ctx.info(\n                f\"Deployment complete: {len(to_deploy)} pipeline(s)\",\n                deployed=[p.pipeline for p in to_deploy],\n            )\n            return True\n\n        except Exception as e:\n            self._ctx.error(\n                f\"Deployment failed: {e}\",\n                error_type=type(e).__name__,\n                suggestion=\"Check catalog configuration and permissions\",\n            )\n            return False\n\n    def _auto_register_pipelines(self, pipeline_names: List[str]) -&gt; None:\n        \"\"\"Auto-register pipelines and nodes before execution.\n\n        This ensures meta_pipelines and meta_nodes are populated automatically\n        when running pipelines, without requiring explicit deploy() calls.\n\n        Uses \"check-before-write\" pattern with batch writes for performance:\n        - Reads existing hashes in one read\n        - Compares version_hash to skip unchanged records\n        - Batch writes only changed/new records\n\n        Args:\n            pipeline_names: List of pipeline names to register\n        \"\"\"\n        if not self.catalog_manager:\n            return\n\n        try:\n            import hashlib\n            import json\n\n            existing_pipelines = self.catalog_manager.get_all_registered_pipelines()\n            existing_nodes = self.catalog_manager.get_all_registered_nodes(pipeline_names)\n\n            pipeline_records = []\n            node_records = []\n\n            for name in pipeline_names:\n                pipeline = self._pipelines[name]\n                config = pipeline.config\n\n                if hasattr(config, \"model_dump\"):\n                    dump = config.model_dump(mode=\"json\")\n                else:\n                    dump = config.model_dump()\n                dump_str = json.dumps(dump, sort_keys=True)\n                pipeline_hash = hashlib.md5(dump_str.encode(\"utf-8\")).hexdigest()\n\n                if existing_pipelines.get(name) != pipeline_hash:\n                    all_tags = set()\n                    for node in config.nodes:\n                        if node.tags:\n                            all_tags.update(node.tags)\n\n                    pipeline_records.append(\n                        {\n                            \"pipeline_name\": name,\n                            \"version_hash\": pipeline_hash,\n                            \"description\": config.description or \"\",\n                            \"layer\": config.layer or \"\",\n                            \"schedule\": \"\",\n                            \"tags_json\": json.dumps(list(all_tags)),\n                        }\n                    )\n\n                pipeline_existing_nodes = existing_nodes.get(name, {})\n                for node in config.nodes:\n                    if hasattr(node, \"model_dump\"):\n                        node_dump = node.model_dump(\n                            mode=\"json\", exclude={\"description\", \"tags\", \"log_level\"}\n                        )\n                    else:\n                        node_dump = node.model_dump(exclude={\"description\", \"tags\", \"log_level\"})\n                    node_dump_str = json.dumps(node_dump, sort_keys=True)\n                    node_hash = hashlib.md5(node_dump_str.encode(\"utf-8\")).hexdigest()\n\n                    if pipeline_existing_nodes.get(node.name) != node_hash:\n                        node_type = \"transform\"\n                        if node.read:\n                            node_type = \"read\"\n                        if node.write:\n                            node_type = \"write\"\n\n                        node_records.append(\n                            {\n                                \"pipeline_name\": name,\n                                \"node_name\": node.name,\n                                \"version_hash\": node_hash,\n                                \"type\": node_type,\n                                \"config_json\": json.dumps(node_dump),\n                            }\n                        )\n\n            if pipeline_records:\n                self.catalog_manager.register_pipelines_batch(pipeline_records)\n                self._ctx.debug(\n                    f\"Batch registered {len(pipeline_records)} changed pipeline(s)\",\n                    pipelines=[r[\"pipeline_name\"] for r in pipeline_records],\n                )\n            else:\n                self._ctx.debug(\"All pipelines unchanged - skipping registration\")\n\n            if node_records:\n                self.catalog_manager.register_nodes_batch(node_records)\n                self._ctx.debug(\n                    f\"Batch registered {len(node_records)} changed node(s)\",\n                    nodes=[r[\"node_name\"] for r in node_records],\n                )\n            else:\n                self._ctx.debug(\"All nodes unchanged - skipping registration\")\n\n        except Exception as e:\n            self._ctx.warning(\n                f\"Auto-registration failed (non-fatal): {e}\",\n                error_type=type(e).__name__,\n            )\n\n    # -------------------------------------------------------------------------\n    # Phase 5: List/Query Methods\n    # -------------------------------------------------------------------------\n\n    def list_registered_pipelines(self) -&gt; \"pd.DataFrame\":\n        \"\"\"List all registered pipelines from the system catalog.\n\n        Returns:\n            DataFrame with pipeline metadata from meta_pipelines\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(\n                self.catalog_manager.tables[\"meta_pipelines\"]\n            )\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list pipelines: {e}\")\n            return pd.DataFrame()\n\n    def list_registered_nodes(self, pipeline: Optional[str] = None) -&gt; \"pd.DataFrame\":\n        \"\"\"List nodes from the system catalog.\n\n        Args:\n            pipeline: Optional pipeline name to filter by\n\n        Returns:\n            DataFrame with node metadata from meta_nodes\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_nodes\"])\n            if not df.empty and pipeline:\n                df = df[df[\"pipeline_name\"] == pipeline]\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list nodes: {e}\")\n            return pd.DataFrame()\n\n    def list_runs(\n        self,\n        pipeline: Optional[str] = None,\n        node: Optional[str] = None,\n        status: Optional[str] = None,\n        limit: int = 10,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"List recent runs with optional filters.\n\n        Args:\n            pipeline: Optional pipeline name to filter by\n            node: Optional node name to filter by\n            status: Optional status to filter by (SUCCESS, FAILURE)\n            limit: Maximum number of runs to return\n\n        Returns:\n            DataFrame with run history from meta_runs\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n            if df.empty:\n                return df\n\n            if pipeline:\n                df = df[df[\"pipeline_name\"] == pipeline]\n            if node:\n                df = df[df[\"node_name\"] == node]\n            if status:\n                df = df[df[\"status\"] == status]\n\n            if \"timestamp\" in df.columns:\n                df = df.sort_values(\"timestamp\", ascending=False)\n\n            return df.head(limit)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list runs: {e}\")\n            return pd.DataFrame()\n\n    def list_tables(self) -&gt; \"pd.DataFrame\":\n        \"\"\"List registered assets from meta_tables.\n\n        Returns:\n            DataFrame with table/asset metadata\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            self._ctx.warning(\"Catalog manager not configured\")\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to list tables: {e}\")\n            return pd.DataFrame()\n\n    # -------------------------------------------------------------------------\n    # Phase 5.2: State Methods\n    # -------------------------------------------------------------------------\n\n    def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get a specific state entry (HWM, content hash, etc.).\n\n        Args:\n            key: The state key to look up\n\n        Returns:\n            Dictionary with state data or None if not found\n        \"\"\"\n\n        if not self.catalog_manager:\n            return None\n\n        try:\n            df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n            if df.empty or \"key\" not in df.columns:\n                return None\n\n            row = df[df[\"key\"] == key]\n            if row.empty:\n                return None\n\n            return row.iloc[0].to_dict()\n        except Exception:\n            return None\n\n    def get_all_state(self, prefix: Optional[str] = None) -&gt; \"pd.DataFrame\":\n        \"\"\"Get all state entries, optionally filtered by key prefix.\n\n        Args:\n            prefix: Optional key prefix to filter by\n\n        Returns:\n            DataFrame with state entries\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n            if not df.empty and prefix and \"key\" in df.columns:\n                df = df[df[\"key\"].str.startswith(prefix)]\n            return df\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get state: {e}\")\n            return pd.DataFrame()\n\n    def clear_state(self, key: str) -&gt; bool:\n        \"\"\"Remove a state entry.\n\n        Args:\n            key: The state key to remove\n\n        Returns:\n            True if deleted, False otherwise\n        \"\"\"\n        if not self.catalog_manager:\n            return False\n\n        try:\n            return self.catalog_manager.clear_state_key(key)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to clear state: {e}\")\n            return False\n\n    # -------------------------------------------------------------------------\n    # Phase 5.3-5.4: Schema/Lineage and Stats Methods\n    # -------------------------------------------------------------------------\n\n    def get_schema_history(\n        self,\n        table: str,\n        limit: int = 5,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Get schema version history for a table.\n\n        Args:\n            table: Table identifier (supports smart path resolution)\n            limit: Maximum number of versions to return\n\n        Returns:\n            DataFrame with schema history\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            resolved_path = self._resolve_table_path(table)\n            history = self.catalog_manager.get_schema_history(resolved_path, limit)\n            return pd.DataFrame(history)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get schema history: {e}\")\n            return pd.DataFrame()\n\n    def get_lineage(\n        self,\n        table: str,\n        direction: str = \"both\",\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Get lineage for a table.\n\n        Args:\n            table: Table identifier (supports smart path resolution)\n            direction: \"upstream\", \"downstream\", or \"both\"\n\n        Returns:\n            DataFrame with lineage relationships\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return pd.DataFrame()\n\n        try:\n            resolved_path = self._resolve_table_path(table)\n\n            results = []\n            if direction in (\"upstream\", \"both\"):\n                upstream = self.catalog_manager.get_upstream(resolved_path)\n                for r in upstream:\n                    r[\"direction\"] = \"upstream\"\n                results.extend(upstream)\n\n            if direction in (\"downstream\", \"both\"):\n                downstream = self.catalog_manager.get_downstream(resolved_path)\n                for r in downstream:\n                    r[\"direction\"] = \"downstream\"\n                results.extend(downstream)\n\n            return pd.DataFrame(results)\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get lineage: {e}\")\n            return pd.DataFrame()\n\n    def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n        \"\"\"Get last run status, duration, timestamp for a pipeline.\n\n        Args:\n            pipeline: Pipeline name\n\n        Returns:\n            Dict with status info\n        \"\"\"\n        if not self.catalog_manager:\n            return {}\n\n        try:\n            runs = self.list_runs(pipeline=pipeline, limit=1)\n            if runs.empty:\n                return {\"status\": \"never_run\", \"pipeline\": pipeline}\n\n            last_run = runs.iloc[0].to_dict()\n            return {\n                \"pipeline\": pipeline,\n                \"last_status\": last_run.get(\"status\"),\n                \"last_run_at\": last_run.get(\"timestamp\"),\n                \"last_duration_ms\": last_run.get(\"duration_ms\"),\n                \"last_node\": last_run.get(\"node_name\"),\n            }\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get pipeline status: {e}\")\n            return {}\n\n    def get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n        \"\"\"Get average duration, row counts, success rate over period.\n\n        Args:\n            node: Node name\n            days: Number of days to look back\n\n        Returns:\n            Dict with node statistics\n        \"\"\"\n        import pandas as pd\n\n        if not self.catalog_manager:\n            return {}\n\n        try:\n            avg_duration = self.catalog_manager.get_average_duration(node, days)\n\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n            if df.empty:\n                return {\"node\": node, \"runs\": 0}\n\n            if \"timestamp\" in df.columns:\n                cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n                if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n                    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n                if df[\"timestamp\"].dt.tz is None:\n                    df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"UTC\")\n                df = df[df[\"timestamp\"] &gt;= cutoff]\n\n            node_runs = df[df[\"node_name\"] == node]\n            if node_runs.empty:\n                return {\"node\": node, \"runs\": 0}\n\n            total = len(node_runs)\n            success = len(node_runs[node_runs[\"status\"] == \"SUCCESS\"])\n            avg_rows = node_runs[\"rows_processed\"].mean() if \"rows_processed\" in node_runs else None\n\n            return {\n                \"node\": node,\n                \"runs\": total,\n                \"success_rate\": success / total if total &gt; 0 else 0,\n                \"avg_duration_s\": avg_duration,\n                \"avg_rows\": avg_rows,\n                \"period_days\": days,\n            }\n        except Exception as e:\n            self._ctx.warning(f\"Failed to get node stats: {e}\")\n            return {}\n\n    # -------------------------------------------------------------------------\n    # Phase 6: Smart Path Resolution\n    # -------------------------------------------------------------------------\n\n    def _resolve_table_path(self, identifier: str) -&gt; str:\n        \"\"\"Resolve a user-friendly identifier to a full table path.\n\n        Accepts:\n        - Relative path: \"bronze/OEE/vw_OSMPerformanceOEE\"\n        - Registered table: \"test.vw_OSMPerformanceOEE\"\n        - Node name: \"opsvisdata_vw_OSMPerformanceOEE\"\n        - Full path: \"abfss://...\" (used as-is)\n\n        Args:\n            identifier: User-friendly table identifier\n\n        Returns:\n            Full table path\n        \"\"\"\n        if self._is_full_path(identifier):\n            return identifier\n\n        if self.catalog_manager:\n            resolved = self._lookup_in_catalog(identifier)\n            if resolved:\n                return resolved\n\n        for pipeline in self._pipelines.values():\n            for node in pipeline.config.nodes:\n                if node.name == identifier and node.write:\n                    conn = self.connections.get(node.write.connection)\n                    if conn:\n                        return conn.get_path(node.write.path or node.write.table)\n\n        sys_conn_name = (\n            self.project_config.system.connection if self.project_config.system else None\n        )\n        if sys_conn_name:\n            sys_conn = self.connections.get(sys_conn_name)\n            if sys_conn:\n                return sys_conn.get_path(identifier)\n\n        return identifier\n\n    def _is_full_path(self, identifier: str) -&gt; bool:\n        \"\"\"Check if identifier is already a full path.\"\"\"\n        full_path_prefixes = (\"abfss://\", \"s3://\", \"gs://\", \"hdfs://\", \"/\", \"C:\", \"D:\")\n        return identifier.startswith(full_path_prefixes)\n\n    def _lookup_in_catalog(self, identifier: str) -&gt; Optional[str]:\n        \"\"\"Look up identifier in meta_tables catalog.\"\"\"\n        if not self.catalog_manager:\n            return None\n\n        try:\n            df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n            if df.empty or \"table_name\" not in df.columns:\n                return None\n\n            match = df[df[\"table_name\"] == identifier]\n            if not match.empty and \"path\" in match.columns:\n                return match.iloc[0][\"path\"]\n\n            if \".\" in identifier:\n                parts = identifier.split(\".\", 1)\n                if len(parts) == 2:\n                    match = df[df[\"table_name\"] == parts[1]]\n                    if not match.empty and \"path\" in match.columns:\n                        return match.iloc[0][\"path\"]\n\n        except Exception:\n            pass\n\n        return None\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.__init__","title":"<code>__init__(project_config, connections)</code>","text":"<p>Initialize pipeline manager.</p> <p>Parameters:</p> Name Type Description Default <code>project_config</code> <code>ProjectConfig</code> <p>Validated project configuration</p> required <code>connections</code> <code>Dict[str, Any]</code> <p>Connection objects (already instantiated)</p> required Source code in <code>odibi\\pipeline.py</code> <pre><code>def __init__(\n    self,\n    project_config: ProjectConfig,\n    connections: Dict[str, Any],\n):\n    \"\"\"Initialize pipeline manager.\n\n    Args:\n        project_config: Validated project configuration\n        connections: Connection objects (already instantiated)\n    \"\"\"\n    self.project_config = project_config\n    self.connections = connections\n    self._pipelines: Dict[str, Pipeline] = {}\n    self.catalog_manager = None\n    self.lineage_adapter = None\n\n    # Configure logging\n    configure_logging(\n        structured=project_config.logging.structured, level=project_config.logging.level.value\n    )\n\n    # Create manager-level logging context\n    self._ctx = create_logging_context(engine=project_config.engine)\n\n    self._ctx.info(\n        \"Initializing PipelineManager\",\n        project=project_config.project,\n        engine=project_config.engine,\n        pipeline_count=len(project_config.pipelines),\n        connection_count=len(connections),\n    )\n\n    # Initialize Lineage Adapter\n    self.lineage_adapter = OpenLineageAdapter(project_config.lineage)\n\n    # Initialize CatalogManager if configured\n    if project_config.system:\n        from odibi.catalog import CatalogManager\n\n        spark = None\n        engine_instance = None\n\n        if project_config.engine == \"spark\":\n            try:\n                from odibi.engine.spark_engine import SparkEngine\n\n                temp_engine = SparkEngine(connections=connections, config={})\n                spark = temp_engine.spark\n                self._ctx.debug(\"Spark session initialized for System Catalog\")\n            except Exception as e:\n                self._ctx.warning(\n                    f\"Failed to initialize Spark for System Catalog: {e}\",\n                    suggestion=\"Check Spark configuration\",\n                )\n\n        sys_conn = connections.get(project_config.system.connection)\n        if sys_conn:\n            base_path = sys_conn.get_path(project_config.system.path)\n\n            if not spark:\n                try:\n                    from odibi.engine.pandas_engine import PandasEngine\n\n                    engine_instance = PandasEngine(config={})\n                    self._ctx.debug(\"PandasEngine initialized for System Catalog\")\n                except Exception as e:\n                    self._ctx.warning(\n                        f\"Failed to initialize PandasEngine for System Catalog: {e}\"\n                    )\n\n            if spark or engine_instance:\n                self.catalog_manager = CatalogManager(\n                    spark=spark,\n                    config=project_config.system,\n                    base_path=base_path,\n                    engine=engine_instance,\n                    connection=sys_conn,\n                )\n                # Set project name for tagging all catalog records\n                self.catalog_manager.project = project_config.project\n\n                # Skip bootstrap if catalog writes are disabled\n                skip_catalog = (\n                    getattr(project_config.performance, \"skip_catalog_writes\", False)\n                    if project_config.performance\n                    else False\n                )\n                if not skip_catalog:\n                    self.catalog_manager.bootstrap()\n                    self._ctx.info(\n                        \"System Catalog initialized\",\n                        path=base_path,\n                        project=project_config.project,\n                    )\n                else:\n                    self._ctx.debug(\n                        \"System Catalog bootstrap skipped (skip_catalog_writes=true)\"\n                    )\n        else:\n            self._ctx.warning(\n                f\"System connection '{project_config.system.connection}' not found\",\n                suggestion=\"Configure the system connection in your config\",\n            )\n\n    # Get story configuration\n    story_config = self._get_story_config()\n\n    # Create all pipeline instances\n    self._ctx.debug(\n        \"Creating pipeline instances\",\n        pipelines=[p.pipeline for p in project_config.pipelines],\n    )\n    for pipeline_config in project_config.pipelines:\n        pipeline_name = pipeline_config.pipeline\n\n        self._pipelines[pipeline_name] = Pipeline(\n            pipeline_config=pipeline_config,\n            engine=project_config.engine,\n            connections=connections,\n            generate_story=story_config.get(\"auto_generate\", True),\n            story_config=story_config,\n            retry_config=project_config.retry,\n            alerts=project_config.alerts,\n            performance_config=project_config.performance,\n            catalog_manager=self.catalog_manager,\n            lineage_adapter=self.lineage_adapter,\n        )\n        self._pipelines[pipeline_name].project_config = project_config\n\n    self._ctx.info(\n        \"PipelineManager ready\",\n        pipelines=list(self._pipelines.keys()),\n    )\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.clear_state","title":"<code>clear_state(key)</code>","text":"<p>Remove a state entry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The state key to remove</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False otherwise</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def clear_state(self, key: str) -&gt; bool:\n    \"\"\"Remove a state entry.\n\n    Args:\n        key: The state key to remove\n\n    Returns:\n        True if deleted, False otherwise\n    \"\"\"\n    if not self.catalog_manager:\n        return False\n\n    try:\n        return self.catalog_manager.clear_state_key(key)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to clear state: {e}\")\n        return False\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.deploy","title":"<code>deploy(pipelines=None)</code>","text":"<p>Deploy pipeline definitions to the System Catalog.</p> <p>This registers pipeline and node configurations in the catalog, enabling drift detection and governance features.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional pipeline name(s) to deploy. If None, deploys all.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if deployment succeeded, False otherwise.</p> Example <p>manager = PipelineManager.from_yaml(\"odibi.yaml\") manager.deploy()  # Deploy all pipelines manager.deploy(\"sales_daily\")  # Deploy specific pipeline</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def deploy(self, pipelines: Optional[Union[str, List[str]]] = None) -&gt; bool:\n    \"\"\"Deploy pipeline definitions to the System Catalog.\n\n    This registers pipeline and node configurations in the catalog,\n    enabling drift detection and governance features.\n\n    Args:\n        pipelines: Optional pipeline name(s) to deploy. If None, deploys all.\n\n    Returns:\n        True if deployment succeeded, False otherwise.\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"odibi.yaml\")\n        &gt;&gt;&gt; manager.deploy()  # Deploy all pipelines\n        &gt;&gt;&gt; manager.deploy(\"sales_daily\")  # Deploy specific pipeline\n    \"\"\"\n    if not self.catalog_manager:\n        self._ctx.warning(\n            \"System Catalog not configured. Cannot deploy.\",\n            suggestion=\"Configure system catalog in your YAML config\",\n        )\n        return False\n\n    if pipelines is None:\n        to_deploy = self.project_config.pipelines\n    elif isinstance(pipelines, str):\n        to_deploy = [p for p in self.project_config.pipelines if p.pipeline == pipelines]\n    else:\n        to_deploy = [p for p in self.project_config.pipelines if p.pipeline in pipelines]\n\n    if not to_deploy:\n        self._ctx.warning(\"No matching pipelines found to deploy.\")\n        return False\n\n    self._ctx.info(\n        f\"Deploying {len(to_deploy)} pipeline(s) to System Catalog\",\n        pipelines=[p.pipeline for p in to_deploy],\n    )\n\n    try:\n        # Skip bootstrap if catalog writes are disabled\n        skip_catalog = (\n            getattr(self.project_config.performance, \"skip_catalog_writes\", False)\n            if self.project_config.performance\n            else False\n        )\n        if not skip_catalog:\n            self.catalog_manager.bootstrap()\n\n        for pipeline_config in to_deploy:\n            self._ctx.debug(\n                f\"Deploying pipeline: {pipeline_config.pipeline}\",\n                node_count=len(pipeline_config.nodes),\n            )\n            self.catalog_manager.register_pipeline(pipeline_config, self.project_config)\n\n            for node in pipeline_config.nodes:\n                self.catalog_manager.register_node(pipeline_config.pipeline, node)\n\n        self._ctx.info(\n            f\"Deployment complete: {len(to_deploy)} pipeline(s)\",\n            deployed=[p.pipeline for p in to_deploy],\n        )\n        return True\n\n    except Exception as e:\n        self._ctx.error(\n            f\"Deployment failed: {e}\",\n            error_type=type(e).__name__,\n            suggestion=\"Check catalog configuration and permissions\",\n        )\n        return False\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.flush_stories","title":"<code>flush_stories(timeout=60.0)</code>","text":"<p>Wait for all pending async story generation to complete.</p> <p>Call this before operations that need story files to be written, such as lineage generation with SemanticLayerRunner.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum seconds to wait per pipeline</p> <code>60.0</code> <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dict mapping pipeline name to story path (or None if no pending story)</p> Example <p>manager.run(pipelines=['bronze', 'silver', 'gold']) manager.flush_stories()  # Wait for all stories to be written semantic_runner.run()    # Now lineage can read the stories</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def flush_stories(self, timeout: float = 60.0) -&gt; Dict[str, Optional[str]]:\n    \"\"\"Wait for all pending async story generation to complete.\n\n    Call this before operations that need story files to be written,\n    such as lineage generation with SemanticLayerRunner.\n\n    Args:\n        timeout: Maximum seconds to wait per pipeline\n\n    Returns:\n        Dict mapping pipeline name to story path (or None if no pending story)\n\n    Example:\n        &gt;&gt;&gt; manager.run(pipelines=['bronze', 'silver', 'gold'])\n        &gt;&gt;&gt; manager.flush_stories()  # Wait for all stories to be written\n        &gt;&gt;&gt; semantic_runner.run()    # Now lineage can read the stories\n    \"\"\"\n    results = {}\n    for name, pipeline in self._pipelines.items():\n        story_path = pipeline.flush_stories(timeout=timeout)\n        if story_path:\n            results[name] = story_path\n            self._ctx.debug(f\"Story flushed for {name}\", path=story_path)\n    if results:\n        self._ctx.info(f\"Flushed {len(results)} pending story writes\")\n    return results\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.from_yaml","title":"<code>from_yaml(yaml_path, env=None)</code>  <code>classmethod</code>","text":"<p>Create PipelineManager from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to YAML configuration file</p> required <code>env</code> <code>str</code> <p>Environment name to apply overrides (e.g. 'prod')</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineManager</code> <p>PipelineManager instance ready to run pipelines</p> Example <p>manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\") results = manager.run()  # Run all pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: str, env: str = None) -&gt; \"PipelineManager\":\n    \"\"\"Create PipelineManager from YAML file.\n\n    Args:\n        yaml_path: Path to YAML configuration file\n        env: Environment name to apply overrides (e.g. 'prod')\n\n    Returns:\n        PipelineManager instance ready to run pipelines\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"config.yaml\", env=\"prod\")\n        &gt;&gt;&gt; results = manager.run()  # Run all pipelines\n    \"\"\"\n    logger.info(f\"Loading configuration from: {yaml_path}\")\n\n    register_standard_library()\n\n    yaml_path_obj = Path(yaml_path)\n    config_dir = yaml_path_obj.parent.absolute()\n\n    import importlib.util\n    import os\n    import sys\n\n    # Load .env file from config directory if it exists\n    env_file = config_dir / \".env\"\n    if env_file.exists():\n        try:\n            from dotenv import load_dotenv\n\n            load_dotenv(env_file, override=True)\n            logger.debug(f\"Loaded environment from: {env_file}\")\n        except ImportError:\n            logger.warning(\"python-dotenv not installed, skipping .env file\")\n\n    def load_transforms_module(path):\n        if os.path.exists(path):\n            try:\n                spec = importlib.util.spec_from_file_location(\"transforms_autodiscovered\", path)\n                if spec and spec.loader:\n                    module = importlib.util.module_from_spec(spec)\n                    sys.modules[\"transforms_autodiscovered\"] = module\n                    spec.loader.exec_module(module)\n                    logger.info(f\"Auto-loaded transforms from: {path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to auto-load transforms from {path}: {e}\")\n\n    load_transforms_module(os.path.join(config_dir, \"transforms.py\"))\n\n    cwd = os.getcwd()\n    if os.path.abspath(cwd) != str(config_dir):\n        load_transforms_module(os.path.join(cwd, \"transforms.py\"))\n\n    try:\n        config = load_yaml_with_env(str(yaml_path_obj), env=env)\n        logger.debug(\"Configuration loaded successfully\")\n    except FileNotFoundError:\n        logger.error(f\"YAML file not found: {yaml_path}\")\n        raise FileNotFoundError(\n            f\"YAML file not found: {yaml_path}. \"\n            f\"Verify the file exists and consider using an absolute path.\"\n        )\n\n    project_config = ProjectConfig(**config)\n    logger.debug(\n        \"Project config validated\",\n        project=project_config.project,\n        pipelines=len(project_config.pipelines),\n    )\n\n    connections = cls._build_connections(project_config.connections)\n\n    return cls(\n        project_config=project_config,\n        connections=connections,\n    )\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_all_state","title":"<code>get_all_state(prefix=None)</code>","text":"<p>Get all state entries, optionally filtered by key prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Optional[str]</code> <p>Optional key prefix to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with state entries</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_all_state(self, prefix: Optional[str] = None) -&gt; \"pd.DataFrame\":\n    \"\"\"Get all state entries, optionally filtered by key prefix.\n\n    Args:\n        prefix: Optional key prefix to filter by\n\n    Returns:\n        DataFrame with state entries\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n        if not df.empty and prefix and \"key\" in df.columns:\n            df = df[df[\"key\"].str.startswith(prefix)]\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get state: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_lineage","title":"<code>get_lineage(table, direction='both')</code>","text":"<p>Get lineage for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table identifier (supports smart path resolution)</p> required <code>direction</code> <code>str</code> <p>\"upstream\", \"downstream\", or \"both\"</p> <code>'both'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with lineage relationships</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_lineage(\n    self,\n    table: str,\n    direction: str = \"both\",\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Get lineage for a table.\n\n    Args:\n        table: Table identifier (supports smart path resolution)\n        direction: \"upstream\", \"downstream\", or \"both\"\n\n    Returns:\n        DataFrame with lineage relationships\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        resolved_path = self._resolve_table_path(table)\n\n        results = []\n        if direction in (\"upstream\", \"both\"):\n            upstream = self.catalog_manager.get_upstream(resolved_path)\n            for r in upstream:\n                r[\"direction\"] = \"upstream\"\n            results.extend(upstream)\n\n        if direction in (\"downstream\", \"both\"):\n            downstream = self.catalog_manager.get_downstream(resolved_path)\n            for r in downstream:\n                r[\"direction\"] = \"downstream\"\n            results.extend(downstream)\n\n        return pd.DataFrame(results)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get lineage: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_node_stats","title":"<code>get_node_stats(node, days=7)</code>","text":"<p>Get average duration, row counts, success rate over period.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Node name</p> required <code>days</code> <code>int</code> <p>Number of days to look back</p> <code>7</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with node statistics</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_node_stats(self, node: str, days: int = 7) -&gt; Dict[str, Any]:\n    \"\"\"Get average duration, row counts, success rate over period.\n\n    Args:\n        node: Node name\n        days: Number of days to look back\n\n    Returns:\n        Dict with node statistics\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return {}\n\n    try:\n        avg_duration = self.catalog_manager.get_average_duration(node, days)\n\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n        if df.empty:\n            return {\"node\": node, \"runs\": 0}\n\n        if \"timestamp\" in df.columns:\n            cutoff = pd.Timestamp.now(tz=\"UTC\") - pd.Timedelta(days=days)\n            if not pd.api.types.is_datetime64_any_dtype(df[\"timestamp\"]):\n                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n            if df[\"timestamp\"].dt.tz is None:\n                df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"UTC\")\n            df = df[df[\"timestamp\"] &gt;= cutoff]\n\n        node_runs = df[df[\"node_name\"] == node]\n        if node_runs.empty:\n            return {\"node\": node, \"runs\": 0}\n\n        total = len(node_runs)\n        success = len(node_runs[node_runs[\"status\"] == \"SUCCESS\"])\n        avg_rows = node_runs[\"rows_processed\"].mean() if \"rows_processed\" in node_runs else None\n\n        return {\n            \"node\": node,\n            \"runs\": total,\n            \"success_rate\": success / total if total &gt; 0 else 0,\n            \"avg_duration_s\": avg_duration,\n            \"avg_rows\": avg_rows,\n            \"period_days\": days,\n        }\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get node stats: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_pipeline","title":"<code>get_pipeline(name)</code>","text":"<p>Get a specific pipeline instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Pipeline instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pipeline not found</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_pipeline(self, name: str) -&gt; Pipeline:\n    \"\"\"Get a specific pipeline instance.\n\n    Args:\n        name: Pipeline name\n\n    Returns:\n        Pipeline instance\n\n    Raises:\n        ValueError: If pipeline not found\n    \"\"\"\n    if name not in self._pipelines:\n        available = \", \".join(self._pipelines.keys())\n        raise ValueError(f\"Pipeline '{name}' not found. Available: {available}\")\n    return self._pipelines[name]\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_pipeline_status","title":"<code>get_pipeline_status(pipeline)</code>","text":"<p>Get last run status, duration, timestamp for a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>str</code> <p>Pipeline name</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with status info</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_pipeline_status(self, pipeline: str) -&gt; Dict[str, Any]:\n    \"\"\"Get last run status, duration, timestamp for a pipeline.\n\n    Args:\n        pipeline: Pipeline name\n\n    Returns:\n        Dict with status info\n    \"\"\"\n    if not self.catalog_manager:\n        return {}\n\n    try:\n        runs = self.list_runs(pipeline=pipeline, limit=1)\n        if runs.empty:\n            return {\"status\": \"never_run\", \"pipeline\": pipeline}\n\n        last_run = runs.iloc[0].to_dict()\n        return {\n            \"pipeline\": pipeline,\n            \"last_status\": last_run.get(\"status\"),\n            \"last_run_at\": last_run.get(\"timestamp\"),\n            \"last_duration_ms\": last_run.get(\"duration_ms\"),\n            \"last_node\": last_run.get(\"node_name\"),\n        }\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get pipeline status: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_schema_history","title":"<code>get_schema_history(table, limit=5)</code>","text":"<p>Get schema version history for a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table identifier (supports smart path resolution)</p> required <code>limit</code> <code>int</code> <p>Maximum number of versions to return</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with schema history</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_schema_history(\n    self,\n    table: str,\n    limit: int = 5,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Get schema version history for a table.\n\n    Args:\n        table: Table identifier (supports smart path resolution)\n        limit: Maximum number of versions to return\n\n    Returns:\n        DataFrame with schema history\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        return pd.DataFrame()\n\n    try:\n        resolved_path = self._resolve_table_path(table)\n        history = self.catalog_manager.get_schema_history(resolved_path, limit)\n        return pd.DataFrame(history)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to get schema history: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.get_state","title":"<code>get_state(key)</code>","text":"<p>Get a specific state entry (HWM, content hash, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The state key to look up</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary with state data or None if not found</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_state(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get a specific state entry (HWM, content hash, etc.).\n\n    Args:\n        key: The state key to look up\n\n    Returns:\n        Dictionary with state data or None if not found\n    \"\"\"\n\n    if not self.catalog_manager:\n        return None\n\n    try:\n        df = self.catalog_manager._read_table(self.catalog_manager.tables[\"meta_state\"])\n        if df.empty or \"key\" not in df.columns:\n            return None\n\n        row = df[df[\"key\"] == key]\n        if row.empty:\n            return None\n\n        return row.iloc[0].to_dict()\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_pipelines","title":"<code>list_pipelines()</code>","text":"<p>Get list of available pipeline names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of pipeline names</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_pipelines(self) -&gt; List[str]:\n    \"\"\"Get list of available pipeline names.\n\n    Returns:\n        List of pipeline names\n    \"\"\"\n    return list(self._pipelines.keys())\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_registered_nodes","title":"<code>list_registered_nodes(pipeline=None)</code>","text":"<p>List nodes from the system catalog.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Optional[str]</code> <p>Optional pipeline name to filter by</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with node metadata from meta_nodes</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_registered_nodes(self, pipeline: Optional[str] = None) -&gt; \"pd.DataFrame\":\n    \"\"\"List nodes from the system catalog.\n\n    Args:\n        pipeline: Optional pipeline name to filter by\n\n    Returns:\n        DataFrame with node metadata from meta_nodes\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_nodes\"])\n        if not df.empty and pipeline:\n            df = df[df[\"pipeline_name\"] == pipeline]\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list nodes: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_registered_pipelines","title":"<code>list_registered_pipelines()</code>","text":"<p>List all registered pipelines from the system catalog.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with pipeline metadata from meta_pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_registered_pipelines(self) -&gt; \"pd.DataFrame\":\n    \"\"\"List all registered pipelines from the system catalog.\n\n    Returns:\n        DataFrame with pipeline metadata from meta_pipelines\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(\n            self.catalog_manager.tables[\"meta_pipelines\"]\n        )\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list pipelines: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_runs","title":"<code>list_runs(pipeline=None, node=None, status=None, limit=10)</code>","text":"<p>List recent runs with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Optional[str]</code> <p>Optional pipeline name to filter by</p> <code>None</code> <code>node</code> <code>Optional[str]</code> <p>Optional node name to filter by</p> <code>None</code> <code>status</code> <code>Optional[str]</code> <p>Optional status to filter by (SUCCESS, FAILURE)</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of runs to return</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with run history from meta_runs</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_runs(\n    self,\n    pipeline: Optional[str] = None,\n    node: Optional[str] = None,\n    status: Optional[str] = None,\n    limit: int = 10,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"List recent runs with optional filters.\n\n    Args:\n        pipeline: Optional pipeline name to filter by\n        node: Optional node name to filter by\n        status: Optional status to filter by (SUCCESS, FAILURE)\n        limit: Maximum number of runs to return\n\n    Returns:\n        DataFrame with run history from meta_runs\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_runs\"])\n        if df.empty:\n            return df\n\n        if pipeline:\n            df = df[df[\"pipeline_name\"] == pipeline]\n        if node:\n            df = df[df[\"node_name\"] == node]\n        if status:\n            df = df[df[\"status\"] == status]\n\n        if \"timestamp\" in df.columns:\n            df = df.sort_values(\"timestamp\", ascending=False)\n\n        return df.head(limit)\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list runs: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.list_tables","title":"<code>list_tables()</code>","text":"<p>List registered assets from meta_tables.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with table/asset metadata</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def list_tables(self) -&gt; \"pd.DataFrame\":\n    \"\"\"List registered assets from meta_tables.\n\n    Returns:\n        DataFrame with table/asset metadata\n    \"\"\"\n    import pandas as pd\n\n    if not self.catalog_manager:\n        self._ctx.warning(\"Catalog manager not configured\")\n        return pd.DataFrame()\n\n    try:\n        df = self.catalog_manager._read_local_table(self.catalog_manager.tables[\"meta_tables\"])\n        return df\n    except Exception as e:\n        self._ctx.warning(f\"Failed to list tables: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.register_outputs","title":"<code>register_outputs(pipelines=None)</code>","text":"<p>Pre-register node outputs from pipeline configs without running them.</p> <p>Scans pipeline nodes for output locations (write blocks, merge/scd2 params) and registers them to meta_outputs. This enables cross-pipeline references without requiring the source pipelines to have run first.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Pipeline name(s) to register. If None, registers all pipelines.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict mapping pipeline name to number of outputs registered</p> Example <p>manager = PipelineManager.from_yaml(\"pipelines.yaml\") counts = manager.register_outputs(\"silver\")  # Register just silver counts = manager.register_outputs()  # Register all pipelines</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def register_outputs(\n    self,\n    pipelines: Optional[Union[str, List[str]]] = None,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Pre-register node outputs from pipeline configs without running them.\n\n    Scans pipeline nodes for output locations (write blocks, merge/scd2 params)\n    and registers them to meta_outputs. This enables cross-pipeline references\n    without requiring the source pipelines to have run first.\n\n    Args:\n        pipelines: Pipeline name(s) to register. If None, registers all pipelines.\n\n    Returns:\n        Dict mapping pipeline name to number of outputs registered\n\n    Example:\n        &gt;&gt;&gt; manager = PipelineManager.from_yaml(\"pipelines.yaml\")\n        &gt;&gt;&gt; counts = manager.register_outputs(\"silver\")  # Register just silver\n        &gt;&gt;&gt; counts = manager.register_outputs()  # Register all pipelines\n    \"\"\"\n    if pipelines is None:\n        pipeline_names = list(self._pipelines.keys())\n    elif isinstance(pipelines, str):\n        pipeline_names = [pipelines]\n    else:\n        pipeline_names = pipelines\n\n    results = {}\n    for name in pipeline_names:\n        if name not in self._pipelines:\n            self._ctx.warning(f\"Pipeline not found: {name}\")\n            continue\n\n        pipeline = self._pipelines[name]\n        count = pipeline.register_outputs()\n        results[name] = count\n\n    total = sum(results.values())\n    self._ctx.info(f\"Pre-registered {total} outputs from {len(results)} pipelines\")\n    return results\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineManager.run","title":"<code>run(pipelines=None, dry_run=False, resume_from_failure=False, parallel=False, max_workers=4, on_error=None, tag=None, node=None, console=False)</code>","text":"<p>Run one, multiple, or all pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>Optional[Union[str, List[str]]]</code> <p>Pipeline name(s) to run.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>Whether to simulate execution.</p> <code>False</code> <code>resume_from_failure</code> <code>bool</code> <p>Whether to skip successfully completed nodes from last run.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run nodes in parallel.</p> <code>False</code> <code>max_workers</code> <code>int</code> <p>Maximum number of worker threads for parallel execution.</p> <code>4</code> <code>on_error</code> <code>Optional[str]</code> <p>Override error handling strategy (fail_fast, fail_later, ignore).</p> <code>None</code> <code>tag</code> <code>Optional[str]</code> <p>Filter nodes by tag (only nodes with this tag will run).</p> <code>None</code> <code>node</code> <code>Optional[Union[str, List[str]]]</code> <p>Run only specific node(s) by name - can be a string or list of strings.</p> <code>None</code> <code>console</code> <code>bool</code> <p>Whether to show rich console output with progress.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[PipelineResults, Dict[str, PipelineResults]]</code> <p>PipelineResults or Dict of results</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def run(\n    self,\n    pipelines: Optional[Union[str, List[str]]] = None,\n    dry_run: bool = False,\n    resume_from_failure: bool = False,\n    parallel: bool = False,\n    max_workers: int = 4,\n    on_error: Optional[str] = None,\n    tag: Optional[str] = None,\n    node: Optional[Union[str, List[str]]] = None,\n    console: bool = False,\n) -&gt; Union[PipelineResults, Dict[str, PipelineResults]]:\n    \"\"\"Run one, multiple, or all pipelines.\n\n    Args:\n        pipelines: Pipeline name(s) to run.\n        dry_run: Whether to simulate execution.\n        resume_from_failure: Whether to skip successfully completed nodes from last run.\n        parallel: Whether to run nodes in parallel.\n        max_workers: Maximum number of worker threads for parallel execution.\n        on_error: Override error handling strategy (fail_fast, fail_later, ignore).\n        tag: Filter nodes by tag (only nodes with this tag will run).\n        node: Run only specific node(s) by name - can be a string or list of strings.\n        console: Whether to show rich console output with progress.\n\n    Returns:\n        PipelineResults or Dict of results\n    \"\"\"\n    if pipelines is None:\n        pipeline_names = list(self._pipelines.keys())\n    elif isinstance(pipelines, str):\n        pipeline_names = [pipelines]\n    else:\n        pipeline_names = pipelines\n\n    for name in pipeline_names:\n        if name not in self._pipelines:\n            available = \", \".join(self._pipelines.keys())\n            self._ctx.error(\n                f\"Pipeline not found: {name}\",\n                available=list(self._pipelines.keys()),\n            )\n            raise ValueError(f\"Pipeline '{name}' not found. Available pipelines: {available}\")\n\n    # Phase 2: Auto-register pipelines and nodes before execution\n    if self.catalog_manager:\n        self._auto_register_pipelines(pipeline_names)\n\n    self._ctx.info(\n        f\"Running {len(pipeline_names)} pipeline(s)\",\n        pipelines=pipeline_names,\n        dry_run=dry_run,\n        parallel=parallel,\n    )\n\n    results = {}\n    for idx, name in enumerate(pipeline_names):\n        # Invalidate cache before each pipeline so it sees latest outputs\n        if self.catalog_manager:\n            self.catalog_manager.invalidate_cache()\n\n        self._ctx.info(\n            f\"Executing pipeline {idx + 1}/{len(pipeline_names)}: {name}\",\n            pipeline=name,\n            order=idx + 1,\n        )\n\n        results[name] = self._pipelines[name].run(\n            dry_run=dry_run,\n            resume_from_failure=resume_from_failure,\n            parallel=parallel,\n            max_workers=max_workers,\n            on_error=on_error,\n            tag=tag,\n            node=node,\n            console=console,\n        )\n\n        result = results[name]\n        status = \"SUCCESS\" if not result.failed else \"FAILED\"\n        self._ctx.info(\n            f\"Pipeline {status}: {name}\",\n            status=status,\n            duration_s=round(result.duration, 2),\n            completed=len(result.completed),\n            failed=len(result.failed),\n        )\n\n        if result.story_path:\n            self._ctx.debug(f\"Story generated: {result.story_path}\")\n\n    # Generate combined lineage if configured\n    has_story = hasattr(self.project_config, \"story\") and self.project_config.story\n    generate_lineage_enabled = has_story and self.project_config.story.generate_lineage\n\n    self._ctx.debug(\n        \"Lineage check\",\n        has_story=has_story,\n        generate_lineage_enabled=generate_lineage_enabled,\n    )\n\n    if generate_lineage_enabled:\n        # Flush any pending async story writes before generating lineage\n        self._ctx.info(\"Generating combined lineage...\")\n        self.flush_stories()\n\n        try:\n            lineage_result = generate_lineage(self.project_config)\n            if lineage_result:\n                self._ctx.info(\n                    \"Combined lineage generated\",\n                    nodes=len(lineage_result.nodes),\n                    edges=len(lineage_result.edges),\n                    json_path=lineage_result.json_path,\n                )\n            else:\n                self._ctx.warning(\"Lineage generation returned None\")\n        except Exception as e:\n            self._ctx.warning(f\"Failed to generate combined lineage: {e}\")\n\n    if len(pipeline_names) == 1:\n        return results[pipeline_names[0]]\n    else:\n        return results\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults","title":"<code>PipelineResults</code>  <code>dataclass</code>","text":"<p>Results from pipeline execution.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>@dataclass\nclass PipelineResults:\n    \"\"\"Results from pipeline execution.\"\"\"\n\n    pipeline_name: str\n    completed: List[str] = field(default_factory=list)\n    failed: List[str] = field(default_factory=list)\n    skipped: List[str] = field(default_factory=list)\n    node_results: Dict[str, NodeResult] = field(default_factory=dict)\n    duration: float = 0.0\n    start_time: Optional[str] = None\n    end_time: Optional[str] = None\n    story_path: Optional[str] = None\n\n    def get_node_result(self, name: str) -&gt; Optional[NodeResult]:\n        \"\"\"Get result for specific node.\n\n        Args:\n            name: Node name\n\n        Returns:\n            NodeResult if available, None otherwise\n        \"\"\"\n        return self.node_results.get(name)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary.\n\n        Returns:\n            Dictionary representation\n        \"\"\"\n        return {\n            \"pipeline_name\": self.pipeline_name,\n            \"completed\": self.completed,\n            \"failed\": self.failed,\n            \"skipped\": self.skipped,\n            \"duration\": self.duration,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"node_count\": len(self.node_results),\n        }\n\n    def debug_summary(self) -&gt; str:\n        \"\"\"Generate a debug summary with next steps for failed pipelines.\n\n        Returns:\n            Formatted string with failure details and suggested next steps.\n            Returns empty string if pipeline succeeded.\n        \"\"\"\n        if not self.failed:\n            return \"\"\n\n        lines = []\n        lines.append(f\"\\n{'=' * 60}\")\n        lines.append(f\"\u274c Pipeline '{self.pipeline_name}' failed\")\n        lines.append(f\"{'=' * 60}\")\n\n        # List failed nodes with errors\n        lines.append(\"\\nFailed nodes:\")\n        for node_name in self.failed:\n            node_res = self.node_results.get(node_name)\n            if node_res and node_res.error:\n                error_msg = str(node_res.error)[:200]\n                lines.append(f\"  \u2022 {node_name}: {error_msg}\")\n            else:\n                lines.append(f\"  \u2022 {node_name}\")\n\n        # Story path if available\n        lines.append(\"\\n\ud83d\udcd6 Next Steps:\")\n        if self.story_path:\n            lines.append(\"  1. View the execution story:\")\n            lines.append(f\"     odibi story show {self.story_path}\")\n            lines.append(\"\")\n            lines.append(\"  2. Inspect a specific failed node:\")\n            first_failed = self.failed[0] if self.failed else \"&lt;node_name&gt;\"\n            lines.append(f\"     odibi story last --node {first_failed}\")\n        else:\n            lines.append(\"  1. Check the logs for error details\")\n\n        lines.append(\"\")\n        lines.append(\"  3. If this is an environment issue:\")\n        lines.append(\"     odibi doctor\")\n        lines.append(\"\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.debug_summary","title":"<code>debug_summary()</code>","text":"<p>Generate a debug summary with next steps for failed pipelines.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with failure details and suggested next steps.</p> <code>str</code> <p>Returns empty string if pipeline succeeded.</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def debug_summary(self) -&gt; str:\n    \"\"\"Generate a debug summary with next steps for failed pipelines.\n\n    Returns:\n        Formatted string with failure details and suggested next steps.\n        Returns empty string if pipeline succeeded.\n    \"\"\"\n    if not self.failed:\n        return \"\"\n\n    lines = []\n    lines.append(f\"\\n{'=' * 60}\")\n    lines.append(f\"\u274c Pipeline '{self.pipeline_name}' failed\")\n    lines.append(f\"{'=' * 60}\")\n\n    # List failed nodes with errors\n    lines.append(\"\\nFailed nodes:\")\n    for node_name in self.failed:\n        node_res = self.node_results.get(node_name)\n        if node_res and node_res.error:\n            error_msg = str(node_res.error)[:200]\n            lines.append(f\"  \u2022 {node_name}: {error_msg}\")\n        else:\n            lines.append(f\"  \u2022 {node_name}\")\n\n    # Story path if available\n    lines.append(\"\\n\ud83d\udcd6 Next Steps:\")\n    if self.story_path:\n        lines.append(\"  1. View the execution story:\")\n        lines.append(f\"     odibi story show {self.story_path}\")\n        lines.append(\"\")\n        lines.append(\"  2. Inspect a specific failed node:\")\n        first_failed = self.failed[0] if self.failed else \"&lt;node_name&gt;\"\n        lines.append(f\"     odibi story last --node {first_failed}\")\n    else:\n        lines.append(\"  1. Check the logs for error details\")\n\n    lines.append(\"\")\n    lines.append(\"  3. If this is an environment issue:\")\n    lines.append(\"     odibi doctor\")\n    lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.get_node_result","title":"<code>get_node_result(name)</code>","text":"<p>Get result for specific node.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Node name</p> required <p>Returns:</p> Type Description <code>Optional[NodeResult]</code> <p>NodeResult if available, None otherwise</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def get_node_result(self, name: str) -&gt; Optional[NodeResult]:\n    \"\"\"Get result for specific node.\n\n    Args:\n        name: Node name\n\n    Returns:\n        NodeResult if available, None otherwise\n    \"\"\"\n    return self.node_results.get(name)\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.PipelineResults.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation</p> Source code in <code>odibi\\pipeline.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"pipeline_name\": self.pipeline_name,\n        \"completed\": self.completed,\n        \"failed\": self.failed,\n        \"skipped\": self.skipped,\n        \"duration\": self.duration,\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"node_count\": len(self.node_results),\n    }\n</code></pre>"},{"location":"reference/api/pipeline/#odibi.pipeline.create_context","title":"<code>create_context(engine, spark_session=None)</code>","text":"<p>Factory function to create appropriate context.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>str</code> <p>Engine type ('pandas' or 'spark')</p> required <code>spark_session</code> <code>Optional[Any]</code> <p>SparkSession (required if engine='spark')</p> <code>None</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context instance for the specified engine</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If engine is invalid or SparkSession missing for Spark</p> Source code in <code>odibi\\context.py</code> <pre><code>def create_context(engine: str, spark_session: Optional[Any] = None) -&gt; Context:\n    \"\"\"Factory function to create appropriate context.\n\n    Args:\n        engine: Engine type ('pandas' or 'spark')\n        spark_session: SparkSession (required if engine='spark')\n\n    Returns:\n        Context instance for the specified engine\n\n    Raises:\n        ValueError: If engine is invalid or SparkSession missing for Spark\n    \"\"\"\n    if engine == \"pandas\":\n        return PandasContext()\n    elif engine == \"spark\":\n        if spark_session is None:\n            raise ValueError(\"SparkSession required for Spark engine\")\n        return SparkContext(spark_session)\n    elif engine == \"polars\":\n        return PolarsContext()\n    else:\n        raise ValueError(f\"Unsupported engine: {engine}. Use 'pandas' or 'spark'\")\n</code></pre>"},{"location":"reference/api/validation/","title":"Validation API","text":""},{"location":"reference/api/validation/#odibi.validation.engine","title":"<code>odibi.validation.engine</code>","text":"<p>Optimized validation engine for executing declarative data quality tests.</p> <p>Performance optimizations: - Fail-fast mode for early exit on first failure - DataFrame caching for Spark with many tests - Lazy evaluation for Polars (avoids early .collect()) - Batched null count aggregation (single scan for NOT_NULL) - Vectorized operations (no Python loops over rows) - Memory-efficient mask operations (no full DataFrame copies)</p>"},{"location":"reference/api/validation/#odibi.validation.engine.Validator","title":"<code>Validator</code>","text":"<p>Validation engine for executing declarative data quality tests. Supports Spark, Pandas, and Polars engines with performance optimizations.</p> Source code in <code>odibi\\validation\\engine.py</code> <pre><code>class Validator:\n    \"\"\"\n    Validation engine for executing declarative data quality tests.\n    Supports Spark, Pandas, and Polars engines with performance optimizations.\n    \"\"\"\n\n    def validate(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Run validation checks against a DataFrame.\n\n        Args:\n            df: Spark, Pandas, or Polars DataFrame\n            config: Validation configuration\n            context: Optional context (e.g. {'columns': ...}) for contracts\n\n        Returns:\n            List of error messages (empty if all checks pass)\n        \"\"\"\n        ctx = get_logging_context()\n        test_count = len(config.tests)\n        failures = []\n        is_spark = False\n        is_polars = False\n        engine_type = \"pandas\"\n\n        try:\n            import pyspark\n\n            if isinstance(df, pyspark.sql.DataFrame):\n                is_spark = True\n                engine_type = \"spark\"\n        except ImportError:\n            pass\n\n        if not is_spark:\n            try:\n                import polars as pl\n\n                if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n                    is_polars = True\n                    engine_type = \"polars\"\n            except ImportError:\n                pass\n\n        ctx.debug(\n            \"Starting validation\",\n            test_count=test_count,\n            engine=engine_type,\n            df_type=type(df).__name__,\n            fail_fast=getattr(config, \"fail_fast\", False),\n        )\n\n        if is_spark:\n            failures = self._validate_spark(df, config, context)\n        elif is_polars:\n            failures = self._validate_polars(df, config, context)\n        else:\n            failures = self._validate_pandas(df, config, context)\n\n        tests_passed = test_count - len(failures)\n        ctx.info(\n            \"Validation complete\",\n            total_tests=test_count,\n            tests_passed=tests_passed,\n            tests_failed=len(failures),\n            engine=engine_type,\n        )\n\n        ctx.log_validation_result(\n            passed=len(failures) == 0,\n            rule_name=\"batch_validation\",\n            failures=failures[:5] if failures else None,\n            total_tests=test_count,\n            tests_passed=tests_passed,\n            tests_failed=len(failures),\n        )\n\n        return failures\n\n    def _handle_failure(self, message: str, test: Any) -&gt; Optional[str]:\n        \"\"\"Handle failure based on severity.\"\"\"\n        ctx = get_logging_context()\n        severity = getattr(test, \"on_fail\", ContractSeverity.FAIL)\n        test_type = getattr(test, \"type\", \"unknown\")\n\n        if severity == ContractSeverity.WARN:\n            ctx.warning(\n                f\"Validation Warning: {message}\",\n                test_type=str(test_type),\n                severity=\"warn\",\n            )\n            return None\n\n        ctx.error(\n            f\"Validation Failed: {message}\",\n            test_type=str(test_type),\n            severity=\"fail\",\n            test_config=str(test),\n        )\n        return message\n\n    def _validate_polars(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Execute checks using Polars with lazy evaluation where possible.\n\n        Optimization: Avoids collecting full LazyFrame. Uses lazy aggregations\n        and only collects scalar results.\n        \"\"\"\n        import polars as pl\n\n        ctx = get_logging_context()\n        fail_fast = getattr(config, \"fail_fast\", False)\n        is_lazy = isinstance(df, pl.LazyFrame)\n\n        if is_lazy:\n            row_count = df.select(pl.len()).collect().item()\n            columns = df.collect_schema().names()\n        else:\n            row_count = len(df)\n            columns = df.columns\n\n        ctx.debug(\"Validating Polars DataFrame\", row_count=row_count, is_lazy=is_lazy)\n\n        failures = []\n\n        for test in config.tests:\n            msg = None\n            test_type = getattr(test, \"type\", \"unknown\")\n            ctx.debug(\"Executing test\", test_type=str(test_type))\n\n            if test.type == TestType.SCHEMA:\n                if context and \"columns\" in context:\n                    expected = set(context[\"columns\"].keys())\n                    actual = set(columns)\n                    if getattr(test, \"strict\", True):\n                        if actual != expected:\n                            msg = f\"Schema mismatch. Expected {expected}, got {actual}\"\n                    else:\n                        missing = expected - actual\n                        if missing:\n                            msg = f\"Schema mismatch. Missing columns: {missing}\"\n\n            elif test.type == TestType.ROW_COUNT:\n                if test.min is not None and row_count &lt; test.min:\n                    msg = f\"Row count {row_count} &lt; min {test.min}\"\n                elif test.max is not None and row_count &gt; test.max:\n                    msg = f\"Row count {row_count} &gt; max {test.max}\"\n\n            elif test.type == TestType.FRESHNESS:\n                col = getattr(test, \"column\", \"updated_at\")\n                if col in columns:\n                    if is_lazy:\n                        max_ts = df.select(pl.col(col).max()).collect().item()\n                    else:\n                        max_ts = df[col].max()\n                    if max_ts:\n                        from datetime import datetime, timedelta, timezone\n\n                        duration_str = test.max_age\n                        delta = None\n                        if duration_str.endswith(\"h\"):\n                            delta = timedelta(hours=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"d\"):\n                            delta = timedelta(days=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"m\"):\n                            delta = timedelta(minutes=int(duration_str[:-1]))\n\n                        if delta:\n                            if datetime.now(timezone.utc) - max_ts &gt; delta:\n                                msg = (\n                                    f\"Data too old. Max timestamp {max_ts} \"\n                                    f\"is older than {test.max_age}\"\n                                )\n                else:\n                    msg = f\"Freshness check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.NOT_NULL:\n                for col in test.columns:\n                    if col in columns:\n                        if is_lazy:\n                            null_count = df.select(pl.col(col).is_null().sum()).collect().item()\n                        else:\n                            null_count = df[col].null_count()\n                        if null_count &gt; 0:\n                            col_msg = f\"Column '{col}' contains {null_count} NULLs\"\n                            ctx.debug(\n                                \"NOT_NULL check failed\",\n                                column=col,\n                                null_count=null_count,\n                                row_count=row_count,\n                            )\n                            res = self._handle_failure(col_msg, test)\n                            if res:\n                                failures.append(res)\n                                if fail_fast:\n                                    return [f for f in failures if f]\n                continue\n\n            elif test.type == TestType.UNIQUE:\n                cols = [c for c in test.columns if c in columns]\n                if len(cols) != len(test.columns):\n                    msg = f\"Unique check failed: Columns {set(test.columns) - set(cols)} not found\"\n                else:\n                    if is_lazy:\n                        dup_count = (\n                            df.group_by(cols)\n                            .agg(pl.len().alias(\"cnt\"))\n                            .filter(pl.col(\"cnt\") &gt; 1)\n                            .select(pl.len())\n                            .collect()\n                            .item()\n                        )\n                    else:\n                        dup_count = (\n                            df.group_by(cols)\n                            .agg(pl.len().alias(\"cnt\"))\n                            .filter(pl.col(\"cnt\") &gt; 1)\n                            .height\n                        )\n                    if dup_count &gt; 0:\n                        msg = f\"Column '{', '.join(cols)}' is not unique\"\n                        ctx.debug(\n                            \"UNIQUE check failed\",\n                            columns=cols,\n                            duplicate_groups=dup_count,\n                        )\n\n            elif test.type == TestType.ACCEPTED_VALUES:\n                col = test.column\n                if col in columns:\n                    if is_lazy:\n                        invalid_count = (\n                            df.filter(~pl.col(col).is_in(test.values))\n                            .select(pl.len())\n                            .collect()\n                            .item()\n                        )\n                    else:\n                        invalid_count = df.filter(~pl.col(col).is_in(test.values)).height\n                    if invalid_count &gt; 0:\n                        if is_lazy:\n                            examples = (\n                                df.filter(~pl.col(col).is_in(test.values))\n                                .select(pl.col(col))\n                                .limit(3)\n                                .collect()[col]\n                                .to_list()\n                            )\n                        else:\n                            invalid_rows = df.filter(~pl.col(col).is_in(test.values))\n                            examples = invalid_rows[col].head(3).to_list()\n                        msg = f\"Column '{col}' contains invalid values. Found: {examples}\"\n                        ctx.debug(\n                            \"ACCEPTED_VALUES check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            examples=examples,\n                        )\n                else:\n                    msg = f\"Accepted values check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.RANGE:\n                col = test.column\n                if col in columns:\n                    cond = pl.lit(False)\n                    if test.min is not None:\n                        cond = cond | (pl.col(col) &lt; test.min)\n                    if test.max is not None:\n                        cond = cond | (pl.col(col) &gt; test.max)\n                    if is_lazy:\n                        invalid_count = df.filter(cond).select(pl.len()).collect().item()\n                    else:\n                        invalid_count = df.filter(cond).height\n                    if invalid_count &gt; 0:\n                        msg = f\"Column '{col}' contains {invalid_count} values out of range\"\n                        ctx.debug(\n                            \"RANGE check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            min=test.min,\n                            max=test.max,\n                        )\n                else:\n                    msg = f\"Range check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.REGEX_MATCH:\n                col = test.column\n                if col in columns:\n                    regex_cond = pl.col(col).is_not_null() &amp; ~pl.col(col).str.contains(test.pattern)\n                    if is_lazy:\n                        invalid_count = df.filter(regex_cond).select(pl.len()).collect().item()\n                    else:\n                        invalid_count = df.filter(regex_cond).height\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Column '{col}' contains {invalid_count} values \"\n                            f\"that does not match pattern '{test.pattern}'\"\n                        )\n                        ctx.debug(\n                            \"REGEX_MATCH check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            pattern=test.pattern,\n                        )\n                else:\n                    msg = f\"Regex check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.CUSTOM_SQL:\n                ctx.warning(\n                    \"CUSTOM_SQL not fully supported in Polars; skipping\",\n                    test_name=getattr(test, \"name\", \"custom_sql\"),\n                )\n                continue\n\n            if msg:\n                res = self._handle_failure(msg, test)\n                if res:\n                    failures.append(res)\n                    if fail_fast:\n                        break\n\n        return [f for f in failures if f]\n\n    def _validate_spark(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Execute checks using Spark SQL with optimizations.\n\n        Optimizations:\n        - Optional DataFrame caching when cache_df=True\n        - Batched null count aggregation (single scan for all NOT_NULL columns)\n        - Fail-fast mode to skip remaining tests\n        - Reuses row_count instead of re-counting\n        \"\"\"\n        from pyspark.sql import functions as F\n\n        ctx = get_logging_context()\n        failures = []\n        fail_fast = getattr(config, \"fail_fast\", False)\n        cache_df = getattr(config, \"cache_df\", False)\n\n        df_work = df\n        if cache_df:\n            df_work = df.cache()\n            ctx.debug(\"DataFrame cached for validation\")\n\n        row_count = df_work.count()\n        ctx.debug(\"Validating Spark DataFrame\", row_count=row_count)\n\n        for test in config.tests:\n            msg = None\n            test_type = getattr(test, \"type\", \"unknown\")\n            ctx.debug(\"Executing test\", test_type=str(test_type))\n\n            if test.type == TestType.ROW_COUNT:\n                if test.min is not None and row_count &lt; test.min:\n                    msg = f\"Row count {row_count} &lt; min {test.min}\"\n                elif test.max is not None and row_count &gt; test.max:\n                    msg = f\"Row count {row_count} &gt; max {test.max}\"\n\n            elif test.type == TestType.SCHEMA:\n                if context and \"columns\" in context:\n                    expected = set(context[\"columns\"].keys())\n                    actual = set(df_work.columns)\n                    if getattr(test, \"strict\", True):\n                        if actual != expected:\n                            msg = f\"Schema mismatch. Expected {expected}, got {actual}\"\n                    else:\n                        missing = expected - actual\n                        if missing:\n                            msg = f\"Schema mismatch. Missing columns: {missing}\"\n\n            elif test.type == TestType.FRESHNESS:\n                col = getattr(test, \"column\", \"updated_at\")\n                if col in df_work.columns:\n                    max_ts = df_work.agg(F.max(col)).collect()[0][0]\n                    if max_ts:\n                        from datetime import datetime, timedelta, timezone\n\n                        duration_str = test.max_age\n                        delta = None\n                        if duration_str.endswith(\"h\"):\n                            delta = timedelta(hours=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"d\"):\n                            delta = timedelta(days=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"m\"):\n                            delta = timedelta(minutes=int(duration_str[:-1]))\n\n                        if delta and (datetime.now(timezone.utc) - max_ts &gt; delta):\n                            msg = (\n                                f\"Data too old. Max timestamp {max_ts} is older than {test.max_age}\"\n                            )\n                else:\n                    msg = f\"Freshness check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.NOT_NULL:\n                valid_cols = [c for c in test.columns if c in df_work.columns]\n                if valid_cols:\n                    null_aggs = [\n                        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n                        for c in valid_cols\n                    ]\n                    null_counts = df_work.agg(*null_aggs).collect()[0].asDict()\n                    for col in valid_cols:\n                        null_count = null_counts.get(col, 0) or 0\n                        if null_count &gt; 0:\n                            col_msg = f\"Column '{col}' contains {null_count} NULLs\"\n                            ctx.debug(\n                                \"NOT_NULL check failed\",\n                                column=col,\n                                null_count=null_count,\n                                row_count=row_count,\n                            )\n                            res = self._handle_failure(col_msg, test)\n                            if res:\n                                failures.append(res)\n                                if fail_fast:\n                                    if cache_df:\n                                        df_work.unpersist()\n                                    return failures\n                continue\n\n            elif test.type == TestType.UNIQUE:\n                cols = [c for c in test.columns if c in df_work.columns]\n                if len(cols) != len(test.columns):\n                    msg = f\"Unique check failed: Columns {set(test.columns) - set(cols)} not found\"\n                else:\n                    dup_count = df_work.groupBy(*cols).count().filter(\"count &gt; 1\").count()\n                    if dup_count &gt; 0:\n                        msg = f\"Column '{', '.join(cols)}' is not unique\"\n                        ctx.debug(\n                            \"UNIQUE check failed\",\n                            columns=cols,\n                            duplicate_groups=dup_count,\n                        )\n\n            elif test.type == TestType.ACCEPTED_VALUES:\n                col = test.column\n                if col in df_work.columns:\n                    invalid_df = df_work.filter(~F.col(col).isin(test.values))\n                    invalid_count = invalid_df.count()\n                    if invalid_count &gt; 0:\n                        examples_rows = invalid_df.select(col).limit(3).collect()\n                        examples = [r[0] for r in examples_rows]\n                        msg = f\"Column '{col}' contains invalid values. Found: {examples}\"\n                        ctx.debug(\n                            \"ACCEPTED_VALUES check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            examples=examples,\n                        )\n                else:\n                    msg = f\"Accepted values check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.RANGE:\n                col = test.column\n                if col in df_work.columns:\n                    cond = F.lit(False)\n                    if test.min is not None:\n                        cond = cond | (F.col(col) &lt; test.min)\n                    if test.max is not None:\n                        cond = cond | (F.col(col) &gt; test.max)\n\n                    invalid_count = df_work.filter(cond).count()\n                    if invalid_count &gt; 0:\n                        msg = f\"Column '{col}' contains {invalid_count} values out of range\"\n                        ctx.debug(\n                            \"RANGE check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            min=test.min,\n                            max=test.max,\n                        )\n                else:\n                    msg = f\"Range check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.REGEX_MATCH:\n                col = test.column\n                if col in df_work.columns:\n                    invalid_count = df_work.filter(\n                        F.col(col).isNotNull() &amp; ~F.col(col).rlike(test.pattern)\n                    ).count()\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Column '{col}' contains {invalid_count} values \"\n                            f\"that does not match pattern '{test.pattern}'\"\n                        )\n                        ctx.debug(\n                            \"REGEX_MATCH check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            pattern=test.pattern,\n                        )\n                else:\n                    msg = f\"Regex check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.CUSTOM_SQL:\n                try:\n                    invalid_count = df_work.filter(f\"NOT ({test.condition})\").count()\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Custom check '{getattr(test, 'name', 'custom_sql')}' failed. \"\n                            f\"Found {invalid_count} invalid rows.\"\n                        )\n                        ctx.debug(\n                            \"CUSTOM_SQL check failed\",\n                            condition=test.condition,\n                            invalid_count=invalid_count,\n                        )\n                except Exception as e:\n                    msg = f\"Failed to execute custom SQL '{test.condition}': {e}\"\n                    ctx.error(\n                        \"CUSTOM_SQL execution error\",\n                        condition=test.condition,\n                        error=str(e),\n                    )\n\n            if msg:\n                res = self._handle_failure(msg, test)\n                if res:\n                    failures.append(res)\n                    if fail_fast:\n                        break\n\n        if cache_df:\n            df_work.unpersist()\n\n        return failures\n\n    def _validate_pandas(\n        self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n    ) -&gt; List[str]:\n        \"\"\"\n        Execute checks using Pandas with optimizations.\n\n        Optimizations:\n        - Single pass for UNIQUE (no double .duplicated() call)\n        - Mask-based operations (no full DataFrame copies for invalid rows)\n        - Memory-efficient example extraction\n        - Fail-fast mode support\n        \"\"\"\n        ctx = get_logging_context()\n        failures = []\n        row_count = len(df)\n        fail_fast = getattr(config, \"fail_fast\", False)\n\n        ctx.debug(\"Validating Pandas DataFrame\", row_count=row_count)\n\n        for test in config.tests:\n            msg = None\n            test_type = getattr(test, \"type\", \"unknown\")\n            ctx.debug(\"Executing test\", test_type=str(test_type))\n\n            if test.type == TestType.SCHEMA:\n                if context and \"columns\" in context:\n                    expected = set(context[\"columns\"].keys())\n                    actual = set(df.columns)\n                    if getattr(test, \"strict\", True):\n                        if actual != expected:\n                            msg = f\"Schema mismatch. Expected {expected}, got {actual}\"\n                    else:\n                        missing = expected - actual\n                        if missing:\n                            msg = f\"Schema mismatch. Missing columns: {missing}\"\n\n            elif test.type == TestType.FRESHNESS:\n                col = getattr(test, \"column\", \"updated_at\")\n                if col in df.columns:\n                    import pandas as pd\n\n                    if not pd.api.types.is_datetime64_any_dtype(df[col]):\n                        try:\n                            s = pd.to_datetime(df[col])\n                            max_ts = s.max()\n                        except Exception:\n                            max_ts = None\n                    else:\n                        max_ts = df[col].max()\n\n                    if max_ts is not None and max_ts is not pd.NaT:\n                        from datetime import datetime, timedelta, timezone\n\n                        duration_str = test.max_age\n                        delta = None\n                        if duration_str.endswith(\"h\"):\n                            delta = timedelta(hours=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"d\"):\n                            delta = timedelta(days=int(duration_str[:-1]))\n                        elif duration_str.endswith(\"m\"):\n                            delta = timedelta(minutes=int(duration_str[:-1]))\n\n                        if delta and (datetime.now(timezone.utc) - max_ts &gt; delta):\n                            msg = (\n                                f\"Data too old. Max timestamp {max_ts} is older than {test.max_age}\"\n                            )\n                else:\n                    msg = f\"Freshness check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.ROW_COUNT:\n                if test.min is not None and row_count &lt; test.min:\n                    msg = f\"Row count {row_count} &lt; min {test.min}\"\n                elif test.max is not None and row_count &gt; test.max:\n                    msg = f\"Row count {row_count} &gt; max {test.max}\"\n\n            elif test.type == TestType.NOT_NULL:\n                for col in test.columns:\n                    if col in df.columns:\n                        null_count = int(df[col].isnull().sum())\n                        if null_count &gt; 0:\n                            col_msg = f\"Column '{col}' contains {null_count} NULLs\"\n                            ctx.debug(\n                                \"NOT_NULL check failed\",\n                                column=col,\n                                null_count=null_count,\n                                row_count=row_count,\n                            )\n                            res = self._handle_failure(col_msg, test)\n                            if res:\n                                failures.append(res)\n                                if fail_fast:\n                                    return [f for f in failures if f]\n                    else:\n                        col_msg = f\"Column '{col}' not found in DataFrame\"\n                        ctx.debug(\n                            \"NOT_NULL check failed - column missing\",\n                            column=col,\n                        )\n                        res = self._handle_failure(col_msg, test)\n                        if res:\n                            failures.append(res)\n                            if fail_fast:\n                                return [f for f in failures if f]\n                continue\n\n            elif test.type == TestType.UNIQUE:\n                cols = [c for c in test.columns if c in df.columns]\n                if len(cols) != len(test.columns):\n                    msg = f\"Unique check failed: Columns {set(test.columns) - set(cols)} not found\"\n                else:\n                    dups = df.duplicated(subset=cols)\n                    dup_count = int(dups.sum())\n                    if dup_count &gt; 0:\n                        msg = f\"Column '{', '.join(cols)}' is not unique\"\n                        ctx.debug(\n                            \"UNIQUE check failed\",\n                            columns=cols,\n                            duplicate_rows=dup_count,\n                        )\n\n            elif test.type == TestType.ACCEPTED_VALUES:\n                col = test.column\n                if col in df.columns:\n                    mask = ~df[col].isin(test.values)\n                    invalid_count = int(mask.sum())\n                    if invalid_count &gt; 0:\n                        examples = df.loc[mask, col].dropna().unique()[:3]\n                        msg = f\"Column '{col}' contains invalid values. Found: {list(examples)}\"\n                        ctx.debug(\n                            \"ACCEPTED_VALUES check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            examples=list(examples),\n                        )\n                else:\n                    msg = f\"Accepted values check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.RANGE:\n                col = test.column\n                if col in df.columns:\n                    invalid_count = 0\n                    if test.min is not None:\n                        invalid_count += int((df[col] &lt; test.min).sum())\n                    if test.max is not None:\n                        invalid_count += int((df[col] &gt; test.max).sum())\n\n                    if invalid_count &gt; 0:\n                        msg = f\"Column '{col}' contains {invalid_count} values out of range\"\n                        ctx.debug(\n                            \"RANGE check failed\",\n                            column=col,\n                            invalid_count=invalid_count,\n                            min=test.min,\n                            max=test.max,\n                        )\n                else:\n                    msg = f\"Range check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.REGEX_MATCH:\n                col = test.column\n                if col in df.columns:\n                    valid_series = df[col].dropna().astype(str)\n                    if not valid_series.empty:\n                        matches = valid_series.str.match(test.pattern)\n                        invalid_count = int((~matches).sum())\n                        if invalid_count &gt; 0:\n                            msg = (\n                                f\"Column '{col}' contains {invalid_count} values \"\n                                f\"that does not match pattern '{test.pattern}'\"\n                            )\n                            ctx.debug(\n                                \"REGEX_MATCH check failed\",\n                                column=col,\n                                invalid_count=invalid_count,\n                                pattern=test.pattern,\n                            )\n                else:\n                    msg = f\"Regex check failed: Column '{col}' not found\"\n\n            elif test.type == TestType.CUSTOM_SQL:\n                try:\n                    mask = ~df.eval(test.condition)\n                    invalid_count = int(mask.sum())\n                    if invalid_count &gt; 0:\n                        msg = (\n                            f\"Custom check '{getattr(test, 'name', 'custom_sql')}' failed. \"\n                            f\"Found {invalid_count} invalid rows.\"\n                        )\n                        ctx.debug(\n                            \"CUSTOM_SQL check failed\",\n                            condition=test.condition,\n                            invalid_count=invalid_count,\n                        )\n                except Exception as e:\n                    msg = f\"Failed to execute custom SQL '{test.condition}': {e}\"\n                    ctx.error(\n                        \"CUSTOM_SQL execution error\",\n                        condition=test.condition,\n                        error=str(e),\n                    )\n\n            if msg:\n                res = self._handle_failure(msg, test)\n                if res:\n                    failures.append(res)\n                    if fail_fast:\n                        break\n\n        return [f for f in failures if f]\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.engine.Validator.validate","title":"<code>validate(df, config, context=None)</code>","text":"<p>Run validation checks against a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>Spark, Pandas, or Polars DataFrame</p> required <code>config</code> <code>ValidationConfig</code> <p>Validation configuration</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Optional context (e.g. {'columns': ...}) for contracts</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of error messages (empty if all checks pass)</p> Source code in <code>odibi\\validation\\engine.py</code> <pre><code>def validate(\n    self, df: Any, config: ValidationConfig, context: Dict[str, Any] = None\n) -&gt; List[str]:\n    \"\"\"\n    Run validation checks against a DataFrame.\n\n    Args:\n        df: Spark, Pandas, or Polars DataFrame\n        config: Validation configuration\n        context: Optional context (e.g. {'columns': ...}) for contracts\n\n    Returns:\n        List of error messages (empty if all checks pass)\n    \"\"\"\n    ctx = get_logging_context()\n    test_count = len(config.tests)\n    failures = []\n    is_spark = False\n    is_polars = False\n    engine_type = \"pandas\"\n\n    try:\n        import pyspark\n\n        if isinstance(df, pyspark.sql.DataFrame):\n            is_spark = True\n            engine_type = \"spark\"\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n                engine_type = \"polars\"\n        except ImportError:\n            pass\n\n    ctx.debug(\n        \"Starting validation\",\n        test_count=test_count,\n        engine=engine_type,\n        df_type=type(df).__name__,\n        fail_fast=getattr(config, \"fail_fast\", False),\n    )\n\n    if is_spark:\n        failures = self._validate_spark(df, config, context)\n    elif is_polars:\n        failures = self._validate_polars(df, config, context)\n    else:\n        failures = self._validate_pandas(df, config, context)\n\n    tests_passed = test_count - len(failures)\n    ctx.info(\n        \"Validation complete\",\n        total_tests=test_count,\n        tests_passed=tests_passed,\n        tests_failed=len(failures),\n        engine=engine_type,\n    )\n\n    ctx.log_validation_result(\n        passed=len(failures) == 0,\n        rule_name=\"batch_validation\",\n        failures=failures[:5] if failures else None,\n        total_tests=test_count,\n        tests_passed=tests_passed,\n        tests_failed=len(failures),\n    )\n\n    return failures\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.gate","title":"<code>odibi.validation.gate</code>","text":"<p>Quality Gate support for batch-level validation.</p> <p>Gates evaluate the entire batch before writing, ensuring data quality thresholds are met at the aggregate level.</p>"},{"location":"reference/api/validation/#odibi.validation.gate.GateResult","title":"<code>GateResult</code>  <code>dataclass</code>","text":"<p>Result of gate evaluation.</p> Source code in <code>odibi\\validation\\gate.py</code> <pre><code>@dataclass\nclass GateResult:\n    \"\"\"Result of gate evaluation.\"\"\"\n\n    passed: bool\n    pass_rate: float\n    total_rows: int\n    passed_rows: int\n    failed_rows: int\n    details: Dict[str, Any] = field(default_factory=dict)\n    action: GateOnFail = GateOnFail.ABORT\n    failure_reasons: List[str] = field(default_factory=list)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.gate.evaluate_gate","title":"<code>evaluate_gate(df, validation_results, gate_config, engine, catalog=None, node_name=None)</code>","text":"<p>Evaluate quality gate on validation results.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame being validated</p> required <code>validation_results</code> <code>Dict[str, List[bool]]</code> <p>Dict of test_name -&gt; per-row boolean results (True=passed)</p> required <code>gate_config</code> <code>GateConfig</code> <p>Gate configuration</p> required <code>engine</code> <code>Any</code> <p>Engine instance</p> required <code>catalog</code> <code>Optional[Any]</code> <p>Optional CatalogManager for historical row count checks</p> <code>None</code> <code>node_name</code> <code>Optional[str]</code> <p>Optional node name for historical lookups</p> <code>None</code> <p>Returns:</p> Type Description <code>GateResult</code> <p>GateResult with pass/fail status and action to take</p> Source code in <code>odibi\\validation\\gate.py</code> <pre><code>def evaluate_gate(\n    df: Any,\n    validation_results: Dict[str, List[bool]],\n    gate_config: GateConfig,\n    engine: Any,\n    catalog: Optional[Any] = None,\n    node_name: Optional[str] = None,\n) -&gt; GateResult:\n    \"\"\"\n    Evaluate quality gate on validation results.\n\n    Args:\n        df: DataFrame being validated\n        validation_results: Dict of test_name -&gt; per-row boolean results (True=passed)\n        gate_config: Gate configuration\n        engine: Engine instance\n        catalog: Optional CatalogManager for historical row count checks\n        node_name: Optional node name for historical lookups\n\n    Returns:\n        GateResult with pass/fail status and action to take\n    \"\"\"\n    is_spark = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if is_spark:\n        total_rows = df.count()\n    elif hasattr(engine, \"count_rows\"):\n        total_rows = engine.count_rows(df)\n    else:\n        total_rows = len(df)\n\n    if total_rows == 0:\n        return GateResult(\n            passed=True,\n            pass_rate=1.0,\n            total_rows=0,\n            passed_rows=0,\n            failed_rows=0,\n            action=gate_config.on_fail,\n            details={\"message\": \"Empty dataset - gate passed by default\"},\n        )\n\n    passed_rows = total_rows\n    if validation_results:\n        all_pass_mask = None\n        for test_name, results in validation_results.items():\n            if len(results) == total_rows:\n                if all_pass_mask is None:\n                    all_pass_mask = results.copy()\n                else:\n                    all_pass_mask = [a and b for a, b in zip(all_pass_mask, results)]\n\n        if all_pass_mask:\n            passed_rows = sum(all_pass_mask)\n\n    pass_rate = passed_rows / total_rows if total_rows &gt; 0 else 1.0\n    failed_rows = total_rows - passed_rows\n\n    details: Dict[str, Any] = {\n        \"overall_pass_rate\": pass_rate,\n        \"per_test_rates\": {},\n        \"row_count_check\": None,\n    }\n\n    gate_passed = True\n    failure_reasons: List[str] = []\n\n    if pass_rate &lt; gate_config.require_pass_rate:\n        gate_passed = False\n        failure_reasons.append(\n            f\"Overall pass rate {pass_rate:.1%} &lt; required {gate_config.require_pass_rate:.1%}\"\n        )\n\n    for threshold in gate_config.thresholds:\n        test_results = validation_results.get(threshold.test)\n        if test_results:\n            test_total = len(test_results)\n            test_passed = sum(test_results)\n            test_pass_rate = test_passed / test_total if test_total &gt; 0 else 1.0\n            details[\"per_test_rates\"][threshold.test] = test_pass_rate\n\n            if test_pass_rate &lt; threshold.min_pass_rate:\n                gate_passed = False\n                failure_reasons.append(\n                    f\"Test '{threshold.test}' pass rate {test_pass_rate:.1%} \"\n                    f\"&lt; required {threshold.min_pass_rate:.1%}\"\n                )\n\n    if gate_config.row_count:\n        row_check = _check_row_count(\n            total_rows,\n            gate_config.row_count,\n            catalog,\n            node_name,\n        )\n        details[\"row_count_check\"] = row_check\n\n        if not row_check[\"passed\"]:\n            gate_passed = False\n            failure_reasons.append(row_check[\"reason\"])\n\n    details[\"failure_reasons\"] = failure_reasons\n\n    if gate_passed:\n        logger.info(f\"Gate passed: {pass_rate:.1%} pass rate ({passed_rows}/{total_rows} rows)\")\n    else:\n        logger.warning(f\"Gate failed: {', '.join(failure_reasons)}\")\n\n    return GateResult(\n        passed=gate_passed,\n        pass_rate=pass_rate,\n        total_rows=total_rows,\n        passed_rows=passed_rows,\n        failed_rows=failed_rows,\n        details=details,\n        action=gate_config.on_fail,\n        failure_reasons=failure_reasons,\n    )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine","title":"<code>odibi.validation.quarantine</code>","text":"<p>Optimized quarantine table support for routing failed validation rows.</p> <p>Performance optimizations: - Removed per-row test_results lists (O(N*tests) memory savings) - Added sampling/limiting for large invalid sets - Single pass for combined mask evaluation - No unnecessary Python list conversions</p> <p>This module provides functionality to: 1. Split DataFrames into valid and invalid portions based on test results 2. Add metadata columns to quarantined rows 3. Write quarantined rows to a dedicated table (with optional sampling)</p>"},{"location":"reference/api/validation/#odibi.validation.quarantine.QuarantineResult","title":"<code>QuarantineResult</code>  <code>dataclass</code>","text":"<p>Result of quarantine operation.</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>@dataclass\nclass QuarantineResult:\n    \"\"\"Result of quarantine operation.\"\"\"\n\n    valid_df: Any\n    invalid_df: Any\n    rows_quarantined: int\n    rows_valid: int\n    test_results: Dict[str, Dict[str, int]] = field(default_factory=dict)\n    failed_test_details: Dict[int, List[str]] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.add_quarantine_metadata","title":"<code>add_quarantine_metadata(invalid_df, test_results, config, engine, node_name, run_id, tests)</code>","text":"<p>Add metadata columns to quarantined rows.</p> <p>Parameters:</p> Name Type Description Default <code>invalid_df</code> <code>Any</code> <p>DataFrame of invalid rows</p> required <code>test_results</code> <code>Dict[str, Any]</code> <p>Dict of test_name -&gt; aggregate results (not per-row)</p> required <code>config</code> <code>QuarantineColumnsConfig</code> <p>QuarantineColumnsConfig specifying which columns to add</p> required <code>engine</code> <code>Any</code> <p>Engine instance</p> required <code>node_name</code> <code>str</code> <p>Name of the originating node</p> required <code>run_id</code> <code>str</code> <p>Current run ID</p> required <code>tests</code> <code>List[TestConfig]</code> <p>List of test configurations (for building failure reasons)</p> required <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame with added metadata columns</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def add_quarantine_metadata(\n    invalid_df: Any,\n    test_results: Dict[str, Any],\n    config: QuarantineColumnsConfig,\n    engine: Any,\n    node_name: str,\n    run_id: str,\n    tests: List[TestConfig],\n) -&gt; Any:\n    \"\"\"\n    Add metadata columns to quarantined rows.\n\n    Args:\n        invalid_df: DataFrame of invalid rows\n        test_results: Dict of test_name -&gt; aggregate results (not per-row)\n        config: QuarantineColumnsConfig specifying which columns to add\n        engine: Engine instance\n        node_name: Name of the originating node\n        run_id: Current run ID\n        tests: List of test configurations (for building failure reasons)\n\n    Returns:\n        DataFrame with added metadata columns\n    \"\"\"\n    is_spark = False\n    is_polars = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(invalid_df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(invalid_df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n        except ImportError:\n            pass\n\n    rejected_at = datetime.now(timezone.utc).isoformat()\n\n    quarantine_tests = [t for t in tests if t.on_fail == ContractSeverity.QUARANTINE]\n    test_names = [t.name or f\"{t.type.value}\" for t in quarantine_tests]\n    failed_tests_str = \",\".join(test_names)\n    rejection_reason = f\"Failed tests: {failed_tests_str}\"\n\n    if is_spark:\n        from pyspark.sql import functions as F\n\n        result_df = invalid_df\n\n        if config.rejection_reason:\n            result_df = result_df.withColumn(\"_rejection_reason\", F.lit(rejection_reason))\n\n        if config.rejected_at:\n            result_df = result_df.withColumn(\"_rejected_at\", F.lit(rejected_at))\n\n        if config.source_batch_id:\n            result_df = result_df.withColumn(\"_source_batch_id\", F.lit(run_id))\n\n        if config.failed_tests:\n            result_df = result_df.withColumn(\"_failed_tests\", F.lit(failed_tests_str))\n\n        if config.original_node:\n            result_df = result_df.withColumn(\"_original_node\", F.lit(node_name))\n\n        return result_df\n\n    elif is_polars:\n        import polars as pl\n\n        result_df = invalid_df\n\n        if config.rejection_reason:\n            result_df = result_df.with_columns(pl.lit(rejection_reason).alias(\"_rejection_reason\"))\n\n        if config.rejected_at:\n            result_df = result_df.with_columns(pl.lit(rejected_at).alias(\"_rejected_at\"))\n\n        if config.source_batch_id:\n            result_df = result_df.with_columns(pl.lit(run_id).alias(\"_source_batch_id\"))\n\n        if config.failed_tests:\n            result_df = result_df.with_columns(pl.lit(failed_tests_str).alias(\"_failed_tests\"))\n\n        if config.original_node:\n            result_df = result_df.with_columns(pl.lit(node_name).alias(\"_original_node\"))\n\n        return result_df\n\n    else:\n        result_df = invalid_df.copy()\n\n        if config.rejection_reason:\n            result_df[\"_rejection_reason\"] = rejection_reason\n\n        if config.rejected_at:\n            result_df[\"_rejected_at\"] = rejected_at\n\n        if config.source_batch_id:\n            result_df[\"_source_batch_id\"] = run_id\n\n        if config.failed_tests:\n            result_df[\"_failed_tests\"] = failed_tests_str\n\n        if config.original_node:\n            result_df[\"_original_node\"] = node_name\n\n        return result_df\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.has_quarantine_tests","title":"<code>has_quarantine_tests(tests)</code>","text":"<p>Check if any tests use quarantine severity.</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def has_quarantine_tests(tests: List[TestConfig]) -&gt; bool:\n    \"\"\"Check if any tests use quarantine severity.\"\"\"\n    return any(t.on_fail == ContractSeverity.QUARANTINE for t in tests)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.split_valid_invalid","title":"<code>split_valid_invalid(df, tests, engine)</code>","text":"<p>Split DataFrame into valid and invalid portions based on quarantine tests.</p> <p>Only tests with on_fail == QUARANTINE are evaluated for splitting. A row is invalid if it fails ANY quarantine test.</p> <p>Performance: Removed per-row test_results lists to save O(N*tests) memory. Now stores only aggregate counts per test.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame to split</p> required <code>tests</code> <code>List[TestConfig]</code> <p>List of test configurations</p> required <code>engine</code> <code>Any</code> <p>Engine instance (Spark, Pandas, or Polars)</p> required <p>Returns:</p> Type Description <code>QuarantineResult</code> <p>QuarantineResult with valid_df, invalid_df, and test metadata</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def split_valid_invalid(\n    df: Any,\n    tests: List[TestConfig],\n    engine: Any,\n) -&gt; QuarantineResult:\n    \"\"\"\n    Split DataFrame into valid and invalid portions based on quarantine tests.\n\n    Only tests with on_fail == QUARANTINE are evaluated for splitting.\n    A row is invalid if it fails ANY quarantine test.\n\n    Performance: Removed per-row test_results lists to save O(N*tests) memory.\n    Now stores only aggregate counts per test.\n\n    Args:\n        df: DataFrame to split\n        tests: List of test configurations\n        engine: Engine instance (Spark, Pandas, or Polars)\n\n    Returns:\n        QuarantineResult with valid_df, invalid_df, and test metadata\n    \"\"\"\n    is_spark = False\n    is_polars = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n        except ImportError:\n            pass\n\n    quarantine_tests = [t for t in tests if t.on_fail == ContractSeverity.QUARANTINE]\n\n    if not quarantine_tests:\n        if is_spark:\n            from pyspark.sql import functions as F\n\n            empty_df = df.filter(F.lit(False))\n        elif is_polars:\n            import polars as pl\n\n            empty_df = df.filter(pl.lit(False))\n        else:\n            empty_df = df.iloc[0:0].copy()\n\n        row_count = engine.count_rows(df) if hasattr(engine, \"count_rows\") else len(df)\n        return QuarantineResult(\n            valid_df=df,\n            invalid_df=empty_df,\n            rows_quarantined=0,\n            rows_valid=row_count,\n            test_results={},\n            failed_test_details={},\n        )\n\n    test_masks = {}\n    test_names = []\n\n    for idx, test in enumerate(quarantine_tests):\n        base_name = test.name or f\"{test.type.value}\"\n        test_name = base_name if base_name not in test_masks else f\"{base_name}_{idx}\"\n        test_names.append(test_name)\n        mask = _evaluate_test_mask(df, test, is_spark, is_polars)\n        test_masks[test_name] = mask\n\n    if is_spark:\n        from pyspark.sql import functions as F\n\n        combined_valid_mask = F.lit(True)\n        for mask in test_masks.values():\n            combined_valid_mask = combined_valid_mask &amp; mask\n\n        df_cached = df.cache()\n\n        valid_df = df_cached.filter(combined_valid_mask)\n        invalid_df = df_cached.filter(~combined_valid_mask)\n\n        valid_df = valid_df.cache()\n        invalid_df = invalid_df.cache()\n\n        rows_valid = valid_df.count()\n        rows_quarantined = invalid_df.count()\n        total = rows_valid + rows_quarantined\n\n        test_results = {}\n        for name, mask in test_masks.items():\n            pass_count = df_cached.filter(mask).count()\n            fail_count = total - pass_count\n            test_results[name] = {\"pass_count\": pass_count, \"fail_count\": fail_count}\n\n        df_cached.unpersist()\n\n    elif is_polars:\n        import polars as pl\n\n        combined_valid_mask = pl.lit(True)\n        for mask in test_masks.values():\n            combined_valid_mask = combined_valid_mask &amp; mask\n\n        valid_df = df.filter(combined_valid_mask)\n        invalid_df = df.filter(~combined_valid_mask)\n\n        rows_valid = len(valid_df)\n        rows_quarantined = len(invalid_df)\n\n        test_results = {}\n\n    else:\n        import pandas as pd\n\n        combined_valid_mask = pd.Series([True] * len(df), index=df.index)\n        for mask in test_masks.values():\n            combined_valid_mask = combined_valid_mask &amp; mask\n\n        valid_df = df[combined_valid_mask].copy()\n        invalid_df = df[~combined_valid_mask].copy()\n\n        rows_valid = len(valid_df)\n        rows_quarantined = len(invalid_df)\n\n        test_results = {}\n        for name, mask in test_masks.items():\n            pass_count = int(mask.sum())\n            fail_count = len(df) - pass_count\n            test_results[name] = {\"pass_count\": pass_count, \"fail_count\": fail_count}\n\n    logger.info(f\"Quarantine split: {rows_valid} valid, {rows_quarantined} invalid\")\n\n    return QuarantineResult(\n        valid_df=valid_df,\n        invalid_df=invalid_df,\n        rows_quarantined=rows_quarantined,\n        rows_valid=rows_valid,\n        test_results=test_results,\n        failed_test_details={},\n    )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.quarantine.write_quarantine","title":"<code>write_quarantine(invalid_df, config, engine, connections)</code>","text":"<p>Write quarantined rows to destination (always append mode).</p> <p>Supports optional sampling/limiting via config.max_rows and config.sample_fraction.</p> <p>Parameters:</p> Name Type Description Default <code>invalid_df</code> <code>Any</code> <p>DataFrame of invalid rows with metadata</p> required <code>config</code> <code>QuarantineConfig</code> <p>QuarantineConfig specifying destination and sampling options</p> required <code>engine</code> <code>Any</code> <p>Engine instance</p> required <code>connections</code> <code>Dict[str, Any]</code> <p>Dict of connection configurations</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with write result metadata</p> Source code in <code>odibi\\validation\\quarantine.py</code> <pre><code>def write_quarantine(\n    invalid_df: Any,\n    config: QuarantineConfig,\n    engine: Any,\n    connections: Dict[str, Any],\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Write quarantined rows to destination (always append mode).\n\n    Supports optional sampling/limiting via config.max_rows and config.sample_fraction.\n\n    Args:\n        invalid_df: DataFrame of invalid rows with metadata\n        config: QuarantineConfig specifying destination and sampling options\n        engine: Engine instance\n        connections: Dict of connection configurations\n\n    Returns:\n        Dict with write result metadata\n    \"\"\"\n    is_spark = False\n    is_polars = False\n\n    try:\n        import pyspark\n\n        if hasattr(engine, \"spark\") or isinstance(invalid_df, pyspark.sql.DataFrame):\n            is_spark = True\n    except ImportError:\n        pass\n\n    if not is_spark:\n        try:\n            import polars as pl\n\n            if isinstance(invalid_df, (pl.DataFrame, pl.LazyFrame)):\n                is_polars = True\n        except ImportError:\n            pass\n\n    invalid_df = _apply_sampling(invalid_df, config, is_spark, is_polars)\n\n    if is_spark:\n        row_count = invalid_df.count()\n    elif is_polars:\n        row_count = len(invalid_df)\n    else:\n        row_count = len(invalid_df)\n\n    if row_count == 0:\n        return {\n            \"rows_quarantined\": 0,\n            \"quarantine_path\": config.path or config.table,\n            \"write_info\": None,\n        }\n\n    connection = connections.get(config.connection)\n    if connection is None:\n        raise ValueError(\n            f\"Quarantine connection '{config.connection}' not found. \"\n            f\"Available: {', '.join(connections.keys())}\"\n        )\n\n    try:\n        write_result = engine.write(\n            invalid_df,\n            connection=connection,\n            format=\"delta\" if config.table else \"parquet\",\n            path=config.path,\n            table=config.table,\n            mode=\"append\",\n        )\n    except Exception as e:\n        logger.error(f\"Failed to write quarantine data: {e}\")\n        raise\n\n    logger.info(f\"Wrote {row_count} rows to quarantine: {config.path or config.table}\")\n\n    return {\n        \"rows_quarantined\": row_count,\n        \"quarantine_path\": config.path or config.table,\n        \"write_info\": write_result,\n    }\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk","title":"<code>odibi.validation.fk</code>","text":""},{"location":"reference/api/validation/#odibi.validation.fk--foreign-key-validation-module","title":"Foreign Key Validation Module","text":"<p>Declare and validate referential integrity between fact and dimension tables.</p> <p>Features: - Declare relationships in YAML - Validate referential integrity on fact load - Detect orphan records - Generate lineage from relationships - Integration with FactPattern</p> Example Config <p>relationships:   - name: orders_to_customers     fact: fact_orders     dimension: dim_customer     fact_key: customer_sk     dimension_key: customer_sk</p> <ul> <li>name: orders_to_products     fact: fact_orders     dimension: dim_product     fact_key: product_sk     dimension_key: product_sk</li> </ul>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidationReport","title":"<code>FKValidationReport</code>  <code>dataclass</code>","text":"<p>Complete FK validation report for a fact table.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>@dataclass\nclass FKValidationReport:\n    \"\"\"Complete FK validation report for a fact table.\"\"\"\n\n    fact_table: str\n    all_valid: bool\n    total_relationships: int\n    valid_relationships: int\n    results: List[FKValidationResult] = field(default_factory=list)\n    orphan_records: List[OrphanRecord] = field(default_factory=list)\n    elapsed_ms: float = 0.0\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidationResult","title":"<code>FKValidationResult</code>  <code>dataclass</code>","text":"<p>Result of FK validation.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>@dataclass\nclass FKValidationResult:\n    \"\"\"Result of FK validation.\"\"\"\n\n    relationship_name: str\n    valid: bool\n    total_rows: int\n    orphan_count: int\n    null_count: int\n    orphan_values: List[Any] = field(default_factory=list)\n    elapsed_ms: float = 0.0\n    error: Optional[str] = None\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator","title":"<code>FKValidator</code>","text":"<p>Validate foreign key relationships between fact and dimension tables.</p> Usage <p>registry = RelationshipRegistry(relationships=[...]) validator = FKValidator(registry) report = validator.validate_fact(fact_df, \"fact_orders\", context)</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>class FKValidator:\n    \"\"\"\n    Validate foreign key relationships between fact and dimension tables.\n\n    Usage:\n        registry = RelationshipRegistry(relationships=[...])\n        validator = FKValidator(registry)\n        report = validator.validate_fact(fact_df, \"fact_orders\", context)\n    \"\"\"\n\n    def __init__(self, registry: RelationshipRegistry):\n        \"\"\"\n        Initialize with relationship registry.\n\n        Args:\n            registry: RelationshipRegistry with relationship definitions\n        \"\"\"\n        self.registry = registry\n\n    def validate_relationship(\n        self,\n        fact_df: Any,\n        relationship: RelationshipConfig,\n        context: EngineContext,\n    ) -&gt; FKValidationResult:\n        \"\"\"\n        Validate a single FK relationship.\n\n        Args:\n            fact_df: Fact DataFrame to validate\n            relationship: Relationship configuration\n            context: EngineContext with dimension data\n\n        Returns:\n            FKValidationResult with validation details\n        \"\"\"\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        ctx.debug(\n            \"Validating FK relationship\",\n            relationship=relationship.name,\n            fact=relationship.fact,\n            dimension=relationship.dimension,\n        )\n\n        try:\n            dim_df = context.get(relationship.dimension)\n        except KeyError:\n            elapsed_ms = (time.time() - start_time) * 1000\n            return FKValidationResult(\n                relationship_name=relationship.name,\n                valid=False,\n                total_rows=0,\n                orphan_count=0,\n                null_count=0,\n                elapsed_ms=elapsed_ms,\n                error=f\"Dimension table '{relationship.dimension}' not found\",\n            )\n\n        try:\n            if context.engine_type == EngineType.SPARK:\n                result = self._validate_spark(fact_df, dim_df, relationship)\n            else:\n                result = self._validate_pandas(fact_df, dim_df, relationship)\n\n            elapsed_ms = (time.time() - start_time) * 1000\n            result.elapsed_ms = elapsed_ms\n\n            if result.valid:\n                ctx.debug(\n                    \"FK validation passed\",\n                    relationship=relationship.name,\n                    total_rows=result.total_rows,\n                )\n            else:\n                ctx.warning(\n                    \"FK validation failed\",\n                    relationship=relationship.name,\n                    orphan_count=result.orphan_count,\n                    null_count=result.null_count,\n                )\n\n            return result\n\n        except Exception as e:\n            elapsed_ms = (time.time() - start_time) * 1000\n            ctx.error(\n                f\"FK validation error: {e}\",\n                relationship=relationship.name,\n            )\n            return FKValidationResult(\n                relationship_name=relationship.name,\n                valid=False,\n                total_rows=0,\n                orphan_count=0,\n                null_count=0,\n                elapsed_ms=elapsed_ms,\n                error=str(e),\n            )\n\n    def _validate_spark(\n        self,\n        fact_df: Any,\n        dim_df: Any,\n        relationship: RelationshipConfig,\n    ) -&gt; FKValidationResult:\n        \"\"\"Validate using Spark.\"\"\"\n        from pyspark.sql import functions as F\n\n        fk_col = relationship.fact_key\n        dk_col = relationship.dimension_key\n\n        total_rows = fact_df.count()\n\n        null_count = fact_df.filter(F.col(fk_col).isNull()).count()\n\n        dim_keys = dim_df.select(F.col(dk_col).alias(\"_dim_key\")).distinct()\n\n        non_null_facts = fact_df.filter(F.col(fk_col).isNotNull())\n        orphans = non_null_facts.join(\n            dim_keys,\n            non_null_facts[fk_col] == dim_keys[\"_dim_key\"],\n            \"left_anti\",\n        )\n\n        orphan_count = orphans.count()\n\n        orphan_values = []\n        if orphan_count &gt; 0 and orphan_count &lt;= 100:\n            orphan_values = [\n                row[fk_col] for row in orphans.select(fk_col).distinct().limit(100).collect()\n            ]\n\n        is_valid = orphan_count == 0 and (relationship.nullable or null_count == 0)\n\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=is_valid,\n            total_rows=total_rows,\n            orphan_count=orphan_count,\n            null_count=null_count,\n            orphan_values=orphan_values,\n        )\n\n    def _validate_pandas(\n        self,\n        fact_df: Any,\n        dim_df: Any,\n        relationship: RelationshipConfig,\n    ) -&gt; FKValidationResult:\n        \"\"\"Validate using Pandas.\"\"\"\n\n        fk_col = relationship.fact_key\n        dk_col = relationship.dimension_key\n\n        total_rows = len(fact_df)\n\n        null_count = int(fact_df[fk_col].isna().sum())\n\n        dim_keys = set(dim_df[dk_col].dropna().unique())\n\n        non_null_fks = fact_df[fk_col].dropna()\n        orphan_mask = ~non_null_fks.isin(dim_keys)\n        orphan_count = int(orphan_mask.sum())\n\n        orphan_values = []\n        if orphan_count &gt; 0:\n            orphan_values = list(non_null_fks[orphan_mask].unique()[:100])\n\n        is_valid = orphan_count == 0 and (relationship.nullable or null_count == 0)\n\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=is_valid,\n            total_rows=total_rows,\n            orphan_count=orphan_count,\n            null_count=null_count,\n            orphan_values=orphan_values,\n        )\n\n    def validate_fact(\n        self,\n        fact_df: Any,\n        fact_table: str,\n        context: EngineContext,\n    ) -&gt; FKValidationReport:\n        \"\"\"\n        Validate all FK relationships for a fact table.\n\n        Args:\n            fact_df: Fact DataFrame to validate\n            fact_table: Fact table name\n            context: EngineContext with dimension data\n\n        Returns:\n            FKValidationReport with all validation results\n        \"\"\"\n        ctx = get_logging_context()\n        start_time = time.time()\n\n        ctx.info(\"Starting FK validation\", fact_table=fact_table)\n\n        relationships = self.registry.get_fact_relationships(fact_table)\n\n        if not relationships:\n            ctx.warning(\n                \"No FK relationships defined\",\n                fact_table=fact_table,\n            )\n            return FKValidationReport(\n                fact_table=fact_table,\n                all_valid=True,\n                total_relationships=0,\n                valid_relationships=0,\n                elapsed_ms=(time.time() - start_time) * 1000,\n            )\n\n        results = []\n        all_orphans = []\n\n        for relationship in relationships:\n            result = self.validate_relationship(fact_df, relationship, context)\n            results.append(result)\n\n            if result.orphan_count &gt; 0:\n                for orphan_val in result.orphan_values:\n                    all_orphans.append(\n                        OrphanRecord(\n                            fact_key_value=orphan_val,\n                            fact_key_column=relationship.fact_key,\n                            dimension_table=relationship.dimension,\n                        )\n                    )\n\n        all_valid = all(r.valid for r in results)\n        valid_count = sum(1 for r in results if r.valid)\n        elapsed_ms = (time.time() - start_time) * 1000\n\n        if all_valid:\n            ctx.info(\n                \"FK validation passed\",\n                fact_table=fact_table,\n                relationships=len(relationships),\n            )\n        else:\n            ctx.warning(\n                \"FK validation failed\",\n                fact_table=fact_table,\n                valid=valid_count,\n                total=len(relationships),\n            )\n\n        return FKValidationReport(\n            fact_table=fact_table,\n            all_valid=all_valid,\n            total_relationships=len(relationships),\n            valid_relationships=valid_count,\n            results=results,\n            orphan_records=all_orphans,\n            elapsed_ms=elapsed_ms,\n        )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator.__init__","title":"<code>__init__(registry)</code>","text":"<p>Initialize with relationship registry.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>RelationshipRegistry</code> <p>RelationshipRegistry with relationship definitions</p> required Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def __init__(self, registry: RelationshipRegistry):\n    \"\"\"\n    Initialize with relationship registry.\n\n    Args:\n        registry: RelationshipRegistry with relationship definitions\n    \"\"\"\n    self.registry = registry\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator.validate_fact","title":"<code>validate_fact(fact_df, fact_table, context)</code>","text":"<p>Validate all FK relationships for a fact table.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame to validate</p> required <code>fact_table</code> <code>str</code> <p>Fact table name</p> required <code>context</code> <code>EngineContext</code> <p>EngineContext with dimension data</p> required <p>Returns:</p> Type Description <code>FKValidationReport</code> <p>FKValidationReport with all validation results</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def validate_fact(\n    self,\n    fact_df: Any,\n    fact_table: str,\n    context: EngineContext,\n) -&gt; FKValidationReport:\n    \"\"\"\n    Validate all FK relationships for a fact table.\n\n    Args:\n        fact_df: Fact DataFrame to validate\n        fact_table: Fact table name\n        context: EngineContext with dimension data\n\n    Returns:\n        FKValidationReport with all validation results\n    \"\"\"\n    ctx = get_logging_context()\n    start_time = time.time()\n\n    ctx.info(\"Starting FK validation\", fact_table=fact_table)\n\n    relationships = self.registry.get_fact_relationships(fact_table)\n\n    if not relationships:\n        ctx.warning(\n            \"No FK relationships defined\",\n            fact_table=fact_table,\n        )\n        return FKValidationReport(\n            fact_table=fact_table,\n            all_valid=True,\n            total_relationships=0,\n            valid_relationships=0,\n            elapsed_ms=(time.time() - start_time) * 1000,\n        )\n\n    results = []\n    all_orphans = []\n\n    for relationship in relationships:\n        result = self.validate_relationship(fact_df, relationship, context)\n        results.append(result)\n\n        if result.orphan_count &gt; 0:\n            for orphan_val in result.orphan_values:\n                all_orphans.append(\n                    OrphanRecord(\n                        fact_key_value=orphan_val,\n                        fact_key_column=relationship.fact_key,\n                        dimension_table=relationship.dimension,\n                    )\n                )\n\n    all_valid = all(r.valid for r in results)\n    valid_count = sum(1 for r in results if r.valid)\n    elapsed_ms = (time.time() - start_time) * 1000\n\n    if all_valid:\n        ctx.info(\n            \"FK validation passed\",\n            fact_table=fact_table,\n            relationships=len(relationships),\n        )\n    else:\n        ctx.warning(\n            \"FK validation failed\",\n            fact_table=fact_table,\n            valid=valid_count,\n            total=len(relationships),\n        )\n\n    return FKValidationReport(\n        fact_table=fact_table,\n        all_valid=all_valid,\n        total_relationships=len(relationships),\n        valid_relationships=valid_count,\n        results=results,\n        orphan_records=all_orphans,\n        elapsed_ms=elapsed_ms,\n    )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.FKValidator.validate_relationship","title":"<code>validate_relationship(fact_df, relationship, context)</code>","text":"<p>Validate a single FK relationship.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame to validate</p> required <code>relationship</code> <code>RelationshipConfig</code> <p>Relationship configuration</p> required <code>context</code> <code>EngineContext</code> <p>EngineContext with dimension data</p> required <p>Returns:</p> Type Description <code>FKValidationResult</code> <p>FKValidationResult with validation details</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def validate_relationship(\n    self,\n    fact_df: Any,\n    relationship: RelationshipConfig,\n    context: EngineContext,\n) -&gt; FKValidationResult:\n    \"\"\"\n    Validate a single FK relationship.\n\n    Args:\n        fact_df: Fact DataFrame to validate\n        relationship: Relationship configuration\n        context: EngineContext with dimension data\n\n    Returns:\n        FKValidationResult with validation details\n    \"\"\"\n    ctx = get_logging_context()\n    start_time = time.time()\n\n    ctx.debug(\n        \"Validating FK relationship\",\n        relationship=relationship.name,\n        fact=relationship.fact,\n        dimension=relationship.dimension,\n    )\n\n    try:\n        dim_df = context.get(relationship.dimension)\n    except KeyError:\n        elapsed_ms = (time.time() - start_time) * 1000\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=False,\n            total_rows=0,\n            orphan_count=0,\n            null_count=0,\n            elapsed_ms=elapsed_ms,\n            error=f\"Dimension table '{relationship.dimension}' not found\",\n        )\n\n    try:\n        if context.engine_type == EngineType.SPARK:\n            result = self._validate_spark(fact_df, dim_df, relationship)\n        else:\n            result = self._validate_pandas(fact_df, dim_df, relationship)\n\n        elapsed_ms = (time.time() - start_time) * 1000\n        result.elapsed_ms = elapsed_ms\n\n        if result.valid:\n            ctx.debug(\n                \"FK validation passed\",\n                relationship=relationship.name,\n                total_rows=result.total_rows,\n            )\n        else:\n            ctx.warning(\n                \"FK validation failed\",\n                relationship=relationship.name,\n                orphan_count=result.orphan_count,\n                null_count=result.null_count,\n            )\n\n        return result\n\n    except Exception as e:\n        elapsed_ms = (time.time() - start_time) * 1000\n        ctx.error(\n            f\"FK validation error: {e}\",\n            relationship=relationship.name,\n        )\n        return FKValidationResult(\n            relationship_name=relationship.name,\n            valid=False,\n            total_rows=0,\n            orphan_count=0,\n            null_count=0,\n            elapsed_ms=elapsed_ms,\n            error=str(e),\n        )\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.OrphanRecord","title":"<code>OrphanRecord</code>  <code>dataclass</code>","text":"<p>Details of an orphan record.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>@dataclass\nclass OrphanRecord:\n    \"\"\"Details of an orphan record.\"\"\"\n\n    fact_key_value: Any\n    fact_key_column: str\n    dimension_table: str\n    row_index: Optional[int] = None\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipConfig","title":"<code>RelationshipConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a foreign key relationship.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique relationship identifier</p> <code>fact</code> <code>str</code> <p>Fact table name</p> <code>dimension</code> <code>str</code> <p>Dimension table name</p> <code>fact_key</code> <code>str</code> <p>Foreign key column in fact table</p> <code>dimension_key</code> <code>str</code> <p>Primary/surrogate key column in dimension</p> <code>nullable</code> <code>bool</code> <p>Whether nulls are allowed in fact_key</p> <code>on_violation</code> <code>str</code> <p>Action on violation (\"warn\", \"error\", \"quarantine\")</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>class RelationshipConfig(BaseModel):\n    \"\"\"\n    Configuration for a foreign key relationship.\n\n    Attributes:\n        name: Unique relationship identifier\n        fact: Fact table name\n        dimension: Dimension table name\n        fact_key: Foreign key column in fact table\n        dimension_key: Primary/surrogate key column in dimension\n        nullable: Whether nulls are allowed in fact_key\n        on_violation: Action on violation (\"warn\", \"error\", \"quarantine\")\n    \"\"\"\n\n    name: str = Field(..., description=\"Unique relationship identifier\")\n    fact: str = Field(..., description=\"Fact table name\")\n    dimension: str = Field(..., description=\"Dimension table name\")\n    fact_key: str = Field(..., description=\"FK column in fact table\")\n    dimension_key: str = Field(..., description=\"PK/SK column in dimension\")\n    nullable: bool = Field(default=False, description=\"Allow nulls in fact_key\")\n    on_violation: str = Field(default=\"error\", description=\"Action on violation\")\n\n    @field_validator(\"name\", \"fact\", \"dimension\", \"fact_key\", \"dimension_key\")\n    @classmethod\n    def validate_not_empty(cls, v: str, info) -&gt; str:\n        if not v or not v.strip():\n            raise ValueError(\n                f\"RelationshipConfig.{info.field_name} cannot be empty. \"\n                f\"Got: {v!r}. Provide a non-empty string value.\"\n            )\n        return v.strip()\n\n    @field_validator(\"on_violation\")\n    @classmethod\n    def validate_on_violation(cls, v: str) -&gt; str:\n        valid = (\"warn\", \"error\", \"quarantine\")\n        if v.lower() not in valid:\n            raise ValueError(f\"Invalid on_violation value. Expected one of {valid}, got: {v!r}.\")\n        return v.lower()\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry","title":"<code>RelationshipRegistry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Registry of all declared relationships.</p> <p>Attributes:</p> Name Type Description <code>relationships</code> <code>List[RelationshipConfig]</code> <p>List of relationship configurations</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>class RelationshipRegistry(BaseModel):\n    \"\"\"\n    Registry of all declared relationships.\n\n    Attributes:\n        relationships: List of relationship configurations\n    \"\"\"\n\n    relationships: List[RelationshipConfig] = Field(\n        default_factory=list, description=\"Relationship definitions\"\n    )\n\n    def get_relationship(self, name: str) -&gt; Optional[RelationshipConfig]:\n        \"\"\"Get a relationship by name.\"\"\"\n        for rel in self.relationships:\n            if rel.name.lower() == name.lower():\n                return rel\n        return None\n\n    def get_fact_relationships(self, fact_table: str) -&gt; List[RelationshipConfig]:\n        \"\"\"Get all relationships for a fact table.\"\"\"\n        return [rel for rel in self.relationships if rel.fact.lower() == fact_table.lower()]\n\n    def get_dimension_relationships(self, dim_table: str) -&gt; List[RelationshipConfig]:\n        \"\"\"Get all relationships referencing a dimension.\"\"\"\n        return [rel for rel in self.relationships if rel.dimension.lower() == dim_table.lower()]\n\n    def generate_lineage(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        Generate lineage map from relationships.\n\n        Returns:\n            Dict mapping fact tables to their dimension dependencies\n        \"\"\"\n        lineage: Dict[str, List[str]] = {}\n        for rel in self.relationships:\n            if rel.fact not in lineage:\n                lineage[rel.fact] = []\n            if rel.dimension not in lineage[rel.fact]:\n                lineage[rel.fact].append(rel.dimension)\n        return lineage\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.generate_lineage","title":"<code>generate_lineage()</code>","text":"<p>Generate lineage map from relationships.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict mapping fact tables to their dimension dependencies</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def generate_lineage(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    Generate lineage map from relationships.\n\n    Returns:\n        Dict mapping fact tables to their dimension dependencies\n    \"\"\"\n    lineage: Dict[str, List[str]] = {}\n    for rel in self.relationships:\n        if rel.fact not in lineage:\n            lineage[rel.fact] = []\n        if rel.dimension not in lineage[rel.fact]:\n            lineage[rel.fact].append(rel.dimension)\n    return lineage\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.get_dimension_relationships","title":"<code>get_dimension_relationships(dim_table)</code>","text":"<p>Get all relationships referencing a dimension.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_dimension_relationships(self, dim_table: str) -&gt; List[RelationshipConfig]:\n    \"\"\"Get all relationships referencing a dimension.\"\"\"\n    return [rel for rel in self.relationships if rel.dimension.lower() == dim_table.lower()]\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.get_fact_relationships","title":"<code>get_fact_relationships(fact_table)</code>","text":"<p>Get all relationships for a fact table.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_fact_relationships(self, fact_table: str) -&gt; List[RelationshipConfig]:\n    \"\"\"Get all relationships for a fact table.\"\"\"\n    return [rel for rel in self.relationships if rel.fact.lower() == fact_table.lower()]\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.RelationshipRegistry.get_relationship","title":"<code>get_relationship(name)</code>","text":"<p>Get a relationship by name.</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_relationship(self, name: str) -&gt; Optional[RelationshipConfig]:\n    \"\"\"Get a relationship by name.\"\"\"\n    for rel in self.relationships:\n        if rel.name.lower() == name.lower():\n            return rel\n    return None\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.get_orphan_records","title":"<code>get_orphan_records(fact_df, relationship, dim_df, engine_type)</code>","text":"<p>Extract orphan records from a fact table.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame</p> required <code>relationship</code> <code>RelationshipConfig</code> <p>Relationship configuration</p> required <code>dim_df</code> <code>Any</code> <p>Dimension DataFrame</p> required <code>engine_type</code> <code>EngineType</code> <p>Engine type (SPARK or PANDAS)</p> required <p>Returns:</p> Type Description <code>Any</code> <p>DataFrame containing orphan records</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def get_orphan_records(\n    fact_df: Any,\n    relationship: RelationshipConfig,\n    dim_df: Any,\n    engine_type: EngineType,\n) -&gt; Any:\n    \"\"\"\n    Extract orphan records from a fact table.\n\n    Args:\n        fact_df: Fact DataFrame\n        relationship: Relationship configuration\n        dim_df: Dimension DataFrame\n        engine_type: Engine type (SPARK or PANDAS)\n\n    Returns:\n        DataFrame containing orphan records\n    \"\"\"\n    fk_col = relationship.fact_key\n    dk_col = relationship.dimension_key\n\n    if engine_type == EngineType.SPARK:\n        from pyspark.sql import functions as F\n\n        dim_keys = dim_df.select(F.col(dk_col).alias(\"_dim_key\")).distinct()\n        non_null_facts = fact_df.filter(F.col(fk_col).isNotNull())\n        orphans = non_null_facts.join(\n            dim_keys,\n            non_null_facts[fk_col] == dim_keys[\"_dim_key\"],\n            \"left_anti\",\n        )\n        return orphans\n    else:\n        dim_keys = set(dim_df[dk_col].dropna().unique())\n        non_null_mask = fact_df[fk_col].notna()\n        orphan_mask = ~fact_df[fk_col].isin(dim_keys) &amp; non_null_mask\n        return fact_df[orphan_mask].copy()\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.parse_relationships_config","title":"<code>parse_relationships_config(config_dict)</code>","text":"<p>Parse relationships from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>Dict[str, Any]</code> <p>Config dict with \"relationships\" key</p> required <p>Returns:</p> Type Description <code>RelationshipRegistry</code> <p>RelationshipRegistry instance</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def parse_relationships_config(config_dict: Dict[str, Any]) -&gt; RelationshipRegistry:\n    \"\"\"\n    Parse relationships from a configuration dictionary.\n\n    Args:\n        config_dict: Config dict with \"relationships\" key\n\n    Returns:\n        RelationshipRegistry instance\n    \"\"\"\n    relationships = []\n    for rel_dict in config_dict.get(\"relationships\", []):\n        relationships.append(RelationshipConfig(**rel_dict))\n    return RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"reference/api/validation/#odibi.validation.fk.validate_fk_on_load","title":"<code>validate_fk_on_load(fact_df, relationships, context, on_failure='error')</code>","text":"<p>Validate FK constraints and optionally filter orphans.</p> <p>This is a convenience function for use in FactPattern.</p> <p>Parameters:</p> Name Type Description Default <code>fact_df</code> <code>Any</code> <p>Fact DataFrame to validate</p> required <code>relationships</code> <code>List[RelationshipConfig]</code> <p>List of relationship configs</p> required <code>context</code> <code>EngineContext</code> <p>EngineContext with dimension data</p> required <code>on_failure</code> <code>str</code> <p>Action on failure (\"error\", \"warn\", \"filter\")</p> <code>'error'</code> <p>Returns:</p> Type Description <code>Any</code> <p>fact_df (possibly filtered if on_failure=\"filter\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If on_failure=\"error\" and validation fails</p> Source code in <code>odibi\\validation\\fk.py</code> <pre><code>def validate_fk_on_load(\n    fact_df: Any,\n    relationships: List[RelationshipConfig],\n    context: EngineContext,\n    on_failure: str = \"error\",\n) -&gt; Any:\n    \"\"\"\n    Validate FK constraints and optionally filter orphans.\n\n    This is a convenience function for use in FactPattern.\n\n    Args:\n        fact_df: Fact DataFrame to validate\n        relationships: List of relationship configs\n        context: EngineContext with dimension data\n        on_failure: Action on failure (\"error\", \"warn\", \"filter\")\n\n    Returns:\n        fact_df (possibly filtered if on_failure=\"filter\")\n\n    Raises:\n        ValueError: If on_failure=\"error\" and validation fails\n    \"\"\"\n    ctx = get_logging_context()\n\n    registry = RelationshipRegistry(relationships=relationships)\n    validator = FKValidator(registry)\n\n    for rel in relationships:\n        result = validator.validate_relationship(fact_df, rel, context)\n\n        if not result.valid:\n            if on_failure == \"error\":\n                raise ValueError(\n                    f\"FK validation failed for '{rel.name}': \"\n                    f\"{result.orphan_count} orphans, {result.null_count} nulls. \"\n                    f\"Sample orphan values: {result.orphan_values[:5]}\"\n                )\n            elif on_failure == \"warn\":\n                ctx.warning(\n                    f\"FK validation warning for '{rel.name}': \"\n                    f\"{result.orphan_count} orphans, {result.null_count} nulls\"\n                )\n            elif on_failure == \"filter\":\n                try:\n                    dim_df = context.get(rel.dimension)\n                except KeyError:\n                    continue\n\n                if context.engine_type == EngineType.SPARK:\n                    from pyspark.sql import functions as F\n\n                    dim_keys = dim_df.select(F.col(rel.dimension_key).alias(\"_fk_key\")).distinct()\n                    fact_df = fact_df.join(\n                        dim_keys,\n                        fact_df[rel.fact_key] == dim_keys[\"_fk_key\"],\n                        \"inner\",\n                    ).drop(\"_fk_key\")\n                else:\n                    dim_keys = set(dim_df[rel.dimension_key].dropna().unique())\n                    fact_df = fact_df[fact_df[rel.fact_key].isin(dim_keys)].copy()\n\n                ctx.info(\n                    f\"Filtered orphans for '{rel.name}'\",\n                    remaining_rows=len(fact_df) if hasattr(fact_df, \"__len__\") else \"N/A\",\n                )\n\n    return fact_df\n</code></pre>"},{"location":"semantics/","title":"Semantic Layer","text":"<p>The Odibi Semantic Layer provides a unified interface for defining and querying business metrics. Define metrics once, query them across any dimension combination.</p>"},{"location":"semantics/#how-it-fits-into-odibi","title":"How It Fits Into Odibi","text":"<p>The semantic layer is a separate module from the core pipeline YAML. It's designed for:</p> <ol> <li>Ad-hoc metric queries via Python API</li> <li>Scheduled metric materialization to pre-compute aggregates</li> <li>Self-service analytics where business users query by metric name</li> </ol> <p>It does NOT replace the pipeline YAML - instead, it works alongside it: - Pipelines build your fact and dimension tables - The semantic layer queries those tables using metric definitions</p> <pre><code>flowchart TB\n    subgraph Pipeline[\"Odibi Pipelines (YAML)\"]\n        R[Read Sources]\n        D[Build Dimensions]\n        F[Build Facts]\n        A[Build Aggregates]\n    end\n\n    subgraph Data[\"Data Layer\"]\n        DIM[(dim_customer&lt;br&gt;dim_product&lt;br&gt;dim_date)]\n        FACT[(fact_orders)]\n        AGG[(agg_daily_sales)]\n    end\n\n    subgraph Semantic[\"Semantic Layer (Python API)\"]\n        M[Metric Definitions]\n        Q[SemanticQuery]\n        MAT[Materializer]\n    end\n\n    subgraph Output[\"Output\"]\n        DF[DataFrame Results]\n        TABLE[Materialized Tables]\n    end\n\n    R --&gt; D --&gt; DIM\n    R --&gt; F --&gt; FACT\n    FACT --&gt; A --&gt; AGG\n\n    DIM --&gt; M\n    FACT --&gt; M\n    M --&gt; Q --&gt; DF\n    M --&gt; MAT --&gt; TABLE\n\n    style Pipeline fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Semantic fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"semantics/#when-to-use-what","title":"When to Use What","text":"Use Case Solution Build dimension tables Use <code>pattern: type: dimension</code> in pipeline YAML Build fact tables Use <code>pattern: type: fact</code> in pipeline YAML Build scheduled aggregates Use <code>pattern: type: aggregation</code> in pipeline YAML Ad-hoc metric queries Use Semantic Layer Python API Self-service BI metrics Use Semantic Layer with materialization"},{"location":"semantics/#configuration","title":"Configuration","text":"<p>The semantic layer is configured via Python, not YAML. You can load config from a YAML file if desired:</p> <pre><code>from odibi.semantics import SemanticQuery, Materializer, parse_semantic_config\nimport yaml\n\n# Load from YAML (optional - can also build programmatically)\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Query interface\nquery = SemanticQuery(config)\nresult = query.execute(\"revenue BY region\", context)\n\n# Materialization\nmaterializer = Materializer(config)\nmaterializer.execute(\"monthly_revenue\", context)\n</code></pre>"},{"location":"semantics/#example-semantic_configyaml","title":"Example semantic_config.yaml","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  - name: avg_order_value\n    expr: \"AVG(total_amount)\"\n    source: fact_orders\n\ndimensions:\n  - name: region\n    source: fact_orders\n    column: region\n\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy: [year, quarter, month_name]\n\n  - name: category\n    source: dim_product\n    column: category\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n</code></pre>"},{"location":"semantics/#core-concepts","title":"Core Concepts","text":""},{"location":"semantics/#metrics","title":"Metrics","text":"<p>Metrics are measurable values that can be aggregated across dimensions:</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"semantics/#dimensions","title":"Dimensions","text":"<p>Dimensions are attributes for grouping and filtering:</p> <pre><code>dimensions:\n  - name: order_date\n    source: dim_date\n    hierarchy: [year, quarter, month, full_date]\n</code></pre>"},{"location":"semantics/#queries","title":"Queries","text":"<p>Query the semantic layer with a simple string syntax:</p> <pre><code>result = query.execute(\"revenue, order_count BY region, month\", context)\n</code></pre>"},{"location":"semantics/#materializations","title":"Materializations","text":"<p>Pre-compute metrics at specific grain:</p> <pre><code>materializations:\n  - name: monthly_revenue\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n</code></pre>"},{"location":"semantics/#quick-start","title":"Quick Start","text":""},{"location":"semantics/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is through the unified <code>Project</code> API, which automatically resolves table paths from your connections:</p> <pre><code>from odibi import Project\n\n# Load project - semantic layer inherits connections from odibi.yaml\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics - tables are auto-loaded from connections\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\n\n# With filters\nresult = project.query(\"revenue BY category WHERE region = 'North'\")\n</code></pre> <p>odibi.yaml with semantic layer:</p> <pre><code>project: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n      - name: dim_customer\n        write:\n          connection: gold\n          table: dim_customer\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n</code></pre> <p>The <code>source: $build_warehouse.fact_orders</code> notation tells the semantic layer to: 1. Look up the <code>fact_orders</code> node in the <code>build_warehouse</code> pipeline 2. Read its <code>write.connection</code> and <code>write.table</code> config 3. Auto-load the Delta table when queried</p> <p>Alternative: connection.path notation</p> <p>For external tables not managed by pipelines, use <code>connection.path</code>:</p> <pre><code>source: gold.fact_orders              # \u2192 /mnt/data/gold/fact_orders\nsource: gold.sales/store_a/metrics    # \u2192 /mnt/data/gold/sales/store_a/metrics (nested paths work!)\n</code></pre> <p>The split happens on the first dot only, so subdirectories are supported.</p>"},{"location":"semantics/#option-b-manual-setup","title":"Option B: Manual Setup","text":""},{"location":"semantics/#1-build-your-data-with-pipelines","title":"1. Build Your Data with Pipelines","text":"<p>First, use standard Odibi pipelines to build your star schema:</p> <pre><code># odibi.yaml - Build the data layer\nproject: my_warehouse\nengine: spark\n\nconnections:\n  warehouse:\n    type: delta\n    path: /mnt/warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_star_schema\n    nodes:\n      - name: dim_customer\n        read:\n          connection: staging\n          path: customers\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols: [name, region]\n        write:\n          connection: warehouse\n          path: dim_customer\n\n      - name: fact_orders\n        depends_on: [dim_customer]\n        read:\n          connection: staging\n          path: orders\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n        write:\n          connection: warehouse\n          path: fact_orders\n</code></pre>"},{"location":"semantics/#2-define-semantic-layer","title":"2. Define Semantic Layer","text":"<p>Create a semantic config (Python or YAML):</p> <pre><code>from odibi.semantics import SemanticLayerConfig, MetricDefinition, DimensionDefinition\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            expr=\"SUM(total_amount)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\"\n        )\n    ],\n    dimensions=[\n        DimensionDefinition(\n            name=\"region\",\n            source=\"dim_customer\",\n            column=\"region\"\n        )\n    ]\n)\n</code></pre>"},{"location":"semantics/#3-query-metrics","title":"3. Query Metrics","text":"<pre><code>from odibi.semantics import SemanticQuery\nfrom odibi.context import EngineContext\n\n# Setup context with your data\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", spark.table(\"warehouse.fact_orders\"))\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\n\n# Query\nquery = SemanticQuery(config)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.show())\n</code></pre>"},{"location":"semantics/#architecture","title":"Architecture","text":"<pre><code>classDiagram\n    class SemanticLayerConfig {\n        +metrics: List[MetricDefinition]\n        +dimensions: List[DimensionDefinition]\n        +materializations: List[MaterializationConfig]\n    }\n\n    class MetricDefinition {\n        +name: str\n        +expr: str\n        +source: str\n        +filters: List[str]\n    }\n\n    class DimensionDefinition {\n        +name: str\n        +source: str\n        +column: str\n        +hierarchy: List[str]\n    }\n\n    class SemanticQuery {\n        +execute(query_string, context)\n    }\n\n    class Materializer {\n        +execute(name, context)\n        +execute_all(context)\n    }\n\n    SemanticLayerConfig --&gt; MetricDefinition\n    SemanticLayerConfig --&gt; DimensionDefinition\n    SemanticQuery --&gt; SemanticLayerConfig\n    Materializer --&gt; SemanticLayerConfig\n</code></pre>"},{"location":"semantics/#next-steps","title":"Next Steps","text":"<ul> <li>Defining Metrics - Create metric and dimension definitions</li> <li>Querying - Query syntax and examples</li> <li>Materializing - Pre-compute and schedule metrics</li> <li>Pattern Docs - Build your data layer with patterns</li> </ul>"},{"location":"semantics/lineage_stitcher/","title":"Lineage Stitcher","text":"<p>The Lineage Stitcher combines lineage data from multiple pipeline stories into a unified view, showing how data flows from raw sources through your entire data warehouse to the semantic layer.</p>"},{"location":"semantics/lineage_stitcher/#what-is-lineage","title":"What is Lineage?","text":"<p>Lineage answers the question: \"Where did this data come from?\"</p> <p>In a data warehouse, data flows through multiple stages:</p> <pre><code>Raw Sources \u2192 Bronze \u2192 Silver \u2192 Gold \u2192 Semantic Views \u2192 BI Dashboards\n</code></pre> <p>Each stage transforms the data. Lineage tracks these transformations so you can:</p> <ul> <li>Trace origins - Find where a metric's data comes from</li> <li>Assess impact - Know what breaks if a source changes</li> <li>Debug issues - Track down where bad data entered the pipeline</li> <li>Document compliance - Prove data provenance for audits</li> </ul>"},{"location":"semantics/lineage_stitcher/#what-is-the-lineage-stitcher","title":"What is the Lineage Stitcher?","text":"<p>Each Odibi pipeline generates its own story with a local lineage graph showing inputs and outputs. The Lineage Stitcher combines these individual graphs into one comprehensive view.</p>"},{"location":"semantics/lineage_stitcher/#without-the-stitcher","title":"Without the Stitcher","text":"<p>You have separate lineage graphs:</p> <pre><code>Bronze Story:\n  raw_orders \u2192 bronze_orders\n  raw_customers \u2192 bronze_customers\n\nSilver Story:\n  bronze_orders \u2192 silver_orders\n  bronze_customers \u2192 dim_customers\n\nGold Story:\n  silver_orders + dim_customers \u2192 fact_orders\n\nSemantic Story:\n  fact_orders \u2192 vw_daily_orders\n  fact_orders \u2192 vw_monthly_orders\n</code></pre>"},{"location":"semantics/lineage_stitcher/#with-the-stitcher","title":"With the Stitcher","text":"<p>One unified graph:</p> <pre><code>raw_orders \u2192 bronze_orders \u2192 silver_orders \u2500\u2510\n                                             \u251c\u2192 fact_orders \u2192 vw_daily_orders\nraw_customers \u2192 bronze_customers \u2192 dim_customers \u2500\u2518          \u2192 vw_monthly_orders\n</code></pre>"},{"location":"semantics/lineage_stitcher/#how-it-works","title":"How It Works","text":"<p>The Lineage Stitcher:</p> <ol> <li>Reads all story JSON files for a given date</li> <li>Extracts the <code>graph_data</code> from each story</li> <li>Deduplicates nodes (same table appearing in multiple stories)</li> <li>Merges edges to create connected paths</li> <li>Generates combined output as JSON and interactive HTML</li> </ol>"},{"location":"semantics/lineage_stitcher/#story-structure","title":"Story Structure","text":"<p>Each pipeline story contains a <code>graph_data</code> section:</p> <pre><code>{\n  \"name\": \"gold_pipeline\",\n  \"graph_data\": {\n    \"nodes\": [\n      {\"id\": \"silver_orders\", \"type\": \"table\", \"layer\": \"silver\"},\n      {\"id\": \"dim_customers\", \"type\": \"table\", \"layer\": \"silver\"},\n      {\"id\": \"fact_orders\", \"type\": \"table\", \"layer\": \"gold\"}\n    ],\n    \"edges\": [\n      {\"from\": \"silver_orders\", \"to\": \"fact_orders\"},\n      {\"from\": \"dim_customers\", \"to\": \"fact_orders\"}\n    ]\n  }\n}\n</code></pre> <p>The stitcher reads these from all stories and combines them.</p>"},{"location":"semantics/lineage_stitcher/#configuration","title":"Configuration","text":"<p>Lineage stitching is configured in your <code>odibi.yaml</code>:</p> <pre><code>story:\n  connection: adls_stories      # Where stories are saved\n  path: stories                  # Base path for stories\n  auto_generate: true            # Generate stories after each run\n  generate_lineage: true         # Generate combined lineage\n</code></pre>"},{"location":"semantics/lineage_stitcher/#key-settings","title":"Key Settings","text":"Setting Required Default Description <code>story.connection</code> Yes - Storage connection for stories <code>story.path</code> Yes - Base path for story files <code>story.auto_generate</code> No <code>true</code> Generate stories after pipeline runs <code>story.generate_lineage</code> No <code>false</code> Generate combined lineage"},{"location":"semantics/lineage_stitcher/#using-the-lineage-generator","title":"Using the Lineage Generator","text":""},{"location":"semantics/lineage_stitcher/#automatic-generation","title":"Automatic Generation","text":"<p>When <code>generate_lineage: true</code> is set, lineage is automatically generated after running the semantic layer:</p> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\n\n# Run semantic layer - lineage generated automatically\nresult = project.run_semantic_layer()\n\n# Lineage paths are in the result\nif result['lineage_paths']:\n    print(f\"Lineage JSON: {result['lineage_paths']['json']}\")\n    print(f\"Lineage HTML: {result['lineage_paths']['html']}\")\n</code></pre>"},{"location":"semantics/lineage_stitcher/#manual-generation","title":"Manual Generation","text":"<p>For more control, use the <code>LineageGenerator</code> class directly:</p> <pre><code>from odibi.story.lineage import LineageGenerator\n\n# Create generator\ngenerator = LineageGenerator(\n    stories_path=\"abfs://datalake@myaccount.dfs.core.windows.net/stories\",\n    storage_options={\"account_key\": \"...\"}\n)\n\n# Generate lineage for today\nresult = generator.generate()\n\n# Or for a specific date\nresult = generator.generate(date=\"2026-01-02\")\n\n# Print summary\nprint(f\"Date: {result.date}\")\nprint(f\"Layers found: {len(result.layers)}\")\nprint(f\"Nodes: {len(result.nodes)}\")\nprint(f\"Edges: {len(result.edges)}\")\n\n# Save to files\npaths = generator.save(result)\nprint(f\"Saved to: {paths['html']}\")\n</code></pre>"},{"location":"semantics/lineage_stitcher/#local-file-system","title":"Local File System","text":"<p>For local development:</p> <pre><code>from odibi.story.lineage import LineageGenerator\n\n# Local stories directory\ngenerator = LineageGenerator(stories_path=\"./stories\")\n\n# Generate and save\nresult = generator.generate()\npaths = generator.save(result)\n\n# Open HTML in browser\nimport webbrowser\nwebbrowser.open(paths['html'])\n</code></pre>"},{"location":"semantics/lineage_stitcher/#output-files","title":"Output Files","text":""},{"location":"semantics/lineage_stitcher/#file-location","title":"File Location","text":"<p>Lineage files are saved to:</p> <pre><code>{stories_path}/lineage/{date}/run_{time}.json\n{stories_path}/lineage/{date}/run_{time}.html\n</code></pre> <p>Example: <pre><code>stories/lineage/2026-01-02/run_10-45-30.json\nstories/lineage/2026-01-02/run_10-45-30.html\n</code></pre></p>"},{"location":"semantics/lineage_stitcher/#json-output","title":"JSON Output","text":"<p>The JSON file contains the complete lineage data:</p> <pre><code>{\n  \"generated_at\": \"2026-01-02T10:45:30\",\n  \"date\": \"2026-01-02\",\n  \"layers\": [\n    {\n      \"name\": \"bronze\",\n      \"story_path\": \"stories/bronze/2026-01-02/run_08-00-15.json\",\n      \"status\": \"success\",\n      \"duration\": 62.5,\n      \"pipeline_layer\": \"bronze\"\n    },\n    {\n      \"name\": \"silver\",\n      \"story_path\": \"stories/silver/2026-01-02/run_08-05-30.json\",\n      \"status\": \"success\",\n      \"duration\": 132.8,\n      \"pipeline_layer\": \"silver\"\n    },\n    {\n      \"name\": \"gold\",\n      \"story_path\": \"stories/gold/2026-01-02/run_08-10-45.json\",\n      \"status\": \"success\",\n      \"duration\": 156.2,\n      \"pipeline_layer\": \"gold\"\n    },\n    {\n      \"name\": \"Sales_semantic\",\n      \"story_path\": \"stories/Sales_semantic/2026-01-02/run_08-15-00.json\",\n      \"status\": \"success\",\n      \"duration\": 27.3,\n      \"pipeline_layer\": \"semantic\"\n    }\n  ],\n  \"nodes\": [\n    {\"id\": \"raw_sales_data\", \"type\": \"source\", \"layer\": \"raw\"},\n    {\"id\": \"bronze_sales\", \"type\": \"table\", \"layer\": \"bronze\"},\n    {\"id\": \"cleaned_sales\", \"type\": \"table\", \"layer\": \"silver\"},\n    {\"id\": \"fact_orders\", \"type\": \"table\", \"layer\": \"gold\"},\n    {\"id\": \"vw_sales_daily\", \"type\": \"view\", \"layer\": \"semantic\"},\n    {\"id\": \"vw_sales_monthly\", \"type\": \"view\", \"layer\": \"semantic\"}\n  ],\n  \"edges\": [\n    {\"from\": \"raw_sales_data\", \"to\": \"bronze_sales\"},\n    {\"from\": \"bronze_sales\", \"to\": \"cleaned_sales\"},\n    {\"from\": \"cleaned_sales\", \"to\": \"fact_orders\"},\n    {\"from\": \"fact_orders\", \"to\": \"vw_sales_daily\"},\n    {\"from\": \"fact_orders\", \"to\": \"vw_sales_monthly\"}\n  ]\n}\n</code></pre>"},{"location":"semantics/lineage_stitcher/#html-output","title":"HTML Output","text":"<p>The HTML file provides an interactive visualization:</p> <ul> <li>Summary panel - Date, layer count, node/edge counts</li> <li>Layer cards - Each pipeline with status and duration</li> <li>Mermaid diagram - Interactive lineage graph</li> <li>Click-to-expand - Details for each layer</li> </ul> <p>The diagram is rendered using Mermaid, showing:</p> <ul> <li>Nodes colored by layer (bronze, silver, gold, semantic)</li> <li>Edges showing data flow direction</li> <li>Grouping by layer for clarity</li> </ul>"},{"location":"semantics/lineage_stitcher/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"semantics/lineage_stitcher/#layers","title":"Layers","text":"<p>Each layer represents a pipeline run found for the date:</p> <pre><code>{\n  \"name\": \"bronze\",           // Pipeline name\n  \"story_path\": \"...\",        // Path to the story file\n  \"status\": \"success\",        // Pipeline status\n  \"duration\": 62.5,           // Execution time in seconds\n  \"pipeline_layer\": \"bronze\"  // Inferred layer type\n}\n</code></pre> <p>The <code>pipeline_layer</code> is inferred from the pipeline name: - Contains \"bronze\" \u2192 bronze layer - Contains \"silver\" \u2192 silver layer - Contains \"gold\" \u2192 gold layer - Contains \"semantic\" \u2192 semantic layer - Otherwise \u2192 unknown</p>"},{"location":"semantics/lineage_stitcher/#nodes","title":"Nodes","text":"<p>Each node represents a data asset:</p> <pre><code>{\n  \"id\": \"fact_orders\",    // Unique identifier\n  \"type\": \"table\",        // \"source\", \"table\", or \"view\"\n  \"layer\": \"gold\"         // Which layer it belongs to\n}\n</code></pre>"},{"location":"semantics/lineage_stitcher/#edges","title":"Edges","text":"<p>Each edge represents data flow:</p> <pre><code>{\n  \"from\": \"cleaned_sales\", // Source node ID\n  \"to\": \"fact_orders\"      // Target node ID\n}\n</code></pre>"},{"location":"semantics/lineage_stitcher/#practical-examples","title":"Practical Examples","text":""},{"location":"semantics/lineage_stitcher/#example-1-generate-lineage-after-full-pipeline-run","title":"Example 1: Generate Lineage After Full Pipeline Run","text":"<pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\n\n# Run all pipelines\nfor pipeline in [\"bronze\", \"silver\", \"gold\"]:\n    result = project.run(pipeline)\n    print(f\"{pipeline}: {result.status}\")\n\n# Run semantic layer with lineage\nresult = project.run_semantic_layer()\n\n# View combined lineage\nif result['lineage_paths']:\n    print(f\"\\nView lineage at: {result['lineage_paths']['html']}\")\n</code></pre>"},{"location":"semantics/lineage_stitcher/#example-2-generate-lineage-for-a-past-date","title":"Example 2: Generate Lineage for a Past Date","text":"<pre><code>from odibi.story.lineage import LineageGenerator\n\ngenerator = LineageGenerator(\n    stories_path=\"abfs://datalake@myaccount.dfs.core.windows.net/stories\",\n    storage_options={\"account_key\": \"...\"}\n)\n\n# Generate for specific date\nresult = generator.generate(date=\"2025-12-15\")\n\nif result.nodes:\n    print(f\"Found {len(result.nodes)} nodes for 2025-12-15\")\n    generator.save(result)\nelse:\n    print(\"No stories found for that date\")\n</code></pre>"},{"location":"semantics/lineage_stitcher/#example-3-analyze-lineage-programmatically","title":"Example 3: Analyze Lineage Programmatically","text":"<pre><code>from odibi.story.lineage import LineageGenerator\n\ngenerator = LineageGenerator(stories_path=\"./stories\")\nresult = generator.generate()\n\n# Find all sources for a specific table\ndef find_sources(target_id, edges, visited=None):\n    \"\"\"Recursively find all upstream sources.\"\"\"\n    if visited is None:\n        visited = set()\n\n    sources = []\n    for edge in edges:\n        if edge.to_node == target_id and edge.from_node not in visited:\n            visited.add(edge.from_node)\n            sources.append(edge.from_node)\n            sources.extend(find_sources(edge.from_node, edges, visited))\n\n    return sources\n\n# What feeds into vw_sales_daily?\nsources = find_sources(\"vw_sales_daily\", result.edges)\nprint(f\"vw_sales_daily depends on: {sources}\")\n# Output: ['fact_orders', 'cleaned_sales', 'bronze_sales', 'raw_sales_data']\n</code></pre>"},{"location":"semantics/lineage_stitcher/#example-4-custom-write-location","title":"Example 4: Custom Write Location","text":"<pre><code>from odibi.story.lineage import LineageGenerator\n\ngenerator = LineageGenerator(stories_path=\"./stories\")\nresult = generator.generate()\n\n# Custom write function\ndef write_to_custom_location(path: str, content: str):\n    \"\"\"Write to a custom location.\"\"\"\n    # Could be S3, GCS, a database, etc.\n    custom_path = path.replace(\"stories/\", \"custom_lineage/\")\n    with open(custom_path, \"w\") as f:\n        f.write(content)\n    print(f\"Wrote to: {custom_path}\")\n\ngenerator.save(result, write_file=write_to_custom_location)\n</code></pre>"},{"location":"semantics/lineage_stitcher/#troubleshooting","title":"Troubleshooting","text":""},{"location":"semantics/lineage_stitcher/#no-lineage-generated","title":"No Lineage Generated","text":"<p>Symptom: Lineage files are empty or missing.</p> <p>Causes and Solutions:</p> <ol> <li>No stories for the date</li> <li>Check that pipelines ran and generated stories</li> <li> <p>Verify <code>story.auto_generate: true</code> is set</p> </li> <li> <p>Stories don't have graph_data</p> </li> <li>Older story format may lack lineage data</li> <li> <p>Re-run pipelines to generate new stories</p> </li> <li> <p>Wrong storage path</p> </li> <li>Verify <code>stories_path</code> matches where stories are saved</li> <li>Check storage credentials</li> </ol>"},{"location":"semantics/lineage_stitcher/#missing-connections","title":"Missing Connections","text":"<p>Symptom: Lineage shows disconnected nodes.</p> <p>Cause: Node IDs don't match across stories.</p> <p>Solution: Ensure consistent table naming: <pre><code># In bronze\nwrite:\n  path: bronze_orders  # Use this exact name\n\n# In silver\nread:\n  path: bronze_orders  # Must match exactly\n</code></pre></p>"},{"location":"semantics/lineage_stitcher/#duplicate-nodes","title":"Duplicate Nodes","text":"<p>Symptom: Same table appears multiple times.</p> <p>Cause: Different naming in different stories.</p> <p>Solution: The stitcher deduplicates by <code>id</code>. Ensure IDs are consistent: - <code>bronze/orders</code> vs <code>bronze_orders</code> \u2192 different IDs - Use consistent naming convention</p>"},{"location":"semantics/lineage_stitcher/#best-practices","title":"Best Practices","text":""},{"location":"semantics/lineage_stitcher/#1-run-all-pipelines-before-generating-lineage","title":"1. Run All Pipelines Before Generating Lineage","text":"<p>For complete lineage, run all pipelines for the date:</p> <pre><code>project.run(\"bronze\")\nproject.run(\"silver\")\nproject.run(\"gold\")\nproject.run_semantic_layer()  # Generates combined lineage\n</code></pre>"},{"location":"semantics/lineage_stitcher/#2-use-consistent-naming","title":"2. Use Consistent Naming","text":"<p>Table names should be consistent across pipelines:</p> <pre><code># Bronze output\nwrite:\n  path: customers\n\n# Silver input (must match)\nread:\n  path: customers\n</code></pre>"},{"location":"semantics/lineage_stitcher/#3-review-lineage-regularly","title":"3. Review Lineage Regularly","text":"<p>Make lineage review part of your data governance:</p> <ul> <li>Check after major changes</li> <li>Include in documentation</li> <li>Share with stakeholders</li> </ul>"},{"location":"semantics/lineage_stitcher/#4-archive-historical-lineage","title":"4. Archive Historical Lineage","text":"<p>Lineage shows the state at a point in time. Keep historical files:</p> <pre><code>stories/lineage/2026-01-01/run_10-00-00.json\nstories/lineage/2026-01-02/run_10-00-00.json\nstories/lineage/2026-01-03/run_10-00-00.json\n</code></pre>"},{"location":"semantics/lineage_stitcher/#5-use-lineage-for-impact-analysis","title":"5. Use Lineage for Impact Analysis","text":"<p>Before making changes, check what depends on a table:</p> <pre><code># What views depend on fact_orders?\ndownstream = [e.to_node for e in result.edges if e.from_node == \"fact_orders\"]\nprint(f\"Changing fact_orders will affect: {downstream}\")\n</code></pre>"},{"location":"semantics/lineage_stitcher/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"semantics/lineage_stitcher/#stories","title":"Stories","text":"<p>The Lineage Stitcher reads from Pipeline Stories. Each story contains local lineage that gets combined.</p>"},{"location":"semantics/lineage_stitcher/#cross-pipeline-lineage","title":"Cross-Pipeline Lineage","text":"<p>For catalog-based lineage tracking (stored in Delta tables), see Cross-Pipeline Lineage. The stitcher is complementary - it works from story files rather than the catalog.</p>"},{"location":"semantics/lineage_stitcher/#semantic-layer-runner","title":"Semantic Layer Runner","text":"<p>When you run the Semantic Layer Runner with <code>generate_lineage: true</code>, it automatically calls the Lineage Stitcher after view creation.</p>"},{"location":"semantics/lineage_stitcher/#api-reference","title":"API Reference","text":""},{"location":"semantics/lineage_stitcher/#lineagegenerator","title":"LineageGenerator","text":"<pre><code>class LineageGenerator:\n    def __init__(\n        self,\n        stories_path: str,              # Base path to stories\n        storage_options: dict = None,   # Credentials for remote storage\n        date: str = None                # Default date (today if not specified)\n    ):\n        ...\n\n    def generate(\n        self,\n        date: str = None                # Date to generate for (YYYY-MM-DD)\n    ) -&gt; LineageResult:\n        \"\"\"Generate combined lineage from all stories for the date.\"\"\"\n        ...\n\n    def save(\n        self,\n        result: LineageResult = None,   # Result to save (uses last if None)\n        write_file: Callable = None     # Custom write function\n    ) -&gt; dict:\n        \"\"\"Save lineage as JSON and HTML. Returns paths.\"\"\"\n        ...\n\n    def render_json(self, result: LineageResult = None) -&gt; str:\n        \"\"\"Render lineage as JSON string.\"\"\"\n        ...\n\n    def render_html(self, result: LineageResult = None) -&gt; str:\n        \"\"\"Render lineage as HTML string with Mermaid diagram.\"\"\"\n        ...\n</code></pre>"},{"location":"semantics/lineage_stitcher/#lineageresult","title":"LineageResult","text":"<pre><code>@dataclass\nclass LineageResult:\n    generated_at: str           # ISO timestamp\n    date: str                   # Date string (YYYY-MM-DD)\n    layers: List[LayerInfo]     # Pipeline layers found\n    nodes: List[LineageNode]    # All nodes\n    edges: List[LineageEdge]    # All edges\n    json_path: str = None       # Path after save\n    html_path: str = None       # Path after save\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        ...\n</code></pre>"},{"location":"semantics/lineage_stitcher/#layerinfo","title":"LayerInfo","text":"<pre><code>@dataclass\nclass LayerInfo:\n    name: str                   # Pipeline name\n    story_path: str             # Path to story file\n    status: str                 # \"success\" or \"failed\"\n    duration: float             # Execution time in seconds\n    pipeline_layer: str = None  # Inferred layer (bronze/silver/gold/semantic)\n</code></pre>"},{"location":"semantics/lineage_stitcher/#lineagenode","title":"LineageNode","text":"<pre><code>@dataclass\nclass LineageNode:\n    id: str                     # Unique identifier\n    type: str                   # \"source\", \"table\", or \"view\"\n    layer: str                  # bronze, silver, gold, semantic, etc.\n</code></pre>"},{"location":"semantics/lineage_stitcher/#lineageedge","title":"LineageEdge","text":"<pre><code>@dataclass\nclass LineageEdge:\n    from_node: str              # Source node ID\n    to_node: str                # Target node ID\n</code></pre>"},{"location":"semantics/lineage_stitcher/#see-also","title":"See Also","text":"<ul> <li>Semantic Layer Runner - Create SQL views from metrics</li> <li>Pipeline Stories - Execution documentation</li> <li>Cross-Pipeline Lineage - Catalog-based lineage tracking</li> </ul>"},{"location":"semantics/materialize/","title":"Materializing Metrics","text":"<p>This guide covers how to pre-compute and persist metrics using the <code>Materializer</code> class.</p> <p>Source Notation: Metrics and dimensions use <code>source</code> to reference tables. Supports <code>$pipeline.node</code> (recommended), <code>connection.path</code>, or bare names. See Source Notation for details.</p>"},{"location":"semantics/materialize/#overview","title":"Overview","text":"<p>Materialization pre-computes aggregated metrics at a specific grain and persists them to an output table. This enables:</p> <ul> <li>Faster query response: Pre-computed aggregates vs. real-time calculation</li> <li>Scheduled refresh: Cron-based updates for dashboards</li> <li>Incremental updates: Merge new data without full recalculation</li> </ul>"},{"location":"semantics/materialize/#materializationconfig","title":"MaterializationConfig","text":"<p>Define materializations in your semantic layer config:</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region     # Unique identifier\n    metrics: [revenue, order_count]     # Metrics to include\n    dimensions: [region, month]         # Grain (GROUP BY)\n    output: gold/agg_monthly_revenue    # Output table path\n    schedule: \"0 2 1 * *\"               # Optional: cron schedule\n    incremental:                        # Optional: incremental config\n      timestamp_column: order_date\n      merge_strategy: replace\n</code></pre>"},{"location":"semantics/materialize/#fields","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique materialization identifier <code>metrics</code> list Yes - Metrics to materialize <code>dimensions</code> list Yes - Dimensions for grouping <code>output</code> str Yes - Output table path <code>schedule</code> str No - Cron schedule for refresh <code>incremental</code> dict No - Incremental refresh config"},{"location":"semantics/materialize/#materializer-class","title":"Materializer Class","text":""},{"location":"semantics/materialize/#basic-usage","title":"Basic Usage","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\n\n# Load config\nconfig = parse_semantic_config(yaml.safe_load(open(\"semantic_layer.yaml\")))\n\n# Create materializer\nmaterializer = Materializer(config)\n\n# Execute single materialization\nresult = materializer.execute(\"monthly_revenue_by_region\", context)\n\nprint(result.name)        # 'monthly_revenue_by_region'\nprint(result.output)      # 'gold/agg_monthly_revenue'\nprint(result.row_count)   # Number of aggregated rows\nprint(result.elapsed_ms)  # Execution time\nprint(result.success)     # True if successful\nprint(result.error)       # Error message if failed\n</code></pre>"},{"location":"semantics/materialize/#execute-all-materializations","title":"Execute All Materializations","text":"<pre><code># Execute all configured materializations\nresults = materializer.execute_all(context)\n\nfor result in results:\n    status = \"OK\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status} ({result.row_count} rows)\")\n</code></pre>"},{"location":"semantics/materialize/#write-callback","title":"Write Callback","text":"<p>Provide a callback to write the output:</p> <pre><code>def write_delta(df, output_path):\n    \"\"\"Write DataFrame to Delta Lake.\"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n\n# Execute with write callback\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_delta\n)\n</code></pre>"},{"location":"semantics/materialize/#scheduling","title":"Scheduling","text":"<p>Materializations can have cron schedules for automated refresh:</p> <pre><code>materializations:\n  - name: daily_revenue\n    metrics: [revenue]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    schedule: \"0 2 * * *\"  # 2am daily\n\n  - name: monthly_summary\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_summary\n    schedule: \"0 3 1 * *\"  # 3am on 1st of month\n</code></pre>"},{"location":"semantics/materialize/#reading-schedules","title":"Reading Schedules","text":"<pre><code># Get schedule for a materialization\nschedule = materializer.get_schedule(\"daily_revenue\")\nprint(schedule)  # \"0 2 * * *\"\n\n# List all materializations with schedules\nfor mat in materializer.list_materializations():\n    print(f\"{mat['name']}: {mat['schedule']}\")\n</code></pre>"},{"location":"semantics/materialize/#integration-with-orchestrators","title":"Integration with Orchestrators","text":"<p>Use schedules with your orchestrator (Airflow, Dagster, etc.):</p> <pre><code># Airflow example\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndef run_materialization(name):\n    materializer.execute(name, context, write_callback=write_delta)\n\nfor mat in materializer.list_materializations():\n    if mat['schedule']:\n        PythonOperator(\n            task_id=f\"materialize_{mat['name']}\",\n            python_callable=run_materialization,\n            op_args=[mat['name']],\n            schedule_interval=mat['schedule']\n        )\n</code></pre>"},{"location":"semantics/materialize/#incremental-materialization","title":"Incremental Materialization","text":"<p>Use <code>IncrementalMaterializer</code> for efficient updates:</p> <pre><code>from odibi.semantics.materialize import IncrementalMaterializer\n\n# Create incremental materializer\ninc_materializer = IncrementalMaterializer(config)\n\n# Load existing materialized data\nexisting_df = spark.read.format(\"delta\").load(\"gold/agg_monthly_revenue\")\n\n# Get last processed timestamp\nlast_timestamp = existing_df.agg({\"load_timestamp\": \"max\"}).collect()[0][0]\n\n# Execute incremental update\nresult = inc_materializer.execute_incremental(\n    name=\"monthly_revenue_by_region\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_timestamp,\n    merge_strategy=\"replace\"\n)\n</code></pre>"},{"location":"semantics/materialize/#merge-strategies","title":"Merge Strategies","text":""},{"location":"semantics/materialize/#replace-strategy","title":"Replace Strategy","text":"<p>New aggregates overwrite existing for matching grain keys:</p> <pre><code>result = inc_materializer.execute_incremental(\n    name=\"monthly_revenue_by_region\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_processed,\n    merge_strategy=\"replace\"\n)\n</code></pre> <p>Behavior: 1. Filter source to <code>order_date &gt; since_timestamp</code> 2. Aggregate new data at grain 3. Remove matching grain keys from existing 4. Union remaining existing + new aggregates</p> <p>Use case: Late-arriving data, corrections, any non-additive metrics</p>"},{"location":"semantics/materialize/#sum-strategy","title":"Sum Strategy","text":"<p>Add new measure values to existing aggregates:</p> <pre><code>result = inc_materializer.execute_incremental(\n    name=\"daily_order_count\",\n    context=context,\n    existing_df=existing_df,\n    timestamp_column=\"created_at\",\n    since_timestamp=last_processed,\n    merge_strategy=\"sum\"\n)\n</code></pre> <p>Behavior: 1. Filter source to <code>created_at &gt; since_timestamp</code> 2. Aggregate new data at grain 3. Full outer join with existing on grain 4. Sum measure values</p> <p>Use case: Purely additive metrics (counts, sums) where data is append-only</p> <p>Warning: Don't use for AVG, DISTINCT counts, or ratios.</p>"},{"location":"semantics/materialize/#full-example","title":"Full Example","text":"<p>Complete materialization pipeline:</p> <pre><code># semantic_layer.yaml\nsemantic_layer:\n  metrics:\n    - name: revenue\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n      filters: [\"status = 'completed'\"]\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n      source: fact_orders\n\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n      source: fact_orders\n\n  dimensions:\n    - name: region\n      source: dim_customer\n      column: region\n\n    - name: month\n      source: dim_date\n      column: month_name\n\n    - name: date_sk\n      source: dim_date\n      column: date_sk\n\n  materializations:\n    - name: daily_revenue\n      metrics: [revenue, order_count]\n      dimensions: [date_sk, region]\n      output: gold/agg_daily_revenue\n      schedule: \"0 2 * * *\"\n\n    - name: monthly_summary\n      metrics: [revenue, order_count, unique_customers]\n      dimensions: [region, month]\n      output: gold/agg_monthly_summary\n      schedule: \"0 3 1 * *\"\n</code></pre> <pre><code>from odibi.semantics import Materializer, IncrementalMaterializer, parse_semantic_config\nfrom odibi.context import EngineContext\nimport yaml\n\n# Load config\nwith open(\"semantic_layer.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f)[\"semantic_layer\"])\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", spark.table(\"silver.fact_orders\"))\ncontext.register(\"dim_customer\", spark.table(\"gold.dim_customer\"))\ncontext.register(\"dim_date\", spark.table(\"gold.dim_date\"))\n\n# Write callback\ndef write_to_delta(df, output_path):\n    df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/warehouse/{output_path}\")\n\n# Full refresh all materializations\nmaterializer = Materializer(config)\nresults = materializer.execute_all(context, write_callback=write_to_delta)\n\n# Print summary\nfor r in results:\n    status = \"SUCCESS\" if r.success else f\"FAILED: {r.error}\"\n    print(f\"{r.name}: {status} - {r.row_count} rows in {r.elapsed_ms:.0f}ms\")\n\n# Incremental refresh for daily\ninc_materializer = IncrementalMaterializer(config)\nexisting_daily = spark.read.format(\"delta\").load(\"/mnt/warehouse/gold/agg_daily_revenue\")\nlast_date = existing_daily.agg({\"date_sk\": \"max\"}).collect()[0][0]\n\nresult = inc_materializer.execute_incremental(\n    name=\"daily_revenue\",\n    context=context,\n    existing_df=existing_daily,\n    timestamp_column=\"order_date\",\n    since_timestamp=last_date,\n    merge_strategy=\"replace\"\n)\n\n# Write incremental result\nif result.success:\n    write_to_delta(result.df, \"gold/agg_daily_revenue\")\n    print(f\"Updated daily_revenue: {result.row_count} rows\")\n</code></pre>"},{"location":"semantics/materialize/#materializationresult","title":"MaterializationResult","text":"Field Type Description <code>name</code> str Materialization name <code>output</code> str Output table path <code>row_count</code> int Number of aggregated rows <code>elapsed_ms</code> float Execution time in milliseconds <code>success</code> bool Whether execution succeeded <code>error</code> str Error message if failed"},{"location":"semantics/materialize/#best-practices","title":"Best Practices","text":""},{"location":"semantics/materialize/#grain-selection","title":"Grain Selection","text":"<ul> <li>Choose grain based on query patterns</li> <li>Finer grain = more rows, but more flexibility</li> <li>Coarser grain = faster queries, less flexibility</li> </ul>"},{"location":"semantics/materialize/#scheduling_1","title":"Scheduling","text":"<ul> <li>Schedule based on source data freshness</li> <li>Daily aggregates: run after nightly ETL</li> <li>Monthly: run after month close</li> </ul>"},{"location":"semantics/materialize/#incremental-strategy","title":"Incremental Strategy","text":"<ul> <li>Use <code>replace</code> for late-arriving data tolerance</li> <li>Use <code>sum</code> only for append-only sources</li> <li>Track <code>since_timestamp</code> in state store</li> </ul>"},{"location":"semantics/materialize/#performance","title":"Performance","text":"<ul> <li>Partition output by time dimension</li> <li>Use Delta Lake for efficient updates</li> <li>Monitor execution times</li> </ul>"},{"location":"semantics/materialize/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Create metric definitions</li> <li>Querying - Interactive metric queries</li> <li>Aggregation Pattern - Pattern-based aggregation</li> </ul>"},{"location":"semantics/metrics/","title":"Defining Metrics","text":"<p>This guide covers how to define metrics and dimensions in the Odibi semantic layer.</p> <p>Source Notation: The <code>source</code> field supports three formats: <code>$pipeline.node</code> (recommended), <code>connection.path</code>, or bare table names. See Source Notation for details.</p>"},{"location":"semantics/metrics/#metricdefinition","title":"MetricDefinition","text":"<p>A metric represents a measurable value that can be aggregated across dimensions.</p>"},{"location":"semantics/metrics/#schema","title":"Schema","text":"<pre><code>metrics:\n  - name: revenue              # Required: unique identifier\n    label: \"Total Revenue\"     # Optional: display name for column alias\n    description: \"...\"         # Optional: human-readable description\n    expr: \"SUM(total_amount)\"  # Required: SQL aggregation expression\n    source: fact_orders        # Required for simple metrics: source table\n    filters:                   # Optional: WHERE conditions\n      - \"status = 'completed'\"\n    type: simple               # Optional: \"simple\" or \"derived\"\n</code></pre>"},{"location":"semantics/metrics/#fields","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique metric identifier (lowercase, alphanumeric + underscore) <code>label</code> str No name Display name used as column alias in generated views <code>description</code> str No - Human-readable description <code>expr</code> str Yes - SQL aggregation expression <code>source</code> str For simple - Source table name <code>filters</code> list No [] WHERE conditions to apply <code>type</code> str No \"simple\" \"simple\" (direct) or \"derived\" (references other metrics)"},{"location":"semantics/metrics/#simple-metrics","title":"Simple Metrics","text":"<p>Simple metrics aggregate directly from source data:</p> <pre><code>metrics:\n  # Count\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  # Sum\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n\n  # Average\n  - name: avg_order_value\n    expr: \"AVG(total_amount)\"\n    source: fact_orders\n\n  # Distinct count\n  - name: unique_customers\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n\n  # Min/Max\n  - name: max_order\n    expr: \"MAX(total_amount)\"\n    source: fact_orders\n\n  # Complex expression\n  - name: total_margin\n    expr: \"SUM(revenue - cost)\"\n    source: fact_orders\n</code></pre>"},{"location":"semantics/metrics/#filtered-metrics","title":"Filtered Metrics","text":"<p>Apply filters to constrain the aggregation:</p> <pre><code>metrics:\n  # Only completed orders\n  - name: completed_revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Multiple filters (AND)\n  - name: domestic_completed_revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n      - \"country = 'USA'\"\n\n  # Time-filtered\n  - name: last_30_days_orders\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"order_date &gt;= CURRENT_DATE - INTERVAL 30 DAY\"\n</code></pre>"},{"location":"semantics/metrics/#derived-metrics","title":"Derived Metrics","text":"<p>Derived metrics reference other metrics (future enhancement):</p> <pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(total_amount)\"\n    source: fact_orders\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n\n  # Derived: revenue / order_count\n  - name: avg_order_value_derived\n    expr: \"revenue / order_count\"\n    type: derived\n</code></pre>"},{"location":"semantics/metrics/#dimensiondefinition","title":"DimensionDefinition","text":"<p>A dimension represents an attribute for grouping and filtering metrics.</p>"},{"location":"semantics/metrics/#schema_1","title":"Schema","text":"<pre><code>dimensions:\n  - name: region               # Required: unique identifier\n    label: \"Sales Region\"      # Optional: display name for column alias\n    source: fact_orders        # Required: source table\n    column: region             # Optional: column name (defaults to name)\n    hierarchy:                 # Optional: drill-down hierarchy\n      - year\n      - quarter\n      - month\n    description: \"...\"         # Optional: human-readable description\n</code></pre>"},{"location":"semantics/metrics/#fields_1","title":"Fields","text":"Field Type Required Default Description <code>name</code> str Yes - Unique dimension identifier <code>label</code> str No name Display name used as column alias in generated views <code>source</code> str Yes - Source table name <code>column</code> str No name Column name in source <code>hierarchy</code> list No [] Ordered drill-down columns <code>description</code> str No - Human-readable description"},{"location":"semantics/metrics/#dimension-examples","title":"Dimension Examples","text":"<pre><code>dimensions:\n  # Simple dimension\n  - name: region\n    source: fact_orders\n    column: region\n\n  # Dimension with different column name\n  - name: customer_region\n    source: dim_customer\n    column: billing_region\n\n  # Date dimension with hierarchy\n  - name: order_date\n    source: dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter\n      - month\n      - week_of_year\n      - full_date\n\n  # Product category hierarchy\n  - name: product\n    source: dim_product\n    column: product_name\n    hierarchy:\n      - department\n      - category\n      - subcategory\n      - product_name\n</code></pre>"},{"location":"semantics/metrics/#complete-yaml-example","title":"Complete YAML Example","text":"<p>Full semantic layer configuration:</p> <pre><code>semantic_layer:\n  metrics:\n    # Revenue metrics\n    - name: revenue\n      description: \"Total revenue from all orders\"\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n\n    - name: completed_revenue\n      description: \"Revenue from completed orders only\"\n      expr: \"SUM(total_amount)\"\n      source: fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n    # Volume metrics\n    - name: order_count\n      description: \"Number of orders\"\n      expr: \"COUNT(*)\"\n      source: fact_orders\n\n    - name: units_sold\n      description: \"Total units sold\"\n      expr: \"SUM(quantity)\"\n      source: fact_orders\n\n    # Customer metrics\n    - name: unique_customers\n      description: \"Distinct customer count\"\n      expr: \"COUNT(DISTINCT customer_sk)\"\n      source: fact_orders\n\n    # Calculated metrics\n    - name: avg_order_value\n      description: \"Average order value\"\n      expr: \"AVG(total_amount)\"\n      source: fact_orders\n\n    - name: avg_units_per_order\n      description: \"Average units per order\"\n      expr: \"AVG(quantity)\"\n      source: fact_orders\n\n  dimensions:\n    # Geographic dimensions\n    - name: region\n      source: dim_customer\n      column: region\n      description: \"Customer region\"\n\n    - name: country\n      source: dim_customer\n      column: country\n\n    - name: city\n      source: dim_customer\n      column: city\n\n    # Time dimensions\n    - name: order_date\n      source: dim_date\n      column: full_date\n      hierarchy: [year, quarter, month, full_date]\n\n    - name: year\n      source: dim_date\n      column: year\n\n    - name: month\n      source: dim_date\n      column: month_name\n\n    - name: quarter\n      source: dim_date\n      column: quarter_name\n\n    # Product dimensions\n    - name: category\n      source: dim_product\n      column: category\n\n    - name: product\n      source: dim_product\n      column: product_name\n      hierarchy: [category, subcategory, product_name]\n\n    # Order dimensions\n    - name: channel\n      source: fact_orders\n      column: sales_channel\n\n    - name: payment_method\n      source: fact_orders\n      column: payment_type\n\n  materializations:\n    - name: daily_revenue_by_region\n      metrics: [revenue, order_count, unique_customers]\n      dimensions: [region, order_date]\n      output: gold/agg_daily_revenue_region\n\n    - name: monthly_revenue_by_category\n      metrics: [revenue, units_sold]\n      dimensions: [category, month]\n      output: gold/agg_monthly_revenue_category\n      schedule: \"0 2 1 * *\"\n</code></pre>"},{"location":"semantics/metrics/#python-api","title":"Python API","text":"<pre><code>from odibi.semantics.metrics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig,\n    parse_semantic_config\n)\n\n# Create metrics programmatically\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue\",\n    expr=\"SUM(total_amount)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Create dimensions\nregion = DimensionDefinition(\n    name=\"region\",\n    source=\"dim_customer\",\n    column=\"region\"\n)\n\norder_date = DimensionDefinition(\n    name=\"order_date\",\n    source=\"dim_date\",\n    column=\"full_date\",\n    hierarchy=[\"year\", \"quarter\", \"month\", \"full_date\"]\n)\n\n# Create config\nconfig = SemanticLayerConfig(\n    metrics=[revenue],\n    dimensions=[region, order_date]\n)\n\n# Or parse from YAML\nconfig = parse_semantic_config({\n    \"metrics\": [...],\n    \"dimensions\": [...],\n    \"materializations\": [...]\n})\n\n# Validate references\nerrors = config.validate_references()\nif errors:\n    print(\"Validation errors:\", errors)\n\n# Lookup by name\nmetric = config.get_metric(\"revenue\")\ndimension = config.get_dimension(\"region\")\n</code></pre>"},{"location":"semantics/metrics/#validation","title":"Validation","text":"<p>The semantic layer validates:</p> <ol> <li>Metric names: Must be alphanumeric + underscore, lowercase</li> <li>Non-empty expressions: <code>expr</code> cannot be empty</li> <li>Materialization references: All referenced metrics/dimensions must exist</li> </ol> <pre><code># Validate the config\nerrors = config.validate_references()\n# Returns: [\"Materialization 'x' references unknown metric 'y'\"]\n</code></pre>"},{"location":"semantics/metrics/#best-practices","title":"Best Practices","text":""},{"location":"semantics/metrics/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use <code>snake_case</code> for metric and dimension names</li> <li>Be descriptive: <code>completed_order_revenue</code> over <code>rev1</code></li> <li>Prefix related metrics: <code>revenue</code>, <code>revenue_completed</code>, <code>revenue_refunded</code></li> </ul>"},{"location":"semantics/metrics/#filter-usage","title":"Filter Usage","text":"<ul> <li>Define filtered variants as separate metrics</li> <li>Makes queries cleaner and consistent</li> <li>Enables caching of common filter combinations</li> </ul>"},{"location":"semantics/metrics/#hierarchy-design","title":"Hierarchy Design","text":"<ul> <li>Order from coarsest to finest grain</li> <li>Match BI tool drill-down expectations</li> <li>Include intermediate levels for flexibility</li> </ul>"},{"location":"semantics/metrics/#see-also","title":"See Also","text":"<ul> <li>Querying - Query syntax and execution</li> <li>Materializing - Pre-compute metrics</li> <li>Semantic Layer Overview - Architecture and concepts</li> </ul>"},{"location":"semantics/query/","title":"Querying the Semantic Layer","text":"<p>This guide covers how to query the Odibi semantic layer using the <code>SemanticQuery</code> interface.</p>"},{"location":"semantics/query/#query-syntax","title":"Query Syntax","text":"<p>The semantic query syntax follows a simple pattern:</p> <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre>"},{"location":"semantics/query/#examples","title":"Examples","text":"<pre><code># Single metric, single dimension\n\"revenue BY region\"\n\n# Multiple metrics, single dimension\n\"revenue, order_count BY region\"\n\n# Multiple metrics, multiple dimensions\n\"revenue, order_count BY region, month\"\n\n# With WHERE filter\n\"revenue BY region WHERE year = 2024\"\n\n# Complex filter\n\"revenue BY category WHERE region = 'North' AND status = 'completed'\"\n</code></pre>"},{"location":"semantics/query/#semanticquery-class","title":"SemanticQuery Class","text":""},{"location":"semantics/query/#initialization","title":"Initialization","text":"<pre><code>from odibi.semantics import SemanticQuery, SemanticLayerConfig\n\n# From config object\nconfig = SemanticLayerConfig(\n    metrics=[...],\n    dimensions=[...],\n    materializations=[...]\n)\nquery = SemanticQuery(config)\n\n# From YAML\nimport yaml\nfrom odibi.semantics.metrics import parse_semantic_config\n\nwith open(\"semantic_layer.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\nquery = SemanticQuery(config)\n</code></pre>"},{"location":"semantics/query/#execute-query","title":"Execute Query","text":"<pre><code>from odibi.context import EngineContext\n\n# Create context with source data\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\n\n# Execute query\nresult = query.execute(\"revenue BY region\", context)\n\n# Access results\nprint(result.df)           # DataFrame with results\nprint(result.metrics)      # ['revenue']\nprint(result.dimensions)   # ['region']\nprint(result.row_count)    # Number of result rows\nprint(result.elapsed_ms)   # Execution time\nprint(result.sql_generated)  # Generated SQL (for debugging)\n</code></pre>"},{"location":"semantics/query/#queryresult","title":"QueryResult","text":"Field Type Description <code>df</code> DataFrame Result DataFrame (Spark or Pandas) <code>metrics</code> List[str] Metrics that were computed <code>dimensions</code> List[str] Dimensions used for grouping <code>row_count</code> int Number of result rows <code>elapsed_ms</code> float Execution time in milliseconds <code>sql_generated</code> str Generated SQL query (for debugging)"},{"location":"semantics/query/#query-examples","title":"Query Examples","text":""},{"location":"semantics/query/#basic-queries","title":"Basic Queries","text":"<pre><code># Total revenue\nresult = query.execute(\"revenue\", context)\n# Returns single row with total\n\n# Revenue by region\nresult = query.execute(\"revenue BY region\", context)\n# Returns one row per region\n\n# Multiple metrics\nresult = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\n</code></pre>"},{"location":"semantics/query/#multi-dimensional-queries","title":"Multi-Dimensional Queries","text":"<pre><code># Two dimensions\nresult = query.execute(\"revenue BY region, category\", context)\n\n# Three dimensions\nresult = query.execute(\"revenue BY region, category, month\", context)\n\n# Time series\nresult = query.execute(\"revenue, order_count BY year, month\", context)\n</code></pre>"},{"location":"semantics/query/#filtered-queries","title":"Filtered Queries","text":"<pre><code># Single filter\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North'\",\n    context\n)\n\n# Multiple filters (combined with AND)\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North' AND year = 2024\",\n    context\n)\n\n# Using metric filters + query filters\n# If metric has filters, they combine with query filters\nresult = query.execute(\"completed_revenue BY region\", context)\n# Metric filter: status = 'completed'\n# Combined: WHERE status = 'completed'\n</code></pre>"},{"location":"semantics/query/#parse-and-validate","title":"Parse and Validate","text":"<p>You can parse and validate queries before execution:</p> <pre><code># Parse query string\nparsed = query.parse(\"revenue, order_count BY region, month WHERE year = 2024\")\n\nprint(parsed.metrics)      # ['revenue', 'order_count']\nprint(parsed.dimensions)   # ['region', 'month']\nprint(parsed.filters)      # ['year = 2024']\nprint(parsed.raw_query)    # Original query string\n\n# Validate against config\nerrors = query.validate(parsed)\nif errors:\n    print(\"Validation errors:\", errors)\n    # [\"Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count']\"]\n</code></pre>"},{"location":"semantics/query/#generated-sql","title":"Generated SQL","text":"<p>View the SQL generated from a semantic query:</p> <pre><code>parsed = query.parse(\"revenue BY region\")\nsql, source = query.generate_sql(parsed)\n\nprint(sql)\n# SELECT region, SUM(total_amount) AS revenue \n# FROM fact_orders \n# GROUP BY region\n\nprint(source)\n# fact_orders\n</code></pre>"},{"location":"semantics/query/#full-python-example","title":"Full Python Example","text":"<pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load semantic layer config\nconfig_dict = {\n    \"metrics\": [\n        {\n            \"name\": \"revenue\",\n            \"expr\": \"SUM(total_amount)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"order_count\",\n            \"expr\": \"COUNT(*)\",\n            \"source\": \"fact_orders\"\n        },\n        {\n            \"name\": \"avg_order_value\",\n            \"expr\": \"AVG(total_amount)\",\n            \"source\": \"fact_orders\"\n        }\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"fact_orders\", \"column\": \"region\"},\n        {\"name\": \"month\", \"source\": \"dim_date\", \"column\": \"month_name\"}\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\nquery = SemanticQuery(config)\n\n# Create context with data\ncontext = EngineContext(\n    df=None, \n    engine_type=EngineType.PANDAS\n)\ncontext.register(\"fact_orders\", orders_df)\ncontext.register(\"dim_date\", dates_df)\n\n# Query 1: Total revenue\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\n\n# Query 2: Revenue by region\nresult = query.execute(\"revenue, order_count BY region\", context)\nprint(\"\\nRevenue by Region:\")\nprint(result.df.to_string(index=False))\n\n# Query 3: Monthly trend\nresult = query.execute(\"revenue BY month\", context)\nprint(\"\\nMonthly Revenue:\")\nfor _, row in result.df.iterrows():\n    print(f\"  {row['month']}: ${row['revenue']:,.2f}\")\n\n# Query 4: Filtered query\nresult = query.execute(\n    \"revenue, avg_order_value BY region WHERE region IN ('North', 'South')\",\n    context\n)\nprint(\"\\nNorth/South Regions:\")\nprint(result.df.to_string(index=False))\n\n# Check execution performance\nprint(f\"\\nQuery executed in {result.elapsed_ms:.2f}ms\")\nprint(f\"Generated SQL: {result.sql_generated}\")\n</code></pre>"},{"location":"semantics/query/#using-with-source-dataframe","title":"Using with Source DataFrame","text":"<p>Override the context lookup with a specific DataFrame:</p> <pre><code># Instead of using context.get(source_table)\n# Pass source_df directly\nresult = query.execute(\n    \"revenue BY region\",\n    context,\n    source_df=my_filtered_dataframe\n)\n</code></pre>"},{"location":"semantics/query/#error-handling","title":"Error Handling","text":"<pre><code>from odibi.semantics import SemanticQuery\n\ntry:\n    result = query.execute(\"invalid_metric BY region\", context)\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\n    # \"Invalid semantic query: Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count']\"\n\ntry:\n    result = query.execute(\"revenue BY invalid_dimension\", context)\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\n    # \"Invalid semantic query: Unknown dimension 'invalid_dimension'. Available: ['region', 'month']\"\n</code></pre>"},{"location":"semantics/query/#engine-support","title":"Engine Support","text":"<p>Queries work with both Spark and Pandas:</p>"},{"location":"semantics/query/#spark","title":"Spark","text":"<pre><code>context = EngineContext(\n    df=None,\n    engine_type=EngineType.SPARK,\n    spark=spark_session\n)\nresult = query.execute(\"revenue BY region\", context)\nresult.df.show()  # Spark DataFrame\n</code></pre>"},{"location":"semantics/query/#pandas","title":"Pandas","text":"<pre><code>context = EngineContext(\n    df=None,\n    engine_type=EngineType.PANDAS\n)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df)  # Pandas DataFrame\n</code></pre>"},{"location":"semantics/query/#performance-tips","title":"Performance Tips","text":"<ol> <li>Materialize frequent queries: Use <code>Materializer</code> for dashboards</li> <li>Pre-filter source data: Pass filtered <code>source_df</code> parameter</li> <li>Limit dimensions: More dimensions = larger result set</li> <li>Use indexed columns: Ensure dimension columns are indexed in source</li> </ol>"},{"location":"semantics/query/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Create metric and dimension definitions</li> <li>Materializing Metrics - Pre-compute for performance</li> <li>Semantic Layer Overview - Architecture and concepts</li> </ul>"},{"location":"semantics/runner/","title":"Semantic Layer Runner","text":"<p>The Semantic Layer Runner executes your metric definitions as SQL Server views and generates execution stories for observability. This guide walks you through everything from basic concepts to production deployment.</p>"},{"location":"semantics/runner/#what-is-the-semantic-layer-runner","title":"What is the Semantic Layer Runner?","text":"<p>In a typical data warehouse, you have:</p> <ol> <li>Bronze layer - Raw data ingested from sources</li> <li>Silver layer - Cleaned and transformed data</li> <li>Gold layer - Business-ready fact and dimension tables</li> <li>Semantic layer - Pre-aggregated views for analytics and BI tools</li> </ol> <p>The Semantic Layer Runner bridges the gap between your Gold layer tables and your BI tools (Power BI, Tableau, etc.) by:</p> <ul> <li>Generating SQL views from your metric definitions</li> <li>Executing those views against SQL Server (or Azure SQL)</li> <li>Creating execution stories that document what happened</li> <li>Generating lineage showing how data flows from sources to views</li> </ul> <p>Think of it as an automated \"view factory\" that turns your YAML metric definitions into real database views.</p>"},{"location":"semantics/runner/#why-use-the-semantic-layer-runner","title":"Why Use the Semantic Layer Runner?","text":""},{"location":"semantics/runner/#without-the-runner","title":"Without the Runner","text":"<p>You would manually: 1. Write SQL view definitions by hand 2. Execute them against SQL Server 3. Track which views succeeded or failed 4. Document the SQL for auditing 5. Update views when metrics change</p>"},{"location":"semantics/runner/#with-the-runner","title":"With the Runner","text":"<p>You: 1. Define metrics in YAML (human-readable) 2. Run one command 3. Get views created, documented, and tracked automatically</p> <p>Benefits: - Consistency - All views follow the same pattern - Auditability - Every execution is documented in stories - Maintainability - Change YAML, re-run, views update - Observability - Know exactly what happened and when</p>"},{"location":"semantics/runner/#prerequisites","title":"Prerequisites","text":"<p>Before using the Semantic Layer Runner, you need:</p> <ol> <li>Gold layer tables - Your fact tables with calculated metrics (e.g., <code>fact_orders</code>)</li> <li>A SQL Server connection - Where views will be created</li> <li>A storage connection - Where stories and SQL files will be saved (e.g., Azure Data Lake)</li> </ol>"},{"location":"semantics/runner/#configuration","title":"Configuration","text":"<p>The Semantic Layer Runner is configured in your <code>odibi.yaml</code> project file. Here's a complete example:</p> <pre><code>project: SalesAnalytics\nengine: spark\n\n# Define your connections\nconnections:\n  # Where your gold layer data lives\n  gold:\n    type: delta\n    account_name: mydatalake\n    container: datalake\n    base_path: gold/sales\n    auth:\n      mode: account_key\n      account_key: ${AZURE_STORAGE_KEY}\n\n  # Where SQL views will be created\n  sql_server:\n    type: azure_sql\n    host: myserver.database.windows.net\n    database: analytics_db\n    port: 1433\n    auth:\n      mode: sql_login\n      username: ${SQL_USER}\n      password: ${SQL_PASSWORD}\n\n  # Where stories will be saved\n  stories:\n    type: azure_blob\n    account_name: mydatalake\n    container: datalake\n    base_path: stories\n    auth:\n      mode: account_key\n      account_key: ${AZURE_STORAGE_KEY}\n\n# Story configuration\nstory:\n  connection: stories\n  path: stories\n  auto_generate: true\n  generate_lineage: true\n\n# Semantic layer configuration\nsemantic:\n  # Which SQL Server connection to use for view creation\n  connection: sql_server\n\n  # Where to save generated SQL files (optional but recommended)\n  sql_output_path: gold/sales/views\n\n  # Define your views\n  views:\n    - name: vw_sales_daily\n      description: \"Daily sales metrics by store\"\n      source: fact_orders\n      db_schema: semantic\n      metrics:\n        - name: revenue\n          expr: \"SUM(revenue)\"\n          description: \"Total revenue\"\n        - name: order_count\n          expr: \"COUNT(*)\"\n          description: \"Total number of orders\"\n        - name: avg_order_value\n          expr: \"AVG(order_value)\"\n          description: \"Average order value\"\n        - name: total_sales\n          expr: \"SUM(total_sales)\"\n          description: \"Total sales amount\"\n      dimensions:\n        - name: date\n          column: Date\n        - name: store\n          column: store_id\n      grain: day\n\n    - name: vw_sales_monthly\n      description: \"Monthly sales metrics by store\"\n      source: fact_orders\n      db_schema: semantic\n      metrics:\n        - name: revenue\n          expr: \"SUM(revenue)\"\n        - name: order_count\n          expr: \"COUNT(*)\"\n        - name: total_sales\n          expr: \"SUM(total_sales)\"\n      dimensions:\n        - name: year_month\n          column: \"FORMAT(Date, 'yyyy-MM')\"\n        - name: store\n          column: store_id\n      grain: month\n</code></pre>"},{"location":"semantics/runner/#configuration-reference","title":"Configuration Reference","text":""},{"location":"semantics/runner/#semanticconnection","title":"<code>semantic.connection</code>","text":"<p>The name of the SQL Server connection where views will be created.</p> <pre><code>semantic:\n  connection: sql_server  # Must match a connection name above\n</code></pre>"},{"location":"semantics/runner/#semanticsql_output_path","title":"<code>semantic.sql_output_path</code>","text":"<p>Optional path where generated SQL files will be saved. Useful for: - Version control of SQL definitions - Auditing what SQL was executed - Debugging view creation issues</p> <pre><code>semantic:\n  sql_output_path: gold/sales/views\n</code></pre>"},{"location":"semantics/runner/#semanticviews","title":"<code>semantic.views</code>","text":"<p>A list of view definitions. Each view becomes a SQL Server view.</p> Field Required Description <code>name</code> Yes View name (will be created as <code>db_schema.name</code>) <code>description</code> No Human-readable description <code>source</code> Yes Source table name (your gold layer fact table) <code>db_schema</code> No SQL Server schema (default: <code>semantic</code>) <code>ensure_schema</code> No Auto-create schema if missing (default: <code>true</code>) <code>metrics</code> Yes List of metrics to include <code>dimensions</code> Yes List of dimensions to group by <code>grain</code> No Aggregation grain: <code>day</code>, <code>week</code>, <code>month</code>, <code>quarter</code>, <code>year</code> <code>filters</code> No WHERE clause filters"},{"location":"semantics/runner/#metric-definition","title":"Metric Definition","text":"<p>Each metric defines an aggregation:</p> <pre><code>metrics:\n  - name: revenue                # Column name in the view\n    expr: \"SUM(revenue)\"         # SQL aggregation expression\n    description: \"Total revenue\" # Optional documentation\n</code></pre> <p>Common expressions: - <code>SUM(column)</code> - Total - <code>AVG(column)</code> - Average - <code>COUNT(*)</code> - Row count - <code>COUNT(DISTINCT column)</code> - Unique count - <code>MAX(column)</code> / <code>MIN(column)</code> - Maximum/Minimum</p>"},{"location":"semantics/runner/#dimension-definition","title":"Dimension Definition","text":"<p>Each dimension defines a grouping column:</p> <pre><code>dimensions:\n  - name: date           # Column name in the view\n    column: Date         # Source column (can be an expression)\n</code></pre> <p>Examples: <pre><code>dimensions:\n  # Simple column reference\n  - name: store_id\n    column: store_id\n\n  # Date formatting\n  - name: year_month\n    column: \"FORMAT(Date, 'yyyy-MM')\"\n\n  # Derived column\n  - name: is_weekend\n    column: \"CASE WHEN DATEPART(dw, Date) IN (1,7) THEN 1 ELSE 0 END\"\n</code></pre></p>"},{"location":"semantics/runner/#running-the-semantic-layer","title":"Running the Semantic Layer","text":""},{"location":"semantics/runner/#using-python","title":"Using Python","text":"<pre><code>from odibi import Project\n\n# Load your project configuration\nproject = Project.load(\"odibi.yaml\")\n\n# Run the semantic layer\n# This will:\n# 1. Generate SQL for each view\n# 2. Execute the SQL against SQL Server\n# 3. Save the SQL files\n# 4. Generate an execution story\nresult = project.run_semantic_layer()\n\n# Check results\nprint(f\"Views created: {result['views_created']}\")\nprint(f\"Views failed: {result['views_failed']}\")\nprint(f\"Duration: {result['duration']:.2f}s\")\n\n# Story paths (if auto_generate is true)\nif result['story_paths']:\n    print(f\"Story JSON: {result['story_paths']['json']}\")\n    print(f\"Story HTML: {result['story_paths']['html']}\")\n</code></pre>"},{"location":"semantics/runner/#using-semanticlayerrunner-directly","title":"Using SemanticLayerRunner Directly","text":"<p>For more control, use the <code>SemanticLayerRunner</code> class:</p> <pre><code>from odibi.semantics.runner import SemanticLayerRunner\nfrom odibi.config import ProjectConfig\n\n# Load configuration\nconfig = ProjectConfig.from_yaml(\"odibi.yaml\")\n\n# Create runner\nrunner = SemanticLayerRunner(config)\n\n# Run with options\nresult = runner.run(\n    generate_story=True,      # Create execution story\n    generate_lineage=True,    # Create combined lineage\n)\n\n# Access metadata\nif runner.metadata:\n    for view in runner.metadata.views:\n        status = \"\u2713\" if view.status == \"success\" else \"\u2717\"\n        print(f\"{status} {view.view_name}: {view.duration:.2f}s\")\n        if view.error_message:\n            print(f\"  Error: {view.error_message}\")\n</code></pre>"},{"location":"semantics/runner/#custom-sql-executor","title":"Custom SQL Executor","text":"<p>If you need custom SQL execution logic:</p> <pre><code>import pyodbc\n\ndef my_sql_executor(sql: str) -&gt; None:\n    \"\"\"Custom SQL executor with logging.\"\"\"\n    conn = pyodbc.connect(my_connection_string)\n    cursor = conn.cursor()\n    print(f\"Executing: {sql[:100]}...\")\n    cursor.execute(sql)\n    conn.commit()\n    cursor.close()\n    conn.close()\n\n# Run with custom executor\nresult = runner.run(\n    execute_sql=my_sql_executor,\n    generate_story=True,\n)\n</code></pre>"},{"location":"semantics/runner/#custom-file-writer","title":"Custom File Writer","text":"<p>For custom storage backends:</p> <pre><code>def write_to_s3(path: str, content: str) -&gt; None:\n    \"\"\"Write files to S3 instead of Azure.\"\"\"\n    import boto3\n    s3 = boto3.client('s3')\n    s3.put_object(Bucket='my-bucket', Key=path, Body=content)\n\nresult = runner.run(\n    write_file=write_to_s3,\n    generate_story=True,\n)\n</code></pre>"},{"location":"semantics/runner/#generated-sql","title":"Generated SQL","text":"<p>The runner generates standard SQL Server view definitions. Here's an example of what gets created:</p>"},{"location":"semantics/runner/#input-yaml","title":"Input (YAML)","text":"<pre><code>views:\n  - name: vw_sales_monthly\n    source: fact_orders\n    db_schema: semantic\n    metrics:\n      - name: revenue\n        expr: \"SUM(revenue)\"\n      - name: total_sales\n        expr: \"SUM(total_sales)\"\n    dimensions:\n      - name: year_month\n        column: \"FORMAT(Date, 'yyyy-MM')\"\n      - name: store_id\n        column: store_id\n    filters:\n      - \"revenue &gt; 0\"\n</code></pre>"},{"location":"semantics/runner/#output-sql","title":"Output (SQL)","text":"<pre><code>-- View: semantic.vw_sales_monthly\n-- Generated by Odibi Semantic Layer Runner\n-- Source: fact_orders\n-- Generated at: 2026-01-02T10:30:00\n\n-- Ensure schema exists\nIF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'semantic')\nBEGIN\n    EXEC('CREATE SCHEMA semantic')\nEND\nGO\n\n-- Create or replace view\nCREATE OR ALTER VIEW semantic.vw_sales_monthly AS\nSELECT\n    FORMAT(Date, 'yyyy-MM') AS year_month,\n    store_id AS store_id,\n    SUM(revenue) AS revenue,\n    SUM(total_sales) AS total_sales\nFROM fact_orders\nWHERE revenue &gt; 0\nGROUP BY FORMAT(Date, 'yyyy-MM'), store_id\nGO\n</code></pre>"},{"location":"semantics/runner/#execution-stories","title":"Execution Stories","text":"<p>Every run generates an execution story - a detailed record of what happened. Stories are saved as both JSON (for programmatic access) and HTML (for human viewing).</p>"},{"location":"semantics/runner/#story-location","title":"Story Location","text":"<p>Stories are saved to: <pre><code>{story.path}/{semantic_name}/{date}/run_{time}.json\n{story.path}/{semantic_name}/{date}/run_{time}.html\n</code></pre></p> <p>Example: <pre><code>stories/Sales_semantic/2026-01-02/run_10-30-45.json\nstories/Sales_semantic/2026-01-02/run_10-30-45.html\n</code></pre></p>"},{"location":"semantics/runner/#story-contents","title":"Story Contents","text":"<p>The JSON story includes:</p> <pre><code>{\n  \"name\": \"Sales_semantic\",\n  \"started_at\": \"2026-01-02T10:30:45\",\n  \"completed_at\": \"2026-01-02T10:31:12\",\n  \"duration\": 27.3,\n  \"views_created\": 5,\n  \"views_failed\": 0,\n  \"views\": [\n    {\n      \"view_name\": \"vw_sales_daily\",\n      \"source_table\": \"fact_orders\",\n      \"status\": \"success\",\n      \"duration\": 2.1,\n      \"sql_generated\": \"CREATE OR ALTER VIEW semantic.vw_sales_daily AS ...\",\n      \"sql_file_path\": \"gold/sales/views/vw_sales_daily.sql\",\n      \"metrics_included\": [\"revenue\", \"order_count\", \"avg_order_value\", \"total_sales\"],\n      \"dimensions_included\": [\"date\", \"store\"]\n    },\n    {\n      \"view_name\": \"vw_sales_monthly\",\n      \"source_table\": \"fact_orders\",\n      \"status\": \"success\",\n      \"duration\": 1.8,\n      \"sql_generated\": \"...\",\n      \"sql_file_path\": \"gold/sales/views/vw_sales_monthly.sql\",\n      \"metrics_included\": [\"revenue\", \"order_count\", \"total_sales\"],\n      \"dimensions_included\": [\"year_month\", \"store\"]\n    }\n  ],\n  \"sql_files_saved\": [\n    \"gold/sales/views/vw_sales_daily.sql\",\n    \"gold/sales/views/vw_sales_monthly.sql\"\n  ],\n  \"graph_data\": {\n    \"nodes\": [\n      {\"id\": \"fact_orders\", \"type\": \"table\", \"layer\": \"gold\"},\n      {\"id\": \"vw_sales_daily\", \"type\": \"view\", \"layer\": \"semantic\"},\n      {\"id\": \"vw_sales_monthly\", \"type\": \"view\", \"layer\": \"semantic\"}\n    ],\n    \"edges\": [\n      {\"from\": \"fact_orders\", \"to\": \"vw_sales_daily\"},\n      {\"from\": \"fact_orders\", \"to\": \"vw_sales_monthly\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"semantics/runner/#html-story","title":"HTML Story","text":"<p>The HTML story provides a visual representation with:</p> <ul> <li>Summary - Overall status, duration, counts</li> <li>View Cards - Each view with status, metrics, dimensions</li> <li>SQL Details - Expandable section showing generated SQL</li> <li>Lineage Diagram - Visual graph of source \u2192 view relationships</li> </ul>"},{"location":"semantics/runner/#error-handling","title":"Error Handling","text":"<p>When a view fails to create, the runner:</p> <ol> <li>Continues processing other views (doesn't stop on first error)</li> <li>Records the error in the story</li> <li>Returns failed views in the result</li> </ol>"},{"location":"semantics/runner/#handling-failures","title":"Handling Failures","text":"<pre><code>result = runner.run()\n\nif result['views_failed']:\n    print(\"Some views failed:\")\n    for view in runner.metadata.views:\n        if view.status == \"failed\":\n            print(f\"  {view.view_name}: {view.error_message}\")\n</code></pre>"},{"location":"semantics/runner/#common-errors","title":"Common Errors","text":"Error Cause Solution <code>Invalid column name</code> Source column doesn't exist Check <code>source</code> table schema <code>Invalid object name</code> Source table doesn't exist Ensure gold layer table exists <code>Cannot create schema</code> Permission denied Grant schema creation rights <code>Login failed</code> Authentication error Check connection credentials"},{"location":"semantics/runner/#best-practices","title":"Best Practices","text":""},{"location":"semantics/runner/#1-use-descriptive-names","title":"1. Use Descriptive Names","text":"<pre><code>views:\n  # Good - clear purpose\n  - name: vw_sales_daily_by_store\n\n  # Bad - unclear\n  - name: v1\n</code></pre>"},{"location":"semantics/runner/#2-document-your-metrics","title":"2. Document Your Metrics","text":"<pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(revenue)\"\n    description: \"Total revenue - sum of all order values\"\n</code></pre>"},{"location":"semantics/runner/#3-separate-schemas-by-purpose","title":"3. Separate Schemas by Purpose","text":"<pre><code>views:\n  # Operational views for daily dashboards\n  - name: vw_sales_daily\n    db_schema: operational\n\n  # Executive views for monthly reports\n  - name: vw_sales_monthly\n    db_schema: executive\n</code></pre>"},{"location":"semantics/runner/#4-save-sql-files-for-auditing","title":"4. Save SQL Files for Auditing","text":"<pre><code>semantic:\n  sql_output_path: gold/views  # Always save SQL\n</code></pre>"},{"location":"semantics/runner/#5-check-stories-after-runs","title":"5. Check Stories After Runs","text":"<p>Always review execution stories, especially in production:</p> <pre><code>result = runner.run()\nif result['story_paths']:\n    print(f\"Review story at: {result['story_paths']['html']}\")\n</code></pre>"},{"location":"semantics/runner/#integration-with-pipelines","title":"Integration with Pipelines","text":"<p>The typical workflow is:</p> <ol> <li>Bronze pipeline - Ingest raw data</li> <li>Silver pipeline - Clean and transform</li> <li>Gold pipeline - Build fact tables (e.g., <code>fact_orders</code>)</li> <li>Semantic layer - Create views from facts</li> </ol> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\n\n# Run all pipelines in order\nproject.run(\"bronze\")\nproject.run(\"silver\")\nproject.run(\"gold\")\n\n# Then create semantic views\nproject.run_semantic_layer()\n</code></pre>"},{"location":"semantics/runner/#scheduling","title":"Scheduling","text":"<p>For production, schedule the semantic layer after your gold pipeline:</p> <pre><code># In Databricks or Airflow\ndef daily_etl():\n    project = Project.load(\"odibi.yaml\")\n\n    # Run ETL\n    project.run(\"bronze\")\n    project.run(\"silver\")\n    project.run(\"gold\")\n\n    # Update semantic views\n    result = project.run_semantic_layer()\n\n    # Alert on failures\n    if result['views_failed']:\n        send_alert(f\"Semantic layer had {len(result['views_failed'])} failures\")\n</code></pre>"},{"location":"semantics/runner/#troubleshooting","title":"Troubleshooting","text":""},{"location":"semantics/runner/#views-not-updating","title":"Views Not Updating","text":"<p>Symptom: You changed the YAML but views show old data.</p> <p>Cause: <code>CREATE OR ALTER VIEW</code> should update, but check: 1. Are you running against the right SQL Server? 2. Check the story for errors 3. Verify the SQL file content</p>"},{"location":"semantics/runner/#permission-denied","title":"Permission Denied","text":"<p>Symptom: <code>Cannot create schema</code> or <code>Cannot create view</code></p> <p>Solution: <pre><code>-- Grant permissions to your service account\nGRANT CREATE SCHEMA TO [your_user];\nGRANT CREATE VIEW TO [your_user];\nGRANT SELECT ON SCHEMA::dbo TO [your_user];  -- For source tables\n</code></pre></p>"},{"location":"semantics/runner/#stories-not-saving","title":"Stories Not Saving","text":"<p>Symptom: Run completes but no story files</p> <p>Check: 1. Is <code>story.auto_generate: true</code> set? 2. Does the <code>story.connection</code> have write access? 3. Check logs for storage errors</p>"},{"location":"semantics/runner/#see-also","title":"See Also","text":"<ul> <li>Defining Metrics - Metric and dimension definitions</li> <li>Querying - Ad-hoc metric queries</li> <li>Materializing - Pre-computing aggregates</li> <li>Lineage Stitcher - Combined lineage generation</li> <li>Stories - Pipeline execution stories</li> </ul>"},{"location":"tutorials/azure_connections/","title":"Azure Connections Tutorial","text":"<p>This tutorial shows how to connect Odibi to Azure data services: Blob Storage, ADLS Gen2, and Azure SQL.</p>"},{"location":"tutorials/azure_connections/#prerequisites","title":"Prerequisites","text":"<ul> <li>Odibi installed (<code>pip install odibi</code>)</li> <li>Azure subscription with appropriate permissions</li> <li>Azure CLI installed (<code>az login</code> completed)</li> </ul>"},{"location":"tutorials/azure_connections/#1-azure-blob-storage","title":"1. Azure Blob Storage","text":""},{"location":"tutorials/azure_connections/#service-principal-authentication-recommended-for-production","title":"Service Principal Authentication (Recommended for Production)","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: landing\n    auth:\n      mode: service_principal\n      tenant_id: \"${AZURE_TENANT_ID}\"\n      client_id: \"${AZURE_CLIENT_ID}\"\n      client_secret: \"${AZURE_CLIENT_SECRET}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#managed-identity-databrickssynapse","title":"Managed Identity (Databricks/Synapse)","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: landing\n    auth:\n      mode: managed_identity\n</code></pre>"},{"location":"tutorials/azure_connections/#sas-token","title":"SAS Token","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    account_name: mystorageaccount\n    container: landing\n    auth:\n      mode: sas\n      token: \"${AZURE_SAS_TOKEN}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#connection-string","title":"Connection String","text":"<pre><code>connections:\n  azure_landing:\n    type: azure_blob\n    connection_string: \"${AZURE_STORAGE_CONNECTION_STRING}\"\n    container: landing\n</code></pre>"},{"location":"tutorials/azure_connections/#2-adls-gen2-hierarchical-namespace","title":"2. ADLS Gen2 (Hierarchical Namespace)","text":"<p>For Delta Lake on Azure, use ADLS Gen2:</p> <pre><code>connections:\n  adls_bronze:\n    type: azure_blob\n    account_name: mydatalake\n    container: bronze\n    is_adls_gen2: true  # Enable hierarchical namespace\n    auth:\n      mode: service_principal\n      tenant_id: \"${AZURE_TENANT_ID}\"\n      client_id: \"${AZURE_CLIENT_ID}\"\n      client_secret: \"${AZURE_CLIENT_SECRET}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#delta-lake-on-adls","title":"Delta Lake on ADLS","text":"<pre><code>connections:\n  delta_silver:\n    type: delta\n    base_path: abfss://silver@mydatalake.dfs.core.windows.net/\n    # Auth inherited from Spark session config\n</code></pre>"},{"location":"tutorials/azure_connections/#3-azure-sql-database","title":"3. Azure SQL Database","text":""},{"location":"tutorials/azure_connections/#sql-authentication","title":"SQL Authentication","text":"<pre><code>connections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: sql_login\n      username: \"${SQL_USER}\"\n      password: \"${SQL_PASSWORD}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#azure-ad-authentication","title":"Azure AD Authentication","text":"<pre><code>connections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: aad_password\n      username: user@company.com\n      password: \"${AAD_PASSWORD}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#managed-identity","title":"Managed Identity","text":"<pre><code>connections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: msi\n</code></pre>"},{"location":"tutorials/azure_connections/#4-complete-azure-project","title":"4. Complete Azure Project","text":"<p>Here's a full project using Azure services:</p> <pre><code># project.yaml\nproject: \"azure_data_platform\"\nengine: spark\n\nconnections:\n  # Landing zone (raw files)\n  landing:\n    type: azure_blob\n    account_name: \"${STORAGE_ACCOUNT}\"\n    container: landing\n    auth:\n      mode: service_principal\n      tenant_id: \"${AZURE_TENANT_ID}\"\n      client_id: \"${AZURE_CLIENT_ID}\"\n      client_secret: \"${AZURE_CLIENT_SECRET}\"\n\n  # Bronze layer (Delta)\n  bronze:\n    type: delta\n    base_path: abfss://bronze@${STORAGE_ACCOUNT}.dfs.core.windows.net/\n\n  # Silver layer (Delta)  \n  silver:\n    type: delta\n    base_path: abfss://silver@${STORAGE_ACCOUNT}.dfs.core.windows.net/\n\n  # Source database\n  erp_sql:\n    type: sqlserver\n    server: erp-server.database.windows.net\n    database: erp_prod\n    auth:\n      mode: msi\n\nstory:\n  connection: landing\n  path: _odibi/stories/\n\nsystem:\n  connection: landing\n  path: _odibi/catalog/\n\npipelines:\n  - pipeline: bronze_ingest\n    layer: bronze\n    nodes:\n      # Ingest from SQL\n      - name: customers_raw\n        read:\n          connection: erp_sql\n          format: sql\n          table: dbo.Customers\n        write:\n          connection: bronze\n          table: raw_customers\n\n      # Ingest from files\n      - name: orders_raw\n        read:\n          connection: landing\n          format: csv\n          path: orders/*.csv\n        write:\n          connection: bronze\n          table: raw_orders\n</code></pre>"},{"location":"tutorials/azure_connections/#5-environment-variables","title":"5. Environment Variables","text":"<p>Store secrets in environment variables or Azure Key Vault:</p> <pre><code># .env (local development - git-ignored!)\nAZURE_TENANT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nAZURE_CLIENT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nAZURE_CLIENT_SECRET=your-secret-here\nSTORAGE_ACCOUNT=mydatalake\nSQL_USER=odibi_user\nSQL_PASSWORD=secure-password\n</code></pre>"},{"location":"tutorials/azure_connections/#using-azure-key-vault","title":"Using Azure Key Vault","text":"<pre><code># Reference Key Vault secrets with ${kv:secret-name}\nconnections:\n  azure_sql:\n    type: sqlserver\n    server: myserver.database.windows.net\n    database: mydb\n    auth:\n      mode: sql_login\n      username: \"${kv:sql-username}\"\n      password: \"${kv:sql-password}\"\n</code></pre>"},{"location":"tutorials/azure_connections/#6-spark-configuration-for-azure","title":"6. Spark Configuration for Azure","text":"<p>When using Spark with Azure, configure the session:</p> <pre><code># conf/spark_azure.py\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"OdibiAzure\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n    .config(\"fs.azure.account.auth.type\", \"OAuth\") \\\n    .config(\"fs.azure.account.oauth.provider.type\", \n            \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n    .config(\"fs.azure.account.oauth2.client.id\", os.environ[\"AZURE_CLIENT_ID\"]) \\\n    .config(\"fs.azure.account.oauth2.client.secret\", os.environ[\"AZURE_CLIENT_SECRET\"]) \\\n    .config(\"fs.azure.account.oauth2.client.endpoint\", \n            f\"https://login.microsoftonline.com/{os.environ['AZURE_TENANT_ID']}/oauth2/token\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"tutorials/azure_connections/#7-running-on-azure-databricks","title":"7. Running on Azure Databricks","text":""},{"location":"tutorials/azure_connections/#cluster-configuration","title":"Cluster Configuration","text":"<p>Add to cluster Spark config: <pre><code>fs.azure.account.auth.type OAuth\nfs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nfs.azure.account.oauth2.client.id {{secrets/odibi/client-id}}\nfs.azure.account.oauth2.client.secret {{secrets/odibi/client-secret}}\nfs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/{{secrets/odibi/tenant-id}}/oauth2/token\n</code></pre></p>"},{"location":"tutorials/azure_connections/#unity-catalog","title":"Unity Catalog","text":"<p>With Unity Catalog, use catalog-based connections:</p> <pre><code>connections:\n  bronze:\n    type: delta\n    catalog: main\n    schema: bronze\n\n  silver:\n    type: delta\n    catalog: main\n    schema: silver\n</code></pre>"},{"location":"tutorials/azure_connections/#8-running-on-azure-synapse","title":"8. Running on Azure Synapse","text":"<pre><code># Synapse notebook\n%%pyspark\nfrom odibi import run_project\n\n# Synapse auto-configures ADLS access via linked services\nrun_project(\"/synapse/project.yaml\", pipelines=[\"bronze_ingest\"])\n</code></pre>"},{"location":"tutorials/azure_connections/#common-issues","title":"Common Issues","text":""},{"location":"tutorials/azure_connections/#403-forbidden-on-blob-access","title":"\"403 Forbidden\" on Blob Access","text":"<ol> <li>Check service principal has \"Storage Blob Data Contributor\" role</li> <li>Verify container name is correct</li> <li>Check firewall allows your IP</li> </ol>"},{"location":"tutorials/azure_connections/#login-failed-on-azure-sql","title":"\"Login failed\" on Azure SQL","text":"<ol> <li>Verify Azure AD admin is set on SQL server</li> <li>Check firewall allows Azure services</li> <li>For MSI, ensure the managed identity has db_datareader/db_datawriter</li> </ol>"},{"location":"tutorials/azure_connections/#slow-adls-performance","title":"Slow ADLS Performance","text":"<p>Enable Delta caching: <pre><code>performance:\n  delta_table_properties:\n    delta.autoOptimize.optimizeWrite: true\n</code></pre></p>"},{"location":"tutorials/azure_connections/#next-steps","title":"Next Steps","text":"<ul> <li>Spark Engine Tutorial - Spark-specific features</li> <li>Getting Started - Basic Odibi concepts</li> <li>Performance Tuning - Optimize large pipelines</li> </ul>"},{"location":"tutorials/azure_connections/#see-also","title":"See Also","text":"<ul> <li>AzureBlobConnectionConfig - Full Azure Blob options</li> <li>SQLServerConnectionConfig - Azure SQL options</li> <li>DeltaConnectionConfig - Delta Lake options</li> </ul>"},{"location":"tutorials/bronze_layer/","title":"Bronze Layer Tutorial","text":"<p>The Bronze Layer is where raw data lands. No transformations, no cleaning\u2014just reliable ingestion with traceability.</p>"},{"location":"tutorials/bronze_layer/#layer-philosophy","title":"Layer Philosophy","text":"<p>\"Raw is sacred. Preserve everything, trust nothing.\"</p> <p>Bronze is your insurance policy. If downstream logic has bugs, you can always reprocess from Bronze.</p> Principle Why Append-only Never lose source data Schema as-is Don't transform on ingest Full fidelity Keep all columns, all rows Traceable Know when each row arrived"},{"location":"tutorials/bronze_layer/#quick-start-file-ingestion","title":"Quick Start: File Ingestion","text":"<p>The simplest Bronze pipeline loads files and appends them:</p> <pre><code># pipelines/bronze/ingest_orders.yaml\npipelines:\n  - pipeline: bronze_orders\n    layer: bronze\n    nodes:\n      - name: raw_orders\n        read:\n          connection: landing\n          format: csv\n          path: orders/*.csv\n          options:\n            header: true\n        write:\n          connection: bronze\n          table: raw_orders\n          mode: append\n</code></pre>"},{"location":"tutorials/bronze_layer/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":""},{"location":"tutorials/bronze_layer/#1-im-reprocessing-files-ive-already-loaded","title":"1. \"I'm reprocessing files I've already loaded\"","text":"<p>Problem: Each run loads all files, creating duplicates.</p> <p>Solution: Use stateful incremental tracking with a high-water mark column.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n      incremental:\n        mode: stateful               # Track HWM (high-water mark)\n        column: file_modified_date   # Column to track\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>How it works: - Odibi records the MAX value of <code>column</code> after each run - On next run, only rows with values &gt; stored HWM are processed - For time-based lookback instead, use <code>mode: rolling_window</code></p> <p>See: Incremental Loading Pattern</p>"},{"location":"tutorials/bronze_layer/#2-files-have-inconsistent-schemas","title":"2. \"Files have inconsistent schemas\"","text":"<p>Problem: New files have extra/missing columns, breaking the pipeline.</p> <p>Solution: Enable Delta schema evolution on write.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n      options:\n        header: true\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n      options:\n        mergeSchema: true            # Delta: allow schema evolution\n</code></pre> <p>How it works: - Spark infers schema from each file - Delta's <code>mergeSchema</code> adds new columns to the target table automatically - Odibi tracks schema changes in the System Catalog</p> <p>See: Schema Tracking</p>"},{"location":"tutorials/bronze_layer/#3-malformed-records-crash-the-pipeline","title":"3. \"Malformed records crash the pipeline\"","text":"<p>Problem: One bad CSV row fails the entire load.</p> <p>Solution: Route bad records to an error path using Spark options.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n      options:\n        mode: PERMISSIVE             # Don't fail on bad rows\n        columnNameOfCorruptRecord: _corrupt_record\n        badRecordsPath: /landing/errors/orders/\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>Result: - Valid rows load normally - Corrupt rows written to <code>badRecordsPath</code> for investigation - Pipeline doesn't fail</p>"},{"location":"tutorials/bronze_layer/#4-empty-source-files-break-downstream","title":"4. \"Empty source files break downstream\"","text":"<p>Problem: Source sends empty files, causing downstream failures.</p> <p>Solution: Add a row count contract.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n    contracts:\n      - type: row_count\n        min: 1                       # Fail if empty\n        severity: error\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>Severity options: | Severity | Behavior | |----------|----------| | <code>error</code> | Fail the node | | <code>warn</code> | Log warning, continue |</p> <p>See: Contracts</p>"},{"location":"tutorials/bronze_layer/#5-source-volume-dropped-90somethings-wrong","title":"5. \"Source volume dropped 90%\u2014something's wrong\"","text":"<p>Problem: Upstream system broke, sending almost no data.</p> <p>Solution: Add volume drop detection.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n    contracts:\n      - type: volume_drop\n        threshold: 0.5               # Fail if &lt;50% of previous run\n        lookback_runs: 3             # Compare to last 3 runs\n        severity: error\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre>"},{"location":"tutorials/bronze_layer/#6-i-need-to-reprocess-a-specific-date-range","title":"6. \"I need to reprocess a specific date range\"","text":"<p>Problem: Bug in source data, need to reload specific dates.</p> <p>Solution: Use Delta's partition replacement.</p> <pre><code>nodes:\n  - name: raw_orders\n    read:\n      connection: landing\n      format: csv\n      path: orders/*.csv\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: overwrite\n      options:\n        replaceWhere: \"file_date &gt;= '2025-01-01' AND file_date &lt;= '2025-01-15'\"\n        partitionBy: [file_date]\n</code></pre> <p>How it works: - <code>replaceWhere</code> only replaces matching partitions - Rest of the table remains unchanged - Useful for targeted reloads without full table rebuild</p> <p>See: Windowed Reprocess Pattern</p>"},{"location":"tutorials/bronze_layer/#7-im-loading-from-sql-server-not-files","title":"7. \"I'm loading from SQL Server, not files\"","text":"<p>Problem: Source is a database table, not files.</p> <p>Solution: Use SQL read with stateful incremental.</p> <pre><code>connections:\n  source_db:\n    type: sql_server\n    server: server.database.windows.net\n    database: sales\n    user: \"${DB_USER}\"\n    password: \"${DB_PASSWORD}\"\n\nnodes:\n  - name: raw_orders\n    read:\n      connection: source_db\n      format: jdbc\n      table: dbo.orders\n      incremental:\n        mode: stateful               # Track HWM\n        column: updated_at           # Track by this column\n    write:\n      connection: bronze\n      table: raw_orders\n      mode: append\n</code></pre> <p>How it works: - First run: loads all data, stores MAX(updated_at) as HWM - Next runs: loads only WHERE updated_at &gt; stored_hwm - HWM is updated after each successful run</p> <p>See: Incremental Loading Pattern</p>"},{"location":"tutorials/bronze_layer/#bronze-layer-checklist","title":"Bronze Layer Checklist","text":"<p>Before moving to Silver, verify:</p> <ul> <li>[ ] Append-only? Raw data is never overwritten (except intentional reprocess)</li> <li>[ ] Incremental? Only new/changed data is loaded each run</li> <li>[ ] Traceable? Each row has arrival metadata (<code>_loaded_at</code>, source file, etc.)</li> <li>[ ] Contracts? Row count, schema, or volume checks in place</li> <li>[ ] Error handling? Bad records routed to error path, not failing pipeline</li> </ul>"},{"location":"tutorials/bronze_layer/#next-steps","title":"Next Steps","text":"<ul> <li>Silver Layer Tutorial \u2014 Clean and transform Bronze data</li> <li>Append-Only Raw Pattern \u2014 Detailed pattern docs</li> <li>Getting Started \u2014 End-to-end first pipeline</li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started with Odibi","text":"<p>This tutorial will guide you through creating your first data pipeline. By the end, you will have a running project that reads data, cleans it, and generates an audit report (\"Data Story\").</p> <p>Prerequisites: *   Python 3.9 or higher installed. *   Basic familiarity with terminal/command line.</p>"},{"location":"tutorials/getting_started/#1-installation","title":"1. Installation","text":"<p>First, install Odibi. We recommend creating a virtual environment to keep your system clean.</p> <pre><code># 1. Create a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# 2. Install Odibi\npip install odibi\n</code></pre> <p>Note: If you plan to use Spark or Azure later, you can install <code>pip install \"odibi[spark,azure]\"</code>, but for this tutorial, the base package is enough.</p>"},{"location":"tutorials/getting_started/#2-create-sample-data","title":"2. Create Sample Data","text":"<p>Odibi shines when working with messy real-world data. Let's create some \"bad\" data to clean.</p> <p>Create a folder named <code>raw_data</code> and a file inside it named <code>customers.csv</code>:</p> <p>raw_data/customers.csv <pre><code>id, name,           email,              joined_at\n1,  Alice,          alice@example.com,  2023-01-01\n2,  Bob,            bob@example.com,    2023-02-15\n3,  Charlie,        NULL,               2023-03-10\n4,  Dave,           dave@example.com,   invalid-date\n</code></pre> (Notice the extra spaces, the NULL value, and the invalid date string.)</p>"},{"location":"tutorials/getting_started/#3-generate-your-project","title":"3. Generate Your Project","text":"<p>Instead of writing configuration files from scratch, use the Odibi Initializer. It creates a project skeleton with best practices baked in.</p> <p>Run this command in your terminal:</p> <pre><code>odibi init-pipeline my_first_project --template local-medallion\n</code></pre> <p>This creates a new folder <code>my_first_project</code> with a standard structure: *   <code>odibi.yaml</code>: The pipeline configuration. *   <code>data/</code>: Folders for your data layers (landing, raw, silver, etc.). *   <code>README.md</code>: Instructions for your project.</p> <p>Move your sample data into the landing zone: <pre><code># On Windows (PowerShell)\nmv raw_data/customers.csv my_first_project/data/landing/\n# On Mac/Linux\nmv raw_data/customers.csv my_first_project/data/landing/\n</code></pre></p> <p>Note: You can also generate a project from existing data using <code>odibi generate-project</code>, but <code>init-pipeline</code> is the recommended way to start fresh.</p>"},{"location":"tutorials/getting_started/#4-explore-the-project","title":"4. Explore the Project","text":"<p>Navigate into your new project:</p> <pre><code>cd my_first_project\n</code></pre> <p>You will see a file structure like this:</p> <ul> <li><code>odibi.yaml</code>: The brain of your project. It defines the pipeline.</li> <li><code>sql/</code>: Contains SQL transformation files.</li> <li><code>data/</code>: (Created automatically) Where data will be stored.</li> </ul> <p>Open <code>odibi.yaml</code> in your text editor. You will see two \"nodes\" (steps): 1.  Ingestion Node: Reads the <code>customers.csv</code> from <code>landing/</code>. 2.  Refinement Node: Merges the data into <code>silver/</code>.</p> <p>Since we used the template, the config is already set up to look for <code>landing/customers.csv</code>.</p>"},{"location":"tutorials/getting_started/#5-run-the-pipeline","title":"5. Run the Pipeline","text":"<p>Now, execute the pipeline:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Odibi will: 1.  Read <code>customers.csv</code> from <code>landing/</code>. 2.  Convert it to Parquet in <code>raw/</code>. 3.  Merge it into a Delta/Parquet table in <code>silver/</code>. 4.  Generate a \"Data Story\".</p>"},{"location":"tutorials/getting_started/#6-view-the-data-story","title":"6. View the Data Story","text":"<p>Data engineering is often invisible. Odibi makes it visible. Every run generates a report.</p> <p>List the generated stories:</p> <pre><code>odibi story list\n</code></pre> <p>You will see output like: <pre><code>\ud83d\udcda Stories in .odibi/stories:\n================================================================================\n  \ud83d\udcc4 main_documentation.html\n     Modified: 2025-11-21 14:30:00\n     Size: 15.2KB\n     Path: .odibi/stories/main_documentation.html\n</code></pre></p> <p>Open the HTML file in your browser to view the report: - Windows: <code>start .odibi/stories/main_documentation.html</code> - Mac: <code>open .odibi/stories/main_documentation.html</code> - Linux: <code>xdg-open .odibi/stories/main_documentation.html</code></p> <p>What to look for in the report: *   Row Counts: Did we lose any rows? *   Schema: Did the column types change? *   Execution Time: How long did it take?</p>"},{"location":"tutorials/getting_started/#7-add-data-validation","title":"7. Add Data Validation","text":"<p>Data pipelines are only as good as their data quality. Let's add validation tests to catch bad data before it corrupts your warehouse.</p>"},{"location":"tutorials/getting_started/#inline-validation-in-yaml","title":"Inline Validation in YAML","text":"<p>Add validation tests directly to your node:</p> <pre><code>nodes:\n  - name: customers\n    read:\n      connection: landing\n      format: csv\n      path: customers.csv\n    validation:\n      tests:\n        - type: not_null\n          columns: [id, name]\n        - type: unique\n          columns: [id]\n        - type: row_count\n          min: 1\n      on_failure: warn  # or \"error\" to stop the pipeline\n    write:\n      connection: raw\n      format: parquet\n      path: customers\n</code></pre>"},{"location":"tutorials/getting_started/#using-contracts-for-input-validation","title":"Using Contracts for Input Validation","text":"<p>Contracts validate data before processing:</p> <pre><code>nodes:\n  - name: validate_orders\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id, amount]\n      - type: freshness\n        column: created_at\n        max_age: \"24h\"\n    read:\n      connection: landing\n      path: orders.csv\n    write:\n      connection: raw\n      path: orders\n</code></pre> <p>If contracts fail, the pipeline stops immediately with clear error messages.</p>"},{"location":"tutorials/getting_started/#running-validation","title":"Running Validation","text":"<p>Run the pipeline and watch for validation warnings:</p> <pre><code>odibi run odibi.yaml\n</code></pre> <p>Validation results appear in both the console output and the Data Story.</p>"},{"location":"tutorials/getting_started/#8-building-dimensions-scd2","title":"8. Building Dimensions (SCD2)","text":"<p>Once you're comfortable with basic pipelines, you can build proper dimensional models. Here's a quick example of a Slowly Changing Dimension Type 2:</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: bronze\n      table: raw_customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id        # Business key\n        surrogate_key: customer_sk      # Generated integer key\n        scd_type: 2                     # Track history\n        track_cols: [name, email, city]\n        target: silver.dim_customer     # Read existing for merge\n        unknown_member: true            # Add SK=0 for orphans\n    write:\n      connection: silver\n      table: dim_customer\n</code></pre> <p>What this does: - Generates integer surrogate keys (<code>customer_sk</code>) - Tracks changes to <code>name</code>, <code>email</code>, <code>city</code> over time - Maintains <code>is_current</code>, <code>valid_from</code>, <code>valid_to</code> columns - Creates an \"unknown\" row (SK=0) for handling orphan fact records</p> <p>For a complete dimensional modeling tutorial, see Dimensional Modeling.</p>"},{"location":"tutorials/getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/getting_started/#modulenotfounderror-no-module-named-odibi","title":"\"ModuleNotFoundError: No module named 'odibi'\"","text":"<p>Cause: Odibi not installed or virtual environment not activated.</p> <p>Fix: <pre><code># Activate your virtual environment first\nsource .venv/bin/activate  # Linux/Mac\n.venv\\Scripts\\activate     # Windows\n\n# Then verify installation\npip show odibi\n</code></pre></p>"},{"location":"tutorials/getting_started/#pipeline-runs-but-no-output-files","title":"Pipeline runs but no output files","text":"<p>Causes: - Write path doesn't exist - Permission denied on output directory - Dry-run mode enabled</p> <p>Fix: <pre><code># Check if dry-run is enabled (remove --dry-run flag)\nodibi run odibi.yaml\n\n# Ensure output directory exists\nmkdir -p data/silver\n</code></pre></p>"},{"location":"tutorials/getting_started/#no-such-file-or-directory-for-input-data","title":"\"No such file or directory\" for input data","text":"<p>Cause: File path in config doesn't match actual location.</p> <p>Fix: Verify the path relative to where you run the command: <pre><code># If config says: path: landing/customers.csv\n# File should be at: ./data/landing/customers.csv (relative to base_path)\n\nls data/landing/customers.csv\n</code></pre></p>"},{"location":"tutorials/getting_started/#story-not-generated","title":"Story not generated","text":"<p>Causes: - Story connection not configured - Story path doesn't exist</p> <p>Fix: Ensure your config has a story section: <pre><code>story:\n  connection: raw_data  # Must match a defined connection\n  path: stories/\n</code></pre></p>"},{"location":"tutorials/getting_started/#9-whats-next","title":"9. What's Next?","text":"<p>You have successfully built a data pipeline with data validation!</p> <ul> <li>Incremental Loading: Learn how to efficiently process only new data using State Tracking (\"Auto-Pilot\").</li> <li>Write Custom Transformations: Learn how to add Python logic (like advanced validation) to your pipeline.</li> <li>Data Validation Guide: Deep dive into all validation options.</li> <li>Spark Engine Tutorial: Scale up with Apache Spark.</li> <li>Azure Connections: Connect to Azure Blob, ADLS, and SQL.</li> <li>Master the CLI: Learn about <code>odibi stress</code> and <code>odibi doctor</code>.</li> </ul>"},{"location":"tutorials/gold_layer/","title":"Gold Layer Tutorial","text":"<p>The Gold Layer is where business-ready datasets live. Fact tables, aggregations, and semantic metrics\u2014optimized for consumption.</p>"},{"location":"tutorials/gold_layer/#layer-philosophy","title":"Layer Philosophy","text":"<p>\"Answers, not data.\"</p> <p>Gold is consumption-optimized. BI tools, dashboards, and ML models read from Gold. Queries should be fast and intuitive.</p> Principle Why Denormalized Fewer joins = faster queries Pre-aggregated Common rollups pre-computed Business-named Column names match business terms SK-based Surrogate keys for dimension lookups"},{"location":"tutorials/gold_layer/#quick-start-fact-table","title":"Quick Start: Fact Table","text":"<p>The most common Gold pattern is a fact table with dimension lookups:</p> <pre><code># pipelines/gold/fact_orders.yaml\npipelines:\n  - pipeline: gold_fact_orders\n    layer: gold\n    nodes:\n      - name: fact_orders\n        read:\n          connection: silver\n          table: orders\n        pattern:\n          type: fact\n          params:\n            grain: [order_id, line_item_id]\n            dimensions:\n              - name: dim_customer\n                lookup_key: customer_id\n                surrogate_key: customer_sk\n                target: silver.dim_customer\n              - name: dim_product\n                lookup_key: product_id\n                surrogate_key: product_sk\n                target: silver.dim_product\n              - name: dim_date\n                lookup_key: order_date\n                surrogate_key: date_sk\n                target: gold.dim_date\n            orphan_handling: unknown\n        write:\n          connection: gold\n          table: fact_orders\n</code></pre>"},{"location":"tutorials/gold_layer/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":""},{"location":"tutorials/gold_layer/#1-how-do-i-build-a-star-schema-fact-table","title":"1. \"How do I build a star schema fact table?\"","text":"<p>Problem: Need to replace natural keys with surrogate keys from dimensions.</p> <p>Solution: Use the fact pattern with dimension lookups.</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: silver\n      table: orders\n    pattern:\n      type: fact\n      params:\n        grain: [order_id]            # One row per order\n        dimensions:\n          - name: dim_customer\n            lookup_key: customer_id\n            surrogate_key: customer_sk\n            target: silver.dim_customer\n          - name: dim_product\n            lookup_key: product_id\n            surrogate_key: product_sk\n            target: silver.dim_product\n    write:\n      connection: gold\n      table: fact_orders\n</code></pre> <p>Result: <pre><code>order_id | customer_sk | product_sk | order_total | order_date\n1        | 42          | 15         | 150.00      | 2025-01-15\n2        | 42          | 23         | 75.00       | 2025-01-16\n</code></pre></p> <p>See: Fact Pattern</p>"},{"location":"tutorials/gold_layer/#2-orders-reference-customers-that-dont-exist-orphans","title":"2. \"Orders reference customers that don't exist (orphans)\"","text":"<p>Problem: Some orders have customer_id values not in dim_customer.</p> <p>Solution: Configure orphan handling.</p> <pre><code>pattern:\n  type: fact\n  params:\n    grain: [order_id]\n    dimensions:\n      - name: dim_customer\n        lookup_key: customer_id\n        surrogate_key: customer_sk\n        target: silver.dim_customer\n    orphan_handling: unknown         # Assign to unknown member\n</code></pre> <p>Options for <code>orphan_handling</code>: | Option | Behavior | |--------|----------| | <code>unknown</code> | Assign SK = -1 (unknown member) | | <code>quarantine</code> | Route to quarantine table | | <code>error</code> | Fail the pipeline | | <code>null</code> | Set SK = NULL |</p> <p>See: Fact Pattern - Orphan Handling</p>"},{"location":"tutorials/gold_layer/#3-i-need-a-date-dimension","title":"3. \"I need a date dimension\"","text":"<p>Problem: Need a standard date dimension for time-based analysis.</p> <p>Solution: Use the date dimension pattern.</p> <pre><code>nodes:\n  - name: dim_date\n    pattern:\n      type: date_dimension\n      params:\n        start_date: \"2020-01-01\"\n        end_date: \"2030-12-31\"\n        columns:\n          - date_sk               # Surrogate key (YYYYMMDD)\n          - full_date             # DATE type\n          - day_of_week           # Monday, Tuesday, ...\n          - day_of_month          # 1-31\n          - month_name            # January, February, ...\n          - month_number          # 1-12\n          - quarter               # Q1, Q2, Q3, Q4\n          - year                  # 2024, 2025, ...\n          - is_weekend            # true/false\n          - fiscal_year           # Custom fiscal calendar\n    write:\n      connection: gold\n      table: dim_date\n</code></pre> <p>See: Date Dimension Pattern</p>"},{"location":"tutorials/gold_layer/#4-i-need-pre-aggregated-metrics","title":"4. \"I need pre-aggregated metrics\"","text":"<p>Problem: Dashboards are slow\u2014need pre-computed rollups.</p> <p>Solution: Use the aggregation pattern.</p> <pre><code>nodes:\n  - name: daily_sales\n    read:\n      connection: gold\n      table: fact_orders\n    pattern:\n      type: aggregation\n      params:\n        dimensions: [date_sk, product_sk]\n        measures:\n          - name: total_revenue\n            expression: \"SUM(order_total)\"\n          - name: order_count\n            expression: \"COUNT(*)\"\n          - name: avg_order_value\n            expression: \"AVG(order_total)\"\n        incremental: true            # Merge new days\n    write:\n      connection: gold\n      table: agg_daily_sales\n</code></pre> <p>Result: <pre><code>date_sk  | product_sk | total_revenue | order_count | avg_order_value\n20250115 | 15         | 1500.00       | 10          | 150.00\n20250115 | 23         | 750.00        | 10          | 75.00\n</code></pre></p> <p>See: Aggregation Pattern</p>"},{"location":"tutorials/gold_layer/#5-i-want-to-define-reusable-metrics-for-bi","title":"5. \"I want to define reusable metrics for BI\"","text":"<p>Problem: Different dashboards calculate \"revenue\" differently.</p> <p>Solution: Define semantic metrics.</p> <pre><code>semantic:\n  metrics:\n    - name: revenue\n      expression: \"SUM(order_total)\"\n      description: \"Total order revenue\"\n      format: currency\n\n    - name: order_count\n      expression: \"COUNT(DISTINCT order_id)\"\n      description: \"Number of unique orders\"\n      format: integer\n\n    - name: aov\n      expression: \"SUM(order_total) / COUNT(DISTINCT order_id)\"\n      description: \"Average order value\"\n      format: currency\n      depends_on: [revenue, order_count]\n\n  dimensions:\n    - name: customer_name\n      column: dim_customer.name\n\n    - name: product_category\n      column: dim_product.category\n\n    - name: order_month\n      column: dim_date.month_name\n</code></pre> <p>See: Semantic Layer, Defining Metrics</p>"},{"location":"tutorials/gold_layer/#6-how-do-i-materialize-semantic-metrics-to-tables","title":"6. \"How do I materialize semantic metrics to tables?\"","text":"<p>Problem: Want to query metrics from SQL, not just the API.</p> <p>Solution: Materialize metrics to Gold tables.</p> <pre><code>nodes:\n  - name: materialized_revenue\n    semantic:\n      materialize:\n        metrics: [revenue, order_count, aov]\n        dimensions: [product_category, order_month]\n        target: gold.revenue_by_category_month\n</code></pre> <p>See: Materializing Metrics</p>"},{"location":"tutorials/gold_layer/#7-reference-data-rarely-changesskip-if-unchanged","title":"7. \"Reference data rarely changes\u2014skip if unchanged\"","text":"<p>Problem: Date dimension regenerates every run unnecessarily.</p> <p>Solution: Skip if content hash is unchanged.</p> <pre><code>nodes:\n  - name: dim_date\n    pattern:\n      type: date_dimension\n      params:\n        start_date: \"2020-01-01\"\n        end_date: \"2030-12-31\"\n    write:\n      connection: gold\n      table: dim_date\n      format: delta\n      skip_if_unchanged: true        # Skip if content hash matches\n</code></pre> <p>How it works: - Before writing, Odibi computes a SHA256 hash of the DataFrame - Compares to hash stored in Delta table metadata - Skips write if hashes match (saves storage and compute)</p> <p>See: Skip If Unchanged Pattern</p>"},{"location":"tutorials/gold_layer/#8-how-do-i-validate-fact-table-grain","title":"8. \"How do I validate fact table grain?\"","text":"<p>Problem: Want to ensure no duplicate rows per grain key.</p> <p>Solution: Add grain validation contract.</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: silver\n      table: orders\n    contracts:\n      - type: unique\n        columns: [order_id, line_item_id]  # Grain columns\n        severity: error\n    pattern:\n      type: fact\n      params:\n        grain: [order_id, line_item_id]\n    write:\n      connection: gold\n      table: fact_orders\n</code></pre>"},{"location":"tutorials/gold_layer/#gold-layer-checklist","title":"Gold Layer Checklist","text":"<p>Before exposing to BI:</p> <ul> <li>[ ] Star schema? Facts reference dimensions via surrogate keys</li> <li>[ ] Grain validated? No duplicate rows per grain key</li> <li>[ ] Orphans handled? Missing dimension members \u2192 unknown or quarantine</li> <li>[ ] Pre-aggregated? Common rollups materialized</li> <li>[ ] Documented? Semantic layer defines metrics and dimensions</li> </ul>"},{"location":"tutorials/gold_layer/#star-schema-example","title":"Star Schema Example","text":"<pre><code>                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502   dim_date      \u2502\n                   \u2502  date_sk (PK)   \u2502\n                   \u2502  full_date      \u2502\n                   \u2502  month_name     \u2502\n                   \u2502  year           \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  dim_customer   \u2502    \u2502   fact_orders    \u2502    \u2502  dim_product    \u2502\n\u2502 customer_sk(PK) \u2502\u25c4\u2500\u2500\u2500\u2502 customer_sk(FK)  \u2502\u2500\u2500\u2500\u25ba\u2502 product_sk (PK) \u2502\n\u2502 customer_id     \u2502    \u2502 product_sk (FK)  \u2502    \u2502 product_id      \u2502\n\u2502 name            \u2502    \u2502 date_sk (FK)     \u2502    \u2502 name            \u2502\n\u2502 city            \u2502    \u2502 order_id         \u2502    \u2502 category        \u2502\n\u2502 state           \u2502    \u2502 order_total      \u2502    \u2502 price           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 quantity         \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/gold_layer/#next-steps","title":"Next Steps","text":"<ul> <li>Fact Pattern \u2014 Detailed fact table configuration</li> <li>Aggregation Pattern \u2014 Pre-computed rollups</li> <li>Semantic Layer Overview \u2014 Reusable metrics</li> <li>Dimensional Modeling Tutorial \u2014 Full walkthrough</li> </ul>"},{"location":"tutorials/silver_layer/","title":"Silver Layer Tutorial","text":"<p>The Silver Layer is where data gets cleaned, deduplicated, and conformed. This is your trusted, query-ready data.</p>"},{"location":"tutorials/silver_layer/#layer-philosophy","title":"Layer Philosophy","text":"<p>\"Clean once, use everywhere.\"</p> <p>Silver is your single source of truth. All downstream consumers (Gold, BI, ML) should read from Silver, not Bronze.</p> Principle Why Deduplicated One row per key Validated Data quality enforced Typed Consistent data types Conformed Standard naming, formats"},{"location":"tutorials/silver_layer/#the-one-source-test","title":"The One-Source Test","text":"<p>\"Could this node run if only ONE source system existed?\"</p> <ul> <li>YES \u2192 Silver \u2713</li> <li>NO \u2192 Probably Gold</li> </ul> <p>Reference Tables Are Allowed</p> <p>Reference/lookup table joins ARE allowed in Silver. The test refers to business source systems, not supporting data.</p> <ul> <li>\u2705 <code>orders</code> JOIN <code>product_codes</code> (lookup) = Silver</li> <li>\u274c <code>sap_orders</code> JOIN <code>salesforce_customers</code> = Gold</li> </ul>"},{"location":"tutorials/silver_layer/#quick-start-merge-from-bronze","title":"Quick Start: Merge from Bronze","text":"<p>The most common Silver pattern merges Bronze data into a deduplicated table:</p> <pre><code># pipelines/silver/orders.yaml\npipelines:\n  - pipeline: silver_orders\n    layer: silver\n    nodes:\n      - name: orders\n        read:\n          connection: bronze\n          table: raw_orders\n        transformer: merge\n        params:\n          target: silver.orders\n          keys: [order_id]\n          strategy: upsert\n        write:\n          connection: silver\n          table: orders\n</code></pre>"},{"location":"tutorials/silver_layer/#common-problems-solutions","title":"Common Problems &amp; Solutions","text":""},{"location":"tutorials/silver_layer/#1-bronze-has-duplicates-how-do-i-get-one-row-per-key","title":"1. \"Bronze has duplicates, how do I get one row per key?\"","text":"<p>Problem: Raw data has multiple versions of the same record.</p> <p>Solution: Use the merge transformer with deduplication.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transformer: deduplicate\n    params:\n      keys: [order_id]\n      order_by: \"updated_at DESC\"    # Keep most recent\n    write:\n      connection: silver\n      table: orders\n</code></pre> <p>Or use merge for incremental upsert:</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transformer: merge\n    params:\n      target: silver.orders\n      keys: [order_id]\n      strategy: upsert\n      audit_cols:\n        created_col: _sys_created_at\n        updated_col: _sys_updated_at\n</code></pre> <p>See: Merge/Upsert Pattern</p>"},{"location":"tutorials/silver_layer/#2-i-need-to-track-dimension-history-scd-type-2","title":"2. \"I need to track dimension history (SCD Type 2)\"","text":"<p>Problem: Customer address changes\u2014need to keep historical versions.</p> <p>Solution: Use the dimension pattern with SCD Type 2.</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: bronze\n      table: raw_customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id\n        surrogate_key: customer_sk\n        scd_type: 2\n        track_cols: [name, email, address, city, state]\n        target: silver.dim_customer\n        effective_from_col: valid_from\n        effective_to_col: valid_to\n        current_flag_col: is_current\n    write:\n      connection: silver\n      table: dim_customer\n</code></pre> <p>Result: <pre><code>customer_sk | customer_id | name     | city      | valid_from | valid_to   | is_current\n1           | C001        | Alice    | Chicago   | 2024-01-01 | 2024-06-01 | false\n2           | C001        | Alice    | Boston    | 2024-06-01 | 9999-12-31 | true\n</code></pre></p> <p>See: SCD2 Pattern</p>"},{"location":"tutorials/silver_layer/#3-just-overwrite-dimensions-i-dont-need-history","title":"3. \"Just overwrite dimensions, I don't need history\"","text":"<p>Problem: Reference data that should just reflect current state.</p> <p>Solution: Use SCD Type 1 (no history).</p> <pre><code>nodes:\n  - name: dim_product\n    read:\n      connection: bronze\n      table: raw_products\n    pattern:\n      type: dimension\n      params:\n        natural_key: product_id\n        surrogate_key: product_sk\n        scd_type: 1                  # Overwrite changes\n        target: silver.dim_product\n    write:\n      connection: silver\n      table: dim_product\n</code></pre> <p>See: Dimension Pattern</p>"},{"location":"tutorials/silver_layer/#4-how-do-i-validate-data-quality-in-silver","title":"4. \"How do I validate data quality in Silver?\"","text":"<p>Problem: Want to catch bad data before it reaches Gold.</p> <p>Solution: Add validation tests with quarantine.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    validation:\n      tests:\n        - column: order_id\n          test: not_null\n        - column: order_total\n          test: positive\n        - column: customer_id\n          test: not_null\n        - column: order_date\n          test: not_future\n      quarantine:\n        connection: silver\n        table: _quarantine_orders\n      gate:\n        require_pass_rate: 0.95      # Allow 5% failures\n    write:\n      connection: silver\n      table: orders\n</code></pre> <p>Result: - Rows passing all tests \u2192 <code>silver.orders</code> - Rows failing tests \u2192 <code>silver._quarantine_orders</code> for review - Pipeline fails if pass rate &lt; 95%</p> <p>See: Quality Gates, Quarantine</p>"},{"location":"tutorials/silver_layer/#5-i-need-to-apply-custom-sql-transformations","title":"5. \"I need to apply custom SQL transformations\"","text":"<p>Problem: Need to clean/transform data with custom logic.</p> <p>Solution: Use transform steps.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transform:\n      steps:\n        - type: sql\n          query: |\n            SELECT\n              order_id,\n              UPPER(TRIM(customer_name)) AS customer_name,\n              CAST(order_date AS DATE) AS order_date,\n              COALESCE(order_total, 0) AS order_total,\n              CASE\n                WHEN status = 'C' THEN 'Completed'\n                WHEN status = 'P' THEN 'Pending'\n                ELSE 'Unknown'\n              END AS order_status\n            FROM {input}\n            WHERE order_id IS NOT NULL\n    write:\n      connection: silver\n      table: orders\n</code></pre> <p>See: Writing Transformations</p>"},{"location":"tutorials/silver_layer/#6-records-were-deleted-in-sourcehow-do-i-detect-that","title":"6. \"Records were deleted in source\u2014how do I detect that?\"","text":"<p>Problem: Source system hard-deletes records, need to flag them.</p> <p>Solution: Use delete detection.</p> <pre><code>nodes:\n  - name: orders\n    read:\n      connection: bronze\n      table: raw_orders\n    transformer: merge\n    params:\n      target: silver.orders\n      keys: [order_id]\n      strategy: upsert\n    delete_detection:\n      mode: sql_compare              # Compare source to target\n      soft_delete_col: is_deleted    # Flag instead of delete\n      deleted_at_col: deleted_at     # Timestamp of detection\n</code></pre> <p>Result: <pre><code>order_id | ... | is_deleted | deleted_at\n1        | ... | false      | NULL\n2        | ... | true       | 2025-01-15 10:30:00  \u2190 Detected as deleted\n</code></pre></p> <p>See: Delete Detection Config</p>"},{"location":"tutorials/silver_layer/#7-i-need-to-check-foreign-key-relationships","title":"7. \"I need to check foreign key relationships\"","text":"<p>Problem: Orders reference customers that don't exist.</p> <p>Solution: Use the FK validation Python API (not YAML\u2014this is a programmatic feature).</p> <pre><code>from odibi.validation.fk import FKValidator, RelationshipRegistry, RelationshipConfig\n\n# Define relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_id\",\n        dimension_key=\"customer_id\",\n        on_violation=\"warn\"  # or \"error\", \"quarantine\"\n    )\n]\n\n# Validate\nregistry = RelationshipRegistry(relationships=relationships)\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(orders_df, \"orders\", context)\n\nif not report.all_valid:\n    print(f\"Found {len(report.orphan_records)} orphan records\")\n</code></pre> <p>See: FK Validation</p>"},{"location":"tutorials/silver_layer/#8-i-need-to-join-data-from-multiple-bronze-tables","title":"8. \"I need to join data from multiple Bronze tables\"","text":"<p>Problem: Order details and order headers in separate tables.</p> <p>Solution: Use multi-read with SQL join.</p> <pre><code>nodes:\n  - name: orders_enriched\n    read:\n      - alias: headers\n        connection: bronze\n        table: raw_order_headers\n      - alias: details\n        connection: bronze\n        table: raw_order_details\n    transform:\n      steps:\n        - type: sql\n          query: |\n            SELECT\n              h.order_id,\n              h.order_date,\n              h.customer_id,\n              d.product_id,\n              d.quantity,\n              d.unit_price\n            FROM headers h\n            JOIN details d ON h.order_id = d.order_id\n    write:\n      connection: silver\n      table: orders_enriched\n</code></pre>"},{"location":"tutorials/silver_layer/#silver-layer-checklist","title":"Silver Layer Checklist","text":"<p>Before moving to Gold, verify:</p> <ul> <li>[ ] Deduplicated? One row per natural key</li> <li>[ ] Validated? Quality tests passing</li> <li>[ ] Typed? Consistent data types (dates, numbers, etc.)</li> <li>[ ] Complete? FK relationships valid (or orphans quarantined)</li> <li>[ ] Conformed? Naming conventions followed</li> </ul>"},{"location":"tutorials/silver_layer/#next-steps","title":"Next Steps","text":"<ul> <li>Gold Layer Tutorial \u2014 Build facts and aggregations</li> <li>Dimension Pattern \u2014 SCD1/SCD2 details</li> <li>Merge/Upsert Pattern \u2014 Deduplication and upsert</li> </ul>"},{"location":"tutorials/spark_engine/","title":"Getting Started with Spark Engine","text":"<p>This tutorial shows how to use Odibi with Apache Spark for large-scale data processing.</p>"},{"location":"tutorials/spark_engine/#prerequisites","title":"Prerequisites","text":"<ul> <li>Odibi installed (<code>pip install odibi</code>)</li> <li>Apache Spark 3.x installed or access to Databricks/Synapse</li> <li>Basic familiarity with Getting Started</li> </ul>"},{"location":"tutorials/spark_engine/#why-spark","title":"Why Spark?","text":"Use Case Recommended Engine Small datasets (&lt;1GB) <code>pandas</code> Medium datasets (1-10GB) <code>polars</code> Large datasets (&gt;10GB) <code>spark</code> Streaming data <code>spark</code> Delta Lake tables <code>spark</code>"},{"location":"tutorials/spark_engine/#1-configure-spark-engine","title":"1. Configure Spark Engine","text":"<p>Set <code>engine: spark</code> in your project configuration:</p> <pre><code># project.yaml\nproject: \"spark_demo\"\nengine: spark  # Use Spark instead of Pandas\n\nconnections:\n  landing:\n    type: local\n    base_path: /data/landing\n\n  bronze:\n    type: delta\n    catalog: spark_catalog\n    schema: bronze\n\n  silver:\n    type: delta\n    catalog: spark_catalog\n    schema: silver\n\nstory:\n  connection: landing\n  path: stories/\n\nsystem:\n  connection: landing\n  path: catalog/\n</code></pre>"},{"location":"tutorials/spark_engine/#2-delta-lake-connections","title":"2. Delta Lake Connections","text":"<p>Spark works best with Delta Lake for ACID transactions and time travel:</p> <pre><code>connections:\n  # Unity Catalog (Databricks)\n  unity_bronze:\n    type: delta\n    catalog: main\n    schema: bronze\n\n  # Hive Metastore\n  hive_silver:\n    type: delta\n    catalog: spark_catalog\n    schema: silver\n\n  # Path-based Delta (no catalog)\n  adls_gold:\n    type: delta\n    base_path: abfss://container@account.dfs.core.windows.net/gold\n</code></pre>"},{"location":"tutorials/spark_engine/#3-basic-spark-pipeline","title":"3. Basic Spark Pipeline","text":"<pre><code># pipelines/bronze/ingest.yaml\npipelines:\n  - pipeline: bronze_ingest\n    layer: bronze\n    nodes:\n      - name: raw_orders\n        read:\n          connection: landing\n          format: csv\n          path: orders/*.csv\n          options:\n            header: true\n            inferSchema: true\n        write:\n          connection: bronze\n          table: raw_orders\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/spark_engine/#4-spark-specific-features","title":"4. Spark-Specific Features","text":""},{"location":"tutorials/spark_engine/#streaming-ingestion","title":"Streaming Ingestion","text":"<p>Process real-time data from Kafka or Event Hub:</p> <pre><code>nodes:\n  - name: stream_events\n    streaming: true  # Enable Spark Structured Streaming\n    read:\n      connection: event_hub\n      format: kafka\n      options:\n        kafka.bootstrap.servers: \"${KAFKA_BROKERS}\"\n        subscribe: events\n        startingOffsets: latest\n    write:\n      connection: bronze\n      table: events\n      mode: append\n      options:\n        checkpointLocation: /checkpoints/events\n</code></pre>"},{"location":"tutorials/spark_engine/#delta-optimizations","title":"Delta Optimizations","text":"<pre><code>nodes:\n  - name: optimize_facts\n    read:\n      connection: silver\n      table: fact_orders\n    post_sql:\n      - \"OPTIMIZE silver.fact_orders ZORDER BY (order_date, customer_sk)\"\n      - \"VACUUM silver.fact_orders RETAIN 168 HOURS\"\n    write:\n      connection: silver\n      table: fact_orders\n      mode: overwrite\n</code></pre>"},{"location":"tutorials/spark_engine/#partition-pruning","title":"Partition Pruning","text":"<pre><code>nodes:\n  - name: partitioned_orders\n    read:\n      connection: bronze\n      table: raw_orders\n    write:\n      connection: silver\n      table: orders\n      options:\n        partitionBy: [order_date]\n        replaceWhere: \"order_date &gt;= '2024-01-01'\"\n</code></pre>"},{"location":"tutorials/spark_engine/#5-performance-tuning","title":"5. Performance Tuning","text":""},{"location":"tutorials/spark_engine/#spark-specific-settings","title":"Spark-Specific Settings","text":"<pre><code>performance:\n  use_arrow: true              # PyArrow for Pandas UDFs\n  default_parallelism: 200     # Spark partitions\n  delta_table_properties:\n    delta.columnMapping.mode: name\n    delta.autoOptimize.optimizeWrite: true\n</code></pre>"},{"location":"tutorials/spark_engine/#skip-expensive-operations","title":"Skip Expensive Operations","text":"<p>For high-throughput Bronze ingestion:</p> <pre><code>performance:\n  skip_null_profiling: true    # Skip NULL count (saves 1 Spark job)\n  skip_catalog_writes: true    # Skip metadata tracking\n  skip_run_logging: true       # Skip run history\n</code></pre>"},{"location":"tutorials/spark_engine/#caching-hot-dataframes","title":"Caching Hot DataFrames","text":"<pre><code>nodes:\n  - name: dim_customer\n    cache: true  # Cache in memory for reuse\n    read:\n      connection: silver\n      table: dim_customer\n\n  - name: fact_orders\n    depends_on: [dim_customer]  # Uses cached dim_customer\n    # ...\n</code></pre>"},{"location":"tutorials/spark_engine/#6-scd2-with-spark","title":"6. SCD2 with Spark","text":"<p>Slowly Changing Dimensions work seamlessly with Spark:</p> <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: bronze\n      table: raw_customers\n    pattern:\n      type: dimension\n      params:\n        natural_key: customer_id\n        surrogate_key: customer_sk\n        scd_type: 2\n        track_cols: [name, email, address, city]\n        target: silver.dim_customer\n        unknown_member: true\n    write:\n      connection: silver\n      table: dim_customer\n</code></pre>"},{"location":"tutorials/spark_engine/#7-running-on-databricks","title":"7. Running on Databricks","text":""},{"location":"tutorials/spark_engine/#option-1-databricks-asset-bundles","title":"Option 1: Databricks Asset Bundles","text":"<pre><code># databricks.yml\nbundle:\n  name: odibi_pipelines\n\nresources:\n  jobs:\n    daily_pipeline:\n      name: \"[${bundle.environment}] Daily Pipeline\"\n      tasks:\n        - task_key: run_odibi\n          python_wheel_task:\n            package_name: odibi\n            entry_point: cli\n            parameters: [\"run\", \"--config\", \"project.yaml\"]\n</code></pre>"},{"location":"tutorials/spark_engine/#option-2-notebook","title":"Option 2: Notebook","text":"<pre><code># Databricks notebook cell\n%pip install odibi\n\nfrom odibi import run_project\n\nrun_project(\"project.yaml\", pipelines=[\"bronze_ingest\"])\n</code></pre>"},{"location":"tutorials/spark_engine/#8-common-issues","title":"8. Common Issues","text":""},{"location":"tutorials/spark_engine/#java-gateway-process-exited","title":"\"Java gateway process exited\"","text":"<p>Spark isn't installed or JAVA_HOME not set: <pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk\nexport SPARK_HOME=/opt/spark\n</code></pre></p>"},{"location":"tutorials/spark_engine/#out-of-memory","title":"Out of Memory","text":"<p>Increase driver/executor memory: <pre><code>spark = SparkSession.builder \\\n    .config(\"spark.driver.memory\", \"8g\") \\\n    .config(\"spark.executor.memory\", \"16g\") \\\n    .getOrCreate()\n</code></pre></p>"},{"location":"tutorials/spark_engine/#slow-small-files","title":"Slow Small Files","text":"<p>Use Delta's auto-optimize: <pre><code>write:\n  options:\n    delta.autoOptimize.optimizeWrite: true\n    delta.autoOptimize.autoCompact: true\n</code></pre></p>"},{"location":"tutorials/spark_engine/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/spark_engine/#py4jjavaerror-java-gateway-process-exited","title":"\"Py4JJavaError: Java gateway process exited\"","text":"<p>Cause: Java not installed or JAVA_HOME not set.</p> <p>Fix: <pre><code># Install Java 11 or 17\n# Ubuntu/Debian\nsudo apt install openjdk-11-jdk\n\n# Mac (Homebrew)\nbrew install openjdk@11\n\n# Set JAVA_HOME\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n</code></pre></p>"},{"location":"tutorials/spark_engine/#sparksession-not-found-or-no-module-named-pyspark","title":"\"SparkSession not found\" or \"No module named pyspark\"","text":"<p>Cause: Spark extras not installed.</p> <p>Fix: <pre><code>pip install \"odibi[spark]\"\n</code></pre></p>"},{"location":"tutorials/spark_engine/#spark-job-hangs-or-is-extremely-slow","title":"Spark job hangs or is extremely slow","text":"<p>Causes: - Too many small files (small file problem) - Insufficient memory for driver/executors - Shuffle spill to disk</p> <p>Fixes: <pre><code># Add performance tuning\nperformance:\n  spark:\n    conf:\n      spark.sql.shuffle.partitions: \"200\"\n      spark.sql.files.maxPartitionBytes: \"134217728\"  # 128MB\n</code></pre></p>"},{"location":"tutorials/spark_engine/#analysisexception-table-not-found","title":"\"AnalysisException: Table not found\"","text":"<p>Cause: Table not registered in Spark catalog.</p> <p>Fix: Use explicit path or register the table: <pre><code>read:\n  connection: delta_lake\n  path: silver/customers    # Use path, not table name\n  format: delta\n</code></pre></p>"},{"location":"tutorials/spark_engine/#delta-lake-merge-fails-with-concurrent-modification","title":"Delta Lake MERGE fails with \"concurrent modification\"","text":"<p>Cause: Multiple jobs writing to same table simultaneously.</p> <p>Fixes: - Enable optimistic concurrency: <code>delta.isolationLevel: WriteSerializable</code> - Use Databricks workflows with job clusters (single writer) - Add retry logic in pipeline config</p>"},{"location":"tutorials/spark_engine/#next-steps","title":"Next Steps","text":"<ul> <li>Azure Connections - Connect to Azure Blob/ADLS</li> <li>Performance Tuning - Optimize large pipelines</li> <li>Dimensional Modeling - Build star schemas</li> </ul>"},{"location":"tutorials/spark_engine/#see-also","title":"See Also","text":"<ul> <li>YAML Schema Reference - Full configuration options</li> <li>Delta Connection Config - Delta settings</li> <li>Glossary - Terminology reference</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/","title":"Introduction to Dimensional Modeling","text":"<p>Welcome to the Odibi dimensional modeling tutorial series. This comprehensive guide will teach you how to build a complete data warehouse using dimensional modeling techniques, from scratch.</p> <p>Prerequisites: Basic SQL knowledge and familiarity with data concepts like tables and columns.</p> <p>What You'll Build: A complete star schema for a retail sales system, plus a semantic layer for business intelligence.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-is-dimensional-modeling","title":"What is Dimensional Modeling?","text":"<p>Dimensional modeling is a technique for organizing data to make it easy to query and understand. It was developed by Ralph Kimball and is the foundation of most data warehouses today.</p> <p>Think of it like organizing a library: - Facts are like the checkout receipts\u2014they record what happened (a book was borrowed) - Dimensions are like the card catalogs\u2014they describe the who, what, where, and when</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#the-star-schema","title":"The Star Schema","text":"<p>The most common dimensional model is the star schema, named because it looks like a star when diagrammed:</p> <pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        int order_sk PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        int quantity\n        decimal unit_price\n        decimal line_total\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string email\n        string region\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n        decimal price\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        string month_name\n        int year\n    }\n</code></pre> <p>The fact table sits in the center and contains measurements (quantities, amounts, counts). The dimension tables surround it and provide context (who bought it, what was purchased, when did it happen).</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#facts-vs-dimensions-the-grocery-receipt-analogy","title":"Facts vs Dimensions: The Grocery Receipt Analogy","text":"<p>Imagine you're at a grocery store and you get a receipt:</p> <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n   FRESH FOODS MARKET\n   Store #42 - Downtown\n   Date: Jan 15, 2024  Time: 2:34 PM\n   Cashier: Maria\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n   Organic Milk 1 gal       $4.99\n   Wheat Bread              $3.49\n   Bananas 2.5 lb           $1.87\n   Cheddar Cheese           $5.99\n\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   SUBTOTAL                $16.34\n   TAX                      $0.82\n   TOTAL                   $17.16\n\n   Paid: VISA ****1234\n\n   Thank you for shopping with us!\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre> <p>Facts (the measurements): - Quantity of each item - Price of each item - Total amount</p> <p>Dimensions (the context): - Who: The customer (you) - What: The products (milk, bread, bananas, cheese) - Where: The store (#42, Downtown) - When: The date and time - How: The payment method (VISA)</p> <p>In a data warehouse, we'd model this as:</p> Concept Fact or Dimension? Example Columns The line items Fact quantity, unit_price, line_total The customer Dimension name, email, loyalty_tier The product Dimension product_name, category, brand The store Dimension store_name, city, region The date Dimension day_of_week, month, year"},{"location":"tutorials/dimensional_modeling/01_introduction/#why-surrogate-keys","title":"Why Surrogate Keys?","text":"<p>You might wonder: \"If customers already have a customer_id, why do we need another key?\"</p> <p>Good question! Here's why we use surrogate keys (like <code>customer_sk</code>) instead of natural keys (like <code>customer_id</code>):</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-1-business-keys-change","title":"Problem 1: Business Keys Change","text":"<p>Imagine your source system uses email as the customer identifier. What happens when a customer changes their email?</p> <p>Using natural key (email): <pre><code>-- Old orders are orphaned!\nSELECT * FROM orders WHERE customer_email = 'alice@oldmail.com';  -- No longer exists\nSELECT * FROM customers WHERE email = 'alice@oldmail.com';        -- Record was updated to new email\n</code></pre></p> <p>Using surrogate key: <pre><code>-- Customer SK never changes, even if email does\nSELECT * FROM fact_orders WHERE customer_sk = 42;     -- Still works!\nSELECT * FROM dim_customer WHERE customer_sk = 42;    -- Returns current info\n</code></pre></p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-2-tracking-history-scd-type-2","title":"Problem 2: Tracking History (SCD Type 2)","text":"<p>When you need to track historical changes, surrogate keys become essential:</p> <p>Customer Data Over Time:</p> customer_sk customer_id email valid_from valid_to is_current 42 C001 alice@oldmail.com 2023-01-01 2024-01-15 false 157 C001 alice@newmail.com 2024-01-15 NULL true <p>The same customer (C001) has two dimension rows with different surrogate keys. Orders placed before Jan 15 link to SK=42 (capturing the email at that time). Orders after link to SK=157.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-3-performance","title":"Problem 3: Performance","text":"<ul> <li>Surrogate keys are simple integers (4 bytes)</li> <li>Natural keys can be long strings (variable length)</li> <li>Integer joins are much faster than string joins</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/#problem-4-multi-source-integration","title":"Problem 4: Multi-Source Integration","text":"<p>When combining data from multiple systems:</p> Source System Customer ID CRM CUST-00042 E-commerce user_42 Support 42 <p>With surrogate keys, all three become customer_sk = 42, regardless of source format.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#the-unknown-member-problem","title":"The Unknown Member Problem","text":"<p>What happens when a fact record references a dimension that doesn't exist?</p> <p>Scenario: An order arrives with <code>customer_id = 'C999'</code>, but there's no customer with that ID in the dimension table.</p> <p>Without unknown member: <pre><code>-- This join loses the order entirely!\nSELECT o.*, c.name\nFROM fact_orders o\nJOIN dim_customer c ON o.customer_sk = c.customer_sk\nWHERE o.order_id = 'ORD999';\n-- Returns: (no rows)\n</code></pre></p> <p>With unknown member (customer_sk = 0):</p> customer_sk customer_id name email 0 -1 Unknown Unknown 1 C001 Alice Johnson alice@example.com 2 C002 Bob Smith bob@example.com <p>Now orphan orders get assigned to customer_sk = 0:</p> <pre><code>SELECT o.*, c.name\nFROM fact_orders o\nJOIN dim_customer c ON o.customer_sk = c.customer_sk\nWHERE o.order_id = 'ORD999';\n-- Returns: order data with name = 'Unknown'\n</code></pre> <p>The order isn't lost\u2014it's explicitly marked as having an unknown customer, which you can investigate later.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-types-handling-changes","title":"SCD Types: Handling Changes","text":"<p>When dimension data changes, how should you handle it? There are three common strategies:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-0-never-update","title":"SCD Type 0: Never Update","text":"<p>The dimension never changes after initial load. Use for truly static data.</p> <p>Example: ISO country codes, fixed reference data</p> country_sk country_code country_name 1 USA United States 2 CAN Canada"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-1-overwrite","title":"SCD Type 1: Overwrite","text":"<p>Update the dimension in place. History is lost, but you always see current data.</p> <p>Example: Customer email (you only care about current contact info)</p> <p>Before:</p> customer_sk customer_id email 1 C001 alice@oldmail.com <p>After (email changed):</p> customer_sk customer_id email 1 C001 alice@newmail.com"},{"location":"tutorials/dimensional_modeling/01_introduction/#scd-type-2-track-history","title":"SCD Type 2: Track History","text":"<p>Create a new row for each version. Full audit trail preserved.</p> <p>Example: Customer address (for accurate point-in-time reporting)</p> <p>Before:</p> customer_sk customer_id city valid_from valid_to is_current 1 C001 Chicago 2020-01-01 NULL true <p>After (customer moved):</p> customer_sk customer_id city valid_from valid_to is_current 1 C001 Chicago 2020-01-01 2024-01-15 false 42 C001 Seattle 2024-01-15 NULL true"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-youll-build-in-this-tutorial-series","title":"What You'll Build in This Tutorial Series","text":"<p>By the end of this series, you'll have built:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-1-dimensional-modeling-foundations","title":"Part 1: Dimensional Modeling Foundations","text":"<ol> <li>Introduction (this tutorial) - Core concepts</li> <li>Dimension Pattern - Build customer dimension with SCD 0/1/2</li> <li>Date Dimension Pattern - Generate complete date dimension</li> <li>Fact Pattern - Build fact table with SK lookups</li> <li>Aggregation Pattern - Build pre-aggregated tables</li> <li>Full Star Schema - Complete working example</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-2-semantic-layer","title":"Part 2: Semantic Layer","text":"<ol> <li>Semantic Layer Intro - What and why</li> <li>Defining Metrics - Revenue, counts, averages</li> <li>Defining Dimensions - Regions, dates, hierarchies</li> <li>Querying Metrics - \"revenue BY region\" syntax</li> <li>Materializing Metrics - Pre-compute for dashboards</li> <li>Semantic Full Example - Complete semantic layer</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#part-3-data-quality","title":"Part 3: Data Quality","text":"<ol> <li>FK Validation - Ensure referential integrity</li> </ol>"},{"location":"tutorials/dimensional_modeling/01_introduction/#sample-data","title":"Sample Data","text":"<p>Throughout these tutorials, we'll use consistent sample data representing a retail business:</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#customers-12-rows","title":"Customers (12 rows)","text":"customer_id name email region city C001 Alice Johnson alice@example.com North Chicago C002 Bob Smith bob@example.com South Houston C003 Carol White carol@example.com North Detroit C004 David Brown david@example.com East New York C005 Emma Davis emma@example.com West Seattle C006 Frank Miller frank@example.com South Miami C007 Grace Lee grace@example.com East Boston C008 Henry Wilson henry@example.com West Portland C009 Ivy Chen ivy@example.com North Minneapolis C010 Jack Taylor jack@example.com South Dallas C011 Karen Martinez karen@example.com East Philadelphia C012 Leo Anderson leo@example.com West Denver"},{"location":"tutorials/dimensional_modeling/01_introduction/#products-10-rows","title":"Products (10 rows)","text":"product_id name category price P001 Laptop Pro 15 Electronics $1,299.99 P002 Wireless Mouse Electronics $29.99 P003 Office Chair Furniture $249.99 P004 USB-C Hub Electronics $49.99 P005 Standing Desk Furniture $599.99 P006 Mechanical Keyboard Electronics $149.99 P007 Monitor 27\" Electronics $399.99 P008 Desk Lamp Furniture $45.99 P009 Webcam HD Electronics $79.99 P010 Filing Cabinet Furniture $189.99"},{"location":"tutorials/dimensional_modeling/01_introduction/#orders-30-rows-across-14-days","title":"Orders (30 rows across 14 days)","text":"<p>Sample orders spanning January 15-28, 2024, with various quantities and statuses.</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#what-you-learned","title":"What You Learned","text":"<p>In this introduction, you learned:</p> <ul> <li>Dimensional modeling organizes data into facts and dimensions</li> <li>Star schemas put the fact table in the center, surrounded by dimensions</li> <li>Facts contain measurements (quantities, amounts)</li> <li>Dimensions provide context (who, what, where, when)</li> <li>Surrogate keys solve problems with changing business keys and enable history tracking</li> <li>Unknown members prevent orphan records from being lost</li> <li>SCD Types define how to handle dimension changes (0=static, 1=overwrite, 2=history)</li> </ul>"},{"location":"tutorials/dimensional_modeling/01_introduction/#next-steps","title":"Next Steps","text":"<p>Ready to build your first dimension table?</p> <p>Next: Dimension Pattern Tutorial - Build a customer dimension with SCD support</p>"},{"location":"tutorials/dimensional_modeling/01_introduction/#navigation","title":"Navigation","text":"Previous Up Next - Tutorials Dimension Pattern"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/","title":"Dimension Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>dimension</code> pattern to build dimension tables with automatic surrogate key generation and SCD (Slowly Changing Dimension) support.</p> <p>What You'll Learn: - How surrogate keys are generated - SCD Type 0 (static) - never update - SCD Type 1 (overwrite) - update in place - SCD Type 2 (history) - track all changes - Unknown member handling</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#source-data","title":"Source Data","text":"<p>We'll start with this customer data (12 rows):</p> <p>Source Data (customers.csv) - 12 rows:</p> customer_id name email region city state C001 Alice Johnson alice@example.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david@example.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace@example.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-1-scd-type-0-static-dimensions","title":"Step 1: SCD Type 0 - Static Dimensions","text":"<p>When to use: Reference data that never changes (country codes, fixed lookups).</p> <p>SCD Type 0 creates surrogate keys but never updates existing records. New records are inserted, but changes to existing records are ignored.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd0\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 0\n            unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#output-dim_customer-13-rows","title":"Output: dim_customer (13 rows)","text":"<p>After running with <code>scd_type: 0</code>, here's the dimension table with generated surrogate keys:</p> customer_sk customer_id name email region city state load_timestamp 0 -1 Unknown Unknown Unknown Unknown Unknown 1900-01-01 00:00:00 1 C001 Alice Johnson alice@example.com North Chicago IL 2024-01-15 10:00:00 2 C002 Bob Smith bob@example.com South Houston TX 2024-01-15 10:00:00 3 C003 Carol White carol@example.com North Detroit MI 2024-01-15 10:00:00 4 C004 David Brown david@example.com East New York NY 2024-01-15 10:00:00 5 C005 Emma Davis emma@example.com West Seattle WA 2024-01-15 10:00:00 6 C006 Frank Miller frank@example.com South Miami FL 2024-01-15 10:00:00 7 C007 Grace Lee grace@example.com East Boston MA 2024-01-15 10:00:00 8 C008 Henry Wilson henry@example.com West Portland OR 2024-01-15 10:00:00 9 C009 Ivy Chen ivy@example.com North Minneapolis MN 2024-01-15 10:00:00 10 C010 Jack Taylor jack@example.com South Dallas TX 2024-01-15 10:00:00 11 C011 Karen Martinez karen@example.com East Philadelphia PA 2024-01-15 10:00:00 12 C012 Leo Anderson leo@example.com West Denver CO 2024-01-15 10:00:00 <p>Key observations: - Row 0 is the unknown member (customer_sk = 0, customer_id = -1) - Surrogate keys are sequential integers starting at 1 - Each source row gets a unique SK</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-2-scd-type-1-overwrite-updates","title":"Step 2: SCD Type 1 - Overwrite Updates","text":"<p>When to use: Attributes where you only need the current value (email, phone, preferences).</p> <p>SCD Type 1 updates existing records in place when changes are detected. No history is preserved.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#the-update-scenario","title":"The Update Scenario","text":"<p>Three customers changed their email addresses:</p> <p>Updated Source Data (customers_updated.csv) - 3 changes highlighted:</p> customer_id name email region city state C001 Alice Johnson alice.johnson@newmail.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david.b@corporate.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace.lee@gmail.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration_1","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd1\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 1\n            track_cols:\n              - name\n              - email\n              - region\n              - city\n              - state\n            target: warehouse.dim_customer\n            unknown_member: true\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#before-vs-after-comparison","title":"Before vs After Comparison","text":"<p>BEFORE (original load):</p> customer_sk customer_id email load_timestamp 0 -1 Unknown 1900-01-01 00:00:00 1 C001 alice@example.com 2024-01-15 10:00:00 4 C004 david@example.com 2024-01-15 10:00:00 7 C007 grace@example.com 2024-01-15 10:00:00 ... ... ... ... <p>AFTER (SCD1 update):</p> customer_sk customer_id email load_timestamp 0 -1 Unknown 1900-01-01 00:00:00 1 C001 alice.johnson@newmail.com 2024-01-20 14:30:00 4 C004 david.b@corporate.com 2024-01-20 14:30:00 7 C007 grace.lee@gmail.com 2024-01-20 14:30:00 ... ... ... ... <p>Key observations: - Same surrogate keys - C001 is still customer_sk = 1 - Values updated in place - old emails are gone - Timestamp updated - shows when the record was last modified - No history preserved - we can't see the old email addresses</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#complete-scd1-output-13-rows","title":"Complete SCD1 Output (13 rows)","text":"customer_sk customer_id name email region city state load_timestamp 0 -1 Unknown Unknown Unknown Unknown Unknown 1900-01-01 00:00:00 1 C001 Alice Johnson alice.johnson@newmail.com North Chicago IL 2024-01-20 14:30:00 2 C002 Bob Smith bob@example.com South Houston TX 2024-01-15 10:00:00 3 C003 Carol White carol@example.com North Detroit MI 2024-01-15 10:00:00 4 C004 David Brown david.b@corporate.com East New York NY 2024-01-20 14:30:00 5 C005 Emma Davis emma@example.com West Seattle WA 2024-01-15 10:00:00 6 C006 Frank Miller frank@example.com South Miami FL 2024-01-15 10:00:00 7 C007 Grace Lee grace.lee@gmail.com East Boston MA 2024-01-20 14:30:00 8 C008 Henry Wilson henry@example.com West Portland OR 2024-01-15 10:00:00 9 C009 Ivy Chen ivy@example.com North Minneapolis MN 2024-01-15 10:00:00 10 C010 Jack Taylor jack@example.com South Dallas TX 2024-01-15 10:00:00 11 C011 Karen Martinez karen@example.com East Philadelphia PA 2024-01-15 10:00:00 12 C012 Leo Anderson leo@example.com West Denver CO 2024-01-15 10:00:00"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#step-3-scd-type-2-full-history-tracking","title":"Step 3: SCD Type 2 - Full History Tracking","text":"<p>When to use: Attributes where historical accuracy matters (address for shipping analysis, tier for billing history).</p> <p>SCD Type 2 preserves full history by creating a new row for each change. Old versions are closed with a <code>valid_to</code> date.</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#the-history-scenario","title":"The History Scenario","text":"<p>Same three customers changed their emails. With SCD2, we keep both versions:</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#yaml-configuration_2","title":"YAML Configuration","text":"<pre><code>project: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_dim_customer_scd2\n    nodes:\n      - name: dim_customer\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols:\n              - name\n              - email\n              - region\n              - city\n              - state\n            target: warehouse.dim_customer\n            valid_from_col: valid_from\n            valid_to_col: valid_to\n            is_current_col: is_current\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#output-full-history-16-rows","title":"Output: Full History (16 rows)","text":"customer_sk customer_id name email region valid_from valid_to is_current 0 -1 Unknown Unknown Unknown 1900-01-01 NULL true 1 C001 Alice Johnson alice@example.com North 2024-01-15 2024-01-20 false 2 C002 Bob Smith bob@example.com South 2024-01-15 NULL true 3 C003 Carol White carol@example.com North 2024-01-15 NULL true 4 C004 David Brown david@example.com East 2024-01-15 2024-01-20 false 5 C005 Emma Davis emma@example.com West 2024-01-15 NULL true 6 C006 Frank Miller frank@example.com South 2024-01-15 NULL true 7 C007 Grace Lee grace@example.com East 2024-01-15 2024-01-20 false 8 C008 Henry Wilson henry@example.com West 2024-01-15 NULL true 9 C009 Ivy Chen ivy@example.com North 2024-01-15 NULL true 10 C010 Jack Taylor jack@example.com South 2024-01-15 NULL true 11 C011 Karen Martinez karen@example.com East 2024-01-15 NULL true 12 C012 Leo Anderson leo@example.com West 2024-01-15 NULL true 13 C001 Alice Johnson alice.johnson@newmail.com North 2024-01-20 NULL true 14 C004 David Brown david.b@corporate.com East 2024-01-20 NULL true 15 C007 Grace Lee grace.lee@gmail.com East 2024-01-20 NULL true <p>Key observations: - New surrogate keys for new versions (13, 14, 15) - Old versions marked closed (is_current = false, valid_to = 2024-01-20) - New versions marked current (is_current = true, valid_to = NULL) - Full history preserved - we can query data as of any point in time</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#how-to-query-scd2","title":"How to Query SCD2","text":"<p>Current view (most common): <pre><code>SELECT * FROM dim_customer WHERE is_current = true;\n</code></pre></p> <p>Point-in-time query (as of January 17): <pre><code>SELECT * FROM dim_customer\nWHERE '2024-01-17' &gt;= valid_from\n  AND ('2024-01-17' &lt; valid_to OR valid_to IS NULL);\n</code></pre></p> <p>Customer C001's email history: <pre><code>SELECT customer_sk, email, valid_from, valid_to\nFROM dim_customer\nWHERE customer_id = 'C001'\nORDER BY valid_from;\n</code></pre></p> customer_sk email valid_from valid_to 1 alice@example.com 2024-01-15 2024-01-20 13 alice.johnson@newmail.com 2024-01-20 NULL"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#understanding-the-unknown-member","title":"Understanding the Unknown Member","text":"<p>The unknown member row (customer_sk = 0) is automatically created when <code>unknown_member: true</code>:</p> customer_sk customer_id name email all other columns 0 -1 Unknown Unknown Unknown <p>Why it matters:</p> <p>When building fact tables, orders might reference a customer_id that doesn't exist in the dimension (data quality issue, late-arriving data, etc.). Instead of: - Failing the pipeline (strict but inflexible) - Losing the order data (dangerous)</p> <p>We assign those orphan records to customer_sk = 0. This: - Preserves all fact data - Makes orphans easily identifiable - Allows later cleanup/investigation</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<p>Here's a complete YAML file you can run:</p> <pre><code># File: odibi_dimension_tutorial.yaml\nproject: dimension_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  # Initial load with SCD2\n  - pipeline: initial_load\n    description: \"First load of customer dimension\"\n    nodes:\n      - name: dim_customer\n        description: \"Customer dimension with SCD2 history tracking\"\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols:\n              - name\n              - email\n              - region\n              - city\n              - state\n            target: warehouse.dim_customer\n            valid_from_col: valid_from\n            valid_to_col: valid_to\n            is_current_col: is_current\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n\n  # Incremental update with changes\n  - pipeline: incremental_update\n    description: \"Process updates to customer dimension\"\n    nodes:\n      - name: dim_customer\n        description: \"Update customer dimension with new email addresses\"\n        read:\n          connection: source\n          path: customers_updated.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols:\n              - name\n              - email\n              - region\n              - city\n              - state\n            target: warehouse.dim_customer\n            valid_from_col: valid_from\n            valid_to_col: valid_to\n            is_current_col: is_current\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#python-api-alternative","title":"Python API Alternative","text":"<p>If you prefer Python over YAML:</p> <pre><code>from odibi.patterns.dimension import DimensionPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load source data\nsource_df = pd.read_csv(\"examples/tutorials/dimensional_modeling/data/customers.csv\")\n\n# Create pattern\npattern = DimensionPattern(params={\n    \"natural_key\": \"customer_id\",\n    \"surrogate_key\": \"customer_sk\",\n    \"scd_type\": 2,\n    \"track_cols\": [\"name\", \"email\", \"region\", \"city\", \"state\"],\n    \"unknown_member\": True,\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"crm\"\n    }\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern\ncontext = EngineContext(df=source_df, engine_type=EngineType.PANDAS)\nresult_df = pattern.execute(context)\n\n# View results\nprint(f\"Generated {len(result_df)} dimension rows\")\nprint(result_df.head(15))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>SCD Type 0 creates surrogate keys but never updates existing records</li> <li>SCD Type 1 updates records in place, losing history but keeping current data</li> <li>SCD Type 2 creates new rows for changes, preserving full history with valid_from/valid_to dates</li> <li>Surrogate keys are auto-generated integers, sequential starting from 1</li> <li>Unknown member (SK=0) provides a default for orphan FK handling</li> <li>track_cols defines which columns trigger a new version in SCD1/SCD2</li> <li>Audit columns (load_timestamp, source_system) track when/where data came from</li> </ul>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you can build customer dimensions, let's create a date dimension that's automatically generated.</p> <p>Next: Date Dimension Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#navigation","title":"Navigation","text":"Previous Up Next Introduction Tutorials Date Dimension"},{"location":"tutorials/dimensional_modeling/02_dimension_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Dimension Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/","title":"Date Dimension Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>date_dimension</code> pattern to generate a complete date dimension table with pre-calculated attributes for reporting and analytics.</p> <p>What You'll Learn: - Why you need a date dimension - How the pattern generates dates automatically - Understanding all 19 generated columns - Fiscal calendar configuration - Unknown date handling</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#why-do-you-need-a-date-dimension","title":"Why Do You Need a Date Dimension?","text":"<p>Consider this question: \"What were our sales on Tuesdays in January?\"</p> <p>Your raw order data looks like this:</p> order_id order_date amount ORD001 2024-01-15 1,299.99 ORD002 2024-01-16 249.99 ORD003 2024-01-23 599.99 <p>Problem: The date <code>2024-01-15</code> doesn't tell you it's a Tuesday. You'd need to calculate that in every query.</p> <p>Without a date dimension: <pre><code>-- Complex, repeated logic in every query\nSELECT\n    DATENAME(weekday, order_date) AS day_of_week,\n    SUM(amount) AS total\nFROM orders\nWHERE MONTH(order_date) = 1\n  AND DATENAME(weekday, order_date) = 'Tuesday'\nGROUP BY DATENAME(weekday, order_date);\n</code></pre></p> <p>With a date dimension: <pre><code>-- Simple join, pre-calculated attributes\nSELECT\n    d.day_of_week,\n    SUM(o.amount) AS total\nFROM fact_orders o\nJOIN dim_date d ON o.date_sk = d.date_sk\nWHERE d.month = 1\n  AND d.day_of_week = 'Tuesday'\nGROUP BY d.day_of_week;\n</code></pre></p> <p>A date dimension also provides: - Fiscal calendar attributes (fiscal year, fiscal quarter) - Holiday flags (is_holiday, holiday_name) - Business day calculations (is_weekend, is_month_start) - Consistent naming (\"January\" not \"1\", \"Q1\" not \"1\")</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#the-date-dimension-pattern","title":"The Date Dimension Pattern","text":"<p>Unlike other patterns that transform source data, the <code>date_dimension</code> pattern generates data. You don't need a <code>read:</code> block\u2014just configure the date range and options.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#basic-yaml-configuration","title":"Basic YAML Configuration","text":"<pre><code>project: date_dimension_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    nodes:\n      - name: dim_date\n        # No read block needed - pattern generates data\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2024-01-15\"\n            end_date: \"2024-01-28\"\n            fiscal_year_start_month: 7\n            unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-1-generate-a-small-date-range","title":"Step 1: Generate a Small Date Range","text":"<p>Let's generate 14 days (January 15-28, 2024) to see exactly what columns are created.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#output-dim_date-15-rows-including-unknown","title":"Output: dim_date (15 rows including unknown)","text":"<p>Here are the first 10 rows showing all 19 columns:</p> date_sk full_date day_of_week day_of_week_num day_of_month day_of_year is_weekend week_of_year month month_name quarter quarter_name year fiscal_year fiscal_quarter is_month_start is_month_end is_year_start is_year_end 0 1900-01-01 Unknown 0 0 0 false 0 0 Unknown 0 Unknown 0 0 0 false false false false 20240115 2024-01-15 Monday 1 15 15 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240116 2024-01-16 Tuesday 2 16 16 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240117 2024-01-17 Wednesday 3 17 17 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240118 2024-01-18 Thursday 4 18 18 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240119 2024-01-19 Friday 5 19 19 false 3 1 January 1 Q1 2024 2024 3 false false false false 20240120 2024-01-20 Saturday 6 20 20 true 3 1 January 1 Q1 2024 2024 3 false false false false 20240121 2024-01-21 Sunday 7 21 21 true 3 1 January 1 Q1 2024 2024 3 false false false false 20240122 2024-01-22 Monday 1 22 22 false 4 1 January 1 Q1 2024 2024 3 false false false false 20240123 2024-01-23 Tuesday 2 23 23 false 4 1 January 1 Q1 2024 2024 3 false false false false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#remaining-rows-24-28","title":"Remaining rows (24-28):","text":"date_sk full_date day_of_week day_of_week_num is_weekend week_of_year fiscal_year fiscal_quarter 20240124 2024-01-24 Wednesday 3 false 4 2024 3 20240125 2024-01-25 Thursday 4 false 4 2024 3 20240126 2024-01-26 Friday 5 false 4 2024 3 20240127 2024-01-27 Saturday 6 true 4 2024 3 20240128 2024-01-28 Sunday 7 true 4 2024 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-2-understanding-the-19-columns","title":"Step 2: Understanding the 19 Columns","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#surrogate-key","title":"Surrogate Key","text":"Column Type Description Example <code>date_sk</code> int Surrogate key in YYYYMMDD format 20240115 <p>The <code>date_sk</code> uses YYYYMMDD format, which: - Is human-readable (you can see the date in the key) - Sorts chronologically - Is efficient for range queries</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#date-columns","title":"Date Columns","text":"Column Type Description Example <code>full_date</code> date The actual date 2024-01-15"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#day-attributes","title":"Day Attributes","text":"Column Type Description Example <code>day_of_week</code> string Day name Monday <code>day_of_week_num</code> int Day number (1=Monday, 7=Sunday) 1 <code>day_of_month</code> int Day of month (1-31) 15 <code>day_of_year</code> int Day of year (1-366) 15 <code>is_weekend</code> bool Weekend flag false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#week-attributes","title":"Week Attributes","text":"Column Type Description Example <code>week_of_year</code> int ISO week number (1-53) 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#month-attributes","title":"Month Attributes","text":"Column Type Description Example <code>month</code> int Month number (1-12) 1 <code>month_name</code> string Month name January <code>is_month_start</code> bool First day of month false <code>is_month_end</code> bool Last day of month false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#quarter-attributes","title":"Quarter Attributes","text":"Column Type Description Example <code>quarter</code> int Calendar quarter (1-4) 1 <code>quarter_name</code> string Quarter name Q1"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#year-attributes","title":"Year Attributes","text":"Column Type Description Example <code>year</code> int Calendar year 2024 <code>is_year_start</code> bool First day of year false <code>is_year_end</code> bool Last day of year false"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-calendar","title":"Fiscal Calendar","text":"Column Type Description Example <code>fiscal_year</code> int Fiscal year 2024 <code>fiscal_quarter</code> int Fiscal quarter (1-4) 3"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-3-understanding-fiscal-calendars","title":"Step 3: Understanding Fiscal Calendars","text":"<p>Many organizations don't use January-December as their fiscal year. Common alternatives:</p> Industry Fiscal Year Start Example US Government October 1 FY2024 = Oct 2023 - Sep 2024 Retail February 1 FY2024 = Feb 2024 - Jan 2025 Education July 1 FY2024 = Jul 2023 - Jun 2024"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#how-fiscal-year-calculation-works","title":"How Fiscal Year Calculation Works","text":"<p>With <code>fiscal_year_start_month: 7</code> (July):</p> Calendar Date Calendar Year Fiscal Year Why June 15, 2024 2024 2024 Before fiscal year start July 1, 2024 2024 2025 Fiscal year starts December 31, 2024 2024 2025 Still FY2025 <p>Formula: If current month &gt;= fiscal_year_start_month, add 1 to calendar year.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-quarters","title":"Fiscal Quarters","text":"<p>With <code>fiscal_year_start_month: 7</code>:</p> Fiscal Quarter Months Q1 July, August, September Q2 October, November, December Q3 January, February, March Q4 April, May, June"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#example-output-with-july-fiscal-year","title":"Example Output with July Fiscal Year","text":"<p>January 2024 dates would show:</p> full_date year fiscal_year quarter fiscal_quarter 2024-01-15 2024 2024 1 3 2024-01-16 2024 2024 1 3 <p>Note: January is calendar Q1 but fiscal Q3 (since the fiscal year started in July).</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#step-4-the-unknown-date-row","title":"Step 4: The Unknown Date Row","text":"<p>When <code>unknown_member: true</code>, a special row is added with <code>date_sk = 0</code>:</p> Column Value date_sk 0 full_date 1900-01-01 day_of_week Unknown day_of_week_num 0 day_of_month 0 day_of_year 0 is_weekend false week_of_year 0 month 0 month_name Unknown quarter 0 quarter_name Unknown year 0 fiscal_year 0 fiscal_quarter 0 is_month_start false is_month_end false is_year_start false is_year_end false <p>Use case: When fact records have NULL or invalid dates, they can be assigned to date_sk = 0 rather than being dropped.</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<p>Here's a complete YAML file for a production-ready date dimension spanning 10 years:</p> <pre><code># File: odibi_date_dimension_tutorial.yaml\nproject: date_dimension_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_date_dimension\n    description: \"Generate date dimension for 10-year range\"\n    nodes:\n      - name: dim_date\n        description: \"Standard date dimension with fiscal calendar\"\n        pattern:\n          type: date_dimension\n          params:\n            # 10-year range for typical warehouse\n            start_date: \"2020-01-01\"\n            end_date: \"2030-12-31\"\n\n            # July fiscal year (education/government style)\n            fiscal_year_start_month: 7\n\n            # Add unknown member for orphan handling\n            unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n</code></pre> <p>This generates 4,018 rows (4,017 days + 1 unknown member).</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#common-fiscal-year-configurations","title":"Common Fiscal Year Configurations","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#standard-calendar-year-default","title":"Standard Calendar Year (Default)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 1  # Default\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#us-governmentretail-october","title":"US Government/Retail (October)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 10\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#education-july","title":"Education (July)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 7\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#uk-tax-year-april","title":"UK Tax Year (April)","text":"<pre><code>params:\n  start_date: \"2020-01-01\"\n  end_date: \"2030-12-31\"\n  fiscal_year_start_month: 4\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#python-api-alternative","title":"Python API Alternative","text":"<pre><code>from odibi.patterns.date_dimension import DateDimensionPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\n\n# Create pattern\npattern = DateDimensionPattern(params={\n    \"start_date\": \"2024-01-15\",\n    \"end_date\": \"2024-01-28\",\n    \"fiscal_year_start_month\": 7,\n    \"unknown_member\": True\n})\n\n# Validate configuration\npattern.validate()\n\n# Execute pattern (no input df needed - generates data)\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\nresult_df = pattern.execute(context)\n\n# View results\nprint(f\"Generated {len(result_df)} date rows\")\nprint(\"\\nColumns:\", result_df.columns.tolist())\nprint(\"\\nSample data:\")\nprint(result_df.head(10).to_string())\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#querying-the-date-dimension","title":"Querying the Date Dimension","text":""},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#what-day-of-the-week-had-the-most-sales","title":"\"What day of the week had the most sales?\"","text":"<pre><code>SELECT\n    d.day_of_week,\n    SUM(f.line_total) AS total_sales,\n    COUNT(*) AS order_count\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.day_of_week\nORDER BY total_sales DESC;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#show-monthly-sales-trend","title":"\"Show monthly sales trend\"","text":"<pre><code>SELECT\n    d.year,\n    d.month_name,\n    SUM(f.line_total) AS monthly_sales\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.year, d.month, d.month_name\nORDER BY d.year, d.month;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#compare-weekday-vs-weekend-sales","title":"\"Compare weekday vs weekend sales\"","text":"<pre><code>SELECT\n    CASE WHEN d.is_weekend THEN 'Weekend' ELSE 'Weekday' END AS period,\n    SUM(f.line_total) AS total_sales,\n    AVG(f.line_total) AS avg_order\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.is_weekend;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#fiscal-year-performance","title":"\"Fiscal year performance\"","text":"<pre><code>SELECT\n    d.fiscal_year,\n    d.fiscal_quarter,\n    SUM(f.line_total) AS quarterly_sales\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.fiscal_year, d.fiscal_quarter\nORDER BY d.fiscal_year, d.fiscal_quarter;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Date dimensions pre-calculate date attributes for easier querying</li> <li>The pattern generates data\u2014no source file needed</li> <li>19 columns are created automatically covering day, week, month, quarter, year, and fiscal calendar</li> <li>Surrogate keys use YYYYMMDD format (e.g., 20240115)</li> <li>Fiscal calendars can start on any month\u2014the pattern calculates fiscal year and quarter automatically</li> <li>Unknown member (date_sk = 0) handles NULL or invalid dates in fact tables</li> </ul>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you have customer and date dimensions, let's build a fact table that links them together.</p> <p>Next: Fact Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#navigation","title":"Navigation","text":"Previous Up Next Dimension Pattern Tutorials Fact Pattern"},{"location":"tutorials/dimensional_modeling/03_date_dimension_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Date Dimension Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/","title":"Fact Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>fact</code> pattern to build fact tables with automatic surrogate key lookups, orphan handling, grain validation, and measure calculations.</p> <p>What You'll Learn: - How surrogate key lookups work - Orphan handling strategies - Grain validation and deduplication - Measure calculations - Joining facts with dimensions</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#source-data","title":"Source Data","text":"<p>We'll use orders data that references customers and products by their natural keys:</p> <p>Source Data (orders.csv) - 30 rows:</p> order_id customer_id product_id order_date quantity unit_price status ORD001 C001 P001 2024-01-15 1 1299.99 completed ORD002 C001 P002 2024-01-15 2 29.99 completed ORD003 C002 P003 2024-01-16 1 249.99 completed ORD004 C003 P004 2024-01-16 3 49.99 completed ORD005 C004 P005 2024-01-17 1 599.99 completed ORD006 C005 P006 2024-01-17 1 149.99 completed ORD007 C006 P007 2024-01-18 2 399.99 completed ORD008 C007 P008 2024-01-18 4 45.99 completed ORD009 C008 P009 2024-01-19 1 79.99 completed ORD010 C009 P010 2024-01-19 1 189.99 completed ORD011 C010 P001 2024-01-20 1 1299.99 completed ORD012 C011 P002 2024-01-20 5 29.99 completed ORD013 C012 P003 2024-01-21 2 249.99 completed ORD014 C001 P004 2024-01-21 1 49.99 completed ORD015 C002 P005 2024-01-22 1 599.99 pending ORD016 C003 P006 2024-01-22 2 149.99 completed ORD017 C004 P007 2024-01-23 1 399.99 completed ORD018 C005 P008 2024-01-23 3 45.99 completed ORD019 C006 P009 2024-01-24 2 79.99 completed ORD020 C007 P010 2024-01-24 1 189.99 completed ORD021 C008 P001 2024-01-25 1 1299.99 completed ORD022 C009 P002 2024-01-25 3 29.99 completed ORD023 C010 P003 2024-01-26 1 249.99 cancelled ORD024 C011 P004 2024-01-26 2 49.99 completed ORD025 C012 P005 2024-01-27 1 599.99 completed ORD026 C001 P006 2024-01-27 1 149.99 completed ORD027 C002 P007 2024-01-28 1 399.99 completed ORD028 C003 P008 2024-01-28 2 45.99 completed ORD029 C004 P009 2024-01-15 1 79.99 completed ORD030 C005 P010 2024-01-16 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dimension-tables-pre-built","title":"Dimension Tables (Pre-built)","text":"<p>Before building fact tables, we need dimension tables. Here are the dimensions from previous tutorials:</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_customer-13-rows-including-unknown","title":"dim_customer (13 rows including unknown)","text":"customer_sk customer_id name region is_current 0 -1 Unknown Unknown true 1 C001 Alice Johnson North true 2 C002 Bob Smith South true 3 C003 Carol White North true 4 C004 David Brown East true 5 C005 Emma Davis West true 6 C006 Frank Miller South true 7 C007 Grace Lee East true 8 C008 Henry Wilson West true 9 C009 Ivy Chen North true 10 C010 Jack Taylor South true 11 C011 Karen Martinez East true 12 C012 Leo Anderson West true"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_product-11-rows-including-unknown","title":"dim_product (11 rows including unknown)","text":"product_sk product_id name category 0 -1 Unknown Unknown 1 P001 Laptop Pro 15 Electronics 2 P002 Wireless Mouse Electronics 3 P003 Office Chair Furniture 4 P004 USB-C Hub Electronics 5 P005 Standing Desk Furniture 6 P006 Mechanical Keyboard Electronics 7 P007 Monitor 27\" Electronics 8 P008 Desk Lamp Furniture 9 P009 Webcam HD Electronics 10 P010 Filing Cabinet Furniture"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#dim_date-15-rows-for-jan-15-28-unknown","title":"dim_date (15 rows for Jan 15-28 + unknown)","text":"date_sk full_date day_of_week month_name 0 1900-01-01 Unknown Unknown 20240115 2024-01-15 Monday January 20240116 2024-01-16 Tuesday January 20240117 2024-01-17 Wednesday January 20240118 2024-01-18 Thursday January 20240119 2024-01-19 Friday January 20240120 2024-01-20 Saturday January 20240121 2024-01-21 Sunday January 20240122 2024-01-22 Monday January 20240123 2024-01-23 Tuesday January 20240124 2024-01-24 Wednesday January 20240125 2024-01-25 Thursday January 20240126 2024-01-26 Friday January 20240127 2024-01-27 Saturday January 20240128 2024-01-28 Sunday January"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-1-understanding-sk-lookups","title":"Step 1: Understanding SK Lookups","text":"<p>The fact pattern's main job is to replace natural keys with surrogate keys:</p> <pre><code>Source:     customer_id = \"C001\"\n                    \u2193\n            Look up in dim_customer where customer_id = \"C001\"\n                    \u2193\nFact:       customer_sk = 1\n</code></pre> <p>This transformation happens for every dimension referenced.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: fact_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_fact_orders\n    nodes:\n      # First, load the dimension tables into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # Then build the fact table\n      - name: fact_orders\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - unit_price\n              - line_total: \"quantity * unit_price\"\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#the-transformation","title":"The Transformation","text":"<p>Here's what happens to the first 10 rows:</p> <p>Before (Source Data):</p> order_id customer_id product_id order_date quantity unit_price ORD001 C001 P001 2024-01-15 1 1299.99 ORD002 C001 P002 2024-01-15 2 29.99 ORD003 C002 P003 2024-01-16 1 249.99 ORD004 C003 P004 2024-01-16 3 49.99 ORD005 C004 P005 2024-01-17 1 599.99 ORD006 C005 P006 2024-01-17 1 149.99 ORD007 C006 P007 2024-01-18 2 399.99 ORD008 C007 P008 2024-01-18 4 45.99 ORD009 C008 P009 2024-01-19 1 79.99 ORD010 C009 P010 2024-01-19 1 189.99 <p>After (Fact Table):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status load_timestamp ORD001 1 1 20240115 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD002 1 2 20240115 2 29.99 59.98 completed 2024-01-30 10:00:00 ORD003 2 3 20240116 1 249.99 249.99 completed 2024-01-30 10:00:00 ORD004 3 4 20240116 3 49.99 149.97 completed 2024-01-30 10:00:00 ORD005 4 5 20240117 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD006 5 6 20240117 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD007 6 7 20240118 2 399.99 799.98 completed 2024-01-30 10:00:00 ORD008 7 8 20240118 4 45.99 183.96 completed 2024-01-30 10:00:00 ORD009 8 9 20240119 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD010 9 10 20240119 1 189.99 189.99 completed 2024-01-30 10:00:00 <p>Key observations: - <code>customer_id = \"C001\"</code> \u2192 <code>customer_sk = 1</code> - <code>product_id = \"P001\"</code> \u2192 <code>product_sk = 1</code> - <code>order_date = \"2024-01-15\"</code> \u2192 <code>date_sk = 20240115</code> - New column <code>line_total</code> calculated as <code>quantity * unit_price</code> - Original natural keys can be dropped or kept (configurable)</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-2-orphan-handling","title":"Step 2: Orphan Handling","text":"<p>What happens when a source record references a dimension value that doesn't exist?</p> <p>Source with Orphans (orders_with_orphans.csv) - 5 orphan rows:</p> order_id customer_id product_id order_date quantity unit_price ... ... ... ... ... ... ORD031 C999 P001 2024-01-17 1 1299.99 ORD032 C888 P002 2024-01-18 2 29.99 ORD033 C777 P003 2024-01-19 1 249.99 ORD034 C666 P004 2024-01-20 1 49.99 ORD035 C555 P005 2024-01-21 1 599.99 <p>Customer IDs C999, C888, C777, C666, C555 don't exist in dim_customer.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-1-unknown-default","title":"Strategy 1: Unknown (Default)","text":"<pre><code>params:\n  orphan_handling: unknown\n</code></pre> <p>Orphans are assigned to the unknown member (SK = 0):</p> order_id customer_sk product_sk note ORD031 0 1 C999 not found \u2192 SK = 0 ORD032 0 2 C888 not found \u2192 SK = 0 ORD033 0 3 C777 not found \u2192 SK = 0 ORD034 0 4 C666 not found \u2192 SK = 0 ORD035 0 5 C555 not found \u2192 SK = 0 <p>Pros: Data isn't lost, can investigate later Cons: Need to ensure unknown member exists in dimension</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-2-reject","title":"Strategy 2: Reject","text":"<pre><code>params:\n  orphan_handling: reject\n</code></pre> <p>Pipeline fails with an error listing orphan values:</p> <pre><code>OrphanRecordError: Found 5 orphan records for dimension 'dim_customer':\n  - customer_id='C999' (1 record)\n  - customer_id='C888' (1 record)\n  - customer_id='C777' (1 record)\n  - customer_id='C666' (1 record)\n  - customer_id='C555' (1 record)\n</code></pre> <p>Pros: Strict data quality enforcement Cons: Entire load fails, need to fix source data</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#strategy-3-quarantine","title":"Strategy 3: Quarantine","text":"<pre><code>params:\n  orphan_handling: quarantine\n</code></pre> <p>Orphan records are routed to a separate quarantine table:</p> <p>fact_orders (valid records): 30 rows</p> <p>fact_orders_quarantine (orphans): 5 rows</p> order_id customer_id orphan_reason ORD031 C999 customer_id not found in dim_customer ORD032 C888 customer_id not found in dim_customer ORD033 C777 customer_id not found in dim_customer ORD034 C666 customer_id not found in dim_customer ORD035 C555 customer_id not found in dim_customer <p>Pros: Valid data loads, orphans are preserved for review Cons: Need to manage quarantine table</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-3-grain-validation","title":"Step 3: Grain Validation","text":"<p>The grain defines what makes a fact row unique. For orders, it's typically <code>order_id</code>.</p> <pre><code>params:\n  grain: [order_id]\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#what-happens-with-duplicate-grain","title":"What Happens with Duplicate Grain?","text":"<p>If your source has duplicate order IDs:</p> order_id customer_id quantity ORD001 C001 1 ORD001 C001 2 <p>The pattern detects this and raises an error:</p> <pre><code>GrainValidationError: Duplicate grain detected in fact_orders\n  Grain columns: ['order_id']\n  Duplicate count: 1\n  Sample duplicates:\n    - order_id='ORD001' (2 occurrences)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#enabling-deduplication","title":"Enabling Deduplication","text":"<p>If duplicates are expected and you want to keep the latest:</p> <pre><code>params:\n  grain: [order_id]\n  deduplicate: true\n  keys: [order_id]\n</code></pre> <p>This keeps only the last occurrence of each order_id.</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#step-4-measure-calculations","title":"Step 4: Measure Calculations","text":"<p>Measures are the numeric values in your fact table. You can: - Pass through existing columns - Rename columns - Calculate derived values</p> <pre><code>params:\n  measures:\n    # Pass through\n    - quantity\n    - unit_price\n\n    # Rename (maps status to order_status)\n    - order_status: status\n\n    # Calculate\n    - line_total: \"quantity * unit_price\"\n    - discount_amount: \"unit_price * 0.1\"\n    - net_total: \"quantity * unit_price * 0.9\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#output-with-calculated-measures","title":"Output with Calculated Measures","text":"order_id quantity unit_price line_total discount_amount net_total ORD001 1 1299.99 1299.99 130.00 1169.99 ORD002 2 29.99 59.98 3.00 53.98 ORD003 1 249.99 249.99 25.00 224.99"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#complete-fact-table-output","title":"Complete Fact Table Output","text":"<p>Here's the complete fact_orders table (30 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status load_timestamp ORD001 1 1 20240115 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD002 1 2 20240115 2 29.99 59.98 completed 2024-01-30 10:00:00 ORD003 2 3 20240116 1 249.99 249.99 completed 2024-01-30 10:00:00 ORD004 3 4 20240116 3 49.99 149.97 completed 2024-01-30 10:00:00 ORD005 4 5 20240117 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD006 5 6 20240117 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD007 6 7 20240118 2 399.99 799.98 completed 2024-01-30 10:00:00 ORD008 7 8 20240118 4 45.99 183.96 completed 2024-01-30 10:00:00 ORD009 8 9 20240119 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD010 9 10 20240119 1 189.99 189.99 completed 2024-01-30 10:00:00 ORD011 10 1 20240120 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD012 11 2 20240120 5 29.99 149.95 completed 2024-01-30 10:00:00 ORD013 12 3 20240121 2 249.99 499.98 completed 2024-01-30 10:00:00 ORD014 1 4 20240121 1 49.99 49.99 completed 2024-01-30 10:00:00 ORD015 2 5 20240122 1 599.99 599.99 pending 2024-01-30 10:00:00 ORD016 3 6 20240122 2 149.99 299.98 completed 2024-01-30 10:00:00 ORD017 4 7 20240123 1 399.99 399.99 completed 2024-01-30 10:00:00 ORD018 5 8 20240123 3 45.99 137.97 completed 2024-01-30 10:00:00 ORD019 6 9 20240124 2 79.99 159.98 completed 2024-01-30 10:00:00 ORD020 7 10 20240124 1 189.99 189.99 completed 2024-01-30 10:00:00 ORD021 8 1 20240125 1 1299.99 1299.99 completed 2024-01-30 10:00:00 ORD022 9 2 20240125 3 29.99 89.97 completed 2024-01-30 10:00:00 ORD023 10 3 20240126 1 249.99 249.99 cancelled 2024-01-30 10:00:00 ORD024 11 4 20240126 2 49.99 99.98 completed 2024-01-30 10:00:00 ORD025 12 5 20240127 1 599.99 599.99 completed 2024-01-30 10:00:00 ORD026 1 6 20240127 1 149.99 149.99 completed 2024-01-30 10:00:00 ORD027 2 7 20240128 1 399.99 399.99 completed 2024-01-30 10:00:00 ORD028 3 8 20240128 2 45.99 91.98 completed 2024-01-30 10:00:00 ORD029 4 9 20240115 1 79.99 79.99 completed 2024-01-30 10:00:00 ORD030 5 10 20240116 1 189.99 189.99 completed 2024-01-30 10:00:00"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#complete-runnable-example","title":"Complete Runnable Example","text":"<pre><code># File: odibi_fact_tutorial.yaml\nproject: fact_tutorial\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_fact_orders\n    description: \"Build fact table with SK lookups\"\n    nodes:\n      # Load dimension tables\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # Build fact table\n      - name: fact_orders\n        description: \"Orders fact table with surrogate key lookups\"\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        pattern:\n          type: fact\n          params:\n            # Define the grain (uniqueness)\n            grain: [order_id]\n\n            # Define dimension lookups\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true  # Filter to is_current = true\n\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n\n            # Handle missing dimension values\n            orphan_handling: unknown\n\n            # Define measures\n            measures:\n              - quantity\n              - unit_price\n              - line_total: \"quantity * unit_price\"\n\n            # Add audit columns\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#python-api-alternative","title":"Python API Alternative","text":"<pre><code>from odibi.patterns.fact import FactPattern\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load source data\norders_df = pd.read_csv(\"examples/tutorials/dimensional_modeling/data/orders.csv\")\n\n# Load dimensions\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\n# Create context and register dimensions\ncontext = EngineContext(df=orders_df, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# Create pattern\npattern = FactPattern(params={\n    \"grain\": [\"order_id\"],\n    \"dimensions\": [\n        {\n            \"source_column\": \"customer_id\",\n            \"dimension_table\": \"dim_customer\",\n            \"dimension_key\": \"customer_id\",\n            \"surrogate_key\": \"customer_sk\",\n            \"scd2\": True\n        },\n        {\n            \"source_column\": \"product_id\",\n            \"dimension_table\": \"dim_product\",\n            \"dimension_key\": \"product_id\",\n            \"surrogate_key\": \"product_sk\"\n        },\n        {\n            \"source_column\": \"order_date\",\n            \"dimension_table\": \"dim_date\",\n            \"dimension_key\": \"full_date\",\n            \"surrogate_key\": \"date_sk\"\n        }\n    ],\n    \"orphan_handling\": \"unknown\",\n    \"measures\": [\n        \"quantity\",\n        \"unit_price\",\n        {\"line_total\": \"quantity * unit_price\"}\n    ],\n    \"audit\": {\n        \"load_timestamp\": True,\n        \"source_system\": \"pos\"\n    }\n})\n\n# Execute pattern\nresult_df = pattern.execute(context)\n\nprint(f\"Generated {len(result_df)} fact rows\")\nprint(result_df.head(10))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#querying-the-star-schema","title":"Querying the Star Schema","text":"<p>Now you can run powerful analytical queries:</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#sales-by-region","title":"\"Sales by region\"","text":"<pre><code>SELECT\n    c.region,\n    COUNT(*) AS order_count,\n    SUM(f.line_total) AS total_revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.region\nORDER BY total_revenue DESC;\n</code></pre> region order_count total_revenue North 8 2,599.87 East 8 2,023.87 South 7 2,449.92 West 7 2,629.88"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#daily-sales-trend","title":"\"Daily sales trend\"","text":"<pre><code>SELECT\n    d.full_date,\n    d.day_of_week,\n    SUM(f.line_total) AS daily_revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nGROUP BY d.full_date, d.day_of_week\nORDER BY d.full_date;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#top-products-by-category","title":"\"Top products by category\"","text":"<pre><code>SELECT\n    p.category,\n    p.name,\n    SUM(f.quantity) AS units_sold,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.category, p.name\nORDER BY revenue DESC;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Surrogate key lookups automatically replace natural keys with dimension SKs</li> <li>scd2: true filters to current dimension rows when looking up SCD2 dimensions</li> <li>Orphan handling strategies: unknown (default), reject, or quarantine</li> <li>Grain validation detects duplicate records at the grain level</li> <li>Measures can be passed through, renamed, or calculated</li> <li>depends_on ensures dimension tables are loaded before the fact pattern runs</li> <li>Audit columns track when and where data was loaded</li> </ul>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#next-steps","title":"Next Steps","text":"<p>Now that you have a complete star schema, let's build aggregated tables for faster reporting.</p> <p>Next: Aggregation Pattern Tutorial</p>"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#navigation","title":"Navigation","text":"Previous Up Next Date Dimension Tutorials Aggregation Pattern"},{"location":"tutorials/dimensional_modeling/04_fact_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Fact Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/","title":"Aggregation Pattern Tutorial","text":"<p>In this tutorial, you'll learn how to use Odibi's <code>aggregation</code> pattern to build pre-aggregated tables for faster reporting and dashboards.</p> <p>What You'll Learn: - Why pre-aggregate fact tables - Defining grain and measures - Using aggregate functions (SUM, COUNT, AVG) - Incremental merge strategies (replace vs sum)</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#why-pre-aggregate","title":"Why Pre-Aggregate?","text":"<p>Consider a dashboard showing \"Daily Revenue by Product Category\":</p> <p>Without pre-aggregation: <pre><code>-- Runs against 10 million fact rows every time\nSELECT\n    d.full_date,\n    p.category,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_sk = d.date_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY d.full_date, p.category;\n-- Takes 30 seconds\n</code></pre></p> <p>With pre-aggregation: <pre><code>-- Runs against 5,000 aggregated rows\nSELECT full_date, category, total_revenue\nFROM agg_daily_category_sales;\n-- Takes 0.1 seconds\n</code></pre></p> <p>Pre-aggregated tables trade storage for speed. For frequently-queried metrics, this is almost always worth it.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#source-data-fact_orders","title":"Source Data: fact_orders","text":"<p>We'll aggregate the fact table from the previous tutorial (30 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD011 10 1 20240120 1 1299.99 1299.99 completed ORD012 11 2 20240120 5 29.99 149.95 completed ORD013 12 3 20240121 2 249.99 499.98 completed ORD014 1 4 20240121 1 49.99 49.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending ORD016 3 6 20240122 2 149.99 299.98 completed ORD017 4 7 20240123 1 399.99 399.99 completed ORD018 5 8 20240123 3 45.99 137.97 completed ORD019 6 9 20240124 2 79.99 159.98 completed ORD020 7 10 20240124 1 189.99 189.99 completed ORD021 8 1 20240125 1 1299.99 1299.99 completed ORD022 9 2 20240125 3 29.99 89.97 completed ORD023 10 3 20240126 1 249.99 249.99 cancelled ORD024 11 4 20240126 2 49.99 99.98 completed ORD025 12 5 20240127 1 599.99 599.99 completed ORD026 1 6 20240127 1 149.99 149.99 completed ORD027 2 7 20240128 1 399.99 399.99 completed ORD028 3 8 20240128 2 45.99 91.98 completed ORD029 4 9 20240115 1 79.99 79.99 completed ORD030 5 10 20240116 1 189.99 189.99 completed"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-1-basic-aggregation-by-date-and-product","title":"Step 1: Basic Aggregation by Date and Product","text":"<p>Let's aggregate orders by date and product to see daily product sales.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>project: aggregation_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    nodes:\n      - name: agg_daily_product_sales\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk, product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: total_quantity\n                expr: \"SUM(quantity)\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#understanding-grain-and-measures","title":"Understanding Grain and Measures","text":"<p>Grain: The columns that define uniqueness in the output. Here, each combination of <code>date_sk</code> + <code>product_sk</code> gets one row.</p> <p>Measures: The aggregations to compute. Each measure needs: - <code>name</code>: Output column name - <code>expr</code>: SQL aggregation expression</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#output-agg_daily_product_sales-26-rows","title":"Output: agg_daily_product_sales (26 rows)","text":"<p>Here are all 26 aggregated rows:</p> date_sk product_sk total_revenue order_count total_quantity load_timestamp 20240115 1 1299.99 1 1 2024-01-30 10:00:00 20240115 2 59.98 1 2 2024-01-30 10:00:00 20240115 9 79.99 1 1 2024-01-30 10:00:00 20240116 3 249.99 1 1 2024-01-30 10:00:00 20240116 4 149.97 1 3 2024-01-30 10:00:00 20240116 10 189.99 1 1 2024-01-30 10:00:00 20240117 5 599.99 1 1 2024-01-30 10:00:00 20240117 6 149.99 1 1 2024-01-30 10:00:00 20240118 7 799.98 1 2 2024-01-30 10:00:00 20240118 8 183.96 1 4 2024-01-30 10:00:00 20240119 9 79.99 1 1 2024-01-30 10:00:00 20240119 10 189.99 1 1 2024-01-30 10:00:00 20240120 1 1299.99 1 1 2024-01-30 10:00:00 20240120 2 149.95 1 5 2024-01-30 10:00:00 20240121 3 499.98 1 2 2024-01-30 10:00:00 20240121 4 49.99 1 1 2024-01-30 10:00:00 20240122 5 599.99 1 1 2024-01-30 10:00:00 20240122 6 299.98 1 2 2024-01-30 10:00:00 20240123 7 399.99 1 1 2024-01-30 10:00:00 20240123 8 137.97 1 3 2024-01-30 10:00:00 20240124 9 159.98 1 2 2024-01-30 10:00:00 20240124 10 189.99 1 1 2024-01-30 10:00:00 20240125 1 1299.99 1 1 2024-01-30 10:00:00 20240125 2 89.97 1 3 2024-01-30 10:00:00 20240126 3 249.99 1 1 2024-01-30 10:00:00 20240126 4 99.98 1 2 2024-01-30 10:00:00 20240127 5 599.99 1 1 2024-01-30 10:00:00 20240127 6 149.99 1 1 2024-01-30 10:00:00 20240128 7 399.99 1 1 2024-01-30 10:00:00 20240128 8 91.98 1 2 2024-01-30 10:00:00 <p>Result: 30 fact rows became 26 aggregate rows (some date+product combinations had multiple orders).</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-2-adding-more-measures","title":"Step 2: Adding More Measures","text":"<p>Let's add average and distinct count measures:</p> <pre><code>params:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: total_quantity\n      expr: \"SUM(quantity)\"\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n    - name: max_order_value\n      expr: \"MAX(line_total)\"\n    - name: min_order_value\n      expr: \"MIN(line_total)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>SUM(column)</code> Total of values <code>SUM(line_total)</code> <code>COUNT(*)</code> Number of rows <code>COUNT(*)</code> <code>COUNT(column)</code> Non-null count <code>COUNT(customer_sk)</code> <code>COUNT(DISTINCT column)</code> Unique values <code>COUNT(DISTINCT customer_sk)</code> <code>AVG(column)</code> Average value <code>AVG(line_total)</code> <code>MAX(column)</code> Maximum value <code>MAX(line_total)</code> <code>MIN(column)</code> Minimum value <code>MIN(line_total)</code>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#complex-expressions","title":"Complex Expressions","text":"<p>You can use expressions within aggregations:</p> <pre><code>measures:\n  # Total after 10% discount\n  - name: discounted_revenue\n    expr: \"SUM(line_total * 0.9)\"\n\n  # Margin (if cost column existed)\n  - name: total_margin\n    expr: \"SUM(line_total - cost)\"\n\n  # Discount rate\n  - name: avg_discount_rate\n    expr: \"AVG(discount_amount / line_total)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-3-aggregating-by-different-grains","title":"Step 3: Aggregating by Different Grains","text":""},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#daily-sales-grain-date-only","title":"Daily Sales (Grain: date only)","text":"<pre><code>params:\n  grain: [date_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: unique_products\n      expr: \"COUNT(DISTINCT product_sk)\"\n    - name: unique_customers\n      expr: \"COUNT(DISTINCT customer_sk)\"\n</code></pre> <p>Output: agg_daily_sales (14 rows)</p> date_sk total_revenue order_count unique_products unique_customers 20240115 1439.96 3 3 2 20240116 589.95 3 3 3 20240117 749.98 2 2 2 20240118 983.94 2 2 2 20240119 269.98 2 2 2 20240120 1449.94 2 2 2 20240121 549.97 2 2 2 20240122 899.97 2 2 2 20240123 537.96 2 2 2 20240124 349.97 2 2 2 20240125 1389.96 2 2 2 20240126 349.97 2 2 2 20240127 749.98 2 2 2 20240128 491.97 2 2 2"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#product-sales-grain-product-only","title":"Product Sales (Grain: product only)","text":"<pre><code>params:\n  grain: [product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n    - name: total_quantity\n      expr: \"SUM(quantity)\"\n</code></pre> <p>Output: agg_product_sales (10 rows)</p> product_sk total_revenue order_count total_quantity 1 3899.97 3 3 2 299.90 3 10 3 999.96 3 4 4 299.94 3 6 5 1799.97 3 3 6 599.96 3 4 7 1599.96 3 4 8 413.91 3 9 9 319.96 3 4 10 569.97 3 3"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#step-4-incremental-merge-strategies","title":"Step 4: Incremental Merge Strategies","text":"<p>For ongoing pipelines, you don't want to rebuild the entire aggregate table. Incremental strategies allow you to update only affected rows.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#replace-strategy","title":"Replace Strategy","text":"<p>How it works: New aggregates completely replace existing rows for matching grain keys.</p> <pre><code>params:\n  grain: [date_sk, product_sk]\n  measures:\n    - name: total_revenue\n      expr: \"SUM(line_total)\"\n    - name: order_count\n      expr: \"COUNT(*)\"\n  incremental:\n    timestamp_column: load_timestamp\n    merge_strategy: replace\n  target: warehouse.agg_daily_product_sales\n</code></pre> <p>Example Scenario:</p> <p>Existing agg_daily_product_sales:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 20240115 2 59.98 1 20240116 3 249.99 1 <p>New orders arrive for 2024-01-15:</p> order_id product_sk date_sk line_total ORD100 1 20240115 1299.99 <p>New aggregate computed:</p> date_sk product_sk total_revenue order_count 20240115 1 2599.98 2 <p>After Replace Merge:</p> date_sk product_sk total_revenue order_count Note 20240115 1 2599.98 2 Replaced 20240115 2 59.98 1 Unchanged 20240116 3 249.99 1 Unchanged <p>Use case: Best for most scenarios. Handles late-arriving data, corrections, and restatements correctly.</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#sum-strategy","title":"Sum Strategy","text":"<p>How it works: New measure values are added to existing values.</p> <pre><code>params:\n  incremental:\n    timestamp_column: load_timestamp\n    merge_strategy: sum\n</code></pre> <p>Example Scenario:</p> <p>Existing:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 <p>New aggregate:</p> date_sk product_sk total_revenue order_count 20240115 1 1299.99 1 <p>After Sum Merge:</p> date_sk product_sk total_revenue order_count 20240115 1 2599.98 2 <p>Use case: Only for purely additive metrics (counts, sums) on append-only data.</p> <p>Warning: Do NOT use sum strategy for: - AVG (would become average of averages, incorrect) - COUNT DISTINCT (would overcount) - MIN/MAX (would be wrong) - Data with updates or late-arriving records</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#complete-yaml-example","title":"Complete YAML Example","text":"<p>Here's a complete example with multiple aggregate tables:</p> <pre><code># File: odibi_aggregation_tutorial.yaml\nproject: aggregation_tutorial\nengine: pandas\n\nconnections:\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\npipelines:\n  - pipeline: build_aggregates\n    description: \"Build all aggregate tables from fact_orders\"\n    nodes:\n      # Daily aggregate at product level\n      - name: agg_daily_product_sales\n        description: \"Daily sales by product\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        pattern:\n          type: aggregation\n          params:\n            grain:\n              - date_sk\n              - product_sk\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: total_quantity\n                expr: \"SUM(quantity)\"\n              - name: avg_order_value\n                expr: \"AVG(line_total)\"\n            having: \"SUM(line_total) &gt; 0\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_product_sales\n          format: parquet\n          mode: overwrite\n\n      # Daily summary (no product breakdown)\n      - name: agg_daily_sales\n        description: \"Daily sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: unique_products\n                expr: \"COUNT(DISTINCT product_sk)\"\n              - name: unique_customers\n                expr: \"COUNT(DISTINCT customer_sk)\"\n              - name: avg_order_value\n                expr: \"AVG(line_total)\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          format: parquet\n          mode: overwrite\n\n      # Product summary (no date breakdown)\n      - name: agg_product_sales\n        description: \"Product sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [product_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: total_quantity\n                expr: \"SUM(quantity)\"\n              - name: unique_customers\n                expr: \"COUNT(DISTINCT customer_sk)\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_product_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#using-aggregates-in-queries","title":"Using Aggregates in Queries","text":""},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#dashboard-query-daily-revenue-trend","title":"Dashboard Query: Daily Revenue Trend","text":"<pre><code>SELECT\n    a.date_sk,\n    d.full_date,\n    d.day_of_week,\n    a.total_revenue,\n    a.order_count\nFROM agg_daily_sales a\nJOIN dim_date d ON a.date_sk = d.date_sk\nORDER BY a.date_sk;\n</code></pre> full_date day_of_week total_revenue order_count 2024-01-15 Monday 1439.96 3 2024-01-16 Tuesday 589.95 3 2024-01-17 Wednesday 749.98 2 2024-01-18 Thursday 983.94 2 2024-01-19 Friday 269.98 2 2024-01-20 Saturday 1449.94 2 2024-01-21 Sunday 549.97 2 2024-01-22 Monday 899.97 2 2024-01-23 Tuesday 537.96 2 2024-01-24 Wednesday 349.97 2 2024-01-25 Thursday 1389.96 2 2024-01-26 Friday 349.97 2 2024-01-27 Saturday 749.98 2 2024-01-28 Sunday 491.97 2"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#dashboard-query-top-products","title":"Dashboard Query: Top Products","text":"<pre><code>SELECT\n    p.name AS product_name,\n    p.category,\n    a.total_revenue,\n    a.order_count,\n    a.total_quantity\nFROM agg_product_sales a\nJOIN dim_product p ON a.product_sk = p.product_sk\nORDER BY a.total_revenue DESC\nLIMIT 5;\n</code></pre> product_name category total_revenue order_count total_quantity Laptop Pro 15 Electronics 3899.97 3 3 Standing Desk Furniture 1799.97 3 3 Monitor 27\" Electronics 1599.96 3 4 Office Chair Furniture 999.96 3 4 Mechanical Keyboard Electronics 599.96 3 4"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Pre-aggregation speeds up reporting by reducing data volume</li> <li>Grain defines the uniqueness (GROUP BY columns) of aggregate rows</li> <li>Measures define what to compute (SUM, COUNT, AVG, etc.)</li> <li>Complex expressions can be used within aggregations</li> <li>Replace strategy is best for most incremental scenarios</li> <li>Sum strategy works only for purely additive metrics on append-only data</li> <li>Aggregate tables are joined back to dimensions for rich reporting</li> </ul>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#next-steps","title":"Next Steps","text":"<p>Now let's put it all together with a complete star schema example.</p> <p>Next: Full Star Schema Tutorial</p>"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#navigation","title":"Navigation","text":"Previous Up Next Fact Pattern Tutorials Full Star Schema"},{"location":"tutorials/dimensional_modeling/05_aggregation_pattern/#reference","title":"Reference","text":"<p>For complete parameter documentation, see: Aggregation Pattern Reference</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/","title":"Full Star Schema Tutorial","text":"<p>In this tutorial, you'll build a complete star schema from start to finish, combining all the patterns from previous tutorials into a single, cohesive data warehouse.</p> <p>What You'll Build: - <code>dim_customer</code> - Customer dimension (SCD2) - <code>dim_product</code> - Product dimension (SCD1) - <code>dim_date</code> - Date dimension (generated) - <code>fact_orders</code> - Orders fact table - <code>agg_daily_sales</code> - Daily sales aggregate</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#the-complete-star-schema","title":"The Complete Star Schema","text":"<pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n    AGG_DAILY_SALES ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        string order_id PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        int quantity\n        decimal unit_price\n        decimal line_total\n        string status\n        timestamp load_timestamp\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string email\n        string region\n        string city\n        string state\n        date valid_from\n        date valid_to\n        bool is_current\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n        string subcategory\n        decimal price\n        decimal cost\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        int month\n        string month_name\n        int quarter\n        int year\n        int fiscal_year\n        int fiscal_quarter\n    }\n\n    AGG_DAILY_SALES {\n        int date_sk PK\n        decimal total_revenue\n        int order_count\n        int unique_customers\n        int unique_products\n    }\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#source-data-files","title":"Source Data Files","text":"<p>All source data is in <code>examples/tutorials/dimensional_modeling/data/</code>:</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#customerscsv-12-rows","title":"customers.csv (12 rows)","text":"customer_id name email region city state C001 Alice Johnson alice@example.com North Chicago IL C002 Bob Smith bob@example.com South Houston TX C003 Carol White carol@example.com North Detroit MI C004 David Brown david@example.com East New York NY C005 Emma Davis emma@example.com West Seattle WA C006 Frank Miller frank@example.com South Miami FL C007 Grace Lee grace@example.com East Boston MA C008 Henry Wilson henry@example.com West Portland OR C009 Ivy Chen ivy@example.com North Minneapolis MN C010 Jack Taylor jack@example.com South Dallas TX C011 Karen Martinez karen@example.com East Philadelphia PA C012 Leo Anderson leo@example.com West Denver CO"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#productscsv-10-rows","title":"products.csv (10 rows)","text":"product_id name category subcategory price cost P001 Laptop Pro 15 Electronics Computers 1299.99 850.00 P002 Wireless Mouse Electronics Accessories 29.99 12.00 P003 Office Chair Furniture Seating 249.99 120.00 P004 USB-C Hub Electronics Accessories 49.99 22.00 P005 Standing Desk Furniture Desks 599.99 320.00 P006 Mechanical Keyboard Electronics Accessories 149.99 65.00 P007 Monitor 27\" Electronics Displays 399.99 210.00 P008 Desk Lamp Furniture Lighting 45.99 18.00 P009 Webcam HD Electronics Accessories 79.99 35.00 P010 Filing Cabinet Furniture Storage 189.99 95.00"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#orderscsv-30-rows","title":"orders.csv (30 rows)","text":"order_id customer_id product_id order_date quantity unit_price status ORD001 C001 P001 2024-01-15 1 1299.99 completed ORD002 C001 P002 2024-01-15 2 29.99 completed ORD003 C002 P003 2024-01-16 1 249.99 completed ORD004 C003 P004 2024-01-16 3 49.99 completed ORD005 C004 P005 2024-01-17 1 599.99 completed ORD006 C005 P006 2024-01-17 1 149.99 completed ORD007 C006 P007 2024-01-18 2 399.99 completed ORD008 C007 P008 2024-01-18 4 45.99 completed ORD009 C008 P009 2024-01-19 1 79.99 completed ORD010 C009 P010 2024-01-19 1 189.99 completed ORD011 C010 P001 2024-01-20 1 1299.99 completed ORD012 C011 P002 2024-01-20 5 29.99 completed ORD013 C012 P003 2024-01-21 2 249.99 completed ORD014 C001 P004 2024-01-21 1 49.99 completed ORD015 C002 P005 2024-01-22 1 599.99 pending ORD016 C003 P006 2024-01-22 2 149.99 completed ORD017 C004 P007 2024-01-23 1 399.99 completed ORD018 C005 P008 2024-01-23 3 45.99 completed ORD019 C006 P009 2024-01-24 2 79.99 completed ORD020 C007 P010 2024-01-24 1 189.99 completed ORD021 C008 P001 2024-01-25 1 1299.99 completed ORD022 C009 P002 2024-01-25 3 29.99 completed ORD023 C010 P003 2024-01-26 1 249.99 cancelled ORD024 C011 P004 2024-01-26 2 49.99 completed ORD025 C012 P005 2024-01-27 1 599.99 completed ORD026 C001 P006 2024-01-27 1 149.99 completed ORD027 C002 P007 2024-01-28 1 399.99 completed ORD028 C003 P008 2024-01-28 2 45.99 completed ORD029 C004 P009 2024-01-15 1 79.99 completed ORD030 C005 P010 2024-01-16 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#complete-yaml-configuration","title":"Complete YAML Configuration","text":"<p>Here's the full pipeline that builds everything:</p> <pre><code># File: examples/tutorials/dimensional_modeling/star_schema.yaml\nproject: retail_star_schema\nengine: pandas\n\nconnections:\n  source:\n    type: local\n    path: ./examples/tutorials/dimensional_modeling/data\n  warehouse:\n    type: local\n    path: ./warehouse\n\nstory:\n  connection: warehouse\n  path: stories\n\nsystem:\n  connection: warehouse\n  path: _system_catalog\n\npipelines:\n  # ==========================================\n  # PIPELINE 1: Build all dimensions\n  # ==========================================\n  - pipeline: build_dimensions\n    description: \"Build customer, product, and date dimensions\"\n    nodes:\n      # ------------------------------------------\n      # Customer Dimension (SCD Type 2)\n      # ------------------------------------------\n      - name: dim_customer\n        description: \"Customer dimension with full history tracking\"\n        read:\n          connection: source\n          path: customers.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: customer_id\n            surrogate_key: customer_sk\n            scd_type: 2\n            track_cols:\n              - name\n              - email\n              - region\n              - city\n              - state\n            target: warehouse.dim_customer\n            valid_from_col: valid_from\n            valid_to_col: valid_to\n            is_current_col: is_current\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"crm\"\n\n        write:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n          mode: overwrite\n\n      # ------------------------------------------\n      # Product Dimension (SCD Type 1)\n      # ------------------------------------------\n      - name: dim_product\n        description: \"Product dimension with overwrite updates\"\n        read:\n          connection: source\n          path: products.csv\n          format: csv\n\n        pattern:\n          type: dimension\n          params:\n            natural_key: product_id\n            surrogate_key: product_sk\n            scd_type: 1\n            track_cols:\n              - name\n              - category\n              - subcategory\n              - price\n              - cost\n            target: warehouse.dim_product\n            unknown_member: true\n            audit:\n              load_timestamp: true\n              source_system: \"inventory\"\n\n        write:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n          mode: overwrite\n\n      # ------------------------------------------\n      # Date Dimension (Generated)\n      # ------------------------------------------\n      - name: dim_date\n        description: \"Date dimension covering Jan 2024\"\n        pattern:\n          type: date_dimension\n          params:\n            start_date: \"2024-01-01\"\n            end_date: \"2024-01-31\"\n            fiscal_year_start_month: 7\n            unknown_member: true\n\n        write:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n          mode: overwrite\n\n  # ==========================================\n  # PIPELINE 2: Build fact tables\n  # ==========================================\n  - pipeline: build_facts\n    description: \"Build fact_orders with SK lookups\"\n    nodes:\n      # Load dimensions into context\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n          format: parquet\n\n      - name: dim_product\n        read:\n          connection: warehouse\n          path: dim_product\n          format: parquet\n\n      - name: dim_date\n        read:\n          connection: warehouse\n          path: dim_date\n          format: parquet\n\n      # ------------------------------------------\n      # Orders Fact Table\n      # ------------------------------------------\n      - name: fact_orders\n        description: \"Orders fact with all dimension lookups\"\n        depends_on: [dim_customer, dim_product, dim_date]\n        read:\n          connection: source\n          path: orders.csv\n          format: csv\n\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n                scd2: true\n              - source_column: product_id\n                dimension_table: dim_product\n                dimension_key: product_id\n                surrogate_key: product_sk\n              - source_column: order_date\n                dimension_table: dim_date\n                dimension_key: full_date\n                surrogate_key: date_sk\n            orphan_handling: unknown\n            measures:\n              - quantity\n              - unit_price\n              - line_total: \"quantity * unit_price\"\n            audit:\n              load_timestamp: true\n              source_system: \"pos\"\n\n        write:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n          mode: overwrite\n\n  # ==========================================\n  # PIPELINE 3: Build aggregates\n  # ==========================================\n  - pipeline: build_aggregates\n    description: \"Build daily sales aggregate\"\n    nodes:\n      # ------------------------------------------\n      # Daily Sales Aggregate\n      # ------------------------------------------\n      - name: agg_daily_sales\n        description: \"Daily sales summary\"\n        read:\n          connection: warehouse\n          path: fact_orders\n          format: parquet\n\n        pattern:\n          type: aggregation\n          params:\n            grain: [date_sk]\n            measures:\n              - name: total_revenue\n                expr: \"SUM(line_total)\"\n              - name: order_count\n                expr: \"COUNT(*)\"\n              - name: unique_customers\n                expr: \"COUNT(DISTINCT customer_sk)\"\n              - name: unique_products\n                expr: \"COUNT(DISTINCT product_sk)\"\n              - name: avg_order_value\n                expr: \"AVG(line_total)\"\n            audit:\n              load_timestamp: true\n\n        write:\n          connection: warehouse\n          path: agg_daily_sales\n          format: parquet\n          mode: overwrite\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Run the pipelines in order:</p> <pre><code># Build dimensions first\nodibi run --config star_schema.yaml --pipeline build_dimensions\n\n# Then build facts (requires dimensions)\nodibi run --config star_schema.yaml --pipeline build_facts\n\n# Finally build aggregates (requires facts)\nodibi run --config star_schema.yaml --pipeline build_aggregates\n</code></pre> <p>Or run everything:</p> <pre><code>odibi run --config star_schema.yaml\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#final-table-schemas-and-sample-data","title":"Final Table Schemas and Sample Data","text":""},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_customer-13-rows","title":"dim_customer (13 rows)","text":"<p>Schema:</p> Column Type Description customer_sk int Surrogate key customer_id string Natural key name string Customer name email string Email address region string Geographic region city string City state string State code valid_from timestamp Version start date valid_to timestamp Version end date (NULL if current) is_current boolean Current version flag load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (13 rows):</p> customer_sk customer_id name email region city is_current 0 -1 Unknown Unknown Unknown Unknown true 1 C001 Alice Johnson alice@example.com North Chicago true 2 C002 Bob Smith bob@example.com South Houston true 3 C003 Carol White carol@example.com North Detroit true 4 C004 David Brown david@example.com East New York true 5 C005 Emma Davis emma@example.com West Seattle true 6 C006 Frank Miller frank@example.com South Miami true 7 C007 Grace Lee grace@example.com East Boston true 8 C008 Henry Wilson henry@example.com West Portland true 9 C009 Ivy Chen ivy@example.com North Minneapolis true 10 C010 Jack Taylor jack@example.com South Dallas true 11 C011 Karen Martinez karen@example.com East Philadelphia true 12 C012 Leo Anderson leo@example.com West Denver true"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_product-11-rows","title":"dim_product (11 rows)","text":"<p>Schema:</p> Column Type Description product_sk int Surrogate key product_id string Natural key name string Product name category string Product category subcategory string Product subcategory price decimal List price cost decimal Unit cost load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (11 rows):</p> product_sk product_id name category subcategory price cost 0 -1 Unknown Unknown Unknown 0.00 0.00 1 P001 Laptop Pro 15 Electronics Computers 1299.99 850.00 2 P002 Wireless Mouse Electronics Accessories 29.99 12.00 3 P003 Office Chair Furniture Seating 249.99 120.00 4 P004 USB-C Hub Electronics Accessories 49.99 22.00 5 P005 Standing Desk Furniture Desks 599.99 320.00 6 P006 Mechanical Keyboard Electronics Accessories 149.99 65.00 7 P007 Monitor 27\" Electronics Displays 399.99 210.00 8 P008 Desk Lamp Furniture Lighting 45.99 18.00 9 P009 Webcam HD Electronics Accessories 79.99 35.00 10 P010 Filing Cabinet Furniture Storage 189.99 95.00"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#dim_date-32-rows-for-january-unknown","title":"dim_date (32 rows for January + unknown)","text":"<p>Sample Data (first 15 rows):</p> date_sk full_date day_of_week month_name quarter_name year fiscal_year fiscal_quarter 0 1900-01-01 Unknown Unknown Unknown 0 0 0 20240101 2024-01-01 Monday January Q1 2024 2024 3 20240102 2024-01-02 Tuesday January Q1 2024 2024 3 20240103 2024-01-03 Wednesday January Q1 2024 2024 3 20240104 2024-01-04 Thursday January Q1 2024 2024 3 20240105 2024-01-05 Friday January Q1 2024 2024 3 20240106 2024-01-06 Saturday January Q1 2024 2024 3 20240107 2024-01-07 Sunday January Q1 2024 2024 3 20240108 2024-01-08 Monday January Q1 2024 2024 3 20240109 2024-01-09 Tuesday January Q1 2024 2024 3 20240110 2024-01-10 Wednesday January Q1 2024 2024 3 20240111 2024-01-11 Thursday January Q1 2024 2024 3 20240112 2024-01-12 Friday January Q1 2024 2024 3 20240113 2024-01-13 Saturday January Q1 2024 2024 3 20240114 2024-01-14 Sunday January Q1 2024 2024 3"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#fact_orders-30-rows","title":"fact_orders (30 rows)","text":"<p>Schema:</p> Column Type Description order_id string Order identifier (grain) customer_sk int Customer surrogate key product_sk int Product surrogate key date_sk int Date surrogate key quantity int Quantity ordered unit_price decimal Price per unit line_total decimal Calculated: quantity \u00d7 unit_price status string Order status load_timestamp timestamp ETL load time source_system string Source system name <p>Sample Data (first 15 rows):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD011 10 1 20240120 1 1299.99 1299.99 completed ORD012 11 2 20240120 5 29.99 149.95 completed ORD013 12 3 20240121 2 249.99 499.98 completed ORD014 1 4 20240121 1 49.99 49.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#agg_daily_sales-14-rows","title":"agg_daily_sales (14 rows)","text":"<p>Sample Data (all 14 rows):</p> date_sk total_revenue order_count unique_customers unique_products avg_order_value 20240115 1439.96 3 2 3 479.99 20240116 589.95 3 3 3 196.65 20240117 749.98 2 2 2 374.99 20240118 983.94 2 2 2 491.97 20240119 269.98 2 2 2 134.99 20240120 1449.94 2 2 2 724.97 20240121 549.97 2 2 2 274.99 20240122 899.97 2 2 2 449.99 20240123 537.96 2 2 2 268.98 20240124 349.97 2 2 2 174.99 20240125 1389.96 2 2 2 694.98 20240126 349.97 2 2 2 174.99 20240127 749.98 2 2 2 374.99 20240128 491.97 2 2 2 245.99"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#sample-analytical-queries","title":"Sample Analytical Queries","text":""},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#revenue-by-region","title":"Revenue by Region","text":"<pre><code>SELECT\n    c.region,\n    COUNT(DISTINCT f.order_id) AS orders,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.region\nORDER BY revenue DESC;\n</code></pre> region orders revenue North 8 2,599.87 West 7 2,629.88 South 7 2,449.92 East 8 2,023.87"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#revenue-by-category","title":"Revenue by Category","text":"<pre><code>SELECT\n    p.category,\n    COUNT(*) AS orders,\n    SUM(f.line_total) AS revenue,\n    AVG(f.line_total) AS avg_order\nFROM fact_orders f\nJOIN dim_product p ON f.product_sk = p.product_sk\nGROUP BY p.category\nORDER BY revenue DESC;\n</code></pre> category orders revenue avg_order Electronics 18 6,619.63 367.76 Furniture 12 3,183.91 265.33"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#daily-trend-with-day-of-week","title":"Daily Trend with Day of Week","text":"<pre><code>SELECT\n    d.full_date,\n    d.day_of_week,\n    a.total_revenue,\n    a.order_count\nFROM agg_daily_sales a\nJOIN dim_date d ON a.date_sk = d.date_sk\nORDER BY a.date_sk;\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#top-customers","title":"Top Customers","text":"<pre><code>SELECT\n    c.name,\n    c.region,\n    COUNT(*) AS orders,\n    SUM(f.line_total) AS total_spent\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE c.is_current = true\nGROUP BY c.customer_sk, c.name, c.region\nORDER BY total_spent DESC\nLIMIT 5;\n</code></pre> name region orders total_spent Alice Johnson North 4 1,559.95 Bob Smith South 3 1,049.97 Carol White North 3 541.93 David Brown East 3 1,079.97 Emma Davis West 3 477.97"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#data-flow-diagram","title":"Data Flow Diagram","text":"<pre><code>flowchart TB\n    subgraph Sources[\"Source Data\"]\n        S1[customers.csv]\n        S2[products.csv]\n        S3[orders.csv]\n    end\n\n    subgraph Dimensions[\"Dimension Layer\"]\n        D1[dim_customer&lt;br/&gt;SCD Type 2]\n        D2[dim_product&lt;br/&gt;SCD Type 1]\n        D3[dim_date&lt;br/&gt;Generated]\n    end\n\n    subgraph Facts[\"Fact Layer\"]\n        F1[fact_orders&lt;br/&gt;30 rows]\n    end\n\n    subgraph Aggregates[\"Aggregate Layer\"]\n        A1[agg_daily_sales&lt;br/&gt;14 rows]\n    end\n\n    S1 --&gt; D1\n    S2 --&gt; D2\n    S3 --&gt; F1\n\n    D1 --&gt; F1\n    D2 --&gt; F1\n    D3 --&gt; F1\n\n    F1 --&gt; A1\n\n    style Sources fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Dimensions fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Facts fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Aggregates fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>How to combine all patterns into a complete star schema</li> <li>The order of operations: dimensions first, then facts, then aggregates</li> <li>How to organize pipelines for maintainability</li> <li>Pipeline dependencies using <code>depends_on</code></li> <li>How to query the completed star schema for analytics</li> </ul>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#congratulations","title":"Congratulations!","text":"<p>You've built a complete dimensional data warehouse! This foundation can scale to handle millions of rows and complex business requirements.</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#next-steps","title":"Next Steps","text":"<p>Now that you have a working star schema, let's add a semantic layer to make querying even easier.</p> <p>Next: Semantic Layer Introduction</p>"},{"location":"tutorials/dimensional_modeling/06_full_star_schema/#navigation","title":"Navigation","text":"Previous Up Next Aggregation Pattern Tutorials Semantic Layer Intro"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/","title":"Semantic Layer Introduction","text":"<p>In this tutorial, you'll learn what a semantic layer is, why it's valuable, and how it sits on top of your star schema to provide a business-friendly query interface.</p> <p>What You'll Learn: - What is a semantic layer? - Why not just write SQL? - Metrics, dimensions, and materializations - When to use pipelines vs semantic layer</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#what-is-a-semantic-layer","title":"What is a Semantic Layer?","text":"<p>A semantic layer is a translation layer between your raw data and your business users. It defines business concepts (like \"revenue\" or \"active customers\") once, and lets everyone query them consistently.</p> <p>Think of it like a glossary for your data:</p> Business Term Technical Definition Revenue <code>SUM(line_total)</code> from <code>fact_orders</code> where <code>status = 'completed'</code> Order Count <code>COUNT(*)</code> from <code>fact_orders</code> Active Customer Customer with order in last 90 days North Region <code>region = 'North'</code> in <code>dim_customer</code> <p>Without a semantic layer, everyone writes their own SQL\u2014and everyone might calculate \"revenue\" differently.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#the-problem-inconsistent-definitions","title":"The Problem: Inconsistent Definitions","text":""},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#without-semantic-layer","title":"Without Semantic Layer","text":"<p>Marketing team: <pre><code>-- \"Revenue\" = all orders\nSELECT SUM(line_total) AS revenue FROM fact_orders;\n-- Result: $9,803.54\n</code></pre></p> <p>Finance team: <pre><code>-- \"Revenue\" = only completed orders\nSELECT SUM(line_total) AS revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Result: $8,953.56\n</code></pre></p> <p>Executive dashboard: <pre><code>-- \"Revenue\" = completed orders, excluding discounts\nSELECT SUM(line_total * 0.95) AS revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Result: $8,505.88\n</code></pre></p> <p>Result: Three different \"revenue\" numbers in the same meeting. Chaos.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#with-semantic-layer","title":"With Semantic Layer","text":"<pre><code># Everyone uses the same metric definition\nresult = query.execute(\"revenue\", context)\n# Result: $8,953.56 (always)\n</code></pre> <p>The semantic layer enforces a single, governed definition of \"revenue.\"</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#why-not-just-write-sql","title":"Why Not Just Write SQL?","text":"<p>SQL is powerful, but it has limitations for business users:</p> Challenge Without Semantic Layer With Semantic Layer Complex joins Users must know table relationships Automatic join handling Consistent definitions Everyone writes their own Define once, use everywhere Filter logic Repeated in every query Embedded in metric definition Aggregation errors Easy to make mistakes Pre-validated expressions Self-service Requires SQL expertise Business-friendly syntax Governance No central control Single source of truth"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#how-the-semantic-layer-fits","title":"How the Semantic Layer Fits","text":"<p>The semantic layer sits on top of your star schema:</p> <pre><code>flowchart TB\n    subgraph Sources[\"Source Systems\"]\n        S1[CRM]\n        S2[ERP]\n        S3[POS]\n    end\n\n    subgraph Pipeline[\"Odibi Pipelines (YAML)\"]\n        P1[dimension pattern]\n        P2[fact pattern]\n        P3[aggregation pattern]\n    end\n\n    subgraph StarSchema[\"Star Schema\"]\n        D1[(dim_customer)]\n        D2[(dim_product)]\n        D3[(dim_date)]\n        F1[(fact_orders)]\n        A1[(agg_daily_sales)]\n    end\n\n    subgraph Semantic[\"Semantic Layer (Python API)\"]\n        M1[Metrics]\n        M2[Dimensions]\n        M3[Materializations]\n    end\n\n    subgraph Consumers[\"Consumers\"]\n        C1[Dashboards]\n        C2[Reports]\n        C3[Ad-hoc Queries]\n        C4[Data Apps]\n    end\n\n    S1 --&gt; P1\n    S2 --&gt; P2\n    S3 --&gt; P3\n\n    P1 --&gt; D1\n    P1 --&gt; D2\n    P2 --&gt; D3\n    P2 --&gt; F1\n    P3 --&gt; A1\n\n    D1 --&gt; M1\n    D2 --&gt; M1\n    D3 --&gt; M1\n    F1 --&gt; M1\n    A1 --&gt; M1\n\n    M1 --&gt; C1\n    M2 --&gt; C2\n    M3 --&gt; C3\n    M1 --&gt; C4\n\n    style Sources fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Pipeline fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style StarSchema fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Semantic fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Consumers fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre> <p>Odibi Pipelines build the data (dimensions, facts, aggregates). Semantic Layer defines how to query the data (metrics, dimensions, materializations).</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#core-concepts","title":"Core Concepts","text":""},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#metrics","title":"Metrics","text":"<p>A metric is a measurable value that can be aggregated. It answers \"how much?\" or \"how many?\"</p> <pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre> <p>Examples: - Revenue (<code>SUM(line_total)</code>) - Order Count (<code>COUNT(*)</code>) - Average Order Value (<code>AVG(line_total)</code>) - Unique Customers (<code>COUNT(DISTINCT customer_sk)</code>)</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#dimensions","title":"Dimensions","text":"<p>A dimension is an attribute for grouping and filtering. It answers \"by what?\" or \"for what?\"</p> <pre><code>dimensions:\n  - name: region\n    source: dim_customer\n    column: region\n</code></pre> <p>Examples: - Region (North, South, East, West) - Category (Electronics, Furniture) - Month (January, February, ...) - Day of Week (Monday, Tuesday, ...)</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#materializations","title":"Materializations","text":"<p>A materialization pre-computes metrics at a specific grain and saves them to a table. It answers \"what should be pre-calculated for dashboards?\"</p> <pre><code>materializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue\n    schedule: \"0 2 1 * *\"  # Monthly\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#when-to-use-what","title":"When to Use What","text":"Task Solution Build dimension tables from source Odibi Pipeline: <code>dimension</code> pattern Build fact tables from source Odibi Pipeline: <code>fact</code> pattern Build scheduled aggregates Odibi Pipeline: <code>aggregation</code> pattern Ad-hoc metric queries Semantic Layer: <code>SemanticQuery</code> Self-service analytics Semantic Layer with dimensions Dashboard metrics Semantic Layer: <code>Materializer</code>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#pipelines-vs-semantic-layer","title":"Pipelines vs Semantic Layer","text":"<p>Use Pipelines when: - Building the star schema from source data - Scheduled ETL/ELT processes - Transforming and cleaning data - Generating surrogate keys</p> <p>Use Semantic Layer when: - Defining business metrics consistently - Enabling self-service analytics - Pre-computing dashboard metrics - Creating a governed metric catalog</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#unified-project-api-recommended","title":"Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is through the unified <code>Project</code> API. This connects your pipelines and semantic layer seamlessly:</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#1-add-semantic-config-to-odibiyaml","title":"1. Add Semantic Config to odibi.yaml","text":"<pre><code># odibi.yaml\nproject: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#2-query-with-two-lines-of-code","title":"2. Query with Two Lines of Code","text":"<pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>That's it! The <code>Project</code> class: - Reads connections and pipelines from your YAML - Resolves <code>$build_warehouse.fact_orders</code> \u2192 node's write path - Auto-loads Delta tables when queried - No manual <code>context.register()</code> calls needed</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#manual-approach","title":"Manual Approach","text":"<p>If you prefer more control, you can use the semantic layer components directly.</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#example-revenue-metric","title":"Example: Revenue Metric","text":"<p>Let's see how a simple metric works:</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#1-define-the-metric","title":"1. Define the Metric","text":"<pre><code>from odibi.semantics import MetricDefinition\n\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from completed orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#2-create-the-config","title":"2. Create the Config","text":"<pre><code>from odibi.semantics import SemanticLayerConfig, DimensionDefinition\n\nconfig = SemanticLayerConfig(\n    metrics=[revenue],\n    dimensions=[\n        DimensionDefinition(\n            name=\"region\",\n            source=\"dim_customer\",\n            column=\"region\"\n        )\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#3-query-the-metric","title":"3. Query the Metric","text":"<pre><code>from odibi.semantics import SemanticQuery\n\nquery = SemanticQuery(config)\n\n# Total revenue\nresult = query.execute(\"revenue\", context)\nprint(result.df)\n# | revenue    |\n# |------------|\n# | 8,953.56   |\n\n# Revenue by region\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df)\n# | region | revenue  |\n# |--------|----------|\n# | North  | 2,549.88 |\n# | South  | 2,349.93 |\n# | East   | 1,923.88 |\n# | West   | 2,129.87 |\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#the-query-syntax","title":"The Query Syntax","text":"<p>Semantic queries use a simple, business-friendly syntax:</p> <pre><code>metric1, metric2 BY dimension1, dimension2 WHERE condition\n</code></pre> <p>Examples:</p> Query Meaning <code>\"revenue\"</code> Total revenue <code>\"revenue BY region\"</code> Revenue grouped by region <code>\"revenue, order_count BY region\"</code> Multiple metrics by region <code>\"revenue BY region, month\"</code> Revenue by region and month <code>\"revenue BY region WHERE year = 2024\"</code> Filtered revenue by region"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#benefits-summary","title":"Benefits Summary","text":"Benefit Description Consistency One definition of \"revenue\" everywhere Governance Central control over metric logic Self-Service Business users query without SQL Performance Pre-computed materializations for dashboards Discoverability Metrics are documented and cataloged Maintainability Change definition once, updates everywhere"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#architecture-overview","title":"Architecture Overview","text":"<pre><code>classDiagram\n    class SemanticLayerConfig {\n        +metrics: List[MetricDefinition]\n        +dimensions: List[DimensionDefinition]\n        +materializations: List[MaterializationConfig]\n        +get_metric(name)\n        +get_dimension(name)\n    }\n\n    class MetricDefinition {\n        +name: str\n        +description: str\n        +expr: str\n        +source: str\n        +filters: List[str]\n    }\n\n    class DimensionDefinition {\n        +name: str\n        +source: str\n        +column: str\n        +hierarchy: List[str]\n    }\n\n    class SemanticQuery {\n        +config: SemanticLayerConfig\n        +execute(query_string, context)\n        +parse(query_string)\n        +validate(parsed_query)\n    }\n\n    class Materializer {\n        +config: SemanticLayerConfig\n        +execute(name, context)\n        +execute_all(context)\n    }\n\n    SemanticLayerConfig --&gt; MetricDefinition\n    SemanticLayerConfig --&gt; DimensionDefinition\n    SemanticQuery --&gt; SemanticLayerConfig\n    Materializer --&gt; SemanticLayerConfig\n</code></pre>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>A semantic layer translates business concepts into technical queries</li> <li>It ensures consistent definitions across all users</li> <li>Metrics are measurable values (SUM, COUNT, AVG)</li> <li>Dimensions are grouping attributes (region, category, date)</li> <li>Materializations pre-compute metrics for performance</li> <li>Pipelines build the data; semantic layer queries it</li> <li>The query syntax is simple: <code>\"metric BY dimension WHERE filter\"</code></li> </ul>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to define metrics in detail.</p> <p>Next: Defining Metrics</p>"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#navigation","title":"Navigation","text":"Previous Up Next Full Star Schema Tutorials Defining Metrics"},{"location":"tutorials/dimensional_modeling/07_semantic_layer_intro/#reference","title":"Reference","text":"<p>For complete documentation, see: Semantic Layer Overview</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/","title":"Defining Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to define metrics in the Odibi semantic layer, from simple aggregations to complex filtered and derived metrics.</p> <p>What You'll Learn: - Simple metrics (SUM, COUNT, AVG) - Filtered metrics (with WHERE conditions) - Multiple metrics together - Derived metrics (calculated from other metrics)</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#source-data-fact_orders","title":"Source Data: fact_orders","text":"<p>We'll use the fact_orders table from Tutorial 06 (15 sample rows shown):</p> order_id customer_sk product_sk date_sk quantity unit_price line_total status ORD001 1 1 20240115 1 1299.99 1299.99 completed ORD002 1 2 20240115 2 29.99 59.98 completed ORD003 2 3 20240116 1 249.99 249.99 completed ORD004 3 4 20240116 3 49.99 149.97 completed ORD005 4 5 20240117 1 599.99 599.99 completed ORD006 5 6 20240117 1 149.99 149.99 completed ORD007 6 7 20240118 2 399.99 799.98 completed ORD008 7 8 20240118 4 45.99 183.96 completed ORD009 8 9 20240119 1 79.99 79.99 completed ORD010 9 10 20240119 1 189.99 189.99 completed ORD015 2 5 20240122 1 599.99 599.99 pending ORD023 10 3 20240126 1 249.99 249.99 cancelled ... ... ... ... ... ... ... ... <p>Total: 30 rows - 27 completed - 1 pending - 2 cancelled</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-1-define-a-simple-metric","title":"Step 1: Define a Simple Metric","text":"<p>Let's start with the most basic metric: total revenue.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import MetricDefinition\n\n# Define a simple revenue metric\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from all orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from all orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"revenue\"</code> How you'll reference this metric in queries <code>label</code> <code>\"Total Revenue\"</code> Display name for column alias in generated views (optional, defaults to name) <code>description</code> <code>\"Total revenue...\"</code> Human-readable documentation <code>expr</code> <code>\"SUM(line_total)\"</code> SQL aggregation expression <code>source</code> <code>\"fact_orders\"</code> Table to aggregate from"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#source-notation","title":"Source Notation","text":"<p>The <code>source</code> field supports three formats:</p> Format Example When to Use $pipeline.node <code>$build_warehouse.fact_orders</code> With <code>Project</code> API (recommended) connection.path <code>gold.fact_orders</code> External tables not in pipelines bare name <code>fact_orders</code> Manual setup with <code>context.register()</code> <p>Note: This tutorial shows both the <code>Project</code> API (recommended) and manual setup approaches. The manual sections use bare names like <code>source: fact_orders</code> because they match what you register with <code>context.register(\"fact_orders\", df)</code>.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#option-1-node-reference-recommended","title":"Option 1: Node Reference (Recommended)","text":"<p>Reference the pipeline node that produces the table. The semantic layer automatically reads from wherever that node writes:</p> <pre><code># odibi.yaml\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References the node above\n</code></pre> <p>This approach: - DRY - No duplication; the node already knows its write location - Auto-synced - If you change the node's write config, the semantic layer follows - Uses the same <code>$pipeline.node</code> pattern as cross-pipeline <code>inputs</code></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#option-2-connectionpath-explicit","title":"Option 2: Connection.Path (Explicit)","text":"<p>For tables that exist outside pipelines or when you want explicit control:</p> <pre><code>semantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: gold.fact_orders    # Resolves to /mnt/data/gold/fact_orders\n</code></pre> <p>Nested paths are supported. The split happens on the first dot only, so everything after becomes the path:</p> <pre><code># Given connection:\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\n# These all work:\nsource: gold.fact_orders              # \u2192 /mnt/data/gold/fact_orders\nsource: gold.sales/store_a/metrics    # \u2192 /mnt/data/gold/sales/store_a/metrics\nsource: gold.domain/v2/fact_sales     # \u2192 /mnt/data/gold/domain/v2/fact_sales\n</code></pre> <p>For Unity Catalog connections with <code>catalog</code> + <code>schema_name</code>:</p> <pre><code>connections:\n  gold:\n    type: delta\n    catalog: main\n    schema_name: gold_db\n\nsource: gold.fact_orders    # \u2192 main.gold_db.fact_orders\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-result","title":"Query Result","text":"<pre><code>result = query.execute(\"revenue\", context)\n</code></pre> revenue 9,803.54 <p>This includes ALL orders (completed, pending, cancelled).</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-2-define-a-filtered-metric","title":"Step 2: Define a Filtered Metric","text":"<p>Usually, you only want \"revenue\" to include completed orders. Add a filter:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_1","title":"Python Code","text":"<pre><code>completed_revenue = MetricDefinition(\n    name=\"completed_revenue\",\n    description=\"Revenue from completed orders only\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_1","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: completed_revenue\n    description: \"Revenue from completed orders only\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#how-filters-work","title":"How Filters Work","text":"<p>Without filter (revenue): <pre><code>SELECT SUM(line_total) AS revenue FROM fact_orders;\n-- Uses all 30 rows\n</code></pre></p> <p>With filter (completed_revenue): <pre><code>SELECT SUM(line_total) AS completed_revenue \nFROM fact_orders \nWHERE status = 'completed';\n-- Uses only 27 completed rows\n</code></pre></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#data-comparison","title":"Data Comparison","text":"<p>All orders (30 rows) - includes:</p> order_id line_total status ORD001 1299.99 completed ORD002 59.98 completed ... ... ... ORD015 599.99 pending (excluded) ORD023 249.99 cancelled (excluded) ... ... ... <p>Completed only (27 rows):</p> order_id line_total status ORD001 1299.99 completed ORD002 59.98 completed ... ... ..."},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-results","title":"Query Results","text":"Metric Value Rows Included revenue 9,803.54 30 (all) completed_revenue 8,953.56 27 (completed only)"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-3-define-multiple-metrics","title":"Step 3: Define Multiple Metrics","text":"<p>Let's define a complete set of metrics:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_2","title":"Python Code","text":"<pre><code>from odibi.semantics import MetricDefinition, SemanticLayerConfig\n\n# Revenue metrics\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    description=\"Total revenue from completed orders\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Count metrics\norder_count = MetricDefinition(\n    name=\"order_count\",\n    description=\"Number of orders\",\n    expr=\"COUNT(*)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\nunique_customers = MetricDefinition(\n    name=\"unique_customers\",\n    description=\"Number of unique customers\",\n    expr=\"COUNT(DISTINCT customer_sk)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Average metrics\navg_order_value = MetricDefinition(\n    name=\"avg_order_value\",\n    description=\"Average order value\",\n    expr=\"AVG(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Volume metrics\ntotal_quantity = MetricDefinition(\n    name=\"total_quantity\",\n    description=\"Total units sold\",\n    expr=\"SUM(quantity)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Combine into config\nconfig = SemanticLayerConfig(\n    metrics=[\n        revenue,\n        order_count,\n        unique_customers,\n        avg_order_value,\n        total_quantity\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_2","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: order_count\n    description: \"Number of orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#query-results_1","title":"Query Results","text":"<p>Single metric: <pre><code>result = query.execute(\"revenue\", context)\n</code></pre></p> revenue 8,953.56 <p>Multiple metrics: <pre><code>result = query.execute(\"revenue, order_count, avg_order_value\", context)\n</code></pre></p> revenue order_count avg_order_value 8,953.56 27 331.61"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#step-4-define-a-derived-metric","title":"Step 4: Define a Derived Metric","text":"<p>A derived metric is calculated from other metrics. It doesn't aggregate directly from the source.</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#scenario-profit-margin","title":"Scenario: Profit Margin","text":"<p>We want to calculate profit margin, which requires cost data. Let's assume we've added a cost column:</p> <p>fact_orders with cost (sample):</p> order_id line_total cost_total ORD001 1299.99 850.00 ORD002 59.98 24.00 ORD003 249.99 120.00"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_3","title":"Python Code","text":"<pre><code># Base metrics\nrevenue = MetricDefinition(\n    name=\"revenue\",\n    expr=\"SUM(line_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\ntotal_cost = MetricDefinition(\n    name=\"total_cost\",\n    expr=\"SUM(cost_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\nprofit = MetricDefinition(\n    name=\"profit\",\n    expr=\"SUM(line_total) - SUM(cost_total)\",\n    source=\"fact_orders\",\n    filters=[\"status = 'completed'\"]\n)\n\n# Derived metric (calculated from other metrics)\nprofit_margin = MetricDefinition(\n    name=\"profit_margin\",\n    description=\"Profit as percentage of revenue\",\n    expr=\"(revenue - total_cost) / revenue\",\n    type=\"derived\"  # Indicates this references other metrics\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_3","title":"YAML Alternative","text":"<pre><code>metrics:\n  - name: revenue\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: total_cost\n    expr: \"SUM(cost_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit\n    expr: \"SUM(line_total) - SUM(cost_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit_margin\n    description: \"Profit as percentage of revenue\"\n    expr: \"(revenue - total_cost) / revenue\"\n    type: derived\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#complete-semanticlayerconfig","title":"Complete SemanticLayerConfig","text":"<p>Here's the complete configuration with all our metrics:</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#python-code_4","title":"Python Code","text":"<pre><code>from odibi.semantics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig\n)\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        # Revenue metrics\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"pending_revenue\",\n            description=\"Revenue from pending orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'pending'\"]\n        ),\n\n        # Count metrics\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"unique_customers\",\n            description=\"Number of unique customers\",\n            expr=\"COUNT(DISTINCT customer_sk)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n\n        # Average metrics\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n\n        # Volume metrics\n        MetricDefinition(\n            name=\"total_quantity\",\n            description=\"Total units sold\",\n            expr=\"SUM(quantity)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[]  # We'll add these in the next tutorial\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#yaml-alternative_4","title":"YAML Alternative","text":"<pre><code># File: semantic_config.yaml\nmetrics:\n  # Revenue metrics\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: pending_revenue\n    description: \"Revenue from pending orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'pending'\"\n\n  # Count metrics\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Average metrics\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Volume metrics\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\ndimensions: []  # Added in next tutorial\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#available-aggregation-functions","title":"Available Aggregation Functions","text":"Function Description Example <code>SUM(column)</code> Sum of values <code>SUM(line_total)</code> <code>COUNT(*)</code> Row count <code>COUNT(*)</code> <code>COUNT(column)</code> Non-null count <code>COUNT(customer_sk)</code> <code>COUNT(DISTINCT column)</code> Unique count <code>COUNT(DISTINCT customer_sk)</code> <code>AVG(column)</code> Average <code>AVG(line_total)</code> <code>MIN(column)</code> Minimum <code>MIN(line_total)</code> <code>MAX(column)</code> Maximum <code>MAX(line_total)</code>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#complex-expressions","title":"Complex Expressions","text":"<pre><code># Percentage of total (within group)\n- name: revenue_share\n  expr: \"SUM(line_total) / SUM(SUM(line_total)) OVER ()\"\n\n# Conditional sum\n- name: high_value_revenue\n  expr: \"SUM(CASE WHEN line_total &gt; 500 THEN line_total ELSE 0 END)\"\n\n# Ratio\n- name: items_per_order\n  expr: \"SUM(quantity) / COUNT(*)\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#metric-naming-best-practices","title":"Metric Naming Best Practices","text":"Do Don't <code>revenue</code> <code>rev</code> <code>order_count</code> <code>cnt</code> <code>completed_revenue</code> <code>rev_comp</code> <code>avg_order_value</code> <code>aov</code> (unless standard) <code>unique_customers</code> <code>cust_distinct</code> <p>Guidelines: - Use <code>snake_case</code> - Be descriptive: <code>completed_order_revenue</code> over <code>rev1</code> - Prefix related metrics: <code>revenue</code>, <code>revenue_completed</code>, <code>revenue_pending</code> - Include the filter in the name: <code>last_30_days_revenue</code></p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#display-labels","title":"Display Labels","text":"<p>Use the <code>label</code> field to specify user-friendly column names in generated views:</p> <pre><code>metrics:\n  - name: total_calendar_hours\n    label: \"Total Calendar Hours\"    # Appears as column alias in views\n    expr: \"SUM([Total Calendar Hour])\"\n    source: oee.oee_fact\n</code></pre> <p>This generates SQL like: <pre><code>SELECT SUM([Total Calendar Hour]) AS [Total Calendar Hours]\n</code></pre></p> <p>Instead of: <pre><code>SELECT SUM([Total Calendar Hour]) AS total_calendar_hours\n</code></pre></p> <p>When to use labels: - When source columns have spaces or special characters - When you want human-readable column names in BI tools - When the internal <code>name</code> differs from the desired display name</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple metrics aggregate directly from source: <code>SUM(line_total)</code></li> <li>Filters constrain which rows are included: <code>status = 'completed'</code></li> <li>Multiple metrics can be defined and queried together</li> <li>Derived metrics calculate from other metrics: <code>revenue - cost</code></li> <li>The expr field uses SQL aggregation syntax</li> <li>The source field specifies which table to query</li> <li>Naming conventions make metrics discoverable</li> </ul>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to define dimensions for grouping and filtering.</p> <p>Next: Defining Dimensions</p>"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#navigation","title":"Navigation","text":"Previous Up Next Semantic Layer Intro Tutorials Defining Dimensions"},{"location":"tutorials/dimensional_modeling/08_defining_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Defining Metrics Reference</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/","title":"Defining Dimensions Tutorial","text":"<p>In this tutorial, you'll learn how to define semantic layer dimensions for grouping and filtering metrics. Dimensions are the \"BY\" part of queries like <code>\"revenue BY region\"</code>.</p> <p>What You'll Learn: - Simple dimensions (single column) - Dimensions with different column names - Hierarchical dimensions (drill-down) - Complete config with metrics AND dimensions</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#star-schema-data","title":"Star Schema Data","text":"<p>We'll use the star schema from Tutorial 06:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_customer-sample","title":"dim_customer (sample)","text":"customer_sk customer_id name region city state 1 C001 Alice Johnson North Chicago IL 2 C002 Bob Smith South Houston TX 3 C003 Carol White North Detroit MI 4 C004 David Brown East New York NY 5 C005 Emma Davis West Seattle WA 6 C006 Frank Miller South Miami FL 7 C007 Grace Lee East Boston MA 8 C008 Henry Wilson West Portland OR 9 C009 Ivy Chen North Minneapolis MN 10 C010 Jack Taylor South Dallas TX 11 C011 Karen Martinez East Philadelphia PA 12 C012 Leo Anderson West Denver CO"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_product-sample","title":"dim_product (sample)","text":"product_sk product_id name category subcategory 1 P001 Laptop Pro 15 Electronics Computers 2 P002 Wireless Mouse Electronics Accessories 3 P003 Office Chair Furniture Seating 4 P004 USB-C Hub Electronics Accessories 5 P005 Standing Desk Furniture Desks 6 P006 Mechanical Keyboard Electronics Accessories 7 P007 Monitor 27\" Electronics Displays 8 P008 Desk Lamp Furniture Lighting 9 P009 Webcam HD Electronics Accessories 10 P010 Filing Cabinet Furniture Storage"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#dim_date-sample","title":"dim_date (sample)","text":"date_sk full_date day_of_week month month_name quarter_name year 20240115 2024-01-15 Monday 1 January Q1 2024 20240116 2024-01-16 Tuesday 1 January Q1 2024 20240117 2024-01-17 Wednesday 1 January Q1 2024 20240118 2024-01-18 Thursday 1 January Q1 2024 20240119 2024-01-19 Friday 1 January Q1 2024 20240120 2024-01-20 Saturday 1 January Q1 2024 20240121 2024-01-21 Sunday 1 January Q1 2024 20240122 2024-01-22 Monday 1 January Q1 2024 20240123 2024-01-23 Tuesday 1 January Q1 2024 20240124 2024-01-24 Wednesday 1 January Q1 2024"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#unified-project-api-recommended","title":"Unified Project API (Recommended)","text":"<p>When using the unified <code>Project</code> API, dimensions can reference pipeline nodes directly using the <code>$pipeline.node</code> notation:</p> <pre><code># odibi.yaml\nproject: my_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: dim_customer\n        write:\n          connection: gold\n          table: dim_customer\n      - name: dim_product\n        write:\n          connection: gold\n          table: dim_product\n      - name: fact_orders\n        write:\n          connection: gold\n          table: fact_orders\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # References node's write target\n      column: region\n\n    - name: category\n      source: $build_warehouse.dim_product    # No path duplication!\n      column: category\n</code></pre> <p>The <code>source: $build_warehouse.dim_customer</code> notation: 1. Looks up the <code>dim_customer</code> node in the <code>build_warehouse</code> pipeline 2. Reads its <code>write.connection</code> and <code>write.table</code> config 3. Resolves the full path automatically</p> <p>Alternative: You can also use <code>source: gold.dim_customer</code> (connection.table) for tables not managed by pipelines.</p> <pre><code>from odibi import Project\n\nproject = Project.load(\"odibi.yaml\")\nresult = project.query(\"revenue BY region, category\")  # Tables auto-loaded\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-1-define-a-simple-dimension","title":"Step 1: Define a Simple Dimension","text":"<p>The simplest dimension maps a column directly:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import DimensionDefinition\n\n# Simple dimension: region from dim_customer\nregion = DimensionDefinition(\n    name=\"region\",\n    source=\"dim_customer\",\n    column=\"region\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: region\n    source: dim_customer\n    column: region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"region\"</code> How you reference it in queries <code>label</code> <code>\"Sales Region\"</code> Display name for column alias in generated views (optional, defaults to name) <code>source</code> <code>\"dim_customer\"</code> Table containing the dimension <code>column</code> <code>\"region\"</code> Column to GROUP BY"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#query-example","title":"Query Example","text":"<pre><code>result = query.execute(\"revenue BY region\", context)\n</code></pre> <p>Result (4 rows):</p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-2-dimension-with-different-column-name","title":"Step 2: Dimension with Different Column Name","text":"<p>Sometimes you want the dimension name to differ from the column name:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_1","title":"Python Code","text":"<pre><code># Dimension name differs from column name\ncustomer_city = DimensionDefinition(\n    name=\"customer_city\",      # Query uses \"customer_city\"\n    source=\"dim_customer\",\n    column=\"city\"              # Actual column is \"city\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_1","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: customer_city\n    source: dim_customer\n    column: city\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#query-example_1","title":"Query Example","text":"<pre><code>result = query.execute(\"revenue BY customer_city\", context)\n</code></pre> <p>Result (12 rows):</p> customer_city revenue Chicago 1,559.95 Houston 1,049.97 Detroit 541.93 New York 1,079.97 Seattle 477.97 Miami 959.95 Boston 573.94 Portland 1,379.98 Minneapolis 469.95 Dallas 1,549.98 Philadelphia 249.92 Denver 1,309.96"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-3-dimension-with-hierarchy","title":"Step 3: Dimension with Hierarchy","text":"<p>A hierarchy defines drill-down levels. Users can start at a high level (year) and drill into details (month, week, day).</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#the-drill-down-concept","title":"The Drill-Down Concept","text":"<pre><code>Year (2024)\n  \u2514\u2500\u2500 Quarter (Q1)\n        \u2514\u2500\u2500 Month (January)\n              \u2514\u2500\u2500 Week (Week 3)\n                    \u2514\u2500\u2500 Day (Jan 15)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_2","title":"Python Code","text":"<pre><code># Date dimension with hierarchy\norder_date = DimensionDefinition(\n    name=\"order_date\",\n    source=\"dim_date\",\n    column=\"full_date\",\n    hierarchy=[\"year\", \"quarter_name\", \"month_name\", \"full_date\"]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_2","title":"YAML Alternative","text":"<pre><code>dimensions:\n  - name: order_date\n    source: dim_date\n    column: full_date\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n      - full_date\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#using-hierarchy-levels-in-queries","title":"Using Hierarchy Levels in Queries","text":"<p>Top level - Year: <pre><code>result = query.execute(\"revenue BY year\", context)\n</code></pre></p> year revenue 2024 8,953.56 <p>Drill down - Quarter: <pre><code>result = query.execute(\"revenue BY quarter_name\", context)\n</code></pre></p> quarter_name revenue Q1 8,953.56 <p>Drill down - Month: <pre><code>result = query.execute(\"revenue BY month_name\", context)\n</code></pre></p> month_name revenue January 8,953.56 <p>Drill down - Day: <pre><code>result = query.execute(\"revenue BY full_date\", context)\n</code></pre></p> full_date revenue 2024-01-15 1,439.96 2024-01-16 589.95 2024-01-17 749.98 ... ..."},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#step-4-define-all-dimensions-for-our-star-schema","title":"Step 4: Define All Dimensions for Our Star Schema","text":"<p>Let's define a complete set of dimensions:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_3","title":"Python Code","text":"<pre><code>from odibi.semantics import DimensionDefinition\n\ndimensions = [\n    # Geographic dimensions (from dim_customer)\n    DimensionDefinition(\n        name=\"region\",\n        source=\"dim_customer\",\n        column=\"region\",\n        description=\"Customer geographic region\"\n    ),\n    DimensionDefinition(\n        name=\"city\",\n        source=\"dim_customer\",\n        column=\"city\"\n    ),\n    DimensionDefinition(\n        name=\"state\",\n        source=\"dim_customer\",\n        column=\"state\"\n    ),\n\n    # Product dimensions (from dim_product)\n    DimensionDefinition(\n        name=\"category\",\n        source=\"dim_product\",\n        column=\"category\",\n        description=\"Product category\"\n    ),\n    DimensionDefinition(\n        name=\"subcategory\",\n        source=\"dim_product\",\n        column=\"subcategory\"\n    ),\n    DimensionDefinition(\n        name=\"product_name\",\n        source=\"dim_product\",\n        column=\"name\",\n        hierarchy=[\"category\", \"subcategory\", \"name\"]\n    ),\n\n    # Time dimensions (from dim_date)\n    DimensionDefinition(\n        name=\"year\",\n        source=\"dim_date\",\n        column=\"year\"\n    ),\n    DimensionDefinition(\n        name=\"quarter\",\n        source=\"dim_date\",\n        column=\"quarter_name\"\n    ),\n    DimensionDefinition(\n        name=\"month\",\n        source=\"dim_date\",\n        column=\"month_name\",\n        hierarchy=[\"year\", \"quarter_name\", \"month_name\"]\n    ),\n    DimensionDefinition(\n        name=\"day_of_week\",\n        source=\"dim_date\",\n        column=\"day_of_week\"\n    ),\n\n    # Order dimensions (from fact_orders)\n    DimensionDefinition(\n        name=\"status\",\n        source=\"fact_orders\",\n        column=\"status\"\n    )\n]\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative_3","title":"YAML Alternative","text":"<pre><code>dimensions:\n  # Geographic dimensions\n  - name: region\n    source: dim_customer\n    column: region\n    description: \"Customer geographic region\"\n\n  - name: city\n    source: dim_customer\n    column: city\n\n  - name: state\n    source: dim_customer\n    column: state\n\n  # Product dimensions\n  - name: category\n    source: dim_product\n    column: category\n    description: \"Product category\"\n\n  - name: subcategory\n    source: dim_product\n    column: subcategory\n\n  - name: product_name\n    source: dim_product\n    column: name\n    hierarchy:\n      - category\n      - subcategory\n      - name\n\n  # Time dimensions\n  - name: year\n    source: dim_date\n    column: year\n\n  - name: quarter\n    source: dim_date\n    column: quarter_name\n\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n\n  - name: day_of_week\n    source: dim_date\n    column: day_of_week\n\n  # Order dimensions\n  - name: status\n    source: fact_orders\n    column: status\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#complete-config-with-metrics-and-dimensions","title":"Complete Config with Metrics AND Dimensions","text":"<p>Here's the full SemanticLayerConfig:</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#python-code_4","title":"Python Code","text":"<pre><code>from odibi.semantics import (\n    MetricDefinition,\n    DimensionDefinition,\n    SemanticLayerConfig\n)\n\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"unique_customers\",\n            description=\"Number of unique customers\",\n            expr=\"COUNT(DISTINCT customer_sk)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"total_quantity\",\n            description=\"Total units sold\",\n            expr=\"SUM(quantity)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[\n        # Geographic\n        DimensionDefinition(name=\"region\", source=\"dim_customer\", column=\"region\"),\n        DimensionDefinition(name=\"city\", source=\"dim_customer\", column=\"city\"),\n        DimensionDefinition(name=\"state\", source=\"dim_customer\", column=\"state\"),\n\n        # Product\n        DimensionDefinition(name=\"category\", source=\"dim_product\", column=\"category\"),\n        DimensionDefinition(name=\"subcategory\", source=\"dim_product\", column=\"subcategory\"),\n        DimensionDefinition(\n            name=\"product_name\", \n            source=\"dim_product\", \n            column=\"name\",\n            hierarchy=[\"category\", \"subcategory\", \"name\"]\n        ),\n\n        # Time\n        DimensionDefinition(name=\"year\", source=\"dim_date\", column=\"year\"),\n        DimensionDefinition(name=\"quarter\", source=\"dim_date\", column=\"quarter_name\"),\n        DimensionDefinition(\n            name=\"month\", \n            source=\"dim_date\", \n            column=\"month_name\",\n            hierarchy=[\"year\", \"quarter_name\", \"month_name\"]\n        ),\n        DimensionDefinition(name=\"day_of_week\", source=\"dim_date\", column=\"day_of_week\"),\n\n        # Order\n        DimensionDefinition(name=\"status\", source=\"fact_orders\", column=\"status\")\n    ]\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#yaml-alternative-semantic_configyaml","title":"YAML Alternative (semantic_config.yaml)","text":"<pre><code># File: semantic_config.yaml\nmetrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: unique_customers\n    description: \"Number of unique customers\"\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: total_quantity\n    description: \"Total units sold\"\n    expr: \"SUM(quantity)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\ndimensions:\n  # Geographic\n  - name: region\n    source: dim_customer\n    column: region\n  - name: city\n    source: dim_customer\n    column: city\n  - name: state\n    source: dim_customer\n    column: state\n\n  # Product\n  - name: category\n    source: dim_product\n    column: category\n  - name: subcategory\n    source: dim_product\n    column: subcategory\n  - name: product_name\n    source: dim_product\n    column: name\n    hierarchy: [category, subcategory, name]\n\n  # Time\n  - name: year\n    source: dim_date\n    column: year\n  - name: quarter\n    source: dim_date\n    column: quarter_name\n  - name: month\n    source: dim_date\n    column: month_name\n    hierarchy: [year, quarter_name, month_name]\n  - name: day_of_week\n    source: dim_date\n    column: day_of_week\n\n  # Order\n  - name: status\n    source: fact_orders\n    column: status\n</code></pre>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#example-queries-with-dimensions","title":"Example Queries with Dimensions","text":"<p>Now you can run rich queries:</p> <p>Revenue by region: <pre><code>result = query.execute(\"revenue BY region\", context)\n</code></pre></p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87 <p>Revenue by category and region: <pre><code>result = query.execute(\"revenue BY category, region\", context)\n</code></pre></p> category region revenue Electronics North 1,549.94 Electronics South 1,449.95 Electronics East 1,323.91 Electronics West 1,079.93 Furniture North 999.94 Furniture South 899.98 Furniture East 599.97 Furniture West 1,049.94 <p>Multiple metrics by day of week: <pre><code>result = query.execute(\"revenue, order_count, avg_order_value BY day_of_week\", context)\n</code></pre></p> day_of_week revenue order_count avg_order_value Monday 2,189.94 5 437.99 Tuesday 1,177.93 5 235.59 Wednesday 1,099.95 4 275.00 Thursday 2,373.90 4 593.48 Friday 619.96 4 155.00 Saturday 1,549.94 3 516.65 Sunday 941.94 2 470.97"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple dimensions map a column for grouping: <code>region</code>, <code>category</code></li> <li>Column renaming lets dimension names differ from columns: <code>customer_city</code> \u2192 <code>city</code></li> <li>Hierarchies define drill-down paths: <code>year &gt; quarter &gt; month</code></li> <li>Dimensions can come from dimension tables or fact tables</li> <li>Complete config includes both metrics and dimensions</li> <li>Queries use dimensions with <code>BY</code>: <code>\"revenue BY region, category\"</code></li> </ul>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to execute queries against our semantic layer.</p> <p>Next: Querying Metrics</p>"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#navigation","title":"Navigation","text":"Previous Up Next Defining Metrics Tutorials Querying Metrics"},{"location":"tutorials/dimensional_modeling/09_defining_dimensions/#reference","title":"Reference","text":"<p>For complete documentation, see: Defining Metrics Reference</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/","title":"Querying Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to execute queries against the semantic layer using both the unified <code>Project</code> API and the <code>SemanticQuery</code> interface.</p> <p>What You'll Learn: - Unified Project API (recommended) - simplest approach - Simple queries (total, no grouping) - Queries with one dimension - Queries with multiple dimensions - Filtered queries - Multiple metrics together</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to query metrics is through the <code>Project</code> class:</p> <pre><code>from odibi import Project\n\n# Load project - semantic layer inherits connections\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics - tables auto-loaded from connections\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n</code></pre> <p>The Project API: - Reads connections from your <code>odibi.yaml</code> - Resolves <code>source: $pipeline.node</code> or <code>connection.path</code> to full paths - Auto-loads Delta tables when queried - No manual <code>context.register()</code> calls needed</p> <p>Example queries: <pre><code># Total revenue\nresult = project.query(\"revenue\")\n\n# Revenue by region\nresult = project.query(\"revenue BY region\")\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count BY region, month\")\n\n# With filter\nresult = project.query(\"revenue BY category WHERE region = 'North'\")\n</code></pre></p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#option-b-manual-semanticquery-interface","title":"Option B: Manual SemanticQuery Interface","text":"<p>For more control, use the <code>SemanticQuery</code> class directly.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#source-data","title":"Source Data","text":"<p>We'll use the star schema and semantic config from previous tutorials:</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#fact_orders-20-sample-rows","title":"fact_orders (20 sample rows)","text":"order_id customer_sk product_sk date_sk quantity line_total status ORD001 1 1 20240115 1 1299.99 completed ORD002 1 2 20240115 2 59.98 completed ORD003 2 3 20240116 1 249.99 completed ORD004 3 4 20240116 3 149.97 completed ORD005 4 5 20240117 1 599.99 completed ORD006 5 6 20240117 1 149.99 completed ORD007 6 7 20240118 2 799.98 completed ORD008 7 8 20240118 4 183.96 completed ORD009 8 9 20240119 1 79.99 completed ORD010 9 10 20240119 1 189.99 completed ORD011 10 1 20240120 1 1299.99 completed ORD012 11 2 20240120 5 149.95 completed ORD013 12 3 20240121 2 499.98 completed ORD014 1 4 20240121 1 49.99 completed ORD015 2 5 20240122 1 599.99 pending ORD016 3 6 20240122 2 299.98 completed ORD017 4 7 20240123 1 399.99 completed ORD018 5 8 20240123 3 137.97 completed ORD019 6 9 20240124 2 159.98 completed ORD020 7 10 20240124 1 189.99 completed"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#dim_customer-12-rows","title":"dim_customer (12 rows)","text":"customer_sk name region city 1 Alice Johnson North Chicago 2 Bob Smith South Houston 3 Carol White North Detroit 4 David Brown East New York 5 Emma Davis West Seattle 6 Frank Miller South Miami 7 Grace Lee East Boston 8 Henry Wilson West Portland 9 Ivy Chen North Minneapolis 10 Jack Taylor South Dallas 11 Karen Martinez East Philadelphia 12 Leo Anderson West Denver"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#dim_product-10-rows","title":"dim_product (10 rows)","text":"product_sk name category 1 Laptop Pro 15 Electronics 2 Wireless Mouse Electronics 3 Office Chair Furniture 4 USB-C Hub Electronics 5 Standing Desk Furniture 6 Mechanical Keyboard Electronics 7 Monitor 27\" Electronics 8 Desk Lamp Furniture 9 Webcam HD Electronics 10 Filing Cabinet Furniture"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-1-simple-query-total-no-grouping","title":"Step 1: Simple Query - Total (No Grouping)","text":"<p>The simplest query returns a single aggregated value.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query","title":"Query","text":"<pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load config\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Create query interface\nquery = SemanticQuery(config)\n\n# Setup context with data\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\ncontext.register(\"dim_product\", dim_product_df)\ncontext.register(\"dim_date\", dim_date_df)\n\n# Execute query\nresult = query.execute(\"revenue\", context)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string","title":"Query String","text":"<pre><code>\"revenue\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql","title":"Generated SQL","text":"<pre><code>SELECT SUM(line_total) AS revenue\nFROM fact_orders\nWHERE status = 'completed'\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-1-row","title":"Result (1 row)","text":"revenue 8,953.56"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#accessing-the-result","title":"Accessing the Result","text":"<pre><code>print(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\n# Output: Total Revenue: $8,953.56\n\nprint(f\"Row count: {result.row_count}\")\n# Output: Row count: 1\n\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n# Output: Execution time: 12.34ms\n\nprint(f\"Generated SQL: {result.sql_generated}\")\n# Output: SELECT SUM(line_total) AS revenue FROM fact_orders WHERE status = 'completed'\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-2-query-with-one-dimension","title":"Step 2: Query with One Dimension","text":"<p>Add a dimension to group the results.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_1","title":"Query String","text":"<pre><code>\"revenue BY region\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_1","title":"Generated SQL","text":"<pre><code>SELECT \n    c.region,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nWHERE f.status = 'completed'\nGROUP BY c.region\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue BY region\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-4-rows","title":"Result (4 rows)","text":"region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-3-query-with-multiple-dimensions","title":"Step 3: Query with Multiple Dimensions","text":"<p>Add more dimensions for a cross-tabulation.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_2","title":"Query String","text":"<pre><code>\"revenue, order_count BY region, category\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_2","title":"Generated SQL","text":"<pre><code>SELECT \n    c.region,\n    p.category,\n    SUM(f.line_total) AS revenue,\n    COUNT(*) AS order_count\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nWHERE f.status = 'completed'\nGROUP BY c.region, p.category\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_1","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue, order_count BY region, category\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-8-rows-region-category","title":"Result (8 rows - region \u00d7 category)","text":"region category revenue order_count North Electronics 1,549.94 4 North Furniture 999.94 3 South Electronics 1,449.95 4 South Furniture 899.98 3 East Electronics 1,323.91 4 East Furniture 599.97 3 West Electronics 1,079.93 3 West Furniture 1,049.94 4"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-4-query-with-filter","title":"Step 4: Query with Filter","text":"<p>Add a WHERE clause to filter results.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_3","title":"Query String","text":"<pre><code>\"revenue BY category WHERE region = 'North'\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#generated-sql_3","title":"Generated SQL","text":"<pre><code>SELECT \n    p.category,\n    SUM(f.line_total) AS revenue\nFROM fact_orders f\nJOIN dim_customer c ON f.customer_sk = c.customer_sk\nJOIN dim_product p ON f.product_sk = p.product_sk\nWHERE f.status = 'completed'\n  AND c.region = 'North'\nGROUP BY p.category\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_2","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue BY category WHERE region = 'North'\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-2-rows","title":"Result (2 rows)","text":"category revenue Electronics 1,549.94 Furniture 999.94"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#multiple-filters","title":"Multiple Filters","text":"<p>You can combine multiple filter conditions:</p> <pre><code># Multiple conditions with AND\nresult = query.execute(\n    \"revenue BY category WHERE region = 'North' AND year = 2024\",\n    context\n)\n\n# IN clause\nresult = query.execute(\n    \"revenue BY region WHERE region IN ('North', 'South')\",\n    context\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#step-5-multiple-metrics","title":"Step 5: Multiple Metrics","text":"<p>Query multiple metrics in a single call.</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-string_4","title":"Query String","text":"<pre><code>\"revenue, order_count, avg_order_value BY region\"\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#python-code_3","title":"Python Code","text":"<pre><code>result = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\nprint(result.df)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#result-4-rows_1","title":"Result (4 rows)","text":"region revenue order_count avg_order_value North 2,549.88 7 364.27 South 2,349.93 7 335.70 East 1,923.88 7 274.84 West 2,129.87 7 304.27"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#complete-python-script","title":"Complete Python Script","text":"<p>Here's a complete, runnable example:</p> <pre><code>from odibi.semantics import SemanticQuery, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\n\n# ===========================================\n# 1. Load the semantic config\n# ===========================================\nconfig_dict = {\n    \"metrics\": [\n        {\n            \"name\": \"revenue\",\n            \"description\": \"Total revenue from completed orders\",\n            \"expr\": \"SUM(line_total)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"order_count\",\n            \"expr\": \"COUNT(*)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        },\n        {\n            \"name\": \"avg_order_value\",\n            \"expr\": \"AVG(line_total)\",\n            \"source\": \"fact_orders\",\n            \"filters\": [\"status = 'completed'\"]\n        }\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"dim_customer\", \"column\": \"region\"},\n        {\"name\": \"category\", \"source\": \"dim_product\", \"column\": \"category\"},\n        {\"name\": \"month\", \"source\": \"dim_date\", \"column\": \"month_name\"}\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\nquery = SemanticQuery(config)\n\n# ===========================================\n# 2. Load data and create context\n# ===========================================\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# ===========================================\n# 3. Execute queries\n# ===========================================\n\n# Query 1: Total revenue\nprint(\"=\" * 50)\nprint(\"Query 1: Total Revenue\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\nprint()\n\n# Query 2: Revenue by region\nprint(\"=\" * 50)\nprint(\"Query 2: Revenue by Region\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 3: Multiple metrics by region\nprint(\"=\" * 50)\nprint(\"Query 3: Multiple Metrics by Region\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue, order_count, avg_order_value BY region\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 4: Revenue by region and category\nprint(\"=\" * 50)\nprint(\"Query 4: Revenue by Region and Category\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue BY region, category\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# Query 5: Filtered query\nprint(\"=\" * 50)\nprint(\"Query 5: North Region Only\")\nprint(\"=\" * 50)\nresult = query.execute(\"revenue, order_count BY category WHERE region = 'North'\", context)\nprint(result.df.to_string(index=False))\nprint()\n\n# ===========================================\n# 4. Show execution details\n# ===========================================\nprint(\"=\" * 50)\nprint(\"Execution Details (last query)\")\nprint(\"=\" * 50)\nprint(f\"Metrics: {result.metrics}\")\nprint(f\"Dimensions: {result.dimensions}\")\nprint(f\"Row count: {result.row_count}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#output","title":"Output","text":"<pre><code>==================================================\nQuery 1: Total Revenue\n==================================================\nTotal Revenue: $8,953.56\n\n==================================================\nQuery 2: Revenue by Region\n==================================================\n region   revenue\n  North  2549.88\n  South  2349.93\n   East  1923.88\n   West  2129.87\n\n==================================================\nQuery 3: Multiple Metrics by Region\n==================================================\n region   revenue  order_count  avg_order_value\n  North  2549.88            7           364.27\n  South  2349.93            7           335.70\n   East  1923.88            7           274.84\n   West  2129.87            7           304.27\n\n==================================================\nQuery 4: Revenue by Region and Category\n==================================================\n region     category   revenue\n  North  Electronics  1549.94\n  North    Furniture   999.94\n  South  Electronics  1449.95\n  South    Furniture   899.98\n   East  Electronics  1323.91\n   East    Furniture   599.97\n   West  Electronics  1079.93\n   West    Furniture  1049.94\n\n==================================================\nQuery 5: North Region Only\n==================================================\n    category   revenue  order_count\n Electronics  1549.94            4\n   Furniture   999.94            3\n\n==================================================\nExecution Details (last query)\n==================================================\nMetrics: ['revenue', 'order_count']\nDimensions: ['category']\nRow count: 2\nExecution time: 8.45ms\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#query-syntax-reference","title":"Query Syntax Reference","text":"Pattern Example Description Single metric <code>\"revenue\"</code> Total, no grouping Metric + dimension <code>\"revenue BY region\"</code> Grouped by one dimension Multiple metrics <code>\"revenue, order_count\"</code> Multiple metrics together Multiple dimensions <code>\"revenue BY region, category\"</code> Cross-tabulation With filter <code>\"revenue BY region WHERE year = 2024\"</code> Filtered results Complex filter <code>\"revenue BY region WHERE region IN ('North', 'South')\"</code> IN clause"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#error-handling","title":"Error Handling","text":"<pre><code># Invalid metric\ntry:\n    result = query.execute(\"invalid_metric BY region\", context)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Unknown metric 'invalid_metric'. Available: ['revenue', 'order_count', 'avg_order_value']\n\n# Invalid dimension\ntry:\n    result = query.execute(\"revenue BY invalid_dimension\", context)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Error: Unknown dimension 'invalid_dimension'. Available: ['region', 'category', 'month']\n</code></pre>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Simple queries return totals: <code>\"revenue\"</code></li> <li>One dimension groups results: <code>\"revenue BY region\"</code></li> <li>Multiple dimensions create cross-tabs: <code>\"revenue BY region, category\"</code></li> <li>Filters constrain results: <code>\"revenue BY region WHERE year = 2024\"</code></li> <li>Multiple metrics can be queried together: <code>\"revenue, order_count BY region\"</code></li> <li>QueryResult contains the DataFrame, metrics, dimensions, and execution info</li> </ul>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's learn how to pre-compute metrics for dashboard performance.</p> <p>Next: Materializing Metrics</p>"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#navigation","title":"Navigation","text":"Previous Up Next Defining Dimensions Tutorials Materializing Metrics"},{"location":"tutorials/dimensional_modeling/10_querying_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Querying Reference</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/","title":"Materializing Metrics Tutorial","text":"<p>In this tutorial, you'll learn how to pre-compute and persist metrics using the <code>Materializer</code> class. Materialization creates pre-aggregated tables for fast dashboard performance.</p> <p>What You'll Learn: - Why materialize metrics - Defining materializations - Executing materializations - Scheduling with cron - Incremental strategies (replace vs sum)</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#unified-project-api-note","title":"Unified Project API Note","text":"<p>When using the unified <code>Project</code> API, materializations are defined in the <code>semantic</code> section of your <code>odibi.yaml</code>. Sources can reference pipeline nodes directly:</p> <pre><code># odibi.yaml\nproject: retail_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n      - name: dim_date\n        write: { connection: gold, table: dim_date }\n\nsemantic:\n  metrics:\n    - name: revenue\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters: [\"status = 'completed'\"]\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n    - name: month\n      source: $build_warehouse.dim_date\n      column: month_name\n\n  materializations:\n    - name: monthly_revenue_by_region\n      metrics: [revenue]\n      dimensions: [region, month]\n      output: gold/agg_monthly_revenue\n      schedule: \"0 2 1 * *\"\n</code></pre> <p>The <code>$pipeline.node</code> notation automatically reads from wherever the node writes. For full control over materialization execution, continue with the <code>Materializer</code> class below.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#why-materialize","title":"Why Materialize?","text":"<p>Ad-hoc queries are powerful but slow for dashboards:</p> Approach Query Time Use Case Ad-hoc query 5-30 seconds Exploratory analysis Materialized table 0.1-0.5 seconds Production dashboards <p>Materialization pre-computes metrics at a specific grain and saves them to a table. Dashboards query the pre-computed table instead of raw data.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#semantic-config-with-materializations","title":"Semantic Config with Materializations","text":"<p>Let's extend our semantic config to include materializations:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># semantic_config.yaml\nmetrics:\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: order_count\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: unique_customers\n    expr: \"COUNT(DISTINCT customer_sk)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\n  - name: avg_order_value\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters: [\"status = 'completed'\"]\n\ndimensions:\n  - name: region\n    source: dim_customer\n    column: region\n\n  - name: category\n    source: dim_product\n    column: category\n\n  - name: month\n    source: dim_date\n    column: month_name\n\n  - name: date_sk\n    source: dim_date\n    column: date_sk\n\nmaterializations:\n  - name: monthly_revenue_by_region\n    metrics: [revenue, order_count]\n    dimensions: [region, month]\n    output: gold/agg_monthly_revenue_region\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n\n  - name: daily_revenue\n    metrics: [revenue, order_count, unique_customers]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    schedule: \"0 3 * * *\"  # 3am daily\n\n  - name: category_summary\n    metrics: [revenue, order_count, avg_order_value]\n    dimensions: [category]\n    output: gold/agg_category_summary\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-1-define-a-materialization","title":"Step 1: Define a Materialization","text":"<p>A materialization specifies which metrics and dimensions to pre-compute:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code","title":"Python Code","text":"<pre><code>from odibi.semantics import MaterializationConfig\n\n# Define a materialization\nmonthly_revenue = MaterializationConfig(\n    name=\"monthly_revenue_by_region\",\n    metrics=[\"revenue\", \"order_count\"],\n    dimensions=[\"region\", \"month\"],\n    output=\"gold/agg_monthly_revenue_region\",\n    schedule=\"0 2 1 * *\"\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#understanding-the-definition","title":"Understanding the Definition","text":"Field Value Purpose <code>name</code> <code>\"monthly_revenue_by_region\"</code> Unique identifier <code>metrics</code> <code>[\"revenue\", \"order_count\"]</code> Which metrics to compute <code>dimensions</code> <code>[\"region\", \"month\"]</code> Grain (GROUP BY) <code>output</code> <code>\"gold/agg_monthly_revenue_region\"</code> Output table path <code>schedule</code> <code>\"0 2 1 * *\"</code> Cron schedule (optional)"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#what-gets-generated","title":"What Gets Generated","text":"<p>The materialization creates a table with: - One row per unique combination of <code>region</code> \u00d7 <code>month</code> - Columns for each metric (<code>revenue</code>, <code>order_count</code>)</p> <p>Output Table (12 rows - 4 regions \u00d7 3 months):</p> region month revenue order_count North January 2,549.88 7 North February 3,120.50 9 North March 2,890.25 8 South January 2,349.93 7 South February 2,780.40 8 South March 3,050.75 9 East January 1,923.88 7 East February 2,450.60 7 East March 2,180.35 6 West January 2,129.87 7 West February 2,890.45 8 West March 2,650.90 7"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-2-execute-a-materialization","title":"Step 2: Execute a Materialization","text":"<p>Use the <code>Materializer</code> class to execute:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_1","title":"Python Code","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport yaml\n\n# Load config\nwith open(\"semantic_config.yaml\") as f:\n    config = parse_semantic_config(yaml.safe_load(f))\n\n# Create context\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders_df)\ncontext.register(\"dim_customer\", dim_customer_df)\ncontext.register(\"dim_product\", dim_product_df)\ncontext.register(\"dim_date\", dim_date_df)\n\n# Create materializer\nmaterializer = Materializer(config)\n\n# Execute single materialization\nresult = materializer.execute(\"monthly_revenue_by_region\", context)\n\n# Check result\nprint(f\"Name: {result.name}\")\nprint(f\"Output: {result.output}\")\nprint(f\"Row count: {result.row_count}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#materializationresult","title":"MaterializationResult","text":"Field Type Description <code>name</code> str Materialization name <code>output</code> str Output table path <code>row_count</code> int Number of rows generated <code>elapsed_ms</code> float Execution time in ms <code>success</code> bool Whether it succeeded <code>error</code> str Error message (if failed) <code>df</code> DataFrame The computed data"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-3-write-the-output","title":"Step 3: Write the Output","text":"<p>Use a callback to write the materialized data:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_2","title":"Python Code","text":"<pre><code># Define write callback\ndef write_to_parquet(df, output_path):\n    \"\"\"Write DataFrame to Parquet.\"\"\"\n    full_path = f\"warehouse/{output_path}.parquet\"\n    df.to_parquet(full_path, index=False)\n    print(f\"Wrote {len(df)} rows to {full_path}\")\n\n# Execute with write callback\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_to_parquet\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#spark-example","title":"Spark Example","text":"<pre><code>def write_to_delta(df, output_path):\n    \"\"\"Write Spark DataFrame to Delta Lake.\"\"\"\n    df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/warehouse/{output_path}\")\n\nresult = materializer.execute(\n    \"monthly_revenue_by_region\",\n    context,\n    write_callback=write_to_delta\n)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-4-understanding-schedules","title":"Step 4: Understanding Schedules","text":"<p>The <code>schedule</code> field uses cron syntax:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0-59)\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0-23)\n\u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 day of month (1-31)\n\u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500 month (1-12)\n\u2502 \u2502 \u2502 \u2502 \u250c\u2500 day of week (0-6, Sun=0)\n\u2502 \u2502 \u2502 \u2502 \u2502\n* * * * *\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#common-schedules","title":"Common Schedules","text":"Schedule Cron When Daily at 2am <code>0 2 * * *</code> Every day at 2:00 AM Monthly at 2am <code>0 2 1 * *</code> 1st of month at 2:00 AM Weekly Sunday <code>0 3 * * 0</code> Sunday at 3:00 AM Hourly <code>0 * * * *</code> Every hour on the hour"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#reading-schedules","title":"Reading Schedules","text":"<pre><code># Get schedule for a materialization\nmat_config = materializer.get_materialization(\"monthly_revenue_by_region\")\nprint(f\"Schedule: {mat_config.schedule}\")\n# Output: Schedule: 0 2 1 * *\n\n# List all materializations with schedules\nfor mat in materializer.list_materializations():\n    print(f\"{mat['name']}: {mat['schedule'] or 'No schedule'}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-5-execute-all-materializations","title":"Step 5: Execute All Materializations","text":"<p>Execute all defined materializations at once:</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#python-code_3","title":"Python Code","text":"<pre><code># Execute all materializations\nresults = materializer.execute_all(context, write_callback=write_to_parquet)\n\n# Print summary\nprint(\"=\" * 60)\nprint(\"Materialization Summary\")\nprint(\"=\" * 60)\nfor result in results:\n    status = \"SUCCESS\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status}\")\n    if result.success:\n        print(f\"    Rows: {result.row_count}, Time: {result.elapsed_ms:.0f}ms\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#output","title":"Output","text":"<pre><code>============================================================\nMaterialization Summary\n============================================================\n  monthly_revenue_by_region: SUCCESS\n    Rows: 12, Time: 45ms\n  daily_revenue: SUCCESS\n    Rows: 14, Time: 32ms\n  category_summary: SUCCESS\n    Rows: 2, Time: 18ms\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#step-6-incremental-materialization","title":"Step 6: Incremental Materialization","text":"<p>For large datasets, use incremental updates instead of full rebuilds.</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#replace-strategy","title":"Replace Strategy","text":"<p>New data replaces existing rows for matching grain keys:</p> <pre><code>materializations:\n  - name: daily_revenue\n    metrics: [revenue, order_count]\n    dimensions: [date_sk]\n    output: gold/agg_daily_revenue\n    incremental:\n      timestamp_column: load_timestamp\n      merge_strategy: replace\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#how-replace-works","title":"How Replace Works","text":"<p>Existing Table:</p> date_sk revenue order_count 20240115 1,439.96 3 20240116 589.95 3 20240117 749.98 2 <p>New Data Arrives (late order for Jan 15):</p> date_sk revenue order_count 20240115 1,539.96 4 <p>After Replace Merge:</p> date_sk revenue order_count Note 20240115 1,539.96 4 Replaced 20240116 589.95 3 Unchanged 20240117 749.98 2 Unchanged"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#sum-strategy-use-with-caution","title":"Sum Strategy (Use with Caution)","text":"<p>New measure values add to existing values:</p> <pre><code>materializations:\n  - name: daily_order_count\n    metrics: [order_count]  # Only COUNT metrics!\n    dimensions: [date_sk]\n    output: gold/agg_daily_count\n    incremental:\n      timestamp_column: created_at\n      merge_strategy: sum\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#how-sum-works","title":"How Sum Works","text":"<p>Existing Table:</p> date_sk order_count 20240115 3 20240116 3 <p>New Orders (2 new orders on Jan 15):</p> date_sk order_count 20240115 2 <p>After Sum Merge:</p> date_sk order_count Note 20240115 5 3 + 2 = 5 20240116 3 Unchanged"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#when-not-to-use-sum","title":"When NOT to Use Sum","text":"<p>Never use sum for: - <code>AVG()</code> - Would become average of averages - <code>COUNT(DISTINCT)</code> - Would overcount - <code>MIN()</code> / <code>MAX()</code> - Would be wrong - Data with corrections/updates</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#complete-python-example","title":"Complete Python Example","text":"<pre><code>from odibi.semantics import Materializer, parse_semantic_config\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\n\n# ===========================================\n# 1. Load config with materializations\n# ===========================================\nconfig_dict = {\n    \"metrics\": [\n        {\"name\": \"revenue\", \"expr\": \"SUM(line_total)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]},\n        {\"name\": \"order_count\", \"expr\": \"COUNT(*)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]},\n        {\"name\": \"avg_order_value\", \"expr\": \"AVG(line_total)\", \"source\": \"fact_orders\",\n         \"filters\": [\"status = 'completed'\"]}\n    ],\n    \"dimensions\": [\n        {\"name\": \"region\", \"source\": \"dim_customer\", \"column\": \"region\"},\n        {\"name\": \"category\", \"source\": \"dim_product\", \"column\": \"category\"},\n        {\"name\": \"date_sk\", \"source\": \"dim_date\", \"column\": \"date_sk\"}\n    ],\n    \"materializations\": [\n        {\n            \"name\": \"daily_summary\",\n            \"metrics\": [\"revenue\", \"order_count\"],\n            \"dimensions\": [\"date_sk\"],\n            \"output\": \"gold/agg_daily_summary\",\n            \"schedule\": \"0 3 * * *\"\n        },\n        {\n            \"name\": \"region_summary\",\n            \"metrics\": [\"revenue\", \"order_count\", \"avg_order_value\"],\n            \"dimensions\": [\"region\"],\n            \"output\": \"gold/agg_region_summary\"\n        },\n        {\n            \"name\": \"category_by_region\",\n            \"metrics\": [\"revenue\", \"order_count\"],\n            \"dimensions\": [\"category\", \"region\"],\n            \"output\": \"gold/agg_category_region\"\n        }\n    ]\n}\n\nconfig = parse_semantic_config(config_dict)\n\n# ===========================================\n# 2. Setup context with data\n# ===========================================\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", pd.read_parquet(\"warehouse/fact_orders\"))\ncontext.register(\"dim_customer\", pd.read_parquet(\"warehouse/dim_customer\"))\ncontext.register(\"dim_product\", pd.read_parquet(\"warehouse/dim_product\"))\ncontext.register(\"dim_date\", pd.read_parquet(\"warehouse/dim_date\"))\n\n# ===========================================\n# 3. Define write callback\n# ===========================================\ndef write_output(df, output_path):\n    full_path = f\"warehouse/{output_path}.parquet\"\n    df.to_parquet(full_path, index=False)\n    print(f\"  \u2192 Wrote {len(df)} rows to {full_path}\")\n\n# ===========================================\n# 4. Execute all materializations\n# ===========================================\nmaterializer = Materializer(config)\n\nprint(\"=\" * 60)\nprint(\"Executing Materializations\")\nprint(\"=\" * 60)\n\nresults = materializer.execute_all(context, write_callback=write_output)\n\n# ===========================================\n# 5. Show results\n# ===========================================\nprint()\nprint(\"=\" * 60)\nprint(\"Results Summary\")\nprint(\"=\" * 60)\n\nfor result in results:\n    status = \"\u2713 SUCCESS\" if result.success else f\"\u2717 FAILED: {result.error}\"\n    print(f\"\\n{result.name}:\")\n    print(f\"  Status: {status}\")\n    print(f\"  Output: {result.output}\")\n    print(f\"  Rows: {result.row_count}\")\n    print(f\"  Time: {result.elapsed_ms:.0f}ms\")\n\n    # Show sample data\n    if result.success and result.df is not None:\n        print(f\"  Sample data:\")\n        print(result.df.head(5).to_string(index=False))\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#output_1","title":"Output","text":"<pre><code>============================================================\nExecuting Materializations\n============================================================\n  \u2192 Wrote 14 rows to warehouse/gold/agg_daily_summary.parquet\n  \u2192 Wrote 4 rows to warehouse/gold/agg_region_summary.parquet\n  \u2192 Wrote 8 rows to warehouse/gold/agg_category_region.parquet\n\n============================================================\nResults Summary\n============================================================\n\ndaily_summary:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_daily_summary\n  Rows: 14\n  Time: 42ms\n  Sample data:\n   date_sk   revenue  order_count\n  20240115  1439.96            3\n  20240116   589.95            3\n  20240117   749.98            2\n  20240118   983.94            2\n  20240119   269.98            2\n\nregion_summary:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_region_summary\n  Rows: 4\n  Time: 28ms\n  Sample data:\n  region   revenue  order_count  avg_order_value\n   North  2549.88            7           364.27\n   South  2349.93            7           335.70\n    East  1923.88            7           274.84\n    West  2129.87            7           304.27\n\ncategory_by_region:\n  Status: \u2713 SUCCESS\n  Output: gold/agg_category_region\n  Rows: 8\n  Time: 35ms\n  Sample data:\n     category  region   revenue  order_count\n  Electronics   North  1549.94            4\n  Electronics   South  1449.95            4\n  Electronics    East  1323.91            4\n  Electronics    West  1079.93            3\n    Furniture   North   999.94            3\n</code></pre>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Materialization pre-computes metrics for fast dashboard queries</li> <li>MaterializationConfig specifies metrics, dimensions, output, and schedule</li> <li>Materializer.execute() runs a single materialization</li> <li>Materializer.execute_all() runs all configured materializations</li> <li>Schedules use cron syntax: <code>\"0 2 * * *\"</code> (daily at 2am)</li> <li>Replace strategy overwrites matching grain keys (recommended)</li> <li>Sum strategy adds to existing values (use with caution)</li> </ul>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#next-steps","title":"Next Steps","text":"<p>Now let's put everything together in a complete semantic layer example.</p> <p>Next: Semantic Full Example</p>"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#navigation","title":"Navigation","text":"Previous Up Next Querying Metrics Tutorials Semantic Full Example"},{"location":"tutorials/dimensional_modeling/11_materializing_metrics/#reference","title":"Reference","text":"<p>For complete documentation, see: Materializing Reference</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/","title":"Semantic Layer Full Example","text":"<p>This tutorial brings together everything you've learned about the semantic layer into a complete, end-to-end example.</p> <p>What You'll See: - Complete semantic config (5 metrics, 5 dimensions, 3 materializations) - Unified Project API (simplest approach) - Full Python script that loads data, queries metrics, and materializes results - All output tables with sample data</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#option-a-unified-project-api-recommended","title":"Option A: Unified Project API (Recommended)","text":"<p>The simplest way to use the semantic layer is with the unified <code>Project</code> API:</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#odibiyaml-with-semantic-layer","title":"odibi.yaml with Semantic Layer","text":"<pre><code># odibi.yaml\nproject: retail_warehouse\nengine: pandas\n\nconnections:\n  gold:\n    type: delta\n    path: /mnt/data/gold\n\npipelines:\n  - pipeline: build_warehouse\n    nodes:\n      - name: fact_orders\n        write: { connection: gold, table: fact_orders }\n      - name: dim_customer\n        write: { connection: gold, table: dim_customer }\n      - name: dim_product\n        write: { connection: gold, table: dim_product }\n      - name: dim_date\n        write: { connection: gold, table: dim_date }\n\n# Semantic layer at project level\nsemantic:\n  metrics:\n    - name: revenue\n      description: \"Total revenue from completed orders\"\n      expr: \"SUM(line_total)\"\n      source: $build_warehouse.fact_orders    # References node's write target\n      filters:\n        - \"status = 'completed'\"\n\n    - name: order_count\n      expr: \"COUNT(*)\"\n      source: $build_warehouse.fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n    - name: avg_order_value\n      expr: \"AVG(line_total)\"\n      source: $build_warehouse.fact_orders\n      filters:\n        - \"status = 'completed'\"\n\n  dimensions:\n    - name: region\n      source: $build_warehouse.dim_customer   # No path duplication!\n      column: region\n\n    - name: category\n      source: $build_warehouse.dim_product\n      column: category\n\n    - name: month\n      source: $build_warehouse.dim_date\n      column: month_name\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#query-with-two-lines","title":"Query with Two Lines","text":"<pre><code>from odibi import Project\n\n# Load project - tables auto-resolved from connections\nproject = Project.load(\"odibi.yaml\")\n\n# Query metrics\nresult = project.query(\"revenue BY region\")\nprint(result.df)\n\n# Multiple metrics and dimensions\nresult = project.query(\"revenue, order_count, avg_order_value BY region, category\")\nprint(result.df)\n</code></pre> <p>That's it! No manual table loading or context registration required.</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#option-b-manual-approach","title":"Option B: Manual Approach","text":"<p>For more control, use the semantic layer components directly.</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#the-complete-semantic-configuration","title":"The Complete Semantic Configuration","text":""},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#semantic_configyaml","title":"semantic_config.yaml","text":"<pre><code># File: semantic_config.yaml\n# Complete semantic layer configuration for retail star schema\n\nmetrics:\n  # Revenue metrics\n  - name: revenue\n    description: \"Total revenue from completed orders\"\n    expr: \"SUM(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: cost\n    description: \"Total cost of goods sold\"\n    expr: \"SUM(quantity * 0.6 * unit_price)\"  # Assume 60% cost ratio\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: profit\n    description: \"Gross profit (revenue - cost)\"\n    expr: \"SUM(line_total * 0.4)\"  # 40% margin\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  # Volume metrics\n  - name: order_count\n    description: \"Number of completed orders\"\n    expr: \"COUNT(*)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\n  - name: avg_order_value\n    description: \"Average order value\"\n    expr: \"AVG(line_total)\"\n    source: fact_orders\n    filters:\n      - \"status = 'completed'\"\n\ndimensions:\n  # Geographic dimensions\n  - name: region\n    description: \"Customer geographic region\"\n    source: dim_customer\n    column: region\n\n  - name: category\n    description: \"Product category\"\n    source: dim_product\n    column: category\n\n  # Time dimensions\n  - name: month\n    description: \"Month name\"\n    source: dim_date\n    column: month_name\n    hierarchy:\n      - year\n      - quarter_name\n      - month_name\n\n  - name: quarter\n    description: \"Quarter name\"\n    source: dim_date\n    column: quarter_name\n\n  - name: year\n    description: \"Calendar year\"\n    source: dim_date\n    column: year\n\nmaterializations:\n  # Daily aggregate for trend analysis\n  - name: daily_metrics\n    description: \"Daily revenue and order metrics\"\n    metrics:\n      - revenue\n      - order_count\n      - avg_order_value\n    dimensions:\n      - year\n      - month\n    output: gold/agg_daily_metrics\n    schedule: \"0 3 * * *\"  # 3am daily\n\n  # Monthly by region for regional dashboards\n  - name: monthly_by_region\n    description: \"Monthly metrics by region\"\n    metrics:\n      - revenue\n      - profit\n      - order_count\n    dimensions:\n      - region\n      - month\n    output: gold/agg_monthly_region\n    schedule: \"0 2 1 * *\"  # 2am on 1st of month\n\n  # Category performance summary\n  - name: category_performance\n    description: \"Category performance metrics\"\n    metrics:\n      - revenue\n      - profit\n      - order_count\n      - avg_order_value\n    dimensions:\n      - category\n    output: gold/agg_category_performance\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#complete-python-script","title":"Complete Python Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete Semantic Layer Example\n\nThis script demonstrates the full workflow:\n1. Load star schema data\n2. Configure semantic layer\n3. Run queries\n4. Materialize metrics\n\"\"\"\n\nfrom odibi.semantics import (\n    SemanticQuery,\n    Materializer,\n    SemanticLayerConfig,\n    MetricDefinition,\n    DimensionDefinition,\n    MaterializationConfig,\n    parse_semantic_config\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\nimport yaml\nfrom pathlib import Path\n\n# =============================================================================\n# 1. LOAD THE STAR SCHEMA DATA\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 1: Loading Star Schema Data\")\nprint(\"=\" * 70)\n\n# Load dimension tables\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\n\nprint(f\"dim_customer: {len(dim_customer)} rows\")\nprint(f\"dim_product:  {len(dim_product)} rows\")\nprint(f\"dim_date:     {len(dim_date)} rows\")\nprint(f\"fact_orders:  {len(fact_orders)} rows\")\nprint()\n\n# =============================================================================\n# 2. CREATE SEMANTIC LAYER CONFIG\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 2: Creating Semantic Layer Configuration\")\nprint(\"=\" * 70)\n\n# Option A: Load from YAML file\n# with open(\"semantic_config.yaml\") as f:\n#     config = parse_semantic_config(yaml.safe_load(f))\n\n# Option B: Build programmatically\nconfig = SemanticLayerConfig(\n    metrics=[\n        MetricDefinition(\n            name=\"revenue\",\n            description=\"Total revenue from completed orders\",\n            expr=\"SUM(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"cost\",\n            description=\"Total cost of goods sold\",\n            expr=\"SUM(quantity * 0.6 * unit_price)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"profit\",\n            description=\"Gross profit (revenue - cost)\",\n            expr=\"SUM(line_total * 0.4)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"order_count\",\n            description=\"Number of completed orders\",\n            expr=\"COUNT(*)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        ),\n        MetricDefinition(\n            name=\"avg_order_value\",\n            description=\"Average order value\",\n            expr=\"AVG(line_total)\",\n            source=\"fact_orders\",\n            filters=[\"status = 'completed'\"]\n        )\n    ],\n    dimensions=[\n        DimensionDefinition(name=\"region\", source=\"dim_customer\", column=\"region\"),\n        DimensionDefinition(name=\"category\", source=\"dim_product\", column=\"category\"),\n        DimensionDefinition(name=\"month\", source=\"dim_date\", column=\"month_name\",\n                           hierarchy=[\"year\", \"quarter_name\", \"month_name\"]),\n        DimensionDefinition(name=\"quarter\", source=\"dim_date\", column=\"quarter_name\"),\n        DimensionDefinition(name=\"year\", source=\"dim_date\", column=\"year\")\n    ],\n    materializations=[\n        MaterializationConfig(\n            name=\"daily_metrics\",\n            metrics=[\"revenue\", \"order_count\", \"avg_order_value\"],\n            dimensions=[\"year\", \"month\"],\n            output=\"gold/agg_daily_metrics\",\n            schedule=\"0 3 * * *\"\n        ),\n        MaterializationConfig(\n            name=\"monthly_by_region\",\n            metrics=[\"revenue\", \"profit\", \"order_count\"],\n            dimensions=[\"region\", \"month\"],\n            output=\"gold/agg_monthly_region\",\n            schedule=\"0 2 1 * *\"\n        ),\n        MaterializationConfig(\n            name=\"category_performance\",\n            metrics=[\"revenue\", \"profit\", \"order_count\", \"avg_order_value\"],\n            dimensions=[\"category\"],\n            output=\"gold/agg_category_performance\"\n        )\n    ]\n)\n\nprint(f\"Metrics defined:          {len(config.metrics)}\")\nprint(f\"Dimensions defined:       {len(config.dimensions)}\")\nprint(f\"Materializations defined: {len(config.materializations)}\")\n\n# List them\nprint(\"\\nMetrics:\")\nfor m in config.metrics:\n    print(f\"  - {m.name}: {m.description}\")\n\nprint(\"\\nDimensions:\")\nfor d in config.dimensions:\n    print(f\"  - {d.name}: from {d.source}.{d.column}\")\n\nprint(\"\\nMaterializations:\")\nfor mat in config.materializations:\n    print(f\"  - {mat.name}: {mat.metrics} BY {mat.dimensions}\")\nprint()\n\n# =============================================================================\n# 3. SETUP CONTEXT\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 3: Setting Up Engine Context\")\nprint(\"=\" * 70)\n\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"fact_orders\", fact_orders)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\nprint(\"Registered tables: fact_orders, dim_customer, dim_product, dim_date\")\nprint()\n\n# =============================================================================\n# 4. RUN QUERIES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 4: Running Semantic Queries\")\nprint(\"=\" * 70)\n\nquery = SemanticQuery(config)\n\n# Query 1: Total revenue (no grouping)\nprint(\"\\n--- Query 1: Total Revenue ---\")\nresult = query.execute(\"revenue\", context)\nprint(f\"Total Revenue: ${result.df['revenue'].iloc[0]:,.2f}\")\nprint(f\"Execution time: {result.elapsed_ms:.2f}ms\")\n\n# Query 2: Revenue by region\nprint(\"\\n--- Query 2: Revenue by Region ---\")\nresult = query.execute(\"revenue BY region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 3: Multiple metrics by region\nprint(\"\\n--- Query 3: Revenue, Profit, Order Count by Region ---\")\nresult = query.execute(\"revenue, profit, order_count BY region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 4: Revenue by category and region\nprint(\"\\n--- Query 4: Revenue by Category and Region ---\")\nresult = query.execute(\"revenue BY category, region\", context)\nprint(result.df.to_string(index=False))\n\n# Query 5: Filtered query - North region only\nprint(\"\\n--- Query 5: North Region Performance ---\")\nresult = query.execute(\"revenue, profit, avg_order_value BY category WHERE region = 'North'\", context)\nprint(result.df.to_string(index=False))\n\nprint()\n\n# =============================================================================\n# 5. MATERIALIZE METRICS\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 5: Materializing Metrics\")\nprint(\"=\" * 70)\n\nmaterializer = Materializer(config)\n\n# Track outputs for display\noutput_tables = {}\n\ndef write_and_store(df, output_path):\n    \"\"\"Write to disk and store for display.\"\"\"\n    output_tables[output_path] = df.copy()\n    full_path = Path(f\"warehouse/{output_path}.parquet\")\n    full_path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_parquet(full_path, index=False)\n    print(f\"  \u2192 Wrote {len(df)} rows to {full_path}\")\n\n# Execute all materializations\nresults = materializer.execute_all(context, write_callback=write_and_store)\n\nprint(\"\\nMaterialization Results:\")\nfor result in results:\n    status = \"SUCCESS\" if result.success else f\"FAILED: {result.error}\"\n    print(f\"  {result.name}: {status} ({result.row_count} rows, {result.elapsed_ms:.0f}ms)\")\n\nprint()\n\n# =============================================================================\n# 6. SHOW OUTPUT TABLES\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"STEP 6: Output Tables\")\nprint(\"=\" * 70)\n\nfor output_path, df in output_tables.items():\n    print(f\"\\n--- {output_path} ({len(df)} rows) ---\")\n    print(df.to_string(index=False))\n\nprint()\nprint(\"=\" * 70)\nprint(\"COMPLETE!\")\nprint(\"=\" * 70)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#sample-output","title":"Sample Output","text":""},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-1-data-loading","title":"Step 1: Data Loading","text":"<pre><code>======================================================================\nSTEP 1: Loading Star Schema Data\n======================================================================\ndim_customer: 13 rows\ndim_product:  11 rows\ndim_date:     32 rows\nfact_orders:  30 rows\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-4-query-results","title":"Step 4: Query Results","text":"<p>Query 1: Total Revenue</p> revenue 8,953.56 <p>Query 2: Revenue by Region (4 rows)</p> region revenue North 2,549.88 South 2,349.93 East 1,923.88 West 2,129.87 <p>Query 3: Multiple Metrics by Region (4 rows)</p> region revenue profit order_count North 2,549.88 1,019.95 7 South 2,349.93 939.97 7 East 1,923.88 769.55 7 West 2,129.87 851.95 7 <p>Query 4: Revenue by Category and Region (8 rows)</p> category region revenue Electronics North 1,549.94 Electronics South 1,449.95 Electronics East 1,323.91 Electronics West 1,079.93 Furniture North 999.94 Furniture South 899.98 Furniture East 599.97 Furniture West 1,049.94 <p>Query 5: North Region Only (2 rows)</p> category revenue profit avg_order_value Electronics 1,549.94 619.98 387.49 Furniture 999.94 399.98 333.31"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-5-materialization-results","title":"Step 5: Materialization Results","text":"<pre><code>======================================================================\nSTEP 5: Materializing Metrics\n======================================================================\n  \u2192 Wrote 1 rows to warehouse/gold/agg_daily_metrics.parquet\n  \u2192 Wrote 4 rows to warehouse/gold/agg_monthly_region.parquet\n  \u2192 Wrote 2 rows to warehouse/gold/agg_category_performance.parquet\n\nMaterialization Results:\n  daily_metrics: SUCCESS (1 rows, 32ms)\n  monthly_by_region: SUCCESS (4 rows, 45ms)\n  category_performance: SUCCESS (2 rows, 28ms)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#step-6-output-tables","title":"Step 6: Output Tables","text":"<p>gold/agg_daily_metrics (1 row)</p> year month revenue order_count avg_order_value 2024 January 8,953.56 27 331.61 <p>gold/agg_monthly_region (4 rows)</p> region month revenue profit order_count North January 2,549.88 1,019.95 7 South January 2,349.93 939.97 7 East January 1,923.88 769.55 7 West January 2,129.87 851.95 7 <p>gold/agg_category_performance (2 rows)</p> category revenue profit order_count avg_order_value Electronics 5,403.73 2,161.49 15 360.25 Furniture 3,549.83 1,419.93 12 295.82"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>flowchart TB\n    subgraph StarSchema[\"Star Schema (Built by Pipelines)\"]\n        DC[(dim_customer&lt;br/&gt;13 rows)]\n        DP[(dim_product&lt;br/&gt;11 rows)]\n        DD[(dim_date&lt;br/&gt;32 rows)]\n        FO[(fact_orders&lt;br/&gt;30 rows)]\n    end\n\n    subgraph SemanticLayer[\"Semantic Layer\"]\n        subgraph Metrics[\"5 Metrics\"]\n            M1[revenue]\n            M2[cost]\n            M3[profit]\n            M4[order_count]\n            M5[avg_order_value]\n        end\n\n        subgraph Dims[\"5 Dimensions\"]\n            D1[region]\n            D2[category]\n            D3[month]\n            D4[quarter]\n            D5[year]\n        end\n\n        subgraph Mats[\"3 Materializations\"]\n            MAT1[daily_metrics]\n            MAT2[monthly_by_region]\n            MAT3[category_performance]\n        end\n    end\n\n    subgraph Output[\"Materialized Tables\"]\n        O1[(agg_daily_metrics&lt;br/&gt;1 row)]\n        O2[(agg_monthly_region&lt;br/&gt;4 rows)]\n        O3[(agg_category_performance&lt;br/&gt;2 rows)]\n    end\n\n    DC --&gt; D1\n    DP --&gt; D2\n    DD --&gt; D3\n    DD --&gt; D4\n    DD --&gt; D5\n    FO --&gt; M1\n    FO --&gt; M2\n    FO --&gt; M3\n    FO --&gt; M4\n    FO --&gt; M5\n\n    MAT1 --&gt; O1\n    MAT2 --&gt; O2\n    MAT3 --&gt; O3\n\n    style StarSchema fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style SemanticLayer fill:#1a1a2e,stroke:#4a90d9,color:#fff\n    style Output fill:#1a1a2e,stroke:#4a90d9,color:#fff\n</code></pre>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#summary","title":"Summary","text":"<p>In this complete example, you saw:</p> <ol> <li>5 Metrics: revenue, cost, profit, order_count, avg_order_value</li> <li>5 Dimensions: region, category, month, quarter, year</li> <li>3 Materializations: daily_metrics, monthly_by_region, category_performance</li> <li>5 Queries: Total, by region, multiple metrics, cross-tab, filtered</li> </ol> <p>The semantic layer provides: - Consistent definitions across all queries - Business-friendly syntax: <code>\"revenue BY region\"</code> - Pre-computed aggregates for fast dashboards - Self-documenting config with descriptions</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial series (Part 2: Semantic Layer), you learned:</p> <ol> <li>What a semantic layer is and why it matters</li> <li>Defining metrics with expressions and filters</li> <li>Defining dimensions for grouping and drill-down</li> <li>Querying with the simple <code>\"metric BY dimension\"</code> syntax</li> <li>Materializing metrics for dashboard performance</li> <li>Putting it all together in a production-ready configuration</li> </ol>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#next-steps","title":"Next Steps","text":"<p>The final tutorial covers FK validation for data quality.</p> <p>Next: FK Validation</p>"},{"location":"tutorials/dimensional_modeling/12_semantic_full_example/#navigation","title":"Navigation","text":"Previous Up Next Materializing Metrics Tutorials FK Validation"},{"location":"tutorials/dimensional_modeling/13_fk_validation/","title":"FK Validation Tutorial","text":"<p>In this tutorial, you'll learn how to validate foreign key relationships between fact and dimension tables to ensure data quality.</p> <p>What You'll Learn: - Why validate foreign keys - Defining relationships - Detecting orphan records - Handling orphans with different strategies - Generating lineage diagrams</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#why-validate-foreign-keys","title":"Why Validate Foreign Keys?","text":"<p>In a star schema, fact tables reference dimension tables via foreign keys. But what happens when: - A customer places an order, but the customer isn't in <code>dim_customer</code>? - An order references a product that was never loaded? - A date value doesn't exist in the date dimension?</p> <p>These are called orphan records\u2014facts that reference non-existent dimensions.</p> <p>Problems with orphans: - Reports show \"Unknown\" values - Counts and sums may be incorrect - Data quality issues go undetected - Downstream analytics are unreliable</p> <p>FK validation helps you detect, report, and handle these issues.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#source-data","title":"Source Data","text":"<p>We'll use the fact_orders table with some intentional orphan records:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#fact_orders-35-rows-including-5-orphans","title":"fact_orders (35 rows including 5 orphans)","text":"order_id customer_sk product_sk date_sk line_total status ORD001 1 1 20240115 1299.99 completed ORD002 1 2 20240115 59.98 completed ... ... ... ... ... ... ORD030 5 10 20240116 189.99 completed ORD031 99 1 20240117 1299.99 completed ORD032 88 2 20240118 29.99 completed ORD033 77 3 20240119 249.99 completed ORD034 66 4 20240120 49.99 completed ORD035 55 5 20240121 599.99 completed <p>Orphan records: ORD031-ORD035 have customer_sk values (99, 88, 77, 66, 55) that don't exist in dim_customer.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#dim_customer-13-rows","title":"dim_customer (13 rows)","text":"customer_sk customer_id name 0 -1 Unknown 1 C001 Alice Johnson 2 C002 Bob Smith ... ... ... 12 C012 Leo Anderson <p>Note: customer_sk values 55, 66, 77, 88, 99 do NOT exist.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-1-define-relationships","title":"Step 1: Define Relationships","text":"<p>First, declare the FK relationships in your star schema:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code","title":"Python Code","text":"<pre><code>from odibi.validation.fk import RelationshipConfig, RelationshipRegistry\n\n# Define all FK relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_dates\",\n        fact=\"fact_orders\",\n        dimension=\"dim_date\",\n        fact_key=\"date_sk\",\n        dimension_key=\"date_sk\",\n        nullable=True,  # Pending orders may not have dates\n        on_violation=\"warn\"\n    )\n]\n\n# Create registry\nregistry = RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#yaml-alternative","title":"YAML Alternative","text":"<pre><code># relationships.yaml\nrelationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_dates\n    fact: fact_orders\n    dimension: dim_date\n    fact_key: date_sk\n    dimension_key: date_sk\n    nullable: true\n    on_violation: warn\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#relationshipconfig-fields","title":"RelationshipConfig Fields","text":"Field Type Required Description <code>name</code> str Yes Unique identifier <code>fact</code> str Yes Fact table name <code>dimension</code> str Yes Dimension table name <code>fact_key</code> str Yes FK column in fact <code>dimension_key</code> str Yes PK/SK column in dimension <code>nullable</code> bool No Whether nulls are allowed (default: false) <code>on_violation</code> str No Action: \"error\", \"warn\", \"filter\" (default: \"error\")"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-2-validate-a-clean-fact-table","title":"Step 2: Validate a Clean Fact Table","text":"<p>Let's first validate a fact table with NO orphans:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_1","title":"Python Code","text":"<pre><code>from odibi.validation.fk import FKValidator\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# Load data\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")  # Clean data - 30 rows\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# Create validator\nvalidator = FKValidator(registry)\n\n# Validate\nreport = validator.validate_fact(fact_orders, \"fact_orders\", context)\n\n# Check results\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Total relationships: {report.total_relationships}\")\nprint(f\"Valid relationships: {report.valid_relationships}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#validation-report-clean-data","title":"Validation Report (Clean Data)","text":"<pre><code>All valid: True\nTotal relationships: 3\nValid relationships: 3\n\nRelationship: orders_to_customers\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n\nRelationship: orders_to_products\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n\nRelationship: orders_to_dates\n  Status: VALID\n  Total rows: 30\n  Orphan count: 0\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-3-validate-with-orphan-records","title":"Step 3: Validate with Orphan Records","text":"<p>Now let's validate the data with 5 orphan records:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_2","title":"Python Code","text":"<pre><code># Load data with orphans\nfact_orders_dirty = pd.read_csv(\"data/orders_with_orphans.csv\")\n\n# This has 35 rows - 5 with invalid customer_sk values\n\n# Validate\nreport = validator.validate_fact(fact_orders_dirty, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Orphan records found: {len(report.orphan_records)}\")\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#validation-report-with-orphans","title":"Validation Report (With Orphans)","text":"<pre><code>All valid: False\nTotal relationships: 3\nValid relationships: 2\nOrphan records found: 5\n\nRelationship: orders_to_customers\n  Status: INVALID\n  Total rows: 35\n  Orphan count: 5\n  Orphan values (sample):\n    - customer_sk = 99 (1 occurrence)\n    - customer_sk = 88 (1 occurrence)\n    - customer_sk = 77 (1 occurrence)\n    - customer_sk = 66 (1 occurrence)\n    - customer_sk = 55 (1 occurrence)\n\nRelationship: orders_to_products\n  Status: VALID\n  Total rows: 35\n  Orphan count: 0\n\nRelationship: orders_to_dates\n  Status: VALID\n  Total rows: 35\n  Orphan count: 0\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#detailed-orphan-records","title":"Detailed Orphan Records","text":"<pre><code># Get the orphan records\nfor orphan in report.orphan_records:\n    print(f\"Order {orphan.order_id}: customer_sk={orphan.customer_sk} not found\")\n</code></pre> order_id customer_sk reason ORD031 99 Not found in dim_customer ORD032 88 Not found in dim_customer ORD033 77 Not found in dim_customer ORD034 66 Not found in dim_customer ORD035 55 Not found in dim_customer"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-4-handle-orphans-with-different-strategies","title":"Step 4: Handle Orphans with Different Strategies","text":""},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-1-error-default","title":"Strategy 1: Error (Default)","text":"<p>Raise an exception when orphans are found:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load\n\ntry:\n    validated_df = validate_fk_on_load(\n        fact_df=fact_orders_dirty,\n        relationships=relationships,\n        context=context,\n        on_failure=\"error\"\n    )\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre> <p>Output:</p> <pre><code>Validation failed: FK validation found 5 orphan records:\n  - orders_to_customers: 5 orphans (customer_sk: 99, 88, 77, 66, 55)\n</code></pre> <p>Use case: Strict data quality\u2014fail the pipeline if any orphans exist.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-2-warn","title":"Strategy 2: Warn","text":"<p>Log a warning but continue processing:</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.WARNING)\n\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=relationships,\n    context=context,\n    on_failure=\"warn\"\n)\n\nprint(f\"Returned {len(validated_df)} rows (including orphans)\")\n</code></pre> <p>Output:</p> <pre><code>WARNING:odibi.validation.fk:FK validation found 5 orphan records for orders_to_customers\n  Orphan values: customer_sk in [99, 88, 77, 66, 55]\nReturned 35 rows (including orphans)\n</code></pre> <p>Use case: Log issues for investigation but don't block processing.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#strategy-3-filter","title":"Strategy 3: Filter","text":"<p>Remove orphan records from the result:</p> <pre><code>validated_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=relationships,\n    context=context,\n    on_failure=\"filter\"\n)\n\nprint(f\"Before: {len(fact_orders_dirty)} rows\")\nprint(f\"After:  {len(validated_df)} rows\")\nprint(f\"Filtered: {len(fact_orders_dirty) - len(validated_df)} orphan rows\")\n</code></pre> <p>Output:</p> <pre><code>Before: 35 rows\nAfter:  30 rows\nFiltered: 5 orphan rows\n</code></pre> <p>Before (35 rows):</p> order_id customer_sk line_total ORD001 1 1299.99 ORD002 1 59.98 ... ... ... ORD030 5 189.99 ORD031 99 1299.99 ORD032 88 29.99 ORD033 77 249.99 ORD034 66 49.99 ORD035 55 599.99 <p>After (30 rows - orphans removed):</p> order_id customer_sk line_total ORD001 1 1299.99 ORD002 1 59.98 ... ... ... ORD030 5 189.99 <p>Use case: Silently exclude bad data (use with caution\u2014you may lose legitimate records).</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#step-5-generate-lineage-from-relationships","title":"Step 5: Generate Lineage from Relationships","text":"<p>The relationship registry can generate a lineage graph:</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#python-code_3","title":"Python Code","text":"<pre><code>lineage = registry.generate_lineage()\nprint(lineage)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#output","title":"Output","text":"<pre><code>{\n    'fact_orders': ['dim_customer', 'dim_product', 'dim_date']\n}\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#mermaid-diagram","title":"Mermaid Diagram","text":"<pre><code># Generate mermaid diagram code\nmermaid_code = registry.to_mermaid()\nprint(mermaid_code)\n</code></pre> <pre><code>erDiagram\n    FACT_ORDERS ||--o{ DIM_CUSTOMER : \"customer_sk\"\n    FACT_ORDERS ||--o{ DIM_PRODUCT : \"product_sk\"\n    FACT_ORDERS ||--o{ DIM_DATE : \"date_sk\"\n\n    FACT_ORDERS {\n        string order_id PK\n        int customer_sk FK\n        int product_sk FK\n        int date_sk FK\n        decimal line_total\n    }\n\n    DIM_CUSTOMER {\n        int customer_sk PK\n        string customer_id\n        string name\n        string region\n    }\n\n    DIM_PRODUCT {\n        int product_sk PK\n        string product_id\n        string name\n        string category\n    }\n\n    DIM_DATE {\n        int date_sk PK\n        date full_date\n        string day_of_week\n        string month_name\n    }\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#complete-example","title":"Complete Example","text":"<p>Here's a complete script for FK validation:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nFK Validation Example\n\nThis script demonstrates:\n1. Defining FK relationships\n2. Validating a fact table\n3. Handling orphan records\n4. Generating lineage\n\"\"\"\n\nfrom odibi.validation.fk import (\n    RelationshipConfig,\n    RelationshipRegistry,\n    FKValidator,\n    validate_fk_on_load\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\nimport pandas as pd\n\n# =============================================================================\n# 1. LOAD DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 1: Load Data\")\nprint(\"=\" * 60)\n\n# Clean fact data\nfact_orders = pd.read_parquet(\"warehouse/fact_orders\")\nprint(f\"fact_orders: {len(fact_orders)} rows\")\n\n# Fact data with orphans\nfact_orders_dirty = pd.read_csv(\n    \"examples/tutorials/dimensional_modeling/data/orders_with_orphans.csv\"\n)\nprint(f\"fact_orders_dirty: {len(fact_orders_dirty)} rows (5 orphans)\")\n\n# Dimensions\ndim_customer = pd.read_parquet(\"warehouse/dim_customer\")\ndim_product = pd.read_parquet(\"warehouse/dim_product\")\ndim_date = pd.read_parquet(\"warehouse/dim_date\")\nprint()\n\n# =============================================================================\n# 2. DEFINE RELATIONSHIPS\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 2: Define Relationships\")\nprint(\"=\" * 60)\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_dates\",\n        fact=\"fact_orders\",\n        dimension=\"dim_date\",\n        fact_key=\"date_sk\",\n        dimension_key=\"date_sk\",\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\nprint(f\"Defined {len(relationships)} relationships\")\nprint()\n\n# =============================================================================\n# 3. SETUP CONTEXT\n# =============================================================================\ncontext = EngineContext(df=None, engine_type=EngineType.PANDAS)\ncontext.register(\"dim_customer\", dim_customer)\ncontext.register(\"dim_product\", dim_product)\ncontext.register(\"dim_date\", dim_date)\n\n# =============================================================================\n# 4. VALIDATE CLEAN DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 3: Validate Clean Data\")\nprint(\"=\" * 60)\n\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(fact_orders, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Relationships checked: {report.total_relationships}\")\nprint(f\"Valid: {report.valid_relationships}\")\nprint()\n\n# =============================================================================\n# 5. VALIDATE DIRTY DATA\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 4: Validate Data with Orphans\")\nprint(\"=\" * 60)\n\nreport = validator.validate_fact(fact_orders_dirty, \"fact_orders\", context)\n\nprint(f\"All valid: {report.all_valid}\")\nprint(f\"Orphan records: {len(report.orphan_records)}\")\n\nfor result in report.results:\n    status = \"VALID\" if result.valid else \"INVALID\"\n    print(f\"\\n  {result.relationship_name}: {status}\")\n    print(f\"    Total rows: {result.total_rows}\")\n    print(f\"    Orphans: {result.orphan_count}\")\n    if not result.valid:\n        print(f\"    Orphan values: {result.orphan_values[:5]}\")\nprint()\n\n# =============================================================================\n# 6. DEMONSTRATE HANDLING STRATEGIES\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 5: Orphan Handling Strategies\")\nprint(\"=\" * 60)\n\n# Filter strategy\nfiltered_df = validate_fk_on_load(\n    fact_df=fact_orders_dirty,\n    relationships=[relationships[0]],  # Just customer relationship\n    context=context,\n    on_failure=\"filter\"\n)\n\nprint(f\"\\nFilter Strategy:\")\nprint(f\"  Before: {len(fact_orders_dirty)} rows\")\nprint(f\"  After:  {len(filtered_df)} rows\")\nprint(f\"  Removed: {len(fact_orders_dirty) - len(filtered_df)} orphan rows\")\nprint()\n\n# =============================================================================\n# 7. GENERATE LINEAGE\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"Step 6: Generate Lineage\")\nprint(\"=\" * 60)\n\nlineage = registry.generate_lineage()\nprint(f\"Lineage: {lineage}\")\n\nprint(\"\\nDimensions referenced by fact_orders:\")\nfor dim in lineage.get(\"fact_orders\", []):\n    print(f\"  \u2192 {dim}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"COMPLETE!\")\nprint(\"=\" * 60)\n</code></pre>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#what-you-learned","title":"What You Learned","text":"<p>In this tutorial, you learned:</p> <ul> <li>Why FK validation matters: Orphan records cause data quality issues</li> <li>Defining relationships: Specify fact, dimension, and key columns</li> <li>Validating facts: Use FKValidator to detect orphans</li> <li>Handling strategies:</li> <li><code>error</code>: Fail the pipeline (strict)</li> <li><code>warn</code>: Log and continue (monitoring)</li> <li><code>filter</code>: Remove orphans (permissive)</li> <li>Generating lineage: Visualize table relationships</li> </ul>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#congratulations","title":"Congratulations!","text":"<p>You've completed the entire dimensional modeling tutorial series!</p> <p>What you built: - Part 1: Complete star schema (dimensions, facts, aggregates) - Part 2: Semantic layer (metrics, dimensions, materializations) - Part 3: FK validation (data quality)</p> <p>You now have all the tools to build production-ready dimensional data warehouses with Odibi.</p>"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#navigation","title":"Navigation","text":"Previous Up Next Semantic Full Example Tutorials -"},{"location":"tutorials/dimensional_modeling/13_fk_validation/#reference","title":"Reference","text":"<p>For complete documentation, see: FK Validation Reference</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/","title":"Pipeline Run Story: schema_evolution_demo","text":"<p>Executed: 2025-11-19T16:56:10.535762 Completed: 2025-11-19T16:56:10.611748 Duration: 0.08s Status: \u2705 Success</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#summary","title":"Summary","text":"<ul> <li>\u2705 Completed: 3 nodes</li> <li>\u274c Failed: 0 nodes</li> <li>\u23ed\ufe0f Skipped: 0 nodes</li> <li>\u23f1\ufe0f Duration: 0.08s</li> </ul> <p>Completed nodes: create_source, enrich_data, cleanup_data</p>"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-create_source","title":"Node: create_source","text":"<p>Status: \u2705 Success Duration: 0.0336s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Output schema: - Columns (3): product, region, sales - Rows: 3</p> <p>Sample output (first 3 rows):</p> product region sales A North 100 B North 150 A South 200"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-enrich_data","title":"Node: enrich_data","text":"<p>Status: \u2705 Success Duration: 0.0264s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Input schema: - Columns (3): product, region, sales</p> <p>Sample input (first 3 rows):</p> product region sales A North 100 B North 150 A South 200 <p>Output schema: - Columns (4): product, region, sales, tax</p> <p>Schema Changes: - \ud83d\udfe2 Added: tax - Rows: 3</p> <p>Sample output (first 3 rows):</p> product region sales tax A North 100 10.0 B North 150 15.0 A South 200 20.0"},{"location":"tutorials/walkthroughs/data/stories/schema_evolution_demo_20251119_165610/#node-cleanup_data","title":"Node: cleanup_data","text":"<p>Status: \u2705 Success Duration: 0.0160s</p> <p>Execution steps: - Applied 1 transform steps</p> <p>Input schema: - Columns (4): product, region, sales, tax</p> <p>Sample input (first 3 rows):</p> product region sales tax A North 100 10.0 B North 150 15.0 A South 200 20.0 <p>Output schema: - Columns (3): product, sales, tax</p> <p>Schema Changes: - \ud83d\udd34 Removed: region - Rows: 3</p> <p>Sample output (first 3 rows):</p> product sales tax A 100 10.0 B 150 15.0 A 200 20.0"},{"location":"validation/","title":"Data Validation","text":"<p>Odibi provides a comprehensive validation framework to ensure data quality at every stage of your pipeline.</p>"},{"location":"validation/#validation-layers","title":"Validation Layers","text":"Layer When it Runs Purpose Contracts Before transform Fail-fast checks on input data Validation Tests After transform Row-level data quality checks Quality Gates After validation Batch-level thresholds and pass rates FK Validation Post-pipeline Referential integrity between tables"},{"location":"validation/#quick-links","title":"Quick Links","text":""},{"location":"validation/#guides","title":"Guides","text":"<ul> <li>Contracts - Pre-transform fail-fast checks (always fail on violation)</li> <li>Validation Tests - Post-transform row-level checks (fail/warn/quarantine)</li> <li>Quality Gates - Batch-level thresholds and pass rates</li> <li>Quarantine - Capture and review invalid records</li> <li>FK Validation - Referential integrity between fact and dimension tables</li> </ul>"},{"location":"validation/#reference","title":"Reference","text":"<ul> <li>Contracts Schema - All contract types</li> <li>Validation Schema - Full validation configuration</li> </ul>"},{"location":"validation/#choosing-the-right-validation","title":"Choosing the Right Validation","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INPUT DATA                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CONTRACTS (Pre-Transform)                                  \u2502\n\u2502  \u2022 not_null on required columns                             \u2502\n\u2502  \u2022 row_count min/max                                        \u2502\n\u2502  \u2022 freshness checks                                         \u2502\n\u2502  \u2192 ALWAYS FAILS on violation                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TRANSFORMATION                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VALIDATION TESTS (Post-Transform)                          \u2502\n\u2502  \u2022 Range checks, format validation                          \u2502\n\u2502  \u2022 Custom SQL conditions                                    \u2502\n\u2502  \u2192 Can WARN, QUARANTINE, or FAIL                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  QUALITY GATES (Batch-Level)                                \u2502\n\u2502  \u2022 Pass rate thresholds (e.g., 95%)                         \u2502\n\u2502  \u2022 Row count anomaly detection                              \u2502\n\u2502  \u2192 Can ABORT, WARN, or WRITE_VALID_ONLY                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    OUTPUT DATA                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"validation/#example-complete-validation-setup","title":"Example: Complete Validation Setup","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: staging\n      path: orders\n\n    # 1. Contracts - Fail fast on bad input\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id]\n      - type: row_count\n        min: 100\n\n    # 2. Transformation\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE amount &gt; 0\"\n\n    # 3. Validation Tests - Check output quality\n    validation:\n      tests:\n        - type: range\n          column: amount\n          min: 0\n          max: 1000000\n        - type: unique\n          columns: [order_id]\n      on_fail: quarantine  # Route bad rows to quarantine\n\n      # 4. Quality Gate - Batch-level threshold\n      gate:\n        require_pass_rate: 0.95\n        on_fail: abort\n\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/#see-also","title":"See Also","text":"<ul> <li>Getting Started: Validation - Tutorial walkthrough</li> <li>Fact Pattern - Orphan handling in fact tables</li> </ul>"},{"location":"validation/contracts/","title":"Contracts","text":"<p>Pre-transform data quality checks that fail fast before any transformation runs.</p>"},{"location":"validation/contracts/#overview","title":"Overview","text":"<p>Contracts validate your input data before it enters the transformation pipeline. Unlike validation tests (which run after transforms), contracts always halt execution on failure\u2014they're your first line of defense against bad data.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INPUT DATA                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CONTRACTS (You are here)                                   \u2502\n\u2502  \u2022 Runs BEFORE transformation                               \u2502\n\u2502  \u2022 Always fails on violation                                \u2502\n\u2502  \u2022 Prevents bad data from entering pipeline                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TRANSFORMATION                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"validation/contracts/#when-to-use-contracts-vs-validation","title":"When to Use Contracts vs Validation","text":"Feature Contracts Validation Tests When it runs Before transform After transform On failure Always fails pipeline Configurable (fail/warn/quarantine) Use case Input data quality Output data quality Example \"Source must have customer_id\" \"Transformed amount must be &gt; 0\" <p>Rule of thumb: Use contracts to validate what you receive, use validation to validate what you produce.</p>"},{"location":"validation/contracts/#quick-start","title":"Quick Start","text":"<pre><code>nodes:\n  - name: process_orders\n    # Contracts run first - before any transformation\n    contracts:\n      - type: not_null\n        columns: [order_id, customer_id]\n      - type: row_count\n        min: 100\n      - type: freshness\n        column: created_at\n        max_age: \"24h\"\n\n    read:\n      connection: bronze\n      path: orders_raw\n\n    transform:\n      steps:\n        - sql: \"SELECT * FROM df WHERE amount &gt; 0\"\n\n    write:\n      connection: silver\n      path: orders\n</code></pre> <p>If any contract fails, the pipeline stops immediately with a <code>ValidationError</code>\u2014no transformation or write happens.</p>"},{"location":"validation/contracts/#available-contract-types","title":"Available Contract Types","text":""},{"location":"validation/contracts/#not_null","title":"not_null","text":"<p>Ensures columns contain no NULL values.</p> <pre><code>contracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n</code></pre> <p>Use for: Primary keys, required fields, foreign keys.</p>"},{"location":"validation/contracts/#unique","title":"unique","text":"<p>Ensures columns (or combination) contain unique values.</p> <pre><code># Single column\ncontracts:\n  - type: unique\n    columns: [order_id]\n\n# Composite key\ncontracts:\n  - type: unique\n    columns: [order_id, line_item_id]\n</code></pre> <p>Use for: Primary keys, natural keys, deduplication verification.</p>"},{"location":"validation/contracts/#row_count","title":"row_count","text":"<p>Validates row count falls within expected bounds.</p> <pre><code>contracts:\n  - type: row_count\n    min: 1000      # At least 1000 rows\n    max: 100000    # At most 100K rows\n</code></pre> <p>Use for: Detect truncated loads, ensure minimum completeness, cap batch sizes.</p>"},{"location":"validation/contracts/#freshness","title":"freshness","text":"<p>Validates data is not stale by checking a timestamp column.</p> <pre><code>contracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n</code></pre> <p>Use for: SLA monitoring, detecting stale source systems.</p>"},{"location":"validation/contracts/#accepted_values","title":"accepted_values","text":"<p>Ensures a column only contains values from an allowed list.</p> <pre><code>contracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected, cancelled]\n</code></pre> <p>Use for: Enum fields, status columns, categorical data.</p>"},{"location":"validation/contracts/#range","title":"range","text":"<p>Ensures column values fall within a specified range.</p> <pre><code>contracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n\n# Date range\ncontracts:\n  - type: range\n    column: order_date\n    min: \"2020-01-01\"\n    max: \"2030-12-31\"\n</code></pre> <p>Use for: Numeric bounds (ages, prices, quantities), date ranges.</p>"},{"location":"validation/contracts/#regex_match","title":"regex_match","text":"<p>Ensures column values match a regex pattern.</p> <pre><code>contracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n\ncontracts:\n  - type: regex_match\n    column: phone\n    pattern: \"^\\\\+?[1-9]\\\\d{1,14}$\"  # E.164 format\n</code></pre> <p>Use for: Format validation (emails, phone numbers, IDs, codes).</p>"},{"location":"validation/contracts/#custom_sql","title":"custom_sql","text":"<p>Runs a custom SQL condition.</p> <pre><code>contracts:\n  - type: custom_sql\n    condition: \"amount &gt; 0 AND quantity &gt; 0\"\n    threshold: 0.01  # Allow up to 1% failures\n</code></pre> <p>Use for: Complex business rules, multi-column conditions.</p>"},{"location":"validation/contracts/#schema","title":"schema","text":"<p>Validates that the DataFrame schema matches expected columns.</p> <pre><code>contracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n\n# Works with column definitions\ncolumns:\n  - name: order_id\n    type: integer\n  - name: customer_id\n    type: integer\n  - name: amount\n    type: decimal\n</code></pre> <p>Use for: Schema stability, detecting upstream drift.</p>"},{"location":"validation/contracts/#distribution","title":"distribution","text":"<p>Checks if a column's statistical distribution is within expected bounds.</p> <pre><code>contracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \"&gt;100\"  # Mean must be &gt; 100\n    on_fail: warn      # Can warn instead of fail\n\ncontracts:\n  - type: distribution\n    column: customer_id\n    metric: null_percentage\n    threshold: \"&lt;0.05\"  # Less than 5% nulls\n</code></pre> <p>Metrics: <code>mean</code>, <code>min</code>, <code>max</code>, <code>null_percentage</code></p> <p>Use for: Anomaly detection, data drift monitoring.</p>"},{"location":"validation/contracts/#real-world-examples","title":"Real-World Examples","text":""},{"location":"validation/contracts/#example-1-bronze-layer-ingestion","title":"Example 1: Bronze Layer Ingestion","text":"<p>Validate raw data before any processing:</p> <pre><code>nodes:\n  - name: ingest_customers\n    contracts:\n      # Must have data\n      - type: row_count\n        min: 1\n\n      # Required fields present\n      - type: not_null\n        columns: [customer_id, email]\n\n      # Data is fresh\n      - type: freshness\n        column: _extracted_at\n        max_age: \"48h\"\n\n    read:\n      connection: source_api\n      path: customers\n      format: json\n\n    write:\n      connection: bronze\n      path: customers_raw\n      mode: append\n      add_metadata: true\n</code></pre>"},{"location":"validation/contracts/#example-2-silver-layer-processing","title":"Example 2: Silver Layer Processing","text":"<p>Validate before expensive transformations:</p> <pre><code>nodes:\n  - name: process_transactions\n    contracts:\n      # Ensure keys exist for joins\n      - type: not_null\n        columns: [transaction_id, account_id, merchant_id]\n\n      # No duplicates in source\n      - type: unique\n        columns: [transaction_id]\n\n      # Amount makes sense\n      - type: range\n        column: amount\n        min: 0.01\n        max: 1000000\n\n      # Valid transaction types\n      - type: accepted_values\n        column: type\n        values: [purchase, refund, chargeback, transfer]\n\n    read:\n      connection: bronze\n      path: transactions_raw\n\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              t.*,\n              m.merchant_name,\n              a.account_type\n            FROM df t\n            LEFT JOIN merchants m ON t.merchant_id = m.id\n            LEFT JOIN accounts a ON t.account_id = a.id\n\n    write:\n      connection: silver\n      path: transactions\n      format: delta\n</code></pre>"},{"location":"validation/contracts/#example-3-cross-system-data","title":"Example 3: Cross-System Data","text":"<p>Validate data from multiple sources before combining:</p> <pre><code>nodes:\n  - name: merge_customer_data\n    contracts:\n      # Schema must match expected structure\n      - type: schema\n        strict: true\n\n      # Prevent data explosion\n      - type: row_count\n        max: 10000000\n\n      # Statistical sanity check\n      - type: distribution\n        column: lifetime_value\n        metric: mean\n        threshold: \"&gt;0\"\n\n      # Custom business rule\n      - type: custom_sql\n        condition: \"signup_date &lt;= last_order_date OR last_order_date IS NULL\"\n        threshold: 0.0  # Zero tolerance\n\n    read:\n      - connection: crm\n        path: customers\n      - connection: ecommerce\n        path: users\n\n    transform:\n      steps:\n        - sql: |\n            SELECT * FROM df1\n            UNION ALL\n            SELECT * FROM df2\n</code></pre>"},{"location":"validation/contracts/#error-handling","title":"Error Handling","text":"<p>When a contract fails, Odibi raises a <code>ValidationError</code> with details:</p> <pre><code>from odibi.exceptions import ValidationError\n\ntry:\n    pipeline.run()\nexcept ValidationError as e:\n    print(f\"Contract failed on node: {e.node_name}\")\n    print(f\"Failures: {e.failures}\")\n    # [{'test': 'not_null', 'column': 'customer_id', 'null_count': 42}]\n</code></pre>"},{"location":"validation/contracts/#best-practices","title":"Best Practices","text":"<ol> <li>Start with not_null and unique - Most contract failures are missing keys or duplicates</li> <li>Use row_count for safety - Prevents empty loads and data explosions</li> <li>Add freshness for SLAs - Know immediately when source systems are stale</li> <li>Keep contracts fast - They run on every execution; avoid expensive checks</li> <li>Don't over-contract - Validate what matters; save detailed checks for validation tests</li> </ol>"},{"location":"validation/contracts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"validation/contracts/#validationerror-contract-failure-but-which-contract","title":"\"ValidationError: Contract Failure\" - but which contract?","text":"<p>Symptom: Contract fails but error message is unclear.</p> <p>Fix: Check the <code>failures</code> list in the error: <pre><code>except ValidationError as e:\n    for failure in e.failures:\n        print(f\"Test: {failure['test']}, Details: {failure}\")\n</code></pre></p> <p>Or check the story report for detailed validation results.</p>"},{"location":"validation/contracts/#contract-passes-locally-but-fails-in-production","title":"Contract passes locally but fails in production","text":"<p>Common Causes: - Different data volumes (row_count thresholds) - Timezone differences (freshness checks) - Case sensitivity differences between engines</p> <p>Fixes: - Use <code>--dry-run</code> in production first to validate - Set <code>max_age</code> with buffer for timezone differences - Test with production-like data volumes locally</p>"},{"location":"validation/contracts/#freshness-contract-always-fails","title":"Freshness contract always fails","text":"<p>Symptom: <code>freshness</code> check fails even with recent data.</p> <p>Causes: - Wrong column name for timestamp - Timestamp column is string, not datetime - Timezone mismatch</p> <p>Fixes: <pre><code>contracts:\n  - type: freshness\n    column: updated_at    # Must be datetime type\n    max_age: \"24h\"        # Include buffer for safety\n</code></pre></p>"},{"location":"validation/contracts/#contract-on-wrong-dataframe","title":"Contract on wrong DataFrame","text":"<p>Symptom: Contract checks input data when you wanted to check transformed data.</p> <p>Explanation: Contracts run on input data (before transform). For output validation, use <code>validation.tests</code> instead:</p> <pre><code># Contracts = input validation (before transform)\ncontracts:\n  - type: not_null\n    columns: [id]\n\n# Validation = output validation (after transform)\nvalidation:\n  tests:\n    - type: not_null\n      columns: [computed_field]\n</code></pre>"},{"location":"validation/contracts/#column-not-found-in-contract","title":"\"Column not found\" in contract","text":"<p>Cause: Column name doesn't exist in input DataFrame.</p> <p>Fix: Verify column names match exactly (case-sensitive): <pre><code># Debug by adding a dry-run first\nodibi run config.yaml --dry-run\n</code></pre></p>"},{"location":"validation/contracts/#see-also","title":"See Also","text":"<ul> <li>Validation Overview - The 4-layer validation model</li> <li>Validation Tests - Post-transform row-level checks</li> <li>Quality Gates - Batch-level thresholds</li> <li>Quarantine - Route bad rows for review</li> <li>YAML Reference - Full contract schema</li> </ul>"},{"location":"validation/fk/","title":"FK Validation","text":"<p>The FK Validation module declares and validates referential integrity between fact and dimension tables.</p>"},{"location":"validation/fk/#how-it-fits-into-odibi","title":"How It Fits Into Odibi","text":"<p>FK Validation is a Python API module that complements the <code>fact</code> pattern. While the <code>fact</code> pattern handles basic orphan handling (unknown member assignment), the FK validation module provides:</p> <ul> <li>Detailed orphan reporting with sample values</li> <li>Multiple validation strategies (error, warn, filter)</li> <li>Relationship registry for documenting your data model</li> <li>Lineage generation from relationships</li> </ul> <p>It's typically used for: 1. Post-pipeline validation/auditing 2. Custom fact loading with advanced orphan handling 3. Documenting relationships for data governance</p>"},{"location":"validation/fk/#quick-start","title":"Quick Start","text":""},{"location":"validation/fk/#1-define-relationships","title":"1. Define Relationships","text":"<pre><code>from odibi.validation.fk import RelationshipConfig, RelationshipRegistry\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\n</code></pre>"},{"location":"validation/fk/#2-validate-a-fact-table","title":"2. Validate a Fact Table","text":"<pre><code>from odibi.validation.fk import FKValidator\nfrom odibi.context import EngineContext\n\n# Setup context with dimension tables\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\ncontext.register(\"dim_product\", spark.table(\"warehouse.dim_product\"))\n\n# Load fact table\nfact_df = spark.table(\"warehouse.fact_orders\")\n\n# Validate\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(fact_df, \"fact_orders\", context)\n\n# Check results\nif report.all_valid:\n    print(\"All FK relationships valid!\")\nelse:\n    print(f\"Found {len(report.orphan_records)} orphan records\")\n    for result in report.results:\n        if not result.valid:\n            print(f\"  {result.relationship_name}: {result.orphan_count} orphans\")\n</code></pre>"},{"location":"validation/fk/#yaml-configuration-optional","title":"YAML Configuration (Optional)","text":"<p>You can define relationships in YAML and load them:</p> <pre><code># relationships.yaml\nrelationships:\n  - name: orders_to_customers\n    fact: fact_orders\n    dimension: dim_customer\n    fact_key: customer_sk\n    dimension_key: customer_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_products\n    fact: fact_orders\n    dimension: dim_product\n    fact_key: product_sk\n    dimension_key: product_sk\n    nullable: false\n    on_violation: error\n\n  - name: orders_to_dates\n    fact: fact_orders\n    dimension: dim_date\n    fact_key: order_date_sk\n    dimension_key: date_sk\n    nullable: true  # Pending orders may not have date\n    on_violation: warn\n</code></pre> <pre><code>from odibi.validation.fk import parse_relationships_config\nimport yaml\n\nwith open(\"relationships.yaml\") as f:\n    config = yaml.safe_load(f)\n\nregistry = parse_relationships_config(config)\n</code></pre>"},{"location":"validation/fk/#relationshipconfig","title":"RelationshipConfig","text":"Field Type Required Default Description <code>name</code> str Yes - Unique relationship identifier <code>fact</code> str Yes - Fact table name <code>dimension</code> str Yes - Dimension table name <code>fact_key</code> str Yes - FK column in fact table <code>dimension_key</code> str Yes - PK/SK column in dimension <code>nullable</code> bool No false Whether nulls are allowed in fact_key <code>on_violation</code> str No \"error\" Action on violation: \"error\", \"warn\", \"quarantine\""},{"location":"validation/fk/#validation-results","title":"Validation Results","text":""},{"location":"validation/fk/#fkvalidationresult-per-relationship","title":"FKValidationResult (per relationship)","text":"Field Type Description <code>relationship_name</code> str Relationship identifier <code>valid</code> bool Whether validation passed <code>total_rows</code> int Total rows in fact table <code>orphan_count</code> int Number of orphan records <code>null_count</code> int Number of null FK values <code>orphan_values</code> list Sample orphan values (up to 100) <code>elapsed_ms</code> float Validation time"},{"location":"validation/fk/#fkvalidationreport-for-entire-fact-table","title":"FKValidationReport (for entire fact table)","text":"Field Type Description <code>fact_table</code> str Fact table name <code>all_valid</code> bool True if all relationships valid <code>total_relationships</code> int Number of relationships checked <code>valid_relationships</code> int Number that passed <code>results</code> List[FKValidationResult] Individual results <code>orphan_records</code> List[OrphanRecord] All orphan records"},{"location":"validation/fk/#validate_fk_on_load","title":"validate_fk_on_load","text":"<p>Convenience function for pipeline integration:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load, RelationshipConfig\n\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    )\n]\n\n# Error on violation (default) - raises ValueError\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"error\"\n)\n\n# Warn on violation - logs warning, returns original\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"warn\"\n)\n\n# Filter orphans - removes orphan rows\nvalidated_df = validate_fk_on_load(\n    fact_df=fact_df,\n    relationships=relationships,\n    context=context,\n    on_failure=\"filter\"\n)\n</code></pre>"},{"location":"validation/fk/#integration-with-fact-pattern","title":"Integration with Fact Pattern","text":"<p>The <code>fact</code> pattern already handles basic orphan handling. Use FK validation for additional auditing:</p> <pre><code># odibi.yaml - Build fact with orphan handling\npipelines:\n  - pipeline: build_facts\n    nodes:\n      - name: dim_customer\n        read:\n          connection: warehouse\n          path: dim_customer\n\n      - name: fact_orders\n        depends_on: [dim_customer]\n        read:\n          connection: staging\n          path: orders\n        pattern:\n          type: fact\n          params:\n            grain: [order_id]\n            dimensions:\n              - source_column: customer_id\n                dimension_table: dim_customer\n                dimension_key: customer_id\n                surrogate_key: customer_sk\n            orphan_handling: unknown  # Assigns SK=0 to orphans\n        write:\n          connection: warehouse\n          path: fact_orders\n</code></pre> <p>Then run FK validation as a post-pipeline check:</p> <pre><code># Post-pipeline audit\nfrom odibi.validation.fk import FKValidator, RelationshipRegistry, RelationshipConfig\n\n# Define expected relationships\nregistry = RelationshipRegistry(relationships=[\n    RelationshipConfig(\n        name=\"verify_customer_fk\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    )\n])\n\nvalidator = FKValidator(registry)\nreport = validator.validate_fact(\n    spark.table(\"warehouse.fact_orders\"),\n    \"fact_orders\",\n    context\n)\n\n# Report findings\nif not report.all_valid:\n    print(f\"WARNING: {report.orphan_records} orphan records found\")\n    # Log to monitoring, send alert, etc.\n</code></pre>"},{"location":"validation/fk/#lineage-generation","title":"Lineage Generation","text":"<p>Generate lineage graph from relationships:</p> <pre><code>registry = RelationshipRegistry(relationships=[\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\"\n    ),\n    RelationshipConfig(\n        name=\"line_items_to_orders\",\n        fact=\"fact_line_items\",\n        dimension=\"fact_orders\",\n        fact_key=\"order_sk\",\n        dimension_key=\"order_sk\"\n    )\n])\n\nlineage = registry.generate_lineage()\n# {\n#     'fact_orders': ['dim_customer', 'dim_product'],\n#     'fact_line_items': ['fact_orders']\n# }\n</code></pre>"},{"location":"validation/fk/#full-example","title":"Full Example","text":"<p>Complete FK validation workflow:</p> <pre><code>from odibi.validation.fk import (\n    RelationshipConfig,\n    RelationshipRegistry,\n    FKValidator,\n    get_orphan_records\n)\nfrom odibi.context import EngineContext\nfrom odibi.enums import EngineType\n\n# Define relationships\nrelationships = [\n    RelationshipConfig(\n        name=\"orders_to_customers\",\n        fact=\"fact_orders\",\n        dimension=\"dim_customer\",\n        fact_key=\"customer_sk\",\n        dimension_key=\"customer_sk\",\n        nullable=False,\n        on_violation=\"error\"\n    ),\n    RelationshipConfig(\n        name=\"orders_to_products\",\n        fact=\"fact_orders\",\n        dimension=\"dim_product\",\n        fact_key=\"product_sk\",\n        dimension_key=\"product_sk\",\n        nullable=False,\n        on_violation=\"warn\"\n    )\n]\n\nregistry = RelationshipRegistry(relationships=relationships)\nvalidator = FKValidator(registry)\n\n# Setup context\ncontext = EngineContext(df=None, engine_type=EngineType.SPARK, spark=spark)\ncontext.register(\"dim_customer\", spark.table(\"warehouse.dim_customer\"))\ncontext.register(\"dim_product\", spark.table(\"warehouse.dim_product\"))\n\n# Load and validate\nfact_df = spark.table(\"warehouse.fact_orders\")\nreport = validator.validate_fact(fact_df, \"fact_orders\", context)\n\n# Report\nprint(f\"Validation {'PASSED' if report.all_valid else 'FAILED'}\")\nprint(f\"Checked {report.total_relationships} relationships\")\n\nfor result in report.results:\n    status = \"PASS\" if result.valid else \"FAIL\"\n    print(f\"  {result.relationship_name}: {status}\")\n    if not result.valid:\n        print(f\"    Orphans: {result.orphan_count}\")\n        print(f\"    Sample values: {result.orphan_values[:5]}\")\n\n# Generate lineage\nlineage = registry.generate_lineage()\nprint(f\"Lineage: {lineage}\")\n</code></pre>"},{"location":"validation/fk/#pipeline-integration-patterns","title":"Pipeline Integration Patterns","text":""},{"location":"validation/fk/#pattern-1-pre-load-validation","title":"Pattern 1: Pre-Load Validation","text":"<p>Validate FK relationships before writing to the warehouse:</p> <pre><code>from odibi.validation.fk import validate_fk_on_load, RelationshipConfig\n\n@transform\ndef validate_and_load_orders(context, current):\n    \"\"\"Validate FKs before writing to warehouse.\"\"\"\n    relationships = [\n        RelationshipConfig(\n            name=\"orders_to_customers\",\n            fact=\"orders\",\n            dimension=\"dim_customer\",\n            fact_key=\"customer_id\",\n            dimension_key=\"customer_id\"\n        )\n    ]\n\n    validated_df = validate_fk_on_load(\n        fact_df=current,\n        relationships=relationships,\n        context=context,\n        on_failure=\"filter\"  # Remove orphans\n    )\n    return validated_df\n</code></pre> <p>YAML configuration: <pre><code>nodes:\n  - name: dim_customer\n    read:\n      connection: warehouse\n      path: dim_customer\n\n  - name: validated_orders\n    depends_on: [dim_customer]\n    read:\n      connection: staging\n      path: orders\n    transform:\n      steps:\n        - function: validate_and_load_orders\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre></p>"},{"location":"validation/fk/#pattern-2-post-pipeline-audit-job","title":"Pattern 2: Post-Pipeline Audit Job","text":"<p>Run FK validation as a separate audit pipeline:</p> <pre><code>pipelines:\n  - pipeline: audit_referential_integrity\n    description: \"Nightly FK validation audit\"\n    nodes:\n      - name: load_dimensions\n        read:\n          connection: warehouse\n          tables:\n            - dim_customer\n            - dim_product\n            - dim_date\n\n      - name: validate_fact_orders\n        depends_on: [load_dimensions]\n        read:\n          connection: warehouse\n          path: fact_orders\n        transform:\n          steps:\n            - function: run_fk_audit\n              params:\n                relationships_file: \"config/fk_relationships.yaml\"\n</code></pre>"},{"location":"validation/fk/#pattern-3-integrated-with-data-quality-gate","title":"Pattern 3: Integrated with Data Quality Gate","text":"<p>Use FK validation as a quality gate:</p> <pre><code>nodes:\n  - name: fact_orders\n    read:\n      connection: staging\n      path: orders\n    pattern:\n      type: fact\n      params:\n        grain: [order_id]\n        dimensions:\n          - source_column: customer_id\n            dimension_table: dim_customer\n            dimension_key: customer_id\n            surrogate_key: customer_sk\n    gate:\n      - type: custom\n        function: fk_validation_gate\n        params:\n          max_orphan_percent: 0.1  # Fail if &gt; 0.1% orphans\n    write:\n      connection: warehouse\n      path: fact_orders\n</code></pre>"},{"location":"validation/fk/#see-also","title":"See Also","text":"<ul> <li>Fact Pattern - Build fact tables with orphan handling</li> <li>Dimension Pattern - Build dimensions with unknown member</li> <li>Patterns Overview - All available patterns</li> </ul>"},{"location":"validation/tests/","title":"Validation Tests","text":"<p>Row-level data quality checks that run after transformation.</p>"},{"location":"validation/tests/#overview","title":"Overview","text":"<p>Validation tests evaluate your output data after transformations complete. Unlike contracts (which always fail), validation tests offer flexible responses: fail the pipeline, log a warning, or quarantine bad rows.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TRANSFORMATION                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  VALIDATION TESTS (You are here)                            \u2502\n\u2502  \u2022 Runs AFTER transformation                                \u2502\n\u2502  \u2022 Configurable response (fail/warn/quarantine)             \u2502\n\u2502  \u2022 Row-level evaluation                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  QUALITY GATES                                              \u2502\n\u2502  \u2022 Batch-level thresholds                                   \u2502\n\u2502  \u2022 \"Did 95% of rows pass?\"                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"validation/tests/#quick-start","title":"Quick Start","text":"<pre><code>nodes:\n  - name: process_orders\n    read:\n      connection: bronze\n      path: orders_raw\n\n    transform:\n      steps:\n        - sql: \"SELECT *, amount * quantity AS total FROM df\"\n\n    # Validation runs after transform\n    validation:\n      tests:\n        - type: not_null\n          columns: [order_id, customer_id]\n        - type: range\n          column: total\n          min: 0\n        - type: unique\n          columns: [order_id]\n\n      on_fail: quarantine  # Route bad rows instead of failing\n\n      quarantine:\n        connection: silver\n        path: quarantine/orders\n\n    write:\n      connection: silver\n      path: orders\n</code></pre>"},{"location":"validation/tests/#test-types-reference","title":"Test Types Reference","text":""},{"location":"validation/tests/#not_null","title":"not_null","text":"<p>Ensures columns contain no NULL values.</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [order_id, customer_id, email]\n      on_fail: quarantine\n</code></pre> Field Type Required Description <code>columns</code> list Yes Columns to check for nulls <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#unique","title":"unique","text":"<p>Ensures column values (or combinations) are unique.</p> <pre><code>validation:\n  tests:\n    # Single column\n    - type: unique\n      columns: [order_id]\n\n    # Composite key\n    - type: unique\n      columns: [order_id, line_item_id]\n      on_fail: fail  # Duplicates are critical\n</code></pre> Field Type Required Description <code>columns</code> list Yes Columns that form the unique key <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#range","title":"range","text":"<p>Ensures values fall within min/max bounds.</p> <pre><code>validation:\n  tests:\n    - type: range\n      column: age\n      min: 0\n      max: 150\n\n    - type: range\n      column: price\n      min: 0.01\n      # max is optional\n\n    - type: range\n      column: order_date\n      min: \"2020-01-01\"\n      max: \"2030-12-31\"\n</code></pre> Field Type Required Description <code>column</code> string Yes Column to check <code>min</code> number/string No Minimum value (inclusive) <code>max</code> number/string No Maximum value (inclusive) <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#accepted_values","title":"accepted_values","text":"<p>Ensures column only contains allowed values.</p> <pre><code>validation:\n  tests:\n    - type: accepted_values\n      column: status\n      values: [pending, approved, rejected, cancelled]\n      on_fail: warn  # Log but don't fail\n</code></pre> Field Type Required Description <code>column</code> string Yes Column to check <code>values</code> list Yes Allowed values <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#regex_match","title":"regex_match","text":"<p>Ensures values match a regex pattern.</p> <pre><code>validation:\n  tests:\n    - type: regex_match\n      column: email\n      pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n\n    - type: regex_match\n      column: phone\n      pattern: \"^\\\\+?[1-9]\\\\d{1,14}$\"\n      on_fail: quarantine\n</code></pre> Field Type Required Description <code>column</code> string Yes Column to check <code>pattern</code> string Yes Regex pattern <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#row_count","title":"row_count","text":"<p>Validates total row count.</p> <pre><code>validation:\n  tests:\n    - type: row_count\n      min: 100\n      max: 1000000\n</code></pre> Field Type Required Description <code>min</code> int No Minimum row count <code>max</code> int No Maximum row count <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#custom_sql","title":"custom_sql","text":"<p>Runs a custom SQL condition.</p> <pre><code>validation:\n  tests:\n    - type: custom_sql\n      name: positive_profit  # Named for clarity\n      condition: \"revenue &gt;= cost\"\n      threshold: 0.05  # Allow up to 5% failures\n\n    - type: custom_sql\n      condition: \"end_date &gt;= start_date OR end_date IS NULL\"\n      threshold: 0.0  # Zero tolerance\n</code></pre> Field Type Required Description <code>condition</code> string Yes SQL WHERE clause (should be true for valid rows) <code>threshold</code> float No Allowed failure rate (0.0 = no failures, 0.05 = 5%) <code>name</code> string No Name for reporting <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#volume_drop","title":"volume_drop","text":"<p>Detects unexpected drops in data volume compared to previous runs.</p> <pre><code>validation:\n  tests:\n    - type: volume_drop\n      max_drop_percentage: 50  # Fail if count drops &gt;50%\n</code></pre> Field Type Required Description <code>max_drop_percentage</code> float Yes Maximum allowed drop (0-100) <code>on_fail</code> string No <code>fail</code>, <code>warn</code>, or <code>quarantine</code>"},{"location":"validation/tests/#on-fail-actions","title":"On-Fail Actions","text":"<p>Each test can specify what happens when it fails:</p> Action Behavior <code>fail</code> Stop pipeline immediately (default) <code>warn</code> Log warning, continue with all rows <code>quarantine</code> Route failed rows to quarantine table, continue with valid rows <pre><code>validation:\n  tests:\n    # Critical - must pass\n    - type: unique\n      columns: [order_id]\n      on_fail: fail\n\n    # Important but recoverable\n    - type: not_null\n      columns: [email]\n      on_fail: quarantine\n\n    # Nice to have\n    - type: regex_match\n      column: phone\n      pattern: \"^\\\\d{10}$\"\n      on_fail: warn\n</code></pre>"},{"location":"validation/tests/#combining-with-quality-gates","title":"Combining with Quality Gates","text":"<p>Quality gates evaluate aggregate pass rates after all tests run:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n    - type: range\n      column: amount\n      min: 0\n\n  gate:\n    require_pass_rate: 0.95  # 95% must pass ALL tests\n    on_fail: abort\n\n    # Per-test thresholds\n    thresholds:\n      - test: not_null\n        min_pass_rate: 0.99  # 99% for not_null\n      - test: range\n        min_pass_rate: 0.90  # 90% for range\n</code></pre> <p>See Quality Gates for full documentation.</p>"},{"location":"validation/tests/#combining-with-quarantine","title":"Combining with Quarantine","text":"<p>Route failed rows to a quarantine table for review:</p> <pre><code>validation:\n  tests:\n    - type: not_null\n      columns: [customer_id, email]\n      on_fail: quarantine\n\n    - type: regex_match\n      column: email\n      pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n      on_fail: quarantine\n\n  quarantine:\n    connection: silver\n    path: quarantine/customers\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n      _failed_tests: true\n</code></pre> <p>See Quarantine for full documentation.</p>"},{"location":"validation/tests/#real-world-examples","title":"Real-World Examples","text":""},{"location":"validation/tests/#example-1-customer-data-validation","title":"Example 1: Customer Data Validation","text":"<pre><code>nodes:\n  - name: validate_customers\n    read:\n      connection: bronze\n      path: customers_raw\n\n    transform:\n      steps:\n        - sql: |\n            SELECT\n              customer_id,\n              LOWER(TRIM(email)) AS email,\n              phone,\n              signup_date,\n              country\n            FROM df\n\n    validation:\n      tests:\n        # Required fields\n        - type: not_null\n          columns: [customer_id, email, signup_date]\n          on_fail: quarantine\n\n        # Email format\n        - type: regex_match\n          column: email\n          pattern: \"^[\\\\w\\\\.-]+@[\\\\w\\\\.-]+\\\\.\\\\w+$\"\n          on_fail: quarantine\n\n        # Valid countries\n        - type: accepted_values\n          column: country\n          values: [US, CA, UK, DE, FR, AU]\n          on_fail: warn  # Log but allow\n\n        # No duplicates\n        - type: unique\n          columns: [customer_id]\n          on_fail: fail  # Critical\n\n      quarantine:\n        connection: silver\n        path: quarantine/customers\n        add_columns:\n          _rejection_reason: true\n          _rejected_at: true\n\n      gate:\n        require_pass_rate: 0.95\n        on_fail: abort\n\n    write:\n      connection: silver\n      path: customers\n      format: delta\n</code></pre>"},{"location":"validation/tests/#example-2-financial-transactions","title":"Example 2: Financial Transactions","text":"<pre><code>nodes:\n  - name: validate_transactions\n    read:\n      connection: bronze\n      path: transactions_raw\n\n    validation:\n      tests:\n        # Business rules\n        - type: custom_sql\n          name: valid_amount\n          condition: \"amount &gt; 0\"\n          on_fail: quarantine\n\n        - type: custom_sql\n          name: balanced_transaction\n          condition: \"debit_amount = credit_amount\"\n          threshold: 0.0  # Zero tolerance\n\n        # Date sanity\n        - type: range\n          column: transaction_date\n          min: \"2020-01-01\"\n          on_fail: quarantine\n\n        # Volume monitoring\n        - type: volume_drop\n          max_drop_percentage: 30\n          on_fail: fail\n\n      quarantine:\n        connection: silver\n        path: quarantine/transactions\n\n    write:\n      connection: silver\n      path: transactions\n</code></pre>"},{"location":"validation/tests/#example-3-mixed-severity-levels","title":"Example 3: Mixed Severity Levels","text":"<pre><code>validation:\n  tests:\n    # CRITICAL - Pipeline must stop\n    - type: unique\n      columns: [transaction_id]\n      on_fail: fail\n\n    # IMPORTANT - Quarantine for review\n    - type: not_null\n      columns: [customer_id, amount]\n      on_fail: quarantine\n\n    # MODERATE - Track but continue\n    - type: accepted_values\n      column: category\n      values: [retail, wholesale, online]\n      on_fail: warn\n\n    # MONITORING - Statistical check\n    - type: custom_sql\n      name: amount_sanity\n      condition: \"amount &lt; 100000\"\n      threshold: 0.01  # Allow 1% outliers\n      on_fail: warn\n</code></pre>"},{"location":"validation/tests/#best-practices","title":"Best Practices","text":"<ol> <li>Name your tests - Use the <code>name</code> field for clarity in logs and quarantine tables</li> <li>Layer your severity - Use <code>fail</code> for critical, <code>quarantine</code> for recoverable, <code>warn</code> for monitoring</li> <li>Combine with gates - Set pass rate thresholds for batch-level control</li> <li>Use quarantine generously - Don't lose bad data; capture it for analysis</li> <li>Keep custom_sql readable - Break complex conditions into multiple named tests</li> </ol>"},{"location":"validation/tests/#see-also","title":"See Also","text":"<ul> <li>Validation Overview - The 4-layer validation model</li> <li>Contracts - Pre-transform fail-fast checks</li> <li>Quality Gates - Batch-level thresholds</li> <li>Quarantine - Route bad rows for review</li> <li>YAML Reference - Full validation schema</li> </ul>"}]}