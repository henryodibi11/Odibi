# ODIBI Framework

[![CI](https://github.com/henryodibi11/Odibi/workflows/CI/badge.svg)](https://github.com/henryodibi11/Odibi/actions)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**Explicit over implicit. Stories over magic. Simple over clever.**

ODIBI is a declarative data engineering framework that makes data pipelines transparent, traceable, and teachable. Whether you're learning data engineering or building production systems, ODIBI helps you write pipelines that are easy to understand, debug, and evolve.

## Core Philosophy

- **Everything is a Node**: Every operation follows `read â†’ transform â†’ write`
- **Everything Explicit**: If it appears in the story, it must be in the config
- **Connections Centralized**: Define once, reference everywhere
- **Dependencies Clear**: No magic, no hidden flow
- **Stories Automatic**: Every run is documented
- **Engine Agnostic**: Same config works on Spark or Pandas
- **Production Ready**: Secrets management, observability, and plugins built-in

## Installation

```bash
# Basic installation (Pandas engine only)
pip install odibi

# With Spark support (for Databricks and large-scale processing)
pip install "odibi[spark]"

# With Azure connectors (ADLS Gen2, Azure SQL)
pip install "odibi[azure]"

# With Telemetry and Observability
pip install "odibi[telemetry]"

# All extras (Spark + Azure + Telemetry + Advanced)
pip install "odibi[all]"
```

---

## Quick Start

### Command-Line Interface (Recommended)

```bash
# Initialize secrets template
odibi secrets init pipeline.yaml

# Validate your configuration
odibi validate pipeline.yaml

# Run your pipeline
odibi run pipeline.yaml

# Get help
odibi --help
```

### Python API

```python
from odibi.pipeline import PipelineManager

# Load and run all pipelines from YAML
manager = PipelineManager.from_yaml("examples/templates/example_local.yaml")
results = manager.run()  # Runs all defined pipelines

# Or run specific pipeline
result = manager.run_pipeline('bronze_to_silver')
print(f"âœ… {len(result.completed)} nodes completed")
```

**What it does:**
- Loads CSV/Parquet/Avro/Delta from configured locations
- Cleans and validates data (SQL transforms)
- Saves output in Delta Lake format (ACID transactions)
- Generates execution story automatically

See [examples/templates/template_full.yaml](examples/templates/template_full.yaml) for all configuration options.

### Option 2: Spark + Azure Pipeline (Production)

For large-scale data processing on Databricks.

1. **Install with extras:**
   ```bash
   pip install "odibi[spark,azure]"
   ```

2. **Configure Azure connections:**
   - See [docs/setup_azure.md](docs/setup_azure.md) for authentication setup
   - See [docs/tutorials/databricks_setup.ipynb](docs/tutorials/databricks_setup.ipynb) for cluster configuration

3. **Run Spark pipeline:**
   ```bash
   odibi run examples/templates/example_spark.yaml
   ```

See [examples/templates/example_spark.yaml](examples/templates/example_spark.yaml) for the full configuration.

---

### ğŸ“ Interactive Tutorial

**Complete walkthrough:** `docs/tutorials/walkthroughs/01_local_pipeline_pandas.ipynb`

```bash
# Install Jupyter if needed
pip install jupyter

# Launch tutorial
jupyter notebook docs/tutorials/walkthroughs/01_local_pipeline_pandas.ipynb
```

**Learn in 30 minutes:**
- âœ… Basic pipelines
- âœ… Transform functions
- âœ… SQL transforms  
- âœ… Multi-source joins
- âœ… Debugging techniques

---

### Your First Pipeline (From Scratch)

**1. Create project.yaml**

```yaml
project: My First Pipeline
engine: pandas

connections:
  local:
    type: local
    base_path: ./data
```

**2. Create pipelines/simple.yaml**

```yaml
pipeline: simple_etl

nodes:
  - name: load_data
    read:
      connection: local
      path: input.csv
      format: csv
    cache: true

  - name: clean_data
    depends_on: [load_data]
    transform:
      steps:
        - "SELECT * FROM load_data WHERE amount > 0"

  - name: save_result
    depends_on: [clean_data]
    write:
      connection: local
      path: output.parquet
      format: parquet
      mode: overwrite
```

**3. Run it**

```bash
odibi run project.yaml
```

That's it! ODIBI will:
- Load your CSV
- Apply transformations
- Save to Parquet
- Generate a complete story of what happened

## Features

### Delta Lake Support (Phase 2B) âœ¨ NEW
Production-ready Delta Lake integration with ACID transactions and time travel:

```python
# Write Delta table
engine.write(df, connection=conn, format="delta", path="sales.delta", mode="append")

# Read with time travel
df_v5 = engine.read(conn, format="delta", path="sales.delta", options={"versionAsOf": 5})

# VACUUM old files
result = engine.vacuum_delta(conn, "sales.delta", retention_hours=168)

# Restore to previous version
engine.restore_delta(conn, "sales.delta", version=5)
```

**Features:**
- âœ… ACID transactions (no partial writes)
- âœ… Time travel (audit trail, debugging)
- âœ… Schema evolution (add columns safely)
- âœ… VACUUM operations (optimize storage)
- âœ… Works with Pandas and Spark engines
- âœ… Full Azure ADLS integration

See [docs/DELTA_LAKE_GUIDE.md](docs/DELTA_LAKE_GUIDE.md) for complete guide.

### Plugins & Extensibility
Add custom connectors without forking the codebase:

```bash
pip install odibi-connector-postgres
```

```yaml
connections:
  my_db:
    type: postgres
    host: localhost
```

### Unified Context API
Transform functions work on both Spark and Pandas without changes:

```python
from odibi import transform

@transform
def enrich_data(context, reference_table: str, threshold: float = 0.5):
    """Enrich data with reference information."""
    ref_data = context.get(reference_table)
    # Your logic here - works on both engines!
    return enriched_df
```

### Smart Dependency Management
Nodes execute in the right order, with automatic parallelization:

```yaml
nodes:
  - name: ref_1
    read: {...}

  - name: ref_2
    read: {...}

  - name: combine
    depends_on: [ref_1, ref_2]  # Waits for both
    transform: {...}
```

### Rich Error Messages
When things fail, you know exactly where and why:

```
âœ— Node execution failed: clean_data
  Location: pipelines/simple.yaml:15
  Step: 1 of 1

  Error: Column 'amount' not found
  Available columns: ['timestamp', 'value', 'status']

  Suggestions:
    1. Check input data schema: odibi run-node load_data --show-output
    2. Verify column name in SQL query
```

### Automatic Documentation
Every run generates a story showing:
- What was executed and when
- Input/output samples
- Schema changes
- Row counts
- Duration
- Success/failures

## Development Workflow

```bash
# Validate config without running
odibi validate project.yaml

# Run pipeline
odibi run project.yaml

# Use Python module syntax
python -m odibi run project.yaml
python -m odibi validate project.yaml
```

**Note:** Advanced CLI commands (run-node, graph, etc.) coming in Phase 3.

## Architecture

```
User Layer (YAML configs + Python transforms)
            â†“
Orchestration Layer (dependency graph, executor)
            â†“
Node Layer (unified read/transform/write)
            â†“
Engine Layer (Spark/Pandas adapters)
            â†“
Connection Layer (Azure/Local/Delta/SQL)
```

## ğŸ” Analysis: Configuration Architecture

### Hierarchy Overview

```
ProjectConfig (top-level)
â”œâ”€â”€ project: str
â”œâ”€â”€ engine: EngineType
â”œâ”€â”€ connections: Dict[str, ConnectionConfig]
â”œâ”€â”€ pipelines: List[PipelineConfig]
â”‚   â””â”€â”€ PipelineConfig
â”‚       â”œâ”€â”€ pipeline: str
â”‚       â””â”€â”€ nodes: List[NodeConfig]
â”‚           â””â”€â”€ NodeConfig
â”‚               â”œâ”€â”€ name: str
â”‚               â”œâ”€â”€ read: Optional[ReadConfig]
â”‚               â”œâ”€â”€ transform: Optional[TransformConfig]
â”‚               â””â”€â”€ write: Optional[WriteConfig]
â”œâ”€â”€ story: StoryConfig
â”œâ”€â”€ retry: RetryConfig
â””â”€â”€ logging: LoggingConfig
```

## Documentation

**ğŸš€ Getting Started:**
- [Quick Start Guide](docs/guides/01_QUICK_START.md) - Get up and running in 5 minutes
- [Getting Started Tutorial](examples/getting_started/) - Complete interactive walkthrough
- [Local Pandas Example](examples/example_local.yaml) - Simple Bronzeâ†’Silverâ†’Gold pipeline

**ğŸ“š Core Guides:**
- [Configuration Guide](docs/CONFIGURATION_EXPLAINED.md) - Complete config reference
- [Delta Lake Guide](docs/DELTA_LAKE_GUIDE.md) - ACID transactions & time travel
- [User Guide](docs/guides/02_USER_GUIDE.md) - Building pipelines
- [Transformation Guide](docs/guides/05_TRANSFORMATION_GUIDE.md) - Custom transforms
- [Troubleshooting](docs/guides/06_TROUBLESHOOTING.md) - Common issues

**ğŸ“– Setup & Deployment:**
- [Databricks Setup](docs/setup_databricks.md) - Community + Azure Databricks
- [Azure Integration](docs/setup_azure.md) - ADLS Gen2 + Azure SQL
- [Local Development](docs/LOCAL_DEVELOPMENT.md) - Local testing
- [Supported Formats](docs/SUPPORTED_FORMATS.md) - File format guide

**ğŸ’¡ Templates & Examples:**
- [template_full.yaml](examples/templates/template_full.yaml) - Complete feature reference
- [template_full_adls.yaml](examples/templates/template_full_adls.yaml) - Azure multi-account
- [example_delta_pipeline.yaml](examples/templates/example_delta_pipeline.yaml) - Delta Lake patterns
- [example_spark.yaml](examples/templates/example_spark.yaml) - Spark on Databricks

**ğŸ“ Advanced Walkthroughs:**
- [Delta Lake Deep Dive](docs/tutorials/walkthroughs/phase2b_delta_lake.ipynb) - All Delta features
- [Production Pipelines](docs/tutorials/walkthroughs/phase2b_production_pipeline.ipynb) - YAML + Key Vault
- [ADLS Integration](docs/tutorials/walkthroughs/phase2a_adls_test.ipynb) - Azure storage

**ğŸ”§ Project Info:**
- [PHASES.md](PHASES.md) - Roadmap & current status
- [CONTRIBUTING.md](CONTRIBUTING.md) - How to contribute
- [CHANGELOG.md](CHANGELOG.md) - Version history
- [Documentation Index](docs/README.md) - All docs organized

---

## Roadmap
 
 ODIBI is being developed in deliberate phases:
 
 - âœ… **Phase 1:** Pandas MVP, Spark scaffolding, Azure connections, CI/CD
 - âœ… **Phase 2A:** Azure ADLS + Key Vault authentication
 - âœ… **Phase 2B:** Delta Lake support (read/write, VACUUM, time travel)
 - âœ… **Phase 2C:** Performance optimization, parallel Key Vault fetching
 - âœ… **Phase 3:** CLI tools, testing utilities, advanced features
 - âœ… **Phase 4:** Production hardening, performance tuning
 - âœ… **Phase 5:** Community ecosystem, plugin system (Complete Nov 2025)
 
 See [PHASES.md](PHASES.md) for detailed roadmap.

---

## Contributing

We welcome contributions! ODIBI is designed to be community-driven from the start.

**Before contributing:**
1. Read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines
2. Check [PHASES.md](PHASES.md) for available work
3. Review [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)

**Quick start for contributors:**
```bash
git clone https://github.com/henryodibi11/Odibi.git
cd Odibi
pip install -e .[dev]
pre-commit install
pytest -v  # All 89 tests should pass
```

---

## Contact & Support

- **GitHub Issues:** https://github.com/henryodibi11/Odibi/issues
- **Email:** henryodibi@outlook.com
- **LinkedIn:** [Henry Odibi](https://www.linkedin.com/in/henry-odibi)

---

## License

MIT
