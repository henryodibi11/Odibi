{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2B Production Pipeline - Delta Lake with YAML Config\n",
    "\n",
    "**Version:** v1.2.0-alpha.2-phase2b  \n",
    "**Focus:** Production-ready Delta Lake pipelines using YAML + Key Vault\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "‚úÖ **Best Practices:**\n",
    "- YAML configuration (not hardcoded Python)\n",
    "- Key Vault authentication (not direct keys)\n",
    "- Delta Lake as default format\n",
    "- Multi-account storage setup\n",
    "- Error handling and validation\n",
    "\n",
    "‚úÖ **Real-World Scenarios:**\n",
    "- Bronze ‚Üí Silver ‚Üí Gold architecture\n",
    "- CSV to Delta conversion\n",
    "- Time travel for auditing\n",
    "- VACUUM maintenance\n",
    "- Version comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Azure Setup:**\n",
    "1. Storage account with containers (bronze, silver, gold)\n",
    "2. Key Vault with storage keys stored as secrets\n",
    "3. Managed Identity or Service Principal with access\n",
    "\n",
    "**Local Setup:**\n",
    "```bash\n",
    "pip install -e \".[pandas,azure]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Production YAML Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production YAML configuration created!\n",
      "\n",
      "Location: d:\\odibi\\walkthroughs\\config_production.yaml\n",
      "\n",
      "üìù Next steps:\n",
      "1. Update YOUR_STORAGE_ACCOUNT with your storage account name\n",
      "2. Update YOUR_KEY_VAULT with your Key Vault name\n",
      "3. Ensure secrets exist in Key Vault:\n",
      "   - bronze-storage-key\n",
      "   - silver-storage-key\n",
      "   - gold-storage-key\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration for production Delta Lake pipeline\n",
    "config = {\n",
    "    \"project\": \"delta_lake_production\",\n",
    "    \"description\": \"Production Delta Lake pipeline with Bronze/Silver/Gold architecture\",\n",
    "    \"engine\": \"pandas\",\n",
    "    \"connections\": {\n",
    "        # Local for testing\n",
    "        \"local\": {\n",
    "            \"type\": \"local\",\n",
    "            \"base_path\": \"./pipeline_data\"\n",
    "        },\n",
    "        \n",
    "        # Bronze layer (raw data from sources)\n",
    "        \"bronze\": {\n",
    "            \"type\": \"azure_adls\",\n",
    "            \"account\": \"your_account_name\",  # ‚Üê Update this\n",
    "            \"container\": \"your_container_name\",  # ‚Üê Update this\n",
    "            \"path_prefix\": \"raw\",\n",
    "            \"auth_mode\": \"key_vault\",  # ‚Üê Best practice!\n",
    "            \"key_vault_name\": \"your_key_valut\",  # ‚Üê Update this\n",
    "            \"secret_name\": \"your_secret_name\"  # ‚Üê Update this\n",
    "        },\n",
    "        \n",
    "        # Silver layer (cleaned, validated data)\n",
    "        \"silver\": {\n",
    "            \"type\": \"azure_adls\",\n",
    "            \"account\": \"your_account_name\",  # ‚Üê Update this\n",
    "            \"container\": \"your_container_name\",  # ‚Üê Update this\n",
    "            \"path_prefix\": \"clean\",\n",
    "            \"auth_mode\": \"key_vault\",\n",
    "            \"key_vault_name\": \"your_key_valut\",  # ‚Üê Update this\n",
    "            \"secret_name\": \"your_secret_name\"  # ‚Üê Update this\n",
    "        },\n",
    "        \n",
    "        # Gold layer (aggregated, business-ready data)\n",
    "        \"gold\": {\n",
    "            \"type\": \"azure_adls\",\n",
    "            \"account\": \"your_account_name\",  # ‚Üê Update this\n",
    "            \"container\": \"your_container_name\",  # ‚Üê Update this\n",
    "            \"path_prefix\": \"aggregated\",\n",
    "            \"auth_mode\": \"key_vault\",\n",
    "            \"key_vault_name\": \"your_key_valut\",  # ‚Üê Update this\n",
    "            \"secret_name\": \"your_secret_name\"  # ‚Üê Update this\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"story\": {\n",
    "        \"connection\": \"local\",\n",
    "        \"path\": \"stories/\",\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \n",
    "    \"retry\": {\n",
    "        \"max_attempts\": 3,\n",
    "        \"backoff_seconds\": 2.0\n",
    "    },\n",
    "    \n",
    "    \"logging\": {\n",
    "        \"level\": \"INFO\"\n",
    "    },\n",
    "    \n",
    "    \"pipelines\": [\n",
    "        # Pipeline 1: Bronze ‚Üí Silver (CSV to Delta)\n",
    "        {\n",
    "            \"pipeline\": \"bronze_to_silver\",\n",
    "            \"name\": \"Bronze to Silver - Sales Data\",\n",
    "            \"description\": \"Convert raw CSV to cleaned Delta Lake tables\",\n",
    "            \"nodes\": [\n",
    "                # Read raw CSV from Bronze\n",
    "                {\n",
    "                    \"name\": \"read_raw_sales\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"bronze\",\n",
    "                        \"path\": \"sales/raw_sales.csv\",\n",
    "                        \"format\": \"csv\"\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Clean and validate (single SQL statement)\n",
    "                {\n",
    "                    \"name\": \"clean_sales\",\n",
    "                    \"depends_on\": [\"read_raw_sales\"],\n",
    "                    \"transform\": {\n",
    "                        \"steps\": [\n",
    "                            \"\"\"\n",
    "                            SELECT \n",
    "                                *,\n",
    "                                now() as processed_at\n",
    "                            FROM read_raw_sales \n",
    "                            WHERE order_id IS NOT NULL \n",
    "                            AND amount > 0\n",
    "                            \"\"\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Write to Silver as Delta (best practice!)\n",
    "                {\n",
    "                    \"name\": \"write_silver_sales\",\n",
    "                    \"depends_on\": [\"clean_sales\"],\n",
    "                    \"write\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",  # ‚Üê Delta format!\n",
    "                        \"format\": \"delta\",\n",
    "                        \"mode\": \"append\"  # ‚Üê Incremental loads\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        # Pipeline 2: Silver ‚Üí Gold (Aggregation)\n",
    "        {\n",
    "            \"pipeline\": \"silver_to_gold\",\n",
    "            \"name\": \"Silver to Gold - Daily Aggregates\",\n",
    "            \"description\": \"Create business-ready aggregated tables\",\n",
    "            \"nodes\": [\n",
    "                # Read from Silver Delta\n",
    "                {\n",
    "                    \"name\": \"read_silver_sales\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",\n",
    "                        \"format\": \"delta\"\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Aggregate by date\n",
    "                {\n",
    "                    \"name\": \"aggregate_daily\",\n",
    "                    \"depends_on\": [\"read_silver_sales\"],\n",
    "                    \"transform\": {\n",
    "                        \"steps\": [\n",
    "                            \"\"\"\n",
    "                            SELECT \n",
    "                                DATE(processed_at) as date,\n",
    "                                COUNT(*) as order_count,\n",
    "                                SUM(amount) as total_amount,\n",
    "                                AVG(amount) as avg_amount,\n",
    "                                MIN(amount) as min_amount,\n",
    "                                MAX(amount) as max_amount\n",
    "                            FROM read_silver_sales\n",
    "                            GROUP BY DATE(processed_at)\n",
    "                            ORDER BY date DESC\n",
    "                            \"\"\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Write to Gold as Delta with partitioning\n",
    "                {\n",
    "                    \"name\": \"write_gold_daily\",\n",
    "                    \"depends_on\": [\"aggregate_daily\"],\n",
    "                    \"write\": {\n",
    "                        \"connection\": \"gold\",\n",
    "                        \"path\": \"sales/daily_summary.delta\",\n",
    "                        \"format\": \"delta\",\n",
    "                        \"mode\": \"overwrite\"  # ‚Üê Full refresh for aggregates\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        # Pipeline 3: Time Travel Audit\n",
    "        {\n",
    "            \"pipeline\": \"audit_delta_versions\",\n",
    "            \"name\": \"Audit Trail - Compare Versions\",\n",
    "            \"description\": \"Compare current vs previous Delta versions\",\n",
    "            \"nodes\": [\n",
    "                # Read current version\n",
    "                {\n",
    "                    \"name\": \"read_current\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",\n",
    "                        \"format\": \"delta\"\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Read previous version (time travel!)\n",
    "                {\n",
    "                    \"name\": \"read_previous\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",\n",
    "                        \"format\": \"delta\",\n",
    "                        \"options\": {\n",
    "                            \"versionAsOf\": 0  # ‚Üê Time travel!\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Compare versions\n",
    "                {\n",
    "                    \"name\": \"compare_versions\",\n",
    "                    \"depends_on\": [\"read_current\", \"read_previous\"],\n",
    "                    \"transform\": {\n",
    "                        \"steps\": [\n",
    "                            \"\"\"\n",
    "                            SELECT \n",
    "                                'Current' as version_type,\n",
    "                                COUNT(*) as row_count,\n",
    "                                SUM(amount) as total_amount\n",
    "                            FROM read_current\n",
    "                            UNION ALL\n",
    "                            SELECT \n",
    "                                'Previous' as version_type,\n",
    "                                COUNT(*) as row_count,\n",
    "                                SUM(amount) as total_amount\n",
    "                            FROM read_previous\n",
    "                            \"\"\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \n",
    "                # Save audit report\n",
    "                {\n",
    "                    \"name\": \"save_audit\",\n",
    "                    \"depends_on\": [\"compare_versions\"],\n",
    "                    \"write\": {\n",
    "                        \"connection\": \"local\",\n",
    "                        \"path\": \"audit/version_comparison.csv\",\n",
    "                        \"format\": \"csv\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = Path(\"config_production.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"‚úÖ Production YAML configuration created!\")\n",
    "print(f\"\\nLocation: {config_path.absolute()}\")\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"1. Update YOUR_STORAGE_ACCOUNT with your storage account name\")\n",
    "print(\"2. Update YOUR_KEY_VAULT with your Key Vault name\")\n",
    "print(\"3. Ensure secrets exist in Key Vault:\")\n",
    "print(\"   - bronze-storage-key\")\n",
    "print(\"   - silver-storage-key\")\n",
    "print(\"   - gold-storage-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Review the Generated Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Configuration:\n",
      "======================================================================\n",
      "project: delta_lake_production\n",
      "description: Production Delta Lake pipeline with Bronze/Silver/Gold architecture\n",
      "engine: pandas\n",
      "connections:\n",
      "  local:\n",
      "    type: local\n",
      "    base_path: ./pipeline_data\n",
      "  bronze:\n",
      "    type: azure_adls\n",
      "    account: ingrglobaldigitalopsteam\n",
      "    container: example-container\n",
      "    path_prefix: raw\n",
      "    auth_mode: key_vault\n",
      "    key_vault_name: GOATKeyVault\n",
      "    secret_name: GoatBlobStorageKey\n",
      "  silver:\n",
      "    type: azure_adls\n",
      "    account: ingrglobaldigitalopsteam\n",
      "    container: example-container\n",
      "    path_prefix: clean\n",
      "    auth_mode: key_vault\n",
      "    key_vault_name: GOATKeyVault\n",
      "    secret_name: GoatBlobStorageKey\n",
      "  gold:\n",
      "    type: azure_adls\n",
      "    account: ingrglobaldigitalopsteam\n",
      "    container: example-container\n",
      "    path_prefix: aggregated\n",
      "    auth_mode: key_vault\n",
      "    key_vault_name: GOATKeyVault\n",
      "    secret_name: GoatBlobStorageKey\n",
      "story:\n",
      "  connection: local\n",
      "  path: stories/\n",
      "  enabled: true\n",
      "retry:\n",
      "  max_attempts: 3\n",
      "  backoff_seconds: 2.0\n",
      "logging:\n",
      "  level: INFO\n",
      "pipelines:\n",
      "- pipeline: bronze_to_silver\n",
      "  name: Bronze to Silver - Sales Data\n",
      "  description: Convert raw CSV to cleaned Delta Lake tables\n",
      "  nodes:\n",
      "  - name: read_raw_sales\n",
      "    read:\n",
      "      connection: bronze\n",
      "      path: sales/raw_sales.csv\n",
      "      format: csv\n",
      "  - name: clean_sales\n",
      "    depends_on:\n",
      "    - read_raw_sales\n",
      "    transform:\n",
      "      steps:\n",
      "      - \"\\n                            SELECT \\n                                *,\\n\\\n",
      "        \\                                now() as processed_at\\n                 \\\n",
      "        \\           FROM read_raw_sales \\n                            WHERE order_id\\\n",
      "        \\ IS NOT NULL \\n                            AND amount > 0\\n             \\\n",
      "        \\               \"\n",
      "  - name: write_silver_sales\n",
      "    depends_on:\n",
      "    - clean_sales\n",
      "    write:\n",
      "      connection: silver\n",
      "      path: sales/sales.delta\n",
      "      format: delta\n",
      "      mode: append\n",
      "- pipeline: silver_to_gold\n",
      "  name: Silver to Gold - Daily Aggregates\n",
      "  description: Create business-ready aggregated tables\n",
      "  nodes:\n",
      "  - name: read_silver_sales\n",
      "    read:\n",
      "      connection: silver\n",
      "      path: sales/sales.delta\n",
      "      format: delta\n",
      "  - name: aggregate_daily\n",
      "    depends_on:\n",
      "    - read_silver_sales\n",
      "    transform:\n",
      "      steps:\n",
      "      - \"\\n                            SELECT \\n                                DATE(processed_at)\\\n",
      "        \\ as date,\\n                                COUNT(*) as order_count,\\n   \\\n",
      "        \\                             SUM(amount) as total_amount,\\n             \\\n",
      "        \\                   AVG(amount) as avg_amount,\\n                         \\\n",
      "        \\       MIN(amount) as min_amount,\\n                                MAX(amount)\\\n",
      "        \\ as max_amount\\n                            FROM read_silver_sales\\n    \\\n",
      "        \\                        GROUP BY DATE(processed_at)\\n                   \\\n",
      "        \\         ORDER BY date DESC\\n                            \"\n",
      "  - name: write_gold_daily\n",
      "    depends_on:\n",
      "    - aggregate_daily\n",
      "    write:\n",
      "      connection: gold\n",
      "      path: sales/daily_summary.delta\n",
      "      format: delta\n",
      "      mode: overwrite\n",
      "- pipeline: audit_delta_versions\n",
      "  name: Audit Trail - Compare Versions\n",
      "  description: Compare current vs previous Delta versions\n",
      "  nodes:\n",
      "  - name: read_current\n",
      "    read:\n",
      "      connection: silver\n",
      "      path: sales/sales.delta\n",
      "      format: delta\n",
      "  - name: read_previous\n",
      "    read:\n",
      "      connection: silver\n",
      "      path: sales/sales.delta\n",
      "      format: delta\n",
      "      options:\n",
      "        versionAsOf: 0\n",
      "  - name: compare_versions\n",
      "    depends_on:\n",
      "    - read_current\n",
      "    - read_previous\n",
      "    transform:\n",
      "      steps:\n",
      "      - \"\\n                            SELECT \\n                                'Current'\\\n",
      "        \\ as version_type,\\n                                COUNT(*) as row_count,\\n\\\n",
      "        \\                                SUM(amount) as total_amount\\n           \\\n",
      "        \\                 FROM read_current\\n                            UNION ALL\\n\\\n",
      "        \\                            SELECT \\n                                'Previous'\\\n",
      "        \\ as version_type,\\n                                COUNT(*) as row_count,\\n\\\n",
      "        \\                                SUM(amount) as total_amount\\n           \\\n",
      "        \\                 FROM read_previous\\n                            \"\n",
      "  - name: save_audit\n",
      "    depends_on:\n",
      "    - compare_versions\n",
      "    write:\n",
      "      connection: local\n",
      "      path: audit/version_comparison.csv\n",
      "      format: csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the configuration\n",
    "print(\"Generated Configuration:\")\n",
    "print(\"=\"*70)\n",
    "with open(\"config_production.yaml\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Update Configuration with Your Azure Details\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Before running pipelines, update the YAML file:\n",
    "\n",
    "```yaml\n",
    "connections:\n",
    "  bronze:\n",
    "    account: \"mycompanystorage\"      # ‚Üê Your storage account\n",
    "    key_vault_name: \"mycompany-kv\"   # ‚Üê Your Key Vault\n",
    "    secret_name: \"bronze-storage-key\" # ‚Üê Secret in Key Vault\n",
    "```\n",
    "\n",
    "**Key Vault Setup:**\n",
    "1. Store your storage account keys as secrets in Key Vault\n",
    "2. Grant your identity access to Key Vault (Get Secret permission)\n",
    "3. Use DefaultAzureCredential (works in Databricks, local with `az login`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Edit config_production.yaml and update:\n",
      "\n",
      "1. YOUR_STORAGE_ACCOUNT ‚Üí your actual storage account name\n",
      "2. YOUR_KEY_VAULT ‚Üí your actual Key Vault name\n",
      "\n",
      "Then run the next cell to validate.\n"
     ]
    }
   ],
   "source": [
    "# Manually edit the file or update programmatically\n",
    "import yaml\n",
    "\n",
    "# Option 1: Edit config_production.yaml manually in VS Code\n",
    "print(\"üìù Edit config_production.yaml and update:\")\n",
    "print(\"\\n1. YOUR_STORAGE_ACCOUNT ‚Üí your actual storage account name\")\n",
    "print(\"2. YOUR_KEY_VAULT ‚Üí your actual Key Vault name\")\n",
    "print(\"\\nThen run the next cell to validate.\")\n",
    "\n",
    "# Option 2: Update programmatically (uncomment and modify)\n",
    "# with open(\"config_production.yaml\", \"r\") as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "# \n",
    "# # Update values\n",
    "# for conn_name in [\"bronze\", \"silver\", \"gold\"]:\n",
    "#     config[\"connections\"][conn_name][\"account\"] = \"your_actual_account\"\n",
    "#     config[\"connections\"][conn_name][\"key_vault_name\"] = \"your_actual_kv\"\n",
    "# \n",
    "# # Save\n",
    "# with open(\"config_production.yaml\", \"w\") as f:\n",
    "#     yaml.dump(config, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Validate Configuration (Local Testing First)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration is valid!\n",
      "\n",
      "Connections defined: ['local', 'bronze', 'silver', 'gold']\n",
      "Pipelines defined: ['bronze_to_silver', 'silver_to_gold', 'audit_delta_versions']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from odibi.pipeline import Pipeline\n",
    "from odibi.config import ProjectConfig\n",
    "\n",
    "# Load and validate configuration\n",
    "try:\n",
    "    with open(\"config_production.yaml\", \"r\") as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    \n",
    "    # Validate with Pydantic\n",
    "    project_config = ProjectConfig(**config_dict)\n",
    "    \n",
    "    print(\"‚úÖ Configuration is valid!\")\n",
    "    print(f\"\\nConnections defined: {list(project_config.connections.keys())}\")\n",
    "    print(f\"Pipelines defined: {[p.pipeline for p in project_config.pipelines]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(\"\\nPlease fix the configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Create Sample Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample data created!\n",
      "\n",
      "Location: pipeline_data\\input\\sales\\raw_sales.csv\n",
      "Rows: 1,000\n",
      "Nulls: 100 (10.0%)\n",
      "Invalid amounts: 50\n",
      "\n",
      "Sample:\n",
      "   order_id    customer   product   amount  quantity region  order_date\n",
      "0       1.0  Customer_0   Monitor  1867.63         1  North  2025-11-06\n",
      "1       NaN  Customer_1  Keyboard  1152.01         3   West  2025-08-16\n",
      "2       3.0  Customer_2    Tablet  1408.47         8  North  2025-11-03\n",
      "3       4.0  Customer_3  Keyboard  1848.87         3   East  2025-09-10\n",
      "4       NaN  Customer_4  Keyboard  1429.12         8  North  2025-09-17\n",
      "5       6.0  Customer_5     Phone  -100.00         1  North  2025-08-14\n",
      "6       7.0  Customer_6    Tablet  1173.76         5  South  2025-09-03\n",
      "7       8.0  Customer_7    Tablet  1233.09         4  North  2025-08-23\n",
      "8       9.0  Customer_8    Tablet   877.05         5   West  2025-11-09\n",
      "9       NaN  Customer_9  Keyboard  1486.07         8   West  2025-09-16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create sample sales data\n",
    "np.random.seed(42)\n",
    "n_rows = 1000\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    \"order_id\": range(1, n_rows + 1),\n",
    "    \"customer\": [f\"Customer_{i % 100}\" for i in range(n_rows)],\n",
    "    \"product\": np.random.choice([\"Laptop\", \"Phone\", \"Tablet\", \"Monitor\", \"Keyboard\"], n_rows),\n",
    "    \"amount\": np.random.uniform(50, 2000, n_rows).round(2),\n",
    "    \"quantity\": np.random.randint(1, 10, n_rows),\n",
    "    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n_rows),\n",
    "    \"order_date\": [(datetime.now() - timedelta(days=np.random.randint(0, 90))).strftime(\"%Y-%m-%d\") for _ in range(n_rows)]\n",
    "})\n",
    "\n",
    "# Add some nulls for testing (10%)\n",
    "null_indices = np.random.choice(n_rows, size=int(n_rows * 0.1), replace=False)\n",
    "sample_data.loc[null_indices, 'order_id'] = None\n",
    "\n",
    "# Add some invalid amounts (5%)\n",
    "invalid_indices = np.random.choice(n_rows, size=int(n_rows * 0.05), replace=False)\n",
    "sample_data.loc[invalid_indices, 'amount'] = -100\n",
    "\n",
    "# Save in correct location (with sales/ subdirectory)\n",
    "output_path = Path(\"pipeline_data/input/sales\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "sample_data.to_csv(output_path / \"raw_sales.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Sample data created!\")\n",
    "print(f\"\\nLocation: {output_path / 'raw_sales.csv'}\")\n",
    "print(f\"Rows: {len(sample_data):,}\")\n",
    "print(f\"Nulls: {sample_data['order_id'].isna().sum()} ({sample_data['order_id'].isna().sum()/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"Invalid amounts: {(sample_data['amount'] < 0).sum()}\")\n",
    "print(\"\\nSample:\")\n",
    "print(sample_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test Locally First (Before Cloud)\n",
    "\n",
    "**Best Practice:** Always test with local connections before running against cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local test configuration created!\n",
      "\n",
      "This uses local file system instead of Azure for testing.\n"
     ]
    }
   ],
   "source": [
    "# Create local test configuration\n",
    "local_config = {\n",
    "    \"project\": \"delta_lake_production\",\n",
    "    \"description\": \"Production Delta Lake pipeline with Bronze/Silver/Gold architecture\",\n",
    "    \"engine\": \"pandas\",\n",
    "    \"connections\": {\n",
    "        \"bronze\": {\"type\": \"local\", \"base_path\": \"./pipeline_data/input\"},\n",
    "        \"silver\": {\"type\": \"local\", \"base_path\": \"./pipeline_data/silver\"},\n",
    "        \"gold\": {\"type\": \"local\", \"base_path\": \"./pipeline_data/gold\"},\n",
    "        \"local\": {\"type\": \"local\", \"base_path\": \"./pipeline_data\"}\n",
    "    },\n",
    "    \"story\": {\"connection\": \"local\", \"path\": \"stories/\", \"enabled\": True},\n",
    "    \"retry\": {\"max_attempts\": 3, \"backoff_seconds\": 2.0},\n",
    "    \"logging\": {\"level\": \"INFO\"},\n",
    "    \"pipelines\": config_dict[\"pipelines\"]  # Use same pipelines\n",
    "}\n",
    "\n",
    "# Save local config\n",
    "with open(\"config_local.yaml\", \"w\") as f:\n",
    "    yaml.dump(local_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"‚úÖ Local test configuration created!\")\n",
    "print(\"\\nThis uses local file system instead of Azure for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Run Pipeline - Bronze to Silver (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available pipelines:\n",
      "  - bronze_to_silver\n",
      "  - silver_to_gold\n",
      "  - audit_delta_versions\n",
      "\n",
      "======================================================================\n",
      "Running: bronze_to_silver\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Running pipeline: bronze_to_silver\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ SUCCESS - bronze_to_silver\n",
      "  Completed: 3 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 0.09s\n",
      "  Story: d:\\odibi\\walkthroughs\\pipeline_data\\stories\\bronze_to_silver_20251109_210212.md\n",
      "\n",
      "‚úÖ Pipeline completed!\n",
      "\n",
      "Results:\n",
      "  Completed: 3 nodes\n",
      "  Failed: 0 nodes\n",
      "  Skipped: 0 nodes\n",
      "  Duration: 0.09s\n",
      "\n",
      "Completed nodes:\n",
      "  ‚úÖ read_raw_sales\n",
      "  ‚úÖ clean_sales\n",
      "  ‚úÖ write_silver_sales\n"
     ]
    }
   ],
   "source": [
    "# Load pipeline from local config\n",
    "manager = Pipeline.from_yaml(\"config_local.yaml\")\n",
    "\n",
    "print(\"Available pipelines:\")\n",
    "for pipeline_name in manager._pipelines.keys():\n",
    "    print(f\"  - {pipeline_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Running: bronze_to_silver\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run Bronze ‚Üí Silver pipeline\n",
    "result = manager.run(\"bronze_to_silver\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed!\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Completed: {len(result.completed)} nodes\")\n",
    "print(f\"  Failed: {len(result.failed)} nodes\")\n",
    "print(f\"  Skipped: {len(result.skipped)} nodes\")\n",
    "print(f\"  Duration: {result.duration:.2f}s\")\n",
    "\n",
    "if result.completed:\n",
    "    print(\"\\nCompleted nodes:\")\n",
    "    for node_name in result.completed:\n",
    "        print(f\"  ‚úÖ {node_name}\")\n",
    "\n",
    "if result.failed:\n",
    "    print(\"\\nFailed nodes:\")\n",
    "    for node_name in result.failed:\n",
    "        print(f\"  ‚ùå {node_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Verify Delta Table Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Delta table in Silver layer:\n",
      "\n",
      "Rows: 2,565\n",
      "Columns: ['order_id', 'customer', 'product', 'amount', 'quantity', 'region', 'order_date', 'processed_at']\n",
      "\n",
      "Data quality after cleaning:\n",
      "  Nulls in order_id: 0 (should be 0)\n",
      "  Invalid amounts: 0 (should be 0)\n",
      "\n",
      "Sample:\n",
      "   order_id    customer   product   amount  quantity region  order_date  \\\n",
      "0       1.0  Customer_0   Monitor  1867.63         1  North  2025-11-06   \n",
      "1       3.0  Customer_2    Tablet  1408.47         8  North  2025-11-03   \n",
      "2       4.0  Customer_3  Keyboard  1848.87         3   East  2025-09-10   \n",
      "3       7.0  Customer_6    Tablet  1173.76         5  South  2025-09-03   \n",
      "4       8.0  Customer_7    Tablet  1233.09         4  North  2025-08-23   \n",
      "\n",
      "                      processed_at  \n",
      "0 2025-11-10 03:02:12.928018+00:00  \n",
      "1 2025-11-10 03:02:12.928018+00:00  \n",
      "2 2025-11-10 03:02:12.928018+00:00  \n",
      "3 2025-11-10 03:02:12.928018+00:00  \n",
      "4 2025-11-10 03:02:12.928018+00:00  \n"
     ]
    }
   ],
   "source": [
    "from odibi.engine.pandas_engine import PandasEngine\n",
    "from odibi.connections.local import LocalConnection\n",
    "\n",
    "engine = PandasEngine()\n",
    "silver_conn = LocalConnection(base_path=\"./pipeline_data/silver\")\n",
    "\n",
    "# Read the Delta table we just created\n",
    "df_silver = engine.read(\n",
    "    connection=silver_conn,\n",
    "    format=\"delta\",\n",
    "    path=\"sales/sales.delta\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Delta table in Silver layer:\")\n",
    "print(f\"\\nRows: {len(df_silver):,}\")\n",
    "print(f\"Columns: {list(df_silver.columns)}\")\n",
    "print(\"\\nData quality after cleaning:\")\n",
    "print(f\"  Nulls in order_id: {df_silver['order_id'].isna().sum()} (should be 0)\")\n",
    "print(f\"  Invalid amounts: {(df_silver['amount'] <= 0).sum()} (should be 0)\")\n",
    "print(\"\\nSample:\")\n",
    "print(df_silver.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Check Delta Table History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Table History:\n",
      "======================================================================\n",
      "\n",
      "Version 2:\n",
      "  Operation: WRITE\n",
      "  Timestamp: 1762743732971\n",
      "\n",
      "Version 1:\n",
      "  Operation: WRITE\n",
      "  Timestamp: 1762743489616\n",
      "\n",
      "Version 0:\n",
      "  Operation: WRITE\n",
      "  Timestamp: 1762743423822\n"
     ]
    }
   ],
   "source": [
    "# Get Delta table history\n",
    "history = engine.get_delta_history(\n",
    "    connection=silver_conn,\n",
    "    path=\"sales/sales.delta\"\n",
    ")\n",
    "\n",
    "print(\"Delta Table History:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for entry in history:\n",
    "    print(f\"\\nVersion {entry['version']}:\")\n",
    "    print(f\"  Operation: {entry['operation']}\")\n",
    "    print(f\"  Timestamp: {entry['timestamp']}\")\n",
    "    if 'operationMetrics' in entry and 'numOutputRows' in entry['operationMetrics']:\n",
    "        print(f\"  Rows: {entry['operationMetrics']['numOutputRows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Run Pipeline - Silver to Gold (Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Running: silver_to_gold\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Running pipeline: silver_to_gold\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ SUCCESS - silver_to_gold\n",
      "  Completed: 3 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 0.04s\n",
      "  Story: d:\\odibi\\walkthroughs\\pipeline_data\\stories\\silver_to_gold_20251109_210213.md\n",
      "\n",
      "‚úÖ Pipeline completed!\n",
      "\n",
      "Results:\n",
      "  Completed: 3 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 0.04s\n",
      "\n",
      "Completed nodes:\n",
      "  ‚úÖ read_silver_sales\n",
      "  ‚úÖ aggregate_daily\n",
      "  ‚úÖ write_gold_daily\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Running: silver_to_gold\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run Silver ‚Üí Gold pipeline\n",
    "result = manager.run(\"silver_to_gold\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed!\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Completed: {len(result.completed)} nodes\")\n",
    "print(f\"  Failed: {len(result.failed)} nodes\")\n",
    "print(f\"  Duration: {result.duration:.2f}s\")\n",
    "\n",
    "if result.completed:\n",
    "    print(\"\\nCompleted nodes:\")\n",
    "    for node_name in result.completed:\n",
    "        print(f\"  ‚úÖ {node_name}\")\n",
    "\n",
    "if result.failed:\n",
    "    print(\"\\nFailed nodes:\")\n",
    "    for node_name in result.failed:\n",
    "        print(f\"  ‚ùå {node_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gold layer aggregates:\n",
      "\n",
      "Days: 1\n",
      "\n",
      "Daily Summary:\n",
      "        date  order_count  total_amount   avg_amount  min_amount  max_amount\n",
      "0 2025-11-09         2565    2628236.85  1024.653743       59.03     1998.86\n"
     ]
    }
   ],
   "source": [
    "# Read Gold layer aggregates\n",
    "gold_conn = LocalConnection(base_path=\"./pipeline_data/gold\")\n",
    "\n",
    "df_gold = engine.read(\n",
    "    connection=gold_conn,\n",
    "    format=\"delta\",\n",
    "    path=\"sales/daily_summary.delta\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Gold layer aggregates:\")\n",
    "print(f\"\\nDays: {len(df_gold)}\")\n",
    "print(f\"\\nDaily Summary:\")\n",
    "print(df_gold.sort_values('date', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Run Second Load (Test Append Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added 100 more orders to source CSV\n",
      "\n",
      "============================================================\n",
      "Running pipeline: bronze_to_silver\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ SUCCESS - bronze_to_silver\n",
      "  Completed: 3 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 0.04s\n",
      "  Story: d:\\odibi\\walkthroughs\\pipeline_data\\stories\\bronze_to_silver_20251109_210213.md\n",
      "\n",
      "‚úÖ Second pipeline run completed!\n",
      "\n",
      "Delta table now has 3,420 rows\n",
      "Added 855 new rows\n"
     ]
    }
   ],
   "source": [
    "# Create more sample data\n",
    "new_data = pd.DataFrame({\n",
    "    \"order_id\": range(n_rows + 1, n_rows + 101),\n",
    "    \"customer\": [f\"Customer_{i % 100}\" for i in range(100)],\n",
    "    \"product\": np.random.choice([\"Laptop\", \"Phone\", \"Tablet\"], 100),\n",
    "    \"amount\": np.random.uniform(100, 1500, 100).round(2),\n",
    "    \"quantity\": np.random.randint(1, 5, 100),\n",
    "    \"region\": np.random.choice([\"North\", \"South\"], 100),\n",
    "    \"order_date\": [datetime.now().strftime(\"%Y-%m-%d\") for _ in range(100)]\n",
    "})\n",
    "\n",
    "# Append to CSV\n",
    "new_data.to_csv(\"pipeline_data/input/raw_sales.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "print(\"‚úÖ Added 100 more orders to source CSV\")\n",
    "\n",
    "# Run pipeline again (should append to Delta)\n",
    "result = manager.run(\"bronze_to_silver\")\n",
    "\n",
    "print(\"\\n‚úÖ Second pipeline run completed!\")\n",
    "\n",
    "# Check Delta table\n",
    "df_silver_v2 = engine.read(silver_conn, format=\"delta\", path=\"sales/sales.delta\")\n",
    "\n",
    "print(f\"\\nDelta table now has {len(df_silver_v2):,} rows\")\n",
    "print(f\"Added {len(df_silver_v2) - len(df_silver):,} new rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Time Travel - Compare Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Audit directory created\n",
      "======================================================================\n",
      "Running: audit_delta_versions\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Running pipeline: audit_delta_versions\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ SUCCESS - audit_delta_versions\n",
      "  Completed: 4 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 0.04s\n",
      "  Story: d:\\odibi\\walkthroughs\\pipeline_data\\stories\\audit_delta_versions_20251109_210213.md\n",
      "\n",
      "‚úÖ Audit pipeline completed!\n",
      "\n",
      "Results:\n",
      "  Completed: 4 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 0.04s\n",
      "\n",
      "Completed nodes:\n",
      "  ‚úÖ read_current\n",
      "  ‚úÖ read_previous\n",
      "  ‚úÖ compare_versions\n",
      "  ‚úÖ save_audit\n",
      "\n",
      "Version Comparison:\n",
      "======================================================================\n",
      "  version_type  row_count  total_amount\n",
      "0      Current       3420    3504315.80\n",
      "1     Previous        855     876078.95\n",
      "\n",
      "Changes:\n",
      "  Rows added: 2,565\n",
      "  Amount added: $2,628,236.85\n"
     ]
    }
   ],
   "source": [
    "# Create audit directory\n",
    "from pathlib import Path\n",
    "Path(\"pipeline_data/audit\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"‚úÖ Audit directory created\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Running: audit_delta_versions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run audit pipeline (uses time travel)\n",
    "result = manager.run(\"audit_delta_versions\")\n",
    "\n",
    "print(\"\\n‚úÖ Audit pipeline completed!\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Completed: {len(result.completed)} nodes\")\n",
    "print(f\"  Failed: {len(result.failed)} nodes\")\n",
    "print(f\"  Duration: {result.duration:.2f}s\")\n",
    "\n",
    "if result.completed:\n",
    "    print(\"\\nCompleted nodes:\")\n",
    "    for node_name in result.completed:\n",
    "        print(f\"  ‚úÖ {node_name}\")\n",
    "\n",
    "if result.failed:\n",
    "    print(\"\\nFailed nodes:\")\n",
    "    for node_name in result.failed:\n",
    "        print(f\"  ‚ùå {node_name}\")\n",
    "\n",
    "# Only read the file if pipeline succeeded\n",
    "if len(result.failed) == 0 and len(result.completed) > 0:\n",
    "    # Read audit report\n",
    "    audit_df = pd.read_csv(\"pipeline_data/audit/version_comparison.csv\")\n",
    "    \n",
    "    print(\"\\nVersion Comparison:\")\n",
    "    print(\"=\"*70)\n",
    "    print(audit_df)\n",
    "    \n",
    "    # Calculate differences\n",
    "    current = audit_df[audit_df['version_type'] == 'Current'].iloc[0]\n",
    "    previous = audit_df[audit_df['version_type'] == 'Previous'].iloc[0]\n",
    "    \n",
    "    print(\"\\nChanges:\")\n",
    "    print(f\"  Rows added: {current['row_count'] - previous['row_count']:,}\")\n",
    "    print(f\"  Amount added: ${current['total_amount'] - previous['total_amount']:,.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pipeline failed - check story for details\")\n",
    "    if result.story_path:\n",
    "        print(f\"Story: {result.story_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: VACUUM - Clean Old Files (Production Pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VACUUM Maintenance:\n",
      "======================================================================\n",
      "\n",
      "Dry run: Would delete 0 files\n",
      "\n",
      "üí° Production VACUUM schedule:\n",
      "\n",
      "# Weekly VACUUM job\n",
      "tables = [\n",
      "    ('silver', 'sales/sales.delta'),\n",
      "    ('gold', 'sales/daily_summary.delta')\n",
      "]\n",
      "\n",
      "for conn_name, table_path in tables:\n",
      "    result = engine.vacuum_delta(\n",
      "        connection=connections[conn_name],\n",
      "        path=table_path,\n",
      "        retention_hours=168  # Keep 7 days for time travel\n",
      "    )\n",
      "    print(f\"{table_path}: cleaned {result['files_deleted']} files\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Production VACUUM pattern\n",
    "print(\"VACUUM Maintenance:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# VACUUM with 7-day retention (production default)\n",
    "result = engine.vacuum_delta(\n",
    "    connection=silver_conn,\n",
    "    path=\"sales/sales.delta\",\n",
    "    retention_hours=168,  # 7 days\n",
    "    dry_run=True  # Preview first\n",
    ")\n",
    "\n",
    "print(f\"\\nDry run: Would delete {result['files_deleted']} files\")\n",
    "\n",
    "# In production, run weekly:\n",
    "print(\"\\nüí° Production VACUUM schedule:\")\n",
    "print(\"\"\"\\n# Weekly VACUUM job\n",
    "tables = [\n",
    "    ('silver', 'sales/sales.delta'),\n",
    "    ('gold', 'sales/daily_summary.delta')\n",
    "]\n",
    "\n",
    "for conn_name, table_path in tables:\n",
    "    result = engine.vacuum_delta(\n",
    "        connection=connections[conn_name],\n",
    "        path=table_path,\n",
    "        retention_hours=168  # Keep 7 days for time travel\n",
    "    )\n",
    "    print(f\"{table_path}: cleaned {result['files_deleted']} files\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Deploy to Production (Azure ADLS + Key Vault)\n",
    "\n",
    "**Now that local testing works, deploy to production:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Deployment Checklist:\n",
      "======================================================================\n",
      "\n",
      "1. Azure Storage Setup:\n",
      "  ‚úì Storage account created\n",
      "  ‚úì Containers: bronze, silver, gold\n",
      "  ‚úì Storage keys copied\n",
      "\n",
      "2. Key Vault Setup:\n",
      "  ‚úì Key Vault created\n",
      "  ‚úì Secrets added: bronze-storage-key, silver-storage-key, gold-storage-key\n",
      "  ‚úì Access policy: Get Secret permission for your identity\n",
      "\n",
      "3. Authentication:\n",
      "  ‚úì Local: 'az login' completed\n",
      "  ‚úì Databricks: Managed Identity configured\n",
      "  ‚úì DefaultAzureCredential working\n",
      "\n",
      "4. Configuration:\n",
      "  ‚úì config_production.yaml updated with real values\n",
      "  ‚úì All YOUR_* placeholders replaced\n",
      "  ‚úì Configuration validated\n",
      "\n",
      "5. Data Upload:\n",
      "  ‚úì Sample CSV uploaded to bronze/raw/sales/raw_sales.csv\n",
      "  ‚úì File accessible via Storage Explorer\n",
      "\n",
      "======================================================================\n",
      "Once checklist complete, run:\n",
      "\n",
      "manager = Pipeline.from_yaml('config_production.yaml')\n",
      "manager.run('bronze_to_silver')\n"
     ]
    }
   ],
   "source": [
    "print(\"Production Deployment Checklist:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checklist = [\n",
    "    (\"1. Azure Storage Setup\", [\n",
    "        \"‚úì Storage account created\",\n",
    "        \"‚úì Containers: bronze, silver, gold\",\n",
    "        \"‚úì Storage keys copied\"\n",
    "    ]),\n",
    "    (\"2. Key Vault Setup\", [\n",
    "        \"‚úì Key Vault created\",\n",
    "        \"‚úì Secrets added: bronze-storage-key, silver-storage-key, gold-storage-key\",\n",
    "        \"‚úì Access policy: Get Secret permission for your identity\"\n",
    "    ]),\n",
    "    (\"3. Authentication\", [\n",
    "        \"‚úì Local: 'az login' completed\",\n",
    "        \"‚úì Databricks: Managed Identity configured\",\n",
    "        \"‚úì DefaultAzureCredential working\"\n",
    "    ]),\n",
    "    (\"4. Configuration\", [\n",
    "        \"‚úì config_production.yaml updated with real values\",\n",
    "        \"‚úì All YOUR_* placeholders replaced\",\n",
    "        \"‚úì Configuration validated\"\n",
    "    ]),\n",
    "    (\"5. Data Upload\", [\n",
    "        \"‚úì Sample CSV uploaded to bronze/raw/sales/raw_sales.csv\",\n",
    "        \"‚úì File accessible via Storage Explorer\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "for step, items in checklist:\n",
    "    print(f\"\\n{step}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Once checklist complete, run:\")\n",
    "print(\"\\nmanager = Pipeline.from_yaml('config_production.yaml')\")\n",
    "print(\"manager.run('bronze_to_silver')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running pipeline: bronze_to_silver\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úÖ SUCCESS - bronze_to_silver\n",
      "  Completed: 3 nodes\n",
      "  Failed: 0 nodes\n",
      "  Duration: 4.67s\n",
      "  Story: d:\\odibi\\walkthroughs\\pipeline_data\\stories\\bronze_to_silver_20251109_210837.md\n",
      "‚úÖ Production pipeline completed!\n",
      "\n",
      "Delta table written to:\n",
      "  abfss://silver@{account}.dfs.core.windows.net/clean/sales/sales.delta\n",
      "Uncomment code above when ready for production deployment.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when ready to run against production Azure\n",
    "\n",
    "# Load production config\n",
    "prod_manager = Pipeline.from_yaml(\"config_production.yaml\")\n",
    "\n",
    "# Run Bronze ‚Üí Silver with Key Vault auth\n",
    "result = prod_manager.run(\"bronze_to_silver\")\n",
    "\n",
    "print(\"‚úÖ Production pipeline completed!\")\n",
    "print(\"\\nDelta table written to:\")\n",
    "print(\"  abfss://silver@{account}.dfs.core.windows.net/clean/sales/sales.delta\")\n",
    "\n",
    "print(\"Uncomment code above when ready for production deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 15: Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2B Production Pipeline - Summary\n",
      "======================================================================\n",
      "\n",
      "‚úÖ What You Learned:\n",
      "\n",
      "1. YAML Configuration:\n",
      "   - All settings in version-controlled YAML\n",
      "   - No hardcoded credentials in code\n",
      "   - Separate configs for local vs production\n",
      "\n",
      "2. Key Vault Authentication (Best Practice):\n",
      "   - Credentials stored in Azure Key Vault\n",
      "   - DefaultAzureCredential for access\n",
      "   - No secrets in code or config files\n",
      "\n",
      "3. Delta Lake as Default:\n",
      "   - format='delta' for all persistent tables\n",
      "   - ACID transactions prevent partial writes\n",
      "   - Time travel for auditing\n",
      "   - Schema evolution support\n",
      "\n",
      "4. Multi-Layer Architecture:\n",
      "   - Bronze: Raw data (CSV from sources)\n",
      "   - Silver: Cleaned, validated Delta tables\n",
      "   - Gold: Aggregated, business-ready Delta tables\n",
      "\n",
      "5. Testing Strategy:\n",
      "   - Test locally first (fast, free)\n",
      "   - Validate configuration before deployment\n",
      "   - Deploy to production when local tests pass\n",
      "\n",
      "6. Maintenance Patterns:\n",
      "   - Weekly VACUUM (retention_hours=168)\n",
      "   - Time travel for auditing changes\n",
      "   - History tracking for debugging\n",
      "\n",
      "üí° Best Practices Applied:\n",
      "   ‚úì Key Vault (not direct keys)\n",
      "   ‚úì YAML config (not Python hardcoding)\n",
      "   ‚úì Delta Lake (not CSV/Parquet)\n",
      "   ‚úì Local testing (before cloud)\n",
      "   ‚úì Append mode (incremental loads)\n",
      "   ‚úì Version control (config files in git)\n",
      "   ‚úì Validation (null removal, data quality)\n",
      "   ‚úì VACUUM (storage cost optimization)\n"
     ]
    }
   ],
   "source": [
    "print(\"Phase 2B Production Pipeline - Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ What You Learned:\")\n",
    "print(\"\\n1. YAML Configuration:\")\n",
    "print(\"   - All settings in version-controlled YAML\")\n",
    "print(\"   - No hardcoded credentials in code\")\n",
    "print(\"   - Separate configs for local vs production\")\n",
    "\n",
    "print(\"\\n2. Key Vault Authentication (Best Practice):\")\n",
    "print(\"   - Credentials stored in Azure Key Vault\")\n",
    "print(\"   - DefaultAzureCredential for access\")\n",
    "print(\"   - No secrets in code or config files\")\n",
    "\n",
    "print(\"\\n3. Delta Lake as Default:\")\n",
    "print(\"   - format='delta' for all persistent tables\")\n",
    "print(\"   - ACID transactions prevent partial writes\")\n",
    "print(\"   - Time travel for auditing\")\n",
    "print(\"   - Schema evolution support\")\n",
    "\n",
    "print(\"\\n4. Multi-Layer Architecture:\")\n",
    "print(\"   - Bronze: Raw data (CSV from sources)\")\n",
    "print(\"   - Silver: Cleaned, validated Delta tables\")\n",
    "print(\"   - Gold: Aggregated, business-ready Delta tables\")\n",
    "\n",
    "print(\"\\n5. Testing Strategy:\")\n",
    "print(\"   - Test locally first (fast, free)\")\n",
    "print(\"   - Validate configuration before deployment\")\n",
    "print(\"   - Deploy to production when local tests pass\")\n",
    "\n",
    "print(\"\\n6. Maintenance Patterns:\")\n",
    "print(\"   - Weekly VACUUM (retention_hours=168)\")\n",
    "print(\"   - Time travel for auditing changes\")\n",
    "print(\"   - History tracking for debugging\")\n",
    "\n",
    "print(\"\\nüí° Best Practices Applied:\")\n",
    "print(\"   ‚úì Key Vault (not direct keys)\")\n",
    "print(\"   ‚úì YAML config (not Python hardcoding)\")\n",
    "print(\"   ‚úì Delta Lake (not CSV/Parquet)\")\n",
    "print(\"   ‚úì Local testing (before cloud)\")\n",
    "print(\"   ‚úì Append mode (incremental loads)\")\n",
    "print(\"   ‚úì Version control (config files in git)\")\n",
    "print(\"   ‚úì Validation (null removal, data quality)\")\n",
    "print(\"   ‚úì VACUUM (storage cost optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test data cleaned up\n",
      "Uncomment code above to remove test data\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to clean up local test data\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(\"./pipeline_data\", ignore_errors=True)\n",
    "Path(\"config_local.yaml\").unlink(missing_ok=True)\n",
    "Path(\"config_production.yaml\").unlink(missing_ok=True)\n",
    "print(\"‚úÖ Test data cleaned up\")\n",
    "\n",
    "print(\"Uncomment code above to remove test data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
