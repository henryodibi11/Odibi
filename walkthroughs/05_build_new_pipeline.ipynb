{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Build a New Pipeline from Scratch\n",
    "\n",
    "## üß≠ Goal\n",
    "\n",
    "Learn how to create your own ODIBI pipeline from scratch.\n",
    "\n",
    "This notebook will:\n",
    "- Guide you through pipeline design\n",
    "- Teach node structure and dependencies\n",
    "- Show transform patterns\n",
    "- Build and run a complete custom pipeline\n",
    "\n",
    "**Estimated time:** 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Environment Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Navigate to project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'walkthroughs' else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "\n",
    "from odibi.pipeline import Pipeline\n",
    "from odibi.config import PipelineConfig, ProjectConfig\n",
    "\n",
    "print(f\"‚úÖ Environment ready\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Use Case: Product Analytics Pipeline\n",
    "\n",
    "We'll build a pipeline to analyze product performance:\n",
    "- **Input:** Products CSV and Orders CSV\n",
    "- **Transform:** Join, filter, aggregate\n",
    "- **Output:** Top-selling products report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "Path(\"data/workshop/bronze\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Products data\n",
    "products = pd.DataFrame({\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headset'],\n",
    "    'category': ['Computer', 'Accessory', 'Accessory', 'Computer', 'Accessory'],\n",
    "    'price': [1200.00, 25.00, 75.00, 350.00, 120.00]\n",
    "})\n",
    "\n",
    "products.to_csv('data/workshop/bronze/products.csv', index=False)\n",
    "\n",
    "# Orders data\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': ['O001', 'O002', 'O003', 'O004', 'O005', 'O006', 'O007', 'O008'],\n",
    "    'product_id': ['P001', 'P002', 'P002', 'P003', 'P001', 'P005', 'P002', 'P004'],\n",
    "    'quantity': [2, 5, 3, 1, 1, 2, 10, 1],\n",
    "    'order_date': ['2024-01-15', '2024-01-16', '2024-01-18', '2024-01-20', \n",
    "                   '2024-01-22', '2024-01-25', '2024-01-28', '2024-02-01']\n",
    "})\n",
    "\n",
    "orders.to_csv('data/workshop/bronze/orders.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Sample data created\\n\")\n",
    "print(\"Products:\")\n",
    "display(products)\n",
    "print(\"\\nOrders:\")\n",
    "display(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 2: Design Pipeline YAML\n",
    "\n",
    "Let's build the pipeline configuration step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline configuration\n",
    "pipeline_yaml = \"\"\"\n",
    "project: Product Analytics Workshop\n",
    "engine: pandas\n",
    "\n",
    "connections:\n",
    "  local:\n",
    "    type: local\n",
    "    base_path: ./data/workshop\n",
    "\n",
    "pipelines:\n",
    "  - name: product_analytics\n",
    "    nodes:\n",
    "      # Node 1: Load products\n",
    "      - name: load_products\n",
    "        read:\n",
    "          connection: local\n",
    "          path: bronze/products.csv\n",
    "          format: csv\n",
    "        cache: true\n",
    "      \n",
    "      # Node 2: Load orders\n",
    "      - name: load_orders\n",
    "        read:\n",
    "          connection: local\n",
    "          path: bronze/orders.csv\n",
    "          format: csv\n",
    "        cache: true\n",
    "      \n",
    "      # Node 3: Join products with orders\n",
    "      - name: enrich_orders\n",
    "        depends_on: [load_products, load_orders]\n",
    "        transform:\n",
    "          steps:\n",
    "            - |\n",
    "              SELECT \n",
    "                o.order_id,\n",
    "                o.product_id,\n",
    "                p.product_name,\n",
    "                p.category,\n",
    "                p.price,\n",
    "                o.quantity,\n",
    "                p.price * o.quantity as revenue,\n",
    "                o.order_date\n",
    "              FROM load_orders o\n",
    "              LEFT JOIN load_products p ON o.product_id = p.product_id\n",
    "      \n",
    "      # Node 4: Calculate product metrics\n",
    "      - name: product_metrics\n",
    "        depends_on: [enrich_orders]\n",
    "        transform:\n",
    "          steps:\n",
    "            - |\n",
    "              SELECT \n",
    "                product_id,\n",
    "                product_name,\n",
    "                category,\n",
    "                SUM(quantity) as total_units_sold,\n",
    "                SUM(revenue) as total_revenue,\n",
    "                COUNT(DISTINCT order_id) as order_count,\n",
    "                AVG(quantity) as avg_quantity_per_order\n",
    "              FROM enrich_orders\n",
    "              GROUP BY product_id, product_name, category\n",
    "              ORDER BY total_revenue DESC\n",
    "      \n",
    "      # Node 5: Save results\n",
    "      - name: save_report\n",
    "        depends_on: [product_metrics]\n",
    "        write:\n",
    "          connection: local\n",
    "          path: silver/product_performance.parquet\n",
    "          format: parquet\n",
    "          mode: overwrite\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Pipeline Configuration Created\")\n",
    "print(\"\\nPipeline Structure:\")\n",
    "print(\"  1. load_products (read CSV)\")\n",
    "print(\"  2. load_orders (read CSV)\")\n",
    "print(\"  3. enrich_orders (join products + orders)\")\n",
    "print(\"  4. product_metrics (aggregate by product)\")\n",
    "print(\"  5. save_report (write Parquet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Step 3: Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse configuration\n",
    "config = yaml.safe_load(pipeline_yaml)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_config = PipelineConfig(**config['pipelines'][0])\n",
    "project_config = ProjectConfig(**{k: v for k, v in config.items() if k != 'pipelines'})\n",
    "\n",
    "pipeline = Pipeline.from_config(pipeline_config, project_config)\n",
    "\n",
    "print(\"üîÑ Running product analytics pipeline...\\n\")\n",
    "result = pipeline.run()\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline Status: {result.status}\")\n",
    "for node_name, node_result in result.node_results.items():\n",
    "    status_icon = \"‚úÖ\" if node_result.status == \"success\" else \"‚ùå\"\n",
    "    print(f\"   {status_icon} {node_name}: {node_result.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the product performance report\n",
    "report = pd.read_parquet('data/workshop/silver/product_performance.parquet')\n",
    "\n",
    "print(\"üìä Product Performance Report:\\n\")\n",
    "display(report)\n",
    "\n",
    "print(f\"\\nüí° Insights:\")\n",
    "print(f\"   ‚Ä¢ Top product: {report.iloc[0]['product_name']} (${report.iloc[0]['total_revenue']:.2f})\")\n",
    "print(f\"   ‚Ä¢ Total products analyzed: {len(report)}\")\n",
    "print(f\"   ‚Ä¢ Total revenue: ${report['total_revenue'].sum():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Step 5: Understand the Patterns\n",
    "\n",
    "Let's break down the key patterns used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üéØ Key Pipeline Patterns:\n",
    "\n",
    "1. **Read Nodes** (no dependencies)\n",
    "   - Load data from external sources\n",
    "   - Set `cache: true` for reuse\n",
    "\n",
    "2. **Transform Nodes** (depend on read nodes)\n",
    "   - Use SQL for joins, filters, aggregations\n",
    "   - Reference upstream nodes by name\n",
    "   - Can have multiple dependencies\n",
    "\n",
    "3. **Write Nodes** (depend on transforms)\n",
    "   - Save results to files\n",
    "   - Specify format (csv, parquet, etc.)\n",
    "   - Set mode (overwrite, append)\n",
    "\n",
    "4. **Dependencies**\n",
    "   - Use `depends_on: [node1, node2]`\n",
    "   - Creates execution order\n",
    "   - Enables parallel execution where possible\n",
    "\n",
    "5. **SQL Transforms**\n",
    "   - Full DuckDB SQL support\n",
    "   - Reference nodes as tables\n",
    "   - Use JOINs, GROUP BY, window functions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™û Reflect\n",
    "\n",
    "**What we learned:**\n",
    "- How to design a pipeline from scratch\n",
    "- Node types: read, transform, write\n",
    "- Dependency management with `depends_on`\n",
    "- SQL transforms for data manipulation\n",
    "- Running and inspecting pipeline results\n",
    "\n",
    "**Your Turn:**\n",
    "Try modifying the pipeline to:\n",
    "- Add a filter for high-revenue products only\n",
    "- Calculate category-level metrics\n",
    "- Add a date range filter\n",
    "- Write results to CSV instead of Parquet\n",
    "\n",
    "**Next Steps:**\n",
    "1. Explore more examples in `examples/getting_started/`\n",
    "2. Read [CONTRIBUTING.md](../CONTRIBUTING.md) to contribute\n",
    "3. Check [PHASES.md](../PHASES.md) for upcoming features\n",
    "4. Join the community at https://github.com/henryodibi11/Odibi\n",
    "\n",
    "**Congratulations!** üéâ You've completed all ODIBI walkthroughs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Self-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Self-Check\n",
    "try:\n",
    "    import sys, os\n",
    "    print(\"Running self-check...\")\n",
    "    \n",
    "    # Verify sample data was created\n",
    "    assert os.path.exists(\"data/workshop/bronze/products.csv\"), \"Missing products.csv\"\n",
    "    assert os.path.exists(\"data/workshop/bronze/orders.csv\"), \"Missing orders.csv\"\n",
    "    \n",
    "    # Verify pipeline ran successfully\n",
    "    assert os.path.exists(\"data/workshop/silver/product_performance.parquet\"), \"Pipeline did not create output\"\n",
    "    \n",
    "    # Verify output quality\n",
    "    import pandas as pd\n",
    "    report = pd.read_parquet(\"data/workshop/silver/product_performance.parquet\")\n",
    "    \n",
    "    assert len(report) > 0, \"Report is empty\"\n",
    "    assert 'total_revenue' in report.columns, \"Missing expected column\"\n",
    "    assert 'product_name' in report.columns, \"Missing expected column\"\n",
    "    \n",
    "    # Verify data integrity\n",
    "    assert report['total_revenue'].sum() > 0, \"Revenue calculation failed\"\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline created {len(report)} product metrics\")\n",
    "    print(f\"‚úÖ Total revenue calculated: ${report['total_revenue'].sum():.2f}\")\n",
    "    \n",
    "    print(\"üéâ Walkthrough 05 verified successfully\")\n",
    "    print(\"\\nüéì Congratulations! You've completed all ODIBI walkthroughs!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Walkthrough failed self-check: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
