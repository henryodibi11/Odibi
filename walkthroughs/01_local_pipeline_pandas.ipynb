{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Local Pipeline with Pandas\n",
    "\n",
    "## üß≠ Goal\n",
    "\n",
    "Run a complete data pipeline using ODIBI's Pandas engine.\n",
    "\n",
    "This notebook will:\n",
    "- Create sample sales data\n",
    "- Run the `example_local.yaml` pipeline\n",
    "- Transform Bronze ‚Üí Silver ‚Üí Gold layers\n",
    "- Inspect output files\n",
    "\n",
    "**Estimated time:** 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready\n",
      "üìÅ Working directory: d:\\odibi\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Environment Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Navigate to project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'walkthroughs' else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Import ODIBI\n",
    "from odibi.pipeline import Pipeline\n",
    "from odibi.config import PipelineConfig, ProjectConfig\n",
    "from odibi.connections import LocalConnection\n",
    "\n",
    "print(f\"‚úÖ Environment ready\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Create Sample Data\n",
    "\n",
    "Let's create some sample sales data for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample data created\n",
      "\n",
      "Sample data preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T001</td>\n",
       "      <td>C001</td>\n",
       "      <td>P001</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T002</td>\n",
       "      <td>C001</td>\n",
       "      <td>P002</td>\n",
       "      <td>75.5</td>\n",
       "      <td>2024-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T003</td>\n",
       "      <td>C002</td>\n",
       "      <td>P001</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2024-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T004</td>\n",
       "      <td>C002</td>\n",
       "      <td>P003</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2024-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T005</td>\n",
       "      <td>C003</td>\n",
       "      <td>P002</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2024-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T006</td>\n",
       "      <td>C001</td>\n",
       "      <td>P001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2024-02-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id customer_id product_id  amount transaction_date\n",
       "0           T001        C001       P001    50.0       2024-01-15\n",
       "1           T002        C001       P002    75.5       2024-01-20\n",
       "2           T003        C002       P001   120.0       2024-01-22\n",
       "3           T004        C002       P003    45.0       2024-01-25\n",
       "4           T005        C003       P002   200.0       2024-02-01\n",
       "5           T006        C001       P001    30.0       2024-02-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data directories\n",
    "Path(\"data/bronze\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create sample sales CSV\n",
    "sales_data = pd.DataFrame({\n",
    "    'transaction_id': ['T001', 'T002', 'T003', 'T004', 'T005', 'T006'],\n",
    "    'customer_id': ['C001', 'C001', 'C002', 'C002', 'C003', 'C001'],\n",
    "    'product_id': ['P001', 'P002', 'P001', 'P003', 'P002', 'P001'],\n",
    "    'amount': [50.00, 75.50, 120.00, 45.00, 200.00, 30.00],\n",
    "    'transaction_date': ['2024-01-15', '2024-01-20', '2024-01-22', '2024-01-25', '2024-02-01', '2024-02-05']\n",
    "})\n",
    "\n",
    "sales_data.to_csv('data/bronze/sales.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Sample data created\")\n",
    "print(\"\\nSample data preview:\")\n",
    "display(sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Run Pipeline\n",
    "\n",
    "Now let's run the Bronze ‚Üí Silver ‚Üí Gold pipeline using `example_local.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function read_csv at 0x000001B095F05080>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Pipeline configuration loaded\n",
      "   Project: Local Pandas Example\n",
      "   Engine: pandas\n",
      "   Pipelines: 2\n"
     ]
    }
   ],
   "source": [
    "# Load pipeline configuration\n",
    "with open('examples/example_local.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Pipeline configuration loaded\")\n",
    "print(f\"   Project: {config['project']}\")\n",
    "print(f\"   Engine: {config['engine']}\")\n",
    "print(f\"   Pipelines: {len(config['pipelines'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Concept: Configuration vs Runtime Objects\n",
    "\n",
    "**Key distinction in ODIBI:**\n",
    "\n",
    "- **Configuration (YAML/dicts)**: Declarative definitions of *what* should happen\n",
    "  - Defines engine type, node names, file paths, connection types\n",
    "  - Used for validation, portability, and version control\n",
    "\n",
    "- **Runtime Objects**: Executable instances that *do* the work\n",
    "  - `LocalConnection` objects perform actual file I/O operations\n",
    "  - `Pipeline` orchestrates execution and calls methods on connections\n",
    "\n",
    "**Why not use `project_config.connections` directly?**\n",
    "- It contains configuration dicts, not executable objects\n",
    "- ODIBI needs connection instances with methods like `get_path()`\n",
    "- This separation enables: (1) validation without I/O, (2) easy testing/mocking, (3) secure credential injection at runtime\n",
    "\n",
    "**In notebooks:** Always instantiate connection objects before passing to Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bronze ‚Üí Silver pipeline\n",
    "print(\"\\nüîÑ Running Bronze ‚Üí Silver pipeline...\\n\")\n",
    "\n",
    "pipeline_config = PipelineConfig(**config['pipelines'][0])\n",
    "project_config = ProjectConfig(**{k: v for k, v in config.items() if k != 'pipelines'})\n",
    "\n",
    "# Create runtime connection instances from config\n",
    "# ODIBI requires objects (e.g., LocalConnection) at runtime to perform reads/writes\n",
    "# The config dict just tells us WHAT to create, not HOW to execute I/O\n",
    "connections = {\n",
    "    'local': LocalConnection(base_path='./data')\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(\n",
    "    pipeline_config=pipeline_config,\n",
    "    engine=project_config.engine,\n",
    "    connections=connections\n",
    ")\n",
    "results = pipeline.run()\n",
    "\n",
    "# Check results\n",
    "print(f\"\\n‚úÖ Pipeline completed\")\n",
    "print(f\"   Completed nodes: {len(results.completed)}\")\n",
    "print(f\"   Failed nodes: {len(results.failed)}\")\n",
    "print(f\"   Nodes: {results.completed}\")\n",
    "\n",
    "# Debug tip: If pipeline fails, inspect failures\n",
    "if results.failed:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed nodes detected:\")\n",
    "    for node_name in results.failed:\n",
    "        node_result = results.get_node_result(node_name)\n",
    "        if node_result and node_result.error:\n",
    "            print(f\"   {node_name}: {node_result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Silver ‚Üí Gold pipeline\n",
    "print(\"\\nüîÑ Running Silver ‚Üí Gold pipeline...\\n\")\n",
    "\n",
    "pipeline_config = PipelineConfig(**config['pipelines'][1])\n",
    "pipeline = Pipeline(\n",
    "    pipeline_config=pipeline_config,\n",
    "    engine=project_config.engine,\n",
    "    connections=connections  # Reuse connection objects from above\n",
    ")\n",
    "results = pipeline.run()\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline completed\")\n",
    "print(f\"   Completed nodes: {len(results.completed)}\")\n",
    "print(f\"   Failed nodes: {len(results.failed)}\")\n",
    "print(f\"   Nodes: {results.completed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° How SQL Works with the Pandas Engine\n",
    "\n",
    "**You might wonder:** How can we use SQL with `engine='pandas'`?\n",
    "\n",
    "**Answer:** ODIBI uses [DuckDB](https://duckdb.org/) to run SQL queries over in-memory Pandas DataFrames:\n",
    "- Each node's output is registered as a SQL view using the **node name**\n",
    "- In the pipeline YAML, you can reference upstream nodes directly in SQL (e.g., `FROM load_raw_sales`)\n",
    "- DuckDB translates SQL to DataFrame operations automatically\n",
    "\n",
    "**Example from `example_local.yaml`:**\n",
    "```sql\n",
    "SELECT transaction_id, customer_id, amount\n",
    "FROM load_raw_sales  -- ‚Üê This is the upstream node name!\n",
    "WHERE amount > 0\n",
    "```\n",
    "\n",
    "This is why node naming is important - they become your SQL table names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Inspect Outputs\n",
    "\n",
    "Let's examine the data at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Bronze layer (original CSV)\n",
    "bronze_data = pd.read_csv('data/bronze/sales.csv')\n",
    "print(\"üìÅ Bronze Layer (Raw Data):\")\n",
    "print(f\"   Rows: {len(bronze_data)}\")\n",
    "display(bronze_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Silver layer (cleaned Parquet)\n",
    "silver_data = pd.read_parquet('data/silver/sales.parquet')\n",
    "print(\"\\nüìÅ Silver Layer (Cleaned Data):\")\n",
    "print(f\"   Rows: {len(silver_data)}\")\n",
    "print(f\"   Columns: {list(silver_data.columns)}\")\n",
    "display(silver_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Gold layer (aggregated analytics)\n",
    "gold_data = pd.read_parquet('data/gold/customer_summary.parquet')\n",
    "print(\"\\nüìÅ Gold Layer (Customer Analytics):\")\n",
    "print(f\"   Rows: {len(gold_data)}\")\n",
    "print(f\"   Columns: {list(gold_data.columns)}\")\n",
    "display(gold_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "**Common issues and solutions:**\n",
    "\n",
    "| Error | Cause | Solution |\n",
    "|-------|-------|----------|\n",
    "| `TypeError: expected Connection, got dict` | Passing `project_config.connections` (raw dicts) to Pipeline | Create `LocalConnection` objects (see Config vs Runtime section above) |\n",
    "| `FileNotFoundError: data/silver/sales.parquet` | Wrong working directory or pipeline failed | Re-run Setup cell to set working directory; check `results.failed` for errors |\n",
    "| `ImportError: Missing optional dependency 'pyarrow'` | Parquet library not installed | Run: `pip install pyarrow` |\n",
    "| `KeyError: 'load_raw_sales'` in SQL | Node name mismatch in dependencies or SQL | Ensure SQL table names match upstream node names exactly |\n",
    "| `AttributeError: 'dict' object has no attribute 'get_path'` | Connection objects not instantiated | See Config vs Runtime section - use `LocalConnection()` not raw dicts |\n",
    "| Pipeline runs but no output files | Pipeline node failed silently | Check `results.failed` and inspect node errors (see debug code above) |\n",
    "\n",
    "**Debug checklist:**\n",
    "1. ‚úÖ Re-run Setup cell to ensure correct working directory\n",
    "2. ‚úÖ Check `results.failed` for any failed nodes\n",
    "3. ‚úÖ Verify `connections` uses `LocalConnection()` objects, not `project_config.connections`\n",
    "4. ‚úÖ Ensure bronze data exists: `data/bronze/sales.csv`\n",
    "5. ‚úÖ Install dependencies: `pip install pyarrow pyyaml pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™û Reflect\n",
    "\n",
    "**What we learned:**\n",
    "- Created sample data programmatically\n",
    "- Ran a multi-layer pipeline (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "- Transformed CSV to Parquet format\n",
    "- Applied SQL-based filtering and aggregation\n",
    "- Inspected outputs at each layer\n",
    "\n",
    "**Key concepts:**\n",
    "- **Bronze:** Raw data, minimal processing\n",
    "- **Silver:** Cleaned, validated, ready for analysis\n",
    "- **Gold:** Business-level aggregates and metrics\n",
    "\n",
    "**Next step:**  \n",
    "Go to **`02_cli_and_testing.ipynb`** to learn about CLI tools and testing (Phase 2 preview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Self-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Self-Check\n",
    "try:\n",
    "    import sys, os\n",
    "    print(\"Running self-check...\")\n",
    "    \n",
    "    # Verify example config exists\n",
    "    assert os.path.exists(\"examples/example_local.yaml\"), \"Missing example_local.yaml\"\n",
    "    \n",
    "    # Verify data layers were created\n",
    "    assert os.path.exists(\"data/bronze/sales.csv\"), \"Missing Bronze layer\"\n",
    "    assert os.path.exists(\"data/silver/sales.parquet\"), \"Missing Silver layer\"\n",
    "    assert os.path.exists(\"data/gold/customer_summary.parquet\"), \"Missing Gold layer\"\n",
    "    \n",
    "    # Verify data integrity\n",
    "    import pandas as pd\n",
    "    gold = pd.read_parquet(\"data/gold/customer_summary.parquet\")\n",
    "    assert len(gold) > 0, \"Gold layer has no data\"\n",
    "    assert 'total_spent' in gold.columns, \"Missing expected column in Gold layer\"\n",
    "    \n",
    "    print(\"‚úÖ Data pipeline ran successfully\")\n",
    "    print(f\"   Bronze: {len(pd.read_csv('data/bronze/sales.csv'))} rows\")\n",
    "    print(f\"   Silver: {len(pd.read_parquet('data/silver/sales.parquet'))} rows\")\n",
    "    print(f\"   Gold: {len(gold)} customers\")\n",
    "    \n",
    "    print(\"üéâ Walkthrough 01 verified successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Walkthrough failed self-check: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
