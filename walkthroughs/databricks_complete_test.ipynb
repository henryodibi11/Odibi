{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIBI Complete Databricks Test - Phase 2 Validation\n",
    "\n",
    "**Purpose:** Validate all Phase 2 features in Databricks environment\n",
    "\n",
    "**What This Tests:**\n",
    "- ‚úÖ Databricks environment validation\n",
    "- ‚úÖ PandasEngine with local data\n",
    "- ‚úÖ SparkEngine with DBFS\n",
    "- ‚úÖ Delta Lake read/write/history/vacuum\n",
    "- ‚úÖ Parallel Key Vault fetching (if configured)\n",
    "- ‚úÖ Multi-account connections\n",
    "- ‚úÖ Complete pipeline execution\n",
    "\n",
    "**Cleanup:** All test data is removed at the end\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install ODIBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install odibi[spark,pandas,azure] --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Validate Databricks Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.utils import validate_databricks_environment\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATABRICKS ENVIRONMENT VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "env_info = validate_databricks_environment(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if env_info[\"is_databricks\"] and env_info[\"spark_available\"]:\n",
    "    print(\"‚úÖ Environment ready for testing!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some features may not work correctly\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Test Environment\n",
    "\n",
    "Define test paths and create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# Test paths (using DBFS)\n",
    "TEST_BASE_PATH = \"/dbfs/tmp/odibi_test\"\n",
    "TEST_BRONZE_PATH = f\"{TEST_BASE_PATH}/bronze\"\n",
    "TEST_SILVER_PATH = f\"{TEST_BASE_PATH}/silver\"\n",
    "TEST_GOLD_PATH = f\"{TEST_BASE_PATH}/gold\"\n",
    "TEST_DELTA_PATH = f\"{TEST_BASE_PATH}/delta\"\n",
    "\n",
    "print(\"üìÅ Test paths configured:\")\n",
    "print(f\"  Base: {TEST_BASE_PATH}\")\n",
    "print(f\"  Bronze: {TEST_BRONZE_PATH}\")\n",
    "print(f\"  Silver: {TEST_SILVER_PATH}\")\n",
    "print(f\"  Gold: {TEST_GOLD_PATH}\")\n",
    "print(f\"  Delta: {TEST_DELTA_PATH}\")\n",
    "\n",
    "# Create test data\n",
    "test_data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"department\": [\"Engineering\", \"Sales\", \"Engineering\", \"HR\", \"Sales\"],\n",
    "    \"salary\": [100000, 80000, 95000, 70000, 85000],\n",
    "    \"hire_date\": pd.to_datetime([\"2020-01-15\", \"2021-03-20\", \"2019-07-10\", \"2022-02-01\", \"2020-11-05\"])\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Test data created:\")\n",
    "print(test_data)\n",
    "print(f\"\\nShape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test PandasEngine with Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.engine import PandasEngine\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: PandasEngine - Local CSV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(TEST_BRONZE_PATH, exist_ok=True)\n",
    "\n",
    "# Initialize engine\n",
    "pandas_engine = PandasEngine()\n",
    "\n",
    "# Write CSV\n",
    "csv_path = f\"{TEST_BRONZE_PATH}/employees.csv\"\n",
    "pandas_engine.write(test_data, csv_path, format=\"csv\")\n",
    "print(f\"‚úì Written to: {csv_path}\")\n",
    "\n",
    "# Read back\n",
    "df_read = pandas_engine.read(csv_path, format=\"csv\")\n",
    "print(f\"‚úì Read back: {df_read.shape}\")\n",
    "print(df_read.head())\n",
    "\n",
    "assert df_read.shape == test_data.shape, \"Shape mismatch!\"\n",
    "print(\"\\n‚úÖ PandasEngine CSV test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test PandasEngine with Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 2: PandasEngine - Parquet\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Write Parquet\n",
    "parquet_path = f\"{TEST_BRONZE_PATH}/employees.parquet\"\n",
    "pandas_engine.write(test_data, parquet_path, format=\"parquet\")\n",
    "print(f\"‚úì Written to: {parquet_path}\")\n",
    "\n",
    "# Read back\n",
    "df_parquet = pandas_engine.read(parquet_path, format=\"parquet\")\n",
    "print(f\"‚úì Read back: {df_parquet.shape}\")\n",
    "print(df_parquet.head())\n",
    "\n",
    "assert df_parquet.shape == test_data.shape, \"Shape mismatch!\"\n",
    "print(\"\\n‚úÖ PandasEngine Parquet test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Delta Lake with PandasEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 3: PandasEngine - Delta Lake\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "delta_path = f\"{TEST_DELTA_PATH}/employees_delta\"\n",
    "\n",
    "# Write Delta table\n",
    "pandas_engine.write(test_data, delta_path, format=\"delta\", mode=\"overwrite\")\n",
    "print(f\"‚úì Written Delta table to: {delta_path}\")\n",
    "\n",
    "# Read back\n",
    "df_delta = pandas_engine.read(delta_path, format=\"delta\")\n",
    "print(f\"‚úì Read back: {df_delta.shape}\")\n",
    "print(df_delta.head())\n",
    "\n",
    "# Append more data\n",
    "new_data = pd.DataFrame({\n",
    "    \"id\": [6, 7],\n",
    "    \"name\": [\"Frank\", \"Grace\"],\n",
    "    \"department\": [\"Engineering\", \"Sales\"],\n",
    "    \"salary\": [92000, 88000],\n",
    "    \"hire_date\": pd.to_datetime([\"2023-01-15\", \"2023-03-20\"])\n",
    "})\n",
    "\n",
    "pandas_engine.write(new_data, delta_path, format=\"delta\", mode=\"append\")\n",
    "print(\"\\n‚úì Appended 2 new rows\")\n",
    "\n",
    "# Read all data\n",
    "df_all = pandas_engine.read(delta_path, format=\"delta\")\n",
    "print(f\"‚úì Total rows after append: {len(df_all)}\")\n",
    "\n",
    "assert len(df_all) == 7, \"Expected 7 rows after append!\"\n",
    "print(\"\\n‚úÖ Delta Lake test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test SparkEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.engine import SparkEngine\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 4: SparkEngine - Parquet\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize SparkEngine\n",
    "spark_engine = SparkEngine(spark_session=spark)\n",
    "\n",
    "# Convert test data to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(test_data)\n",
    "\n",
    "# Write with Spark\n",
    "spark_parquet_path = f\"dbfs:{TEST_SILVER_PATH}/employees_spark.parquet\"\n",
    "spark_engine.write(spark_df, spark_parquet_path, format=\"parquet\", mode=\"overwrite\")\n",
    "print(f\"‚úì Written to: {spark_parquet_path}\")\n",
    "\n",
    "# Read back\n",
    "df_spark = spark_engine.read(spark_parquet_path, format=\"parquet\")\n",
    "print(f\"‚úì Read back: {df_spark.count()} rows\")\n",
    "df_spark.show(5)\n",
    "\n",
    "assert df_spark.count() == 5, \"Expected 5 rows!\"\n",
    "print(\"‚úÖ SparkEngine Parquet test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test SparkEngine with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 5: SparkEngine - Delta Lake\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "delta_spark_path = f\"dbfs:{TEST_DELTA_PATH}/employees_spark_delta\"\n",
    "\n",
    "# Write Delta with Spark\n",
    "spark_engine.write(spark_df, delta_spark_path, format=\"delta\", mode=\"overwrite\")\n",
    "print(f\"‚úì Written Delta table to: {delta_spark_path}\")\n",
    "\n",
    "# Read back\n",
    "df_delta_spark = spark_engine.read(delta_spark_path, format=\"delta\")\n",
    "print(f\"‚úì Read back: {df_delta_spark.count()} rows\")\n",
    "df_delta_spark.show(5)\n",
    "\n",
    "# Test SQL transform\n",
    "result = spark_engine.execute_sql(\n",
    "    \"SELECT department, AVG(salary) as avg_salary FROM employees GROUP BY department\",\n",
    "    {\"employees\": df_delta_spark}\n",
    ")\n",
    "print(\"\\n‚úì SQL Transform executed:\")\n",
    "result.show()\n",
    "\n",
    "assert result.count() > 0, \"SQL transform failed!\"\n",
    "print(\"‚úÖ SparkEngine Delta Lake test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Parallel Key Vault Fetching (If Configured)\n",
    "\n",
    "**Note:** This will use direct_key mode for testing. In production, use Key Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.connections import AzureADLS\n",
    "from odibi.utils import configure_connections_parallel\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 6: Parallel Connection Configuration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test connections (using direct_key for demo)\n",
    "test_connections = {\n",
    "    \"bronze\": AzureADLS(\n",
    "        account=\"teststorage1\",\n",
    "        container=\"bronze\",\n",
    "        auth_mode=\"direct_key\",\n",
    "        account_key=\"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n",
    "        validate=True\n",
    "    ),\n",
    "    \"silver\": AzureADLS(\n",
    "        account=\"teststorage2\",\n",
    "        container=\"silver\",\n",
    "        auth_mode=\"direct_key\",\n",
    "        account_key=\"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n",
    "        validate=True\n",
    "    ),\n",
    "    \"gold\": AzureADLS(\n",
    "        account=\"teststorage3\",\n",
    "        container=\"gold\",\n",
    "        auth_mode=\"direct_key\",\n",
    "        account_key=\"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n",
    "        validate=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"üìä Testing with {len(test_connections)} connections\\n\")\n",
    "\n",
    "# Configure in parallel\n",
    "start = time.time()\n",
    "configured, errors = configure_connections_parallel(\n",
    "    test_connections,\n",
    "    prefetch_secrets=True,\n",
    "    max_workers=5,\n",
    "    timeout=30.0,\n",
    "    verbose=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Configuration completed in {elapsed:.3f}s\")\n",
    "print(f\"‚úì Errors: {len(errors)}\")\n",
    "\n",
    "assert len(errors) == 0, f\"Configuration errors: {errors}\"\n",
    "print(\"\\n‚úÖ Parallel configuration test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test Complete Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.config import ProjectConfig\n",
    "from odibi.pipeline import Pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 7: Complete Pipeline Execution\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create pipeline config\n",
    "pipeline_config = {\n",
    "    \"name\": \"test_pipeline\",\n",
    "    \"description\": \"Test pipeline for validation\",\n",
    "    \"nodes\": [\n",
    "        {\n",
    "            \"name\": \"load_data\",\n",
    "            \"read\": {\n",
    "                \"path\": csv_path,\n",
    "                \"format\": \"csv\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"transform_data\",\n",
    "            \"depends_on\": [\"load_data\"],\n",
    "            \"transform\": {\n",
    "                \"sql\": \"SELECT * FROM load_data WHERE salary > 80000\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"save_results\",\n",
    "            \"depends_on\": [\"transform_data\"],\n",
    "            \"write\": {\n",
    "                \"path\": f\"{TEST_GOLD_PATH}/high_earners.parquet\",\n",
    "                \"format\": \"parquet\",\n",
    "                \"mode\": \"overwrite\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "project_config = {\n",
    "    \"engine\": \"pandas\",\n",
    "    \"story\": {\"enabled\": False},\n",
    "    \"connections\": {},\n",
    "    \"pipelines\": [pipeline_config]\n",
    "}\n",
    "\n",
    "config = ProjectConfig(**project_config)\n",
    "pipeline = Pipeline(config.pipelines[0], engine=pandas_engine)\n",
    "\n",
    "# Run pipeline\n",
    "print(\"\\nüîÑ Running pipeline...\\n\")\n",
    "results = pipeline.run()\n",
    "\n",
    "print(\"\\nüìä Pipeline Results:\")\n",
    "for node_name, result in results.items():\n",
    "    print(f\"  {node_name}: {result.status}\")\n",
    "\n",
    "# Verify results\n",
    "assert all(r.status == \"success\" for r in results.values()), \"Some nodes failed!\"\n",
    "\n",
    "# Check output file\n",
    "output_df = pandas_engine.read(f\"{TEST_GOLD_PATH}/high_earners.parquet\", format=\"parquet\")\n",
    "print(f\"\\n‚úì Output file contains {len(output_df)} rows\")\n",
    "print(output_df)\n",
    "\n",
    "assert len(output_df) == 4, \"Expected 4 high earners!\"\n",
    "print(\"\\n‚úÖ Pipeline execution test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ All tests PASSED!\\n\")\n",
    "print(\"Tests completed:\")\n",
    "print(\"  1. ‚úì PandasEngine CSV\")\n",
    "print(\"  2. ‚úì PandasEngine Parquet\")\n",
    "print(\"  3. ‚úì PandasEngine Delta Lake\")\n",
    "print(\"  4. ‚úì SparkEngine Parquet\")\n",
    "print(\"  5. ‚úì SparkEngine Delta Lake + SQL\")\n",
    "print(\"  6. ‚úì Parallel Connection Configuration\")\n",
    "print(\"  7. ‚úì Complete Pipeline Execution\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ ODIBI Phase 2 validation complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Cleanup - Remove All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Remove all test data\n",
    "    if os.path.exists(TEST_BASE_PATH):\n",
    "        shutil.rmtree(TEST_BASE_PATH)\n",
    "        print(f\"‚úì Removed: {TEST_BASE_PATH}\")\n",
    "    \n",
    "    # Also clean using dbutils if available\n",
    "    try:\n",
    "        dbutils.fs.rm(f\"dbfs:{TEST_BASE_PATH}\", recurse=True)\n",
    "        print(f\"‚úì Removed from DBFS: {TEST_BASE_PATH}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\n‚úÖ All test data cleaned up!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Cleanup error: {e}\")\n",
    "    print(\"   You may need to manually remove: \" + TEST_BASE_PATH)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüéä Testing complete! All data cleaned up.\")\n",
    "print(\"\\nODIBI Phase 2 is production-ready! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
