{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2A Walkthrough: Azure ADLS + Key Vault Authentication\n",
    "\n",
    "**Purpose:** Test Phase 2A implementation with real Azure ADLS connections\n",
    "\n",
    "**What we'll test:**\n",
    "1. ‚úÖ Direct key authentication (local dev)\n",
    "2. ‚úÖ Key Vault authentication (production pattern)\n",
    "3. ‚úÖ Multi-account connections\n",
    "4. ‚úÖ Reading CSV from ADLS\n",
    "5. ‚úÖ Writing Parquet to ADLS\n",
    "6. ‚úÖ Both PandasEngine and SparkEngine\n",
    "\n",
    "**Prerequisites:**\n",
    "- Azure storage account with some test data\n",
    "- Storage account key OR Azure Key Vault access\n",
    "- For Spark: PySpark installed (`pip install pyspark`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'C:\\Users\\hodibi\\OneDrive - Ingredion\\Desktop\\Repos\\Odibi')\n",
    "\n",
    "# Verify it worked\n",
    "import odibi\n",
    "print(f\"‚úÖ ODIBI loaded from: {odibi.__file__}\")\n",
    "# Or manually:\n",
    "%pip install azure-identity azure-keyvault-secrets adlfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Replace these values with your actual Azure resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "   Storage Account 1: ingrglobaldigitalopsteam/example-container\n",
      "   Storage Account 2: ingrglobaldigitalopsteam/example-container\n",
      "   Auth Mode: key_vault\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURE YOUR AZURE RESOURCES HERE =====\n",
    "\n",
    "# Storage Account 1 (Bronze/Source)\n",
    "STORAGE_ACCOUNT_1 = \"mystorageaccount1\"  # Replace with your storage account name\n",
    "CONTAINER_1 = \"bronze\"  # Replace with your container name\n",
    "\n",
    "# Storage Account 2 (Silver/Target) - Optional, can use same account\n",
    "STORAGE_ACCOUNT_2 = \"mystorageaccount2\"  # Replace or set same as STORAGE_ACCOUNT_1\n",
    "CONTAINER_2 = \"silver\"  # Replace with your container name\n",
    "\n",
    "# Authentication Mode: Choose one\n",
    "AUTH_MODE = \"direct_key\"  # or \"key_vault\"\n",
    "\n",
    "# For direct_key mode: Set your storage account keys\n",
    "import os\n",
    "ACCOUNT_KEY_1 = os.getenv(\"STORAGE_KEY_1\", \"your-storage-key-1-here\")\n",
    "ACCOUNT_KEY_2 = os.getenv(\"STORAGE_KEY_2\", \"your-storage-key-2-here\")\n",
    "\n",
    "# For key_vault mode: Set your Key Vault details\n",
    "KEY_VAULT_NAME = \"your-keyvault-name\"\n",
    "SECRET_NAME_1 = \"bronze-storage-key\"\n",
    "SECRET_NAME_2 = \"silver-storage-key\"\n",
    "\n",
    "# Test data paths\n",
    "TEST_CSV_PATH = \"test/sample_data.csv\"  # Path to existing CSV in your storage\n",
    "OUTPUT_PARQUET_PATH = \"test/output/sample_data.parquet\"  # Where to write output\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Storage Account 1: {STORAGE_ACCOUNT_1}/{CONTAINER_1}\")\n",
    "print(f\"   Storage Account 2: {STORAGE_ACCOUNT_2}/{CONTAINER_2}\")\n",
    "print(f\"   Auth Mode: {AUTH_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Create Sample Test Data Locally\n",
    "\n",
    "First, let's create some sample data to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "dates = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(100)]\n",
    "sample_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'product': np.random.choice(['Widget A', 'Widget B', 'Widget C'], 100),\n",
    "    'quantity': np.random.randint(1, 100, 100),\n",
    "    'price': np.round(np.random.uniform(10, 500, 100), 2),\n",
    "    'customer': [f'Customer_{i%10}' for i in range(100)]\n",
    "})\n",
    "\n",
    "sample_data['amount'] = sample_data['quantity'] * sample_data['price']\n",
    "\n",
    "print(f\"‚úÖ Created sample data with {len(sample_data)} rows\")\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Direct Key Authentication - Write Test Data to ADLS\n",
    "\n",
    "Upload our sample data to ADLS using direct key authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created Bronze connection: ingrglobaldigitalopsteam/example-container\n",
      "   Writing to: abfss://example-container@ingrglobaldigitalopsteam.dfs.core.windows.net/test/sample_data.csv\n",
      "‚úÖ Successfully wrote 100 rows to ADLS\n"
     ]
    }
   ],
   "source": [
    "from odibi.connections.azure_adls import AzureADLS\n",
    "\n",
    "# Create connection with direct key\n",
    "bronze_conn = AzureADLS(\n",
    "    account=STORAGE_ACCOUNT_1,\n",
    "    container=CONTAINER_1,\n",
    "    auth_mode=\"direct_key\",\n",
    "    account_key=ACCOUNT_KEY_1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created Bronze connection: {bronze_conn.account}/{bronze_conn.container}\")\n",
    "\n",
    "# Get the full ADLS URI\n",
    "test_csv_uri = bronze_conn.uri(TEST_CSV_PATH)\n",
    "print(f\"   Writing to: {test_csv_uri}\")\n",
    "\n",
    "# Write sample data to ADLS\n",
    "storage_options = bronze_conn.pandas_storage_options()\n",
    "sample_data.to_csv(test_csv_uri, index=False, storage_options=storage_options)\n",
    "\n",
    "print(f\"‚úÖ Successfully wrote {len(sample_data)} rows to ADLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: PandasEngine - Read from ADLS\n",
    "\n",
    "Test reading the CSV we just uploaded using PandasEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully read 100 rows from ADLS\n",
      "   Columns: ['date', 'product', 'quantity', 'price', 'customer', 'amount']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>product</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>customer</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>Widget C</td>\n",
       "      <td>8</td>\n",
       "      <td>188.18</td>\n",
       "      <td>Customer_0</td>\n",
       "      <td>1505.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Widget A</td>\n",
       "      <td>88</td>\n",
       "      <td>486.17</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>42782.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>Widget C</td>\n",
       "      <td>63</td>\n",
       "      <td>481.60</td>\n",
       "      <td>Customer_2</td>\n",
       "      <td>30340.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>Widget C</td>\n",
       "      <td>11</td>\n",
       "      <td>133.37</td>\n",
       "      <td>Customer_3</td>\n",
       "      <td>1467.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>Widget A</td>\n",
       "      <td>81</td>\n",
       "      <td>253.65</td>\n",
       "      <td>Customer_4</td>\n",
       "      <td>20545.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   product  quantity   price    customer    amount\n",
       "0  2024-01-01  Widget C         8  188.18  Customer_0   1505.44\n",
       "1  2024-01-02  Widget A        88  486.17  Customer_1  42782.96\n",
       "2  2024-01-03  Widget C        63  481.60  Customer_2  30340.80\n",
       "3  2024-01-04  Widget C        11  133.37  Customer_3   1467.07\n",
       "4  2024-01-05  Widget A        81  253.65  Customer_4  20545.65"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odibi.engine.pandas_engine import PandasEngine\n",
    "\n",
    "# Create Pandas engine\n",
    "pandas_engine = PandasEngine()\n",
    "\n",
    "# Read from ADLS\n",
    "df_read = pandas_engine.read(\n",
    "    connection=bronze_conn,\n",
    "    format=\"csv\",\n",
    "    path=TEST_CSV_PATH\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Successfully read {len(df_read)} rows from ADLS\")\n",
    "print(f\"   Columns: {list(df_read.columns)}\")\n",
    "df_read.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: PandasEngine - Write Parquet to Different Account\n",
    "\n",
    "Test multi-account support by writing to a different storage account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created Silver connection: ingrglobaldigitalopsteam/example-container\n",
      "‚úÖ Successfully wrote Parquet to: abfss://example-container@ingrglobaldigitalopsteam.dfs.core.windows.net/test/output/sample_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create second connection (silver account)\n",
    "silver_conn = AzureADLS(\n",
    "    account=STORAGE_ACCOUNT_2,\n",
    "    container=CONTAINER_2,\n",
    "    auth_mode=\"direct_key\",\n",
    "    account_key=ACCOUNT_KEY_2\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created Silver connection: {silver_conn.account}/{silver_conn.container}\")\n",
    "\n",
    "# Write to silver as Parquet\n",
    "pandas_engine.write(\n",
    "    df=df_read,\n",
    "    connection=silver_conn,\n",
    "    format=\"parquet\",\n",
    "    path=OUTPUT_PARQUET_PATH,\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "output_uri = silver_conn.uri(OUTPUT_PARQUET_PATH)\n",
    "print(f\"‚úÖ Successfully wrote Parquet to: {output_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Verify Multi-Account Write\n",
    "\n",
    "Read back the Parquet file to verify the multi-account write worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully read 100 rows from Silver account\n",
      "   Original rows: 100\n",
      "   Parquet rows: 100\n",
      "‚úÖ Row counts match - multi-account write successful!\n"
     ]
    }
   ],
   "source": [
    "# Read back from silver account\n",
    "df_parquet = pandas_engine.read(\n",
    "    connection=silver_conn,\n",
    "    format=\"parquet\",\n",
    "    path=OUTPUT_PARQUET_PATH\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Successfully read {len(df_parquet)} rows from Silver account\")\n",
    "print(f\"   Original rows: {len(sample_data)}\")\n",
    "print(f\"   Parquet rows: {len(df_parquet)}\")\n",
    "assert len(df_parquet) == len(sample_data), \"Row count mismatch!\"\n",
    "print(\"‚úÖ Row counts match - multi-account write successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Key Vault Authentication (Optional)\n",
    "\n",
    "Test Key Vault authentication mode (requires Azure CLI login or managed identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test Key Vault mode\n",
    "# NOTE: Requires Azure CLI auth or running in Databricks with managed identity\n",
    "\n",
    "# try:\n",
    "#     bronze_kv = AzureADLS(\n",
    "#         account=STORAGE_ACCOUNT_1,\n",
    "#         container=CONTAINER_1,\n",
    "#         auth_mode=\"key_vault\",\n",
    "#         key_vault_name=KEY_VAULT_NAME,\n",
    "#         secret_name=SECRET_NAME_1\n",
    "#     )\n",
    "#     \n",
    "#     # Try to fetch key (this will use DefaultAzureCredential)\n",
    "#     key = bronze_kv.get_storage_key()\n",
    "#     print(f\"‚úÖ Successfully retrieved key from Key Vault: {KEY_VAULT_NAME}\")\n",
    "#     print(f\"   Key length: {len(key)} characters\")\n",
    "#     \n",
    "#     # Test read with Key Vault auth\n",
    "#     df_kv = pandas_engine.read(\n",
    "#         connection=bronze_kv,\n",
    "#         format=\"csv\",\n",
    "#         path=TEST_CSV_PATH\n",
    "#     )\n",
    "#     print(f\"‚úÖ Successfully read {len(df_kv)} rows using Key Vault auth\")\n",
    "#     \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Key Vault test failed: {e}\")\n",
    "#     print(\"   Make sure you're authenticated (az login) or running in Databricks\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Key Vault test commented out - uncomment to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: SparkEngine with Multi-Account (Optional)\n",
    "\n",
    "Test Spark engine with multiple storage accounts configured upfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test Spark engine\n",
    "# NOTE: Requires PySpark installation and Java\n",
    "\n",
    "# try:\n",
    "#     from odibi.engine.spark_engine import SparkEngine\n",
    "#     \n",
    "#     # Create connections dict\n",
    "#     connections = {\n",
    "#         'bronze': bronze_conn,\n",
    "#         'silver': silver_conn\n",
    "#     }\n",
    "#     \n",
    "#     # Create Spark engine (will configure all connections)\n",
    "#     spark_engine = SparkEngine(connections=connections)\n",
    "#     print(\"‚úÖ SparkEngine created with multi-account configuration\")\n",
    "#     \n",
    "#     # Read CSV with Spark\n",
    "#     spark_df = spark_engine.read(\n",
    "#         connection=bronze_conn,\n",
    "#         format=\"csv\",\n",
    "#         path=TEST_CSV_PATH,\n",
    "#         options={\"header\": \"true\", \"inferSchema\": \"true\"}\n",
    "#     )\n",
    "#     \n",
    "#     print(f\"‚úÖ Read {spark_df.count()} rows with Spark\")\n",
    "#     spark_df.show(5)\n",
    "#     \n",
    "#     # Write to silver with Spark\n",
    "#     spark_engine.write(\n",
    "#         df=spark_df,\n",
    "#         connection=silver_conn,\n",
    "#         format=\"parquet\",\n",
    "#         path=\"test/output/spark_output.parquet\",\n",
    "#         mode=\"overwrite\"\n",
    "#     )\n",
    "#     print(\"‚úÖ Successfully wrote with Spark to different account\")\n",
    "#     \n",
    "# except ImportError:\n",
    "#     print(\"‚ö†Ô∏è  PySpark not installed - skipping Spark tests\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Spark test failed: {e}\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Spark test commented out - uncomment to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Validation Tests\n",
    "\n",
    "Test that validation works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation caught missing key_vault_name: key_vault mode requires 'key_vault_name' and 'secret_name' for connection to test/test\n",
      "‚úÖ Validation caught missing account_key: direct_key mode requires 'account_key' for connection to test/test\n",
      "‚úÖ Validation caught invalid auth_mode: Unsupported auth_mode: 'invalid_mode'. Use 'key_vault' or 'direct_key'.\n",
      "\n",
      "‚úÖ All validation tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Missing key_vault_name should fail\n",
    "try:\n",
    "    bad_conn = AzureADLS(\n",
    "        account=\"test\",\n",
    "        container=\"test\",\n",
    "        auth_mode=\"key_vault\",\n",
    "        secret_name=\"test-secret\"\n",
    "        # Missing key_vault_name\n",
    "    )\n",
    "    print(\"‚ùå Should have raised ValueError\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Validation caught missing key_vault_name: {e}\")\n",
    "\n",
    "# Test 2: Missing account_key should fail\n",
    "try:\n",
    "    bad_conn = AzureADLS(\n",
    "        account=\"test\",\n",
    "        container=\"test\",\n",
    "        auth_mode=\"direct_key\"\n",
    "        # Missing account_key\n",
    "    )\n",
    "    print(\"‚ùå Should have raised ValueError\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Validation caught missing account_key: {e}\")\n",
    "\n",
    "# Test 3: Invalid auth_mode should fail\n",
    "try:\n",
    "    bad_conn = AzureADLS(\n",
    "        account=\"test\",\n",
    "        container=\"test\",\n",
    "        auth_mode=\"invalid_mode\"\n",
    "    )\n",
    "    print(\"‚ùå Should have raised ValueError\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Validation caught invalid auth_mode: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All validation tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Phase 2A Walkthrough - Test Summary\n",
      "============================================================\n",
      "\n",
      "‚úÖ Direct key authentication - PASSED\n",
      "‚úÖ PandasEngine read from ADLS - PASSED\n",
      "‚úÖ Multi-account connection - PASSED\n",
      "‚úÖ Write Parquet to different account - PASSED\n",
      "‚úÖ Validation tests - PASSED\n",
      "\n",
      "Optional tests (commented out):\n",
      "   - Key Vault authentication\n",
      "   - SparkEngine with multi-account\n",
      "\n",
      "Files created in ADLS:\n",
      "   - abfss://example-container@ingrglobaldigitalopsteam.dfs.core.windows.net/test/sample_data.csv\n",
      "   - abfss://example-container@ingrglobaldigitalopsteam.dfs.core.windows.net/test/output/sample_data.parquet\n",
      "\n",
      "üéâ Phase 2A implementation validated with real ADLS connections!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Phase 2A Walkthrough - Test Summary\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"‚úÖ Direct key authentication - PASSED\")\n",
    "print(\"‚úÖ PandasEngine read from ADLS - PASSED\")\n",
    "print(\"‚úÖ Multi-account connection - PASSED\")\n",
    "print(\"‚úÖ Write Parquet to different account - PASSED\")\n",
    "print(\"‚úÖ Validation tests - PASSED\")\n",
    "print(\"\")\n",
    "print(\"Optional tests (commented out):\")\n",
    "print(\"   - Key Vault authentication\")\n",
    "print(\"   - SparkEngine with multi-account\")\n",
    "print(\"\")\n",
    "print(\"Files created in ADLS:\")\n",
    "print(f\"   - {bronze_conn.uri(TEST_CSV_PATH)}\")\n",
    "print(f\"   - {silver_conn.uri(OUTPUT_PARQUET_PATH)}\")\n",
    "print(\"\")\n",
    "print(\"üéâ Phase 2A implementation validated with real ADLS connections!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Cleanup Test Data\n",
    "\n",
    "Uncomment to delete test files from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Deleted: abfss://example-container@ingrglobaldigitalopsteam.dfs.core.windows.net/test/sample_data.csv\n",
      "‚úÖ Deleted: abfss://example-container@ingrglobaldigitalopsteam.dfs.core.windows.net/test/output/sample_data.parquet\n",
      "‚ÑπÔ∏è  Cleanup commented out - uncomment to delete test files\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to cleanup test data\n",
    "import adlfs\n",
    "\n",
    "fs = adlfs.AzureBlobFileSystem(**bronze_conn.pandas_storage_options())\n",
    "try:\n",
    "    fs.rm(bronze_conn.uri(TEST_CSV_PATH))\n",
    "    print(f\"‚úÖ Deleted: {bronze_conn.uri(TEST_CSV_PATH)}\")\n",
    "except:\n",
    "    print(\"File not found or already deleted\")\n",
    "\n",
    "fs2 = adlfs.AzureBlobFileSystem(**silver_conn.pandas_storage_options())\n",
    "try:\n",
    "    fs2.rm(silver_conn.uri(OUTPUT_PARQUET_PATH), recursive=True)\n",
    "    print(f\"‚úÖ Deleted: {silver_conn.uri(OUTPUT_PARQUET_PATH)}\")\n",
    "except:\n",
    "    print(\"File not found or already deleted\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Cleanup commented out - uncomment to delete test files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
