{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Spark Engine Preview (Phase 1 Scaffold)\n",
    "\n",
    "## üß≠ Goal\n",
    "\n",
    "Understand ODIBI's Spark engine architecture and configuration.\n",
    "\n",
    "This notebook will:\n",
    "- Explore the Spark engine scaffold (Phase 1)\n",
    "- Show Spark pipeline YAML structure\n",
    "- Explain Azure integration points\n",
    "- Preview what's coming in Phase 3\n",
    "\n",
    "**Note:** Spark execution is coming in Phase 3 (Q2 2026). This shows the architecture.\n",
    "\n",
    "**Estimated time:** 3 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Environment Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Navigate to project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'walkthroughs' else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"‚úÖ Environment ready\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Spark Engine Architecture\n",
    "\n",
    "ODIBI's Spark engine follows the same interface as Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Spark engine code\n",
    "with open('odibi/engine/spark_engine.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(\"üì¶ Spark Engine Class Structure:\\n\")\n",
    "\n",
    "# Show class definition and key methods\n",
    "lines = content.split('\\n')\n",
    "for i, line in enumerate(lines[:50], 1):\n",
    "    if 'class SparkEngine' in line or 'def ' in line:\n",
    "        print(f\"{i:3}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing SparkEngine (will show helpful error if pyspark not installed)\n",
    "try:\n",
    "    from odibi.engine.spark_engine import SparkEngine\n",
    "    print(\"‚úÖ SparkEngine imported successfully\")\n",
    "    print(f\"   Engine name: {SparkEngine.name}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Expected: {e}\")\n",
    "    print(\"\\nüí° This is normal! Spark engine requires:\")\n",
    "    print(\"   pip install 'odibi[spark]'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Spark Pipeline Configuration\n",
    "\n",
    "Let's examine the `example_spark.yaml` configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spark example configuration\n",
    "with open('examples/example_spark.yaml', 'r') as f:\n",
    "    spark_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üîß Spark Pipeline Configuration:\\n\")\n",
    "print(f\"Project: {spark_config['project']}\")\n",
    "print(f\"Engine: {spark_config['engine']}\")\n",
    "print(f\"\\nConnections:\")\n",
    "for conn_name, conn_config in spark_config['connections'].items():\n",
    "    print(f\"  ‚Ä¢ {conn_name}: {conn_config['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Azure ADLS connection structure\n",
    "print(\"\\n‚òÅÔ∏è Azure ADLS Connection:\")\n",
    "print(yaml.dump(spark_config['connections']['adls_bronze'], default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pipeline structure\n",
    "print(\"\\nüìä Pipeline Structure:\")\n",
    "for pipeline in spark_config['pipelines']:\n",
    "    print(f\"\\nPipeline: {pipeline['name']}\")\n",
    "    print(f\"  Nodes: {len(pipeline['nodes'])}\")\n",
    "    for node in pipeline['nodes']:\n",
    "        print(f\"    ‚Ä¢ {node['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Azure Connection Resolution\n",
    "\n",
    "The Azure ADLS connection can build URIs even in Phase 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ADLS URI generation (Phase 1 feature that works now)\n",
    "from odibi.connections.azure_adls import AzureADLS\n",
    "\n",
    "# Create connection\n",
    "adls = AzureADLS(\n",
    "    account=\"mystorageaccount\",\n",
    "    container=\"datalake\",\n",
    "    path_prefix=\"bronze/\",\n",
    "    auth_mode=\"managed_identity\"\n",
    ")\n",
    "\n",
    "# Build URI\n",
    "uri = adls.uri(\"sensors/2024/01/data.parquet\")\n",
    "\n",
    "print(\"‚úÖ ADLS URI Construction (Phase 1 Working):\")\n",
    "print(f\"\\n   Input path: sensors/2024/01/data.parquet\")\n",
    "print(f\"   Output URI: {uri}\")\n",
    "print(f\"\\n   This URI will be used by Spark in Phase 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™û Reflect\n",
    "\n",
    "**What we learned:**\n",
    "- Spark engine architecture (scaffolded in Phase 1)\n",
    "- Spark pipeline YAML structure\n",
    "- Azure ADLS connection configuration\n",
    "- URI construction works today (Phase 1)\n",
    "- Spark execution coming in Phase 3\n",
    "\n",
    "**Phase 1 (Current) Features:**\n",
    "- ‚úÖ SparkEngine class defined\n",
    "- ‚úÖ Azure connections (ADLS, SQL, DBFS)\n",
    "- ‚úÖ Path/URI resolution\n",
    "- ‚úÖ Import guards with helpful errors\n",
    "\n",
    "**Phase 3 (Q2 2026) Features:**\n",
    "- ‚è≥ `SparkEngine.read()` - Load Parquet/CSV from ADLS\n",
    "- ‚è≥ `SparkEngine.write()` - Save to ADLS/Delta\n",
    "- ‚è≥ `SparkEngine.execute_sql()` - Run Spark SQL transforms\n",
    "- ‚è≥ Integration tests with local Spark session\n",
    "\n",
    "**Next step:**  \n",
    "Go to **`04_ci_cd_and_precommit.ipynb`** to learn about code quality automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Self-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Self-Check\n",
    "try:\n",
    "    import sys, os\n",
    "    print(\"Running self-check...\")\n",
    "    \n",
    "    # Verify Spark scaffolding exists\n",
    "    assert os.path.exists(\"odibi/engine/spark_engine.py\"), \"Missing Spark engine\"\n",
    "    assert os.path.exists(\"odibi/connections/azure_adls.py\"), \"Missing ADLS connection\"\n",
    "    assert os.path.exists(\"examples/example_spark.yaml\"), \"Missing Spark example\"\n",
    "    \n",
    "    # Verify Azure connections work\n",
    "    from odibi.connections.azure_adls import AzureADLS\n",
    "    conn = AzureADLS(account=\"test\", container=\"data\")\n",
    "    uri = conn.uri(\"file.csv\")\n",
    "    assert uri.startswith(\"abfss://\"), \"URI generation failed\"\n",
    "    print(f\"‚úÖ Azure ADLS connection works\")\n",
    "    \n",
    "    # Verify docs exist\n",
    "    assert os.path.exists(\"docs/setup_databricks.md\"), \"Missing Databricks setup guide\"\n",
    "    assert os.path.exists(\"docs/setup_azure.md\"), \"Missing Azure setup guide\"\n",
    "    \n",
    "    print(\"üéâ Walkthrough 03 verified successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Walkthrough failed self-check: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
