{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIBI Framework - Phase 2: Orchestration\n",
    "\n",
    "**What's new:** Dependency graphs, Pipeline executor, Engine system, End-to-end pipelines!\n",
    "\n",
    "This notebook demonstrates the orchestration layer that makes pipelines actually run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project root: d:\\odibi\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"✅ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "from odibi.config import PipelineConfig, NodeConfig, ReadConfig, WriteConfig, TransformConfig\n",
    "from odibi.graph import DependencyGraph\n",
    "from odibi.pipeline import Pipeline\n",
    "from odibi.connections import LocalConnection\n",
    "from odibi.registry import transform, FunctionRegistry\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Dependency Graph\n",
    "\n",
    "The graph builder analyzes node dependencies and determines execution order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution order:\n",
      "  1. load1\n",
      "  2. transform2\n",
      "  3. save3\n"
     ]
    }
   ],
   "source": [
    "# Test 1.1: Simple linear dependency chain\n",
    "nodes = [\n",
    "    NodeConfig(\n",
    "        name=\"load\",\n",
    "        read=ReadConfig(connection=\"local\", format=\"csv\", path=\"input.csv\")\n",
    "    ),\n",
    "    NodeConfig(\n",
    "        name=\"transform\",\n",
    "        depends_on=[\"load\"],\n",
    "        transform=TransformConfig(steps=[\"SELECT * FROM load\"])\n",
    "    ),\n",
    "    NodeConfig(\n",
    "        name=\"save\",\n",
    "        depends_on=[\"transform\"],\n",
    "        write=WriteConfig(connection=\"local\", format=\"csv\", path=\"output.csv\")\n",
    "    )\n",
    "]\n",
    "\n",
    "graph = DependencyGraph(nodes)\n",
    "execution_order = graph.topological_sort()\n",
    "\n",
    "print(\"Execution order:\")\n",
    "for i, node_name in enumerate(execution_order, 1):\n",
    "    print(f\"  {i}. {node_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution layers (nodes in same layer can run in parallel):\n",
      "  Layer 1: root\n",
      "  Layer 2: branch_a, branch_b\n",
      "  Layer 3: merge\n"
     ]
    }
   ],
   "source": [
    "# Test 1.2: Parallel nodes (diamond pattern)\n",
    "nodes = [\n",
    "    NodeConfig(name=\"root\", read=ReadConfig(connection=\"local\", format=\"csv\", path=\"a.csv\")),\n",
    "    NodeConfig(name=\"branch_a\", depends_on=[\"root\"], transform=TransformConfig(steps=[\"SELECT * FROM root\"])),\n",
    "    NodeConfig(name=\"branch_b\", depends_on=[\"root\"], transform=TransformConfig(steps=[\"SELECT * FROM root\"])),\n",
    "    NodeConfig(name=\"merge\", depends_on=[\"branch_a\", \"branch_b\"], transform=TransformConfig(steps=[\"SELECT * FROM branch_a\"]))\n",
    "]\n",
    "\n",
    "graph = DependencyGraph(nodes)\n",
    "\n",
    "# Get execution layers (for parallel execution)\n",
    "layers = graph.get_execution_layers()\n",
    "\n",
    "print(\"Execution layers (nodes in same layer can run in parallel):\")\n",
    "for i, layer in enumerate(layers, 1):\n",
    "    print(f\"  Layer {i}: {', '.join(layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Graph:\n",
      "\n",
      "Layer 1:\n",
      "  - root\n",
      "\n",
      "Layer 2:\n",
      "  - branch_a (depends on: root)\n",
      "  - branch_b (depends on: root)\n",
      "\n",
      "Layer 3:\n",
      "  - merge (depends on: branch_a, branch_b)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1.3: Visualize the graph\n",
    "print(graph.visualize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cycle detected:\n",
      "   ✗ Dependency error: Circular dependency detected\n",
      "  Cycle detected: node1 → node2 → node1\n"
     ]
    }
   ],
   "source": [
    "# Test 1.4: Cycle detection (this should fail)\n",
    "try:\n",
    "    circular_nodes = [\n",
    "        NodeConfig(name=\"node1\", depends_on=[\"node2\"], read=ReadConfig(connection=\"local\", format=\"csv\", path=\"a.csv\")),\n",
    "        NodeConfig(name=\"node2\", depends_on=[\"node1\"], transform=TransformConfig(steps=[\"SELECT * FROM node1\"]))\n",
    "    ]\n",
    "    graph = DependencyGraph(circular_nodes)\n",
    "    print(\"❌ Should have detected cycle!\")\n",
    "except Exception as e:\n",
    "    print(\"✅ Cycle detected:\")\n",
    "    print(f\"   {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies of 'c': {'a', 'b'}\n",
      "Dependents of 'a': {'c', 'b'}\n",
      "Independent nodes: ['a']\n"
     ]
    }
   ],
   "source": [
    "# Test 1.5: Get dependencies and dependents\n",
    "nodes = [\n",
    "    NodeConfig(name=\"a\", read=ReadConfig(connection=\"local\", format=\"csv\", path=\"a.csv\")),\n",
    "    NodeConfig(name=\"b\", depends_on=[\"a\"], transform=TransformConfig(steps=[\"SELECT * FROM a\"])),\n",
    "    NodeConfig(name=\"c\", depends_on=[\"b\"], transform=TransformConfig(steps=[\"SELECT * FROM b\"]))\n",
    "]\n",
    "\n",
    "graph = DependencyGraph(nodes)\n",
    "\n",
    "print(f\"Dependencies of 'c': {graph.get_dependencies('c')}\")\n",
    "print(f\"Dependents of 'a': {graph.get_dependents('a')}\")\n",
    "print(f\"Independent nodes: {graph.get_independent_nodes()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pipeline Execution\n",
    "\n",
    "The pipeline executor runs nodes in the correct order, handles failures, and tracks results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test data created at: C:\\Users\\henry\\AppData\\Local\\Temp\\tmp8x8d5oxv\n",
      "\n",
      "Sample data:\n",
      "   id product  amount\n",
      "0   1       A     100\n",
      "1   2       B     200\n",
      "2   3       A     150\n",
      "3   4       C     300\n",
      "4   5       B     250\n"
     ]
    }
   ],
   "source": [
    "# Setup: Create test data\n",
    "test_dir = tempfile.mkdtemp()\n",
    "test_path = Path(test_dir)\n",
    "\n",
    "# Create sample CSV\n",
    "sample_data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"product\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n",
    "    \"amount\": [100, 200, 150, 300, 250]\n",
    "})\n",
    "sample_data.to_csv(test_path / \"sales.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Test data created at: {test_path}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Results:\n",
      "  Completed: ['load_sales']\n",
      "  Failed: []\n",
      "  Skipped: []\n",
      "  Duration: 0.0106s\n",
      "\n",
      "✅ Loaded 5 rows\n",
      "   id product  amount\n",
      "0   1       A     100\n",
      "1   2       B     200\n",
      "2   3       A     150\n",
      "3   4       C     300\n",
      "4   5       B     250\n"
     ]
    }
   ],
   "source": [
    "# Test 2.1: Simple read-only pipeline\n",
    "pipeline_config = PipelineConfig(\n",
    "    pipeline=\"simple_read\",\n",
    "    nodes=[\n",
    "        NodeConfig(\n",
    "            name=\"load_sales\",\n",
    "            read=ReadConfig(connection=\"local\", format=\"csv\", path=\"sales.csv\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "connections = {\"local\": LocalConnection(base_path=str(test_path))}\n",
    "pipeline = Pipeline(pipeline_config, connections=connections)\n",
    "\n",
    "results = pipeline.run()\n",
    "\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"  Completed: {results.completed}\")\n",
    "print(f\"  Failed: {results.failed}\")\n",
    "print(f\"  Skipped: {results.skipped}\")\n",
    "print(f\"  Duration: {results.duration:.4f}s\")\n",
    "\n",
    "# Check the loaded data\n",
    "loaded_data = pipeline.context.get(\"load_sales\")\n",
    "print(f\"\\n✅ Loaded {len(loaded_data)} rows\")\n",
    "print(loaded_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.2: Pipeline with transform function\n",
    "# Clear registry first\n",
    "FunctionRegistry._functions.clear()\n",
    "FunctionRegistry._signatures.clear()\n",
    "\n",
    "@transform\n",
    "def filter_high_value(context, source: str, min_amount: float = 150):\n",
    "    \"\"\"Filter for high-value transactions.\"\"\"\n",
    "    df = context.get(source)\n",
    "    return df[df[\"amount\"] >= min_amount]\n",
    "\n",
    "@transform\n",
    "def calculate_total(context, source: str):\n",
    "    \"\"\"Calculate total amount.\"\"\"\n",
    "    df = context.get(source)\n",
    "    df = df.copy()\n",
    "    df[\"total\"] = df[\"amount\"]  # Simple total for demo\n",
    "    return df\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    pipeline=\"transform_pipeline\",\n",
    "    nodes=[\n",
    "        NodeConfig(\n",
    "            name=\"load\",\n",
    "            read=ReadConfig(connection=\"local\", format=\"csv\", path=\"sales.csv\")\n",
    "        ),\n",
    "        NodeConfig(\n",
    "            name=\"filter\",\n",
    "            depends_on=[\"load\"],\n",
    "            transform=TransformConfig(\n",
    "                steps=[\n",
    "                    {\"function\": \"filter_high_value\", \"params\": {\"source\": \"load\", \"min_amount\": 200}}\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        NodeConfig(\n",
    "            name=\"calculate\",\n",
    "            depends_on=[\"filter\"],\n",
    "            transform=TransformConfig(\n",
    "                steps=[\n",
    "                    {\"function\": \"calculate_total\", \"params\": {\"source\": \"filter\"}}\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(pipeline_config, connections=connections)\n",
    "results = pipeline.run()\n",
    "\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"  Completed: {results.completed}\")\n",
    "print(f\"  Duration: {results.duration:.4f}s\")\n",
    "\n",
    "# Check final result\n",
    "final_data = pipeline.context.get(\"calculate\")\n",
    "print(f\"\\n✅ Final result has {len(final_data)} rows (filtered from {len(sample_data)})\")\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Results:\n",
      "  Completed: ['load', 'save']\n",
      "\n",
      "✅ Output file created: C:\\Users\\henry\\AppData\\Local\\Temp\\tmp8x8d5oxv\\output.parquet\n",
      "   Saved 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Test 2.3: Pipeline with write node\n",
    "pipeline_config = PipelineConfig(\n",
    "    pipeline=\"full_etl\",\n",
    "    nodes=[\n",
    "        NodeConfig(\n",
    "            name=\"load\",\n",
    "            read=ReadConfig(connection=\"local\", format=\"csv\", path=\"sales.csv\")\n",
    "        ),\n",
    "        NodeConfig(\n",
    "            name=\"save\",\n",
    "            depends_on=[\"load\"],\n",
    "            write=WriteConfig(\n",
    "                connection=\"local\",\n",
    "                format=\"parquet\",\n",
    "                path=\"output.parquet\",\n",
    "                mode=\"overwrite\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(pipeline_config, connections=connections)\n",
    "results = pipeline.run()\n",
    "\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"  Completed: {results.completed}\")\n",
    "\n",
    "# Verify file was created\n",
    "output_file = test_path / \"output.parquet\"\n",
    "if output_file.exists():\n",
    "    print(f\"\\n✅ Output file created: {output_file}\")\n",
    "    saved_data = pd.read_parquet(output_file)\n",
    "    print(f\"   Saved {len(saved_data)} rows\")\n",
    "else:\n",
    "    print(\"❌ Output file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Results (with failure):\n",
      "  ✅ Completed: ['success1', 'success2']\n",
      "  ❌ Failed: ['fail']\n",
      "  ⏭ Skipped: ['dependent']\n",
      "\n",
      "Observations:\n",
      "  - success1 completed (runs first)\n",
      "  - fail node failed (intentional)\n",
      "  - dependent skipped (because fail failed)\n",
      "  - success2 completed (independent of failure)\n"
     ]
    }
   ],
   "source": [
    "# Test 2.4: Failure handling\n",
    "@transform\n",
    "def failing_function(context, source: str):\n",
    "    \"\"\"This function intentionally fails.\"\"\"\n",
    "    raise ValueError(\"Intentional failure for testing\")\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    pipeline=\"failure_test\",\n",
    "    nodes=[\n",
    "        NodeConfig(name=\"success1\", read=ReadConfig(connection=\"local\", format=\"csv\", path=\"sales.csv\")),\n",
    "        NodeConfig(\n",
    "            name=\"fail\",\n",
    "            depends_on=[\"success1\"],\n",
    "            transform=TransformConfig(\n",
    "                steps=[{\"function\": \"failing_function\", \"params\": {\"source\": \"success1\"}}]\n",
    "            )\n",
    "        ),\n",
    "        NodeConfig(\n",
    "            name=\"dependent\",\n",
    "            depends_on=[\"fail\"],\n",
    "            transform=TransformConfig(steps=[\"SELECT * FROM fail\"])\n",
    "        ),\n",
    "        NodeConfig(name=\"success2\", read=ReadConfig(connection=\"local\", format=\"csv\", path=\"sales.csv\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(pipeline_config, connections=connections)\n",
    "results = pipeline.run()\n",
    "\n",
    "print(\"Pipeline Results (with failure):\")\n",
    "print(f\"  ✅ Completed: {results.completed}\")\n",
    "print(f\"  ❌ Failed: {results.failed}\")\n",
    "print(f\"  ⏭ Skipped: {results.skipped}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  - success1 completed (runs first)\")\n",
    "print(\"  - fail node failed (intentional)\")\n",
    "print(\"  - dependent skipped (because fail failed)\")\n",
    "print(\"  - success2 completed (independent of failure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.5: Run single node with mock data\n",
    "@transform\n",
    "def double_amounts(context, source: str):\n",
    "    \"\"\"Double all amounts.\"\"\"\n",
    "    df = context.get(source)\n",
    "    df = df.copy()\n",
    "    df[\"amount\"] = df[\"amount\"] * 2\n",
    "    return df\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    pipeline=\"test\",\n",
    "    nodes=[\n",
    "        NodeConfig(\n",
    "            name=\"process\",\n",
    "            transform=TransformConfig(\n",
    "                steps=[{\"function\": \"double_amounts\", \"params\": {\"source\": \"input\"}}]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(pipeline_config, connections={})\n",
    "\n",
    "# Run with mock data\n",
    "mock_df = pd.DataFrame({\"id\": [1, 2], \"amount\": [100, 200]})\n",
    "result = pipeline.run_node(\"process\", mock_data={\"input\": mock_df})\n",
    "\n",
    "print(\"Single Node Execution:\")\n",
    "print(f\"  Success: {result.success}\")\n",
    "print(f\"  Duration: {result.duration:.4f}s\")\n",
    "\n",
    "output = pipeline.context.get(\"process\")\n",
    "print(\"\\nInput:\")\n",
    "print(mock_df)\n",
    "print(\"\\nOutput (amounts doubled):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  Valid: True\n",
      "  Node count: 3\n",
      "  Execution order: ['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "# Test 2.6: Validate pipeline without running\n",
    "pipeline_config = PipelineConfig(\n",
    "    pipeline=\"validation_test\",\n",
    "    nodes=[\n",
    "        NodeConfig(name=\"a\", read=ReadConfig(connection=\"local\", format=\"csv\", path=\"a.csv\")),\n",
    "        NodeConfig(name=\"b\", depends_on=[\"a\"], transform=TransformConfig(steps=[\"SELECT * FROM a\"])),\n",
    "        NodeConfig(name=\"c\", depends_on=[\"b\"], transform=TransformConfig(steps=[\"SELECT * FROM b\"]))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(pipeline_config, connections=connections)\n",
    "validation = pipeline.validate()\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(f\"  Valid: {validation['valid']}\")\n",
    "print(f\"  Node count: {validation['node_count']}\")\n",
    "print(f\"  Execution order: {validation['execution_order']}\")\n",
    "if validation['errors']:\n",
    "    print(f\"  Errors: {validation['errors']}\")\n",
    "if validation['warnings']:\n",
    "    print(f\"  Warnings: {validation['warnings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test directory cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "shutil.rmtree(test_dir)\n",
    "print(\"✅ Test directory cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Phase 2 accomplishments:**\n",
    "\n",
    "✅ **Dependency Graph** - Analyzes dependencies, detects cycles, plans execution  \n",
    "✅ **Pipeline Executor** - Runs nodes in order, handles failures, tracks results  \n",
    "✅ **Engine System** - Abstracts read/write/transform operations  \n",
    "✅ **Connection System** - Abstracts data sources/destinations  \n",
    "✅ **End-to-End Pipelines** - Everything works together!\n",
    "\n",
    "**What you can do now:**\n",
    "- Build real data pipelines with YAML configs\n",
    "- Create reusable transform functions with `@transform`\n",
    "- Read/write CSV, Parquet, JSON\n",
    "- Execute SQL queries (with DuckDB)\n",
    "- Handle failures gracefully\n",
    "- Debug with single-node execution\n",
    "\n",
    "**Next:** CLI tools, more formats, production features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame\n",
      "(data=None, index: 'Axes | None' = None, columns: 'Axes | None' = None, dtype: 'Dtype | None' = None, copy: 'bool | None' = None) -> 'None'\n"
     ]
    }
   ],
   "source": [
    "func = pd.DataFrame\n",
    "sig = inspect.signature(func)\n",
    "print(func.__name__)\n",
    "print(sig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
