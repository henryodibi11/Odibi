{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIBI Framework - Test Exploration\n",
    "\n",
    "This notebook lets you run and explore the tests interactively to understand how each component works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add odibi to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path includes: {project_root in [Path(p) for p in sys.path]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports work\n",
    "import pandas as pd\n",
    "from odibi.config import NodeConfig, ReadConfig, WriteConfig, TransformConfig, PipelineConfig\n",
    "from odibi.context import PandasContext, create_context\n",
    "from odibi.registry import transform, FunctionRegistry, validate_function_params\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Config Validation Tests\n",
    "\n",
    "Test Pydantic schemas that validate YAML configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.1: Valid ReadConfig with path\n",
    "from pydantic import ValidationError\n",
    "\n",
    "config = ReadConfig(\n",
    "    connection=\"local\",\n",
    "    format=\"csv\",\n",
    "    path=\"data/input.csv\"\n",
    ")\n",
    "\n",
    "print(\"Valid ReadConfig:\")\n",
    "print(f\"  Connection: {config.connection}\")\n",
    "print(f\"  Format: {config.format}\")\n",
    "print(f\"  Path: {config.path}\")\n",
    "print(f\"  Table: {config.table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.2: Invalid ReadConfig - missing path AND table (should fail)\n",
    "try:\n",
    "    config = ReadConfig(\n",
    "        connection=\"local\",\n",
    "        format=\"csv\"\n",
    "        # Missing both path and table!\n",
    "    )\n",
    "    print(\"❌ Should have failed!\")\n",
    "except ValidationError as e:\n",
    "    print(\"✅ Validation caught the error:\")\n",
    "    print(f\"   {e.errors()[0]['msg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.3: Valid NodeConfig with read operation\n",
    "node = NodeConfig(\n",
    "    name=\"load_data\",\n",
    "    description=\"Load CSV data\",\n",
    "    read=ReadConfig(\n",
    "        connection=\"local\",\n",
    "        format=\"csv\",\n",
    "        path=\"input.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Valid NodeConfig: {node.name}\")\n",
    "print(f\"  Has read: {node.read is not None}\")\n",
    "print(f\"  Has transform: {node.transform is not None}\")\n",
    "print(f\"  Has write: {node.write is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.4: Invalid NodeConfig - no operations (should fail)\n",
    "try:\n",
    "    node = NodeConfig(\n",
    "        name=\"empty_node\"\n",
    "        # No read, transform, or write!\n",
    "    )\n",
    "    print(\"❌ Should have failed!\")\n",
    "except ValidationError as e:\n",
    "    print(\"✅ Validation caught the error:\")\n",
    "    error_msg = str(e)\n",
    "    print(f\"   Error contains 'must have at least one': {'must have at least one' in error_msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.5: NodeConfig with dependencies\n",
    "node = NodeConfig(\n",
    "    name=\"process_data\",\n",
    "    depends_on=[\"load_data\", \"load_reference\"],\n",
    "    transform=TransformConfig(\n",
    "        steps=[\"SELECT * FROM load_data\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Node '{node.name}' depends on: {node.depends_on}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1.6: PipelineConfig rejects duplicate node names\n",
    "try:\n",
    "    pipeline = PipelineConfig(\n",
    "        pipeline=\"test_pipeline\",\n",
    "        nodes=[\n",
    "            NodeConfig(\n",
    "                name=\"duplicate\",\n",
    "                read=ReadConfig(connection=\"local\", format=\"csv\", path=\"a.csv\")\n",
    "            ),\n",
    "            NodeConfig(\n",
    "                name=\"duplicate\",  # Same name!\n",
    "                read=ReadConfig(connection=\"local\", format=\"csv\", path=\"b.csv\")\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(\"❌ Should have failed!\")\n",
    "except ValidationError as e:\n",
    "    print(\"✅ Validation caught duplicate names:\")\n",
    "    error_msg = str(e)\n",
    "    print(f\"   Error contains 'Duplicate': {'Duplicate' in error_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Context API Tests\n",
    "\n",
    "Test the unified Context for passing DataFrames between nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.1: Register and retrieve DataFrame\n",
    "ctx = PandasContext()\n",
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"value\": [10, 20, 30]\n",
    "})\n",
    "\n",
    "ctx.register(\"my_data\", df)\n",
    "retrieved = ctx.get(\"my_data\")\n",
    "\n",
    "print(\"✅ Registered and retrieved DataFrame:\")\n",
    "print(retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.2: Check if DataFrame exists\n",
    "print(f\"Has 'my_data': {ctx.has('my_data')}\")\n",
    "print(f\"Has 'does_not_exist': {ctx.has('does_not_exist')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.3: Error when DataFrame not found\n",
    "try:\n",
    "    ctx.get(\"missing_dataframe\")\n",
    "    print(\"❌ Should have failed!\")\n",
    "except KeyError as e:\n",
    "    print(\"✅ Got helpful KeyError:\")\n",
    "    print(f\"   {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.4: List all registered names\n",
    "ctx.register(\"data1\", pd.DataFrame({\"a\": [1]}))\n",
    "ctx.register(\"data2\", pd.DataFrame({\"b\": [2]}))\n",
    "\n",
    "print(f\"Registered DataFrames: {ctx.list_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.5: Clear all DataFrames\n",
    "print(f\"Before clear: {len(ctx.list_names())} DataFrames\")\n",
    "ctx.clear()\n",
    "print(f\"After clear: {len(ctx.list_names())} DataFrames\")\n",
    "print(f\"Has 'my_data': {ctx.has('my_data')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.6: Type validation - rejects non-DataFrame\n",
    "ctx = PandasContext()\n",
    "try:\n",
    "    ctx.register(\"invalid\", {\"not\": \"a dataframe\"})\n",
    "    print(\"❌ Should have failed!\")\n",
    "except TypeError as e:\n",
    "    print(\"✅ Type validation works:\")\n",
    "    print(f\"   {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2.7: Simulating a pipeline - data flow between nodes\n",
    "ctx = PandasContext()\n",
    "\n",
    "# Node 1: Load raw data\n",
    "raw = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4],\n",
    "    \"value\": [5, 15, 25, 35]\n",
    "})\n",
    "ctx.register(\"raw_data\", raw)\n",
    "print(\"Node 1: Loaded raw data\")\n",
    "\n",
    "# Node 2: Filter data\n",
    "raw = ctx.get(\"raw_data\")\n",
    "filtered = raw[raw[\"value\"] > 10]\n",
    "ctx.register(\"filtered_data\", filtered)\n",
    "print(\"Node 2: Filtered data (value > 10)\")\n",
    "\n",
    "# Node 3: Compute summary\n",
    "filtered = ctx.get(\"filtered_data\")\n",
    "print(f\"Node 3: Final result has {len(filtered)} rows\")\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Function Registry Tests\n",
    "\n",
    "Test the `@transform` decorator and parameter validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.1: Register a function with @transform\n",
    "# Clear registry first (in case running multiple times)\n",
    "FunctionRegistry._functions.clear()\n",
    "FunctionRegistry._signatures.clear()\n",
    "\n",
    "@transform\n",
    "def my_transform(context, param1: str, param2: int = 10):\n",
    "    \"\"\"Example transform function.\"\"\"\n",
    "    return f\"Got {param1} and {param2}\"\n",
    "\n",
    "print(f\"Registered functions: {FunctionRegistry.list_functions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.2: Call the decorated function\n",
    "ctx = PandasContext()\n",
    "result = my_transform(ctx, \"hello\", 42)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.3: Validate parameters - valid case\n",
    "try:\n",
    "    validate_function_params(\n",
    "        \"my_transform\",\n",
    "        {\"param1\": \"value\", \"param2\": 20}\n",
    "    )\n",
    "    print(\"✅ Parameters validated successfully\")\n",
    "except ValueError as e:\n",
    "    print(f\"❌ Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.4: Validate parameters - missing required param\n",
    "try:\n",
    "    validate_function_params(\n",
    "        \"my_transform\",\n",
    "        {\"param2\": 20}  # Missing param1!\n",
    "    )\n",
    "    print(\"❌ Should have failed!\")\n",
    "except ValueError as e:\n",
    "    print(\"✅ Caught missing parameter:\")\n",
    "    print(f\"   {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.5: Validate parameters - unexpected param\n",
    "try:\n",
    "    validate_function_params(\n",
    "        \"my_transform\",\n",
    "        {\"param1\": \"value\", \"unknown_param\": \"oops\"}\n",
    "    )\n",
    "    print(\"❌ Should have failed!\")\n",
    "except ValueError as e:\n",
    "    print(\"✅ Caught unexpected parameter:\")\n",
    "    print(f\"   {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.6: Get function metadata\n",
    "info = FunctionRegistry.get_function_info(\"my_transform\")\n",
    "\n",
    "print(\"Function info:\")\n",
    "print(f\"  Name: {info['name']}\")\n",
    "print(f\"  Docstring: {info['docstring']}\")\n",
    "print(f\"  Parameters:\")\n",
    "for param_name, param_info in info['parameters'].items():\n",
    "    required = \"required\" if param_info['required'] else \"optional\"\n",
    "    default = f\" (default: {param_info['default']})\" if param_info['default'] is not None else \"\"\n",
    "    print(f\"    - {param_name}: {required}{default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3.7: Real-world transform function\n",
    "@transform\n",
    "def filter_by_threshold(context, source_table: str, threshold: float):\n",
    "    \"\"\"Filter data by threshold value.\"\"\"\n",
    "    df = context.get(source_table)\n",
    "    return df[df[\"value\"] > threshold]\n",
    "\n",
    "# Set up test data\n",
    "ctx = PandasContext()\n",
    "data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4],\n",
    "    \"value\": [5.0, 15.0, 25.0, 35.0]\n",
    "})\n",
    "ctx.register(\"source_data\", data)\n",
    "\n",
    "# Execute transform\n",
    "result = filter_by_threshold(ctx, source_table=\"source_data\", threshold=20.0)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nFiltered data (threshold=20.0):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Integration Example\n",
    "\n",
    "Putting it all together: config + context + transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complete pipeline config\n",
    "pipeline = PipelineConfig(\n",
    "    pipeline=\"sales_pipeline\",\n",
    "    description=\"Process sales data\",\n",
    "    nodes=[\n",
    "        NodeConfig(\n",
    "            name=\"load_sales\",\n",
    "            description=\"Load raw sales data\",\n",
    "            read=ReadConfig(\n",
    "                connection=\"local\",\n",
    "                format=\"csv\",\n",
    "                path=\"sales.csv\"\n",
    "            ),\n",
    "            cache=True\n",
    "        ),\n",
    "        NodeConfig(\n",
    "            name=\"clean_sales\",\n",
    "            description=\"Remove invalid records\",\n",
    "            depends_on=[\"load_sales\"],\n",
    "            transform=TransformConfig(\n",
    "                steps=[\n",
    "                    {\n",
    "                        \"function\": \"filter_by_threshold\",\n",
    "                        \"params\": {\n",
    "                            \"source_table\": \"load_sales\",\n",
    "                            \"threshold\": 0.0\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        NodeConfig(\n",
    "            name=\"save_results\",\n",
    "            description=\"Save cleaned data\",\n",
    "            depends_on=[\"clean_sales\"],\n",
    "            write=WriteConfig(\n",
    "                connection=\"local\",\n",
    "                format=\"parquet\",\n",
    "                path=\"cleaned_sales.parquet\",\n",
    "                mode=\"overwrite\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Pipeline: {pipeline.pipeline}\")\n",
    "print(f\"Nodes: {len(pipeline.nodes)}\")\n",
    "for node in pipeline.nodes:\n",
    "    deps = f\" (depends on: {', '.join(node.depends_on)})\" if node.depends_on else \"\"\n",
    "    print(f\"  - {node.name}{deps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the transform step parameters\n",
    "transform_step = pipeline.nodes[1].transform.steps[0]\n",
    "function_name = transform_step[\"function\"]\n",
    "params = transform_step[\"params\"]\n",
    "\n",
    "print(f\"Validating transform function: {function_name}\")\n",
    "print(f\"Parameters: {params}\")\n",
    "\n",
    "try:\n",
    "    validate_function_params(function_name, params)\n",
    "    print(\"✅ Transform parameters are valid!\")\n",
    "except ValueError as e:\n",
    "    print(f\"❌ Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we've proven:**\n",
    "\n",
    "1. ✅ **Config validation works** - Invalid configs are caught with clear error messages\n",
    "2. ✅ **Context API works** - DataFrames can be registered and retrieved by name\n",
    "3. ✅ **Function registry works** - Transform functions are validated and type-safe\n",
    "4. ✅ **Integration ready** - All components work together\n",
    "\n",
    "**What's missing:**\n",
    "- Dependency graph builder (to order nodes)\n",
    "- Pipeline executor (to actually run nodes)\n",
    "- Engine implementation (to read/write data)\n",
    "\n",
    "**Next step:** Build the orchestration layer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
