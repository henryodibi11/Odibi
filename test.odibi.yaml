# =============================================================================
# ODIBI MASTER REFERENCE (v2.4.0)
# =============================================================================
# This is your "Super Cheatsheet" for the Odibi Framework.
# It covers configuration, connection examples, and transformation patterns.
#
# SECTIONS:
# 1. Project & Engine Settings
# 2. Connections (Data Sources & Auth Options)
# 3. Pipelines (Nodes, Transforms, SQL, Python)
# 4. Observability (Lineage, Data Quality, Stories)
# =============================================================================

project: my_odibi_project
version: "2.4.0"
description: "A reference data pipeline project"
owner: "Data Team"

# -----------------------------------------------------------------------------
# 1. ENGINE & PERFORMANCE
# -----------------------------------------------------------------------------
# 'pandas': Best for <10GB data, local dev, and complex Python logic.
# 'spark':  Best for >10GB data, Databricks/Synapse, and distributed processing.
engine: pandas

performance:
  use_arrow: true  # (v2.2+) Uses Arrow memory layout for 50% less RAM & faster I/O

retry:
  enabled: true
  max_attempts: 3
  backoff: exponential  # Options: exponential, linear, constant

logging:
  level: INFO           # Options: DEBUG, INFO, WARNING, ERROR
  structured: false     # Set true for JSON logs (Splunk/Datadog)

# -----------------------------------------------------------------------------
# 2. CONNECTIONS (DATA SOURCES)
# -----------------------------------------------------------------------------
connections:
  # --- Type: Local Filesystem ---
  local_data:
    type: local
    base_path: ./data

  # --- Type: Delta Lake (The Standard) ---
  # For Pandas: Points to a local directory of Delta tables
  # For Spark: Points to Hive Metastore / Unity Catalog
  delta_lake:
    type: delta
    path: ./data/delta_tables     # Pandas/Local
    # catalog: spark_catalog      # Spark
    # schema: default             # Spark

  # --- Type: Azure Data Lake Gen2 (Reference) ---
  # adls_raw:
  #   type: azure_blob
  #   account_name: ${AZURE_STORAGE_ACCOUNT}
  #   container: raw-data
  #   validation_mode: lazy  # lazy (check on use) vs eager (check on startup)
  #
  #   # Auth Option 1: Default (Env Vars / Managed Identity) - RECOMMENDED
  #   # Uses AZURE_CLIENT_ID, AZURE_CLIENT_SECRET, etc. automatically.
  #
  #   # Auth Option 2: Explicit Access Key
  #   # auth: { account_key: ${AZURE_STORAGE_KEY} }
  #
  #   # Auth Option 3: SAS Token
  #   # auth: { sas_token: ${AZURE_SAS_TOKEN} }
  #
  #   # Auth Option 4: Key Vault (Secure)
  #   # auth:
  #   #   key_vault_name: my-keyvault
  #   #   secret_name: adls-access-key

  # --- Type: SQL Database (Reference) ---
  # warehouse_db:
  #   type: sql_server
  #   host: ${DB_HOST}
  #   database: analytics
  #   auth:
  #     username: ${DB_USER}
  #     password: ${DB_PASS}
  #     # Or use Key Vault:
  #     # key_vault_name: my-kv
  #     # secret_name: db-password

# -----------------------------------------------------------------------------
# 3. PIPELINES
# -----------------------------------------------------------------------------
pipelines:
  - pipeline: main_etl
    description: "End-to-End ETL Example"
    layer: silver
    
    nodes:
      # =======================================================================
      # NODE 1: READ (Ingestion)
      # =======================================================================
      - name: read_input
        description: "Ingest raw data"
        
        # --- Scenario A: File-Based Read ---
        read:
          connection: local_data
          format: csv
          path: source/customers.csv
          options: {header: true, sep: ","}

        # --- Scenario B: Table-Based Read ---
        # read:
        #   connection: warehouse_db
        #   format: sql_server
        #   table: sales.customers
        
        # Cache result in memory if referenced by multiple downstream nodes
        cache: false

      # =======================================================================
      # NODE 2: TRANSFORM (Cleaning & Logic)
      # =======================================================================
      - name: clean_data
        depends_on: [read_input]
        
        transform:
          steps:
            # A. SQL Transformation (Standard)
            - sql: |
                SELECT 
                  id,
                  trim(lower(email)) as email,
                  first_name, 
                  last_name,
                  region
                FROM read_input
                WHERE id IS NOT NULL

            # B. Standard Library (v2.2+) - No code needed
            - transformer: sql_core.clean_text
              params: 
                columns: ["first_name", "last_name"]
                case: "title"

            # C. Data Quality Flagging (v2.4+)
            - transformer: advanced.validate_and_flag
              params:
                rules:
                  valid_email: "email LIKE '%@%'"
                flag_col: "dq_issues"

        # PII Protection: Automatically mask these in logs/stories
        sensitive: [email, last_name]

      # =======================================================================
      # NODE 3: MERGE / UPSERT (Loading)
      # =======================================================================
      - name: upsert_to_silver
        description: "Upsert to Delta Lake (SCD Type 1)"
        depends_on: [clean_data]
        
        # The 'merge' transformer handles Insert/Update/Delete logic automatically
        transformer: merge
        params:
          target_connection: delta_lake
          target_path: silver/customers
          
          merge_keys: ["id"]
          strategy: upsert  # Options: upsert, append_only, delete_match, overwrite
          
          audit_cols:
            created_col: "created_at"
            updated_col: "updated_at"

      # =======================================================================
      # NODE 4: AGGREGATION & WRITE (Gold)
      # =======================================================================
      - name: gold_stats
        depends_on: [upsert_to_silver]
        
        transform:
          steps:
            - sql: "SELECT region, count(*) as count FROM upsert_to_silver GROUP BY 1"
            
        write:
          connection: local_data
          format: parquet
          path: gold/region_stats.parquet
          mode: overwrite

        # Validation: Fail the pipeline if these checks fail
        validation:
          not_empty: true
          no_nulls: [region]

# -----------------------------------------------------------------------------
# 4. OBSERVABILITY & STORIES (v2.4)
# -----------------------------------------------------------------------------
story:
  connection: local_data
  path: odibi_stories/
  max_sample_rows: 20
  auto_generate: true
  
  # The HTML report will automatically include:
  # - Interactive Lineage Graph (Mermaid.js)
  # - Data Drift (Row additions/removals since last run)
  # - Schema Changes
  # - SQL & Config Snapshots
