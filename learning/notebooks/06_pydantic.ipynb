{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Pydantic and Type Safety\n",
    "## Making invalid state unrepresentable\n",
    "\n",
    "Odibi's entire configuration system is built on **Pydantic**. \n",
    "When someone writes a YAML config, Pydantic validates every field automatically. \n",
    "Wrong type? Error. Missing required field? Error. Invalid enum value? Error.\n",
    "\n",
    "By the end of this notebook, you will understand `odibi/config.py` completely \n",
    "and be able to build your own config system for mini-odibi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Type Hints\n",
    "\n",
    "Type hints tell Python (and other developers) what types a function expects. \n",
    "Python does NOT enforce them at runtime -- but Pydantic does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "# Basic type hints\n",
    "def process_node(name: str, rows: int, active: bool = True) -> str:\n",
    "    \"\"\"Process a node and return a status message.\"\"\"\n",
    "    return f\"{name}: {rows} rows, active={active}\"\n",
    "\n",
    "print(process_node(\"customers\", 1542))\n",
    "\n",
    "# Complex type hints\n",
    "def summarize(nodes: List[str], counts: Dict[str, int]) -> Optional[str]:\n",
    "    \"\"\"Summarize pipeline. Returns None if no nodes.\"\"\"\n",
    "    if not nodes:\n",
    "        return None\n",
    "    total = sum(counts.get(n, 0) for n in nodes)\n",
    "    return f\"{len(nodes)} nodes, {total} total rows\"\n",
    "\n",
    "result = summarize([\"customers\", \"orders\"], {\"customers\": 1542, \"orders\": 8930})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Pydantic BaseModel\n",
    "\n",
    "Pydantic models look like dataclasses but with automatic validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    \"\"\"Configuration for a single pipeline node.\"\"\"\n",
    "    name: str\n",
    "    source: str\n",
    "    format: str = \"csv\"\n",
    "    write_mode: str = \"overwrite\"\n",
    "    keys: List[str] = []\n",
    "    enabled: bool = True\n",
    "\n",
    "# Create with valid data\n",
    "node = NodeConfig(name=\"customers\", source=\"raw_customers.csv\", write_mode=\"upsert\")\n",
    "print(node)\n",
    "print(node.name)\n",
    "print(node.keys)  # [] (default)\n",
    "\n",
    "# Convert to dict\n",
    "print(node.model_dump())\n",
    "\n",
    "# Pydantic auto-converts types when possible\n",
    "node2 = NodeConfig(name=\"orders\", source=\"orders.csv\", enabled=\"true\")  # String \"true\" -> bool True\n",
    "print(node2.enabled)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Create a `ConnectionConfig` model with: name (str), type (str, default 'local'), \n",
    "base_path (str), options (Optional[Dict[str, str]], default None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1\n",
    "# YOUR CODE HERE\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Validators\n",
    "\n",
    "Pydantic validators let you add custom validation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, model_validator\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class WriteMode(str, Enum):\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "    UPSERT = \"upsert\"\n",
    "\n",
    "class ValidatedNodeConfig(BaseModel):\n",
    "    name: str\n",
    "    source: str\n",
    "    write_mode: WriteMode = WriteMode.OVERWRITE\n",
    "    keys: List[str] = []\n",
    "\n",
    "    @field_validator('name')\n",
    "    @classmethod\n",
    "    def name_must_not_be_empty(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"name cannot be empty\")\n",
    "        return v.strip().lower()\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def keys_required_for_upsert(self):\n",
    "        if self.write_mode == WriteMode.UPSERT and not self.keys:\n",
    "            raise ValueError(\"keys are required when write_mode is upsert\")\n",
    "        return self\n",
    "\n",
    "# Valid\n",
    "n1 = ValidatedNodeConfig(name=\"  Customers  \", source=\"data.csv\")\n",
    "print(n1.name)  # customers (cleaned by validator)\n",
    "\n",
    "# Invalid: upsert without keys\n",
    "try:\n",
    "    n2 = ValidatedNodeConfig(name=\"orders\", source=\"o.csv\", write_mode=\"upsert\")\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Create a `TransformConfig` model with: type (str), params (dict). \n",
    "Add a validator that ensures type is not empty and params is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Nested Models and YAML Loading\n",
    "\n",
    "Models can contain other models. This is how Odibi builds pipeline configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class TransformStep(BaseModel):\n",
    "    type: str\n",
    "    params: dict = {}\n",
    "\n",
    "class NodeDef(BaseModel):\n",
    "    name: str\n",
    "    source: str\n",
    "    transforms: List[TransformStep] = []\n",
    "    write_mode: str = \"overwrite\"\n",
    "\n",
    "class PipelineConfig(BaseModel):\n",
    "    name: str\n",
    "    engine: str = \"pandas\"\n",
    "    nodes: List[NodeDef]\n",
    "\n",
    "# Build from a dict (this is what happens when you load YAML)\n",
    "config_dict = {\n",
    "    \"name\": \"sales_pipeline\",\n",
    "    \"engine\": \"pandas\",\n",
    "    \"nodes\": [\n",
    "        {\n",
    "            \"name\": \"customers\",\n",
    "            \"source\": \"data/customers.csv\",\n",
    "            \"transforms\": [{\"type\": \"rename_columns\", \"params\": {\"old\": \"id\", \"new\": \"customer_id\"}}],\n",
    "            \"write_mode\": \"upsert\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "pipeline = PipelineConfig(**config_dict)\n",
    "print(pipeline.name)\n",
    "print(pipeline.nodes[0].name)\n",
    "print(pipeline.nodes[0].transforms[0].type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from YAML\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a sample YAML config\n",
    "yaml_content = \"\"\"\n",
    "name: test_pipeline\n",
    "engine: pandas\n",
    "nodes:\n",
    "  - name: customers\n",
    "    source: data/customers.csv\n",
    "    write_mode: upsert\n",
    "    transforms:\n",
    "      - type: rename_columns\n",
    "        params:\n",
    "          old: id\n",
    "          new: customer_id\n",
    "  - name: orders\n",
    "    source: data/orders.csv\n",
    "    write_mode: append\n",
    "\"\"\"\n",
    "\n",
    "# Parse YAML -> dict -> Pydantic model\n",
    "raw = yaml.safe_load(yaml_content)\n",
    "pipeline = PipelineConfig(**raw)\n",
    "\n",
    "print(f\"Pipeline: {pipeline.name}\")\n",
    "print(f\"Engine: {pipeline.engine}\")\n",
    "for node in pipeline.nodes:\n",
    "    print(f\"  Node: {node.name} ({node.write_mode}), transforms: {len(node.transforms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Build a mini-odibi config\n",
    "Create Pydantic models for: ConnectionConfig, NodeConfig, PipelineConfig. \n",
    "Load them from a YAML string. This is the config system you will use in Phase 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint\n",
    "\n",
    "You now understand Pydantic:\n",
    "- Type hints and why they matter\n",
    "- BaseModel for automatic validation\n",
    "- Field validators (@field_validator, @model_validator)\n",
    "- Enum integration\n",
    "- Nested models\n",
    "- YAML -> dict -> Pydantic model pipeline\n",
    "\n",
    "You can now read `odibi/config.py` (4000+ lines) and understand every pattern in it.\n",
    "\n",
    "**Next:** Phase 7 -- Pandas Deep Dive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
