# Odibi Project Rules for Cline

## IMPORTANT: File Paths
Always use absolute paths starting with `D:/odibi/` when creating files.
Example: `D:/odibi/examples/custom_transformers/data/products.csv`
Do NOT use relative paths like `examples/custom_transformers/...`

## Required Reading
Before making changes to odibi, read `docs/ODIBI_DEEP_CONTEXT.md` for complete framework context (2,200+ lines covering all features).

## CLI Introspection (USE THESE!)
Discover features programmatically before generating code.

**IMPORTANT**: Use `python -m odibi` (not bare `odibi`) to ensure the CLI works without activating the virtual environment:
```bash
python -m odibi list transformers --format json   # All 52+ transformers
python -m odibi list patterns --format json       # All 6 patterns
python -m odibi list connections --format json    # All connection types
python -m odibi explain <name>                    # Detailed docs for any feature
python -m odibi validate config.yaml              # Validate YAML configs
```

**Fallback order if MCP times out:**
1. Try CLI commands above
2. Read `docs/ODIBI_DEEP_CONTEXT.md` for comprehensive reference

## Project Structure
```
odibi/
├── engine/          # Engine implementations (spark.py, pandas.py, polars.py)
├── patterns/        # DWH patterns (dimension.py, fact.py, scd2.py, etc.)
├── transformers/    # SQL transforms (must work on all engines)
├── validation/      # Data quality, quarantine, FK validation
├── connections/     # Storage connections (local, azure, delta, etc.)
├── cli/             # CLI commands
├── config.py        # All Pydantic config models (source of truth)
├── context.py       # Engine contexts (Spark temp views, DuckDB)
├── pipeline.py      # Pipeline orchestration
tests/unit/          # Tests matching source structure
docs/                # Documentation
```

## Adding New Features

### New Transformer
1. Check existing: `python -m odibi list transformers`
2. Add to `odibi/transformers/` following existing patterns
3. Register in `odibi/transformers/__init__.py`
4. Add tests in `tests/unit/transformers/`
5. Must work on Pandas AND Spark (engine parity)

### New Pattern
1. Check existing: `python -m odibi list patterns`
2. Add to `odibi/patterns/` extending `Pattern` base class
3. Register in `odibi/patterns/__init__.py`
4. Add tests

### New Connection
1. Check existing: `python -m odibi list connections`
2. Add to `odibi/connections/`
3. Register in `odibi/connections/factory.py`

## Key Patterns
1. **Engine Parity**: All code must work on Pandas, Spark, AND Polars
2. **SQL-based transforms**: Use SQL (DuckDB for Pandas, Spark SQL for Spark)
3. **Pydantic models**: All config uses Pydantic for validation
4. **Structured logging**: Use `get_logging_context()` for logs

## Critical Runtime Behavior
- Spark: Node outputs registered via `createOrReplaceTempView(node_name)`
- Pandas: SQL executed via DuckDB (different syntax than Spark SQL)
- Node names: Must be alphanumeric + underscore only (Spark compatibility)

## Testing
```bash
pytest tests/ -v                        # Run all tests
pytest tests/unit/test_X.py -v          # Run specific test
pytest tests/unit/test_X.py::test_name  # Run single test
pytest --tb=short                       # Shorter tracebacks
```

## Linting (RUN AFTER EVERY FILE CHANGE!)
```bash
ruff check <file> --fix                 # Fix lint issues
ruff format <file>                      # Format code
```
**ALWAYS run both commands after creating or modifying any Python file.**

## MCP Servers (USE AUTOMATICALLY)

### odibi-knowledge - USE THIS FIRST! (21 tools)
**Before writing ANY odibi code, call these tools:**

**Core Tools:**
- `list_transformers` - See all 52+ available transformers
- `list_patterns` - See all 6 DWH patterns
- `list_connections` - See all connection types
- `explain(name)` - Get docs for any transformer/pattern/connection

**Code Generation Tools:**
- `get_transformer_signature` - Get exact function signature for custom transformers
- `get_yaml_structure` - Get exact YAML pipeline structure
- `generate_transformer` - Generate complete transformer Python code
- `generate_pipeline_yaml` - Generate complete pipeline YAML config
- `validate_yaml` - Validate YAML before saving

**Decision Support Tools:**
- `suggest_pattern` - Recommend the right pattern for your use case
- `get_engine_differences` - Spark vs Pandas vs Polars SQL differences
- `get_validation_rules` - All validation rule types with examples

**Documentation Tools:**
- `get_deep_context` - Get full framework documentation (2300+ lines)
- `get_doc(path)` - Get any specific doc (e.g., "docs/patterns/scd2.md")
- `search_docs(query)` - Search all docs for a term
- `get_example(name)` - Get working example for any pattern/transformer

**Debugging Tools:**
- `diagnose_error` - Diagnose odibi errors and get fix suggestions
- `query_codebase` - Semantic search over odibi code (RAG)
- `reindex` / `get_index_stats` - Manage the semantic index

**Example workflow for creating a custom transformer:**
1. Call `get_transformer_signature` to get the exact pattern
2. Call `list_transformers` to see if similar exists
3. Write the code following the exact signature
4. Call `get_yaml_structure` before writing YAML config

### context7 - Library Documentation
Use context7 when working with external libraries:
- PySpark APIs (window functions, DataFrame ops)
- Pandas APIs
- DuckDB SQL syntax
- Pydantic model validation
- Delta Lake operations
- Azure SDK

### duckdb - Test SQL Before Implementing
Use duckdb MCP to test SQL queries before adding to transformers:
```
Test this SQL: SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY date) FROM table
```

### memory - Persistent Context
Use memory to store/recall important project decisions:
- "Remember: odibi uses DuckDB for Pandas SQL execution"
- "Remember: all transformers need Pandas AND Spark parity"

### git - Version Control
Use git MCP for commits, branches, diffs instead of shell commands.

### filesystem - File Operations
Use filesystem MCP for reading/writing files, searching directories:
- Read file contents
- Create/write files
- List directory contents
- Search for files

### sequential-thinking - Complex Reasoning
Use sequential-thinking MCP for multi-step problem solving:
- Planning implementations
- Debugging complex issues
- Architectural decisions

### fetch - Web Documentation
Use fetch MCP to retrieve web pages and documentation:
- Library documentation
- API references
- Release notes

### When to Use Which MCP
| Task | MCP |
|------|-----|
| **Write odibi code/YAML** | **odibi-knowledge** (FIRST!) |
| Look up PySpark/Pandas API | context7 |
| Test SQL syntax | duckdb |
| Store decisions for later | memory |
| Commit changes | git |
| Read/write files | filesystem |
| Complex problem solving | sequential-thinking |
| Fetch web docs | fetch |

## Don't
- Don't add features without tests
- Don't break engine parity (if Pandas has it, Spark/Polars need it too)
- Don't use hyphens, dots, or spaces in node names
- Don't suppress linter errors with `# type: ignore` without good reason
- Don't hardcode paths - use connection configs
- Don't use emojis in Python code (Windows encoding issues with charmap codec)

## Code Style
- No comments explaining obvious code
- Docstrings for public functions
- Type hints for function signatures
- Follow existing patterns in the codebase

## Debugging Tips
```bash
python -m odibi run config.yaml --dry-run         # Test without writing
python -m odibi run config.yaml --log-level DEBUG # Verbose logging
python -m odibi story last                        # View last execution report
python -m odibi doctor                            # Check environment health
```

## CRITICAL: Transformer Function Signature
```python
# CORRECT signature - params is a Pydantic model
def my_transformer(context: EngineContext, params: MyParams) -> EngineContext:
    sql = f"SELECT *, {params.column} as new_col FROM df"
    return context.sql(sql)

# WRONG - do NOT use this pattern
def my_transformer(context, current, column, value):  # WRONG!
```

## CRITICAL: Custom Transformer Pattern
```python
from pydantic import BaseModel, Field
from odibi.context import EngineContext
from odibi.registry import FunctionRegistry

class MyParams(BaseModel):
    column: str = Field(..., description="Column name")
    value: str = Field(default="x", description="Default value")

def my_transformer(context: EngineContext, params: MyParams) -> EngineContext:
    sql = f"SELECT *, NULLIF({params.column}, '{params.value}') AS result FROM df"
    return context.sql(sql)

# Register it
FunctionRegistry.register(my_transformer, "my_transformer", MyParams)
```

## CRITICAL: Running Pipelines
```python
# CORRECT
from odibi.pipeline import PipelineManager
pm = PipelineManager.from_yaml("config.yaml")
pm.run("pipeline_name")

# WRONG
pm = PipelineManager("config.yaml")  # WRONG!
```

## CRITICAL: YAML Transform Syntax
```yaml
# CORRECT - use steps with function and params
transform:
  steps:
    - function: my_transformer
      params:
        column: name
        value: "N/A"

# WRONG - this syntax does NOT work
transform:
  - my_transformer:
      column: name  # WRONG!
```

## CRITICAL: YAML Input/Output Syntax
```yaml
nodes:
  - name: my_node
    inputs:                          # NOT 'source:'
      input_1:
        connection: my_connection
        path: data/input.csv
        format: csv                  # REQUIRED: csv, parquet, json, delta
    outputs:                         # NOT 'sink:'
      output_1:
        connection: my_connection
        path: data/output.parquet
        format: parquet              # REQUIRED
```

## CRITICAL: YAML Project Structure
```yaml
# Every pipeline YAML needs these top-level keys:
project: my_project          # Required
connections: {}              # Required
story:                       # Required
  connection: conn_name
  path: stories
system:                      # Required
  connection: conn_name
  path: _system
pipelines: []                # Required - list of pipeline configs
```
