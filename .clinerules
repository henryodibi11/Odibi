# Odibi Project Rules for Cline

## üî¥ EXECUTION PRIORITY (HIGHEST PRIORITY)

**For running pipelines and CLI commands, USE SHELL (Terminal) - it's more reliable.**

**For odibi KNOWLEDGE tools (discovery, profiling, YAML generation), use MCP tools.**

| Task | Use |
|------|-----|
| Run pipelines | **Shell**: `python -m odibi run X.yaml` |
| Run odibi CLI | **Shell**: `python -m odibi doctor` |
| Run Python code | **Shell**: `python -c "..."` |
| Profile sources | **MCP**: `profile_source()` |
| Generate YAML | **MCP**: `generate_bronze_node()` |
| List transformers | **MCP**: `list_transformers()` |
| Explain features | **MCP**: `explain()` |
| Diagnose | **MCP**: `diagnose()` |

**Knowledge/discovery = MCP tools. Execution = Shell.**

---

## üö® AGENT BEHAVIOR (READ THIS FIRST)

**You are an ACTIVE agent, not a passive assistant. ACT, don't just suggest.**

### Core Principles
1. **EXECUTE, don't describe** - Use `run_python()` to test code, don't just show it
2. **SEARCH, don't give up** - Use `find_path()` when paths are wrong
3. **RUN, don't just generate** - Use `execute_pipeline()` after creating YAML
4. **ITERATE until done** - If something fails, diagnose and try again
5. **NEVER say "I can't"** - Always try execution tools first

### When Stuck (MANDATORY RECOVERY)
```
diagnose()                    # 1. Check environment
find_path("**/*.yaml")        # 2. Search for files
run_python("print(os.getcwd())")  # 3. Understand state
```

### Execution Tools (USE THESE!)
| Tool | When to Use |
|------|-------------|
| `run_python(code)` | Test code, analyze data, debug, explore, **WRITE FILES** |
| `run_odibi(args)` | Run CLI: "doctor", "run project.yaml", "list transformers" |
| `find_path(pattern)` | Find files: "**/*.yaml", "**/stories/**" |
| `execute_pipeline(path)` | Run pipelines after generating YAML |

### ‚ö†Ô∏è WRITE FILES WITH run_python (NOT filesystem MCP)
The filesystem MCP has path bugs on Windows. Use `run_python` instead.

### üö´ YAML RULES (DO NOT VIOLATE)
1. **NEVER write YAML manually** - Use `generate_bronze_node` tool, it returns correct YAML
2. **ALWAYS use the `yaml_content` field** from `generate_bronze_node` response AS-IS
3. **Imported pipeline files MUST have `pipelines:` list** (not `pipeline:` singular)
4. **NEVER modify generated YAML** unless user explicitly asks
5. **ALWAYS validate with `validate_odibi_config`** before saving

**Correct imported pipeline structure:**
```yaml
pipelines:
  - pipeline: my_pipeline
    nodes:
      - name: my_node
        read:
          connection: my_conn
          path: data.csv
          format: csv
```

**WRONG (will fail):**
```yaml
pipeline: my_pipeline  # WRONG - missing pipelines: list wrapper
```

### Agent Loop Pattern
```
1. User asks for something
2. Gather context (bootstrap_context, profile_source, etc.)
3. Generate solution (YAML, code, etc.)
4. EXECUTE it (run_python, execute_pipeline)
5. If error ‚Üí diagnose ‚Üí fix ‚Üí retry (go to step 4)
6. Only respond "done" when verified working
```

---

## About Odibi
Odibi is a declarative data pipeline framework with 52+ transformers, 6 DWH patterns, and ~57 MCP tools for AI-assisted development.

## MCP Documentation
- **Tool Reference**: `docs/guides/mcp_guide.md` - All tools with examples
- **AI Recipes**: `docs/guides/mcp_recipes.md` - 14 workflow recipes for common tasks
- **System Prompt**: `docs/guides/mcp_system_prompt.md` - Prompts and snippets

## MCP Tool Selection Guide

**Choose the right tool automatically based on task:**

| Task Type | MCP to Use |
|-----------|------------|
| Odibi transformers, patterns, YAML | `odibi-knowledge` tools |
| Query pipeline runs, debug failures | `odibi-knowledge` MCP Facade tools |
| Complex planning, tradeoff analysis | `sequential-thinking` |
| Read/write/search files | `filesystem` |
| Web documentation, API docs | `fetch` |
| Remember user preferences/context | `memory` |
| Git history, commits, branches | `git` |

**Auto-trigger rules:**
- "explain", "list", "generate YAML", "transformer", "pattern" ‚Üí use `odibi-knowledge`
- "why did pipeline fail", "show sample", "lineage", "schema" ‚Üí use MCP Facade tools
- "plan", "analyze tradeoffs", "step by step" ‚Üí use `sequential-thinking`
- "remember", "recall", "my preferences" ‚Üí use `memory`
- "fetch docs", "get documentation for" ‚Üí use `fetch`
- "create file", "write to", "save as" ‚Üí use `filesystem`

## ‚ö†Ô∏è FIRST THING TO DO (MANDATORY)

**At the START of every conversation, call `bootstrap_context` to auto-gather project context:**

```
bootstrap_context()  # Returns: project, connections, pipelines, outputs, patterns, YAML rules
```

This gives you everything needed to understand the project in one call.

---

## ‚ö†Ô∏è CONTEXT-FIRST RULE (MANDATORY)

**Before suggesting ANY solution, the AI MUST gather full context using these workflows.**

### Workflow A: Full Source Understanding (BEFORE Building Anything)
```
map_environment(connection, path)      # 1. Scout what files/tables exist
profile_source(connection, path)       # 2. See schema, sample data, encoding, AI suggestions
```
**ONLY AFTER seeing real data** may the AI suggest patterns or generate YAML.

### Workflow B: Pipeline Deep Dive (BEFORE Modifying Anything)
```
list_outputs(pipeline)                 # 1. What outputs exist?
output_schema(pipeline, output)        # 2. What's the schema?
lineage_graph(pipeline, true)          # 3. How do nodes connect?
node_describe(pipeline, node)          # 4. What does this node do?
node_sample_in(pipeline, node, 10)     # 5. What data flows in?
story_read(pipeline)                   # 6. Recent run history?
```

### Workflow C: Framework Mastery Check (BEFORE Suggesting Solutions)
```
explain(name)                          # Verify transformer/pattern understanding
get_example(pattern_name)              # Get working example
get_yaml_structure()                   # Verify YAML structure
```
**NEVER guess parameters** ‚Äî always use `explain` first.

### Workflow D: Complete Debug Investigation (BEFORE Suggesting Fixes)
```
story_read(pipeline)                   # 1. Which node failed?
node_describe(pipeline, failed_node)   # 2. What was it supposed to do?
node_sample_in(pipeline, failed_node)  # 3. What data actually arrived?
node_failed_rows(pipeline, failed_node)# 4. What rows failed validation?
diagnose_error(error_message)          # 5. Get AI diagnosis
```

### Workflow F: New Project Onboarding (Complete Context)
```
get_deep_context()                     # Full framework docs
list_patterns()                        # Available patterns
list_connections()                     # Available connections
list_transformers()                    # Available transformers
list_outputs(pipeline)                 # For each pipeline in scope
lineage_graph(pipeline)                # Understand data flow
```

## Anti-Patterns (NEVER DO)

| ‚ùå Don't | ‚úÖ Do Instead |
|----------|--------------|
| Guess column names | Use `profile_source` to see real schema |
| Assume transformer params | Use `explain` to verify |
| Generate YAML without validation | Run `test_node` or `validate_yaml` first |
| Suggest fixes without evidence | Use `node_sample_in` to see data |
| Skip lineage understanding | Use `lineage_graph` before changes |

## Context Checklist (Verify Before Acting)

- [ ] Seen actual source data (`profile_source`)
- [ ] Know exact column names/types (`profile_source`)
- [ ] Understand the pattern (`explain`)
- [ ] Validated YAML (`test_node` or `validate_yaml`)
- [ ] Checked lineage impact (`lineage_graph`)

---

## AI Workflow Recipes (Quick Reference)

### Recipe: Build New Pipeline (Lazy Bronze)
1. `map_environment(connection, path)` ‚Üí scout what exists
2. `profile_source(connection, path)` ‚Üí get schema, encoding, AI suggestions
3. `generate_bronze_node(profile)` ‚Üí generate YAML
4. `test_node(yaml)` ‚Üí validate before saving
5. `suggest_pattern` / `get_example` ‚Üí for complex patterns

**CRITICAL: When displaying `generate_bronze_node` response:**
- Always show the COMPLETE `yaml_content` field from the response
- This is a full runnable project YAML with connections, system, story, and pipelines
- Do NOT truncate or show only the pipeline portion
- The user needs the complete YAML to save and run with `python -m odibi run <file>.yaml`

### Recipe: Debug Pipeline Failure
1. `story_read` ‚Üí check run status
2. `node_describe` ‚Üí get failed node details
3. `node_sample_in` ‚Üí see what data node received
4. `node_failed_rows` ‚Üí see validation failures
5. `diagnose_error` ‚Üí get fix suggestions

### Recipe: Explore Available Data
1. `map_environment(connection, path)` ‚Üí scout files/tables
2. `profile_source(connection, path)` ‚Üí get full schema and samples
3. `profile_folder(connection, folder)` ‚Üí batch profile all files

### Recipe: Learn About Odibi Feature
1. `explain(name)` ‚Üí get specific feature docs
2. `search_docs(query)` ‚Üí search documentation
3. `query_codebase(question)` ‚Üí search source code

## CRITICAL: Safe Editing Practices

**NEVER replace entire files.** Make surgical, targeted edits:
- Edit ONE function/docstring at a time
- Show the specific change, not the whole file
- Use diff-style edits when possible
- After each edit, verify the file still has all its code

**Before editing large files:**
1. Count functions/classes in the file
2. After edit, verify the same count exists
3. If code was accidentally removed, STOP and restore via `git checkout -- <file>`

**Preferred workflow for docstring improvements:**
1. List each function needing changes
2. Edit each docstring ONE AT A TIME
3. Run `ruff check <file>` after each batch

## CRITICAL: Verification After Changes

**After EVERY code change:**
1. Run `ruff check <file> --fix` to fix lint issues
2. Run `ruff format <file>` to format
3. Run relevant tests: `pytest tests/unit/test_<module>.py -v`
4. If tests fail, fix before moving on

**Before committing or saying "done":**
- Verify the file is syntactically valid: `python -m py_compile <file>`
- Check for import errors: `python -c "import odibi.<module>"`

## CRITICAL: What NOT To Do

**NEVER:**
- Delete code unless explicitly asked
- Modify multiple unrelated files in one edit
- Skip running tests after changes
- Assume imports exist ‚Äî check first
- Add dependencies without asking
- Change function signatures without updating all callers
- Use `# type: ignore` or `# noqa` to hide errors

**ALWAYS ask before:**
- Deleting any function, class, or file
- Refactoring across multiple files
- Adding new dependencies to pyproject.toml
- Changing public API signatures

## CRITICAL: Handling Errors

**If you encounter an error:**
1. Show the full error message
2. Explain what went wrong
3. Propose a fix but WAIT for approval before applying
4. If you broke something, restore with `git checkout -- <file>`

**If tests fail after your change:**
1. Do NOT proceed to other tasks
2. Fix the failing test first
3. If you can't fix it, revert your change

## IMPORTANT: Use MCP Tools First!

Before writing ANY odibi code, call these odibi-knowledge MCP tools:

### MCP Facade Tools (Query Pipelines & Debug)

Use these to query running pipelines, view data, and debug failures:

**Smart Discovery Tools** - Lazy Bronze workflow:
| Tool | Parameters | Example |
|------|------------|---------|
| `map_environment` | `connection`, `path` | `map_environment("my_adls", "raw/")` |
| `profile_source` | `connection`, `path` | `profile_source("my_adls", "raw/data.csv")` |
| `profile_folder` | `connection`, `folder_path`, `pattern` | `profile_folder("my_adls", "raw/", "*.csv")` |
| `generate_bronze_node` | `profile`, `node_name`, `local_output` | `generate_bronze_node(profile_result)` ‚Üí **ALWAYS display full `yaml_content`** |
| `test_node` | `node_yaml`, `max_rows` | `test_node(yaml_content, 100)` |

**Discovery Tools** - Quick lookups:
| Tool | Parameters | Example |
|------|------------|---------|
| `describe_table` | `connection`, `table`, `schema` | `describe_table("my_database", "dim_date", "dbo")` |
| `list_sheets` | `connection`, `path` | `list_sheets("my_storage", "data.xlsx")` |
| `list_schemas` | `connection` | `list_schemas("my_database")` |

**Story Tools** - Inspect pipeline runs:
| Tool | Parameters | Example |
|------|------------|---------|
| `story_read` | `pipeline` | `story_read("bronze")` |
| `story_diff` | `pipeline`, `run_a`, `run_b` | `story_diff("bronze", "latest", "previous")` |
| `node_describe` | `pipeline`, `node` | `node_describe("bronze", "customers_raw")` |

**Sample Tools** - View data:
| Tool | Parameters | Example |
|------|------------|---------|
| `node_sample` | `pipeline`, `node`, `max_rows` | `node_sample("bronze", "customers_raw", 10)` |
| `node_sample_in` | `pipeline`, `node`, `input_name`, `max_rows` | `node_sample_in("bronze", "customers_raw", "default", 10)` |
| `node_failed_rows` | `pipeline`, `node`, `max_rows` | `node_failed_rows("bronze", "customers_raw", 50)` |

**Lineage Tools**:
| Tool | Parameters | Example |
|------|------------|---------|
| `lineage_upstream` | `pipeline`, `node`, `depth` | `lineage_upstream("silver", "fact_orders", 3)` |
| `lineage_downstream` | `pipeline`, `node`, `depth` | `lineage_downstream("bronze", "customers_raw", 3)` |
| `lineage_graph` | `pipeline`, `include_external` | `lineage_graph("bronze", false)` |

**Schema Tools**:
| Tool | Parameters | Example |
|------|------------|---------|
| `list_outputs` | `pipeline` | `list_outputs("bronze")` |
| `output_schema` | `pipeline`, `output_name` | `output_schema("bronze", "customers_raw")` |
| `compare_schemas` | `source_connection`, `source_path`, `target_connection`, `target_path` | `compare_schemas("raw", "data.csv", "bronze", "output.parquet")` |

**Catalog Tools**:
| Tool | Parameters | Example |
|------|------------|---------|
| `node_stats` | `pipeline`, `node` | `node_stats("bronze", "customers_raw")` |
| `pipeline_stats` | `pipeline` | `pipeline_stats("bronze")` |
| `failure_summary` | `pipeline`, `max_failures` | `failure_summary("bronze", 10)` |
| `schema_history` | `pipeline`, `node` | `schema_history("bronze", "customers_raw")` |

### Core Tools
- `list_transformers` - See all 52+ available transformers
- `list_patterns` - See all 6 DWH patterns
- `list_connections` - See all connection types
- `explain(name)` - Get docs for any transformer/pattern/connection

### Code Generation Tools
- `get_transformer_signature` - Get exact function signature for custom transformers
- `get_yaml_structure` - Get exact YAML pipeline structure
- `generate_transformer` - Generate complete transformer Python code
- `generate_pipeline_yaml` - Generate complete pipeline YAML config
- `validate_yaml` - Validate YAML before saving

### Decision Support Tools
- `suggest_pattern` - Recommend the right pattern for your use case
- `get_engine_differences` - Spark vs Pandas vs Polars SQL differences
- `get_validation_rules` - All validation rule types with examples

### Documentation Tools
- `get_deep_context` - Get full framework documentation (2300+ lines)
- `get_doc(path)` - Get any specific doc (e.g., "docs/patterns/scd2.md")
- `search_docs(query)` - Search all docs for a term
- `get_example(name)` - Get working example for any pattern/transformer

### Debugging Tools
- `diagnose_error` - Diagnose odibi errors and get fix suggestions
- `query_codebase` - Semantic search over odibi code (RAG)

**Example workflow for creating a custom transformer:**
1. Call `get_transformer_signature` to get the exact pattern
2. Call `list_transformers` to see if similar exists
3. Write the code following the exact signature
4. Call `get_yaml_structure` before writing YAML config

**Fallback order if MCP times out:**
1. Try CLI commands: `python -m odibi list transformers --format json`
2. Read `docs/ODIBI_DEEP_CONTEXT.md` for comprehensive reference

## Required Reading

Before making changes to odibi, read `docs/ODIBI_DEEP_CONTEXT.md` for complete framework context (2,200+ lines covering all features).

## File Paths

Use paths relative to the workspace root. The workspace is wherever ODIBI_CONFIG points to.
For this laptop: `C:/Users/hodibi/OneDrive - Ingredion/Desktop/Repos/Odibi`

## CRITICAL: Proactive Troubleshooting Behavior

**When a tool fails or returns "not found":**

1. **DO NOT give up immediately.** Instead:
   - Use `diagnose` tool to check environment, paths, and connections
   - Use filesystem tools to explore the actual directory structure
   - Create and run a Python script to investigate the issue

2. **When `node_sample` or `story_read` fails:**
   ```
   diagnose()                           # Check paths, env vars, connections
   # Then use filesystem to explore:
   filesystem.list_directory("projects/data/_stories")
   filesystem.list_directory("projects/data/_system")
   ```

3. **When paths seem wrong:**
   - Check if the path exists using filesystem tools
   - Check the connection's `base_path` configuration
   - Run a quick Python script to verify:
   ```python
   from pathlib import Path
   print(Path("projects/data/_stories").exists())
   print(list(Path("projects/data").iterdir()))
   ```

4. **When environment variables aren't working:**
   - Run `diagnose()` to see what's actually set
   - Create a debug script to check env loading

5. **When connections fail:**
   - Check the .env file exists and is properly formatted
   - Verify no quotes around values in .env
   - Check for Base64 padding issues (keys should be 88 chars ending in ==)

**NEVER say "I can't do this" without first:**
- Calling `diagnose()` to understand the environment
- Using `find_path("**/*.yaml")` to search for files
- Using `run_python()` to execute diagnostic code directly

## CRITICAL: Execution Tools (BE ACTIVE!)

These tools let you ACT instead of just suggesting:

| Tool | What It Does | Example |
|------|--------------|---------|
| `run_python` | Execute Python code, return output | `run_python("import os; print(os.getcwd())")` |
| `run_odibi` | Run odibi CLI commands | `run_odibi("run projects/bronze.yaml --dry-run")` |
| `find_path` | Search for files by glob pattern | `find_path("**/*.yaml")` |
| `execute_pipeline` | Run a pipeline | `execute_pipeline("projects/bronze.yaml", dry_run=True)` |

**When stuck on paths:**
```
find_path("**/*.yaml")           # Find all YAML files
find_path("**/stories/**")       # Find story directories
find_path("**/*bronze*")         # Find anything with 'bronze'
```

**When you need to understand data:**
```
run_python('''
import pandas as pd
df = pd.read_csv("data.csv")
print(df.head())
print(df.dtypes)
''')
```

**When you need to run a pipeline:**
```
execute_pipeline("projects/bronze.yaml", dry_run=True)  # Validate first
execute_pipeline("projects/bronze.yaml")                 # Then run for real
```

**Example troubleshooting workflow (ACTIVE):**
```
# Step 1: Something failed - diagnose environment
diagnose()

# Step 2: Can't find a file? SEARCH for it
find_path("**/*.yaml")
find_path("**/stories/**")

# Step 3: Need to understand data? RUN Python
run_python('''
from pathlib import Path
import os
print("CWD:", os.getcwd())
for p in Path(".").rglob("*.yaml"):
    print("Found:", p)
''')

# Step 4: Test a pipeline
execute_pipeline("projects/bronze.yaml", dry_run=True)
```

## Project Structure

```
odibi/
‚îú‚îÄ‚îÄ engine/          # Engine implementations (spark.py, pandas.py, polars.py)
‚îú‚îÄ‚îÄ patterns/        # DWH patterns (dimension.py, fact.py, scd2.py, etc.)
‚îú‚îÄ‚îÄ transformers/    # SQL transforms (must work on all engines)
‚îú‚îÄ‚îÄ validation/      # Data quality, quarantine, FK validation
‚îú‚îÄ‚îÄ connections/     # Storage connections (local, azure, delta, etc.)
‚îú‚îÄ‚îÄ cli/             # CLI commands
‚îú‚îÄ‚îÄ config.py        # All Pydantic config models (source of truth)
‚îú‚îÄ‚îÄ context.py       # Engine contexts (Spark temp views, DuckDB)
‚îú‚îÄ‚îÄ pipeline.py      # Pipeline orchestration
tests/unit/          # Tests matching source structure
docs/                # Documentation
```

## Key Patterns

1. **Engine Parity**: All code must work on Pandas, Spark, AND Polars
2. **SQL-based transforms**: Use SQL (DuckDB for Pandas, Spark SQL for Spark)
3. **Pydantic models**: All config uses Pydantic for validation
4. **Structured logging**: Use `get_logging_context()` for logs

## Critical Runtime Behavior

- Spark: Node outputs registered via `createOrReplaceTempView(node_name)`
- Pandas: SQL executed via DuckDB (different syntax than Spark SQL)
- Node names: Must be alphanumeric + underscore only (Spark compatibility)

## Adding New Features

### New Transformer
1. Check existing: `python -m odibi list transformers`
2. Add to `odibi/transformers/` following existing patterns
3. Register in `odibi/transformers/__init__.py`
4. Add tests in `tests/unit/transformers/`
5. Must work on Pandas AND Spark (engine parity)

### New Pattern
1. Check existing: `python -m odibi list patterns`
2. Add to `odibi/patterns/` extending `Pattern` base class
3. Register in `odibi/patterns/__init__.py`
4. Add tests

### New Connection
1. Check existing: `python -m odibi list connections`
2. Add to `odibi/connections/`
3. Register in `odibi/connections/factory.py`

## Testing

```bash
pytest tests/ -v                        # Run all tests
pytest tests/unit/test_X.py -v          # Run specific test
pytest tests/unit/test_X.py::test_name  # Run single test
pytest --tb=short                       # Shorter tracebacks
```

## Linting (RUN AFTER EVERY FILE CHANGE!)

```bash
ruff check <file> --fix                 # Fix lint issues
ruff format <file>                      # Format code
```
**ALWAYS run both commands after creating or modifying any Python file.**

## Don't

- Don't add features without tests
- Don't break engine parity (if Pandas has it, Spark/Polars need it too)
- Don't use hyphens, dots, or spaces in node names
- Don't suppress linter errors with `# type: ignore` without good reason
- Don't hardcode paths - use connection configs
- Don't use emojis in Python code (Windows encoding issues with charmap codec)

## Code Style

- No comments explaining obvious code
- Docstrings for public functions
- Type hints for function signatures
- Follow existing patterns in the codebase

## Debugging Tips

```bash
python -m odibi run config.yaml --dry-run         # Test without writing
python -m odibi run config.yaml --log-level DEBUG # Verbose logging
python -m odibi story last                        # View last execution report
python -m odibi doctor                            # Check environment health
```

## CRITICAL: Transformer Function Signature

```python
# CORRECT signature - params is a Pydantic model
def my_transformer(context: EngineContext, params: MyParams) -> EngineContext:
    sql = f"SELECT *, {params.column} as new_col FROM df"
    return context.sql(sql)

# WRONG - do NOT use this pattern
def my_transformer(context, current, column, value):  # WRONG!
```

## CRITICAL: Custom Transformer Pattern

```python
from pydantic import BaseModel, Field
from odibi.context import EngineContext
from odibi.registry import FunctionRegistry

class MyParams(BaseModel):
    column: str = Field(..., description="Column name")
    value: str = Field(default="x", description="Default value")

def my_transformer(context: EngineContext, params: MyParams) -> EngineContext:
    sql = f"SELECT *, NULLIF({params.column}, '{params.value}') AS result FROM df"
    return context.sql(sql)

# Register it
FunctionRegistry.register(my_transformer, "my_transformer", MyParams)
```

## CRITICAL: Running Pipelines

```python
# CORRECT
from odibi.pipeline import PipelineManager
pm = PipelineManager.from_yaml("config.yaml")
pm.run("pipeline_name")

# WRONG
pm = PipelineManager("config.yaml")  # WRONG!
```

## CRITICAL: YAML Transform Syntax

```yaml
# CORRECT - use steps with function and params
transform:
  steps:
    - function: my_transformer
      params:
        column: name
        value: "N/A"

# WRONG - this syntax does NOT work
transform:
  - my_transformer:
      column: name  # WRONG!
```

## CRITICAL: YAML Input/Output Syntax

```yaml
nodes:
  - name: my_node
    read:                            # Flat structure, NOT nested under 'default:'
      connection: my_connection
      path: data/input.csv
      format: csv                    # REQUIRED: csv, parquet, json, delta, sql
      options:                       # Optional - use pandas-compatible names
        sep: "\t"                    # NOT 'delimiter'
        skiprows: 3                  # NOT 'skipRows' (lowercase!)
        encoding: utf-8
    write:                           # Flat structure, NOT nested
      connection: my_connection
      path: data/output
      format: delta                  # REQUIRED: delta, parquet, csv
```

**Option name mapping (use pandas names):**
- `delimiter` ‚Üí `sep`
- `skipRows` ‚Üí `skiprows`
- `header: 'true'` ‚Üí `header: 0` (or omit, defaults to first row)

## CRITICAL: YAML Project Structure

```yaml
# Every pipeline YAML needs these top-level keys:
project: my_project          # Required
connections: {}              # Required
story:                       # Required
  connection: conn_name
  path: stories
system:                      # Required
  connection: conn_name
  path: _system
pipelines: []                # Required - list of pipeline configs
```

## Exploration Mode (Data Discovery)

For data exploration without building pipelines, use a minimal config:

```yaml
# exploration.yaml - connections only, no pipelines needed
project: my_exploration  # optional
connections:
  my_sql:
    type: azure_sql
    connection_string: ${SQL_CONN}
  local:
    type: local
    path: ./data
```

**Environment Variables:**
- `ODIBI_CONFIG` - Path to exploration.yaml (connections for discovery)
- `ODIBI_PROJECTS_DIR` - Path to projects folder (for auto-discovery of generated projects)

**Smart discovery tools work in exploration mode:**
- `map_environment`, `profile_source`, `profile_folder`
- `generate_bronze_node`, `test_node`
- `describe_table`, `list_sheets`, `list_schemas`
- `list_projects` - Lists all projects in ODIBI_PROJECTS_DIR

**These tools auto-discover projects from ODIBI_PROJECTS_DIR:**
- `story_read`, `node_sample`, `node_sample_in`, `node_failed_rows`
- `lineage_*`, `node_describe`, `list_outputs`, `output_schema`

**Workflow:**
1. Use exploration.yaml for data discovery (`map_environment`, `profile_source`)
2. Generate project YAML with `generate_bronze_node` ‚Üí saves to projects/ folder
3. Run pipeline: `python -m odibi run projects/my_project.yaml`
4. Sampling tools auto-discover the project by pipeline name
