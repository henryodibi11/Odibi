# Architecture Guide - Odibi System Design

**Visual guide to how Odibi works. See the big picture!**

---

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER                                  â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚                   config.yaml                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CONFIG LAYER                               â”‚
â”‚                                                               â”‚
â”‚  config.yaml â†’ Pydantic Models â†’ ProjectConfig               â”‚
â”‚                     â†“                                         â”‚
â”‚              Validation happens here                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PIPELINE LAYER                              â”‚
â”‚                                                               â”‚
â”‚  Pipeline â†’ DependencyGraph â†’ Execution Order                â”‚
â”‚                     â†“                                         â”‚
â”‚              [Node A, Node B, Node C]                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NODE LAYER                                 â”‚
â”‚                                                               â”‚
â”‚  Node â†’ Read/Transform/Write â†’ Engine                        â”‚
â”‚           â†“                      â†“                            â”‚
â”‚    Transformation           PandasEngine                      â”‚
â”‚      Registry               or SparkEngine                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STORAGE LAYER                              â”‚
â”‚                                                               â”‚
â”‚  Connections â†’ Local / Azure / SQL                           â”‚
â”‚                     â†“                                         â”‚
â”‚               Actual Data                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STORY LAYER                                â”‚
â”‚                                                               â”‚
â”‚  Metadata â†’ Renderers â†’ HTML/MD/JSON                         â”‚
â”‚      â†“                                                        â”‚
â”‚  Automatic audit trail                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Pipeline Execution Flow

### Step-by-Step: What Happens When You Run `odibi run config.yaml`

```
1. CLI Entry Point (cli/main.py)
   â”‚
   â”œâ”€â†’ Parse arguments
   â””â”€â†’ Call run_command(args)

2. Load Configuration (cli/run.py)
   â”‚
   â”œâ”€â†’ Read YAML file
   â”œâ”€â†’ Parse to ProjectConfig (Pydantic validation)
   â””â”€â†’ Create PipelineManager

3. Build Dependency Graph (graph.py)
   â”‚
   â”œâ”€â†’ Extract all nodes
   â”œâ”€â†’ Build dependency edges
   â”œâ”€â†’ Check for cycles
   â””â”€â†’ Topological sort â†’ execution order

4. Execute Nodes (pipeline.py)
   â”‚
   â”œâ”€â†’ For each node in order:
   â”‚   â”‚
   â”‚   â”œâ”€â†’ Create Node instance (node.py)
   â”‚   â”œâ”€â†’ Execute read/transform/write
   â”‚   â”‚   â”‚
   â”‚   â”‚   â”œâ”€â†’ Read: Engine.read() â†’ DataFrame
   â”‚   â”‚   â”œâ”€â†’ Transform: Registry.get(operation) â†’ transformed DataFrame  
   â”‚   â”‚   â””â”€â†’ Write: Engine.write(DataFrame)
   â”‚   â”‚
   â”‚   â”œâ”€â†’ Store result in Context
   â”‚   â””â”€â†’ Track metadata (timing, rows, schema)
   â”‚
   â””â”€â†’ All nodes complete

5. Generate Story (story/generator.py)
   â”‚
   â”œâ”€â†’ Collect all node metadata
   â”œâ”€â†’ Calculate aggregates (success rate, total rows)
   â”œâ”€â†’ Render to HTML/MD/JSON
   â””â”€â†’ Save to stories/runs/

6. Return to User
   â”‚
   â””â”€â†’ "Pipeline completed successfully" âœ…
```

---

## Module Dependencies

### Core Dependencies

```
config.py (no dependencies - pure Pydantic models)
    â†“
context.py (stores DataFrames)
    â†“
transformations/ (registry + decorators)
    â†“
operations/ (uses transformations)
    â†“
engine/ (executes operations)
    â†“
node.py (uses engine + context)
    â†“
graph.py (orders nodes)
    â†“
pipeline.py (orchestrates everything)
    â†“
story/ (documents execution)
    â†“
cli/ (user interface)
```

### Module Relationships

```
transformations/
    â”œâ”€â†’ Used by: operations/, node.py, story/doc_story.py
    â””â”€â†’ Uses: Nothing (core module)

operations/
    â”œâ”€â†’ Used by: Pipelines (via registry lookup)
    â””â”€â†’ Uses: transformations/

story/
    â”œâ”€â†’ Used by: pipeline.py, cli/story.py
    â””â”€â†’ Uses: config.py, validation/, transformations/

connections/
    â”œâ”€â†’ Used by: engine/
    â””â”€â†’ Uses: Nothing (independent connectors)

engine/
    â”œâ”€â†’ Used by: node.py
    â””â”€â†’ Uses: connections/

cli/
    â”œâ”€â†’ Used by: Users!
    â””â”€â†’ Uses: Everything
```

**Key insight:** `transformations/` is at the core. Everything builds on it.

---

## Data Flow

### How Data Moves Through a Pipeline

```
1. User YAML Config
   â†“
2. Parsed to ProjectConfig (in-memory objects)
   â†“
3. Pipeline.run() starts execution
   â†“
4. For each node:

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Node Execution                           â”‚
   â”‚                                          â”‚
   â”‚  1. Read Phase (if configured)           â”‚
   â”‚     â””â”€â†’ Engine.read() â†’ DataFrame        â”‚
   â”‚           â””â”€â†’ Connection.get_path()      â”‚
   â”‚                 â””â”€â†’ Actual file/DB       â”‚
   â”‚                                          â”‚
   â”‚  2. Transform Phase (if configured)      â”‚
   â”‚     â”œâ”€â†’ Get DataFrame from context       â”‚
   â”‚     â”œâ”€â†’ Registry.get(operation)          â”‚
   â”‚     â””â”€â†’ func(df, **params) â†’ DataFrame   â”‚
   â”‚                                          â”‚
   â”‚  3. Write Phase (if configured)          â”‚
   â”‚     â””â”€â†’ Engine.write(DataFrame)          â”‚
   â”‚           â””â”€â†’ Connection + format        â”‚
   â”‚                                          â”‚
   â”‚  4. Store Result                         â”‚
   â”‚     â””â”€â†’ Context.set(node_name, df)       â”‚
   â”‚                                          â”‚
   â”‚  5. Track Metadata                       â”‚
   â”‚     â””â”€â†’ NodeExecutionMetadata            â”‚
   â”‚           â”œâ”€â†’ Row counts                 â”‚
   â”‚           â”œâ”€â†’ Schema                     â”‚
   â”‚           â””â”€â†’ Timing                     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
5. All nodes complete
   â†“
6. Generate Story
   â””â”€â†’ PipelineStoryMetadata
       â”œâ”€â†’ All node metadata
       â””â”€â†’ Rendered to HTML/MD/JSON
```

---

## Transformation Lifecycle

### Registration (Import Time)

```python
# When Python imports odibi/operations/unpivot.py:

@transformation("unpivot", category="reshaping")  # â† This runs immediately!
def unpivot(df, ...):
    ...

# What happens:
# 1. transformation("unpivot", ...) returns a decorator
# 2. Decorator wraps unpivot function
# 3. Decorator calls registry.register("unpivot", wrapped_unpivot)
# 4. Registry stores it globally
# 5. Function is now available to all pipelines
```

### Lookup (Runtime)

```python
# During pipeline execution:

# 1. Node config says: operation="unpivot"
# 2. Node calls: registry.get("unpivot")
# 3. Registry returns the function
# 4. Node calls: func(df, id_vars="ID", ...)
# 5. Result returned
```

### Explanation (Story Generation)

```python
# During story generation:

# 1. Story generator calls: func.get_explanation(**params, **context)
# 2. ExplainableFunction looks for attached explain_func
# 3. If found: calls explain_func(**params, **context)
# 4. Returns formatted markdown
# 5. Included in HTML story
```

---

## Storage Architecture

### Connection Abstraction

```
BaseConnection (interface)
    â†“
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        â”‚          â”‚             â”‚
Local   ADLS    AzureSQL      (more...)
â”‚        â”‚          â”‚
â†“        â†“          â†“
./data  Azure Blob  SQL Database
```

**All connections implement:**
- `get_path(relative_path)` - Resolve full path
- `validate()` - Check configuration

**Storage-specific methods:**
- ADLS: `pandas_storage_options()`, `configure_spark()`
- AzureSQL: `read_sql()`, `write_table()`, `get_engine()`
- Local: (just path manipulation)

### Engine Abstraction

```
Engine (interface)
    â†“
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚
PandasEngine  SparkEngine
â”‚                â”‚
â†“                â†“
DataFrame    pyspark.DataFrame
```

**All engines implement:**
- `read(connection, path, format, options)`
- `write(df, connection, path, format, mode, options)`
- `execute_sql(df, query)`

**Why?** Swap Pandas â†” Spark without changing config!

---

## Story Generation Architecture

### Three Types of Stories

```
1. RUN STORIES (automatic)
   â”‚
   â””â”€â†’ Generated during pipeline.run()
       â”œâ”€â†’ Captures actual execution
       â”œâ”€â†’ Saved to stories/runs/
       â””â”€â†’ For audit/debugging

2. DOC STORIES (on-demand)
   â”‚
   â””â”€â†’ Generated via CLI: odibi story generate
       â”œâ”€â†’ Pulls operation explanations
       â”œâ”€â†’ For stakeholder communication
       â””â”€â†’ Saved to docs/

3. DIFF STORIES (comparison)
   â”‚
   â””â”€â†’ Generated via CLI: odibi story diff
       â”œâ”€â†’ Compares two run stories
       â”œâ”€â†’ Shows what changed
       â””â”€â†’ For troubleshooting
```

### Story Generation Pipeline

```
Execution â†’ Metadata Collection â†’ Rendering â†’ Output
   â†“              â†“                   â†“          â†“
Nodes run    NodeExecution      Renderer    HTML file
             Metadata          (HTML/MD/JSON)
             tracked
```

---

## The Registry Pattern (Deep Dive)

### Why This Pattern?

**Problem:** How do we make operations available globally?

**Bad Solution 1:** Import everything

```python
from odibi.operations import pivot, unpivot, join, sql, ...
# Breaks as we add more operations
```

**Bad Solution 2:** String-based imports

```python
op_module = __import__(f"odibi.operations.{operation_name}")
# Fragile, hard to debug
```

**Good Solution:** Registry

```python
# Operations register themselves:
@transformation("pivot")
def pivot(...): ...

# Look up by name:
func = registry.get("pivot")

# Easy! Scalable! Type-safe!
```

### Registry Singleton Pattern

**One registry for entire process:**

```python
# odibi/transformations/registry.py

# Create once at module level
_global_registry = TransformationRegistry()

def get_registry():
    return _global_registry  # Always same instance
```

**Benefits:**
- âœ… Single source of truth
- âœ… Operations registered once
- âœ… Available everywhere
- âœ… Easy to test (registry.clear() in tests)

---

## Error Handling Strategy

### Validation Layers

```
Layer 1: Pydantic (config validation)
   â†“
Layer 2: Connection.validate() (connection validation)
   â†“
Layer 3: Graph.validate() (dependency validation)
   â†“
Layer 4: Runtime (execution errors)
```

**Fail fast:** Catch errors before execution starts!

### Error Propagation

```python
try:
    # Node execution
    result = node.execute()
except Exception as e:
    # Caught by node.py
    node_result = NodeResult(
        success=False,
        error=e
    )
    # Stored in metadata
    # Shown in story
    # Pipeline continues (or stops, depending on config)
```

**Stories capture all errors** - makes debugging easy!

---

## Performance Characteristics

### Time Complexity

- **Config loading:** O(1) - just YAML parse
- **Dependency graph:** O(n + e) - n nodes, e edges
- **Node execution:** O(n) - linear in number of nodes
- **Story generation:** O(n) - linear in number of nodes

### Space Complexity

- **Context storage:** O(n Ã— m) - n nodes, m average DataFrame size
- **Metadata:** O(n) - one metadata object per node
- **Stories:** O(n) - proportional to nodes

### Optimization Points

**1. Parallel Execution** (future)
```
Current: A â†’ B â†’ C â†’ D (sequential)
Future:  A â†’ B â”
         A â†’ C â”´â†’ D (parallel)
```

**2. Lazy Evaluation** (future)
```
Current: Execute all nodes
Future:  Only execute nodes needed for requested output
```

**3. Incremental Processing** (future)
```
Current: Reprocess all data every time
Future:  Only process new/changed data
```

---

## Testing Architecture

### Test Pyramid

```
        /\
       /E2E\          â† 10 tests (slow, comprehensive)
      /â”€â”€â”€â”€â”€â”€\
     /Integration\    â† 30 tests (medium speed)
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
   /   Unit Tests  \  â† 380+ tests (fast, focused)
  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

**Unit Tests** (380+)
- Test individual functions/classes
- Use mocks for external dependencies
- Run in <5 seconds
- Cover edge cases

**Integration Tests** (30)
- Test components working together
- Use real files (temp directories)
- Run in <10 seconds
- Cover common scenarios

**E2E Tests** (10)
- Test complete pipelines
- Real configs, real data
- Run in <30 seconds
- Cover critical paths

### Mocking Strategy

**Mock external dependencies, not internal ones:**

```python
# âœ… Good: Mock external SQLAlchemy
@patch('sqlalchemy.create_engine')
def test_azure_sql(mock_engine):
    conn = AzureSQL(...)
    # Test connection logic without real DB

# âŒ Bad: Mock internal functions
@patch('odibi.operations.pivot')
def test_something(mock_pivot):
    # Doesn't test real code!
```

---

## Design Patterns Used

### 1. Registry Pattern

**Where:** `transformations/registry.py`

**Purpose:** Centralized operation lookup

**Example:** All operations register themselves globally

### 2. Factory Pattern

**Where:** `story/renderers.py` (`get_renderer()`)

**Purpose:** Create renderers by format name

```python
renderer = get_renderer("html")  # Returns HTMLStoryRenderer
renderer = get_renderer("md")    # Returns MarkdownStoryRenderer
```

### 3. Decorator Pattern

**Where:** `transformations/decorators.py`, `explanation.py`

**Purpose:** Enhance functions with metadata

```python
@transformation("op")  # Adds registration
@op.explain           # Adds explanation
```

### 4. Strategy Pattern

**Where:** `engine/` (PandasEngine vs SparkEngine)

**Purpose:** Swap execution strategies

```python
# Same interface, different implementation:
engine = PandasEngine()  # or SparkEngine()
df = engine.read(...)    # Works with either!
```

### 5. Builder Pattern

**Where:** `story/doc_story.py`

**Purpose:** Construct complex documentation

```python
generator = DocStoryGenerator(config)
generator.generate(
    output_path="doc.html",
    format="html",
    theme=CORPORATE_THEME
)
```

### 6. Template Method Pattern

**Where:** `story/renderers.py`

**Purpose:** Define rendering algorithm skeleton

```python
class BaseRenderer:
    def render_to_file(self, metadata, path):
        content = self.render(metadata)  # Subclass implements
        self._save(content, path)        # Common logic
```

---

## Key Abstractions

### 1. Engine Abstraction

**Why?** Support multiple execution backends

```python
# User doesn't care if Pandas or Spark:
df = engine.read(connection, "data.parquet", "parquet")

# PandasEngine: uses pd.read_parquet()
# SparkEngine: uses spark.read.parquet()
# Same interface!
```

### 2. Connection Abstraction

**Why?** Support multiple storage systems

```python
# User writes: path: "data.csv"
# Connection resolves to:
# - Local: ./data/data.csv
# - ADLS: abfss://container@account.dfs.core.windows.net/data.csv
# - SQL: Table reference

# Same code, different storage!
```

### 3. Transformation Abstraction

**Why?** User-defined operations work same as built-in

```python
# Built-in:
@transformation("pivot")
def pivot(...): ...

# User-defined:
@transformation("my_custom_op")
def my_custom_op(...): ...

# Both registered the same way!
# Both available in YAML!
```

---

## Extensibility Points

### Where You Can Extend Odibi

**1. Add New Operations**
```
Location: odibi/operations/
Pattern: Use @transformation decorator
Impact: Available in all pipelines
```

**2. Add New Connections**
```
Location: odibi/connections/
Pattern: Extend BaseConnection
Impact: New storage backends
```

**3. Add New Engines**
```
Location: odibi/engine/
Pattern: Implement Engine interface
Impact: New execution backends
```

**4. Add New Renderers**
```
Location: odibi/story/renderers.py
Pattern: Implement .render() method
Impact: New story output formats
```

**5. Add New Themes**
```
Location: odibi/story/themes.py
Pattern: Create StoryTheme instance
Impact: Custom branding
```

**6. Add New Validators**
```
Location: odibi/validation/
Pattern: Create validator class
Impact: Quality enforcement
```

---

## Configuration Model

### Pydantic Model Hierarchy

```
ProjectConfig (root)
    â”œâ”€â”€ connections: Dict[str, ConnectionConfig]
    â”œâ”€â”€ story: StoryConfig
    â””â”€â”€ pipelines: List[PipelineConfig]
            â””â”€â”€ nodes: List[NodeConfig]
                    â”œâ”€â”€ read: ReadConfig (optional)
                    â”œâ”€â”€ transform: TransformConfig (optional)
                    â””â”€â”€ write: WriteConfig (optional)
```

### Validation Flow

```
YAML file
    â†“
yaml.safe_load() â†’ dict
    â†“
ProjectConfig(**dict)  â† Pydantic validation happens here!
    â†“
If valid: ProjectConfig instance
If invalid: ValidationError with helpful message
```

**Example error:**
```
ValidationError: 1 validation error for NodeConfig
name
  Field required [type=missing]
```

---

## Thread Safety

### Current State: Single-Threaded

**Registry:** Thread-safe (read-only after startup)
**Context:** NOT thread-safe (single pipeline execution)
**Pipeline:** NOT thread-safe (sequential execution)

### Future: Parallel Execution

**Possible:**
```python
# Execute independent nodes in parallel
Layer 0: [A]
Layer 1: [B, C]  â† Can run in parallel!
Layer 2: [D]
```

**Required changes:**
- Thread-safe Context
- Parallel node execution
- Coordinated metadata collection

---

## Memory Management

### DataFrame Lifecycle

```
1. Read â†’ DataFrame created (stored in memory)
   â†“
2. Transform â†’ New DataFrame (old one can be GC'd if not reused)
   â†“
3. Write â†’ DataFrame written to disk
   â†“
4. Context stores DataFrame for downstream nodes
   â†“
5. Pipeline completes â†’ Context cleared â†’ memory freed
```

### Large Dataset Strategies

**Option 1: Don't store in context**
```python
# Future: Streaming mode
# Don't keep DataFrames in memory
# Process and write immediately
```

**Option 2: Use Spark**
```yaml
engine: spark
# Spark handles large data with partitioning
```

**Option 3: Chunk processing**
```python
# Write in chunks
chunksize: 10000  # Process 10K rows at a time
```

---

## Security Considerations

### Credential Handling

**âœ… Good:**
```yaml
connections:
  azure:
    auth_mode: key_vault  # Credentials in Key Vault
    key_vault_name: myvault
    secret_name: storage-key
```

**âŒ Bad:**
```yaml
connections:
  azure:
    account_key: "hardcoded_key_here"  # DON'T!
```

### SQL Injection Protection

**Odibi uses DuckDB** which executes on DataFrames (not databases):
- No SQL injection risk
- DataFrames are local
- Safe execution

For **Azure SQL**, use parameterized queries:

```python
# âœ… Safe
conn.read_sql(
    "SELECT * FROM users WHERE id = :user_id",
    params={"user_id": 123}
)

# âŒ Unsafe
conn.read_sql(f"SELECT * FROM users WHERE id = {user_id}")
```

---

## Next Steps

**You now understand the architecture!**

Learn how to build on it:
- **[Transformation Guide](05_TRANSFORMATION_GUIDE.md)** - Create custom operations
- **[Troubleshooting](06_TROUBLESHOOTING.md)** - Debug issues
- **Read the code!** Start with `operations/` directory

---

**Remember:** The tests are comprehensive examples. Use them! ğŸ§ª
