# Golden Path: From Zero to Production

Get a working Odibi pipeline in under 10 minutes.

---

## What is Odibi?

Odibi is a declarative data engineering framework. You describe *what* you want in YAML; Odibi handles *how*.

- **YAML-first**: One config file defines your entire pipeline
- **Auditable**: Every run generates a "Data Story" (HTML report)
- **Dual-engine**: Works with Pandas (local) or Spark (production)

---

## Step 1: Install

```bash
pip install odibi
```

---

## Step 2: Run One Command

```bash
odibi init-pipeline my_project --template local-medallion
cd my_project
```

This creates:
```
my_project/
├── odibi.yaml          # Your pipeline config
├── data/
│   ├── landing/        # Drop raw files here
│   ├── raw/            # Parquet copies
│   └── silver/         # Cleaned, merged tables
└── README.md
```

---

## Step 3: Add Sample Data

Create `data/landing/customers.csv`:

```csv
id,name,email,joined_at
1,Alice,alice@example.com,2023-01-01
2,Bob,bob@example.com,2023-02-15
3,Charlie,,2023-03-10
```

---

## Step 4: Your First Pipeline

This is the minimal config (already generated by the template):

```yaml
project: my_project

connections:
  landing:
    type: local
    base_path: ./data/landing
  raw:
    type: local
    base_path: ./data/raw
  silver:
    type: local
    base_path: ./data/silver

story:
  connection: raw
  path: stories

system:
  connection: raw
  path: _system

pipelines:
  - pipeline: customers
    nodes:
      - name: ingest_customers
        read:
          connection: landing
          format: csv
          path: customers.csv
        write:
          connection: raw
          format: parquet
          path: customers
```

**What happens:**
- Reads `customers.csv` from `landing/`
- Writes Parquet to `raw/customers/`
- Generates an HTML report in `raw/stories/`

---

## Step 5: Run It

```bash
odibi run odibi.yaml
```

Open the generated story:
```bash
# Windows
start .odibi/stories/*.html

# Mac/Linux
open .odibi/stories/*.html
```

---

## What Files Get Created?

| Folder | Contents | Purpose |
|--------|----------|---------|
| `data/raw/customers/` | `.parquet` files | Your data in columnar format |
| `data/raw/stories/` | `.html`, `.json` | Run audit reports |
| `data/raw/_system/` | Catalog metadata | Schema tracking, run history |

---

## Read Next (Pick 3 Max)

1. **[Playbook](playbook/README.md)** — Find solutions to specific problems
2. **[Patterns](patterns/README.md)** — SCD2, Merge, Aggregation explained
3. **[YAML Reference](reference/yaml_schema.md)** — All configuration options

---

## Quick Examples

### Example 1: Add Data Validation

Catch nulls and duplicates before they corrupt your warehouse:

```yaml
- name: ingest_customers
  read:
    connection: landing
    format: csv
    path: customers.csv
  contracts:
    - type: not_null
      columns: [id, name]
    - type: unique
      columns: [id]
  write:
    connection: raw
    format: parquet
    path: customers
```

[→ Schema Reference: contracts](reference/yaml_schema.md#contractconfig)

---

### Example 2: Incremental Loading (Only New Rows)

Don't re-read millions of rows every run:

```yaml
- name: ingest_orders
  read:
    connection: source_db
    format: jdbc
    table: orders
  incremental:
    mode: stateful
    column: updated_at
  write:
    connection: raw
    format: parquet
    path: orders
```

[→ Pattern: Incremental Stateful](patterns/incremental_stateful.md)

---

### Example 3: Build an SCD2 Dimension

Track customer changes over time:

```yaml
- name: dim_customer
  read:
    connection: raw
    format: parquet
    path: customers
  pattern:
    type: dimension
    params:
      natural_key: id
      surrogate_key: customer_sk
      scd_type: 2
      track_cols: [name, email]
  write:
    connection: silver
    format: delta
    table: dim_customer
```

[→ Pattern: SCD2](patterns/scd2.md)

---

## Common First Questions

**Q: Which engine should I use?**
- Pandas: Files < 1GB, local dev
- Spark: Files > 1GB, Delta Lake, production

**Q: Where do I put secrets?**
Use environment variables: `${AZURE_KEY}` in YAML.
[→ Guide: Secrets](guides/secrets.md)

**Q: How do I run on Databricks?**
Switch `engine: spark` and configure Azure connections.
[→ Tutorial: Spark Engine](tutorials/spark_engine.md)

---

*Questions? Open an issue on [GitHub](https://github.com/henryodibi11/Odibi/issues).*
