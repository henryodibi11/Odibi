{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Transformation Decorator and Composition\n",
    "\n",
    "## üß≠ Goal\n",
    "\n",
    "Understand how the `@transformation` decorator works internally and how to compose transformations into pipelines.\n",
    "\n",
    "This notebook will:\n",
    "- Explain how the `@transformation` decorator attaches metadata to functions\n",
    "- Show the decorator pattern: `@transformation(name, version, category, tags)`\n",
    "- Demonstrate function metadata attachment and inspection\n",
    "- Build a mini pipeline by composing multiple decorated functions\n",
    "- Show how metadata flows through the pipeline\n",
    "- Export pipeline execution metadata\n",
    "\n",
    "**Estimated time:** 30 seconds\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Core Concepts\n",
    "\n",
    "**The Decorator Pattern:**\n",
    "```python\n",
    "@transformation(name=\"clean_data\", version=\"1.0.0\", category=\"cleaning\")\n",
    "def clean_data(df):\n",
    "    return df.dropna()\n",
    "\n",
    "# The decorator:\n",
    "# 1. Attaches metadata to the function\n",
    "# 2. Registers it in the global registry\n",
    "# 3. Returns the original function (unchanged behavior)\n",
    "```\n",
    "\n",
    "**Function Composition:**\n",
    "```python\n",
    "# Chain transformations together\n",
    "result = transform_c(transform_b(transform_a(df)))\n",
    "\n",
    "# Each function carries metadata about what it does\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Environment Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Navigate to project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'walkthroughs' else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Create artifacts directory\n",
    "artifacts_dir = Path('walkthroughs/.artifacts/07_decorator')\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Import ODIBI transformation system\n",
    "from odibi.transformations import get_registry, transformation\n",
    "\n",
    "print(\"‚úÖ Environment ready\")\n",
    "print(f\"üìÅ Artifacts: {artifacts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Run: Inspect Decorator Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple transformation to inspect\n",
    "@transformation(\"demo_transform\", version=\"1.0.0\", category=\"demo\", tags=[\"example\", \"test\"])\n",
    "def demo_transform(df):\n",
    "    \"\"\"A simple demo transformation.\"\"\"\n",
    "    return df\n",
    "\n",
    "print(\"üîç Inspecting Decorator Metadata:\\n\")\n",
    "\n",
    "# Check function attributes\n",
    "print(f\"Function name: {demo_transform.__name__}\")\n",
    "print(f\"Function docstring: {demo_transform.__doc__}\")\n",
    "\n",
    "# Get metadata from registry\n",
    "registry = get_registry()\n",
    "metadata = registry.get_metadata(\"demo_transform\")\n",
    "\n",
    "print(\"\\nMetadata attached by decorator:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ The decorator enriches functions with metadata without changing behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Inspect: Test Metadata Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test DataFrame\n",
    "df_test = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"value\": [10, 20, 30]\n",
    "})\n",
    "\n",
    "print(\"üß™ Testing that decorated functions still work normally:\\n\")\n",
    "print(\"Input DataFrame:\")\n",
    "print(df_test)\n",
    "\n",
    "# Call the decorated function\n",
    "result = demo_transform(df_test)\n",
    "\n",
    "print(\"\\nOutput DataFrame (unchanged):\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n‚úÖ Function behavior preserved despite decoration\")\n",
    "print(\"‚úÖ Metadata still accessible via registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Create: Build a Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create sample data with messy names and missing values\n",
    "df_raw = pd.DataFrame({\n",
    "    \"Product Name\": [\"Widget A\", \"Gadget B\", \"Gizmo C\"],\n",
    "    \"Q1 Sales\": [100, 150, 200],\n",
    "    \"Q2 Sales\": [120, 160, 220],\n",
    "    \"Q3 Sales\": [110, 140, 210]\n",
    "})\n",
    "\n",
    "print(\"üìä Original Data:\")\n",
    "print(df_raw)\n",
    "print(f\"Shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define transformation functions with full metadata\n",
    "\n",
    "@transformation(\n",
    "    name=\"clean_column_names\",\n",
    "    version=\"1.0.0\",\n",
    "    category=\"cleaning\",\n",
    "    tags=[\"names\", \"standardization\"]\n",
    ")\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Convert column names to lowercase and replace spaces with underscores.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    return df\n",
    "\n",
    "@transformation(\n",
    "    name=\"add_total_column\",\n",
    "    version=\"1.0.0\",\n",
    "    category=\"aggregation\",\n",
    "    tags=[\"sum\", \"calculated\"]\n",
    ")\n",
    "def add_total_column(df):\n",
    "    \"\"\"Add a total column summing all numeric columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    df['total'] = df[numeric_cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "@transformation(\n",
    "    name=\"normalize_values\",\n",
    "    version=\"1.0.0\",\n",
    "    category=\"normalization\",\n",
    "    tags=[\"scaling\", \"percentage\"]\n",
    ")\n",
    "def normalize_values(df):\n",
    "    \"\"\"Convert numeric values to percentages of total.\"\"\"\n",
    "    df = df.copy()\n",
    "    if 'total' in df.columns:\n",
    "        numeric_cols = [col for col in df.select_dtypes(include=['number']).columns \n",
    "                       if col != 'total']\n",
    "        for col in numeric_cols:\n",
    "            df[f\"{col}_pct\"] = (df[col] / df['total'] * 100).round(2)\n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Created 3 custom transformations:\")\n",
    "print(\"  1. clean_column_names\")\n",
    "print(\"  2. add_total_column\")\n",
    "print(\"  3. normalize_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Compose: Chain Transformations Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track pipeline execution\n",
    "pipeline_steps = []\n",
    "transformation_metadata = {}\n",
    "\n",
    "print(\"üîó Composing Pipeline: clean ‚Üí add_total ‚Üí normalize\\n\")\n",
    "\n",
    "# Step 1: Clean column names\n",
    "df_step1 = clean_column_names(df_raw)\n",
    "pipeline_steps.append({\n",
    "    \"step\": 1,\n",
    "    \"transformation\": \"clean_column_names\",\n",
    "    \"columns_in\": list(df_raw.columns),\n",
    "    \"columns_out\": list(df_step1.columns),\n",
    "    \"shape\": str(df_step1.shape)\n",
    "})\n",
    "transformation_metadata[\"clean_column_names\"] = registry.get_metadata(\"clean_column_names\")\n",
    "\n",
    "print(\"Step 1 - After clean_column_names:\")\n",
    "print(df_step1)\n",
    "print()\n",
    "\n",
    "# Step 2: Add total column\n",
    "df_step2 = add_total_column(df_step1)\n",
    "pipeline_steps.append({\n",
    "    \"step\": 2,\n",
    "    \"transformation\": \"add_total_column\",\n",
    "    \"columns_in\": list(df_step1.columns),\n",
    "    \"columns_out\": list(df_step2.columns),\n",
    "    \"shape\": str(df_step2.shape)\n",
    "})\n",
    "transformation_metadata[\"add_total_column\"] = registry.get_metadata(\"add_total_column\")\n",
    "\n",
    "print(\"Step 2 - After add_total_column:\")\n",
    "print(df_step2)\n",
    "print()\n",
    "\n",
    "# Step 3: Normalize values\n",
    "df_final = normalize_values(df_step2)\n",
    "pipeline_steps.append({\n",
    "    \"step\": 3,\n",
    "    \"transformation\": \"normalize_values\",\n",
    "    \"columns_in\": list(df_step2.columns),\n",
    "    \"columns_out\": list(df_final.columns),\n",
    "    \"shape\": str(df_final.shape)\n",
    "})\n",
    "transformation_metadata[\"normalize_values\"] = registry.get_metadata(\"normalize_values\")\n",
    "\n",
    "print(\"Step 3 - After normalize_values (FINAL):\")\n",
    "print(df_final)\n",
    "print()\n",
    "\n",
    "print(f\"‚úÖ Pipeline complete: {df_raw.shape} ‚Üí {df_final.shape}\")\n",
    "print(f\"‚úÖ Columns: {len(df_raw.columns)} ‚Üí {len(df_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Pipeline Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final output\n",
    "output_file = artifacts_dir / 'pipeline_output.parquet'\n",
    "df_final.to_parquet(output_file, index=False)\n",
    "print(f\"‚úÖ Saved pipeline output: {output_file}\")\n",
    "\n",
    "# Save transformation metadata\n",
    "metadata_file = artifacts_dir / 'transformation_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(transformation_metadata, f, indent=2)\n",
    "print(f\"‚úÖ Saved transformation metadata: {metadata_file}\")\n",
    "\n",
    "# Save pipeline execution steps\n",
    "steps_file = artifacts_dir / 'pipeline_steps.json'\n",
    "pipeline_summary = {\n",
    "    \"executed_at\": datetime.now().isoformat(),\n",
    "    \"total_steps\": len(pipeline_steps),\n",
    "    \"steps\": pipeline_steps\n",
    "}\n",
    "with open(steps_file, 'w') as f:\n",
    "    json.dump(pipeline_summary, f, indent=2)\n",
    "print(f\"‚úÖ Saved pipeline steps: {steps_file}\")\n",
    "\n",
    "print(\"\\nüì¶ All artifacts exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Self-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Check artifacts exist\n",
    "    assert (artifacts_dir / 'pipeline_output.parquet').exists(), \"pipeline_output.parquet not found\"\n",
    "    assert (artifacts_dir / 'transformation_metadata.json').exists(), \"transformation_metadata.json not found\"\n",
    "    assert (artifacts_dir / 'pipeline_steps.json').exists(), \"pipeline_steps.json not found\"\n",
    "    \n",
    "    # Load and validate pipeline output\n",
    "    df_check = pd.read_parquet(artifacts_dir / 'pipeline_output.parquet')\n",
    "    expected_columns = ['product_name', 'q1_sales', 'q2_sales', 'q3_sales', 'total', \n",
    "                       'q1_sales_pct', 'q2_sales_pct', 'q3_sales_pct']\n",
    "    assert list(df_check.columns) == expected_columns, f\"Expected columns {expected_columns}, got {list(df_check.columns)}\"\n",
    "    assert len(df_check) == 3, f\"Expected 3 rows, got {len(df_check)}\"\n",
    "    \n",
    "    # Validate transformation metadata\n",
    "    with open(artifacts_dir / 'transformation_metadata.json') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    required_transforms = ['clean_column_names', 'add_total_column', 'normalize_values']\n",
    "    for name in required_transforms:\n",
    "        assert name in metadata, f\"Missing metadata for {name}\"\n",
    "        assert 'name' in metadata[name], f\"Missing 'name' in {name} metadata\"\n",
    "        assert 'version' in metadata[name], f\"Missing 'version' in {name} metadata\"\n",
    "        assert 'category' in metadata[name], f\"Missing 'category' in {name} metadata\"\n",
    "        assert 'tags' in metadata[name], f\"Missing 'tags' in {name} metadata\"\n",
    "    \n",
    "    # Validate pipeline steps\n",
    "    with open(artifacts_dir / 'pipeline_steps.json') as f:\n",
    "        steps = json.load(f)\n",
    "    \n",
    "    assert 'steps' in steps, \"Missing 'steps' in pipeline_steps.json\"\n",
    "    assert len(steps['steps']) == 3, f\"Expected 3 pipeline steps, got {len(steps['steps'])}\"\n",
    "    assert steps['total_steps'] == 3, \"total_steps should be 3\"\n",
    "    \n",
    "    # Check runtime\n",
    "    elapsed = time.time() - start_time\n",
    "    assert elapsed < 30, f\"Runtime {elapsed:.1f}s exceeds 30s budget\"\n",
    "    \n",
    "    print(\"üéâ Walkthrough verified successfully!\")\n",
    "    print(f\"‚è±Ô∏è  Runtime: {elapsed:.2f}s\")\n",
    "    print(f\"üìä Pipeline steps: {len(steps['steps'])}\")\n",
    "    print(f\"üìã Transformations: {len(metadata)}\")\n",
    "    print(\"‚úÖ All checks passed!\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Walkthrough failed: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Reflection\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Decorator Mechanics**: The `@transformation` decorator attaches metadata without changing function behavior\n",
    "2. **Function Composition**: Transformations can be chained together to build data pipelines\n",
    "3. **Metadata Flow**: Each transformation carries its own metadata (name, version, category, tags)\n",
    "4. **Pipeline Tracking**: You can capture and export information about pipeline execution\n",
    "\n",
    "### Where This Fits in ODIBI\n",
    "\n",
    "```\n",
    "Pipeline Construction:\n",
    "YAML Definition ‚Üí Parser ‚Üí Compose Functions ‚Üí Execute Pipeline ‚Üí Track Metadata\n",
    "                           ‚Üë\n",
    "                   This notebook showed composition!\n",
    "```\n",
    "\n",
    "The decorator pattern makes functions **self-documenting** and **traceable**. When ODIBI executes a pipeline, it can track exactly which transformations ran, their versions, and their metadata.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Decorators are wrappers**: They enhance functions with extra capabilities\n",
    "- **Composition is powerful**: Complex pipelines are just functions calling functions\n",
    "- **Metadata enables traceability**: You know exactly what happened to your data\n",
    "- **Functions stay pure**: Decorated functions work exactly like regular functions\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è≠ Next Steps\n",
    "\n",
    "**Continue to:** [08_advanced_transformations.ipynb](08_advanced_transformations.ipynb)\n",
    "\n",
    "Learn about advanced transformation patterns including error handling, validation, and parameterization.\n",
    "\n",
    "**Deep dive:**\n",
    "- Read `odibi/transformations/decorators.py` - The decorator implementation\n",
    "- Read `odibi/core/pipeline.py` - How pipelines compose transformations\n",
    "- Experiment with creating your own transformation chains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
