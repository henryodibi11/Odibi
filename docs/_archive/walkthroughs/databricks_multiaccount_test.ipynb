{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Account ADLS Test - Phase 2A Core Feature\n",
    "\n",
    "**Tests:**\n",
    "1. ‚úÖ Multi-account ADLS configuration with SparkEngine\n",
    "2. ‚úÖ Read from account A, write to account B\n",
    "3. ‚úÖ Schema introspection\n",
    "4. ‚úÖ Delta Lake time travel\n",
    "5. ‚úÖ Parallel Key Vault setup\n",
    "\n",
    "**Cleanup:** All test data removed at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"git+https://github.com/henryodibi11/Odibi.git#egg=odibi[spark,pandas,azure]\" --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from odibi.engine import SparkEngine\n",
    "from odibi.connections import AzureADLS, LocalConnection\n",
    "from odibi.utils import configure_connections_parallel\n",
    "import os\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# Test paths (simulating 2 different storage accounts with local DBFS)\n",
    "ACCOUNT_A_BASE = \"/dbfs/tmp/odibi_test_account_a\"  # Simulate storage account A\n",
    "ACCOUNT_B_BASE = \"/dbfs/tmp/odibi_test_account_b\"  # Simulate storage account B\n",
    "\n",
    "os.makedirs(ACCOUNT_A_BASE, exist_ok=True)\n",
    "os.makedirs(ACCOUNT_B_BASE, exist_ok=True)\n",
    "\n",
    "# Test data\n",
    "employees_v1 = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "        \"department\": [\"Engineering\", \"Sales\", \"Engineering\"],\n",
    "        \"salary\": [100000, 80000, 95000],\n",
    "    }\n",
    ")\n",
    "\n",
    "employees_v2 = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [1, 2, 3, 4],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "        \"department\": [\"Engineering\", \"Sales\", \"Engineering\", \"HR\"],\n",
    "        \"salary\": [105000, 82000, 98000, 70000],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì Setup complete\")\n",
    "print(f\"  Account A: {ACCOUNT_A_BASE}\")\n",
    "print(f\"  Account B: {ACCOUNT_B_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Multi-Account ADLS Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Multi-Account ADLS Configuration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create 2 ADLS connections (simulating 2 storage accounts)\n",
    "# In production, these would be real Azure storage accounts\n",
    "adls_connections = {\n",
    "    \"account_a\": AzureADLS(\n",
    "        account=\"datalakea\",\n",
    "        container=\"bronze\",\n",
    "        auth_mode=\"direct_key\",\n",
    "        account_key=\"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n",
    "        validate=True,\n",
    "    ),\n",
    "    \"account_b\": AzureADLS(\n",
    "        account=\"datalakeb\",\n",
    "        container=\"silver\",\n",
    "        auth_mode=\"direct_key\",\n",
    "        account_key=\"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n",
    "        validate=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"\\n‚ö° Configuring 2 storage accounts in parallel...\\n\")\n",
    "\n",
    "# Configure connections in parallel (Phase 2C feature)\n",
    "configured_adls, errors = configure_connections_parallel(\n",
    "    adls_connections, prefetch_secrets=True, max_workers=2, verbose=True\n",
    ")\n",
    "\n",
    "assert len(errors) == 0, f\"Configuration errors: {errors}\"\n",
    "print(\"\\n‚úì Both accounts configured successfully\")\n",
    "\n",
    "# Create SparkEngine with multi-account configuration\n",
    "multi_spark_engine = SparkEngine(connections=configured_adls, spark_session=spark)\n",
    "print(\"‚úì SparkEngine initialized with 2 storage accounts\")\n",
    "\n",
    "# Verify Spark session has both account keys\n",
    "spark_conf = spark.sparkContext.getConf()\n",
    "key_a = spark_conf.get(\"fs.azure.account.key.datalakea.dfs.core.windows.net\")\n",
    "key_b = spark_conf.get(\"fs.azure.account.key.datalakeb.dfs.core.windows.net\")\n",
    "\n",
    "assert key_a is not None, \"Account A not configured in Spark!\"\n",
    "assert key_b is not None, \"Account B not configured in Spark!\"\n",
    "print(\"‚úì Both storage account keys configured in Spark session\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TEST 1 PASSED - Multi-account ADLS works!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Cross-Account Data Transfer\n",
    "\n",
    "Read from Account A, write to Account B (medallion architecture pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 2: Cross-Account Data Transfer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use local connections to simulate the accounts\n",
    "conn_a = LocalConnection(base_path=ACCOUNT_A_BASE)\n",
    "conn_b = LocalConnection(base_path=ACCOUNT_B_BASE)\n",
    "\n",
    "# Write to Account A (bronze)\n",
    "spark_df = spark.createDataFrame(employees_v1)\n",
    "local_engine = SparkEngine(spark_session=spark)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  Writing data to Account A (bronze)...\")\n",
    "local_engine.write(\n",
    "    spark_df, connection=conn_a, path=\"raw_employees\", format=\"parquet\", mode=\"overwrite\"\n",
    ")\n",
    "print(f\"   ‚úì Written to: {ACCOUNT_A_BASE}/raw_employees\")\n",
    "\n",
    "# Read from Account A\n",
    "print(\"\\n2Ô∏è‚É£  Reading from Account A...\")\n",
    "df_from_a = local_engine.read(connection=conn_a, path=\"raw_employees\", format=\"parquet\")\n",
    "print(f\"   ‚úì Read {df_from_a.count()} rows from Account A\")\n",
    "\n",
    "# Transform data\n",
    "print(\"\\n3Ô∏è‚É£  Transforming data (SQL)...\")\n",
    "result = local_engine.execute_sql(\n",
    "    \"SELECT department, AVG(salary) as avg_salary FROM employees GROUP BY department\",\n",
    "    {\"employees\": df_from_a},\n",
    ")\n",
    "print(\"   ‚úì Transformation complete\")\n",
    "result.show()\n",
    "\n",
    "# Write to Account B (silver)\n",
    "print(\"\\n4Ô∏è‚É£  Writing transformed data to Account B (silver)...\")\n",
    "local_engine.write(\n",
    "    result, connection=conn_b, path=\"dept_salary_agg\", format=\"parquet\", mode=\"overwrite\"\n",
    ")\n",
    "print(f\"   ‚úì Written to: {ACCOUNT_B_BASE}/dept_salary_agg\")\n",
    "\n",
    "# Verify in Account B\n",
    "print(\"\\n5Ô∏è‚É£  Verifying data in Account B...\")\n",
    "df_from_b = local_engine.read(connection=conn_b, path=\"dept_salary_agg\", format=\"parquet\")\n",
    "print(f\"   ‚úì Read {df_from_b.count()} rows from Account B\")\n",
    "df_from_b.show()\n",
    "\n",
    "assert df_from_b.count() > 0, \"No data in Account B!\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TEST 2 PASSED - Cross-account transfer works!\")\n",
    "print(\"   Bronze (Account A) ‚Üí Transform ‚Üí Silver (Account B)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Schema Introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 3: Schema Introspection\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_df = spark.createDataFrame(employees_v1)\n",
    "\n",
    "# Test get_schema\n",
    "print(\"\\n1Ô∏è‚É£  Testing get_schema()...\")\n",
    "schema = local_engine.get_schema(test_df)\n",
    "print(f\"   Schema: {schema}\")\n",
    "assert len(schema) == 4, \"Expected 4 columns!\"\n",
    "assert schema[0][0] == \"id\", \"First column should be 'id'\"\n",
    "print(\"   ‚úì get_schema() works\")\n",
    "\n",
    "# Test get_shape\n",
    "print(\"\\n2Ô∏è‚É£  Testing get_shape()...\")\n",
    "shape = local_engine.get_shape(test_df)\n",
    "print(f\"   Shape: {shape}\")\n",
    "assert shape == (3, 4), \"Expected (3, 4)!\"\n",
    "print(\"   ‚úì get_shape() works\")\n",
    "\n",
    "# Test count_rows\n",
    "print(\"\\n3Ô∏è‚É£  Testing count_rows()...\")\n",
    "count = local_engine.count_rows(test_df)\n",
    "print(f\"   Row count: {count}\")\n",
    "assert count == 3, \"Expected 3 rows!\"\n",
    "print(\"   ‚úì count_rows() works\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TEST 3 PASSED - Schema introspection works!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Delta Lake Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 4: Delta Lake Time Travel\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Write version 1\n",
    "print(\"\\n1Ô∏è‚É£  Writing Delta table version 1 (3 employees)...\")\n",
    "df_v1 = spark.createDataFrame(employees_v1)\n",
    "local_engine.write(\n",
    "    df_v1, connection=conn_a, path=\"employees_delta\", format=\"delta\", mode=\"overwrite\"\n",
    ")\n",
    "print(\"   ‚úì Version 1 written\")\n",
    "\n",
    "# Write version 2\n",
    "print(\"\\n2Ô∏è‚É£  Writing Delta table version 2 (4 employees)...\")\n",
    "df_v2 = spark.createDataFrame(employees_v2)\n",
    "local_engine.write(\n",
    "    df_v2, connection=conn_a, path=\"employees_delta\", format=\"delta\", mode=\"overwrite\"\n",
    ")\n",
    "print(\"   ‚úì Version 2 written\")\n",
    "\n",
    "# Read latest version\n",
    "print(\"\\n3Ô∏è‚É£  Reading latest version...\")\n",
    "df_latest = local_engine.read(connection=conn_a, path=\"employees_delta\", format=\"delta\")\n",
    "print(f\"   ‚úì Latest version has {df_latest.count()} rows\")\n",
    "assert df_latest.count() == 4, \"Latest should have 4 rows!\"\n",
    "\n",
    "# Read version 0 (time travel)\n",
    "print(\"\\n4Ô∏è‚É£  Reading version 0 (time travel)...\")\n",
    "df_v0 = local_engine.read(\n",
    "    connection=conn_a, path=\"employees_delta\", format=\"delta\", options={\"versionAsOf\": \"0\"}\n",
    ")\n",
    "print(f\"   ‚úì Version 0 has {df_v0.count()} rows\")\n",
    "assert df_v0.count() == 3, \"Version 0 should have 3 rows!\"\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£  Comparing versions...\")\n",
    "print(f\"   Version 0: {df_v0.count()} rows (original)\")\n",
    "print(f\"   Latest:    {df_latest.count()} rows (updated)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TEST 4 PASSED - Delta time travel works!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: URI Generation for ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TEST 5: ADLS URI Generation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test URI generation for both accounts\n",
    "print(\"\\n1Ô∏è‚É£  Account A URI:\")\n",
    "uri_a = configured_adls[\"account_a\"].uri(\"test/data.parquet\")\n",
    "print(f\"   {uri_a}\")\n",
    "assert \"abfss://bronze@datalakea.dfs.core.windows.net\" in uri_a\n",
    "assert \"test/data.parquet\" in uri_a\n",
    "print(\"   ‚úì Account A URI correct\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  Account B URI:\")\n",
    "uri_b = configured_adls[\"account_b\"].uri(\"aggregated/results.parquet\")\n",
    "print(f\"   {uri_b}\")\n",
    "assert \"abfss://silver@datalakeb.dfs.core.windows.net\" in uri_b\n",
    "assert \"aggregated/results.parquet\" in uri_b\n",
    "print(\"   ‚úì Account B URI correct\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TEST 5 PASSED - ADLS URI generation works!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup - Remove All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Remove Account A\n",
    "if os.path.exists(ACCOUNT_A_BASE):\n",
    "    shutil.rmtree(ACCOUNT_A_BASE)\n",
    "    print(f\"‚úì Removed Account A: {ACCOUNT_A_BASE}\")\n",
    "\n",
    "# Remove Account B\n",
    "if os.path.exists(ACCOUNT_B_BASE):\n",
    "    shutil.rmtree(ACCOUNT_B_BASE)\n",
    "    print(f\"‚úì Removed Account B: {ACCOUNT_B_BASE}\")\n",
    "\n",
    "# Clean DBFS\n",
    "try:\n",
    "    dbutils.fs.rm(f\"dbfs:{ACCOUNT_A_BASE}\", recurse=True)\n",
    "    dbutils.fs.rm(f\"dbfs:{ACCOUNT_B_BASE}\", recurse=True)\n",
    "    print(\"‚úì Cleaned up DBFS\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ ALL TESTS PASSED!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ Multi-account ADLS configuration\")\n",
    "print(\"‚úÖ Cross-account data transfer (A ‚Üí B)\")\n",
    "print(\"‚úÖ Schema introspection (get_schema, get_shape, count_rows)\")\n",
    "print(\"‚úÖ Delta Lake time travel\")\n",
    "print(\"‚úÖ ADLS URI generation\")\n",
    "print(\"\\nüöÄ Phase 2A/2B/2C fully validated in Databricks!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
