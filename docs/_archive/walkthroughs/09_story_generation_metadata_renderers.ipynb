{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Story Generation & Metadata Renderers\n",
    "\n",
    "## ðŸ§­ Goal\n",
    "\n",
    "Understand how ODIBI tracks pipeline execution metadata and generates stories in multiple formats.\n",
    "\n",
    "This notebook will:\n",
    "- Show how ODIBI captures pipeline execution metadata automatically\n",
    "- Demonstrate PipelineStoryMetadata and NodeExecutionMetadata\n",
    "- Run a small pipeline and capture metadata\n",
    "- Use HTMLStoryRenderer, MarkdownStoryRenderer, JSONStoryRenderer\n",
    "- Generate stories in all 3 formats (HTML, Markdown, JSON)\n",
    "- Show what gets tracked: duration, rows in/out, schema changes, status\n",
    "\n",
    "**Estimated time:** 30 seconds\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Core Concepts\n",
    "\n",
    "**Pipeline Metadata Tracking:**\n",
    "```python\n",
    "# ODIBI automatically tracks execution details\n",
    "result = pipeline.run()\n",
    "metadata = result.metadata  # Contains all execution info\n",
    "\n",
    "# Render stories in multiple formats\n",
    "HTMLStoryRenderer().render_to_file(metadata, 'story.html')\n",
    "MarkdownStoryRenderer().render_to_file(metadata, 'story.md')\n",
    "JSONStoryRenderer().render_to_file(metadata, 'story.json')\n",
    "```\n",
    "\n",
    "**What Gets Tracked?**\n",
    "- â±ï¸ Duration (total and per-node)\n",
    "- ðŸ“Š Rows in/out and change percentages\n",
    "- ðŸ“‹ Schema changes (columns added/removed)\n",
    "- âœ… Status (success/failed/skipped)\n",
    "- ðŸ“ Operation names and parameters\n",
    "- ðŸ• Timestamps (start/end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Environment Setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Navigate to project root\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"walkthroughs\" else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Create artifacts directory\n",
    "artifacts_dir = Path(\"walkthroughs/.artifacts/09_story\")\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Import ODIBI story components\n",
    "from odibi.story import (\n",
    "    PipelineStoryMetadata,\n",
    "    NodeExecutionMetadata,\n",
    "    HTMLStoryRenderer,\n",
    "    MarkdownStoryRenderer,\n",
    "    JSONStoryRenderer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Environment ready\")\n",
    "print(f\"ðŸ“ Artifacts: {artifacts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ï¸ Run: Create and Execute a 3-Node Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ðŸ§ª Simulate a 3-node pipeline execution with metadata tracking\n",
    "print(\"ðŸš€ Running 3-node pipeline...\\n\")\n",
    "\n",
    "# Create pipeline metadata container\n",
    "pipeline_metadata = PipelineStoryMetadata(\n",
    "    pipeline_name=\"demo_data_transformation\",\n",
    "    pipeline_layer=\"bronze_to_silver\",\n",
    "    project=\"ODIBI\",\n",
    "    plant=\"Demo Plant\",\n",
    "    asset=\"Sensor Data\",\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Node 1: Load Data\n",
    "node1_start = time.time()\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"sensor_id\": [\"S001\", \"S002\", \"S003\", \"S004\", \"S005\"],\n",
    "        \"temperature\": [22.5, 23.1, 21.8, 24.2, 22.9],\n",
    "        \"humidity\": [45, 48, 43, 52, 46],\n",
    "        \"timestamp\": pd.date_range(\"2024-01-01\", periods=5, freq=\"h\"),\n",
    "    }\n",
    ")\n",
    "node1_duration = time.time() - node1_start\n",
    "\n",
    "node1_metadata = NodeExecutionMetadata(\n",
    "    node_name=\"load_sensor_data\",\n",
    "    operation=\"read_csv\",\n",
    "    status=\"success\",\n",
    "    duration=node1_duration,\n",
    "    rows_in=0,\n",
    "    rows_out=len(df1),\n",
    "    schema_in=[],\n",
    "    schema_out=list(df1.columns),\n",
    "    started_at=datetime.now().isoformat(),\n",
    "    completed_at=datetime.now().isoformat(),\n",
    ")\n",
    "node1_metadata.calculate_row_change()\n",
    "node1_metadata.calculate_schema_changes()\n",
    "pipeline_metadata.add_node(node1_metadata)\n",
    "\n",
    "print(\"âœ… Node 1: load_sensor_data\")\n",
    "print(f\"   Rows out: {len(df1)}\")\n",
    "print(f\"   Schema: {list(df1.columns)}\")\n",
    "print(f\"   Duration: {node1_duration:.4f}s\\n\")\n",
    "\n",
    "# Node 2: Add Calculated Column\n",
    "node2_start = time.time()\n",
    "df2 = df1.copy()\n",
    "df2[\"heat_index\"] = df2[\"temperature\"] * 1.1 + df2[\"humidity\"] * 0.05\n",
    "node2_duration = time.time() - node2_start\n",
    "\n",
    "node2_metadata = NodeExecutionMetadata(\n",
    "    node_name=\"calculate_heat_index\",\n",
    "    operation=\"add_column\",\n",
    "    status=\"success\",\n",
    "    duration=node2_duration,\n",
    "    rows_in=len(df1),\n",
    "    rows_out=len(df2),\n",
    "    schema_in=list(df1.columns),\n",
    "    schema_out=list(df2.columns),\n",
    "    started_at=datetime.now().isoformat(),\n",
    "    completed_at=datetime.now().isoformat(),\n",
    ")\n",
    "node2_metadata.calculate_row_change()\n",
    "node2_metadata.calculate_schema_changes()\n",
    "pipeline_metadata.add_node(node2_metadata)\n",
    "\n",
    "print(\"âœ… Node 2: calculate_heat_index\")\n",
    "print(f\"   Rows in: {len(df1)} â†’ Rows out: {len(df2)}\")\n",
    "print(f\"   Columns added: {node2_metadata.columns_added}\")\n",
    "print(f\"   Duration: {node2_duration:.4f}s\\n\")\n",
    "\n",
    "# Node 3: Filter High Temperature\n",
    "node3_start = time.time()\n",
    "df3 = df2[df2[\"temperature\"] > 22.0].copy()\n",
    "node3_duration = time.time() - node3_start\n",
    "\n",
    "node3_metadata = NodeExecutionMetadata(\n",
    "    node_name=\"filter_high_temp\",\n",
    "    operation=\"filter\",\n",
    "    status=\"success\",\n",
    "    duration=node3_duration,\n",
    "    rows_in=len(df2),\n",
    "    rows_out=len(df3),\n",
    "    schema_in=list(df2.columns),\n",
    "    schema_out=list(df3.columns),\n",
    "    started_at=datetime.now().isoformat(),\n",
    "    completed_at=datetime.now().isoformat(),\n",
    ")\n",
    "node3_metadata.calculate_row_change()\n",
    "node3_metadata.calculate_schema_changes()\n",
    "pipeline_metadata.add_node(node3_metadata)\n",
    "\n",
    "print(\"âœ… Node 3: filter_high_temp\")\n",
    "print(f\"   Rows in: {len(df2)} â†’ Rows out: {len(df3)}\")\n",
    "print(f\"   Row change: {node3_metadata.rows_change} ({node3_metadata.rows_change_pct:+.1f}%)\")\n",
    "print(f\"   Duration: {node3_duration:.4f}s\\n\")\n",
    "\n",
    "# Complete pipeline metadata\n",
    "total_duration = time.time() - start_time\n",
    "pipeline_metadata.duration = total_duration\n",
    "pipeline_metadata.completed_at = datetime.now().isoformat()\n",
    "\n",
    "print(\"ðŸŽ‰ Pipeline completed!\")\n",
    "print(f\"   Total nodes: {pipeline_metadata.total_nodes}\")\n",
    "print(f\"   Completed: {pipeline_metadata.completed_nodes}\")\n",
    "print(f\"   Success rate: {pipeline_metadata.get_success_rate():.1f}%\")\n",
    "print(f\"   Total duration: {total_duration:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Inspect: Examine Captured Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect pipeline-level metadata\n",
    "print(\"ðŸ“Š Pipeline Metadata:\\n\")\n",
    "print(f\"Name: {pipeline_metadata.pipeline_name}\")\n",
    "print(f\"Layer: {pipeline_metadata.pipeline_layer}\")\n",
    "print(f\"Duration: {pipeline_metadata.duration:.4f}s\")\n",
    "print(f\"Total nodes: {pipeline_metadata.total_nodes}\")\n",
    "print(f\"Completed: {pipeline_metadata.completed_nodes}\")\n",
    "print(f\"Failed: {pipeline_metadata.failed_nodes}\")\n",
    "print(f\"Success rate: {pipeline_metadata.get_success_rate():.1f}%\")\n",
    "print(f\"Total rows processed: {pipeline_metadata.get_total_rows_processed()}\\n\")\n",
    "\n",
    "# Inspect node-level metadata\n",
    "print(\"ðŸ“‹ Node Execution Details:\\n\")\n",
    "for node in pipeline_metadata.nodes:\n",
    "    print(f\"  ðŸ”¹ {node.node_name}\")\n",
    "    print(f\"     Operation: {node.operation}\")\n",
    "    print(f\"     Status: {node.status}\")\n",
    "    print(f\"     Duration: {node.duration:.4f}s\")\n",
    "    print(f\"     Rows: {node.rows_in} â†’ {node.rows_out}\")\n",
    "    if node.rows_change is not None:\n",
    "        print(f\"     Change: {node.rows_change:+d} ({node.rows_change_pct:+.1f}%)\")\n",
    "    if node.columns_added:\n",
    "        print(f\"     Columns added: {node.columns_added}\")\n",
    "    if node.columns_removed:\n",
    "        print(f\"     Columns removed: {node.columns_removed}\")\n",
    "    print()\n",
    "\n",
    "# Save metadata as JSON for inspection\n",
    "with open(artifacts_dir / \"pipeline_metadata.json\", \"w\") as f:\n",
    "    json.dump(pipeline_metadata.to_dict(), f, indent=2)\n",
    "\n",
    "print(f\"âœ… Metadata exported to: {artifacts_dir / 'pipeline_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Create: Generate Stories in All 3 Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize renderers\n",
    "html_renderer = HTMLStoryRenderer()\n",
    "markdown_renderer = MarkdownStoryRenderer()\n",
    "json_renderer = JSONStoryRenderer()\n",
    "\n",
    "print(\"ðŸ“ Generating stories in multiple formats...\\n\")\n",
    "\n",
    "# Generate HTML Story\n",
    "html_path = html_renderer.render_to_file(pipeline_metadata, artifacts_dir / \"story.html\")\n",
    "print(f\"âœ… HTML story: {html_path}\")\n",
    "print(\"   Open in browser to view interactive report\")\n",
    "\n",
    "# Generate Markdown Story\n",
    "markdown_path = markdown_renderer.render_to_file(pipeline_metadata, artifacts_dir / \"story.md\")\n",
    "print(f\"âœ… Markdown story: {markdown_path}\")\n",
    "print(\"   Human-readable documentation\")\n",
    "\n",
    "# Generate JSON Story\n",
    "json_path = json_renderer.render_to_file(pipeline_metadata, artifacts_dir / \"story.json\")\n",
    "print(f\"âœ… JSON story: {json_path}\")\n",
    "print(\"   Machine-readable for API integration\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All 3 story formats generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– View: Preview Markdown Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the markdown story\n",
    "with open(artifacts_dir / \"story.md\", \"r\") as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "print(\"ðŸ“– Markdown Story Preview:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(markdown_content[:1000])  # Show first 1000 characters\n",
    "print(\"...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâ„¹ï¸  Full content: {len(markdown_content)} characters\")\n",
    "print(f\"ðŸ“„ Read complete story at: {artifacts_dir / 'story.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Analyze: What Was Tracked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the JSON story\n",
    "with open(artifacts_dir / \"story.json\", \"r\") as f:\n",
    "    story_data = json.load(f)\n",
    "\n",
    "print(\"ðŸ”¬ Tracking Analysis:\\n\")\n",
    "\n",
    "# Pipeline-level tracking\n",
    "print(\"ðŸ“Š Pipeline-Level Tracking:\")\n",
    "pipeline_keys = [\n",
    "    \"pipeline_name\",\n",
    "    \"duration\",\n",
    "    \"total_nodes\",\n",
    "    \"completed_nodes\",\n",
    "    \"success_rate\",\n",
    "    \"total_rows_processed\",\n",
    "]\n",
    "for key in pipeline_keys:\n",
    "    if key in story_data:\n",
    "        print(f\"   âœ“ {key}: {story_data[key]}\")\n",
    "\n",
    "# Node-level tracking\n",
    "print(\"\\nðŸ“‹ Node-Level Tracking (per node):\")\n",
    "if story_data[\"nodes\"]:\n",
    "    sample_node = story_data[\"nodes\"][0]\n",
    "    node_keys = [\n",
    "        \"node_name\",\n",
    "        \"operation\",\n",
    "        \"status\",\n",
    "        \"duration\",\n",
    "        \"rows_in\",\n",
    "        \"rows_out\",\n",
    "        \"rows_change\",\n",
    "        \"rows_change_pct\",\n",
    "        \"schema_in\",\n",
    "        \"schema_out\",\n",
    "        \"columns_added\",\n",
    "        \"columns_removed\",\n",
    "    ]\n",
    "    for key in node_keys:\n",
    "        if key in sample_node:\n",
    "            print(f\"   âœ“ {key}\")\n",
    "\n",
    "# Timestamp tracking\n",
    "print(\"\\nðŸ• Timestamp Tracking:\")\n",
    "timestamp_keys = [\"started_at\", \"completed_at\"]\n",
    "for key in timestamp_keys:\n",
    "    if key in story_data:\n",
    "        print(f\"   âœ“ {key}: {story_data[key][:19]}\")\n",
    "\n",
    "# Context tracking\n",
    "print(\"\\nðŸ¢ Context Tracking:\")\n",
    "context_keys = [\"project\", \"plant\", \"asset\", \"pipeline_layer\"]\n",
    "for key in context_keys:\n",
    "    if key in story_data and story_data[key]:\n",
    "        print(f\"   âœ“ {key}: {story_data[key]}\")\n",
    "\n",
    "print(\"\\nâœ… Complete metadata tracking enabled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Self-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "check_start = time.time()\n",
    "\n",
    "try:\n",
    "    # Check all 4 artifact files exist\n",
    "    required_files = [\"story.html\", \"story.md\", \"story.json\", \"pipeline_metadata.json\"]\n",
    "\n",
    "    for filename in required_files:\n",
    "        file_path = artifacts_dir / filename\n",
    "        assert file_path.exists(), f\"{filename} not found\"\n",
    "\n",
    "    print(\"âœ… All 4 artifact files exist\")\n",
    "\n",
    "    # Validate story.json structure\n",
    "    with open(artifacts_dir / \"story.json\") as f:\n",
    "        story = json.load(f)\n",
    "\n",
    "    assert \"pipeline_name\" in story, \"Missing pipeline_name\"\n",
    "    assert \"duration\" in story, \"Missing duration\"\n",
    "    assert \"nodes\" in story, \"Missing nodes\"\n",
    "    assert story[\"pipeline_name\"] == \"demo_data_transformation\", \"Wrong pipeline name\"\n",
    "\n",
    "    print(\"âœ… story.json has valid structure\")\n",
    "    print(f\"   - pipeline_name: {story['pipeline_name']}\")\n",
    "    print(f\"   - duration: {story['duration']:.4f}s\")\n",
    "    print(f\"   - nodes: {len(story['nodes'])}\")\n",
    "\n",
    "    # Validate node count\n",
    "    assert len(story[\"nodes\"]) >= 3, f\"Expected at least 3 nodes, got {len(story['nodes'])}\"\n",
    "    print(\"âœ… Pipeline has at least 3 nodes\")\n",
    "\n",
    "    # Validate HTML contains expected content\n",
    "    with open(artifacts_dir / \"story.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    html_keywords = [\"Pipeline\", \"story\", \"metadata\", \"demo_data_transformation\"]\n",
    "    found_keyword = any(keyword.lower() in html_content.lower() for keyword in html_keywords)\n",
    "    assert found_keyword, \"HTML missing expected keywords\"\n",
    "    print(\"âœ… HTML contains pipeline story content\")\n",
    "\n",
    "    # Validate pipeline metadata\n",
    "    with open(artifacts_dir / \"pipeline_metadata.json\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    assert metadata[\"total_nodes\"] >= 3, \"Metadata missing nodes\"\n",
    "    assert metadata[\"completed_nodes\"] >= 3, \"Not all nodes completed\"\n",
    "    print(\"âœ… Pipeline metadata complete\")\n",
    "    print(f\"   - total_nodes: {metadata['total_nodes']}\")\n",
    "    print(f\"   - completed_nodes: {metadata['completed_nodes']}\")\n",
    "\n",
    "    # Check node metadata fields\n",
    "    node = story[\"nodes\"][0]\n",
    "    required_fields = [\"node_name\", \"operation\", \"status\", \"duration\", \"rows_in\", \"rows_out\"]\n",
    "    for field in required_fields:\n",
    "        assert field in node, f\"Node missing field: {field}\"\n",
    "    print(\"âœ… Node metadata has all required fields\")\n",
    "\n",
    "    # Check runtime\n",
    "    elapsed = time.time() - check_start\n",
    "    # Note: Total runtime includes the pipeline execution time above\n",
    "    assert pipeline_metadata.duration < 30, (\n",
    "        f\"Pipeline runtime {pipeline_metadata.duration:.1f}s exceeds 30s budget\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Walkthrough verified successfully!\")\n",
    "    print(f\"â±ï¸  Check time: {elapsed:.2f}s\")\n",
    "    print(f\"â±ï¸  Pipeline time: {pipeline_metadata.duration:.2f}s\")\n",
    "    print(f\"ðŸ“Š Nodes executed: {metadata['total_nodes']}\")\n",
    "    print(f\"ðŸ“ Artifacts created: {len(required_files)}\")\n",
    "    print(\"âœ… All checks passed!\")\n",
    "\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ Walkthrough failed: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unexpected error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Reflection\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Automatic Metadata Tracking**: ODIBI captures comprehensive execution metadata without manual instrumentation\n",
    "2. **Multi-Format Stories**: Same metadata can be rendered as HTML (interactive), Markdown (readable), or JSON (machine-readable)\n",
    "3. **Rich Metrics**: Tracks performance, data quality, schema evolution, and execution status\n",
    "4. **Hierarchical Structure**: Pipeline-level and node-level metadata provide both overview and details\n",
    "\n",
    "### Where This Fits in ODIBI\n",
    "\n",
    "```\n",
    "Pipeline Execution Flow:\n",
    "YAML â†’ Parse â†’ Execute Nodes â†’ Capture Metadata â†’ Generate Stories\n",
    "                        â†“              â†“              â†“\n",
    "                   NodeResult  â†’  Metadata  â†’  HTML/MD/JSON\n",
    "                                      â†‘\n",
    "                         This notebook explained this!\n",
    "```\n",
    "\n",
    "Story generation is **essential for observability and auditability**. It creates automatic documentation and provides insights into:\n",
    "- What happened during execution\n",
    "- How long each step took\n",
    "- What data transformations occurred\n",
    "- Where failures occurred and why\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Zero-config tracking**: Metadata collection happens automatically during pipeline execution\n",
    "- **Multiple audiences**: HTML for humans, JSON for machines, Markdown for documentation\n",
    "- **Complete audit trail**: Every pipeline run creates a permanent, shareable record\n",
    "- **Schema evolution**: Track how data structure changes through the pipeline\n",
    "- **Performance insights**: Duration tracking helps identify bottlenecks\n",
    "\n",
    "### Story Renderer Use Cases\n",
    "\n",
    "- **HTMLStoryRenderer**: Interactive dashboards, executive reports, team reviews\n",
    "- **MarkdownStoryRenderer**: Documentation, README files, GitHub wikis\n",
    "- **JSONStoryRenderer**: API responses, monitoring systems, data warehouses\n",
    "\n",
    "---\n",
    "\n",
    "## â­ Next Steps\n",
    "\n",
    "**Continue exploring:**\n",
    "- Learn about custom story themes for branded reports\n",
    "- Explore StoryGenerator for automated story creation during pipeline runs\n",
    "- Integrate story metadata with monitoring and alerting systems\n",
    "\n",
    "**Deep dive:**\n",
    "- Read `odibi/story/metadata.py` - Metadata data classes and tracking logic\n",
    "- Read `odibi/story/renderers.py` - Story rendering implementations\n",
    "- Read `odibi/story/generator.py` - Automatic story generation\n",
    "- Explore `odibi/story/templates/` - HTML template customization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
