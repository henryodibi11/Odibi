{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2B Walkthrough: Delta Lake Support\n",
    "\n",
    "**Version:** v1.2.0-alpha.2-phase2b  \n",
    "**Date:** November 9, 2025  \n",
    "**Status:** âœ… Complete\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This walkthrough demonstrates:\n",
    "1. âœ… Delta Lake read/write with PandasEngine\n",
    "2. âœ… Delta Lake read/write with SparkEngine\n",
    "3. âœ… Time travel (read specific versions)\n",
    "4. âœ… VACUUM operations (clean old files)\n",
    "5. âœ… History tracking\n",
    "6. âœ… Restore to previous versions\n",
    "7. âœ… Partitioning with warnings\n",
    "8. âœ… Delta + ADLS integration\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install ODIBI with Delta support\n",
    "pip install -e \".[pandas,azure]\"\n",
    "\n",
    "# For Spark engine (optional)\n",
    "pip install -e \".[spark,azure]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add odibi to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from odibi.engine.pandas_engine import PandasEngine\n",
    "from odibi.connections.local import LocalConnection\n",
    "\n",
    "print(\"âœ… ODIBI imported successfully\")\n",
    "print(\"\\nPhase 2B: Delta Lake Support\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_sales = pd.DataFrame({\n",
    "    \"order_id\": [1, 2, 3, 4, 5],\n",
    "    \"customer\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"product\": [\"Laptop\", \"Phone\", \"Tablet\", \"Laptop\", \"Phone\"],\n",
    "    \"amount\": [1200, 800, 400, 1200, 750],\n",
    "    \"year\": [2024, 2024, 2024, 2024, 2024],\n",
    "    \"month\": [1, 1, 2, 2, 3]\n",
    "})\n",
    "\n",
    "print(\"Sample Sales Data:\")\n",
    "print(sample_sales)\n",
    "print(f\"\\nTotal: ${sample_sales['amount'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup local connection and engine\n",
    "engine = PandasEngine()\n",
    "local_conn = LocalConnection(base_path=\"./delta_test_data\")\n",
    "\n",
    "print(\"âœ… PandasEngine initialized\")\n",
    "print(f\"âœ… Local connection: {local_conn.base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Delta Lake Basics - Write and Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Delta table (version 0)\n",
    "delta_path = \"sales.delta\"\n",
    "\n",
    "engine.write(\n",
    "    sample_sales,\n",
    "    connection=local_conn,\n",
    "    format=\"delta\",\n",
    "    path=delta_path,\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Delta table created!\")\n",
    "print(f\"\\nLocation: {local_conn.get_path(delta_path)}\")\n",
    "print(\"\\nDelta table structure:\")\n",
    "print(\"  sales.delta/\")\n",
    "print(\"  â”œâ”€â”€ _delta_log/        (transaction log)\")\n",
    "print(\"  â”‚   â””â”€â”€ 00000000000000000000.json\")\n",
    "print(\"  â””â”€â”€ *.parquet          (data files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta table\n",
    "df_read = engine.read(\n",
    "    connection=local_conn,\n",
    "    format=\"delta\",\n",
    "    path=delta_path\n",
    ")\n",
    "\n",
    "print(\"âœ… Delta table read successfully!\")\n",
    "print(f\"\\nRows: {len(df_read)}\")\n",
    "print(df_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Append Mode - Creating Multiple Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append more sales (version 1)\n",
    "new_sales = pd.DataFrame({\n",
    "    \"order_id\": [6, 7],\n",
    "    \"customer\": [\"Frank\", \"Grace\"],\n",
    "    \"product\": [\"Tablet\", \"Laptop\"],\n",
    "    \"amount\": [400, 1300],\n",
    "    \"year\": [2024, 2024],\n",
    "    \"month\": [3, 4]\n",
    "})\n",
    "\n",
    "engine.write(\n",
    "    new_sales,\n",
    "    connection=local_conn,\n",
    "    format=\"delta\",\n",
    "    path=delta_path,\n",
    "    mode=\"append\"  # â† Append mode!\n",
    ")\n",
    "\n",
    "print(\"âœ… Appended 2 more orders (version 1 created)\")\n",
    "print(new_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read latest version\n",
    "df_latest = engine.read(local_conn, format=\"delta\", path=delta_path)\n",
    "\n",
    "print(\"âœ… Latest version:\")\n",
    "print(f\"\\nRows: {len(df_latest)} (was 5, now 7)\")\n",
    "print(f\"Total: ${df_latest['amount'].sum():,}\")\n",
    "print(\"\\nLast 3 orders:\")\n",
    "print(df_latest.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Time Travel - Read Specific Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 0 (original data)\n",
    "df_v0 = engine.read(\n",
    "    connection=local_conn,\n",
    "    format=\"delta\",\n",
    "    path=delta_path,\n",
    "    options={\"versionAsOf\": 0}  # â† Time travel!\n",
    ")\n",
    "\n",
    "print(\"âœ… Time travel to version 0:\")\n",
    "print(f\"\\nRows: {len(df_v0)} (original 5 orders)\")\n",
    "print(f\"Total: ${df_v0['amount'].sum():,}\")\n",
    "print(\"\\nData:\")\n",
    "print(df_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare versions\n",
    "print(\"Version Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Version 0: {len(df_v0)} rows, ${df_v0['amount'].sum():,}\")\n",
    "print(f\"Version 1: {len(df_latest)} rows, ${df_latest['amount'].sum():,}\")\n",
    "print(f\"\\nDifference: +{len(df_latest) - len(df_v0)} rows, +${df_latest['amount'].sum() - df_v0['amount'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: History Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table history\n",
    "history = engine.get_delta_history(\n",
    "    connection=local_conn,\n",
    "    path=delta_path\n",
    ")\n",
    "\n",
    "print(\"âœ… Delta Table History:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for entry in history:\n",
    "    print(f\"\\nVersion {entry['version']}:\")\n",
    "    print(f\"  Operation: {entry['operation']}\")\n",
    "    print(f\"  Timestamp: {entry['timestamp']}\")\n",
    "    if 'operationMetrics' in entry:\n",
    "        metrics = entry['operationMetrics']\n",
    "        if 'numOutputRows' in metrics:\n",
    "            print(f\"  Rows: {metrics['numOutputRows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating More Versions for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite with \"bad\" data (version 2)\n",
    "bad_data = pd.DataFrame({\n",
    "    \"order_id\": [999],\n",
    "    \"customer\": [\"CORRUPTED\"],\n",
    "    \"product\": [\"ERROR\"],\n",
    "    \"amount\": [0],\n",
    "    \"year\": [9999],\n",
    "    \"month\": [99]\n",
    "})\n",
    "\n",
    "engine.write(\n",
    "    bad_data,\n",
    "    connection=local_conn,\n",
    "    format=\"delta\",\n",
    "    path=delta_path,\n",
    "    mode=\"overwrite\"  # Oops! Overwrote everything\n",
    ")\n",
    "\n",
    "print(\"âŒ Oops! Accidentally overwrote the table with bad data\")\n",
    "print(bad_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current state\n",
    "df_corrupted = engine.read(local_conn, format=\"delta\", path=delta_path)\n",
    "\n",
    "print(\"Current table state (version 2):\")\n",
    "print(df_corrupted)\n",
    "print(\"\\nâŒ Data is corrupted! Only 1 row with error values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Restore to Previous Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore to version 1 (before corruption)\n",
    "engine.restore_delta(\n",
    "    connection=local_conn,\n",
    "    path=delta_path,\n",
    "    version=1  # â† Restore to version 1!\n",
    ")\n",
    "\n",
    "print(\"âœ… Restored to version 1!\")\n",
    "print(\"\\nRestoring table to before corruption...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify restoration\n",
    "df_restored = engine.read(local_conn, format=\"delta\", path=delta_path)\n",
    "\n",
    "print(\"âœ… Table restored successfully!\")\n",
    "print(f\"\\nRows: {len(df_restored)} (back to 7 orders)\")\n",
    "print(f\"Total: ${df_restored['amount'].sum():,}\")\n",
    "print(\"\\nRestored data:\")\n",
    "print(df_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check updated history\n",
    "history_after_restore = engine.get_delta_history(local_conn, delta_path, limit=5)\n",
    "\n",
    "print(\"Updated History (last 5 versions):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for entry in history_after_restore:\n",
    "    version = entry['version']\n",
    "    operation = entry['operation']\n",
    "    print(f\"Version {version}: {operation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: VACUUM - Clean Old Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACUUM dry run (preview what would be deleted)\n",
    "result_dry_run = engine.vacuum_delta(\n",
    "    connection=local_conn,\n",
    "    path=delta_path,\n",
    "    retention_hours=0,  # For testing only\n",
    "    dry_run=True,\n",
    "    enforce_retention_duration=False  # â† Add this for testing!\n",
    ")\n",
    "\n",
    "print(\"VACUUM Dry Run Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Files that would be deleted: {result_dry_run['files_deleted']}\")\n",
    "print(\"\\nâš ï¸  This is a preview - no files were actually deleted\")\n",
    "print(\"âš ï¸  Note: enforce_retention_duration=False is for testing only!\")\n",
    "print(\"     In production, use default retention_hours=168\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACUUM for real (clean old files)\n",
    "result_vacuum = engine.vacuum_delta(\n",
    "    connection=local_conn,\n",
    "    path=delta_path,\n",
    "    retention_hours=0,  # For testing only\n",
    "    dry_run=False,\n",
    "    enforce_retention_duration=False  # â† Add this for testing!\n",
    ")\n",
    "\n",
    "print(\"âœ… VACUUM completed!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Files deleted: {result_vacuum['files_deleted']}\")\n",
    "print(\"\\nðŸ’¡ In production, use:\")\n",
    "print(\"   retention_hours=168 (7 days)\")\n",
    "print(\"   enforce_retention_duration=True (default)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Partitioning (with Warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for partitioning\n",
    "import warnings\n",
    "\n",
    "large_sales = pd.DataFrame({\n",
    "    \"order_id\": range(1, 101),\n",
    "    \"customer\": [f\"Customer_{i}\" for i in range(1, 101)],\n",
    "    \"product\": [\"Laptop\", \"Phone\", \"Tablet\"] * 33 + [\"Laptop\"],\n",
    "    \"amount\": [1000 + i*10 for i in range(100)],\n",
    "    \"year\": [2024] * 100,\n",
    "    \"month\": [i % 12 + 1 for i in range(100)]\n",
    "})\n",
    "\n",
    "print(f\"Created large dataset: {len(large_sales)} rows\")\n",
    "print(\"\\nSample:\")\n",
    "print(large_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with partitioning (will emit warning)\n",
    "warnings.simplefilter('always')  # Show all warnings\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    \n",
    "    engine.write(\n",
    "        large_sales,\n",
    "        connection=local_conn,\n",
    "        format=\"delta\",\n",
    "        path=\"partitioned_sales.delta\",\n",
    "        mode=\"overwrite\",\n",
    "        options={\"partition_by\": [\"month\"]}  # â† Partition by month\n",
    "    )\n",
    "    \n",
    "    # Show warning\n",
    "    if w:\n",
    "        print(\"âš ï¸  Warning emitted:\")\n",
    "        print(f\"   {w[0].message}\")\n",
    "\n",
    "print(\"\\nâœ… Partitioned Delta table created!\")\n",
    "print(\"\\nPartitioned structure:\")\n",
    "print(\"  partitioned_sales.delta/\")\n",
    "print(\"  â”œâ”€â”€ month=1/\")\n",
    "print(\"  â”‚   â””â”€â”€ *.parquet\")\n",
    "print(\"  â”œâ”€â”€ month=2/\")\n",
    "print(\"  â”‚   â””â”€â”€ *.parquet\")\n",
    "print(\"  â””â”€â”€ ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Delta + ADLS Integration\n",
    "\n",
    "**Note:** This section requires Azure credentials. Update with your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADLS connection setup (update with your values)\n",
    "from odibi.connections.azure_adls import AzureADLS\n",
    "\n",
    "# Option 1: Direct key (for local testing)\n",
    "adls_conn = AzureADLS(\n",
    "    account=\"YOUR_STORAGE_ACCOUNT\",\n",
    "    container=\"YOUR_CONTAINER\",\n",
    "    path_prefix=\"delta_test\",\n",
    "    auth_mode=\"key_vault\",\n",
    "    key_vault_name=\"YOUR_KEY_VAULT_NAME\",\n",
    "    secret_name=\"YOUR_ACCOUNT_KEY\",\n",
    "    validate=False  # Set True when ready to test\n",
    ")\n",
    "\n",
    "print(\"âš ï¸  ADLS connection configured (validation disabled)\")\n",
    "print(\"\\nTo test with real ADLS:\")\n",
    "print(\"1. Update account, container, and account_key\")\n",
    "print(\"2. Set validate=True\")\n",
    "print(\"3. Run the following cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Delta to ADLS (uncomment when ready)\n",
    "\n",
    "engine.write(\n",
    "    sample_sales,\n",
    "    connection=adls_conn,\n",
    "    format=\"delta\",\n",
    "    path=\"sales.delta\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "print(\"âœ… Delta table written to ADLS!\")\n",
    "print(f\"Location: abfss://{adls_conn.container}@{adls_conn.account}.dfs.core.windows.net/{adls_conn.path_prefix}/sales.delta\")\n",
    "\n",
    "print(\"Uncomment code above to test with real ADLS connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta from ADLS (uncomment when ready)\n",
    "\n",
    "df_from_adls = engine.read(\n",
    "    connection=adls_conn,\n",
    "    format=\"delta\",\n",
    "    path=\"sales.delta\"\n",
    ")\n",
    "print(\"âœ… Delta table read from ADLS!\")\n",
    "print(df_from_adls)\n",
    "\n",
    "print(\"Uncomment code above to test with real ADLS connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: SparkEngine Delta Support (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if Spark is available\n",
    "try:\n",
    "    from odibi.engine.spark_engine import SparkEngine\n",
    "    SPARK_AVAILABLE = True\n",
    "    print(\"âœ… Spark is available!\")\n",
    "except ImportError as e:\n",
    "    SPARK_AVAILABLE = False\n",
    "    print(\"âš ï¸  Spark not available\")\n",
    "    print(\"   Install with: pip install -e '.[spark]'\")\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_AVAILABLE:\n",
    "    # Initialize SparkEngine\n",
    "    spark_engine = SparkEngine(connections={\"local\": local_conn})\n",
    "    \n",
    "    print(\"âœ… SparkEngine initialized\")\n",
    "    print(f\"   Spark version: {spark_engine.spark.version}\")\n",
    "    print(f\"   Delta configured: {hasattr(spark_engine.spark, '_jsparkSession')}\")\n",
    "else:\n",
    "    print(\"Skipping Spark tests (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_AVAILABLE:\n",
    "    # Write Delta with Spark\n",
    "    \n",
    "    spark_df = spark_engine.spark.createDataFrame(sample_sales)\n",
    "    \n",
    "    spark_engine.write(\n",
    "        spark_df,\n",
    "        connection=local_conn,\n",
    "        format=\"delta\",\n",
    "        path=\"spark_sales.delta\",\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Delta table written with Spark!\")\n",
    "    print(f\"   Location: {local_conn.get_path('spark_sales.delta')}\")\n",
    "else:\n",
    "    print(\"Skipping (Spark not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_AVAILABLE:\n",
    "    # Read Delta with Spark\n",
    "    spark_df_read = spark_engine.read(\n",
    "        connection=local_conn,\n",
    "        format=\"delta\",\n",
    "        path=\"spark_sales.delta\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Delta table read with Spark!\")\n",
    "    print(\"\\nSchema:\")\n",
    "    spark_df_read.printSchema()\n",
    "    \n",
    "    print(\"\\nData (first 5 rows):\")\n",
    "    spark_df_read.show(5)\n",
    "else:\n",
    "    print(\"Skipping (Spark not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 2B: Delta Lake Support - Summary\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nâœ… What You Learned:\")\n",
    "print(\"\\n1. Delta Lake Basics:\")\n",
    "print(\"   - Write with format='delta'\")\n",
    "print(\"   - Read Delta tables\")\n",
    "print(\"   - Append mode creates versions\")\n",
    "print(\"\\n2. Time Travel:\")\n",
    "print(\"   - Read specific versions with versionAsOf\")\n",
    "print(\"   - Compare different versions\")\n",
    "print(\"\\n3. History & Restore:\")\n",
    "print(\"   - Track all versions with get_delta_history()\")\n",
    "print(\"   - Restore to previous version with restore_delta()\")\n",
    "print(\"\\n4. VACUUM:\")\n",
    "print(\"   - Clean old files to save storage\")\n",
    "print(\"   - Dry run mode for preview\")\n",
    "print(\"\\n5. Partitioning:\")\n",
    "print(\"   - Partition by low-cardinality columns\")\n",
    "print(\"   - Performance warnings\")\n",
    "print(\"\\n6. ADLS Integration:\")\n",
    "print(\"   - Delta works with Azure storage\")\n",
    "print(\"   - Same API for local and cloud\")\n",
    "print(\"\\n7. Spark Support:\")\n",
    "print(\"   - SparkEngine supports Delta\")\n",
    "print(\"   - Auto-configuration with delta-spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ’¡ Best Practices:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n1. Use Delta for production data\")\n",
    "print(\"   - ACID transactions\")\n",
    "print(\"   - Schema evolution\")\n",
    "print(\"   - Time travel\")\n",
    "print(\"\\n2. Run VACUUM regularly\")\n",
    "print(\"   - Default: retention_hours=168 (7 days)\")\n",
    "print(\"   - Saves storage costs\")\n",
    "print(\"   - Keeps time travel history\")\n",
    "print(\"\\n3. Partition wisely\")\n",
    "print(\"   - Low-cardinality columns only\")\n",
    "print(\"   - Each partition > 1000 rows\")\n",
    "print(\"   - Total partitions < 1000\")\n",
    "print(\"\\n4. Use time travel for:\")\n",
    "print(\"   - Debugging (what changed?)\")\n",
    "print(\"   - Auditing (who wrote what?)\")\n",
    "print(\"   - Recovery (restore good version)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n1. Test with your ADLS connection:\")\n",
    "print(\"   - Update Part 10 with real credentials\")\n",
    "print(\"   - Write and read Delta from cloud\")\n",
    "print(\"\\n2. Build a real pipeline:\")\n",
    "print(\"   - See examples/template_full.yaml\")\n",
    "print(\"   - Use format='delta' for write nodes\")\n",
    "print(\"\\n3. Explore Phase 2C (Performance):\")\n",
    "print(\"   - Parallel Key Vault fetching\")\n",
    "print(\"   - Enhanced error handling\")\n",
    "print(\"   - Production setup tools\")\n",
    "print(\"\\n4. Check documentation:\")\n",
    "print(\"   - docs/PHASE2_DESIGN_DECISIONS.md\")\n",
    "print(\"   - docs/SUPPORTED_FORMATS.md\")\n",
    "print(\"   - CHANGELOG.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up test data\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(\"./delta_test_data\", ignore_errors=True)\n",
    "print(\"âœ… Test data cleaned up\")\n",
    "\n",
    "print(\"Uncomment code above to remove test data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
