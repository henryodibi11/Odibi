{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2B Production Pipeline - Delta Lake with YAML Config\n",
    "\n",
    "**Version:** v1.2.0-alpha.2-phase2b  \n",
    "**Focus:** Production-ready Delta Lake pipelines using YAML + Key Vault\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "‚úÖ **Best Practices:**\n",
    "- YAML configuration (not hardcoded Python)\n",
    "- Key Vault authentication (not direct keys)\n",
    "- Delta Lake as default format\n",
    "- Multi-account storage setup\n",
    "- Error handling and validation\n",
    "\n",
    "‚úÖ **Real-World Scenarios:**\n",
    "- Bronze ‚Üí Silver ‚Üí Gold architecture\n",
    "- CSV to Delta conversion\n",
    "- Time travel for auditing\n",
    "- VACUUM maintenance\n",
    "- Version comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Azure Setup:**\n",
    "1. Storage account with containers (bronze, silver, gold)\n",
    "2. Key Vault with storage keys stored as secrets\n",
    "3. Managed Identity or Service Principal with access\n",
    "\n",
    "**Local Setup:**\n",
    "```bash\n",
    "pip install -e \".[pandas,azure]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create Production YAML Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration for production Delta Lake pipeline\n",
    "config = {\n",
    "    \"project\": \"delta_lake_production\",\n",
    "    \"description\": \"Production Delta Lake pipeline with Bronze/Silver/Gold architecture\",\n",
    "    \"engine\": \"pandas\",\n",
    "    \"connections\": {\n",
    "        # Local for testing\n",
    "        \"local\": {\"type\": \"local\", \"base_path\": \"./pipeline_data\"},\n",
    "        # Bronze layer (raw data from sources)\n",
    "        \"bronze\": {\n",
    "            \"type\": \"azure_adls\",\n",
    "            \"account\": \"your_account_name\",  # ‚Üê Update this\n",
    "            \"container\": \"your_container_name\",  # ‚Üê Update this\n",
    "            \"path_prefix\": \"raw\",\n",
    "            \"auth_mode\": \"key_vault\",  # ‚Üê Best practice!\n",
    "            \"auth\": {\n",
    "                \"key_vault_name\": \"your_key_valut\",  # ‚Üê Update this\n",
    "                \"secret_name\": \"your_secret_name\",  # ‚Üê Update this\n",
    "            },\n",
    "        },\n",
    "        # Silver layer (cleaned, validated data)\n",
    "        \"silver\": {\n",
    "            \"type\": \"azure_adls\",\n",
    "            \"account\": \"your_account_name\",  # ‚Üê Update this\n",
    "            \"container\": \"your_container_name\",  # ‚Üê Update this\n",
    "            \"path_prefix\": \"clean\",\n",
    "            \"auth_mode\": \"key_vault\",\n",
    "            \"auth\": {\n",
    "                \"key_vault_name\": \"your_key_valut\",  # ‚Üê Update this\n",
    "                \"secret_name\": \"your_secret_name\",  # ‚Üê Update this\n",
    "            },\n",
    "        },\n",
    "        # Gold layer (aggregated, business-ready data)\n",
    "        \"gold\": {\n",
    "            \"type\": \"azure_adls\",\n",
    "            \"account\": \"your_account_name\",  # ‚Üê Update this\n",
    "            \"container\": \"your_container_name\",  # ‚Üê Update this\n",
    "            \"path_prefix\": \"aggregated\",\n",
    "            \"auth_mode\": \"key_vault\",\n",
    "            \"auth\": {\n",
    "                \"key_vault_name\": \"your_key_valut\",  # ‚Üê Update this\n",
    "                \"secret_name\": \"your_secret_name\",  # ‚Üê Update this\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"story\": {\"connection\": \"local\", \"path\": \"stories/\", \"enabled\": True},\n",
    "    \"retry\": {\"max_attempts\": 3, \"backoff_seconds\": 2.0},\n",
    "    \"logging\": {\"level\": \"INFO\"},\n",
    "    \"pipelines\": [\n",
    "        # Pipeline 1: Bronze ‚Üí Silver (CSV to Delta)\n",
    "        {\n",
    "            \"pipeline\": \"bronze_to_silver\",\n",
    "            \"name\": \"Bronze to Silver - Sales Data\",\n",
    "            \"description\": \"Convert raw CSV to cleaned Delta Lake tables\",\n",
    "            \"nodes\": [\n",
    "                # Read raw CSV from Bronze\n",
    "                {\n",
    "                    \"name\": \"read_raw_sales\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"bronze\",\n",
    "                        \"path\": \"sales/raw_sales.csv\",\n",
    "                        \"format\": \"csv\",\n",
    "                    },\n",
    "                },\n",
    "                # Clean and validate (single SQL statement)\n",
    "                {\n",
    "                    \"name\": \"clean_sales\",\n",
    "                    \"depends_on\": [\"read_raw_sales\"],\n",
    "                    \"transform\": {\n",
    "                        \"steps\": [\n",
    "                            \"\"\"\n",
    "                            SELECT \n",
    "                                *,\n",
    "                                now() as processed_at\n",
    "                            FROM read_raw_sales \n",
    "                            WHERE order_id IS NOT NULL \n",
    "                            AND amount > 0\n",
    "                            \"\"\"\n",
    "                        ]\n",
    "                    },\n",
    "                },\n",
    "                # Write to Silver as Delta (best practice!)\n",
    "                {\n",
    "                    \"name\": \"write_silver_sales\",\n",
    "                    \"depends_on\": [\"clean_sales\"],\n",
    "                    \"write\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",  # ‚Üê Delta format!\n",
    "                        \"format\": \"delta\",\n",
    "                        \"mode\": \"append\",  # ‚Üê Incremental loads\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        # Pipeline 2: Silver ‚Üí Gold (Aggregation)\n",
    "        {\n",
    "            \"pipeline\": \"silver_to_gold\",\n",
    "            \"name\": \"Silver to Gold - Daily Aggregates\",\n",
    "            \"description\": \"Create business-ready aggregated tables\",\n",
    "            \"nodes\": [\n",
    "                # Read from Silver Delta\n",
    "                {\n",
    "                    \"name\": \"read_silver_sales\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",\n",
    "                        \"format\": \"delta\",\n",
    "                    },\n",
    "                },\n",
    "                # Aggregate by date\n",
    "                {\n",
    "                    \"name\": \"aggregate_daily\",\n",
    "                    \"depends_on\": [\"read_silver_sales\"],\n",
    "                    \"transform\": {\n",
    "                        \"steps\": [\n",
    "                            \"\"\"\n",
    "                            SELECT \n",
    "                                DATE(processed_at) as date,\n",
    "                                COUNT(*) as order_count,\n",
    "                                SUM(amount) as total_amount,\n",
    "                                AVG(amount) as avg_amount,\n",
    "                                MIN(amount) as min_amount,\n",
    "                                MAX(amount) as max_amount\n",
    "                            FROM read_silver_sales\n",
    "                            GROUP BY DATE(processed_at)\n",
    "                            ORDER BY date DESC\n",
    "                            \"\"\"\n",
    "                        ]\n",
    "                    },\n",
    "                },\n",
    "                # Write to Gold as Delta with partitioning\n",
    "                {\n",
    "                    \"name\": \"write_gold_daily\",\n",
    "                    \"depends_on\": [\"aggregate_daily\"],\n",
    "                    \"write\": {\n",
    "                        \"connection\": \"gold\",\n",
    "                        \"path\": \"sales/daily_summary.delta\",\n",
    "                        \"format\": \"delta\",\n",
    "                        \"mode\": \"overwrite\",  # ‚Üê Full refresh for aggregates\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        # Pipeline 3: Time Travel Audit\n",
    "        {\n",
    "            \"pipeline\": \"audit_delta_versions\",\n",
    "            \"name\": \"Audit Trail - Compare Versions\",\n",
    "            \"description\": \"Compare current vs previous Delta versions\",\n",
    "            \"nodes\": [\n",
    "                # Read current version\n",
    "                {\n",
    "                    \"name\": \"read_current\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",\n",
    "                        \"format\": \"delta\",\n",
    "                    },\n",
    "                },\n",
    "                # Read previous version (time travel!)\n",
    "                {\n",
    "                    \"name\": \"read_previous\",\n",
    "                    \"read\": {\n",
    "                        \"connection\": \"silver\",\n",
    "                        \"path\": \"sales/sales.delta\",\n",
    "                        \"format\": \"delta\",\n",
    "                        \"options\": {\n",
    "                            \"versionAsOf\": 0  # ‚Üê Time travel!\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                # Compare versions\n",
    "                {\n",
    "                    \"name\": \"compare_versions\",\n",
    "                    \"depends_on\": [\"read_current\", \"read_previous\"],\n",
    "                    \"transform\": {\n",
    "                        \"steps\": [\n",
    "                            \"\"\"\n",
    "                            SELECT \n",
    "                                'Current' as version_type,\n",
    "                                COUNT(*) as row_count,\n",
    "                                SUM(amount) as total_amount\n",
    "                            FROM read_current\n",
    "                            UNION ALL\n",
    "                            SELECT \n",
    "                                'Previous' as version_type,\n",
    "                                COUNT(*) as row_count,\n",
    "                                SUM(amount) as total_amount\n",
    "                            FROM read_previous\n",
    "                            \"\"\"\n",
    "                        ]\n",
    "                    },\n",
    "                },\n",
    "                # Save audit report\n",
    "                {\n",
    "                    \"name\": \"save_audit\",\n",
    "                    \"depends_on\": [\"compare_versions\"],\n",
    "                    \"write\": {\n",
    "                        \"connection\": \"local\",\n",
    "                        \"path\": \"audit/version_comparison.csv\",\n",
    "                        \"format\": \"csv\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = Path(\"config_production.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"‚úÖ Production YAML configuration created!\")\n",
    "print(f\"\\nLocation: {config_path.absolute()}\")\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"1. Update YOUR_STORAGE_ACCOUNT with your storage account name\")\n",
    "print(\"2. Update YOUR_KEY_VAULT with your Key Vault name\")\n",
    "print(\"3. Ensure secrets exist in Key Vault:\")\n",
    "print(\"   - bronze-storage-key\")\n",
    "print(\"   - silver-storage-key\")\n",
    "print(\"   - gold-storage-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Review the Generated Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the configuration\n",
    "print(\"Generated Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "with open(\"config_production.yaml\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Update Configuration with Your Azure Details\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Before running pipelines, update the YAML file:\n",
    "\n",
    "```yaml\n",
    "connections:\n",
    "  bronze:\n",
    "    account: \"mycompanystorage\"      # ‚Üê Your storage account\n",
    "    auth:\n",
    "      key_vault_name: \"mycompany-kv\"   # ‚Üê Your Key Vault\n",
    "      secret_name: \"bronze-storage-key\" # ‚Üê Secret in Key Vault\n",
    "```\n",
    "\n",
    "**Key Vault Setup:**\n",
    "1. Store your storage account keys as secrets in Key Vault\n",
    "2. Grant your identity access to Key Vault (Get Secret permission)\n",
    "3. Use DefaultAzureCredential (works in Databricks, local with `az login`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually edit the file or update programmatically\n",
    "import yaml\n",
    "\n",
    "# Option 1: Edit config_production.yaml manually in VS Code\n",
    "print(\"üìù Edit config_production.yaml and update:\")\n",
    "print(\"\\n1. YOUR_STORAGE_ACCOUNT ‚Üí your actual storage account name\")\n",
    "print(\"2. YOUR_KEY_VAULT ‚Üí your actual Key Vault name\")\n",
    "print(\"\\nThen run the next cell to validate.\")\n",
    "\n",
    "# Option 2: Update programmatically (uncomment and modify)\n",
    "# with open(\"config_production.yaml\", \"r\") as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "#\n",
    "# # Update values\n",
    "# for conn_name in [\"bronze\", \"silver\", \"gold\"]:\n",
    "#     config[\"connections\"][conn_name][\"account\"] = \"your_actual_account\"\n",
    "#     config[\"connections\"][conn_name][\"auth\"][\"key_vault_name\"] = \"your_actual_kv\"\n",
    "#\n",
    "# # Save\n",
    "# with open(\"config_production.yaml\", \"w\") as f:\n",
    "#     yaml.dump(config, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Validate Configuration (Local Testing First)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from odibi.pipeline import Pipeline\n",
    "from odibi.config import ProjectConfig\n",
    "\n",
    "# Load and validate configuration\n",
    "try:\n",
    "    with open(\"config_production.yaml\", \"r\") as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "\n",
    "    # Validate with Pydantic\n",
    "    project_config = ProjectConfig(**config_dict)\n",
    "\n",
    "    print(\"‚úÖ Configuration is valid!\")\n",
    "    print(f\"\\nConnections defined: {list(project_config.connections.keys())}\")\n",
    "    print(f\"Pipelines defined: {[p.pipeline for p in project_config.pipelines]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(\"\\nPlease fix the configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Create Sample Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Create sample sales data\n",
    "np.random.seed(42)\n",
    "n_rows = 1000\n",
    "\n",
    "sample_data = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": range(1, n_rows + 1),\n",
    "        \"customer\": [f\"Customer_{i % 100}\" for i in range(n_rows)],\n",
    "        \"product\": np.random.choice([\"Laptop\", \"Phone\", \"Tablet\", \"Monitor\", \"Keyboard\"], n_rows),\n",
    "        \"amount\": np.random.uniform(50, 2000, n_rows).round(2),\n",
    "        \"quantity\": np.random.randint(1, 10, n_rows),\n",
    "        \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n_rows),\n",
    "        \"order_date\": [\n",
    "            (datetime.now() - timedelta(days=np.random.randint(0, 90))).strftime(\"%Y-%m-%d\")\n",
    "            for _ in range(n_rows)\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add some nulls for testing (10%)\n",
    "null_indices = np.random.choice(n_rows, size=int(n_rows * 0.1), replace=False)\n",
    "sample_data.loc[null_indices, \"order_id\"] = None\n",
    "\n",
    "# Add some invalid amounts (5%)\n",
    "invalid_indices = np.random.choice(n_rows, size=int(n_rows * 0.05), replace=False)\n",
    "sample_data.loc[invalid_indices, \"amount\"] = -100\n",
    "\n",
    "# Save in correct location (with sales/ subdirectory)\n",
    "output_path = Path(\"pipeline_data/input/sales\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "sample_data.to_csv(output_path / \"raw_sales.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Sample data created!\")\n",
    "print(f\"\\nLocation: {output_path / 'raw_sales.csv'}\")\n",
    "print(f\"Rows: {len(sample_data):,}\")\n",
    "print(\n",
    "    f\"Nulls: {sample_data['order_id'].isna().sum()} ({sample_data['order_id'].isna().sum() / len(sample_data) * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"Invalid amounts: {(sample_data['amount'] < 0).sum()}\")\n",
    "print(\"\\nSample:\")\n",
    "print(sample_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test Locally First (Before Cloud)\n",
    "\n",
    "**Best Practice:** Always test with local connections before running against cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local test configuration\n",
    "local_config = {\n",
    "    \"project\": \"delta_lake_production\",\n",
    "    \"description\": \"Production Delta Lake pipeline with Bronze/Silver/Gold architecture\",\n",
    "    \"engine\": \"pandas\",\n",
    "    \"connections\": {\n",
    "        \"bronze\": {\"type\": \"local\", \"base_path\": \"./pipeline_data/input\"},\n",
    "        \"silver\": {\"type\": \"local\", \"base_path\": \"./pipeline_data/silver\"},\n",
    "        \"gold\": {\"type\": \"local\", \"base_path\": \"./pipeline_data/gold\"},\n",
    "        \"local\": {\"type\": \"local\", \"base_path\": \"./pipeline_data\"},\n",
    "    },\n",
    "    \"story\": {\"connection\": \"local\", \"path\": \"stories/\", \"enabled\": True},\n",
    "    \"retry\": {\"max_attempts\": 3, \"backoff_seconds\": 2.0},\n",
    "    \"logging\": {\"level\": \"INFO\"},\n",
    "    \"pipelines\": config_dict[\"pipelines\"],  # Use same pipelines\n",
    "}\n",
    "\n",
    "# Save local config\n",
    "with open(\"config_local.yaml\", \"w\") as f:\n",
    "    yaml.dump(local_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"‚úÖ Local test configuration created!\")\n",
    "print(\"\\nThis uses local file system instead of Azure for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Run Pipeline - Bronze to Silver (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline from local config\n",
    "manager = Pipeline.from_yaml(\"config_local.yaml\")\n",
    "\n",
    "print(\"Available pipelines:\")\n",
    "for pipeline_name in manager._pipelines.keys():\n",
    "    print(f\"  - {pipeline_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Running: bronze_to_silver\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run Bronze ‚Üí Silver pipeline\n",
    "result = manager.run(\"bronze_to_silver\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed!\")\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Completed: {len(result.completed)} nodes\")\n",
    "print(f\"  Failed: {len(result.failed)} nodes\")\n",
    "print(f\"  Skipped: {len(result.skipped)} nodes\")\n",
    "print(f\"  Duration: {result.duration:.2f}s\")\n",
    "\n",
    "if result.completed:\n",
    "    print(\"\\nCompleted nodes:\")\n",
    "    for node_name in result.completed:\n",
    "        print(f\"  ‚úÖ {node_name}\")\n",
    "\n",
    "if result.failed:\n",
    "    print(\"\\nFailed nodes:\")\n",
    "    for node_name in result.failed:\n",
    "        print(f\"  ‚ùå {node_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Verify Delta Table Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.engine.pandas_engine import PandasEngine\n",
    "from odibi.connections.local import LocalConnection\n",
    "\n",
    "engine = PandasEngine()\n",
    "silver_conn = LocalConnection(base_path=\"./pipeline_data/silver\")\n",
    "\n",
    "# Read the Delta table we just created\n",
    "df_silver = engine.read(connection=silver_conn, format=\"delta\", path=\"sales/sales.delta\")\n",
    "\n",
    "print(\"‚úÖ Delta table in Silver layer:\")\n",
    "print(f\"\\nRows: {len(df_silver):,}\")\n",
    "print(f\"Columns: {list(df_silver.columns)}\")\n",
    "print(\"\\nData quality after cleaning:\")\n",
    "print(f\"  Nulls in order_id: {df_silver['order_id'].isna().sum()} (should be 0)\")\n",
    "print(f\"  Invalid amounts: {(df_silver['amount'] <= 0).sum()} (should be 0)\")\n",
    "print(\"\\nSample:\")\n",
    "print(df_silver.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Check Delta Table History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Delta table history\n",
    "history = engine.get_delta_history(connection=silver_conn, path=\"sales/sales.delta\")\n",
    "\n",
    "print(\"Delta Table History:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for entry in history:\n",
    "    print(f\"\\nVersion {entry['version']}:\")\n",
    "    print(f\"  Operation: {entry['operation']}\")\n",
    "    print(f\"  Timestamp: {entry['timestamp']}\")\n",
    "    if \"operationMetrics\" in entry and \"numOutputRows\" in entry[\"operationMetrics\"]:\n",
    "        print(f\"  Rows: {entry['operationMetrics']['numOutputRows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Run Pipeline - Silver to Gold (Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Running: silver_to_gold\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run Silver ‚Üí Gold pipeline\n",
    "result = manager.run(\"silver_to_gold\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed!\")\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Completed: {len(result.completed)} nodes\")\n",
    "print(f\"  Failed: {len(result.failed)} nodes\")\n",
    "print(f\"  Duration: {result.duration:.2f}s\")\n",
    "\n",
    "if result.completed:\n",
    "    print(\"\\nCompleted nodes:\")\n",
    "    for node_name in result.completed:\n",
    "        print(f\"  ‚úÖ {node_name}\")\n",
    "\n",
    "if result.failed:\n",
    "    print(\"\\nFailed nodes:\")\n",
    "    for node_name in result.failed:\n",
    "        print(f\"  ‚ùå {node_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Gold layer aggregates\n",
    "gold_conn = LocalConnection(base_path=\"./pipeline_data/gold\")\n",
    "\n",
    "df_gold = engine.read(connection=gold_conn, format=\"delta\", path=\"sales/daily_summary.delta\")\n",
    "\n",
    "print(\"‚úÖ Gold layer aggregates:\")\n",
    "print(f\"\\nDays: {len(df_gold)}\")\n",
    "print(\"\\nDaily Summary:\")\n",
    "print(df_gold.sort_values(\"date\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Run Second Load (Test Append Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more sample data\n",
    "new_data = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": range(n_rows + 1, n_rows + 101),\n",
    "        \"customer\": [f\"Customer_{i % 100}\" for i in range(100)],\n",
    "        \"product\": np.random.choice([\"Laptop\", \"Phone\", \"Tablet\"], 100),\n",
    "        \"amount\": np.random.uniform(100, 1500, 100).round(2),\n",
    "        \"quantity\": np.random.randint(1, 5, 100),\n",
    "        \"region\": np.random.choice([\"North\", \"South\"], 100),\n",
    "        \"order_date\": [datetime.now().strftime(\"%Y-%m-%d\") for _ in range(100)],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Append to CSV\n",
    "new_data.to_csv(\"pipeline_data/input/raw_sales.csv\", mode=\"a\", header=False, index=False)\n",
    "\n",
    "print(\"‚úÖ Added 100 more orders to source CSV\")\n",
    "\n",
    "# Run pipeline again (should append to Delta)\n",
    "result = manager.run(\"bronze_to_silver\")\n",
    "\n",
    "print(\"\\n‚úÖ Second pipeline run completed!\")\n",
    "\n",
    "# Check Delta table\n",
    "df_silver_v2 = engine.read(silver_conn, format=\"delta\", path=\"sales/sales.delta\")\n",
    "\n",
    "print(f\"\\nDelta table now has {len(df_silver_v2):,} rows\")\n",
    "print(f\"Added {len(df_silver_v2) - len(df_silver):,} new rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Time Travel - Compare Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create audit directory\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"pipeline_data/audit\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"‚úÖ Audit directory created\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Running: audit_delta_versions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run audit pipeline (uses time travel)\n",
    "result = manager.run(\"audit_delta_versions\")\n",
    "\n",
    "print(\"\\n‚úÖ Audit pipeline completed!\")\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Completed: {len(result.completed)} nodes\")\n",
    "print(f\"  Failed: {len(result.failed)} nodes\")\n",
    "print(f\"  Duration: {result.duration:.2f}s\")\n",
    "\n",
    "if result.completed:\n",
    "    print(\"\\nCompleted nodes:\")\n",
    "    for node_name in result.completed:\n",
    "        print(f\"  ‚úÖ {node_name}\")\n",
    "\n",
    "if result.failed:\n",
    "    print(\"\\nFailed nodes:\")\n",
    "    for node_name in result.failed:\n",
    "        print(f\"  ‚ùå {node_name}\")\n",
    "\n",
    "# Only read the file if pipeline succeeded\n",
    "if len(result.failed) == 0 and len(result.completed) > 0:\n",
    "    # Read audit report\n",
    "    audit_df = pd.read_csv(\"pipeline_data/audit/version_comparison.csv\")\n",
    "\n",
    "    print(\"\\nVersion Comparison:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(audit_df)\n",
    "\n",
    "    # Calculate differences\n",
    "    current = audit_df[audit_df[\"version_type\"] == \"Current\"].iloc[0]\n",
    "    previous = audit_df[audit_df[\"version_type\"] == \"Previous\"].iloc[0]\n",
    "\n",
    "    print(\"\\nChanges:\")\n",
    "    print(f\"  Rows added: {current['row_count'] - previous['row_count']:,}\")\n",
    "    print(f\"  Amount added: ${current['total_amount'] - previous['total_amount']:,.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pipeline failed - check story for details\")\n",
    "    if result.story_path:\n",
    "        print(f\"Story: {result.story_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: VACUUM - Clean Old Files (Production Pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production VACUUM pattern\n",
    "print(\"VACUUM Maintenance:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# VACUUM with 7-day retention (production default)\n",
    "result = engine.vacuum_delta(\n",
    "    connection=silver_conn,\n",
    "    path=\"sales/sales.delta\",\n",
    "    retention_hours=168,  # 7 days\n",
    "    dry_run=True,  # Preview first\n",
    ")\n",
    "\n",
    "print(f\"\\nDry run: Would delete {result['files_deleted']} files\")\n",
    "\n",
    "# In production, run weekly:\n",
    "print(\"\\nüí° Production VACUUM schedule:\")\n",
    "print(\"\"\"\\n# Weekly VACUUM job\n",
    "tables = [\n",
    "    ('silver', 'sales/sales.delta'),\n",
    "    ('gold', 'sales/daily_summary.delta')\n",
    "]\n",
    "\n",
    "for conn_name, table_path in tables:\n",
    "    result = engine.vacuum_delta(\n",
    "        connection=connections[conn_name],\n",
    "        path=table_path,\n",
    "        retention_hours=168  # Keep 7 days for time travel\n",
    "    )\n",
    "    print(f\"{table_path}: cleaned {result['files_deleted']} files\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Deploy to Production (Azure ADLS + Key Vault)\n",
    "\n",
    "**Now that local testing works, deploy to production:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Production Deployment Checklist:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checklist = [\n",
    "    (\n",
    "        \"1. Azure Storage Setup\",\n",
    "        [\n",
    "            \"‚úì Storage account created\",\n",
    "            \"‚úì Containers: bronze, silver, gold\",\n",
    "            \"‚úì Storage keys copied\",\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"2. Key Vault Setup\",\n",
    "        [\n",
    "            \"‚úì Key Vault created\",\n",
    "            \"‚úì Secrets added: bronze-storage-key, silver-storage-key, gold-storage-key\",\n",
    "            \"‚úì Access policy: Get Secret permission for your identity\",\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"3. Authentication\",\n",
    "        [\n",
    "            \"‚úì Local: 'az login' completed\",\n",
    "            \"‚úì Databricks: Managed Identity configured\",\n",
    "            \"‚úì DefaultAzureCredential working\",\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"4. Configuration\",\n",
    "        [\n",
    "            \"‚úì config_production.yaml updated with real values\",\n",
    "            \"‚úì All YOUR_* placeholders replaced\",\n",
    "            \"‚úì Configuration validated\",\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"5. Data Upload\",\n",
    "        [\n",
    "            \"‚úì Sample CSV uploaded to bronze/raw/sales/raw_sales.csv\",\n",
    "            \"‚úì File accessible via Storage Explorer\",\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "for step, items in checklist:\n",
    "    print(f\"\\n{step}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Once checklist complete, run:\")\n",
    "print(\"\\nmanager = Pipeline.from_yaml('config_production.yaml')\")\n",
    "print(\"manager.run('bronze_to_silver')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when ready to run against production Azure\n",
    "\n",
    "# Load production config\n",
    "prod_manager = Pipeline.from_yaml(\"config_production.yaml\")\n",
    "\n",
    "# Run Bronze ‚Üí Silver with Key Vault auth\n",
    "result = prod_manager.run(\"bronze_to_silver\")\n",
    "\n",
    "print(\"‚úÖ Production pipeline completed!\")\n",
    "print(\"\\nDelta table written to:\")\n",
    "print(\"  abfss://silver@{account}.dfs.core.windows.net/clean/sales/sales.delta\")\n",
    "\n",
    "print(\"Uncomment code above when ready for production deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 15: Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 2B Production Pipeline - Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ What You Learned:\")\n",
    "print(\"\\n1. YAML Configuration:\")\n",
    "print(\"   - All settings in version-controlled YAML\")\n",
    "print(\"   - No hardcoded credentials in code\")\n",
    "print(\"   - Separate configs for local vs production\")\n",
    "\n",
    "print(\"\\n2. Key Vault Authentication (Best Practice):\")\n",
    "print(\"   - Credentials stored in Azure Key Vault\")\n",
    "print(\"   - DefaultAzureCredential for access\")\n",
    "print(\"   - No secrets in code or config files\")\n",
    "\n",
    "print(\"\\n3. Delta Lake as Default:\")\n",
    "print(\"   - format='delta' for all persistent tables\")\n",
    "print(\"   - ACID transactions prevent partial writes\")\n",
    "print(\"   - Time travel for auditing\")\n",
    "print(\"   - Schema evolution support\")\n",
    "\n",
    "print(\"\\n4. Multi-Layer Architecture:\")\n",
    "print(\"   - Bronze: Raw data (CSV from sources)\")\n",
    "print(\"   - Silver: Cleaned, validated Delta tables\")\n",
    "print(\"   - Gold: Aggregated, business-ready Delta tables\")\n",
    "\n",
    "print(\"\\n5. Testing Strategy:\")\n",
    "print(\"   - Test locally first (fast, free)\")\n",
    "print(\"   - Validate configuration before deployment\")\n",
    "print(\"   - Deploy to production when local tests pass\")\n",
    "\n",
    "print(\"\\n6. Maintenance Patterns:\")\n",
    "print(\"   - Weekly VACUUM (retention_hours=168)\")\n",
    "print(\"   - Time travel for auditing changes\")\n",
    "print(\"   - History tracking for debugging\")\n",
    "\n",
    "print(\"\\nüí° Best Practices Applied:\")\n",
    "print(\"   ‚úì Key Vault (not direct keys)\")\n",
    "print(\"   ‚úì YAML config (not Python hardcoding)\")\n",
    "print(\"   ‚úì Delta Lake (not CSV/Parquet)\")\n",
    "print(\"   ‚úì Local testing (before cloud)\")\n",
    "print(\"   ‚úì Append mode (incremental loads)\")\n",
    "print(\"   ‚úì Version control (config files in git)\")\n",
    "print(\"   ‚úì Validation (null removal, data quality)\")\n",
    "print(\"   ‚úì VACUUM (storage cost optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up local test data\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"./pipeline_data\", ignore_errors=True)\n",
    "Path(\"config_local.yaml\").unlink(missing_ok=True)\n",
    "Path(\"config_production.yaml\").unlink(missing_ok=True)\n",
    "print(\"‚úÖ Test data cleaned up\")\n",
    "\n",
    "print(\"Uncomment code above to remove test data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
