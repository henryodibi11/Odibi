{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODIBI Databricks Setup Guide\n",
    "\n",
    "**Phase 2C: Interactive setup for Databricks + Azure Key Vault**\n",
    "\n",
    "This notebook helps you configure ODIBI in a Databricks environment with Azure Data Lake Storage and Key Vault authentication.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. ‚úÖ Validates your Databricks environment\n",
    "2. ‚úÖ Tests Azure Key Vault connectivity\n",
    "3. ‚úÖ Configures multiple ADLS connections in parallel (3x faster)\n",
    "4. ‚úÖ Verifies your setup with a test pipeline\n",
    "5. ‚úÖ Provides troubleshooting tips\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Databricks cluster with runtime 12.2+ (Delta Lake support)\n",
    "- ODIBI installed: `%pip install odibi[azure,spark]`\n",
    "- Azure Key Vault with storage account keys stored\n",
    "- Managed identity or service principal configured\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install ODIBI\n",
    "\n",
    "Install ODIBI with Azure and Spark extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install odibi[azure,spark] --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Validate Databricks Environment\n",
    "\n",
    "Check that we're running in Databricks with Spark available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.utils import validate_databricks_environment\n",
    "\n",
    "env_info = validate_databricks_environment(verbose=True)\n",
    "\n",
    "if not env_info[\"is_databricks\"]:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Not running in Databricks environment\")\n",
    "    print(\"This notebook is designed for Databricks. Some features may not work.\")\n",
    "\n",
    "if not env_info[\"spark_available\"]:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Spark session not available\")\n",
    "    print(\"Please start your cluster and retry.\")\n",
    "\n",
    "print(\"\\n‚úì Environment validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Your Connections\n",
    "\n",
    "**Edit the configuration below** with your storage accounts and Key Vault details:\n",
    "\n",
    "### Configuration Template\n",
    "\n",
    "```python\n",
    "connections_config = {\n",
    "    \"bronze\": {\n",
    "        \"account\": \"mystorageaccount1\",\n",
    "        \"container\": \"bronze\",\n",
    "        \"auth_mode\": \"key_vault\",\n",
    "        \"key_vault_name\": \"mykeyvault\",\n",
    "        \"secret_name\": \"storage1-key\",\n",
    "    },\n",
    "    \"silver\": {\n",
    "        \"account\": \"mystorageaccount2\",\n",
    "        \"container\": \"silver\",\n",
    "        \"auth_mode\": \"key_vault\",\n",
    "        \"key_vault_name\": \"mykeyvault\",\n",
    "        \"secret_name\": \"storage2-key\",\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù EDIT THIS CONFIGURATION\n",
    "connections_config = {\n",
    "    \"bronze\": {\n",
    "        \"account\": \"YOUR_STORAGE_ACCOUNT_1\",\n",
    "        \"container\": \"bronze\",\n",
    "        \"auth_mode\": \"key_vault\",\n",
    "        \"key_vault_name\": \"YOUR_KEY_VAULT_NAME\",\n",
    "        \"secret_name\": \"YOUR_SECRET_NAME_1\",\n",
    "    },\n",
    "    # Add more connections as needed\n",
    "    # \"silver\": {...},\n",
    "    # \"gold\": {...},\n",
    "}\n",
    "\n",
    "print(f\"‚úì Configured {len(connections_config)} connection(s)\")\n",
    "for name in connections_config.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create and Test Connections\n",
    "\n",
    "Create ADLS connections from your configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.connections import AzureADLS\n",
    "\n",
    "# Create connections (validation deferred)\n",
    "connections = {}\n",
    "for name, config in connections_config.items():\n",
    "    connections[name] = AzureADLS(**config, validate=True)\n",
    "    print(f\"‚úì Created connection: {name}\")\n",
    "\n",
    "print(f\"\\n‚úì All {len(connections)} connections created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fetch Secrets in Parallel ‚ö°\n",
    "\n",
    "**Performance boost!** Fetch all Key Vault secrets in parallel (3x faster than sequential):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.utils import configure_connections_parallel\n",
    "import time\n",
    "\n",
    "print(\"üîÑ Fetching Key Vault secrets in parallel...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "connections, errors = configure_connections_parallel(\n",
    "    connections,\n",
    "    prefetch_secrets=True,\n",
    "    max_workers=5,\n",
    "    timeout=30.0,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è  Encountered {len(errors)} error(s):\")\n",
    "    for error in errors:\n",
    "        print(f\"  ‚úó {error}\")\n",
    "else:\n",
    "    print(f\"\\n‚úì All secrets fetched successfully in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Spark Engine\n",
    "\n",
    "Create a Spark engine with your configured connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odibi.engine import SparkEngine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get active Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "if not spark:\n",
    "    print(\"‚ö†Ô∏è  No active Spark session. Starting a new one...\")\n",
    "    spark = SparkSession.builder.appName(\"ODIBI-Setup\").getOrCreate()\n",
    "\n",
    "# Create Spark engine with connections\n",
    "engine = SparkEngine(connections=connections, spark_session=spark)\n",
    "\n",
    "print(f\"\\n‚úì SparkEngine configured with {len(connections)} connection(s)\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  App name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Your Setup\n",
    "\n",
    "Run a simple test to verify everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "        \"value\": [100, 200, 300],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üìä Sample data:\")\n",
    "print(test_data)\n",
    "\n",
    "# Get first connection for testing\n",
    "test_conn_name = list(connections.keys())[0]\n",
    "test_conn = connections[test_conn_name]\n",
    "\n",
    "test_path = \"odibi_test/sample.parquet\"\n",
    "test_uri = test_conn.uri(test_path)\n",
    "\n",
    "print(f\"\\nüìù Test path: {test_uri}\")\n",
    "print(\"\\nüîÑ Writing test data...\")\n",
    "\n",
    "try:\n",
    "    # Write using engine\n",
    "    engine.write(\n",
    "        data=test_data,\n",
    "        path=test_uri,\n",
    "        format=\"parquet\",\n",
    "        mode=\"overwrite\",\n",
    "    )\n",
    "    print(\"‚úì Write successful\")\n",
    "\n",
    "    # Read back\n",
    "    print(\"\\nüîÑ Reading test data back...\")\n",
    "    result_df = engine.read(test_uri, format=\"parquet\")\n",
    "\n",
    "    print(\"‚úì Read successful\")\n",
    "    print(\"\\nüìä Result:\")\n",
    "    result_df.show()\n",
    "\n",
    "    print(\"\\nüéâ SUCCESS! Your ODIBI setup is working correctly!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test failed: {e}\")\n",
    "    print(\"\\nSee troubleshooting section below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Performance Comparison\n",
    "\n",
    "Compare sequential vs parallel Key Vault fetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if you have 2+ Key Vault connections\n",
    "kv_connections = {\n",
    "    name: conn\n",
    "    for name, conn in connections.items()\n",
    "    if hasattr(conn, \"auth_mode\") and conn.auth_mode == \"key_vault\"\n",
    "}\n",
    "\n",
    "if len(kv_connections) >= 2:\n",
    "    print(f\"üìä Comparing performance with {len(kv_connections)} Key Vault connections\\n\")\n",
    "\n",
    "    # Clear cached keys\n",
    "    for conn in kv_connections.values():\n",
    "        conn._cached_key = None\n",
    "\n",
    "    # Sequential fetch\n",
    "    print(\"üêå Sequential fetch...\")\n",
    "    start = time.time()\n",
    "    for conn in kv_connections.values():\n",
    "        _ = conn.get_storage_key()\n",
    "    sequential_time = time.time() - start\n",
    "    print(f\"   Time: {sequential_time:.2f}s\\n\")\n",
    "\n",
    "    # Clear cached keys again\n",
    "    for conn in kv_connections.values():\n",
    "        conn._cached_key = None\n",
    "\n",
    "    # Parallel fetch\n",
    "    print(\"‚ö° Parallel fetch...\")\n",
    "    start = time.time()\n",
    "    _, _ = configure_connections_parallel(kv_connections, verbose=False)\n",
    "    parallel_time = time.time() - start\n",
    "    print(f\"   Time: {parallel_time:.2f}s\\n\")\n",
    "\n",
    "    speedup = sequential_time / parallel_time\n",
    "    print(f\"üöÄ Speedup: {speedup:.1f}x faster with parallel fetching!\")\n",
    "else:\n",
    "    print(\n",
    "        f\"‚ÑπÔ∏è  Need 2+ Key Vault connections for performance comparison (have {len(kv_connections)})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "#### 1. Key Vault Access Denied\n",
    "```\n",
    "Error: (Forbidden) The user, group or application does not have secrets get permission\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Ensure your Databricks cluster has a managed identity assigned\n",
    "- Grant the managed identity \"Get\" permission on secrets in Key Vault\n",
    "- Or use Azure CLI: `az keyvault set-policy --name <vault> --object-id <id> --secret-permissions get`\n",
    "\n",
    "#### 2. Timeout Errors\n",
    "```\n",
    "TimeoutError: Key Vault fetch timed out after 30s\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Check network connectivity from Databricks to Azure\n",
    "- Increase timeout: `configure_connections_parallel(..., timeout=60.0)`\n",
    "- Verify Key Vault firewall settings\n",
    "\n",
    "#### 3. Storage Account Access Denied\n",
    "```\n",
    "Error: Operation returned an invalid status code 'Forbidden'\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Verify the storage account key in Key Vault is correct\n",
    "- Check storage account firewall allows Databricks access\n",
    "- Ensure container exists\n",
    "\n",
    "#### 4. Module Import Errors\n",
    "```\n",
    "ImportError: azure-identity not found\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Re-run: `%pip install odibi[azure,spark]`\n",
    "- Restart Python: `dbutils.library.restartPython()`\n",
    "\n",
    "---\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "- **Documentation:** See `docs/` folder in the ODIBI repository\n",
    "- **Examples:** Check `examples/template_full_adls.yaml`\n",
    "- **Issues:** https://github.com/henryodibi11/Odibi/issues\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. ‚úÖ Create a full YAML pipeline configuration\n",
    "2. ‚úÖ See `examples/template_full_adls.yaml` for templates\n",
    "3. ‚úÖ Check out walkthroughs in `walkthroughs/` folder\n",
    "4. ‚úÖ Review `docs/SUPPORTED_FORMATS.md` for file format options\n",
    "\n",
    "**Happy data engineering! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
