# Odibi API Reference

*Auto-generated by `odibi.introspect`*

## Module `odibi.cli.graph`
Graph CLI Command

### Functions
#### `graph_command`
```python
def graph_command(args)
```
> Handle graph subcommand.


---

## Module `odibi.cli.init_pipeline`
### Functions
#### `add_init_parser`
```python
def add_init_parser(subparsers)
```
> Add arguments for init-pipeline command.


#### `init_pipeline_command`
```python
def init_pipeline_command(args)
```
> Execute the init-pipeline command.


---

## Module `odibi.cli.main`
Main CLI entry point.

### Functions
#### `main`
```python
def main()
```
> Main CLI entry point.


---

## Module `odibi.cli.run`
Run command implementation.

### Functions
#### `load_extensions`
```python
def load_extensions(path: pathlib.Path)
```
> Load python extensions (transforms.py, plugins.py) from path.


#### `run_command`
```python
def run_command(args)
```
> Execute pipeline from config file.


---

## Module `odibi.cli.secrets`
Secrets management CLI commands.

### Functions
#### `extract_env_vars`
```python
def extract_env_vars(config_path: str) -> Set[str]
```
> Extract environment variables used in config file.


#### `init_command`
```python
def init_command(args) -> int
```
> Create .env.template from config file usage.


#### `check_keyvault_access`
```python
def check_keyvault_access(config_path: str) -> bool
```
> Check if Key Vault secrets are accessible.


#### `validate_command`
```python
def validate_command(args) -> int
```
> Check if required environment variables and Key Vault secrets are accessible.


#### `add_secrets_parser`
```python
def add_secrets_parser(subparsers)
```
> Add secrets subparser to main parser.


#### `secrets_command`
```python
def secrets_command(args) -> int
```
> Dispatcher for secrets commands.


### Classes
#### `class SimpleConnection`
Simple connection wrapper for Key Vault checking.

- **__init__**`(self, name: str, data: dict)`
  - Initialize self.  See help(type(self)) for accurate signature.

---

## Module `odibi.cli.story`
Story CLI Commands

### Functions
#### `story_command`
```python
def story_command(args)
```
> Handle story subcommands.


#### `generate_command`
```python
def generate_command(args)
```
> Generate documentation story from pipeline config.


#### `diff_command`
```python
def diff_command(args)
```
> Compare two pipeline run stories.


#### `list_command`
```python
def list_command(args)
```
> List available story files.


#### `add_story_parser`
```python
def add_story_parser(subparsers)
```
> Add story subcommand parser.


---

## Module `odibi.cli.validate`
Validate command implementation.

### Functions
#### `validate_command`
```python
def validate_command(args)
```
> Validate config file.


---

## Module `odibi.config`
Configuration models for ODIBI framework.

### Classes
#### `class EngineType`
Supported execution engines.


#### `class ConnectionType`
Supported connection types.


#### `class WriteMode`
Write modes for output operations.


#### `class LogLevel`
Logging levels.


#### `class AlertType`
Types of alerting channels.


#### `class AlertConfig`
Configuration for alerts.


#### `class ErrorStrategy`
Strategy for handling node failures.


#### `class BaseConnectionConfig`
Base configuration for all connections.


#### `class LocalConnectionConfig`
Local filesystem connection.


#### `class AzureBlobConnectionConfig`
Azure Blob Storage connection.


#### `class DeltaConnectionConfig`
Delta Lake connection.


#### `class SQLServerConnectionConfig`
SQL Server connection.


#### `class HttpConnectionConfig`
HTTP connection.


#### `class ReadConfig`
Configuration for reading data.

- **move_query_to_options**`(self)`
  - Move top-level query to options.
- **check_table_or_path**`(self)`
  - Ensure either table or path is provided.

#### `class TransformStep`
Single transformation step.

- **check_step_type**`(self)`
  - Ensure exactly one step type is provided.

#### `class TransformConfig`
Configuration for transforming data.


#### `class ValidationConfig`
Configuration for data validation.


#### `class WriteConfig`
Configuration for writing data.

- **check_table_or_path**`(self)`
  - Ensure either table or path is provided.

#### `class NodeConfig`
Configuration for a single node.

- **check_at_least_one_operation**`(self)`
  - Ensure at least one operation is defined.

#### `class PipelineConfig`
Configuration for a pipeline.

- **check_unique_node_names**`(nodes: List[odibi.config.NodeConfig]) -> List[odibi.config.NodeConfig]`
  - Ensure all node names are unique within the pipeline.

#### `class RetryConfig`
Retry configuration.


#### `class LoggingConfig`
Logging configuration.


#### `class PerformanceConfig`
Performance tuning configuration.


#### `class StoryConfig`
Story generation configuration.


#### `class ProjectConfig`
Complete project configuration from YAML.

- **validate_story_connection_exists**`(self)`
  - Ensure story.connection is defined in connections.
- **check_environments_not_implemented**`(self)`
  - Check environments implementation.

---

## Module `odibi.connections.azure_adls`
Azure Data Lake Storage Gen2 connection (Phase 2A: Multi-mode authentication).

### Classes
#### `class AzureADLS`
Azure Data Lake Storage Gen2 connection.

- **__init__**`(self, account: str, container: str, path_prefix: str = '', auth_mode: str = 'key_vault', key_vault_name: Optional[str] = None, secret_name: Optional[str] = None, account_key: Optional[str] = None, sas_token: Optional[str] = None, tenant_id: Optional[str] = None, client_id: Optional[str] = None, client_secret: Optional[str] = None, validate: bool = True, **kwargs)`
  - Initialize ADLS connection.
- **validate**`(self) -> None`
  - Validate ADLS connection configuration.
- **get_storage_key**`(self, timeout: float = 30.0) -> Optional[str]`
  - Get storage account key (cached).
- **get_client_secret**`(self) -> Optional[str]`
  - Get Service Principal client secret (cached or literal).
- **pandas_storage_options**`(self) -> Dict[str, Any]`
  - Get storage options for pandas/fsspec.
- **configure_spark**`(self, spark) -> None`
  - Configure Spark session with storage credentials.
- **uri**`(self, path: str) -> str`
  - Build abfss:// URI for given path.
- **get_path**`(self, relative_path: str) -> str`
  - Get full abfss:// URI for relative path.

---

## Module `odibi.connections.azure_sql`
Azure SQL Database Connection

### Classes
#### `class AzureSQL`
Azure SQL Database connection.

- **__init__**`(self, server: str, database: str, driver: str = 'ODBC Driver 18 for SQL Server', username: Optional[str] = None, password: Optional[str] = None, auth_mode: str = 'aad_msi', key_vault_name: Optional[str] = None, secret_name: Optional[str] = None, port: int = 1433, timeout: int = 30, **kwargs)`
  - Initialize Azure SQL connection.
- **get_password**`(self) -> Optional[str]`
  - Get password (cached).
- **odbc_dsn**`(self) -> str`
  - Build ODBC connection string.
- **get_path**`(self, relative_path: str) -> str`
  - Get table reference for relative path.
- **validate**`(self) -> None`
  - Validate Azure SQL connection configuration.
- **get_engine**`(self)`
  - Get or create SQLAlchemy engine.
- **read_sql**`(self, query: str, params: Optional[Dict[str, Any]] = None) -> pandas.core.frame.DataFrame`
  - Execute SQL query and return results as DataFrame.
- **read_table**`(self, table_name: str, schema: Optional[str] = 'dbo') -> pandas.core.frame.DataFrame`
  - Read entire table into DataFrame.
- **write_table**`(self, df: pandas.core.frame.DataFrame, table_name: str, schema: Optional[str] = 'dbo', if_exists: str = 'replace', index: bool = False, chunksize: Optional[int] = 1000) -> int`
  - Write DataFrame to SQL table.
- **execute**`(self, sql: str, params: Optional[Dict[str, Any]] = None) -> Any`
  - Execute SQL statement (INSERT, UPDATE, DELETE, etc.).
- **close**`(self)`
  - Close database connection and dispose of engine.
- **get_spark_options**`(self) -> Dict[str, str]`
  - Get Spark JDBC options.

---

## Module `odibi.connections.base`
Base connection interface.

### Classes
#### `class BaseConnection`
Abstract base class for connections.

- **get_path**`(self, relative_path: str) -> str`
  - Get full path for a relative path.
- **validate**`(self) -> None`
  - Validate connection configuration.

---

## Module `odibi.connections.http`
HTTP Connection implementation.

### Classes
#### `class HttpConnection`
Connection to HTTP/HTTPS APIs.

- **__init__**`(self, base_url: str, headers: Optional[Dict[str, str]] = None, auth: Optional[Dict[str, str]] = None, validate: bool = True)`
  - Initialize HTTP connection.
- **validate**`(self) -> None`
  - Validate connection configuration.
- **get_path**`(self, path: str) -> str`
  - Resolve endpoint path.
- **pandas_storage_options**`(self) -> Dict[str, Any]`
  - Get storage options for Pandas/fsspec.

---

## Module `odibi.connections.local`
Local filesystem connection.

### Classes
#### `class LocalConnection`
Connection to local filesystem or URI-based paths (e.g. dbfs:/, file://).

- **__init__**`(self, base_path: str = './data')`
  - Initialize local connection.
- **get_path**`(self, relative_path: str) -> str`
  - Get full path for a relative path.
- **validate**`(self) -> None`
  - Validate that base path exists or can be created.

---

## Module `odibi.connections.local_dbfs`
Local DBFS mock for testing Databricks pipelines locally.

### Classes
#### `class LocalDBFS`
Mock DBFS connection for local development.

- **__init__**`(self, root: Union[str, pathlib.Path] = '.dbfs')`
  - Initialize local DBFS mock.
- **resolve**`(self, path: str) -> str`
  - Resolve dbfs:/ path to local filesystem path.
- **ensure_dir**`(self, path: str) -> None`
  - Create parent directories for given path.
- **get_path**`(self, relative_path: str) -> str`
  - Get local filesystem path for DBFS path.
- **validate**`(self) -> None`
  - Validate local DBFS configuration.

---

## Module `odibi.context`
### Functions
#### `create_context`
```python
def create_context(engine: str, spark_session: Optional[Any] = None) -> odibi.context.Context
```
> Factory function to create appropriate context.


### Classes
#### `class EngineContext`
The context passed to transformations.

- **__init__**`(self, context: 'Context', df: Any, engine_type: odibi.enums.EngineType, sql_executor: Optional[Any] = None)`
  - Initialize self.  See help(type(self)) for accurate signature.
- **with_df**`(self, df: Any) -> 'EngineContext'`
  - Returns a new context with updated DataFrame.
- **get**`(self, name: str) -> Any`
  - Get a dataset from global context.
- **register_temp_view**`(self, name: str, df: Any) -> None`
  - Register a temporary view for SQL.
- **sql**`(self, query: str) -> 'EngineContext'`
  - Execute SQL on the current DataFrame (aliased as 'df').

#### `class Context`
Abstract base for execution context.

- **register**`(self, name: str, df: Any) -> None`
  - Register a DataFrame for use in downstream nodes.
- **get**`(self, name: str) -> Any`
  - Retrieve a registered DataFrame.
- **has**`(self, name: str) -> bool`
  - Check if a DataFrame exists in context.
- **list_names**`(self) -> list[str]`
  - List all registered DataFrame names.
- **clear**`(self) -> None`
  - Clear all registered DataFrames.

#### `class PandasContext`
Context implementation for Pandas engine.

- **__init__**`(self) -> None`
  - Initialize Pandas context.
- **register**`(self, name: str, df: Union[pandas.core.frame.DataFrame, collections.abc.Iterator[pandas.core.frame.DataFrame]]) -> None`
  - Register a Pandas DataFrame or Iterator.
- **get**`(self, name: str) -> Union[pandas.core.frame.DataFrame, collections.abc.Iterator[pandas.core.frame.DataFrame]]`
  - Retrieve a registered Pandas DataFrame or Iterator.
- **has**`(self, name: str) -> bool`
  - Check if a DataFrame exists.
- **list_names**`(self) -> list[str]`
  - List all registered DataFrame names.
- **clear**`(self) -> None`
  - Clear all registered DataFrames.

#### `class SparkContext`
Context implementation for Spark engine.

- **__init__**`(self, spark_session: Any) -> None`
  - Initialize Spark context.
- **register**`(self, name: str, df: Any) -> None`
  - Register a Spark DataFrame as temp view.
- **get**`(self, name: str) -> Any`
  - Retrieve a registered Spark DataFrame.
- **has**`(self, name: str) -> bool`
  - Check if a DataFrame exists.
- **list_names**`(self) -> list[str]`
  - List all registered DataFrame names.
- **clear**`(self) -> None`
  - Clear all registered temp views.

---

## Module `odibi.diagnostics.delta`
Delta Lake Diagnostics

### Functions
#### `get_delta_diff`
```python
def get_delta_diff(table_path: str, version_a: int, version_b: int, spark: Optional[Any] = None, deep: bool = False, keys: Optional[List[str]] = None) -> odibi.diagnostics.delta.DeltaDiffResult
```
> Compare two versions of a Delta table.


#### `detect_drift`
```python
def detect_drift(table_path: str, current_version: int, baseline_version: int, spark: Optional[Any] = None, threshold_pct: float = 10.0) -> Optional[str]
```
> Check for significant drift between versions.


### Classes
#### `class DeltaDiffResult`
Result of comparing two Delta table versions.

- **__init__**`(self, table_path: str, version_a: int, version_b: int, rows_change: int, files_change: int, size_change_bytes: int, schema_added: List[str], schema_removed: List[str], schema_current: Optional[List[str]] = None, schema_previous: Optional[List[str]] = None, rows_added: Optional[int] = None, rows_removed: Optional[int] = None, rows_updated: Optional[int] = None, operations: List[str] = None, sample_added: Optional[List[Dict[str, Any]]] = None, sample_removed: Optional[List[Dict[str, Any]]] = None, sample_updated: Optional[List[Dict[str, Any]]] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.

---

## Module `odibi.diagnostics.diff`
ODIBI Diff Tools

### Functions
#### `diff_nodes`
```python
def diff_nodes(node_a: odibi.story.metadata.NodeExecutionMetadata, node_b: odibi.story.metadata.NodeExecutionMetadata) -> odibi.diagnostics.diff.NodeDiffResult
```
> Compare two executions of the same node.


#### `diff_runs`
```python
def diff_runs(run_a: odibi.story.metadata.PipelineStoryMetadata, run_b: odibi.story.metadata.PipelineStoryMetadata) -> odibi.diagnostics.diff.RunDiffResult
```
> Compare two pipeline runs node by node.


### Classes
#### `class NodeDiffResult`
Difference between two node executions.

- **__init__**`(self, node_name: str, status_change: Optional[str] = None, rows_out_a: int = 0, rows_out_b: int = 0, rows_diff: int = 0, schema_change: bool = False, columns_added: List[str] = <factory>, columns_removed: List[str] = <factory>, sql_changed: bool = False, config_changed: bool = False, transformation_changed: bool = False, delta_version_change: Optional[str] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class RunDiffResult`
Difference between two pipeline runs.

- **__init__**`(self, run_id_a: str, run_id_b: str, node_diffs: Dict[str, odibi.diagnostics.diff.NodeDiffResult] = <factory>, nodes_added: List[str] = <factory>, nodes_removed: List[str] = <factory>, drift_source_nodes: List[str] = <factory>, impacted_downstream_nodes: List[str] = <factory>) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.

---

## Module `odibi.diagnostics.manager`
Diagnostics Manager

### Classes
#### `class HistoryManager`
Manages access to pipeline run history.

- **__init__**`(self, history_path: str = 'stories/')`
  - Initialize history manager.
- **list_runs**`(self, pipeline_name: str) -> List[Dict[str, str]]`
  - List available runs for a pipeline.
- **get_latest_run**`(self, pipeline_name: str) -> Optional[odibi.story.metadata.PipelineStoryMetadata]`
  - Get the most recent run metadata.
- **get_run_by_id**`(self, pipeline_name: str, run_id: str) -> Optional[odibi.story.metadata.PipelineStoryMetadata]`
  - Get specific run metadata.
- **get_previous_run**`(self, pipeline_name: str, current_run_id: str) -> Optional[odibi.story.metadata.PipelineStoryMetadata]`
  - Get the run immediately preceding the specified one.
- **load_run**`(self, path: str) -> odibi.story.metadata.PipelineStoryMetadata`
  - Load run metadata from JSON file.

---

## Module `odibi.engine`
Engine implementations for ODIBI.

### Functions
#### `get_spark_engine`
```python
def get_spark_engine()
```

---

## Module `odibi.engine.base`
Base engine interface.

### Classes
#### `class Engine`
Abstract base class for execution engines.

- **register_format**`(fmt: str, reader: Optional[Any] = None, writer: Optional[Any] = None)`
  - Register custom format reader/writer.
- **read**`(self, connection: Any, format: str, table: Optional[str] = None, path: Optional[str] = None, options: Optional[Dict[str, Any]] = None) -> Any`
  - Read data from source.
- **write**`(self, df: Any, connection: Any, format: str, table: Optional[str] = None, path: Optional[str] = None, mode: str = 'overwrite', options: Optional[Dict[str, Any]] = None) -> None`
  - Write data to destination.
- **execute_sql**`(self, sql: str, context: odibi.context.Context) -> Any`
  - Execute SQL query.
- **execute_operation**`(self, operation: str, params: Dict[str, Any], df: Any) -> Any`
  - Execute built-in operation (pivot, etc.).
- **get_schema**`(self, df: Any) -> Any`
  - Get DataFrame schema.
- **get_shape**`(self, df: Any) -> tuple`
  - Get DataFrame shape.
- **count_rows**`(self, df: Any) -> int`
  - Count rows in DataFrame.
- **count_nulls**`(self, df: Any, columns: List[str]) -> Dict[str, int]`
  - Count nulls in specified columns.
- **validate_schema**`(self, df: Any, schema_rules: Dict[str, Any]) -> List[str]`
  - Validate DataFrame schema.
- **validate_data**`(self, df: Any, validation_config: Any) -> List[str]`
  - Validate data against rules.
- **get_sample**`(self, df: Any, n: int = 10) -> List[Dict[str, Any]]`
  - Get sample rows as list of dictionaries.
- **get_source_files**`(self, df: Any) -> List[str]`
  - Get list of source files that generated this DataFrame.
- **profile_nulls**`(self, df: Any) -> Dict[str, float]`
  - Calculate null percentage for each column.
- **table_exists**`(self, connection: Any, table: Optional[str] = None, path: Optional[str] = None) -> bool`
  - Check if table or location exists.

---

## Module `odibi.engine.pandas_engine`
Pandas engine implementation.

### Classes
#### `class PandasEngine`
Pandas-based execution engine.

- **__init__**`(self, connections: Optional[Dict[str, Any]] = None, config: Optional[Dict[str, Any]] = None)`
  - Initialize Pandas engine.
- **read**`(self, connection: Any, format: str, table: Optional[str] = None, path: Optional[str] = None, streaming: bool = False, options: Optional[Dict[str, Any]] = None) -> Union[pandas.core.frame.DataFrame, Iterator[pandas.core.frame.DataFrame]]`
  - Read data using Pandas.
- **write**`(self, df: Union[pandas.core.frame.DataFrame, Iterator[pandas.core.frame.DataFrame]], connection: Any, format: str, table: Optional[str] = None, path: Optional[str] = None, register_table: Optional[str] = None, mode: str = 'overwrite', options: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]`
  - Write data using Pandas.
- **execute_sql**`(self, sql: str, context: odibi.context.Context) -> pandas.core.frame.DataFrame`
  - Execute SQL query using DuckDB (if available) or pandasql.
- **execute_operation**`(self, operation: str, params: Dict[str, Any], df: Union[pandas.core.frame.DataFrame, Iterator[pandas.core.frame.DataFrame]]) -> pandas.core.frame.DataFrame`
  - Execute built-in operation.
- **get_schema**`(self, df: pandas.core.frame.DataFrame) -> Dict[str, str]`
  - Get DataFrame schema with types.
- **get_shape**`(self, df: pandas.core.frame.DataFrame) -> tuple`
  - Get DataFrame shape.
- **count_rows**`(self, df: pandas.core.frame.DataFrame) -> int`
  - Count rows in DataFrame.
- **count_nulls**`(self, df: pandas.core.frame.DataFrame, columns: List[str]) -> Dict[str, int]`
  - Count nulls in specified columns.
- **validate_schema**`(self, df: pandas.core.frame.DataFrame, schema_rules: Dict[str, Any]) -> List[str]`
  - Validate DataFrame schema.
- **validate_data**`(self, df: pandas.core.frame.DataFrame, validation_config: Any) -> List[str]`
  - Validate DataFrame against rules.
- **get_sample**`(self, df: pandas.core.frame.DataFrame, n: int = 10) -> List[Dict[str, Any]]`
  - Get sample rows as list of dictionaries.
- **table_exists**`(self, connection: Any, table: Optional[str] = None, path: Optional[str] = None) -> bool`
  - Check if table or location exists.
- **vacuum_delta**`(self, connection: Any, path: str, retention_hours: int = 168, dry_run: bool = False, enforce_retention_duration: bool = True) -> Dict[str, Any]`
  - VACUUM a Delta table to remove old files.
- **get_delta_history**`(self, connection: Any, path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]`
  - Get Delta table history.
- **restore_delta**`(self, connection: Any, path: str, version: int) -> None`
  - Restore Delta table to a specific version.
- **get_source_files**`(self, df: pandas.core.frame.DataFrame) -> List[str]`
  - Get list of source files that generated this DataFrame.
- **profile_nulls**`(self, df: pandas.core.frame.DataFrame) -> Dict[str, float]`
  - Calculate null percentage for each column.

---

## Module `odibi.engine.registry`
Engine registry for dynamic engine loading.

### Functions
#### `register_engine`
```python
def register_engine(name: str, engine_cls: Type[odibi.engine.base.Engine]) -> None
```
> Register a new engine class.


#### `get_engine_class`
```python
def get_engine_class(name: str) -> Type[odibi.engine.base.Engine]
```
> Get engine class by name.


---

## Module `odibi.engine.spark_engine`
Spark execution engine (Phase 2B: Delta Lake support).

### Classes
#### `class SparkEngine`
Spark execution engine with PySpark backend.

- **__init__**`(self, connections: Optional[Dict[str, Any]] = None, spark_session=None, config: Optional[Dict[str, Any]] = None)`
  - Initialize Spark engine with import guard.
- **get_schema**`(self, df) -> Dict[str, str]`
  - Get DataFrame schema with types.
- **get_shape**`(self, df) -> Tuple[int, int]`
  - Get DataFrame shape as (rows, columns).
- **count_rows**`(self, df) -> int`
  - Count rows in DataFrame.
- **read**`(self, connection: Any, format: str, table: Optional[str] = None, path: Optional[str] = None, streaming: bool = False, options: Optional[Dict[str, Any]] = None)`
  - Read data using Spark.
- **write**`(self, df, connection: Any, format: str, table: Optional[str] = None, path: Optional[str] = None, register_table: Optional[str] = None, mode: str = 'overwrite', options: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]`
  - Write data using Spark.
- **execute_sql**`(self, sql: str, context) -> Any`
  - Execute SQL query using Spark SQL.
- **execute_transform**`(self, *args, **kwargs)`
- **execute_operation**`(self, operation: str, params: Dict[str, Any], df) -> Any`
  - Execute built-in operation on Spark DataFrame.
- **count_nulls**`(self, df, columns: List[str]) -> Dict[str, int]`
  - Count nulls in specified columns.
- **validate_schema**`(self, df, schema_rules: Dict[str, Any]) -> List[str]`
  - Validate DataFrame schema.
- **validate_data**`(self, df, validation_config: Any) -> List[str]`
  - Validate DataFrame against rules.
- **get_sample**`(self, df, n: int = 10) -> List[Dict[str, Any]]`
  - Get sample rows as list of dictionaries.
- **table_exists**`(self, connection: Any, table: Optional[str] = None, path: Optional[str] = None) -> bool`
  - Check if table or location exists.
- **vacuum_delta**`(self, connection: Any, path: str, retention_hours: int = 168) -> None`
  - VACUUM a Delta table to remove old files.
- **get_delta_history**`(self, connection: Any, path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]`
  - Get Delta table history.
- **restore_delta**`(self, connection: Any, path: str, version: int) -> None`
  - Restore Delta table to a specific version.
- **get_source_files**`(self, df) -> List[str]`
  - Get list of source files that generated this DataFrame.
- **profile_nulls**`(self, df) -> Dict[str, float]`
  - Calculate null percentage for each column.

---

## Module `odibi.enums`
### Classes
#### `class EngineType`
str(object='') -> str


---

## Module `odibi.exceptions`
Custom exceptions for ODIBI framework.

### Classes
#### `class OdibiException`
Base exception for all ODIBI errors.


#### `class ConfigValidationError`
Configuration validation failed.

- **__init__**`(self, message: str, file: Optional[str] = None, line: Optional[int] = None)`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class ConnectionError`
Connection failed or invalid.

- **__init__**`(self, connection_name: str, reason: str, suggestions: Optional[List[str]] = None)`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class DependencyError`
Dependency graph error (cycles, missing nodes, etc.).

- **__init__**`(self, message: str, cycle: Optional[List[str]] = None)`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class ExecutionContext`
Runtime context for error reporting.

- **__init__**`(self, node_name: str, config_file: Optional[str] = None, config_line: Optional[int] = None, step_index: Optional[int] = None, total_steps: Optional[int] = None, input_schema: Optional[List[str]] = None, input_shape: Optional[tuple] = None, previous_steps: Optional[List[str]] = None)`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class NodeExecutionError`
Node execution failed.

- **__init__**`(self, message: str, context: odibi.exceptions.ExecutionContext, original_error: Optional[Exception] = None, suggestions: Optional[List[str]] = None, story_path: Optional[str] = None)`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class TransformError`
Transform step failed.


#### `class ValidationError`
Data validation failed.

- **__init__**`(self, node_name: str, failures: List[str])`
  - Initialize self.  See help(type(self)) for accurate signature.

---

## Module `odibi.graph`
Dependency graph builder and analyzer.

### Classes
#### `class DependencyGraph`
Builds and analyzes dependency graph from node configurations.

- **__init__**`(self, nodes: List[odibi.config.NodeConfig])`
  - Initialize dependency graph.
- **topological_sort**`(self) -> List[str]`
  - Return nodes in topological order (dependencies first).
- **get_execution_layers**`(self) -> List[List[str]]`
  - Group nodes into execution layers for parallel execution.
- **get_dependencies**`(self, node_name: str) -> Set[str]`
  - Get all dependencies (direct and transitive) for a node.
- **get_dependents**`(self, node_name: str) -> Set[str]`
  - Get all dependents (direct and transitive) for a node.
- **get_independent_nodes**`(self) -> List[str]`
  - Get nodes that have no dependencies.
- **visualize**`(self) -> str`
  - Generate a text visualization of the graph.

---

## Module `odibi.introspect`
Introspection tool for generating API documentation.

### Functions
#### `get_summary`
```python
def get_summary(obj: Any) -> Optional[str]
```
> Extract the first line of the docstring.


#### `format_type_hint`
```python
def format_type_hint(annotation: Any) -> str
```
> Format a type hint as a string.


#### `get_parameters`
```python
def get_parameters(func: Any) -> List[odibi.introspect.ParameterDoc]
```
> Extract parameters from a function.


#### `scan_module`
```python
def scan_module(module_name: str) -> Optional[odibi.introspect.ModuleDoc]
```
> Introspect a single module.


#### `scan_package`
```python
def scan_package(package_name: str) -> List[odibi.introspect.ModuleDoc]
```
> Recursively scan a package.


#### `render_markdown`
```python
def render_markdown(modules: List[odibi.introspect.ModuleDoc]) -> str
```
> Render documentation to Markdown.


#### `generate_docs`
```python
def generate_docs(output_path: str = 'docs/api.md')
```
> Run introspection and save to file.


### Classes
#### `class ParameterDoc`
!!! abstract "Usage Documentation"


#### `class FunctionDoc`
!!! abstract "Usage Documentation"


#### `class ClassDoc`
!!! abstract "Usage Documentation"


#### `class ModuleDoc`
!!! abstract "Usage Documentation"


---

## Module `odibi.node`
Node execution engine.

### Classes
#### `class NodeResult`
Result of node execution.


#### `class Node`
Base node execution logic.

- **__init__**`(self, config: odibi.config.NodeConfig, context: odibi.context.Context, engine: Any, connections: Dict[str, Any], config_file: Optional[str] = None, max_sample_rows: int = 10, dry_run: bool = False, retry_config: Optional[odibi.config.RetryConfig] = None)`
  - Initialize node.
- **restore**`(self) -> bool`
  - Restore node state from previous execution (if persisted).
- **execute**`(self) -> odibi.node.NodeResult`
  - Execute the node with telemetry and retry logic.

---

## Module `odibi.pipeline`
Pipeline executor and orchestration.

### Classes
#### `class PipelineResults`
Results from pipeline execution.

- **__init__**`(self, pipeline_name: str, completed: List[str] = <factory>, failed: List[str] = <factory>, skipped: List[str] = <factory>, node_results: Dict[str, odibi.node.NodeResult] = <factory>, duration: float = 0.0, start_time: Optional[str] = None, end_time: Optional[str] = None, story_path: Optional[str] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.
- **get_node_result**`(self, name: str) -> Optional[odibi.node.NodeResult]`
  - Get result for specific node.
- **to_dict**`(self) -> Dict[str, Any]`
  - Convert to dictionary.

#### `class Pipeline`
Pipeline executor and orchestrator.

- **__init__**`(self, pipeline_config: odibi.config.PipelineConfig, engine: str = 'pandas', connections: Optional[Dict[str, Any]] = None, generate_story: bool = True, story_config: Optional[Dict[str, Any]] = None, retry_config: Optional[odibi.config.RetryConfig] = None, alerts: Optional[List[odibi.config.AlertConfig]] = None, performance_config: Optional[Any] = None)`
  - Initialize pipeline.
- **from_yaml**`(yaml_path: str) -> 'PipelineManager'`
  - Create PipelineManager from YAML file (recommended).
- **run**`(self, parallel: bool = False, dry_run: bool = False, resume_from_failure: bool = False, max_workers: int = 4) -> odibi.pipeline.PipelineResults`
  - Execute the pipeline.
- **run_node**`(self, node_name: str, mock_data: Optional[Dict[str, Any]] = None) -> odibi.node.NodeResult`
  - Execute a single node (for testing/debugging).
- **validate**`(self) -> Dict[str, Any]`
  - Validate pipeline without executing.
- **get_execution_layers**`(self) -> List[List[str]]`
  - Get nodes grouped by execution layers.
- **visualize**`(self) -> str`
  - Get text visualization of pipeline.

#### `class PipelineManager`
Manages multiple pipelines from a YAML configuration.

- **__init__**`(self, project_config: odibi.config.ProjectConfig, connections: Dict[str, Any])`
  - Initialize pipeline manager.
- **from_yaml**`(yaml_path: str, env: str = None) -> 'PipelineManager'`
  - Create PipelineManager from YAML file.
- **run**`(self, pipelines: Union[str, List[str], NoneType] = None, dry_run: bool = False, resume_from_failure: bool = False) -> Union[odibi.pipeline.PipelineResults, Dict[str, odibi.pipeline.PipelineResults]]`
  - Run one, multiple, or all pipelines.
- **list_pipelines**`(self) -> List[str]`
  - Get list of available pipeline names.
- **get_pipeline**`(self, name: str) -> odibi.pipeline.Pipeline`
  - Get a specific pipeline instance.

---

## Module `odibi.plugins`
Plugin system for Odibi.

### Functions
#### `register_connection_factory`
```python
def register_connection_factory(type_name: str, factory: Any)
```
> Register a connection factory.


#### `get_connection_factory`
```python
def get_connection_factory(type_name: str) -> Optional[Any]
```
> Get a registered connection factory.


#### `load_plugins`
```python
def load_plugins()
```
> Load plugins from entry points.


---

## Module `odibi.registry`
Function registry for transform functions.

### Functions
#### `transform`
```python
def transform(name_or_func: Union[str, Callable] = None, **kwargs) -> Callable
```
> Decorator to register a transform function.


#### `get_registered_function`
```python
def get_registered_function(name: str) -> Callable
```
> Get a registered transform function.


#### `validate_function_params`
```python
def validate_function_params(name: str, params: Dict[str, Any]) -> None
```
> Validate parameters for a registered function.


### Classes
#### `class FunctionRegistry`
Global registry of transform functions with type validation.

- **register**`(func: Callable, name: str = None, param_model: Any = None) -> Callable`
  - Register a transform function.
- **get**`(name: str) -> Callable`
  - Retrieve a registered function.
- **validate_params**`(name: str, params: Dict[str, Any]) -> None`
  - Validate parameters against function signature or Pydantic model.
- **list_functions**`() -> list[str]`
  - List all registered function names.
- **get_function_info**`(name: str) -> Dict[str, Any]`
  - Get detailed information about a registered function.

---

## Module `odibi.state`
### Classes
#### `class StateManager`
Manages execution state for checkpointing.

- **__init__**`(self, project_root: str = '.')`
  - Initialize self.  See help(type(self)) for accurate signature.
- **save_pipeline_run**`(self, pipeline_name: str, results: Any)`
  - Save pipeline run results with concurrency locking.
- **get_last_run_status**`(self, pipeline_name: str, node_name: str) -> Optional[bool]`
  - Get success status of a node from last run.

---

## Module `odibi.story.doc_story`
Documentation Story Generator

### Classes
#### `class DocStoryGenerator`
Generates documentation stories for pipelines.

- **__init__**`(self, pipeline_config: odibi.config.PipelineConfig, project_config: Optional[odibi.config.ProjectConfig] = None)`
  - Initialize doc story generator.
- **generate**`(self, output_path: str, format: str = 'html', validate: bool = True, include_flow_diagram: bool = True, theme=None) -> str`
  - Generate documentation story.

---

## Module `odibi.story.generator`
Story generator for pipeline execution documentation.

### Functions
#### `multiline_presenter`
```python
def multiline_presenter(dumper, data)
```
> YAML representer for MultilineString.


### Classes
#### `class MultilineString`
String subclass to force YAML block scalar style.


#### `class StoryGenerator`
Generates markdown documentation of pipeline execution.

- **__init__**`(self, pipeline_name: str, max_sample_rows: int = 10, output_path: str = 'stories/', retention_days: int = 30, retention_count: int = 100, storage_options: Optional[Dict[str, Any]] = None)`
  - Initialize story generator.
- **generate**`(self, node_results: Dict[str, odibi.node.NodeResult], completed: List[str], failed: List[str], skipped: List[str], duration: float, start_time: str, end_time: str, context: Any = None, config: Optional[Dict[str, Any]] = None) -> str`
  - Generate story HTML and JSON.
- **cleanup**`(self) -> None`
  - Remove old stories based on retention policy.

---

## Module `odibi.story.metadata`
Story Metadata Tracking

### Classes
#### `class DeltaWriteInfo`
Metadata specific to Delta Lake writes.

- **__init__**`(self, version: int, timestamp: Optional[datetime.datetime] = None, operation: Optional[str] = None, operation_metrics: Dict[str, Any] = <factory>, read_version: Optional[int] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class NodeExecutionMetadata`
Metadata for a single node execution.

- **__init__**`(self, node_name: str, operation: str, status: str, duration: float, rows_in: Optional[int] = None, rows_out: Optional[int] = None, rows_change: Optional[int] = None, rows_change_pct: Optional[float] = None, sample_in: Optional[List[Dict[str, Any]]] = None, sample_data: Optional[List[Dict[str, Any]]] = None, schema_in: Optional[List[str]] = None, schema_out: Optional[List[str]] = None, columns_added: List[str] = <factory>, columns_removed: List[str] = <factory>, columns_renamed: List[str] = <factory>, executed_sql: List[str] = <factory>, sql_hash: Optional[str] = None, transformation_stack: List[str] = <factory>, config_snapshot: Optional[Dict[str, Any]] = None, delta_info: Optional[odibi.story.metadata.DeltaWriteInfo] = None, data_diff: Optional[Dict[str, Any]] = None, environment: Optional[Dict[str, Any]] = None, source_files: List[str] = <factory>, null_profile: Optional[Dict[str, float]] = None, error_message: Optional[str] = None, error_type: Optional[str] = None, started_at: Optional[str] = None, completed_at: Optional[str] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.
- **calculate_row_change**`(self)`
  - Calculate row count change metrics.
- **calculate_schema_changes**`(self)`
  - Calculate schema changes between input and output.
- **to_dict**`(self) -> Dict[str, Any]`
  - Convert to dictionary.
- **from_dict**`(data: Dict[str, Any]) -> 'NodeExecutionMetadata'`
  - Create instance from dictionary.

#### `class PipelineStoryMetadata`
Complete metadata for a pipeline run story.

- **__init__**`(self, pipeline_name: str, pipeline_layer: Optional[str] = None, run_id: str = <factory>, started_at: str = <factory>, completed_at: Optional[str] = None, duration: float = 0.0, total_nodes: int = 0, completed_nodes: int = 0, failed_nodes: int = 0, skipped_nodes: int = 0, nodes: List[odibi.story.metadata.NodeExecutionMetadata] = <factory>, project: Optional[str] = None, plant: Optional[str] = None, asset: Optional[str] = None, business_unit: Optional[str] = None, theme: str = 'default', include_samples: bool = True, max_sample_rows: int = 10) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.
- **add_node**`(self, node_metadata: odibi.story.metadata.NodeExecutionMetadata)`
  - Add node execution metadata.
- **get_success_rate**`(self) -> float`
  - Calculate success rate as percentage.
- **get_total_rows_processed**`(self) -> int`
  - Calculate total rows processed across all nodes.
- **to_dict**`(self) -> Dict[str, Any]`
  - Convert to dictionary.
- **from_dict**`(data: Dict[str, Any]) -> 'PipelineStoryMetadata'`
  - Create instance from dictionary.
- **from_json**`(path: str) -> 'PipelineStoryMetadata'`
  - Load from a JSON file.

---

## Module `odibi.story.renderers`
Story Renderers

### Functions
#### `get_renderer`
```python
def get_renderer(format: str)
```
> Get renderer for specified format.


### Classes
#### `class HTMLStoryRenderer`
Renders pipeline stories as HTML.

- **__init__**`(self, template_path: Optional[str] = None, theme=None)`
  - Initialize HTML renderer.
- **render**`(self, metadata: odibi.story.metadata.PipelineStoryMetadata) -> str`
  - Render story as HTML.
- **render_to_file**`(self, metadata: odibi.story.metadata.PipelineStoryMetadata, output_path: str) -> str`
  - Render story and save to file.

#### `class MarkdownStoryRenderer`
Renders pipeline stories as Markdown.

- **render**`(self, metadata: odibi.story.metadata.PipelineStoryMetadata) -> str`
  - Render story as Markdown.
- **render_to_file**`(self, metadata: odibi.story.metadata.PipelineStoryMetadata, output_path: str) -> str`
  - Render story and save to file.

#### `class JSONStoryRenderer`
Renders pipeline stories as JSON.

- **render**`(self, metadata: odibi.story.metadata.PipelineStoryMetadata) -> str`
  - Render story as JSON.
- **render_to_file**`(self, metadata: odibi.story.metadata.PipelineStoryMetadata, output_path: str) -> str`
  - Render story and save to file.

---

## Module `odibi.story.themes`
Theme System

### Functions
#### `get_theme`
```python
def get_theme(name: str) -> odibi.story.themes.StoryTheme
```
> Get theme by name.


#### `list_themes`
```python
def list_themes() -> Dict[str, odibi.story.themes.StoryTheme]
```
> List all available built-in themes.


### Classes
#### `class StoryTheme`
Story theme configuration.

- **__init__**`(self, name: str, primary_color: str = '#0066cc', success_color: str = '#28a745', error_color: str = '#dc3545', warning_color: str = '#ffc107', bg_color: str = '#ffffff', text_color: str = '#333333', border_color: str = '#dddddd', code_bg: str = '#f5f5f5', font_family: str = "system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif", heading_font: str = 'inherit', code_font: str = "Consolas, Monaco, 'Courier New', monospace", font_size: str = '16px', logo_url: Optional[str] = None, company_name: Optional[str] = None, footer_text: Optional[str] = None, max_width: str = '1200px', sidebar: bool = False, custom_css: Optional[str] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.
- **to_css_vars**`(self) -> Dict[str, str]`
  - Convert theme to CSS variables.
- **to_css_string**`(self) -> str`
  - Generate CSS string from theme.
- **from_dict**`(data: Dict[str, Any]) -> 'StoryTheme'`
  - Create theme from dictionary.
- **from_yaml**`(path: str) -> 'StoryTheme'`
  - Load theme from YAML file.

---

## Module `odibi.testing.assertions`
Testing Assertions

### Functions
#### `assert_frame_equal`
```python
def assert_frame_equal(left: Any, right: Any, check_dtype: bool = True, check_exact: bool = False, atol: float = 1e-08, rtol: float = 1e-05) -> None
```
> Assert that two DataFrames are equal.


#### `assert_schema_equal`
```python
def assert_schema_equal(left: Any, right: Any) -> None
```
> Assert that two DataFrames have the same schema (column names and types).


---

## Module `odibi.testing.fixtures`
Testing Fixtures

### Functions
#### `temp_directory`
```python
def temp_directory() -> Generator[str, NoneType, NoneType]
```
> Create a temporary directory for test artifacts.


#### `generate_sample_data`
```python
def generate_sample_data(rows: int = 10, engine_type: str = 'pandas', schema: Optional[Dict[str, str]] = None) -> Any
```
> Generate a sample DataFrame (Pandas or Spark).


---

## Module `odibi.transformers`
### Functions
#### `register_standard_library`
```python
def register_standard_library()
```
> Registers all standard transformations into the global registry.


---

## Module `odibi.transformers.advanced`
### Functions
#### `deduplicate`
```python
def deduplicate(context: odibi.context.EngineContext, params: odibi.transformers.advanced.DeduplicateParams) -> odibi.context.EngineContext
```
> Deduplicates data using Window functions.


#### `explode_list_column`
```python
def explode_list_column(context: odibi.context.EngineContext, params: odibi.transformers.advanced.ExplodeParams) -> odibi.context.EngineContext
```

#### `dict_based_mapping`
```python
def dict_based_mapping(context: odibi.context.EngineContext, params: odibi.transformers.advanced.DictMappingParams) -> odibi.context.EngineContext
```

#### `regex_replace`
```python
def regex_replace(context: odibi.context.EngineContext, params: odibi.transformers.advanced.RegexReplaceParams) -> odibi.context.EngineContext
```
> SQL-based Regex replacement.


#### `unpack_struct`
```python
def unpack_struct(context: odibi.context.EngineContext, params: odibi.transformers.advanced.UnpackStructParams) -> odibi.context.EngineContext
```
> Flattens a struct/dict column into top-level columns.


#### `hash_columns`
```python
def hash_columns(context: odibi.context.EngineContext, params: odibi.transformers.advanced.HashParams) -> odibi.context.EngineContext
```
> Hashes columns for PII/Anonymization.


#### `generate_surrogate_key`
```python
def generate_surrogate_key(context: odibi.context.EngineContext, params: odibi.transformers.advanced.SurrogateKeyParams) -> odibi.context.EngineContext
```
> Generates a deterministic surrogate key (MD5) from a combination of columns.


#### `parse_json`
```python
def parse_json(context: odibi.context.EngineContext, params: odibi.transformers.advanced.ParseJsonParams) -> odibi.context.EngineContext
```
> Parses a JSON string column into a Struct/Map column.


#### `validate_and_flag`
```python
def validate_and_flag(context: odibi.context.EngineContext, params: odibi.transformers.advanced.ValidateAndFlagParams) -> odibi.context.EngineContext
```
> Validates rules and appends a column with a list/string of failed rule names.


#### `window_calculation`
```python
def window_calculation(context: odibi.context.EngineContext, params: odibi.transformers.advanced.WindowCalculationParams) -> odibi.context.EngineContext
```
> Generic wrapper for Window functions.


### Classes
#### `class DeduplicateParams`
!!! abstract "Usage Documentation"


#### `class ExplodeParams`
!!! abstract "Usage Documentation"


#### `class DictMappingParams`
!!! abstract "Usage Documentation"


#### `class RegexReplaceParams`
!!! abstract "Usage Documentation"


#### `class UnpackStructParams`
!!! abstract "Usage Documentation"


#### `class HashParams`
!!! abstract "Usage Documentation"


#### `class SurrogateKeyParams`
!!! abstract "Usage Documentation"


#### `class ParseJsonParams`
!!! abstract "Usage Documentation"


#### `class ValidateAndFlagParams`
!!! abstract "Usage Documentation"


#### `class WindowCalculationParams`
!!! abstract "Usage Documentation"


---

## Module `odibi.transformers.merge_transformer`
### Functions
#### `merge`
```python
def merge(context, current, **params)
```
> Merge transformer implementation.


---

## Module `odibi.transformers.relational`
### Functions
#### `join`
```python
def join(context: odibi.context.EngineContext, params: odibi.transformers.relational.JoinParams) -> odibi.context.EngineContext
```
> Joins the current dataset with another dataset from the context.


#### `union`
```python
def union(context: odibi.context.EngineContext, params: odibi.transformers.relational.UnionParams) -> odibi.context.EngineContext
```
> Unions current dataset with others.


#### `pivot`
```python
def pivot(context: odibi.context.EngineContext, params: odibi.transformers.relational.PivotParams) -> odibi.context.EngineContext
```
> Pivots row values into columns.


#### `unpivot`
```python
def unpivot(context: odibi.context.EngineContext, params: odibi.transformers.relational.UnpivotParams) -> odibi.context.EngineContext
```
> Unpivots columns into rows (Melt/Stack).


#### `aggregate`
```python
def aggregate(context: odibi.context.EngineContext, params: odibi.transformers.relational.AggregateParams) -> odibi.context.EngineContext
```
> Performs grouping and aggregation via SQL.


### Classes
#### `class JoinParams`
!!! abstract "Usage Documentation"


#### `class UnionParams`
!!! abstract "Usage Documentation"


#### `class PivotParams`
!!! abstract "Usage Documentation"


#### `class UnpivotParams`
!!! abstract "Usage Documentation"


#### `class AggregateParams`
!!! abstract "Usage Documentation"


---

## Module `odibi.transformers.sql_core`
### Functions
#### `filter_rows`
```python
def filter_rows(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.FilterRowsParams) -> odibi.context.EngineContext
```
> Filters rows using a standard SQL WHERE clause.


#### `derive_columns`
```python
def derive_columns(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.DeriveColumnsParams) -> odibi.context.EngineContext
```
> Appends new columns based on SQL expressions.


#### `cast_columns`
```python
def cast_columns(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.CastColumnsParams) -> odibi.context.EngineContext
```
> Casts specific columns to new types while keeping others intact.


#### `clean_text`
```python
def clean_text(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.CleanTextParams) -> odibi.context.EngineContext
```
> Applies string cleaning operations (Trim/Case) via SQL.


#### `extract_date_parts`
```python
def extract_date_parts(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.ExtractDateParams) -> odibi.context.EngineContext
```
> Extracts date parts using ANSI SQL extract/functions.


#### `normalize_schema`
```python
def normalize_schema(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.NormalizeSchemaParams) -> odibi.context.EngineContext
```
> Structural transformation to rename, drop, and reorder columns.


#### `sort`
```python
def sort(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.SortParams) -> odibi.context.EngineContext
```
> Sorts the dataset.


#### `limit`
```python
def limit(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.LimitParams) -> odibi.context.EngineContext
```
> Limits result size.


#### `sample`
```python
def sample(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.SampleParams) -> odibi.context.EngineContext
```
> Samples data using random filtering.


#### `distinct`
```python
def distinct(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.DistinctParams) -> odibi.context.EngineContext
```
> Returns unique rows (SELECT DISTINCT).


#### `fill_nulls`
```python
def fill_nulls(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.FillNullsParams) -> odibi.context.EngineContext
```
> Replaces null values with specified defaults using COALESCE.


#### `split_part`
```python
def split_part(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.SplitPartParams) -> odibi.context.EngineContext
```
> Extracts the Nth part of a string after splitting by a delimiter.


#### `date_add`
```python
def date_add(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.DateAddParams) -> odibi.context.EngineContext
```
> Adds an interval to a date/timestamp column.


#### `date_trunc`
```python
def date_trunc(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.DateTruncParams) -> odibi.context.EngineContext
```
> Truncates a date/timestamp to the specified precision.


#### `date_diff`
```python
def date_diff(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.DateDiffParams) -> odibi.context.EngineContext
```
> Calculates difference between two dates/timestamps.


#### `case_when`
```python
def case_when(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.CaseWhenParams) -> odibi.context.EngineContext
```
> Implements structured CASE WHEN logic.


#### `convert_timezone`
```python
def convert_timezone(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.ConvertTimezoneParams) -> odibi.context.EngineContext
```
> Converts a timestamp from one timezone to another.


#### `concat_columns`
```python
def concat_columns(context: odibi.context.EngineContext, params: odibi.transformers.sql_core.ConcatColumnsParams) -> odibi.context.EngineContext
```
> Concatenates multiple columns into one string.


### Classes
#### `class FilterRowsParams`
!!! abstract "Usage Documentation"


#### `class DeriveColumnsParams`
!!! abstract "Usage Documentation"


#### `class CastColumnsParams`
!!! abstract "Usage Documentation"


#### `class CleanTextParams`
!!! abstract "Usage Documentation"


#### `class ExtractDateParams`
!!! abstract "Usage Documentation"


#### `class NormalizeSchemaParams`
!!! abstract "Usage Documentation"


#### `class SortParams`
!!! abstract "Usage Documentation"


#### `class LimitParams`
!!! abstract "Usage Documentation"


#### `class SampleParams`
!!! abstract "Usage Documentation"


#### `class DistinctParams`
!!! abstract "Usage Documentation"


#### `class FillNullsParams`
!!! abstract "Usage Documentation"


#### `class SplitPartParams`
!!! abstract "Usage Documentation"


#### `class DateAddParams`
!!! abstract "Usage Documentation"


#### `class DateTruncParams`
!!! abstract "Usage Documentation"


#### `class DateDiffParams`
!!! abstract "Usage Documentation"


#### `class CaseWhenParams`
!!! abstract "Usage Documentation"


#### `class ConvertTimezoneParams`
!!! abstract "Usage Documentation"


#### `class ConcatColumnsParams`
!!! abstract "Usage Documentation"


---

## Module `odibi.utils.alerting`
Alerting utilities for notifications.

### Functions
#### `send_alert`
```python
def send_alert(config: odibi.config.AlertConfig, message: str, context: Dict[str, Any]) -> None
```
> Send alert to configured channel.


---

## Module `odibi.utils.config_loader`
### Functions
#### `load_yaml_with_env`
```python
def load_yaml_with_env(path: str, env: str = None) -> Dict[str, Any]
```
> Load YAML file with environment variable substitution and imports.


---

## Module `odibi.utils.encoding`
Encoding detection utilities.

### Functions
#### `detect_encoding`
```python
def detect_encoding(connection: Any, path: str, sample_bytes: int = 65536, candidates: Optional[List[str]] = None) -> Optional[str]
```
> Detect text encoding of a file.


---

## Module `odibi.utils.logging`
### Functions
#### `configure_logging`
```python
def configure_logging(structured: bool, level: str)
```
> Configure the global logger.


### Classes
#### `class StructuredLogger`
Logger that supports both human-readable and JSON output with secret redaction.

- **__init__**`(self, structured: bool = False, level: str = 'INFO')`
  - Initialize self.  See help(type(self)) for accurate signature.
- **register_secret**`(self, secret: str)`
  - Register a secret string to be redacted from logs.
- **info**`(self, message: str, **kwargs)`
- **warning**`(self, message: str, **kwargs)`
- **error**`(self, message: str, **kwargs)`
- **debug**`(self, message: str, **kwargs)`

---

## Module `odibi.utils.setup_helpers`
Setup helpers for ODIBI - Phase 2C performance utilities.

### Functions
#### `fetch_keyvault_secret`
```python
def fetch_keyvault_secret(connection_name: str, key_vault_name: str, secret_name: str, timeout: float = 30.0) -> odibi.utils.setup_helpers.KeyVaultFetchResult
```
> Fetch a single Key Vault secret with timeout protection.


#### `fetch_keyvault_secrets_parallel`
```python
def fetch_keyvault_secrets_parallel(connections: Dict[str, Any], max_workers: int = 5, timeout: float = 30.0, verbose: bool = True) -> Dict[str, odibi.utils.setup_helpers.KeyVaultFetchResult]
```
> Fetch Key Vault secrets in parallel for multiple connections.


#### `configure_connections_parallel`
```python
def configure_connections_parallel(connections: Dict[str, Any], prefetch_secrets: bool = True, max_workers: int = 5, timeout: float = 30.0, verbose: bool = True) -> Tuple[Dict[str, Any], List[str]]
```
> Configure connections with parallel Key Vault fetching.


#### `validate_databricks_environment`
```python
def validate_databricks_environment(verbose: bool = True) -> Dict[str, Any]
```
> Validate that we're running in a Databricks environment.


### Classes
#### `class KeyVaultFetchResult`
Result of a Key Vault secret fetch operation.

- **__init__**`(self, connection_name: str, account: str, success: bool, secret_value: Optional[str] = None, error: Optional[Exception] = None, duration_ms: Optional[float] = None) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.

---

## Module `odibi.utils.telemetry`
Telemetry utilities for OpenTelemetry integration.

### Functions
#### `get_tracer`
```python
def get_tracer(name: str)
```
> Get a tracer (real or mock).


#### `get_meter`
```python
def get_meter(name: str)
```
> Get a meter (real or mock).


#### `setup_telemetry`
```python
def setup_telemetry(service_name: str = 'odibi')
```
> Configure OpenTelemetry if available and configured.


### Classes
#### `class StatusCode`

#### `class Status`
- **__init__**`(self, status_code, description='')`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class MockSpan`
- **set_attribute**`(self, key, value)`
- **set_status**`(self, status)`
- **record_exception**`(self, exception)`
- **add_event**`(self, name, attributes=None)`

#### `class MockTracer`
- **start_as_current_span**`(self, name, kind=None, attributes=None)`

#### `class MockCounter`
- **add**`(self, amount, attributes=None)`

#### `class MockHistogram`
- **record**`(self, amount, attributes=None)`

#### `class MockMeter`
- **create_counter**`(self, name, unit='', description='')`
- **create_histogram**`(self, name, unit='', description='')`

---

## Module `odibi.validation.explanation_linter`
Explanation Quality Linter

### Classes
#### `class LintIssue`
A linting issue found in an explanation.

- **__init__**`(self, severity: str, message: str, rule: str) -> None`
  - Initialize self.  See help(type(self)) for accurate signature.

#### `class ExplanationLinter`
Lints explanation text for quality issues.

- **__init__**`(self)`
  - Initialize self.  See help(type(self)) for accurate signature.
- **lint**`(self, explanation: str, operation_name: str = 'unknown') -> List[odibi.validation.explanation_linter.LintIssue]`
  - Lint an explanation and return issues.
- **has_errors**`(self) -> bool`
  - Check if any errors were found.
- **format_issues**`(self) -> str`
  - Format all issues as string.

---
