{
  "$defs": {
    "AcceptedValuesTest": {
      "description": "Ensures a column only contains values from an allowed list.\n\n**When to Use:** Enum-like fields, status columns, categorical data validation.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: accepted_values\n    column: status\n    values: [pending, approved, rejected]\n```",
      "properties": {
        "type": {
          "const": "accepted_values",
          "default": "accepted_values",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "column": {
          "description": "Column to check",
          "title": "Column",
          "type": "string"
        },
        "values": {
          "description": "Allowed values",
          "items": {},
          "title": "Values",
          "type": "array"
        }
      },
      "required": [
        "column",
        "values"
      ],
      "title": "AcceptedValuesTest",
      "type": "object"
    },
    "AlertConfig": {
      "description": "Configuration for alerts with throttling support.\n\nSupports Slack, Teams, and generic webhooks with event-specific payloads.\n\n**Available Events:**\n- `on_start` - Pipeline started\n- `on_success` - Pipeline completed successfully\n- `on_failure` - Pipeline failed\n- `on_quarantine` - Rows were quarantined\n- `on_gate_block` - Quality gate blocked the pipeline\n- `on_threshold_breach` - A threshold was exceeded\n\nExample:\n```yaml\nalerts:\n  - type: slack\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events:\n      - on_failure\n      - on_quarantine\n      - on_gate_block\n    metadata:\n      throttle_minutes: 15\n      max_per_hour: 10\n      channel: \"#data-alerts\"\n```",
      "properties": {
        "type": {
          "$ref": "#/$defs/AlertType"
        },
        "url": {
          "description": "Webhook URL",
          "title": "Url",
          "type": "string"
        },
        "on_events": {
          "default": [
            "on_failure"
          ],
          "description": "Events to trigger alert: on_start, on_success, on_failure, on_quarantine, on_gate_block, on_threshold_breach",
          "items": {
            "$ref": "#/$defs/AlertEvent"
          },
          "title": "On Events",
          "type": "array"
        },
        "metadata": {
          "description": "Extra metadata: throttle_minutes, max_per_hour, channel, etc.",
          "title": "Metadata",
          "type": "object"
        }
      },
      "required": [
        "type",
        "url"
      ],
      "title": "AlertConfig",
      "type": "object"
    },
    "AlertEvent": {
      "description": "Events that trigger alerts.",
      "enum": [
        "on_start",
        "on_success",
        "on_failure",
        "on_quarantine",
        "on_gate_block",
        "on_threshold_breach"
      ],
      "title": "AlertEvent",
      "type": "string"
    },
    "AlertType": {
      "description": "Types of alerting channels.",
      "enum": [
        "webhook",
        "slack",
        "teams",
        "teams_workflow"
      ],
      "title": "AlertType",
      "type": "string"
    },
    "AutoOptimizeConfig": {
      "description": "Configuration for Delta Lake automatic optimization.\n\nExample:\n```yaml\nauto_optimize:\n  enabled: true\n  vacuum_retention_hours: 168\n```",
      "properties": {
        "enabled": {
          "default": true,
          "description": "Enable auto optimization",
          "title": "Enabled",
          "type": "boolean"
        },
        "vacuum_retention_hours": {
          "default": 168,
          "description": "Hours to retain history for VACUUM (default 7 days). Set to 0 to disable VACUUM.",
          "title": "Vacuum Retention Hours",
          "type": "integer"
        }
      },
      "title": "AutoOptimizeConfig",
      "type": "object"
    },
    "AzureBlobAccountKeyAuth": {
      "properties": {
        "mode": {
          "const": "account_key",
          "default": "account_key",
          "title": "Mode"
        },
        "account_key": {
          "title": "Account Key",
          "type": "string"
        }
      },
      "required": [
        "account_key"
      ],
      "title": "AzureBlobAccountKeyAuth",
      "type": "object"
    },
    "AzureBlobConnectionConfig": {
      "description": "Azure Blob Storage / ADLS Gen2 connection.\n\n**When to Use:** Azure-based data lakes, landing zones, raw data storage.\n\n**See Also:** [DeltaConnectionConfig](#deltaconnectionconfig) for Delta-specific options\n\nScenario 1: Prod with Key Vault-managed key\n```yaml\nadls_bronze:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"key_vault\"\n    key_vault: \"kv-data\"\n    secret: \"adls-account-key\"\n```\n\nScenario 2: Local dev with inline account key\n```yaml\nadls_dev:\n  type: \"azure_blob\"\n  account_name: \"devaccount\"\n  container: \"sandbox\"\n  auth:\n    mode: \"account_key\"\n    account_key: \"${ADLS_ACCOUNT_KEY}\"\n```\n\nScenario 3: MSI (no secrets)\n```yaml\nadls_msi:\n  type: \"azure_blob\"\n  account_name: \"myaccount\"\n  container: \"bronze\"\n  auth:\n    mode: \"aad_msi\"\n    # optional: client_id for user-assigned identity\n    client_id: \"00000000-0000-0000-0000-000000000000\"\n```",
      "properties": {
        "type": {
          "const": "azure_blob",
          "default": "azure_blob",
          "title": "Type"
        },
        "validation_mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationMode"
            }
          ],
          "default": "lazy"
        },
        "account_name": {
          "title": "Account Name",
          "type": "string"
        },
        "container": {
          "title": "Container",
          "type": "string"
        },
        "auth": {
          "discriminator": {
            "mapping": {
              "aad_msi": "#/$defs/AzureBlobMsiAuth",
              "account_key": "#/$defs/AzureBlobAccountKeyAuth",
              "connection_string": "#/$defs/AzureBlobConnectionStringAuth",
              "key_vault": "#/$defs/AzureBlobKeyVaultAuth",
              "sas": "#/$defs/AzureBlobSasAuth"
            },
            "propertyName": "mode"
          },
          "oneOf": [
            {
              "$ref": "#/$defs/AzureBlobKeyVaultAuth"
            },
            {
              "$ref": "#/$defs/AzureBlobAccountKeyAuth"
            },
            {
              "$ref": "#/$defs/AzureBlobSasAuth"
            },
            {
              "$ref": "#/$defs/AzureBlobConnectionStringAuth"
            },
            {
              "$ref": "#/$defs/AzureBlobMsiAuth"
            }
          ],
          "title": "Auth"
        }
      },
      "required": [
        "account_name",
        "container"
      ],
      "title": "AzureBlobConnectionConfig",
      "type": "object"
    },
    "AzureBlobConnectionStringAuth": {
      "properties": {
        "mode": {
          "const": "connection_string",
          "default": "connection_string",
          "title": "Mode"
        },
        "connection_string": {
          "title": "Connection String",
          "type": "string"
        }
      },
      "required": [
        "connection_string"
      ],
      "title": "AzureBlobConnectionStringAuth",
      "type": "object"
    },
    "AzureBlobKeyVaultAuth": {
      "properties": {
        "mode": {
          "const": "key_vault",
          "default": "key_vault",
          "title": "Mode"
        },
        "key_vault": {
          "title": "Key Vault",
          "type": "string"
        },
        "secret": {
          "title": "Secret",
          "type": "string"
        }
      },
      "required": [
        "key_vault",
        "secret"
      ],
      "title": "AzureBlobKeyVaultAuth",
      "type": "object"
    },
    "AzureBlobMsiAuth": {
      "properties": {
        "mode": {
          "const": "aad_msi",
          "default": "aad_msi",
          "title": "Mode"
        },
        "client_id": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Client Id"
        }
      },
      "title": "AzureBlobMsiAuth",
      "type": "object"
    },
    "AzureBlobSasAuth": {
      "properties": {
        "mode": {
          "const": "sas",
          "default": "sas",
          "title": "Mode"
        },
        "sas_token": {
          "title": "Sas Token",
          "type": "string"
        }
      },
      "required": [
        "sas_token"
      ],
      "title": "AzureBlobSasAuth",
      "type": "object"
    },
    "BackoffStrategy": {
      "enum": [
        "exponential",
        "linear",
        "constant"
      ],
      "title": "BackoffStrategy",
      "type": "string"
    },
    "ColumnMetadata": {
      "description": "Metadata for a column in the data dictionary.",
      "properties": {
        "description": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Column description",
          "title": "Description"
        },
        "pii": {
          "default": false,
          "description": "Contains PII?",
          "title": "Pii",
          "type": "boolean"
        },
        "tags": {
          "description": "Tags (e.g. 'business_key', 'measure')",
          "items": {
            "type": "string"
          },
          "title": "Tags",
          "type": "array"
        }
      },
      "title": "ColumnMetadata",
      "type": "object"
    },
    "ContractSeverity": {
      "enum": [
        "warn",
        "fail",
        "quarantine"
      ],
      "title": "ContractSeverity",
      "type": "string"
    },
    "CustomConnectionConfig": {
      "additionalProperties": true,
      "description": "Configuration for custom/plugin connections.\nAllows any fields.",
      "properties": {
        "type": {
          "title": "Type",
          "type": "string"
        },
        "validation_mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationMode"
            }
          ],
          "default": "lazy"
        }
      },
      "required": [
        "type"
      ],
      "title": "CustomConnectionConfig",
      "type": "object"
    },
    "CustomSQLTest": {
      "description": "Runs a custom SQL condition and fails if too many rows violate it.\n\n```yaml\ncontracts:\n  - type: custom_sql\n    condition: \"amount > 0\"\n    threshold: 0.01  # Allow up to 1% failures\n```",
      "properties": {
        "type": {
          "const": "custom_sql",
          "default": "custom_sql",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "condition": {
          "description": "SQL condition that should be true for valid rows",
          "title": "Condition",
          "type": "string"
        },
        "threshold": {
          "default": 0.0,
          "description": "Failure rate threshold (0.0 = strictly no failures allowed)",
          "title": "Threshold",
          "type": "number"
        }
      },
      "required": [
        "condition"
      ],
      "title": "CustomSQLTest",
      "type": "object"
    },
    "DeltaConnectionConfig": {
      "description": "Delta Lake connection for ACID-compliant data lakes.\n\n**When to Use:**\n- Production data lakes on Azure/AWS/GCP\n- Need time travel, ACID transactions, schema evolution\n- Upsert/merge operations\n\n**See Also:** [WriteConfig](#writeconfig) for Delta write options\n\nScenario 1: Delta via metastore\n```yaml\ndelta_silver:\n  type: \"delta\"\n  catalog: \"spark_catalog\"\n  schema: \"silver_db\"\n```\n\nScenario 2: Direct path + Node usage\n```yaml\ndelta_local:\n  type: \"local\"\n  base_path: \"dbfs:/mnt/delta\"\n\n# In pipeline:\n# read:\n#   connection: \"delta_local\"\n#   format: \"delta\"\n#   path: \"bronze/orders\"\n```",
      "properties": {
        "type": {
          "const": "delta",
          "default": "delta",
          "title": "Type"
        },
        "validation_mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationMode"
            }
          ],
          "default": "lazy"
        },
        "catalog": {
          "description": "Spark catalog name (e.g. 'spark_catalog')",
          "title": "Catalog",
          "type": "string"
        },
        "schema": {
          "description": "Database/schema name",
          "title": "Schema",
          "type": "string"
        },
        "table": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional default table name for this connection (used by story/pipeline helpers)",
          "title": "Table"
        }
      },
      "required": [
        "catalog",
        "schema"
      ],
      "title": "DeltaConnectionConfig",
      "type": "object"
    },
    "DistributionContract": {
      "description": "Checks if a column's statistical distribution is within expected bounds.\n\n**When to Use:** Detect data drift, anomaly detection, statistical monitoring.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: distribution\n    column: price\n    metric: mean\n    threshold: \">100\"  # Mean must be > 100\n    on_fail: warn\n```",
      "properties": {
        "type": {
          "const": "distribution",
          "default": "distribution",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "warn"
        },
        "column": {
          "description": "Column to analyze",
          "title": "Column",
          "type": "string"
        },
        "metric": {
          "description": "Statistical metric to check",
          "enum": [
            "mean",
            "min",
            "max",
            "null_percentage"
          ],
          "title": "Metric",
          "type": "string"
        },
        "threshold": {
          "description": "Threshold expression (e.g., '>100', '<0.05')",
          "title": "Threshold",
          "type": "string"
        }
      },
      "required": [
        "column",
        "metric",
        "threshold"
      ],
      "title": "DistributionContract",
      "type": "object"
    },
    "EngineType": {
      "description": "Supported execution engines.",
      "enum": [
        "spark",
        "pandas",
        "polars"
      ],
      "title": "EngineType",
      "type": "string"
    },
    "ErrorStrategy": {
      "description": "Strategy for handling node failures.",
      "enum": [
        "fail_fast",
        "fail_later",
        "ignore"
      ],
      "title": "ErrorStrategy",
      "type": "string"
    },
    "FreshnessContract": {
      "description": "Validates that data is not stale by checking a timestamp column.\n\n**When to Use:** Source systems that should update regularly, SLA monitoring.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: freshness\n    column: updated_at\n    max_age: \"24h\"  # Fail if no data newer than 24 hours\n```",
      "properties": {
        "type": {
          "const": "freshness",
          "default": "freshness",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail"
        },
        "column": {
          "default": "updated_at",
          "description": "Timestamp column to check",
          "title": "Column",
          "type": "string"
        },
        "max_age": {
          "description": "Maximum allowed age (e.g., '24h', '7d')",
          "title": "Max Age",
          "type": "string"
        }
      },
      "required": [
        "max_age"
      ],
      "title": "FreshnessContract",
      "type": "object"
    },
    "GateConfig": {
      "description": "Quality gate configuration for batch-level validation.\n\n**When to Use:** Pipeline-level pass/fail thresholds, row count limits, change detection.\n\n**See Also:** Quality Gates, [ValidationConfig](#validationconfig)\n\nGates evaluate the entire batch before writing, ensuring\ndata quality thresholds are met.\n\nExample:\n```yaml\ngate:\n  require_pass_rate: 0.95\n  on_fail: abort\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n  row_count:\n    min: 100\n    change_threshold: 0.5\n```",
      "properties": {
        "require_pass_rate": {
          "default": 0.95,
          "description": "Minimum percentage of rows passing ALL tests",
          "maximum": 1.0,
          "minimum": 0.0,
          "title": "Require Pass Rate",
          "type": "number"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/GateOnFail"
            }
          ],
          "default": "abort",
          "description": "Action when gate fails"
        },
        "thresholds": {
          "description": "Per-test thresholds (overrides global require_pass_rate)",
          "items": {
            "$ref": "#/$defs/GateThreshold"
          },
          "title": "Thresholds",
          "type": "array"
        },
        "row_count": {
          "anyOf": [
            {
              "$ref": "#/$defs/RowCountGate"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Row count anomaly detection"
        }
      },
      "title": "GateConfig",
      "type": "object"
    },
    "GateOnFail": {
      "description": "Action when quality gate fails.\n\nValues:\n* `abort` - Stop pipeline, write nothing (default)\n* `warn_and_write` - Log warning, write all rows anyway\n* `write_valid_only` - Write only rows that passed validation",
      "enum": [
        "abort",
        "warn_and_write",
        "write_valid_only"
      ],
      "title": "GateOnFail",
      "type": "string"
    },
    "GateThreshold": {
      "description": "Per-test threshold configuration for quality gates.\n\nAllows setting different pass rate requirements for specific tests.\n\nExample:\n```yaml\ngate:\n  thresholds:\n    - test: not_null\n      min_pass_rate: 0.99\n    - test: unique\n      min_pass_rate: 1.0\n```",
      "properties": {
        "test": {
          "description": "Test name or type to apply threshold to",
          "title": "Test",
          "type": "string"
        },
        "min_pass_rate": {
          "description": "Minimum pass rate required (0.0-1.0, e.g., 0.99 = 99%)",
          "maximum": 1.0,
          "minimum": 0.0,
          "title": "Min Pass Rate",
          "type": "number"
        }
      },
      "required": [
        "test",
        "min_pass_rate"
      ],
      "title": "GateThreshold",
      "type": "object"
    },
    "HttpApiKeyAuth": {
      "properties": {
        "mode": {
          "const": "api_key",
          "default": "api_key",
          "title": "Mode"
        },
        "header_name": {
          "default": "Authorization",
          "title": "Header Name",
          "type": "string"
        },
        "value_template": {
          "default": "Bearer {token}",
          "title": "Value Template",
          "type": "string"
        }
      },
      "title": "HttpApiKeyAuth",
      "type": "object"
    },
    "HttpBasicAuth": {
      "properties": {
        "mode": {
          "const": "basic",
          "default": "basic",
          "title": "Mode"
        },
        "username": {
          "title": "Username",
          "type": "string"
        },
        "password": {
          "title": "Password",
          "type": "string"
        }
      },
      "required": [
        "username",
        "password"
      ],
      "title": "HttpBasicAuth",
      "type": "object"
    },
    "HttpBearerAuth": {
      "properties": {
        "mode": {
          "const": "bearer",
          "default": "bearer",
          "title": "Mode"
        },
        "token": {
          "title": "Token",
          "type": "string"
        }
      },
      "required": [
        "token"
      ],
      "title": "HttpBearerAuth",
      "type": "object"
    },
    "HttpConnectionConfig": {
      "description": "HTTP connection.\n\nScenario: Bearer token via env var\n```yaml\napi_source:\n  type: \"http\"\n  base_url: \"https://api.example.com\"\n  headers:\n    User-Agent: \"odibi-pipeline\"\n  auth:\n    mode: \"bearer\"\n    token: \"${API_TOKEN}\"\n```",
      "properties": {
        "type": {
          "const": "http",
          "default": "http",
          "title": "Type"
        },
        "validation_mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationMode"
            }
          ],
          "default": "lazy"
        },
        "base_url": {
          "title": "Base Url",
          "type": "string"
        },
        "headers": {
          "additionalProperties": {
            "type": "string"
          },
          "title": "Headers",
          "type": "object"
        },
        "auth": {
          "discriminator": {
            "mapping": {
              "api_key": "#/$defs/HttpApiKeyAuth",
              "basic": "#/$defs/HttpBasicAuth",
              "bearer": "#/$defs/HttpBearerAuth",
              "none": "#/$defs/HttpNoAuth"
            },
            "propertyName": "mode"
          },
          "oneOf": [
            {
              "$ref": "#/$defs/HttpNoAuth"
            },
            {
              "$ref": "#/$defs/HttpBasicAuth"
            },
            {
              "$ref": "#/$defs/HttpBearerAuth"
            },
            {
              "$ref": "#/$defs/HttpApiKeyAuth"
            }
          ],
          "title": "Auth"
        }
      },
      "required": [
        "base_url"
      ],
      "title": "HttpConnectionConfig",
      "type": "object"
    },
    "HttpNoAuth": {
      "properties": {
        "mode": {
          "const": "none",
          "default": "none",
          "title": "Mode"
        }
      },
      "title": "HttpNoAuth",
      "type": "object"
    },
    "IncrementalConfig": {
      "description": "Configuration for automatic incremental loading.\n\n**When to Use:** Load only new/changed data instead of full table scans.\n\n**See Also:** [ReadConfig](#readconfig)\n\n**Modes:**\n1. **Rolling Window** (Default): Uses a time-based lookback from NOW().\n   Good for: Stateless loading where you just want \"recent\" data.\n   Args: `lookback`, `unit`\n\n2. **Stateful**: Tracks the High-Water Mark (HWM) of the key column.\n   Good for: Exact incremental ingestion (e.g. CDC-like).\n   Args: `state_key` (optional), `watermark_lag` (optional)\n\nGenerates SQL:\n- Rolling: `WHERE column >= NOW() - lookback`\n- Stateful: `WHERE column > :last_hwm`\n\nExample (Rolling Window):\n```yaml\nincremental:\n  mode: \"rolling_window\"\n  column: \"updated_at\"\n  lookback: 3\n  unit: \"day\"\n```\n\nExample (Stateful HWM):\n```yaml\nincremental:\n  mode: \"stateful\"\n  column: \"id\"\n  # Optional: track separate column for HWM state\n  state_key: \"last_processed_id\"\n```\n\nExample (Stateful with Watermark Lag):\n```yaml\nincremental:\n  mode: \"stateful\"\n  column: \"updated_at\"\n  # Handle late-arriving data: look back 2 hours from HWM\n  watermark_lag: \"2h\"\n```\n\nExample (Oracle Date Format):\n```yaml\nincremental:\n  mode: \"rolling_window\"\n  column: \"EVENT_START\"\n  lookback: 3\n  unit: \"day\"\n  # For string columns with Oracle format (DD-MON-YY)\n  date_format: \"oracle\"\n```\n\nSupported date_format values:\n- `oracle`: DD-MON-YY for Oracle databases (uses TO_TIMESTAMP)\n- `oracle_sqlserver`: DD-MON-YY format stored in SQL Server (uses TRY_CONVERT)\n- `sql_server`: Uses CONVERT with style 120\n- `us`: MM/DD/YYYY format\n- `eu`: DD/MM/YYYY format\n- `iso`: YYYY-MM-DDTHH:MM:SS format",
      "properties": {
        "mode": {
          "allOf": [
            {
              "$ref": "#/$defs/IncrementalMode"
            }
          ],
          "default": "rolling_window",
          "description": "Incremental strategy: 'rolling_window' or 'stateful'"
        },
        "key_column": {
          "description": "Primary column to filter on (e.g., updated_at)",
          "title": "Key Column",
          "type": "string"
        },
        "fallback_column": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Backup column if primary is NULL (e.g., created_at). Generates COALESCE(col, fallback) >= ...",
          "title": "Fallback Column"
        },
        "lookback": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Time units to look back (Rolling Window only)",
          "title": "Lookback"
        },
        "unit": {
          "anyOf": [
            {
              "$ref": "#/$defs/IncrementalUnit"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Time unit for lookback (Rolling Window only). Options: 'hour', 'day', 'month', 'year'"
        },
        "state_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Unique ID for state tracking. Defaults to node name if not provided.",
          "title": "State Key"
        },
        "watermark_lag": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Safety buffer for late-arriving data in stateful mode. Subtracts this duration from the stored HWM when filtering. Format: '<number><unit>' where unit is 's', 'm', 'h', or 'd'. Examples: '2h' (2 hours), '30m' (30 minutes), '1d' (1 day). Use when source has replication lag or eventual consistency.",
          "title": "Watermark Lag"
        },
        "date_format": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Source date format when the column is stored as a string. Options: 'oracle' (DD-MON-YY for Oracle DB), 'oracle_sqlserver' (DD-MON-YY format in SQL Server), 'sql_server' (uses CONVERT with style 120), 'us' (MM/DD/YYYY), 'eu' (DD/MM/YYYY), 'iso' (YYYY-MM-DDTHH:MM:SS). When set, SQL pushdown will use appropriate CONVERT/TO_TIMESTAMP functions.",
          "title": "Date Format"
        }
      },
      "required": [
        "key_column"
      ],
      "title": "IncrementalConfig",
      "type": "object"
    },
    "IncrementalMode": {
      "description": "Mode for incremental loading.",
      "enum": [
        "rolling_window",
        "stateful"
      ],
      "title": "IncrementalMode",
      "type": "string"
    },
    "IncrementalUnit": {
      "description": "Time units for incremental lookback.\n\nValues:\n* `hour`\n* `day`\n* `month`\n* `year`",
      "enum": [
        "hour",
        "day",
        "month",
        "year"
      ],
      "title": "IncrementalUnit",
      "type": "string"
    },
    "LineageConfig": {
      "description": "Configuration for OpenLineage integration.\n\nExample:\n```yaml\nlineage:\n  url: \"http://localhost:5000\"\n  namespace: \"my_project\"\n```",
      "properties": {
        "url": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "OpenLineage API URL",
          "title": "Url"
        },
        "namespace": {
          "default": "odibi",
          "description": "Namespace for jobs",
          "title": "Namespace",
          "type": "string"
        },
        "api_key": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "API Key",
          "title": "Api Key"
        }
      },
      "title": "LineageConfig",
      "type": "object"
    },
    "LocalConnectionConfig": {
      "description": "Local filesystem connection.\n\n**When to Use:** Development, testing, small datasets, local processing.\n\n**See Also:** [AzureBlobConnectionConfig](#azureblobconnectionconfig) for cloud alternatives.\n\nExample:\n```yaml\nlocal_data:\n  type: \"local\"\n  base_path: \"./data\"\n```",
      "properties": {
        "type": {
          "const": "local",
          "default": "local",
          "title": "Type"
        },
        "validation_mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationMode"
            }
          ],
          "default": "lazy"
        },
        "base_path": {
          "default": "./data",
          "description": "Base directory path",
          "title": "Base Path",
          "type": "string"
        }
      },
      "title": "LocalConnectionConfig",
      "type": "object"
    },
    "LogLevel": {
      "description": "Logging levels.",
      "enum": [
        "DEBUG",
        "INFO",
        "WARNING",
        "ERROR"
      ],
      "title": "LogLevel",
      "type": "string"
    },
    "LoggingConfig": {
      "description": "Logging configuration.\n\nExample:\n```yaml\nlogging:\n  level: \"INFO\"\n  structured: true\n```",
      "properties": {
        "level": {
          "allOf": [
            {
              "$ref": "#/$defs/LogLevel"
            }
          ],
          "default": "INFO"
        },
        "structured": {
          "default": false,
          "description": "Output JSON logs",
          "title": "Structured",
          "type": "boolean"
        },
        "metadata": {
          "description": "Extra metadata in logs",
          "title": "Metadata",
          "type": "object"
        }
      },
      "title": "LoggingConfig",
      "type": "object"
    },
    "NodeConfig": {
      "description": "Configuration for a single node.\n\n### \ud83e\udde0 \"The Smart Node\" Pattern\n\n**Business Problem:**\n\"We need complex dependencies, caching for heavy computations, and the ability to run only specific parts of the pipeline.\"\n\n**The Solution:**\nNodes are the building blocks. They handle dependencies (`depends_on`), execution control (`tags`, `enabled`), and performance (`cache`).\n\n### \ud83d\udd78\ufe0f DAG & Dependencies\n**The Glue of the Pipeline.**\nNodes don't run in isolation. They form a Directed Acyclic Graph (DAG).\n\n*   **`depends_on`**: Critical! If Node B reads from Node A (in memory), you MUST list `[\"Node A\"]`.\n    *   *Implicit Data Flow*: If a node has no `read` block, it automatically picks up the DataFrame from its first dependency.\n\n### \ud83e\udde0 Smart Read & Incremental Loading\n\n**Automated History Management.**\n\nOdibi intelligently determines whether to perform a **Full Load** or an **Incremental Load** based on the state of the target.\n\n**The \"Smart Read\" Logic:**\n1.  **First Run (Full Load):** If the target table (defined in `write`) does **not exist**:\n    *   Incremental filtering rules are **ignored**.\n    *   The entire source dataset is read.\n    *   Use `write.first_run_query` (optional) to override the read query for this initial bootstrap (e.g., to backfill only 1 year of history instead of all time).\n\n2.  **Subsequent Runs (Incremental Load):** If the target table **exists**:\n    *   **Rolling Window:** Filters source data where `column >= NOW() - lookback`.\n    *   **Stateful:** Filters source data where `column > last_high_water_mark`.\n\nThis ensures you don't need separate \"init\" and \"update\" pipelines. One config handles both lifecycle states.\n\n### \ud83c\udff7\ufe0f Orchestration Tags\n**Run What You Need.**\nTags allow you to execute slices of your pipeline.\n*   `odibi run --tag daily` -> Runs all nodes with \"daily\" tag.\n*   `odibi run --tag critical` -> Runs high-priority nodes.\n\n### \ud83e\udd16 Choosing Your Logic: Transformer vs. Transform\n\n**1. The \"Transformer\" (Top-Level)**\n*   **What it is:** A pre-packaged, heavy-duty operation that defines the *entire purpose* of the node.\n*   **When to use:** When applying a standard Data Engineering pattern (e.g., SCD2, Merge, Deduplicate).\n*   **Analogy:** \"Run this App.\"\n*   **Syntax:** `transformer: \"scd2\"` + `params: {...}`\n\n**2. The \"Transform Steps\" (Process Chain)**\n*   **What it is:** A sequence of smaller steps (SQL, functions, operations) executed in order.\n*   **When to use:** For custom business logic, data cleaning, or feature engineering pipelines.\n*   **Analogy:** \"Run this Script.\"\n*   **Syntax:** `transform: { steps: [...] }`\n\n*Note: You can use both! The `transformer` runs first, then `transform` steps refine the result.*\n\n### \ud83d\udd17 Chaining Operations\n**You can mix and match!**\nThe execution order is always:\n1.  **Read** (or Dependency Injection)\n2.  **Transformer** (The \"App\" logic, e.g., Deduplicate)\n3.  **Transform Steps** (The \"Script\" logic, e.g., cleanup)\n4.  **Validation**\n5.  **Write**\n\n*Constraint:* You must define **at least one** of `read`, `transformer`, `transform`, or `write`.\n\n### \u26a1 Example: App vs. Script\n\n**Scenario 1: The Full ETL Flow (Chained)**\n*Shows explicit Read, Transform Chain, and Write.*\n\n```yaml\n# 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]\n\n  # \"clean_text\" is a registered function from the Transformer Catalog\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n```\n\n**Scenario 2: The \"App\" Node (Top-Level Transformer)**\n*Shows a node that applies a pattern (Deduplicate) to incoming data.*\n\n```yaml\n- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication (From Transformer Catalog)\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n```\n\n**Scenario 3: The Tagged Runner (Reporting)**\n*Shows how tags allow running specific slices (e.g., `odibi run --tag daily`).*\n\n```yaml\n- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  depends_on: [\"deduped_users\"]\n\n  # Ad-hoc aggregation script\n  transform:\n    steps:\n      - sql: \"SELECT date_trunc('day', updated_at) as day, count(*) as total FROM df GROUP BY 1\"\n\n  write: { connection: \"local_data\", format: \"csv\", path: \"reports/daily_stats.csv\" }\n```\n\n**Scenario 4: The \"Kitchen Sink\" (All Operations)**\n*Shows Read -> Transformer -> Transform -> Write execution order.*\n\n**Why this works:**\n1.  **Internal Chaining (`df`):** In every step (Transformer or SQL), `df` refers to the output of the *previous* step.\n2.  **External Access (`depends_on`):** If you added `depends_on: [\"other_node\"]`, you could also run `SELECT * FROM other_node` in your SQL steps!\n\n```yaml\n- name: \"complex_flow\"\n  # 1. Read -> Creates initial 'df'\n  read: { connection: \"bronze\", format: \"parquet\", path: \"users\" }\n\n  # 2. Transformer (The \"App\": Deduplicate first)\n  # Takes 'df' (from Read), dedups it, returns new 'df'\n  transformer: \"deduplicate\"\n  params: { keys: [\"user_id\"], order_by: \"updated_at DESC\" }\n\n  # 3. Transform Steps (The \"Script\": Filter AFTER deduplication)\n  # SQL sees the deduped data as 'df'\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE status = 'active'\"\n\n  # 4. Write -> Saves the final filtered 'df'\n  write: { connection: \"silver\", format: \"delta\", table: \"active_unique_users\" }\n```\n\n### \ud83d\udcda Transformer Catalog\n\nThese are the built-in functions you can use in two ways:\n\n1.  **As a Top-Level Transformer:** `transformer: \"name\"` (Defines the node's main logic)\n2.  **As a Step in a Chain:** `transform: { steps: [{ function: \"name\" }] }` (Part of a sequence)\n\n*Note: `merge` and `scd2` are special \"Heavy Lifters\" and should generally be used as Top-Level Transformers.*\n\n**Data Engineering Patterns**\n*   `merge`: Upsert/Merge into target (Delta/SQL). *([Params](#mergeparams))*\n*   `scd2`: Slowly Changing Dimensions Type 2. *([Params](#scd2params))*\n*   `deduplicate`: Remove duplicates using window functions. *([Params](#deduplicateparams))*\n\n**Relational Algebra**\n*   `join`: Join two datasets. *([Params](#joinparams))*\n*   `union`: Stack datasets vertically. *([Params](#unionparams))*\n*   `pivot`: Rotate rows to columns. *([Params](#pivotparams))*\n*   `unpivot`: Rotate columns to rows (melt). *([Params](#unpivotparams))*\n*   `aggregate`: Group by and sum/count/avg. *([Params](#aggregateparams))*\n\n**Data Quality & Cleaning**\n*   `validate_and_flag`: Check rules and flag invalid rows. *([Params](#validateandflagparams))*\n*   `clean_text`: Trim and normalize case. *([Params](#cleantextparams))*\n*   `filter_rows`: SQL-based filtering. *([Params](#filterrowsparams))*\n*   `fill_nulls`: Replace NULLs with defaults. *([Params](#fillnullsparams))*\n\n**Feature Engineering**\n*   `derive_columns`: Create new cols via SQL expressions. *([Params](#derivecolumnsparams))*\n*   `case_when`: Conditional logic (if-else). *([Params](#casewhenparams))*\n*   `generate_surrogate_key`: Create MD5 keys from columns. *([Params](#surrogatekeyparams))*\n*   `date_diff`, `date_add`, `date_trunc`: Date arithmetic.\n\n**Scenario 1: The Full ETL Flow**\n*(Show two nodes: one loader, one processor)*\n\n```yaml\n# 1. Ingest (The Dependency)\n- name: \"load_raw_users\"\n  read: { connection: \"s3_landing\", format: \"json\", path: \"users/*.json\" }\n  write: { connection: \"bronze\", format: \"parquet\", path: \"users_raw\" }\n\n# 2. Process (The Consumer)\n- name: \"clean_users\"\n  depends_on: [\"load_raw_users\"]  # <--- Explicit dependency\n\n  # Explicit Transformation Steps\n  transform:\n    steps:\n      - sql: \"SELECT * FROM df WHERE email IS NOT NULL\"\n      - function: \"clean_text\"\n        params: { columns: [\"email\"], case: \"lower\" }\n\n  write: { connection: \"silver\", format: \"delta\", table: \"dim_users\" }\n```\n\n**Scenario 2: The \"App\" Node (Transformer)**\n*(Show a node that is a Transformer, no read needed if it picks up from dependency)*\n\n```yaml\n- name: \"deduped_users\"\n  depends_on: [\"clean_users\"]\n\n  # The \"App\": Deduplication\n  transformer: \"deduplicate\"\n  params:\n    keys: [\"user_id\"]\n    order_by: \"updated_at DESC\"\n\n  write: { connection: \"gold\", format: \"delta\", table: \"users_unique\" }\n```\n\n**Scenario 3: The Tagged Runner**\n*Run only this with `odibi run --tag daily`*\n```yaml\n- name: \"daily_report\"\n  tags: [\"daily\", \"reporting\"]\n  # ...\n```\n\n**Scenario 4: Pre/Post SQL Hooks**\n*Setup and cleanup with SQL statements.*\n```yaml\n- name: \"optimize_sales\"\n  depends_on: [\"load_sales\"]\n  pre_sql:\n    - \"SET spark.sql.shuffle.partitions = 200\"\n    - \"CREATE TEMP VIEW staging AS SELECT * FROM bronze.raw_sales\"\n  transform:\n    steps:\n      - sql: \"SELECT * FROM staging WHERE amount > 0\"\n  post_sql:\n    - \"OPTIMIZE gold.fact_sales ZORDER BY (customer_id)\"\n    - \"VACUUM gold.fact_sales RETAIN 168 HOURS\"\n  write:\n    connection: \"gold\"\n    format: \"delta\"\n    table: \"fact_sales\"\n```\n\n**Scenario 5: Materialization Strategies**\n*Choose how output is persisted.*\n```yaml\n# Option 1: View (no physical storage, logical model)\n- name: \"vw_active_customers\"\n  materialized: \"view\"  # Creates SQL view instead of table\n  transform:\n    steps:\n      - sql: \"SELECT * FROM customers WHERE status = 'active'\"\n  write:\n    connection: \"gold\"\n    table: \"vw_active_customers\"\n\n# Option 2: Incremental (append to existing Delta table)\n- name: \"fact_events\"\n  materialized: \"incremental\"  # Uses APPEND mode\n  read:\n    connection: \"bronze\"\n    table: \"raw_events\"\n    incremental:\n      mode: \"stateful\"\n      column: \"event_time\"\n  write:\n    connection: \"silver\"\n    format: \"delta\"\n    table: \"fact_events\"\n\n# Option 3: Table (default - full overwrite)\n- name: \"dim_products\"\n  materialized: \"table\"  # Default behavior\n  # ...\n```",
      "properties": {
        "name": {
          "description": "Unique node name",
          "title": "Name",
          "type": "string"
        },
        "description": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Human-readable description",
          "title": "Description"
        },
        "runbook_url": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "URL to troubleshooting guide or runbook. Shown as 'Troubleshooting guide \u2192' link on failures.",
          "title": "Runbook Url"
        },
        "enabled": {
          "default": true,
          "description": "If False, node is skipped during execution",
          "title": "Enabled",
          "type": "boolean"
        },
        "tags": {
          "description": "Operational tags for selective execution (e.g., 'daily', 'critical'). Use with `odibi run --tag`.",
          "items": {
            "type": "string"
          },
          "title": "Tags",
          "type": "array"
        },
        "depends_on": {
          "description": "List of parent nodes that must complete before this node runs. The output of these nodes is available for reading.",
          "items": {
            "type": "string"
          },
          "title": "Depends On",
          "type": "array"
        },
        "columns": {
          "additionalProperties": {
            "$ref": "#/$defs/ColumnMetadata"
          },
          "description": "Data Dictionary defining the output schema. Used for documentation, PII tagging, and validation.",
          "title": "Columns",
          "type": "object"
        },
        "read": {
          "anyOf": [
            {
              "$ref": "#/$defs/ReadConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Input operation (Load). If missing, data is taken from the first dependency."
        },
        "inputs": {
          "anyOf": [
            {
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "object"
                  }
                ]
              },
              "type": "object"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Multi-input support for cross-pipeline dependencies. Map input names to either: (a) $pipeline.node reference (e.g., '$read_bronze.shift_events') (b) Explicit read config dict. Cannot be used with 'read'. Example: inputs: {events: '$read_bronze.events', calendar: {connection: 'goat', path: 'cal'}}",
          "title": "Inputs"
        },
        "transform": {
          "anyOf": [
            {
              "$ref": "#/$defs/TransformConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Chain of fine-grained transformation steps (SQL, functions). Runs after 'transformer' if both are present."
        },
        "write": {
          "anyOf": [
            {
              "$ref": "#/$defs/WriteConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Output operation (Save to file/table)."
        },
        "streaming": {
          "default": false,
          "description": "Enable streaming execution for this node (Spark only)",
          "title": "Streaming",
          "type": "boolean"
        },
        "transformer": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Name of the 'App' logic to run (e.g., 'deduplicate', 'scd2'). See Transformer Catalog for options.",
          "title": "Transformer"
        },
        "params": {
          "description": "Parameters for transformer",
          "title": "Params",
          "type": "object"
        },
        "pre_sql": {
          "description": "List of SQL statements to execute before node runs. Use for setup: temp tables, variable initialization, grants. Example: ['SET spark.sql.shuffle.partitions=200', 'CREATE TEMP VIEW src AS SELECT * FROM raw']",
          "items": {
            "type": "string"
          },
          "title": "Pre Sql",
          "type": "array"
        },
        "post_sql": {
          "description": "List of SQL statements to execute after node completes. Use for cleanup, optimization, or audit logging. Example: ['OPTIMIZE gold.fact_sales', 'VACUUM gold.fact_sales RETAIN 168 HOURS']",
          "items": {
            "type": "string"
          },
          "title": "Post Sql",
          "type": "array"
        },
        "materialized": {
          "anyOf": [
            {
              "enum": [
                "table",
                "view",
                "incremental"
              ],
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Materialization strategy. Options: 'table' (default physical write), 'view' (creates SQL view instead of table), 'incremental' (uses append mode for Delta tables). Views are useful for Gold layer logical models.",
          "title": "Materialized"
        },
        "cache": {
          "default": false,
          "description": "Cache result for reuse",
          "title": "Cache",
          "type": "boolean"
        },
        "log_level": {
          "anyOf": [
            {
              "$ref": "#/$defs/LogLevel"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Override log level for this node"
        },
        "on_error": {
          "allOf": [
            {
              "$ref": "#/$defs/ErrorStrategy"
            }
          ],
          "default": "fail_later",
          "description": "Failure handling strategy"
        },
        "validation": {
          "anyOf": [
            {
              "$ref": "#/$defs/ValidationConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null
        },
        "contracts": {
          "description": "Pre-condition contracts (Circuit Breakers). Runs on input data before transformation.",
          "items": {
            "discriminator": {
              "mapping": {
                "accepted_values": "#/$defs/AcceptedValuesTest",
                "custom_sql": "#/$defs/CustomSQLTest",
                "distribution": "#/$defs/DistributionContract",
                "freshness": "#/$defs/FreshnessContract",
                "not_null": "#/$defs/NotNullTest",
                "range": "#/$defs/RangeTest",
                "regex_match": "#/$defs/RegexMatchTest",
                "row_count": "#/$defs/RowCountTest",
                "schema": "#/$defs/SchemaContract",
                "unique": "#/$defs/UniqueTest",
                "volume_drop": "#/$defs/VolumeDropTest"
              },
              "propertyName": "type"
            },
            "oneOf": [
              {
                "$ref": "#/$defs/NotNullTest"
              },
              {
                "$ref": "#/$defs/UniqueTest"
              },
              {
                "$ref": "#/$defs/AcceptedValuesTest"
              },
              {
                "$ref": "#/$defs/RowCountTest"
              },
              {
                "$ref": "#/$defs/CustomSQLTest"
              },
              {
                "$ref": "#/$defs/RangeTest"
              },
              {
                "$ref": "#/$defs/RegexMatchTest"
              },
              {
                "$ref": "#/$defs/VolumeDropTest"
              },
              {
                "$ref": "#/$defs/SchemaContract"
              },
              {
                "$ref": "#/$defs/DistributionContract"
              },
              {
                "$ref": "#/$defs/FreshnessContract"
              }
            ]
          },
          "title": "Contracts",
          "type": "array"
        },
        "schema_policy": {
          "anyOf": [
            {
              "$ref": "#/$defs/SchemaPolicyConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Schema drift handling policy"
        },
        "privacy": {
          "anyOf": [
            {
              "$ref": "#/$defs/PrivacyConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Privacy Suite: PII anonymization settings"
        },
        "sensitive": {
          "anyOf": [
            {
              "type": "boolean"
            },
            {
              "items": {
                "type": "string"
              },
              "type": "array"
            }
          ],
          "default": false,
          "description": "If true or list of columns, masks sample data in stories",
          "title": "Sensitive"
        },
        "_source_yaml": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Internal: source YAML file path for sql_file resolution",
          "title": " Source Yaml"
        }
      },
      "required": [
        "name"
      ],
      "title": "NodeConfig",
      "type": "object"
    },
    "NotNullTest": {
      "description": "Ensures specified columns contain no NULL values.\n\n**When to Use:** Primary keys, required fields, foreign keys that must resolve.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: not_null\n    columns: [order_id, customer_id, created_at]\n```",
      "properties": {
        "type": {
          "const": "not_null",
          "default": "not_null",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "columns": {
          "description": "Columns that must not contain nulls",
          "items": {
            "type": "string"
          },
          "title": "Columns",
          "type": "array"
        }
      },
      "required": [
        "columns"
      ],
      "title": "NotNullTest",
      "type": "object"
    },
    "OnFailAction": {
      "enum": [
        "alert",
        "ignore"
      ],
      "title": "OnFailAction",
      "type": "string"
    },
    "OnMissingColumns": {
      "enum": [
        "fail",
        "fill_null"
      ],
      "title": "OnMissingColumns",
      "type": "string"
    },
    "OnNewColumns": {
      "enum": [
        "ignore",
        "fail",
        "add_nullable"
      ],
      "title": "OnNewColumns",
      "type": "string"
    },
    "PerformanceConfig": {
      "description": "Performance tuning configuration.\n\nExample:\n```yaml\nperformance:\n  use_arrow: true\n  spark_config:\n    \"spark.sql.shuffle.partitions\": \"200\"\n    \"spark.sql.adaptive.enabled\": \"true\"\n    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\"\n  delta_table_properties:\n    \"delta.columnMapping.mode\": \"name\"\n```\n\n**Spark Config Notes:**\n- Configs are applied via `spark.conf.set()` at runtime\n- For existing sessions (e.g., Databricks), only runtime-settable configs will take effect\n- Session-level configs (e.g., `spark.executor.memory`) require session restart\n- Common runtime-safe configs: shuffle partitions, adaptive query execution, Delta optimizations",
      "properties": {
        "use_arrow": {
          "default": true,
          "description": "Use Apache Arrow-backed DataFrames (Pandas only). Reduces memory and speeds up I/O.",
          "title": "Use Arrow",
          "type": "boolean"
        },
        "spark_config": {
          "additionalProperties": {
            "type": "string"
          },
          "description": "Spark configuration settings applied at runtime via spark.conf.set(). Example: {'spark.sql.shuffle.partitions': '200', 'spark.sql.adaptive.enabled': 'true'}. Note: Some configs require session restart and cannot be set at runtime.",
          "title": "Spark Config",
          "type": "object"
        },
        "delta_table_properties": {
          "additionalProperties": {
            "type": "string"
          },
          "description": "Default table properties applied to all Delta writes. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names.",
          "title": "Delta Table Properties",
          "type": "object"
        },
        "skip_null_profiling": {
          "default": false,
          "description": "Skip null profiling in metadata collection phase. Reduces execution time for large DataFrames by avoiding an additional Spark job.",
          "title": "Skip Null Profiling",
          "type": "boolean"
        },
        "skip_catalog_writes": {
          "default": false,
          "description": "Skip catalog metadata writes (register_asset, track_schema, log_pattern, record_lineage) after each node write. Significantly improves performance for high-throughput pipelines like Bronze layer ingestion. Set to true when catalog tracking is not needed.",
          "title": "Skip Catalog Writes",
          "type": "boolean"
        },
        "skip_run_logging": {
          "default": false,
          "description": "Skip batch catalog writes at pipeline end (log_runs_batch, register_outputs_batch). Saves 10-20s per pipeline run. Enable when you don't need run history in the catalog. Stories are still generated and contain full execution details.",
          "title": "Skip Run Logging",
          "type": "boolean"
        }
      },
      "title": "PerformanceConfig",
      "type": "object"
    },
    "PipelineConfig": {
      "description": "Configuration for a pipeline.\n\nExample:\n```yaml\npipelines:\n  - pipeline: \"user_onboarding\"\n    description: \"Ingest and process new users\"\n    layer: \"silver\"\n    nodes:\n      - name: \"node1\"\n        ...\n```",
      "properties": {
        "pipeline": {
          "description": "Pipeline name",
          "title": "Pipeline",
          "type": "string"
        },
        "description": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Pipeline description",
          "title": "Description"
        },
        "layer": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Logical layer (bronze/silver/gold)",
          "title": "Layer"
        },
        "nodes": {
          "description": "List of nodes in this pipeline",
          "items": {
            "$ref": "#/$defs/NodeConfig"
          },
          "title": "Nodes",
          "type": "array"
        }
      },
      "required": [
        "pipeline",
        "nodes"
      ],
      "title": "PipelineConfig",
      "type": "object"
    },
    "PrivacyConfig": {
      "description": "Configuration for PII anonymization.\n\n### \ud83d\udd10 Privacy & PII Protection\n\n**How It Works:**\n1. Mark columns as `pii: true` in the `columns` metadata\n2. Configure a `privacy` block with the anonymization method\n3. During node execution, all columns marked as PII (and inherited from dependencies) are anonymized\n4. Upstream PII markings are inherited by downstream nodes\n\n**Example:**\n```yaml\ncolumns:\n  customer_email:\n    pii: true  # Mark as PII\n  customer_id:\n    pii: false\n\nprivacy:\n  method: hash       # hash, mask, or redact\n  salt: \"secret_key\" # Optional: makes hash unique/secure\n  declassify: []     # Remove columns from PII protection\n```\n\n**Methods:**\n- `hash`: SHA256 hash (length 64). With salt, prevents pre-computed rainbow tables.\n- `mask`: Show only last 4 chars, replace rest with `*`. Example: `john@email.com` \u2192 `****@email.com`\n- `redact`: Replace entire value with `[REDACTED]`\n\n**Important:**\n- `pii: true` alone does NOTHING. You must set a `privacy.method` to actually mask data.\n- PII inheritance: If dependency outputs PII columns, this node inherits them unless declassified.\n- Salt is optional but recommended for hash to prevent attacks.",
      "properties": {
        "method": {
          "allOf": [
            {
              "$ref": "#/$defs/PrivacyMethod"
            }
          ],
          "description": "Anonymization method: 'hash' (SHA256), 'mask' (show last 4), or 'redact' ([REDACTED])"
        },
        "salt": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Salt for hashing (optional but recommended). Appended before hashing to create unique hashes. Example: 'company_secret_key_2025'",
          "title": "Salt"
        },
        "declassify": {
          "description": "List of columns to remove from PII protection (stops inheritance from upstream). Example: ['customer_id']",
          "items": {
            "type": "string"
          },
          "title": "Declassify",
          "type": "array"
        }
      },
      "required": [
        "method"
      ],
      "title": "PrivacyConfig",
      "type": "object"
    },
    "PrivacyMethod": {
      "description": "Supported privacy anonymization methods.",
      "enum": [
        "hash",
        "mask",
        "redact"
      ],
      "title": "PrivacyMethod",
      "type": "string"
    },
    "QuarantineColumnsConfig": {
      "description": "Columns added to quarantined rows for debugging and reprocessing.\n\nExample:\n```yaml\nquarantine:\n  connection: silver\n  path: customers_quarantine\n  add_columns:\n    _rejection_reason: true\n    _rejected_at: true\n    _source_batch_id: true\n    _failed_tests: true\n    _original_node: false\n```",
      "properties": {
        "rejection_reason": {
          "default": true,
          "description": "Add _rejection_reason column with test failure description",
          "title": "Rejection Reason",
          "type": "boolean"
        },
        "rejected_at": {
          "default": true,
          "description": "Add _rejected_at column with UTC timestamp",
          "title": "Rejected At",
          "type": "boolean"
        },
        "source_batch_id": {
          "default": true,
          "description": "Add _source_batch_id column with run ID for traceability",
          "title": "Source Batch Id",
          "type": "boolean"
        },
        "failed_tests": {
          "default": true,
          "description": "Add _failed_tests column with comma-separated list of failed test names",
          "title": "Failed Tests",
          "type": "boolean"
        },
        "original_node": {
          "default": false,
          "description": "Add _original_node column with source node name",
          "title": "Original Node",
          "type": "boolean"
        }
      },
      "title": "QuarantineColumnsConfig",
      "type": "object"
    },
    "QuarantineConfig": {
      "description": "Configuration for quarantine table routing.\n\n**When to Use:** Capture invalid records for review/reprocessing instead of failing the pipeline.\n\n**See Also:** [Quarantine Guide](../features/quarantine.md), [ValidationConfig](#validationconfig)\n\nRoutes rows that fail validation tests to a quarantine table\nwith rejection metadata for later analysis/reprocessing.\n\nExample:\n```yaml\nvalidation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n    add_columns:\n      _rejection_reason: true\n      _rejected_at: true\n    max_rows: 10000\n    sample_fraction: 0.1\n```",
      "properties": {
        "connection": {
          "description": "Connection for quarantine writes",
          "title": "Connection",
          "type": "string"
        },
        "path": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Path for quarantine data",
          "title": "Path"
        },
        "table": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Table name for quarantine",
          "title": "Table"
        },
        "add_columns": {
          "allOf": [
            {
              "$ref": "#/$defs/QuarantineColumnsConfig"
            }
          ],
          "description": "Metadata columns to add to quarantined rows"
        },
        "retention_days": {
          "anyOf": [
            {
              "minimum": 1,
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": 90,
          "description": "Days to retain quarantined data (auto-cleanup)",
          "title": "Retention Days"
        },
        "max_rows": {
          "anyOf": [
            {
              "minimum": 1,
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Maximum number of rows to quarantine per run. Limits storage for high-failure batches.",
          "title": "Max Rows"
        },
        "sample_fraction": {
          "anyOf": [
            {
              "maximum": 1.0,
              "minimum": 0.0,
              "type": "number"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Sample fraction of invalid rows to quarantine (0.0-1.0). Use for sampling large invalid sets.",
          "title": "Sample Fraction"
        }
      },
      "required": [
        "connection"
      ],
      "title": "QuarantineConfig",
      "type": "object"
    },
    "RangeTest": {
      "description": "Ensures column values fall within a specified range.\n\n**When to Use:** Numeric bounds validation (ages, prices, quantities), date ranges.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: range\n    column: age\n    min: 0\n    max: 150\n```",
      "properties": {
        "type": {
          "const": "range",
          "default": "range",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "column": {
          "description": "Column to check",
          "title": "Column",
          "type": "string"
        },
        "min": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "number"
            },
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Minimum value (inclusive)",
          "title": "Min"
        },
        "max": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "number"
            },
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Maximum value (inclusive)",
          "title": "Max"
        }
      },
      "required": [
        "column"
      ],
      "title": "RangeTest",
      "type": "object"
    },
    "ReadConfig": {
      "description": "Configuration for reading data into a node.\n\n**When to Use:** First node in a pipeline, or any node that reads from storage.\n\n**Key Concepts:**\n- `connection`: References a named connection from `connections:` section\n- `format`: File format (csv, parquet, delta, json, sql)\n- `incremental`: Enable incremental loading (only new data)\n\n**See Also:**\n- [Incremental Loading](../patterns/incremental_stateful.md) - HWM-based loading\n- [IncrementalConfig](#incrementalconfig) - Incremental loading options\n\n### \ud83d\udcd6 \"Universal Reader\" Guide\n\n**Business Problem:**\n\"I need to read from files, databases, streams, and even travel back in time to see how data looked yesterday.\"\n\n**Recipe 1: The Time Traveler (Delta/Iceberg)**\n*Reproduce a bug by seeing the data exactly as it was.*\n```yaml\nread:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  time_travel:\n    as_of_timestamp: \"2023-10-25T14:00:00Z\"\n```\n\n**Recipe 2: The Streamer**\n*Process data in real-time.*\n```yaml\nread:\n  connection: \"event_hub\"\n  format: \"json\"\n  streaming: true\n```\n\n**Recipe 3: The SQL Query**\n*Push down filtering to the source database.*\n```yaml\nread:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  # Use the query option to filter at source!\n  query: \"SELECT * FROM huge_table WHERE date >= '2024-01-01'\"\n```\n\n**Recipe 4: Archive Bad Records (Spark)**\n*Capture malformed records for later inspection.*\n```yaml\nread:\n  connection: \"landing\"\n  format: \"json\"\n  path: \"events/*.json\"\n  archive_options:\n    badRecordsPath: \"/mnt/quarantine/bad_records\"\n```\n\n**Recipe 5: Optimize JDBC Parallelism (Spark)**\n*Control partition count for SQL sources to reduce task overhead.*\n```yaml\nread:\n  connection: \"enterprise_dw\"\n  format: \"sql\"\n  table: \"small_lookup_table\"\n  options:\n    numPartitions: 1  # Single partition for small tables\n```\n\n**Performance Tip:** For small tables (<100K rows), use `numPartitions: 1` to avoid\nexcessive Spark task scheduling overhead. For large tables, increase partitions\nto enable parallel reads (requires partitionColumn, lowerBound, upperBound).",
      "properties": {
        "connection": {
          "description": "Connection name from project.yaml",
          "title": "Connection",
          "type": "string"
        },
        "format": {
          "anyOf": [
            {
              "$ref": "#/$defs/ReadFormat"
            },
            {
              "type": "string"
            }
          ],
          "description": "Data format (csv, parquet, delta, etc.)",
          "title": "Format"
        },
        "table": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Table name for SQL/Delta",
          "title": "Table"
        },
        "path": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Path for file-based sources",
          "title": "Path"
        },
        "streaming": {
          "default": false,
          "description": "Enable streaming read (Spark only)",
          "title": "Streaming",
          "type": "boolean"
        },
        "schema_ddl": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Schema for streaming reads from file sources (required for Avro, JSON, CSV). Use Spark DDL format: 'col1 STRING, col2 INT, col3 TIMESTAMP'. Not required for Delta (schema is inferred from table metadata).",
          "title": "Schema Ddl"
        },
        "query": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "SQL query to filter at source (pushdown). Mutually exclusive with table/path if supported by connector.",
          "title": "Query"
        },
        "filter": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "SQL WHERE clause filter (pushed down to source for SQL formats). Example: \"DAY > '2022-12-31'\"",
          "title": "Filter"
        },
        "incremental": {
          "anyOf": [
            {
              "$ref": "#/$defs/IncrementalConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Automatic incremental loading strategy (CDC-like). If set, generates query based on target state (HWM)."
        },
        "time_travel": {
          "anyOf": [
            {
              "$ref": "#/$defs/TimeTravelConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Time travel options (Delta only)"
        },
        "archive_options": {
          "description": "Options for archiving bad records (e.g. badRecordsPath for Spark)",
          "title": "Archive Options",
          "type": "object"
        },
        "options": {
          "description": "Format-specific options",
          "title": "Options",
          "type": "object"
        }
      },
      "required": [
        "connection",
        "format"
      ],
      "title": "ReadConfig",
      "type": "object"
    },
    "ReadFormat": {
      "enum": [
        "csv",
        "parquet",
        "delta",
        "json",
        "sql"
      ],
      "title": "ReadFormat",
      "type": "string"
    },
    "RegexMatchTest": {
      "description": "Ensures column values match a regex pattern.\n\n**When to Use:** Format validation (emails, phone numbers, IDs, codes).\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: regex_match\n    column: email\n    pattern: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n```",
      "properties": {
        "type": {
          "const": "regex_match",
          "default": "regex_match",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "column": {
          "description": "Column to check",
          "title": "Column",
          "type": "string"
        },
        "pattern": {
          "description": "Regex pattern to match",
          "title": "Pattern",
          "type": "string"
        }
      },
      "required": [
        "column",
        "pattern"
      ],
      "title": "RegexMatchTest",
      "type": "object"
    },
    "RetryConfig": {
      "description": "Retry configuration.\n\nExample:\n```yaml\nretry:\n  enabled: true\n  max_attempts: 3\n  backoff: \"exponential\"\n```",
      "properties": {
        "enabled": {
          "default": true,
          "title": "Enabled",
          "type": "boolean"
        },
        "max_attempts": {
          "default": 3,
          "maximum": 10,
          "minimum": 1,
          "title": "Max Attempts",
          "type": "integer"
        },
        "backoff": {
          "allOf": [
            {
              "$ref": "#/$defs/BackoffStrategy"
            }
          ],
          "default": "exponential"
        }
      },
      "title": "RetryConfig",
      "type": "object"
    },
    "RowCountGate": {
      "description": "Row count anomaly detection for quality gates.\n\nValidates that batch size falls within expected bounds and\ndetects significant changes from previous runs.\n\nExample:\n```yaml\ngate:\n  row_count:\n    min: 100\n    max: 1000000\n    change_threshold: 0.5\n```",
      "properties": {
        "min": {
          "anyOf": [
            {
              "minimum": 0,
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Minimum expected row count",
          "title": "Min"
        },
        "max": {
          "anyOf": [
            {
              "minimum": 0,
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Maximum expected row count",
          "title": "Max"
        },
        "change_threshold": {
          "anyOf": [
            {
              "maximum": 1.0,
              "minimum": 0.0,
              "type": "number"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Max allowed change vs previous run (e.g., 0.5 = 50% change triggers failure)",
          "title": "Change Threshold"
        }
      },
      "title": "RowCountGate",
      "type": "object"
    },
    "RowCountTest": {
      "description": "Validates that row count falls within expected bounds.\n\n**When to Use:** Ensure minimum data completeness, detect truncated loads, cap batch sizes.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates), [GateConfig](#gateconfig)\n\n```yaml\ncontracts:\n  - type: row_count\n    min: 1000\n    max: 100000\n```",
      "properties": {
        "type": {
          "const": "row_count",
          "default": "row_count",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "min": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Minimum row count",
          "title": "Min"
        },
        "max": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Maximum row count",
          "title": "Max"
        }
      },
      "title": "RowCountTest",
      "type": "object"
    },
    "SQLAadPasswordAuth": {
      "properties": {
        "mode": {
          "const": "aad_password",
          "default": "aad_password",
          "title": "Mode"
        },
        "tenant_id": {
          "title": "Tenant Id",
          "type": "string"
        },
        "client_id": {
          "title": "Client Id",
          "type": "string"
        },
        "client_secret": {
          "title": "Client Secret",
          "type": "string"
        }
      },
      "required": [
        "tenant_id",
        "client_id",
        "client_secret"
      ],
      "title": "SQLAadPasswordAuth",
      "type": "object"
    },
    "SQLConnectionStringAuth": {
      "properties": {
        "mode": {
          "const": "connection_string",
          "default": "connection_string",
          "title": "Mode"
        },
        "connection_string": {
          "title": "Connection String",
          "type": "string"
        }
      },
      "required": [
        "connection_string"
      ],
      "title": "SQLConnectionStringAuth",
      "type": "object"
    },
    "SQLLoginAuth": {
      "properties": {
        "mode": {
          "const": "sql_login",
          "default": "sql_login",
          "title": "Mode"
        },
        "username": {
          "title": "Username",
          "type": "string"
        },
        "password": {
          "title": "Password",
          "type": "string"
        }
      },
      "required": [
        "username",
        "password"
      ],
      "title": "SQLLoginAuth",
      "type": "object"
    },
    "SQLMsiAuth": {
      "properties": {
        "mode": {
          "const": "aad_msi",
          "default": "aad_msi",
          "title": "Mode"
        },
        "client_id": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Client Id"
        }
      },
      "title": "SQLMsiAuth",
      "type": "object"
    },
    "SQLServerConnectionConfig": {
      "description": "SQL Server / Azure SQL Database connection.\n\n**When to Use:** Reading from SQL Server sources, Azure SQL DB, Azure Synapse.\n\n**See Also:** [ReadConfig](#readconfig) for query options\n\nScenario 1: Managed identity (AAD MSI)\n```yaml\nsql_dw_msi:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"aad_msi\"\n```\n\nScenario 2: SQL login\n```yaml\nsql_dw_login:\n  type: \"sql_server\"\n  host: \"server.database.windows.net\"\n  database: \"dw\"\n  auth:\n    mode: \"sql_login\"\n    username: \"dw_writer\"\n    password: \"${DW_PASSWORD}\"\n```",
      "properties": {
        "type": {
          "const": "sql_server",
          "default": "sql_server",
          "title": "Type"
        },
        "validation_mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationMode"
            }
          ],
          "default": "lazy"
        },
        "host": {
          "title": "Host",
          "type": "string"
        },
        "database": {
          "title": "Database",
          "type": "string"
        },
        "port": {
          "default": 1433,
          "title": "Port",
          "type": "integer"
        },
        "auth": {
          "discriminator": {
            "mapping": {
              "aad_msi": "#/$defs/SQLMsiAuth",
              "aad_password": "#/$defs/SQLAadPasswordAuth",
              "connection_string": "#/$defs/SQLConnectionStringAuth",
              "sql_login": "#/$defs/SQLLoginAuth"
            },
            "propertyName": "mode"
          },
          "oneOf": [
            {
              "$ref": "#/$defs/SQLLoginAuth"
            },
            {
              "$ref": "#/$defs/SQLAadPasswordAuth"
            },
            {
              "$ref": "#/$defs/SQLMsiAuth"
            },
            {
              "$ref": "#/$defs/SQLConnectionStringAuth"
            }
          ],
          "title": "Auth"
        }
      },
      "required": [
        "host",
        "database"
      ],
      "title": "SQLServerConnectionConfig",
      "type": "object"
    },
    "SchemaContract": {
      "description": "Validates that the DataFrame schema matches expected columns.\n\n**When to Use:** Enforce schema stability, detect upstream schema drift, ensure column presence.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates), [SchemaPolicyConfig](#schemapolicyconfig)\n\nUses the `columns` metadata from NodeConfig to verify schema.\n\n```yaml\ncontracts:\n  - type: schema\n    strict: true  # Fail if extra columns present\n```",
      "properties": {
        "type": {
          "const": "schema",
          "default": "schema",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail"
        },
        "strict": {
          "default": true,
          "description": "If true, fail on unexpected columns",
          "title": "Strict",
          "type": "boolean"
        }
      },
      "title": "SchemaContract",
      "type": "object"
    },
    "SchemaMode": {
      "enum": [
        "enforce",
        "evolve"
      ],
      "title": "SchemaMode",
      "type": "string"
    },
    "SchemaPolicyConfig": {
      "description": "Configuration for Schema Management (Drift Handling).\n\nControls how the node handles differences between input data and target table schema.",
      "properties": {
        "mode": {
          "allOf": [
            {
              "$ref": "#/$defs/SchemaMode"
            }
          ],
          "default": "enforce",
          "description": "Schema evolution mode: 'enforce' or 'evolve'"
        },
        "on_new_columns": {
          "anyOf": [
            {
              "$ref": "#/$defs/OnNewColumns"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Action for new columns in input: 'ignore', 'fail', 'add_nullable'"
        },
        "on_missing_columns": {
          "allOf": [
            {
              "$ref": "#/$defs/OnMissingColumns"
            }
          ],
          "default": "fill_null",
          "description": "Action for missing columns in input: 'fail', 'fill_null'"
        }
      },
      "title": "SchemaPolicyConfig",
      "type": "object"
    },
    "SqlServerAuditColsConfig": {
      "description": "Audit column configuration for SQL Server merge operations.\n\nThese columns are automatically populated with GETUTCDATE() during merge:\n- `created_col`: Set on INSERT only\n- `updated_col`: Set on INSERT and UPDATE\n\nExample:\n```yaml\naudit_cols:\n  created_col: created_ts\n  updated_col: updated_ts\n```",
      "properties": {
        "created_col": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Column name for creation timestamp (set on INSERT)",
          "title": "Created Col"
        },
        "updated_col": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Column name for update timestamp (set on INSERT and UPDATE)",
          "title": "Updated Col"
        }
      },
      "title": "SqlServerAuditColsConfig",
      "type": "object"
    },
    "SqlServerMergeOptions": {
      "description": "Options for SQL Server MERGE operations (Phase 1).\n\nEnables incremental sync from Spark to SQL Server using T-SQL MERGE.\nData is written to a staging table, then merged into the target.\n\n### Basic Usage\n```yaml\nwrite:\n  connection: azure_sql\n  format: sql_server\n  table: sales.fact_orders\n  mode: merge\n  merge_keys: [DateId, store_id]\n  merge_options:\n    update_condition: \"source._hash_diff != target._hash_diff\"\n    exclude_columns: [_hash_diff]\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n```\n\n### Conditions\n- `update_condition`: Only update rows matching this condition (e.g., hash diff)\n- `delete_condition`: Delete rows matching this condition (soft delete pattern)\n- `insert_condition`: Only insert rows matching this condition",
      "properties": {
        "update_condition": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "SQL condition for WHEN MATCHED UPDATE. Use 'source.' and 'target.' prefixes. Example: 'source._hash_diff != target._hash_diff'",
          "title": "Update Condition"
        },
        "delete_condition": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "SQL condition for WHEN MATCHED DELETE. Example: 'source._is_deleted = 1'",
          "title": "Delete Condition"
        },
        "insert_condition": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "SQL condition for WHEN NOT MATCHED INSERT. Example: 'source.is_valid = 1'",
          "title": "Insert Condition"
        },
        "exclude_columns": {
          "description": "Columns to exclude from MERGE (not written to target table)",
          "items": {
            "type": "string"
          },
          "title": "Exclude Columns",
          "type": "array"
        },
        "staging_schema": {
          "default": "staging",
          "description": "Schema for staging table. Table name: {staging_schema}.{table}_staging",
          "title": "Staging Schema",
          "type": "string"
        },
        "audit_cols": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerAuditColsConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Audit columns for created/updated timestamps"
        },
        "validations": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerMergeValidationConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Validation checks before merge (null keys, duplicate keys)"
        },
        "auto_create_schema": {
          "default": false,
          "description": "Auto-create schema if it doesn't exist (Phase 4). Runs CREATE SCHEMA IF NOT EXISTS.",
          "title": "Auto Create Schema",
          "type": "boolean"
        },
        "auto_create_table": {
          "default": false,
          "description": "Auto-create target table if it doesn't exist (Phase 4). Infers schema from DataFrame.",
          "title": "Auto Create Table",
          "type": "boolean"
        },
        "schema_evolution": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerSchemaEvolutionConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Schema evolution configuration (Phase 4). Controls handling of schema differences."
        },
        "batch_size": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Batch size for staging table writes (Phase 4). Chunks large DataFrames for memory efficiency.",
          "title": "Batch Size"
        },
        "primary_key_on_merge_keys": {
          "default": false,
          "description": "Create a clustered primary key on merge_keys when auto-creating table. Enforces uniqueness.",
          "title": "Primary Key On Merge Keys",
          "type": "boolean"
        },
        "index_on_merge_keys": {
          "default": false,
          "description": "Create a nonclustered index on merge_keys. Use if primary key already exists elsewhere.",
          "title": "Index On Merge Keys",
          "type": "boolean"
        },
        "incremental": {
          "default": false,
          "description": "Enable incremental merge optimization. When True, reads target table's keys and hashes to determine which rows changed, then only writes changed rows to staging. Significantly faster when few rows change between runs.",
          "title": "Incremental",
          "type": "boolean"
        },
        "hash_column": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Name of pre-computed hash column in DataFrame for change detection. Used when incremental=True. If not specified, will auto-detect '_hash_diff' column.",
          "title": "Hash Column"
        },
        "change_detection_columns": {
          "anyOf": [
            {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Columns to use for computing change detection hash. Used when incremental=True and no hash_column is specified. If None, uses all non-key columns.",
          "title": "Change Detection Columns"
        }
      },
      "title": "SqlServerMergeOptions",
      "type": "object"
    },
    "SqlServerMergeValidationConfig": {
      "description": "Validation configuration for SQL Server merge/overwrite operations.\n\nValidates source data before writing to SQL Server.\n\nExample:\n```yaml\nmerge_options:\n  validations:\n    check_null_keys: true\n    check_duplicate_keys: true\n    fail_on_validation_error: true\n```",
      "properties": {
        "check_null_keys": {
          "default": true,
          "description": "Fail if merge_keys contain NULL values",
          "title": "Check Null Keys",
          "type": "boolean"
        },
        "check_duplicate_keys": {
          "default": true,
          "description": "Fail if merge_keys have duplicate combinations",
          "title": "Check Duplicate Keys",
          "type": "boolean"
        },
        "fail_on_validation_error": {
          "default": true,
          "description": "If False, log warning instead of failing on validation errors",
          "title": "Fail On Validation Error",
          "type": "boolean"
        }
      },
      "title": "SqlServerMergeValidationConfig",
      "type": "object"
    },
    "SqlServerOverwriteOptions": {
      "description": "Options for SQL Server overwrite operations (Phase 2).\n\nEnhanced overwrite with multiple strategies for different use cases.\n\n### Strategies\n- `truncate_insert`: TRUNCATE TABLE then INSERT (fastest, requires TRUNCATE permission)\n- `drop_create`: DROP TABLE, CREATE TABLE, INSERT (refreshes schema)\n- `delete_insert`: DELETE FROM then INSERT (works with limited permissions)\n\n### Example\n```yaml\nwrite:\n  connection: azure_sql\n  format: sql_server\n  table: fact.combined_downtime\n  mode: overwrite\n  overwrite_options:\n    strategy: truncate_insert\n    audit_cols:\n      created_col: created_ts\n      updated_col: updated_ts\n```",
      "properties": {
        "strategy": {
          "allOf": [
            {
              "$ref": "#/$defs/SqlServerOverwriteStrategy"
            }
          ],
          "default": "truncate_insert",
          "description": "Overwrite strategy: truncate_insert, drop_create, delete_insert"
        },
        "audit_cols": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerAuditColsConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Audit columns for created/updated timestamps"
        },
        "validations": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerMergeValidationConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Validation checks before overwrite"
        },
        "auto_create_schema": {
          "default": false,
          "description": "Auto-create schema if it doesn't exist (Phase 4). Runs CREATE SCHEMA IF NOT EXISTS.",
          "title": "Auto Create Schema",
          "type": "boolean"
        },
        "auto_create_table": {
          "default": false,
          "description": "Auto-create target table if it doesn't exist (Phase 4). Infers schema from DataFrame.",
          "title": "Auto Create Table",
          "type": "boolean"
        },
        "schema_evolution": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerSchemaEvolutionConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Schema evolution configuration (Phase 4). Controls handling of schema differences."
        },
        "batch_size": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Batch size for writes (Phase 4). Chunks large DataFrames for memory efficiency.",
          "title": "Batch Size"
        }
      },
      "title": "SqlServerOverwriteOptions",
      "type": "object"
    },
    "SqlServerOverwriteStrategy": {
      "description": "Strategies for SQL Server overwrite operations.",
      "enum": [
        "truncate_insert",
        "drop_create",
        "delete_insert"
      ],
      "title": "SqlServerOverwriteStrategy",
      "type": "string"
    },
    "SqlServerSchemaEvolutionConfig": {
      "description": "Schema evolution configuration for SQL Server operations (Phase 4).\n\nControls automatic schema changes when DataFrame schema differs from target table.\n\nExample:\n```yaml\nmerge_options:\n  schema_evolution:\n    mode: evolve\n    add_columns: true\n```",
      "properties": {
        "mode": {
          "allOf": [
            {
              "$ref": "#/$defs/SqlServerSchemaEvolutionMode"
            }
          ],
          "default": "strict",
          "description": "Schema evolution mode: strict (fail), evolve (add columns), ignore (skip mismatched)"
        },
        "add_columns": {
          "default": false,
          "description": "If mode='evolve', automatically add new columns via ALTER TABLE ADD COLUMN",
          "title": "Add Columns",
          "type": "boolean"
        }
      },
      "title": "SqlServerSchemaEvolutionConfig",
      "type": "object"
    },
    "SqlServerSchemaEvolutionMode": {
      "description": "Schema evolution modes for SQL Server writes (Phase 4).\n\nControls how schema differences between DataFrame and target table are handled.",
      "enum": [
        "strict",
        "evolve",
        "ignore"
      ],
      "title": "SqlServerSchemaEvolutionMode",
      "type": "string"
    },
    "StoryConfig": {
      "description": "Story generation configuration.\n\nStories are ODIBI's core value - execution reports with lineage.\nThey must use a connection for consistent, traceable output.\n\nExample:\n```yaml\nstory:\n  connection: \"local_data\"\n  path: \"stories/\"\n  retention_days: 30\n  failure_sample_size: 100\n  max_failure_samples: 500\n  max_sampled_validations: 5\n```\n\n**Failure Sample Settings:**\n- `failure_sample_size`: Number of failed rows to capture per validation (default: 100)\n- `max_failure_samples`: Total failed rows across all validations (default: 500)\n- `max_sampled_validations`: After this many validations, show only counts (default: 5)",
      "properties": {
        "connection": {
          "description": "Connection name for story output (uses connection's path resolution)",
          "title": "Connection",
          "type": "string"
        },
        "path": {
          "description": "Path for stories (relative to connection base_path)",
          "title": "Path",
          "type": "string"
        },
        "max_sample_rows": {
          "default": 10,
          "maximum": 100,
          "minimum": 0,
          "title": "Max Sample Rows",
          "type": "integer"
        },
        "auto_generate": {
          "default": true,
          "title": "Auto Generate",
          "type": "boolean"
        },
        "retention_days": {
          "anyOf": [
            {
              "minimum": 1,
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": 30,
          "description": "Days to keep stories",
          "title": "Retention Days"
        },
        "retention_count": {
          "anyOf": [
            {
              "minimum": 1,
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": 100,
          "description": "Max number of stories to keep",
          "title": "Retention Count"
        },
        "failure_sample_size": {
          "default": 100,
          "description": "Number of failed rows to capture per validation rule",
          "maximum": 1000,
          "minimum": 0,
          "title": "Failure Sample Size",
          "type": "integer"
        },
        "max_failure_samples": {
          "default": 500,
          "description": "Maximum total failed rows across all validations",
          "maximum": 5000,
          "minimum": 0,
          "title": "Max Failure Samples",
          "type": "integer"
        },
        "max_sampled_validations": {
          "default": 5,
          "description": "After this many validations, show only counts (no samples)",
          "maximum": 20,
          "minimum": 1,
          "title": "Max Sampled Validations",
          "type": "integer"
        },
        "async_generation": {
          "default": false,
          "description": "Generate stories asynchronously (fire-and-forget). Pipeline returns immediately while story writes in background. Improves multi-pipeline performance by ~5-10s per pipeline.",
          "title": "Async Generation",
          "type": "boolean"
        },
        "generate_lineage": {
          "default": true,
          "description": "Generate combined lineage graph from all stories. Creates a unified view of data flow across pipelines.",
          "title": "Generate Lineage",
          "type": "boolean"
        }
      },
      "required": [
        "connection",
        "path"
      ],
      "title": "StoryConfig",
      "type": "object"
    },
    "StreamingWriteConfig": {
      "description": "Configuration for Spark Structured Streaming writes.\n\n### \ud83d\ude80 \"Real-Time Pipeline\" Guide\n\n**Business Problem:**\n\"I need to process data continuously as it arrives from Kafka/Event Hubs\nand write it to Delta Lake in near real-time.\"\n\n**The Solution:**\nConfigure streaming write with checkpoint location for fault tolerance\nand trigger interval for processing frequency.\n\n**Recipe: Streaming Ingestion**\n```yaml\nwrite:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_stream\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_stream\"\n    trigger:\n      processing_time: \"10 seconds\"\n```\n\n**Recipe: One-Time Streaming (Batch-like)**\n```yaml\nwrite:\n  connection: \"silver_lake\"\n  format: \"delta\"\n  table: \"events_batch\"\n  streaming:\n    output_mode: append\n    checkpoint_location: \"/checkpoints/events_batch\"\n    trigger:\n      available_now: true\n```",
      "properties": {
        "output_mode": {
          "default": "append",
          "description": "Output mode for streaming writes. 'append' - Only new rows. 'update' - Updated rows only. 'complete' - Entire result table (requires aggregation).",
          "enum": [
            "append",
            "update",
            "complete"
          ],
          "title": "Output Mode",
          "type": "string"
        },
        "checkpoint_location": {
          "description": "Path for streaming checkpoints. Required for fault tolerance. Must be a reliable storage location (e.g., cloud storage, DBFS).",
          "title": "Checkpoint Location",
          "type": "string"
        },
        "trigger": {
          "anyOf": [
            {
              "$ref": "#/$defs/TriggerConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Trigger configuration. If not specified, processes data as fast as possible. Use 'processing_time' for micro-batch intervals, 'once' for single batch, 'available_now' for processing all available data then stopping."
        },
        "query_name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Name for the streaming query (useful for monitoring and debugging)",
          "title": "Query Name"
        },
        "await_termination": {
          "anyOf": [
            {
              "type": "boolean"
            },
            {
              "type": "null"
            }
          ],
          "default": false,
          "description": "Wait for the streaming query to terminate. Set to True for batch-like streaming with 'once' or 'available_now' triggers.",
          "title": "Await Termination"
        },
        "timeout_seconds": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Timeout in seconds when await_termination is True. If None, waits indefinitely.",
          "title": "Timeout Seconds"
        }
      },
      "required": [
        "checkpoint_location"
      ],
      "title": "StreamingWriteConfig",
      "type": "object"
    },
    "SystemConfig": {
      "description": "Configuration for the Odibi System Catalog (The Brain).\n\nStores metadata, state, and pattern configurations.",
      "properties": {
        "connection": {
          "description": "Connection to store system tables (e.g., 'adls_bronze')",
          "title": "Connection",
          "type": "string"
        },
        "path": {
          "default": "_odibi_system",
          "description": "Path relative to connection root",
          "title": "Path",
          "type": "string"
        }
      },
      "required": [
        "connection"
      ],
      "title": "SystemConfig",
      "type": "object"
    },
    "TimeTravelConfig": {
      "description": "Configuration for time travel reading (Delta/Iceberg).\n\nExample:\n```yaml\ntime_travel:\n  as_of_version: 10\n  # OR\n  as_of_timestamp: \"2023-10-01T12:00:00Z\"\n```",
      "properties": {
        "as_of_version": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Version number to time travel to",
          "title": "As Of Version"
        },
        "as_of_timestamp": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Timestamp string to time travel to",
          "title": "As Of Timestamp"
        }
      },
      "title": "TimeTravelConfig",
      "type": "object"
    },
    "TransformConfig": {
      "description": "Configuration for transformation steps within a node.\n\n**When to Use:** Custom business logic, data cleaning, SQL transformations.\n\n**Key Concepts:**\n- `steps`: Ordered list of operations (SQL, functions, or both)\n- Each step receives the DataFrame from the previous step\n- Steps execute in order: step1 \u2192 step2 \u2192 step3\n\n**See Also:** [Transformer Catalog](#nodeconfig)\n\n**Transformer vs Transform:**\n- `transformer`: Single heavy operation (scd2, merge, deduplicate)\n- `transform.steps`: Chain of lighter operations\n\n### \ud83d\udd27 \"Transformation Pipeline\" Guide\n\n**Business Problem:**\n\"I have complex logic that mixes SQL for speed and Python for complex calculations.\"\n\n**The Solution:**\nChain multiple steps together. Output of Step 1 becomes input of Step 2.\n\n**Function Registry:**\nThe `function` step type looks up functions registered with `@transform` (or `@register`).\nThis allows you to use the *same* registered functions as both top-level Transformers and steps in a chain.\n\n**Recipe: The Mix-and-Match**\n```yaml\ntransform:\n  steps:\n    # Step 1: SQL Filter (Fast)\n    - sql: \"SELECT * FROM df WHERE status = 'ACTIVE'\"\n\n    # Step 2: Custom Python Function (Complex Logic)\n    # Looks up 'calculate_lifetime_value' in the registry\n    - function: \"calculate_lifetime_value\"\n      params: { discount_rate: 0.05 }\n\n    # Step 3: Built-in Operation (Standard)\n    - operation: \"drop_duplicates\"\n      params: { subset: [\"user_id\"] }\n```",
      "properties": {
        "steps": {
          "description": "List of transformation steps (SQL strings or TransformStep configs)",
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/$defs/TransformStep"
              }
            ]
          },
          "title": "Steps",
          "type": "array"
        }
      },
      "required": [
        "steps"
      ],
      "title": "TransformConfig",
      "type": "object"
    },
    "TransformStep": {
      "description": "Single transformation step.\n\nSupports four step types (exactly one required):\n\n* `sql` - Inline SQL query string\n* `sql_file` - Path to external .sql file (relative to the YAML file defining the node)\n* `function` - Registered Python function name\n* `operation` - Built-in operation (e.g., drop_duplicates)\n\n**sql_file Example:**\n\nIf your project structure is:\n```\nproject.yaml              # imports pipelines/silver/silver.yaml\npipelines/\n  silver/\n    silver.yaml           # defines the node\n    sql/\n      transform.sql       # your SQL file\n```\n\nIn `silver.yaml`, use a path relative to `silver.yaml`:\n```yaml\ntransform:\n  steps:\n    - sql_file: sql/transform.sql   # relative to silver.yaml\n```\n\n**Important:** The path is resolved relative to the YAML file where the node is defined,\nNOT the project.yaml that imports it. Do NOT use absolute paths like `/pipelines/silver/sql/...`.",
      "properties": {
        "sql": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Inline SQL query. Use `df` to reference the current DataFrame.",
          "title": "Sql"
        },
        "sql_file": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Path to external .sql file, relative to the YAML file defining the node. Example: 'sql/transform.sql' resolves relative to the node's source YAML.",
          "title": "Sql File"
        },
        "function": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Name of a registered Python function (@transform or @register).",
          "title": "Function"
        },
        "operation": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Built-in operation name (e.g., drop_duplicates, fill_na).",
          "title": "Operation"
        },
        "params": {
          "description": "Parameters to pass to function or operation.",
          "title": "Params",
          "type": "object"
        }
      },
      "title": "TransformStep",
      "type": "object"
    },
    "TriggerConfig": {
      "description": "Configuration for streaming trigger intervals.\n\nSpecify exactly one of the trigger options.\n\nExample:\n```yaml\ntrigger:\n  processing_time: \"10 seconds\"\n```\n\nOr for one-time processing:\n```yaml\ntrigger:\n  once: true\n```",
      "properties": {
        "processing_time": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Trigger interval as duration string (e.g., '10 seconds', '1 minute')",
          "title": "Processing Time"
        },
        "once": {
          "anyOf": [
            {
              "type": "boolean"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Process all available data once and stop",
          "title": "Once"
        },
        "available_now": {
          "anyOf": [
            {
              "type": "boolean"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Process all available data in multiple batches, then stop",
          "title": "Available Now"
        },
        "continuous": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Continuous processing with checkpoint interval (e.g., '1 second')",
          "title": "Continuous"
        }
      },
      "title": "TriggerConfig",
      "type": "object"
    },
    "UniqueTest": {
      "description": "Ensures specified columns (or combination) contain unique values.\n\n**When to Use:** Primary keys, natural keys, deduplication verification.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates)\n\n```yaml\ncontracts:\n  - type: unique\n    columns: [order_id]  # Single column\n  # OR composite key:\n  - type: unique\n    columns: [customer_id, order_date]  # Composite uniqueness\n```",
      "properties": {
        "type": {
          "const": "unique",
          "default": "unique",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "columns": {
          "description": "Columns that must be unique (composite key if multiple)",
          "items": {
            "type": "string"
          },
          "title": "Columns",
          "type": "array"
        }
      },
      "required": [
        "columns"
      ],
      "title": "UniqueTest",
      "type": "object"
    },
    "ValidationAction": {
      "enum": [
        "fail",
        "warn"
      ],
      "title": "ValidationAction",
      "type": "string"
    },
    "ValidationConfig": {
      "description": "Configuration for data validation (post-transform checks).\n\n**When to Use:** Output data quality checks that run after transformation but before writing.\n\n**See Also:** Validation Guide, Quarantine Guide, Contracts Overview (pre-transform checks)\n\n### \ud83d\udee1\ufe0f \"The Indestructible Pipeline\" Pattern\n\n**Business Problem:**\n\"Bad data polluted our Gold reports, causing executives to make wrong decisions. We need to stop it *before* it lands.\"\n\n**The Solution:**\nA Quality Gate that runs *after* transformation but *before* writing.\n\n**Recipe: The Quality Gate**\n```yaml\nvalidation:\n  mode: \"fail\"          # fail (stop pipeline) or warn (log only)\n  on_fail: \"alert\"      # alert or ignore\n\n  tests:\n    # 1. Completeness\n    - type: \"not_null\"\n      columns: [\"transaction_id\", \"customer_id\"]\n\n    # 2. Integrity\n    - type: \"unique\"\n      columns: [\"transaction_id\"]\n\n    - type: \"accepted_values\"\n      column: \"status\"\n      values: [\"PENDING\", \"COMPLETED\", \"FAILED\"]\n\n    # 3. Ranges & Patterns\n    - type: \"range\"\n      column: \"age\"\n      min: 18\n      max: 120\n\n    - type: \"regex_match\"\n      column: \"email\"\n      pattern: \"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n\n    # 4. Business Logic (SQL)\n    - type: \"custom_sql\"\n      name: \"dates_ordered\"\n      condition: \"created_at <= completed_at\"\n      threshold: 0.01   # Allow 1% failure\n```\n\n**Recipe: Quarantine + Gate**\n```yaml\nvalidation:\n  tests:\n    - type: not_null\n      columns: [customer_id]\n      on_fail: quarantine\n  quarantine:\n    connection: silver\n    path: customers_quarantine\n  gate:\n    require_pass_rate: 0.95\n    on_fail: abort\n```",
      "properties": {
        "mode": {
          "allOf": [
            {
              "$ref": "#/$defs/ValidationAction"
            }
          ],
          "default": "fail",
          "description": "Execution mode: 'fail' (stop pipeline) or 'warn' (log only)"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/OnFailAction"
            }
          ],
          "default": "alert",
          "description": "Action on failure: 'alert' (send notification) or 'ignore'"
        },
        "tests": {
          "description": "List of validation tests",
          "items": {
            "discriminator": {
              "mapping": {
                "accepted_values": "#/$defs/AcceptedValuesTest",
                "custom_sql": "#/$defs/CustomSQLTest",
                "distribution": "#/$defs/DistributionContract",
                "freshness": "#/$defs/FreshnessContract",
                "not_null": "#/$defs/NotNullTest",
                "range": "#/$defs/RangeTest",
                "regex_match": "#/$defs/RegexMatchTest",
                "row_count": "#/$defs/RowCountTest",
                "schema": "#/$defs/SchemaContract",
                "unique": "#/$defs/UniqueTest",
                "volume_drop": "#/$defs/VolumeDropTest"
              },
              "propertyName": "type"
            },
            "oneOf": [
              {
                "$ref": "#/$defs/NotNullTest"
              },
              {
                "$ref": "#/$defs/UniqueTest"
              },
              {
                "$ref": "#/$defs/AcceptedValuesTest"
              },
              {
                "$ref": "#/$defs/RowCountTest"
              },
              {
                "$ref": "#/$defs/CustomSQLTest"
              },
              {
                "$ref": "#/$defs/RangeTest"
              },
              {
                "$ref": "#/$defs/RegexMatchTest"
              },
              {
                "$ref": "#/$defs/VolumeDropTest"
              },
              {
                "$ref": "#/$defs/SchemaContract"
              },
              {
                "$ref": "#/$defs/DistributionContract"
              },
              {
                "$ref": "#/$defs/FreshnessContract"
              }
            ]
          },
          "title": "Tests",
          "type": "array"
        },
        "quarantine": {
          "anyOf": [
            {
              "$ref": "#/$defs/QuarantineConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Quarantine configuration for failed rows"
        },
        "gate": {
          "anyOf": [
            {
              "$ref": "#/$defs/GateConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Quality gate configuration for batch-level validation"
        },
        "fail_fast": {
          "default": false,
          "description": "Stop validation on first failure. Skips remaining tests for faster feedback.",
          "title": "Fail Fast",
          "type": "boolean"
        },
        "cache_df": {
          "default": false,
          "description": "Cache DataFrame before validation (Spark only). Improves performance with many tests.",
          "title": "Cache Df",
          "type": "boolean"
        }
      },
      "title": "ValidationConfig",
      "type": "object"
    },
    "ValidationMode": {
      "description": "Validation execution mode.",
      "enum": [
        "lazy",
        "eager"
      ],
      "title": "ValidationMode",
      "type": "string"
    },
    "VolumeDropTest": {
      "description": "Checks if row count dropped significantly compared to history.\n\n**When to Use:** Detect source outages, partial loads, or data pipeline issues.\n\n**See Also:** [Contracts Overview](#contracts-data-quality-gates), [RowCountTest](#rowcounttest)\n\nFormula: `(current - avg) / avg < -threshold`\n\n```yaml\ncontracts:\n  - type: volume_drop\n    threshold: 0.5  # Fail if > 50% drop from 7-day average\n    lookback_days: 7\n```",
      "properties": {
        "type": {
          "const": "volume_drop",
          "default": "volume_drop",
          "title": "Type"
        },
        "name": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Optional name for the check",
          "title": "Name"
        },
        "on_fail": {
          "allOf": [
            {
              "$ref": "#/$defs/ContractSeverity"
            }
          ],
          "default": "fail",
          "description": "Action on failure"
        },
        "threshold": {
          "default": 0.5,
          "description": "Max allowed drop (0.5 = 50% drop)",
          "title": "Threshold",
          "type": "number"
        },
        "lookback_days": {
          "default": 7,
          "description": "Days of history to average",
          "title": "Lookback Days",
          "type": "integer"
        }
      },
      "title": "VolumeDropTest",
      "type": "object"
    },
    "WriteConfig": {
      "description": "Configuration for writing data from a node.\n\n**When to Use:** Any node that persists data to storage.\n\n**Key Concepts:**\n- `mode`: How to handle existing data (overwrite, append, upsert)\n- `keys`: Required for upsert mode - columns that identify unique records\n- `partition_by`: Columns to partition output by (improves query performance)\n\n**See Also:**\n- [Performance Tuning](../guides/performance_tuning.md) - Partitioning strategies\n\n### \ud83d\ude80 \"Big Data Performance\" Guide\n\n**Business Problem:**\n\"My dashboards are slow because the query scans terabytes of data just to find one day's sales.\"\n\n**The Solution:**\nUse **Partitioning** for coarse filtering (skipping huge chunks) and **Z-Ordering** for fine-grained skipping (colocating related data).\n\n**Recipe: Lakehouse Optimized**\n```yaml\nwrite:\n  connection: \"gold_lake\"\n  format: \"delta\"\n  table: \"fact_sales\"\n  mode: \"append\"\n\n  # 1. Partitioning: Physical folders.\n  # Use for low-cardinality columns often used in WHERE clauses.\n  # WARNING: Do NOT partition by high-cardinality cols like ID or Timestamp!\n  partition_by: [\"country_code\", \"txn_year_month\"]\n\n  # 2. Z-Ordering: Data clustering.\n  # Use for high-cardinality columns often used in JOINs or predicates.\n  zorder_by: [\"customer_id\", \"product_id\"]\n\n  # 3. Table Properties: Engine tuning.\n  table_properties:\n    \"delta.autoOptimize.optimizeWrite\": \"true\"\n    \"delta.autoOptimize.autoCompact\": \"true\"\n```",
      "properties": {
        "connection": {
          "description": "Connection name from project.yaml",
          "title": "Connection",
          "type": "string"
        },
        "format": {
          "anyOf": [
            {
              "$ref": "#/$defs/ReadFormat"
            },
            {
              "type": "string"
            }
          ],
          "description": "Output format (csv, parquet, delta, etc.)",
          "title": "Format"
        },
        "table": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Table name for SQL/Delta",
          "title": "Table"
        },
        "path": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Path for file-based outputs",
          "title": "Path"
        },
        "register_table": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Register file output as external table (Spark/Delta only)",
          "title": "Register Table"
        },
        "mode": {
          "allOf": [
            {
              "$ref": "#/$defs/WriteMode"
            }
          ],
          "default": "overwrite",
          "description": "Write mode. Options: 'overwrite', 'append', 'upsert', 'append_once'"
        },
        "partition_by": {
          "description": "List of columns to physically partition the output by (folder structure). Use for low-cardinality columns (e.g. date, country).",
          "items": {
            "type": "string"
          },
          "title": "Partition By",
          "type": "array"
        },
        "zorder_by": {
          "description": "List of columns to Z-Order by. Improves read performance for high-cardinality columns used in filters/joins (Delta only).",
          "items": {
            "type": "string"
          },
          "title": "Zorder By",
          "type": "array"
        },
        "table_properties": {
          "additionalProperties": {
            "type": "string"
          },
          "description": "Delta table properties. Overrides global performance.delta_table_properties. Example: {'delta.columnMapping.mode': 'name'} to allow special characters in column names.",
          "title": "Table Properties",
          "type": "object"
        },
        "merge_schema": {
          "default": false,
          "description": "Allow schema evolution (mergeSchema option in Delta)",
          "title": "Merge Schema",
          "type": "boolean"
        },
        "first_run_query": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "SQL query for full-load on first run (High Water Mark pattern). If set, uses this query when target table doesn't exist, then switches to incremental. Only applies to SQL reads.",
          "title": "First Run Query"
        },
        "options": {
          "description": "Format-specific options",
          "title": "Options",
          "type": "object"
        },
        "auto_optimize": {
          "anyOf": [
            {
              "type": "boolean"
            },
            {
              "$ref": "#/$defs/AutoOptimizeConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Auto-run OPTIMIZE and VACUUM after write (Delta only)",
          "title": "Auto Optimize"
        },
        "add_metadata": {
          "anyOf": [
            {
              "type": "boolean"
            },
            {
              "$ref": "#/$defs/WriteMetadataConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Add metadata columns for Bronze layer lineage. Set to `true` to add all applicable columns, or provide a WriteMetadataConfig for selective columns. Columns: _extracted_at, _source_file (file sources), _source_connection, _source_table (SQL sources).",
          "title": "Add Metadata"
        },
        "skip_if_unchanged": {
          "default": false,
          "description": "Skip write if DataFrame content is identical to previous write. Computes SHA256 hash of entire DataFrame and compares to stored hash in Delta table metadata. Useful for snapshot tables without timestamps to avoid redundant appends. Only supported for Delta format.",
          "title": "Skip If Unchanged",
          "type": "boolean"
        },
        "skip_hash_columns": {
          "anyOf": [
            {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Columns to include in hash computation for skip_if_unchanged. If None, all columns are used. Specify a subset to ignore volatile columns like timestamps.",
          "title": "Skip Hash Columns"
        },
        "skip_hash_sort_columns": {
          "anyOf": [
            {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Columns to sort by before hashing for deterministic comparison. Required if row order may vary between runs. Typically your business key columns.",
          "title": "Skip Hash Sort Columns"
        },
        "streaming": {
          "anyOf": [
            {
              "$ref": "#/$defs/StreamingWriteConfig"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Streaming write configuration for Spark Structured Streaming. When set, uses writeStream instead of batch write. Requires a streaming DataFrame from a streaming read source."
        },
        "merge_keys": {
          "anyOf": [
            {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Key columns for SQL Server MERGE operations. Required when mode='merge'. These columns form the ON clause of the MERGE statement.",
          "title": "Merge Keys"
        },
        "merge_options": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerMergeOptions"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Options for SQL Server MERGE operations (conditions, staging, audit cols)"
        },
        "overwrite_options": {
          "anyOf": [
            {
              "$ref": "#/$defs/SqlServerOverwriteOptions"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Options for SQL Server overwrite operations (strategy, audit cols)"
        }
      },
      "required": [
        "connection",
        "format"
      ],
      "title": "WriteConfig",
      "type": "object"
    },
    "WriteMetadataConfig": {
      "description": "Configuration for metadata columns added during Bronze writes.\n\n### \ud83d\udccb Bronze Metadata Guide\n\n**Business Problem:**\n\"We need lineage tracking and debugging info for our Bronze layer data.\"\n\n**The Solution:**\nAdd metadata columns during ingestion for traceability.\n\n**Recipe 1: Add All Metadata (Recommended)**\n```yaml\nwrite:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata: true  # adds all applicable columns\n```\n\n**Recipe 2: Selective Metadata**\n```yaml\nwrite:\n  connection: bronze\n  table: customers\n  mode: append\n  add_metadata:\n    extracted_at: true\n    source_file: true\n    source_connection: false\n    source_table: false\n```\n\n**Available Columns:**\n- `_extracted_at`: Pipeline execution timestamp (all sources)\n- `_source_file`: Source filename/path (file sources only)\n- `_source_connection`: Connection name used (all sources)\n- `_source_table`: Table or query name (SQL sources only)",
      "properties": {
        "extracted_at": {
          "default": true,
          "description": "Add _extracted_at column with pipeline execution timestamp",
          "title": "Extracted At",
          "type": "boolean"
        },
        "source_file": {
          "default": true,
          "description": "Add _source_file column with source filename (file sources only)",
          "title": "Source File",
          "type": "boolean"
        },
        "source_connection": {
          "default": false,
          "description": "Add _source_connection column with connection name",
          "title": "Source Connection",
          "type": "boolean"
        },
        "source_table": {
          "default": false,
          "description": "Add _source_table column with table/query name (SQL sources only)",
          "title": "Source Table",
          "type": "boolean"
        }
      },
      "title": "WriteMetadataConfig",
      "type": "object"
    },
    "WriteMode": {
      "description": "Write modes for output operations.",
      "enum": [
        "overwrite",
        "append",
        "upsert",
        "append_once",
        "merge"
      ],
      "title": "WriteMode",
      "type": "string"
    }
  },
  "description": "Complete project configuration from YAML.\n\n### \ud83c\udfe2 \"Enterprise Setup\" Guide\n\n**Business Problem:**\n\"We need a robust production environment with alerts, retries, and proper logging.\"\n\n**Recipe: Production Ready**\n```yaml\nproject: \"Customer360\"\nengine: \"spark\"\n\n# 1. Resilience\nretry:\n    enabled: true\n    max_attempts: 3\n    backoff: \"exponential\"\n\n# 2. Observability\nlogging:\n    level: \"INFO\"\n    structured: true  # JSON logs for Splunk/Datadog\n\n# 3. Alerting\nalerts:\n    - type: \"slack\"\n    url: \"${SLACK_WEBHOOK_URL}\"\n    on_events: [\"on_failure\"]\n\n# ... connections and pipelines ...\n```",
  "properties": {
    "project": {
      "description": "Project name",
      "title": "Project",
      "type": "string"
    },
    "engine": {
      "allOf": [
        {
          "$ref": "#/$defs/EngineType"
        }
      ],
      "default": "pandas",
      "description": "Execution engine"
    },
    "connections": {
      "additionalProperties": {
        "anyOf": [
          {
            "$ref": "#/$defs/LocalConnectionConfig"
          },
          {
            "$ref": "#/$defs/AzureBlobConnectionConfig"
          },
          {
            "$ref": "#/$defs/DeltaConnectionConfig"
          },
          {
            "$ref": "#/$defs/SQLServerConnectionConfig"
          },
          {
            "$ref": "#/$defs/HttpConnectionConfig"
          },
          {
            "$ref": "#/$defs/CustomConnectionConfig"
          }
        ]
      },
      "description": "Named connections (at least one required)",
      "title": "Connections",
      "type": "object"
    },
    "pipelines": {
      "description": "Pipeline definitions (at least one required)",
      "items": {
        "$ref": "#/$defs/PipelineConfig"
      },
      "title": "Pipelines",
      "type": "array"
    },
    "story": {
      "allOf": [
        {
          "$ref": "#/$defs/StoryConfig"
        }
      ],
      "description": "Story generation configuration (mandatory)"
    },
    "system": {
      "allOf": [
        {
          "$ref": "#/$defs/SystemConfig"
        }
      ],
      "description": "System Catalog configuration (mandatory)"
    },
    "lineage": {
      "anyOf": [
        {
          "$ref": "#/$defs/LineageConfig"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "OpenLineage configuration"
    },
    "description": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Project description",
      "title": "Description"
    },
    "version": {
      "default": "1.0.0",
      "description": "Project version",
      "title": "Version",
      "type": "string"
    },
    "owner": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Project owner/contact",
      "title": "Owner"
    },
    "vars": {
      "description": "Global variables for substitution (e.g. ${vars.env})",
      "title": "Vars",
      "type": "object"
    },
    "retry": {
      "$ref": "#/$defs/RetryConfig"
    },
    "logging": {
      "$ref": "#/$defs/LoggingConfig"
    },
    "alerts": {
      "description": "Alert configurations",
      "items": {
        "$ref": "#/$defs/AlertConfig"
      },
      "title": "Alerts",
      "type": "array"
    },
    "performance": {
      "allOf": [
        {
          "$ref": "#/$defs/PerformanceConfig"
        }
      ],
      "description": "Performance tuning"
    },
    "environments": {
      "anyOf": [
        {
          "additionalProperties": {
            "type": "object"
          },
          "type": "object"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Structure: same as ProjectConfig but with only overridden fields. Not yet validated strictly.",
      "title": "Environments"
    },
    "semantic": {
      "anyOf": [
        {
          "type": "object"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "description": "Semantic layer configuration. Can be inline or reference external file. Contains metrics, dimensions, and materializations for self-service analytics. Example: semantic: { config: 'semantic_config.yaml' } or inline definitions.",
      "title": "Semantic"
    }
  },
  "required": [
    "project",
    "connections",
    "pipelines",
    "story",
    "system"
  ],
  "title": "ProjectConfig",
  "type": "object",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://raw.githubusercontent.com/henryodibi11/Odibi/main/docs/schemas/odibi.json"
}
