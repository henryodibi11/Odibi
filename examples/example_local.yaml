# Example: Local Pandas Pipeline
# This is a minimal configuration showing ODIBI's core features with the Pandas engine.
# Perfect for learning, local development, and testing.

project: Local Pandas Example
engine: pandas  # Uses Pandas for all operations (default)

# Connections define where data lives
connections:
  local:
    type: local
    base_path: ./data  # Relative to where you run odibi

# Pipelines contain the actual data transformation logic
pipelines:
  - pipeline: bronze_to_silver
    layer: transformation
    nodes:
      # Node 1: Load raw data from CSV (Bronze layer)
      - name: load_raw_sales
        read:
          connection: local
          path: bronze/sales.csv
          format: csv
          options:
            header: true
            dtype:
              transaction_id: str
              amount: float
        cache: true  # Cache in memory for reuse

      # Node 2: Clean and validate data (Silver layer)
      - name: clean_sales
        depends_on: [load_raw_sales]
        transform:
          steps:
            # SQL transforms run against cached DataFrames
            - |
              SELECT
                transaction_id,
                customer_id,
                product_id,
                amount,
                transaction_date
              FROM load_raw_sales
              WHERE amount > 0  -- Remove invalid transactions
                AND transaction_date IS NOT NULL

      # Node 3: Save cleaned data as Parquet
      - name: save_silver
        depends_on: [clean_sales]
        write:
          connection: local
          path: silver/sales.parquet
          format: parquet
          mode: overwrite

  # Second pipeline: Aggregation (Silver â†’ Gold)
  - pipeline: silver_to_gold
    layer: aggregation
    nodes:
      # Node 1: Load cleaned data
      - name: load_silver_sales
        read:
          connection: local
          path: silver/sales.parquet
          format: parquet
        cache: true

      # Node 2: Aggregate by customer
      - name: customer_summary
        depends_on: [load_silver_sales]
        transform:
          steps:
            - |
              SELECT
                customer_id,
                COUNT(*) as transaction_count,
                SUM(amount) as total_spent,
                AVG(amount) as avg_transaction,
                MAX(transaction_date) as last_purchase_date
              FROM load_silver_sales
              GROUP BY customer_id
              HAVING total_spent > 100  -- Only customers with >$100 spend

      # Node 3: Write analytics-ready data (Gold layer)
      - name: save_gold
        depends_on: [customer_summary]
        write:
          connection: local
          path: gold/customer_summary.parquet
          format: parquet
          mode: overwrite

# How to run this example:
#
# 1. Prepare sample data:
#    mkdir -p data/bronze
#    echo "transaction_id,customer_id,product_id,amount,transaction_date" > data/bronze/sales.csv
#    echo "T001,C001,P001,50.00,2024-01-15" >> data/bronze/sales.csv
#    echo "T002,C001,P002,75.50,2024-01-20" >> data/bronze/sales.csv
#    echo "T003,C002,P001,120.00,2024-01-22" >> data/bronze/sales.csv
#
# 2. Run the pipeline:
#    python -m odibi.cli run examples/example_local.yaml
#
# 3. Check outputs:
#    ls data/silver/  # Should contain sales.parquet
#    ls data/gold/    # Should contain customer_summary.parquet
#
# 4. View the story (execution report):
#    Generated automatically in stories/ directory
