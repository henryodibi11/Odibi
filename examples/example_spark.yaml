# Example: Spark Pipeline with Azure Sources
# This configuration demonstrates ODIBI's Spark engine with Azure Data Lake Storage.
# Phase 1: Template only - full execution coming in Phase 3

project: Spark Azure Pipeline
engine: spark  # Requires: pip install "odibi[spark]"

# Azure connections for production data sources
connections:
  # Azure Data Lake Storage Gen2 (production)
  adls_bronze:
    type: azure_adls
    account: mystorageaccount
    container: datalake
    path_prefix: bronze/
    auth_mode: managed_identity  # Uses cluster's managed identity

  adls_silver:
    type: azure_adls
    account: mystorageaccount
    container: datalake
    path_prefix: silver/
    auth_mode: managed_identity

  # Local DBFS for Databricks development/testing
  dbfs:
    type: local_dbfs
    root: /dbfs/FileStore/odibi_data

  # Azure SQL for dimensional data
  azure_sql:
    type: azure_sql
    server: myserver.database.windows.net
    database: analytics_db
    # Uses managed identity - no credentials needed

# Multi-source ETL pipeline
pipelines:
  - name: manufacturing_etl
    layer: transformation
    nodes:
      # Load sensor data from ADLS (Parquet format)
      - name: load_sensor_data
        read:
          connection: adls_bronze
          path: sensors/2024/**/*.parquet  # Partitioned by year
          format: parquet
          options:
            mergeSchema: true  # Handle schema evolution
        cache: true

      # Load product metadata from Azure SQL
      - name: load_product_dim
        read:
          connection: azure_sql
          table: dim_product
          options:
            partitionColumn: product_id
            numPartitions: 10
        cache: true

      # Join and transform (Spark SQL)
      - name: enrich_sensor_data
        depends_on: [load_sensor_data, load_product_dim]
        transform:
          steps:
            - |
              SELECT
                s.sensor_id,
                s.timestamp,
                s.temperature,
                s.pressure,
                s.vibration,
                p.product_name,
                p.product_category,
                p.manufacturing_line
              FROM load_sensor_data s
              LEFT JOIN load_product_dim p
                ON s.product_id = p.product_id
              WHERE s.timestamp >= current_date() - INTERVAL 7 DAYS

      # Calculate rolling aggregates (window functions)
      - name: calculate_metrics
        depends_on: [enrich_sensor_data]
        transform:
          steps:
            - |
              SELECT
                sensor_id,
                product_name,
                timestamp,
                temperature,
                AVG(temperature) OVER (
                  PARTITION BY sensor_id
                  ORDER BY timestamp
                  ROWS BETWEEN 10 PRECEDING AND CURRENT ROW
                ) as temp_rolling_avg,
                STDDEV(temperature) OVER (
                  PARTITION BY sensor_id
                  ORDER BY timestamp
                  ROWS BETWEEN 10 PRECEDING AND CURRENT ROW
                ) as temp_std_dev
              FROM enrich_sensor_data

      # Write to Silver layer (partitioned by date)
      - name: write_silver_sensor_metrics
        depends_on: [calculate_metrics]
        write:
          connection: adls_silver
          path: sensor_metrics
          format: parquet
          mode: overwrite
          options:
            partitionBy: date(timestamp)
            compression: snappy

  # Aggregation pipeline (Silver â†’ Gold)
  - name: manufacturing_analytics
    layer: aggregation
    nodes:
      # Load enriched sensor data
      - name: load_silver_metrics
        read:
          connection: adls_silver
          path: sensor_metrics
          format: parquet
        cache: true

      # Daily production summary
      - name: daily_summary
        depends_on: [load_silver_metrics]
        transform:
          steps:
            - |
              SELECT
                date(timestamp) as production_date,
                product_name,
                manufacturing_line,
                COUNT(*) as reading_count,
                AVG(temperature) as avg_temp,
                MAX(temperature) as max_temp,
                MIN(temperature) as min_temp,
                STDDEV(temperature) as temp_variability
              FROM load_silver_metrics
              GROUP BY
                date(timestamp),
                product_name,
                manufacturing_line

      # Write Gold analytics tables
      - name: write_gold_daily_summary
        depends_on: [daily_summary]
        write:
          connection: adls_silver
          path: ../gold/daily_production_summary  # Go up to gold folder
          format: delta  # Use Delta Lake for ACID transactions
          mode: append
          options:
            mergeSchema: true
            optimizeWrite: true

# Phase 1 Notes:
# - This is a template configuration showing Spark syntax
# - Actual Spark read/write/transform will be implemented in Phase 3
# - Connection path resolution is already working (Phase 1 complete)
#
# To use in Phase 3:
# 1. Ensure cluster has access to Azure resources (managed identity)
# 2. Install ODIBI: pip install "odibi[spark,azure]"
# 3. Run: python -m odibi.cli run examples/example_spark.yaml
#
# Expected behavior:
# - Loads sensor data from ADLS bronze layer
# - Enriches with product dimensions from Azure SQL
# - Calculates rolling metrics with window functions
# - Writes partitioned Parquet to silver layer
# - Aggregates daily summaries to gold layer (Delta format)
