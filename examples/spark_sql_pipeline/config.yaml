# ============================================
# Example: End-to-End Spark SQL Pipeline
# ============================================
# Demonstrates:
# 1. Reading raw CSV data
# 2. Using SQL for complex transformations
# 3. Writing to Delta Lake (Parquet with transaction log)
#
# Run with: odibi run examples/spark_sql_pipeline/config.yaml
# ============================================

project: spark_demo
engine: spark
description: "Demonstrates SQL-heavy transformations using Spark engine"

# 1. Define Connections
connections:
  # Source data (Mock raw landing zone)
  raw_zone:
    type: local
    base_path: ./data/raw
    
  # Target data (Mock processed zone)
  processed_zone:
    type: local
    base_path: ./data/processed

# 2. Define Story (Execution Report)
story:
  connection: processed_zone
  path: stories/spark_demo
  auto_generate: true

# 3. Define Pipelines
pipelines:
  - pipeline: sales_analysis
    description: "Aggregates daily sales using SQL"
    nodes:
      # Step A: Ingest Raw Data
      - name: load_transactions
        description: "Load raw CSV transactions"
        read:
          connection: raw_zone
          path: transactions.csv
          format: csv
          options:
            header: true
            inferSchema: true

      # Step B: Filter & Clean (SQL)
      - name: filter_valid_sales
        description: "Remove refunds and invalid amounts"
        depends_on: [load_transactions]
        transform:
          steps:
            # Register temp view 'transactions' automatically from input
            - sql: |
                SELECT 
                  transaction_id,
                  customer_id,
                  product_id,
                  amount,
                  cast(transaction_date as date) as sale_date
                FROM load_transactions 
                WHERE amount > 0 
                  AND status = 'COMPLETED'

      # Step C: Aggregate (SQL)
      - name: daily_sales_summary
        description: "Calculate total sales per day"
        depends_on: [filter_valid_sales]
        transform:
          steps:
            - sql: |
                SELECT 
                  sale_date,
                  count(transaction_id) as total_transactions,
                  sum(amount) as total_revenue,
                  avg(amount) as avg_ticket_size
                FROM filter_valid_sales
                GROUP BY sale_date
                ORDER BY sale_date DESC

      # Step D: Write to Delta Table
      - name: save_summary
        description: "Save result to Delta table for downstream consumption"
        depends_on: [daily_sales_summary]
        write:
          connection: processed_zone
          path: sales_summary_delta
          format: delta
          mode: overwrite
          options:
            overwriteSchema: true
