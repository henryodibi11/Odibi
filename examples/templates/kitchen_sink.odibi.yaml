# =============================================================================
# ODIBI "ULTIMATE CHEATSHEET" & MASTER REFERENCE (v2.4.0)
# =============================================================================
# Welcome! This file controls your data pipeline.
# It is designed to be BOTH a "Zero to Hero" guide for beginners AND
# a comprehensive reference for advanced users.
#
# KEY CONCEPTS:
# 1. PIPELINE: A series of steps (like an assembly line).
# 2. NODE: A single step in that line (e.g., "Read", "Transform", "Save").
# 3. CONNECTION: A saved "address book" for where data lives.
# 4. OPERATION: A pre-built tool to modify data (like a "function").
#
# HOW TO USE THIS FILE:
# - Lines starting with '#' are comments.
# - Copy sections you need, delete the rest.
# - See the "STANDARD LIBRARY REFERENCE" at the bottom for all available operations.
# =============================================================================

# -----------------------------------------------------------------------------
# 1. PROJECT SETTINGS
# -----------------------------------------------------------------------------
project: my_master_project
version: "2.4.0"
description: "The ultimate reference pipeline"
owner: "Data Engineering Team"

# -----------------------------------------------------------------------------
# 2. THE ENGINE
# -----------------------------------------------------------------------------
# 'pandas': Fast, runs on laptop, good for <10GB files ("Sports Car").
# 'spark':  Powerful, runs on clusters, good for TBs ("Freight Train").
engine: pandas

# Performance Tuning
performance:
  # 'Arrow' makes reading files 2x faster and uses 50% less RAM.
  use_arrow: true

# Retry Policy (Resilience)
retry:
  enabled: true
  max_attempts: 3
  backoff: exponential  # 1s, 2s, 4s...

# Logging
logging:
  level: INFO
  structured: false     # Set 'true' for JSON logs (Splunk/Datadog)
  metadata: {env: "dev"}

# -----------------------------------------------------------------------------
# 3. CONNECTIONS (The Address Book)
# -----------------------------------------------------------------------------
connections:

  # A. Local Filesystem
  local_fs:
    type: local
    base_path: ./data

  # B. Delta Lake (Time Travel enabled storage)
  delta_lake:
    type: delta
    path: ./data/delta_tables

  # C. Azure Data Lake Storage Gen2 (ADLS)
  # azure_data:
  #   type: azure_blob
  #   account_name: ${AZURE_STORAGE_ACCOUNT}
  #   container: analytics
  #   # Auth: Managed Identity is automatic. Explicit keys below:
  #   # auth:
  #   #   account_key: ${AZURE_STORAGE_KEY}
  #   #   sas_token: ${AZURE_SAS_TOKEN}

  # D. SQL Server / Azure SQL
  # sql_db:
  #   type: sql_server
  #   host: ${DB_HOST}
  #   port: 1433
  #   database: warehouse
  #   auth:
  #     username: ${DB_USER}
  #     password: ${DB_PASS}

  # E. HTTP / API (Web Data)
  # web_api:
  #   type: http
  #   base_url: "https://api.example.com"
  #   headers: {Authorization: "Bearer ${API_TOKEN}"}

# -----------------------------------------------------------------------------
# 4. THE PIPELINE (Complex Example)
# -----------------------------------------------------------------------------
pipelines:
  - pipeline: master_demo
    description: "Demonstrates reading, transforming, merging, and validating"
    layer: silver
    
    nodes:
      # -----------------------------------------------------------------------
      # NODE 1: Ingest Raw Data
      # -----------------------------------------------------------------------
      - name: read_customers
        description: "Read CSV with specific options"
        read:
          connection: local_fs
          path: source/customers.csv
          format: csv
          options: {header: true, sep: ","}
        on_error: fail_fast

      # -----------------------------------------------------------------------
      # NODE 2: Transform & Clean
      # -----------------------------------------------------------------------
      - name: clean_customers
        depends_on: [read_customers]
        
        transform:
          steps:
            # 1. SQL Transformation (Flexible)
            - sql: |
                SELECT 
                  id, 
                  email, 
                  first_name, 
                  last_name, 
                  country,
                  signup_date,
                  amount
                FROM read_customers
                WHERE id IS NOT NULL

            # 2. Standard Library: Clean Text
            - operation: sql_core.clean_text
              params:
                columns: ["first_name", "last_name", "country"]
                case: "upper"   # JOHN -> JOHN
                trim: true      # " John " -> "John"

            # 3. Standard Library: Validate & Flag (Data Quality)
            - operation: advanced.validate_and_flag
              params:
                flag_col: "dq_issues"
                rules:
                  valid_email: "email LIKE '%@%'"
                  positive_amt: "amount >= 0"
            
            # 4. Standard Library: Deduplicate
            - operation: advanced.deduplicate
              params:
                keys: ["email"]
                order_by: "signup_date DESC" # Keep most recent

        # Privacy: Hide PII in logs
        sensitive: [email, first_name, last_name]

      # -----------------------------------------------------------------------
      # NODE 3: Merge (Upsert) to Delta
      # -----------------------------------------------------------------------
      - name: upsert_customers
        depends_on: [clean_customers]
        
        # "MERGE" strategy: Update existing IDs, Insert new ones.
        # This uses a top-level transformer logic.
        transformer: merge
        params:
          target: data/delta_tables/silver/customers
          keys: ["id"]
          strategy: upsert
          
          # Audit Columns (History tracking)
          audit_cols:
            created_col: "row_created_at"
            updated_col: "row_updated_at"

      # -----------------------------------------------------------------------
      # NODE 4: Aggregate & Validate
      # -----------------------------------------------------------------------
      - name: country_stats
        depends_on: [upsert_customers]
        
        transform:
          steps:
            - operation: relational.aggregate
              params:
                group_by: ["country"]
                aggregations:
                  amount: sum
                  id: count
        
        write:
          connection: local_fs
          path: gold/country_stats.parquet
          format: parquet
          mode: overwrite
          
        # Pipeline Validation Gates
        validation:
          not_empty: true
          no_nulls: [country]
          ranges: {amount: {min: 0}}

# -----------------------------------------------------------------------------
# 5. OBSERVABILITY
# -----------------------------------------------------------------------------
story:
  connection: local_fs
  path: odibi_stories/
  auto_generate: true
  max_sample_rows: 20
  retention_days: 30
  # Tracks schema changes, row counts, and data lineage visually.

# =============================================================================
# STANDARD LIBRARY REFERENCE (v2.4.0)
# =============================================================================
# Below is the complete list of available transformers.
# Copy-paste these into your 'transform: steps:' section as needed.
# =============================================================================

# --- SQL CORE (odibi.transformers.sql_core) ---

# - operation: sql_core.filter_rows
#   params: { condition: "age > 18 AND status = 'active'" }

# - operation: sql_core.derive_columns
#   params: { derivations: { full_name: "first_name || ' ' || last_name" } }

# - operation: sql_core.cast_columns
#   params: { casts: { id: "int", price: "float", active: "bool" } }

# - operation: sql_core.clean_text
#   params: { columns: ["city"], case: "upper", trim: true }

# - operation: sql_core.extract_date_parts
#   params: { source_col: "timestamp", parts: ["year", "month", "day"] }

# - operation: sql_core.normalize_schema
#   params: { rename: {old: new}, drop: [bad_col], select_order: [col1, col2] }

# - operation: sql_core.sort
#   params: { by: ["date"], ascending: false }

# - operation: sql_core.limit
#   params: { n: 100 }

# - operation: sql_core.distinct
#   params: { columns: ["category"] } # Optional: specific columns

# - operation: sql_core.fill_nulls
#   params: { values: { amount: 0.0, category: "Unknown" } }

# - operation: sql_core.split_part
#   params: { col: "email", delimiter: "@", index: 1 }

# - operation: sql_core.date_add
#   params: { col: "start_date", value: 1, unit: "day" }

# - operation: sql_core.date_diff
#   params: { start_col: "start", end_col: "end", unit: "day" }

# - operation: sql_core.case_when
#   params:
#     output_col: "segment"
#     cases: [{condition: "age < 18", value: "'Minor'"}, {condition: "age >= 18", value: "'Adult'"}]
#     default: "'Unknown'"

# - operation: sql_core.convert_timezone
#   params: { col: "ts", source_tz: "UTC", target_tz: "America/New_York" }

# - operation: sql_core.concat_columns
#   params: { columns: ["addr", "city"], separator: ", ", output_col: "full_address" }


# --- ADVANCED (odibi.transformers.advanced) ---

# - operation: advanced.deduplicate
#   params: { keys: ["id"], order_by: "updated_at DESC" }

# - operation: advanced.validate_and_flag
#   params: { rules: {is_positive: "amt > 0"}, flag_col: "issues" }

# - operation: advanced.explode_list_column
#   params: { column: "items", outer: true }

# - operation: advanced.regex_replace
#   params: { column: "text", pattern: "[0-9]+", replacement: "#" }

# - operation: advanced.hash_columns
#   params: { columns: ["email"], algorithm: "sha256" } # Anonymization

# - operation: advanced.generate_surrogate_key
#   params: { columns: ["customer_id", "region_id"], output_col: "row_key" }

# - operation: advanced.parse_json
#   params: { column: "json_str", json_schema: "id INT, name STRING" }

# - operation: advanced.window_calculation
#   params: { target_col: "running_total", function: "sum(amt)", partition_by: ["user"], order_by: "date" }


# --- RELATIONAL (odibi.transformers.relational) ---

# - operation: relational.join
#   params: { right_dataset: "other_node", on: "id", how: "left", prefix: "other" }

# - operation: relational.union
#   params: { datasets: ["node_a", "node_b"], by_name: true }

# - operation: relational.pivot
#   params: { group_by: ["region"], pivot_col: "category", agg_col: "sales", agg_func: "sum" }

# - operation: relational.unpivot
#   params: { id_cols: ["id"], value_vars: ["q1", "q2"], var_name: "quarter", value_name: "sales" }

# - operation: relational.aggregate
#   params: { group_by: ["region"], aggregations: { sales: "sum", id: "count" } }
