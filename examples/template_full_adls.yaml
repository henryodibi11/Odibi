# ODIBI Configuration - Multi-Account ADLS Example
# Phase 2A: Azure ADLS Integration with Key Vault
# Phase 2B: Delta Lake Support
# This example demonstrates reading from one storage account and writing to another

project: Multi-Account ADLS Pipeline
engine: pandas  # or 'spark' for large-scale processing

# Connection definitions - multi-account ADLS support
connections:
  # Bronze layer - raw data
  bronze:
    type: azure_adls
    account: mystorageaccount1
    container: bronze
    path_prefix: raw  # Optional: all paths will be prefixed with "raw/"
    auth_mode: key_vault  # Recommended for production
    auth:
      key_vault_name: company-keyvault
      secret_name: bronze-storage-key

  # Silver layer - cleaned data
  silver:
    type: azure_adls
    account: mystorageaccount2  # Different storage account
    container: silver
    path_prefix: cleaned
    auth_mode: key_vault
    auth:
      key_vault_name: company-keyvault
      secret_name: silver-storage-key

  # Local development (alternative - direct_key mode)
  # Uncomment for local testing
  # bronze_local:
  #   type: azure_adls
  #   account: mystorageaccount1
  #   container: bronze
  #   auth_mode: direct_key
  #   auth:
  #     account_key: "${BRONZE_STORAGE_KEY}"  # From environment variable

# Story configuration (where execution reports are saved)
story:
  connection: bronze  # Use ADLS for stories (or local for development)
  path: observability/stories/
  max_sample_rows: 10

# Pipeline definitions
pipelines:
  - pipeline: ingest_and_clean
    nodes:
      # Read CSV from bronze (account 1)
      - name: load_raw_sales
        read:
          connection: bronze
          path: sales/2024/sales_raw.csv
          format: csv
          options:
            sep: ","
            header: true

      # Transform - clean the data
      - name: clean_sales
        depends_on: [load_raw_sales]
        transform:
          steps:
            - "SELECT * FROM load_raw_sales WHERE amount > 0"
            - "SELECT *, UPPER(customer_name) AS customer_upper FROM clean_sales"  # Fixed: was clean_temp

      # Write Parquet to silver (account 2)
      - name: save_clean_sales
        depends_on: [clean_sales]
        write:
          connection: silver
          path: sales/2024/sales_cleaned.parquet
          format: parquet
          mode: overwrite
          options:
            compression: snappy

  # ========================================
  # Pipeline 2: Delta Lake on ADLS
  # ========================================
  - pipeline: delta_on_adls
    description: "Write Delta Lake tables to Azure storage"
    nodes:
      # Read from bronze
      - name: load_bronze_data
        read:
          connection: bronze
          path: sales/2024/sales_raw.csv
          format: csv
        cache: true
      
      # Clean data
      - name: prepare_for_delta
        depends_on: [load_bronze_data]
        transform:
          steps:
            - "SELECT * FROM load_bronze_data WHERE amount > 0 AND customer_id IS NOT NULL"
      
      # Write to Delta Lake on ADLS (Silver layer)
      - name: save_delta_on_adls
        description: "Save to Delta Lake with ACID transactions"
        depends_on: [prepare_for_delta]
        write:
          connection: silver
          path: sales/2024/sales_delta  # No .delta extension needed for ADLS
          format: delta  # Delta Lake format
          mode: append
          options:
            mergeSchema: true  # Allow schema evolution
            # partition_by: ['year', 'month']  # Optional: partition for large datasets
      
      # Read back with time travel
      - name: read_delta_version
        read:
          connection: silver
          path: sales/2024/sales_delta
          format: delta
          options:
            versionAsOf: 0  # Read first version (time travel)
        cache: true

# ============================================
# AUTHENTICATION MODES
# ============================================
# ADLS supports multiple authentication methods:
#
# 1. key_vault (Recommended for production):
#    auth_mode: key_vault
#    auth:
#      key_vault_name: my-keyvault
#      secret_name: storage-key
#
# 2. managed_identity (For Databricks/Azure VMs):
#    auth_mode: managed_identity
#    # No additional fields needed - uses cluster/VM identity
#
# 3. service_principal (For service accounts):
#    auth_mode: service_principal
#    auth:
#      tenant_id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
#      client_id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
#      client_secret: ${SERVICE_PRINCIPAL_SECRET}
#
# 4. direct_key (For local development only):
#    auth_mode: direct_key
#    auth:
#      account_key: ${STORAGE_ACCOUNT_KEY}

# ============================================
# SUPPORTED FORMATS WITH ADLS
# ============================================
# All formats work seamlessly with Azure storage:
# - csv: Comma-separated values
# - parquet: Columnar format (recommended for analytics)
# - json: JSON lines format (newline-delimited)
# - excel: Excel files (.xlsx)
# - avro: Apache Avro binary format
# - delta: Delta Lake tables with ACID transactions (Phase 2B ✅)

# ============================================
# HOW TO USE THIS TEMPLATE
# ============================================
# 1. Replace placeholders:
#    - mystorageaccount1/2 → your Azure storage account names
#    - company-keyvault → your Azure Key Vault name
#    - bronze-storage-key/silver-storage-key → your Key Vault secret names
#
# 2. Choose authentication mode:
#    - Production: use key_vault or managed_identity
#    - Development: use direct_key with environment variables
#
# 3. Update paths to match your data structure
#
# 4. Run:
#    odibi run template_full_adls.yaml
#
# For complete documentation, see:
# - docs/setup_azure.md - Azure authentication setup
# - docs/DELTA_LAKE_GUIDE.md - Delta Lake features
# - docs/CONFIGURATION_EXPLAINED.md - Full config reference
