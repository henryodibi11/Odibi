# ============================================
# ODIBI Complete Configuration Reference
# ============================================
# This template shows ALL available options.
# Mandatory fields are marked with (Required).
# Optional fields show their defaults explicitly.
# 
# For minimal example, see: examples/example_local.yaml
# For documentation, see: docs/CONFIGURATION_EXPLAINED.md

# ============================================
# PROJECT METADATA
# ============================================

project: My Data Pipeline  # (Required) Project name
description: "Complete ETL pipeline for sales data"  # (Optional) Human-readable description
version: "1.0.0"  # (Optional) Project version (default: "1.0.0")
owner: "data-engineering@company.com"  # (Optional) Project owner/contact
engine: pandas  # (Required) Execution engine: 'pandas' or 'spark'

# ============================================
# CONNECTIONS (Required - at least one)
# ============================================
# Every read/write/story operation uses a connection.
# Connections define WHERE data/artifacts live.

connections:
  # Example: Local filesystem for data (Phase 1 - Available Now)
  data:
    type: local
    base_path: ./data  # Base directory for all data paths
  
  # Example: Separate connection for outputs/observability
  outputs:
    type: local
    base_path: ./outputs  # Stories, logs, metadata
  
  # Example: Azure Data Lake Storage Gen2 (Phase 3 - Coming Soon)
  # azure_lake:
  #   type: azure_adls
  #   account: mystorageaccount
  #   container: data
  #   path_prefix: /project1
  #   auth_mode: managed_identity  # or 'service_principal', 'account_key'
  
  # Example: Azure SQL Database (Phase 3 - Coming Soon)
  # azure_db:
  #   type: azure_sql
  #   host: myserver.database.windows.net
  #   database: analytics
  #   port: 1433  # default
  #   auth_mode: managed_identity

# ============================================
# STORY GENERATION (Required)
# ============================================
# Stories are ODIBI's core value - execution reports with lineage.
# Must specify WHERE stories are saved (using a connection).

story:
  connection: outputs  # (Required) Connection name for story output
  path: stories/  # (Required) Path relative to connection base_path
                  # Result: ./outputs/stories/ (using 'outputs' connection)
  max_sample_rows: 10  # (Optional) Rows in data previews (default: 10, range: 0-100)
  auto_generate: true  # (Optional) Generate after each run (default: true)

# ============================================
# RETRY CONFIGURATION (Optional)
# ============================================
# Global retry settings for all operations.
# Can be overridden at node level in Phase 2.

retry:
  enabled: true  # (Optional) Enable retry logic (default: true)
  max_attempts: 3  # (Optional) Max retries (default: 3, range: 1-10)
  backoff: exponential  # (Optional) Strategy: 'exponential', 'linear', 'constant'

# ============================================
# LOGGING CONFIGURATION (Optional)
# ============================================
# Global logging settings.

logging:
  level: INFO  # (Optional) Log level: 'DEBUG', 'INFO', 'WARNING', 'ERROR' (default: INFO)
  structured: false  # (Optional) Output JSON logs for machine parsing (default: false)
  metadata:  # (Optional) Extra metadata in all log entries
    environment: development
    team: data-engineering

# ============================================
# ENVIRONMENT OVERRIDES (Phase 3 - Not Implemented)
# ============================================
# Override configurations per environment (dev/staging/prod).
# Attempting to use this will raise NotImplementedError.
# Uncomment when Phase 3 is released.

# environments:
#   dev:
#     story:
#       connection: outputs
#       path: dev_stories/
#     logging:
#       level: DEBUG
#   
#   prod:
#     story:
#       connection: azure_lake
#       path: prod/observability/stories/
#     logging:
#       level: WARNING
#       structured: true

# ============================================
# PIPELINES (Required - at least one)
# ============================================
# Define data transformation workflows.

pipelines:
  # ========================================
  # Pipeline 1: Bronze to Silver
  # ========================================
  - pipeline: bronze_to_silver  # (Required) Unique pipeline name
    description: "Clean and validate raw sales data"  # (Optional) Human-readable
    layer: transformation  # (Optional) Logical layer identifier
    
    nodes:
      # ====================================
      # Node: Read from CSV
      # ====================================
      - name: load_raw_sales  # (Required) Unique node name within pipeline
        description: "Load raw sales transactions"  # (Optional) Node description
        
        read:  # Read operation
          connection: data  # (Required) Connection name from connections section
          path: bronze/sales.csv  # (Required) Path relative to connection base_path
                                  # Result: ./data/bronze/sales.csv
          format: csv  # (Required) Format: 'csv', 'parquet', 'json', 'excel', 'avro'
          
          # Format-specific options (passed to pandas.read_csv or spark.read)
          options:  # (Optional)
            header: 0  # CSV has header row
            dtype:  # Specify column types
              transaction_id: str
              amount: float
            encoding: utf-8
        
        cache: true  # (Optional) Cache result in memory for reuse (default: false)
        
        # Validation (Optional) - Phase 2 feature
        # validation:
        #   not_empty: true
        #   no_nulls:
        #     - transaction_id
      
      # ====================================
      # Node: Transform with SQL
      # ====================================
      - name: clean_sales
        description: "Filter and clean sales data"
        depends_on: [load_raw_sales]  # (Optional) Wait for these nodes first
        
        transform:  # Transform operation
          steps:  # (Required) List of SQL queries or function calls
            # SQL query - references cached DataFrames by node name
            - |
              SELECT
                transaction_id,
                customer_id,
                product_id,
                amount,
                transaction_date
              FROM load_raw_sales
              WHERE amount > 0
                AND transaction_date IS NOT NULL
            
            # You can chain multiple SQL transforms
            # Each operates on the result of the previous
            # - |
            #   SELECT *,
            #     amount * 1.1 as amount_with_tax
            #   FROM clean_sales
      
      # ====================================
      # Node: Custom Python Transform
      # ====================================
      - name: add_metrics
        description: "Add custom business metrics"
        depends_on: [clean_sales]
        
        transform:
          steps:
            # Call registered Python function
            # Register with: @odibi.transform decorator
            - function: calculate_revenue_metrics
              params:  # Function parameters
                window_days: 30
                metric_type: revenue
        
        cache: true
      
      # ====================================
      # Node: Write to Parquet
      # ====================================
      - name: save_silver
        description: "Save cleaned data to silver layer"
        depends_on: [add_metrics]
        
        write:  # Write operation
          connection: data  # (Required) Connection name
          path: silver/sales.parquet  # (Required) Path relative to connection
                                      # Result: ./data/silver/sales.parquet
          format: parquet  # (Required) Format: 'csv', 'parquet', 'json', 'excel', 'avro'
          mode: overwrite  # (Required) Write mode: 'overwrite' or 'append'
          
          # Format-specific options
          options:  # (Optional)
            compression: snappy  # Parquet: snappy, gzip, brotli
            # partition_by: ['year', 'month']  # Spark only (Phase 3)

  # ========================================
  # Pipeline 2: Silver to Gold
  # ========================================
  - pipeline: silver_to_gold
    description: "Aggregate and enrich for analytics"
    layer: aggregation
    
    nodes:
      - name: load_silver
        read:
          connection: data
          path: silver/sales.parquet
          format: parquet
        cache: true
      
      - name: customer_summary
        depends_on: [load_silver]
        transform:
          steps:
            - |
              SELECT
                customer_id,
                COUNT(*) as transaction_count,
                SUM(amount) as total_spent,
                AVG(amount) as avg_transaction,
                MAX(transaction_date) as last_purchase_date
              FROM load_silver
              GROUP BY customer_id
              HAVING total_spent > 100
      
      - name: save_gold
        depends_on: [customer_summary]
        write:
          connection: data
          path: gold/customer_summary.parquet
          format: parquet
          mode: overwrite

# ============================================
# SUPPORTED FORMATS (Reference)
# ============================================
# Phase 1 (Available Now):
# - csv: Comma-separated values
# - parquet: Columnar format (recommended for large datasets)
# - json: JSON format
# - excel: Excel spreadsheets (.xlsx)
# - avro: Avro binary format (requires odibi[pandas])
#
# Phase 3 (Coming Soon):
# - delta: Delta Lake tables (requires odibi[spark])
# - sql: SQL database tables (requires connection-specific drivers)

# ============================================
# NODE OPERATIONS REFERENCE
# ============================================
# Every node must have at least ONE of:
# - read: Load data from connection
# - transform: Transform data (SQL or function)
# - write: Save data to connection
#
# A node can have ALL THREE:
# - read → transform → write (ETL pattern)
#
# Or combinations:
# - read only (load data, cache for downstream)
# - transform only (operate on depends_on data)
# - write only (save depends_on data)
# - read + transform (load and clean)
# - transform + write (clean and save)

# ============================================
# HOW TO USE THIS TEMPLATE
# ============================================
#
# 1. Copy this file to your project
# 2. Update project metadata (name, description, owner)
# 3. Configure connections (where your data lives)
# 4. Configure story output (where reports go)
# 5. Define pipelines (your transformation logic)
# 6. Remove optional sections you don't need
# 7. Run: python -c "from odibi.pipeline import Pipeline; Pipeline.from_yaml('config.yaml').run()"
#
# For minimal example: examples/example_local.yaml
# For documentation: docs/CONFIGURATION_EXPLAINED.md
# For walkthroughs: walkthroughs/01_local_pipeline_pandas.ipynb
