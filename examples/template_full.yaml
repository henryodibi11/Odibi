# ============================================
# ODIBI Complete Configuration Reference
# ============================================
# This template shows ALL available options.
# Mandatory fields are marked with (Required).
# Optional fields show their defaults explicitly.
# 
# For minimal example, see: examples/example_local.yaml
# For documentation, see: docs/CONFIGURATION_EXPLAINED.md

# ============================================
# PROJECT METADATA
# ============================================

project: My Data Pipeline  # (Required) Project name
description: "Complete ETL pipeline for sales data"  # (Optional) Human-readable description
version: "1.0.0"  # (Optional) Project version (default: "1.0.0")
owner: "data-engineering@company.com"  # (Optional) Project owner/contact
engine: pandas  # (Required) Execution engine: 'pandas' or 'spark'

# ============================================
# CONNECTIONS (Required - at least one)
# ============================================
# Every read/write/story operation uses a connection.
# Connections define WHERE data/artifacts live.

connections:
  # Example: Local filesystem for data (Phase 1 - Available Now)
  data:
    type: local
    base_path: ./data  # Base directory for all data paths
  
  # Example: Separate connection for outputs/observability
  outputs:
    type: local
    base_path: ./outputs  # Stories, logs, metadata
  
  # Example: Azure Data Lake Storage Gen2 (Phase 2A ✅)
  azure_lake:
    type: azure_adls
    account: mystorageaccount
    container: data
    path_prefix: /project1
    auth_mode: key_vault  # Recommended: 'key_vault', or 'managed_identity', 'service_principal'
    key_vault_name: company-keyvault  # Required for key_vault mode
    secret_name: storage-account-key  # Required for key_vault mode
    # For managed_identity: no additional fields needed (uses cluster identity)
    # For service_principal: add tenant_id, client_id, client_secret
  
  # Example: Azure SQL Database (Phase 2A ✅)
  azure_db:
    type: azure_sql
    host: myserver.database.windows.net
    database: analytics
    port: 1433  # default
    auth_mode: key_vault  # or 'managed_identity', 'service_principal'
    key_vault_name: company-keyvault  # For key_vault mode
    secret_name: sql-connection-string  # For key_vault mode

# ============================================
# STORY GENERATION (Required)
# ============================================
# Stories are ODIBI's core value - execution reports with lineage.
# Must specify WHERE stories are saved (using a connection).

story:
  connection: outputs  # (Required) Connection name for story output
  path: stories/  # (Required) Path relative to connection base_path
                  # Result: ./outputs/stories/ (using 'outputs' connection)
  max_sample_rows: 10  # (Optional) Rows in data previews (default: 10, range: 0-100)
  auto_generate: true  # (Optional) Generate after each run (default: true)

# ============================================
# RETRY CONFIGURATION (Optional)
# ============================================
# Global retry settings for all operations.
# Can be overridden at node level in Phase 2.

retry:
  enabled: true  # (Optional) Enable retry logic (default: true)
  max_attempts: 3  # (Optional) Max retries (default: 3, range: 1-10)
  backoff: exponential  # (Optional) Strategy: 'exponential', 'linear', 'constant'

# ============================================
# LOGGING CONFIGURATION (Optional)
# ============================================
# Global logging settings.

logging:
  level: INFO  # (Optional) Log level: 'DEBUG', 'INFO', 'WARNING', 'ERROR' (default: INFO)
  structured: false  # (Optional) Output JSON logs for machine parsing (default: false)
  metadata:  # (Optional) Extra metadata in all log entries
    environment: development
    team: data-engineering

# ============================================
# ENVIRONMENT OVERRIDES (Phase 3 - Not Implemented)
# ============================================
# Override configurations per environment (dev/staging/prod).
# Attempting to use this will raise NotImplementedError.
# Uncomment when Phase 3 is released.

# environments:
#   dev:
#     story:
#       connection: outputs
#       path: dev_stories/
#     logging:
#       level: DEBUG
#   
#   prod:
#     story:
#       connection: azure_lake
#       path: prod/observability/stories/
#     logging:
#       level: WARNING
#       structured: true

# ============================================
# PIPELINES (Required - at least one)
# ============================================
# Define data transformation workflows.

pipelines:
  # ========================================
  # Pipeline 1: Bronze to Silver
  # ========================================
  - pipeline: bronze_to_silver  # (Required) Unique pipeline name
    description: "Clean and validate raw sales data"  # (Optional) Human-readable
    layer: transformation  # (Optional) Logical layer identifier
    
    nodes:
      # ====================================
      # Node: Read from CSV
      # ====================================
      - name: load_raw_sales  # (Required) Unique node name within pipeline
        description: "Load raw sales transactions"  # (Optional) Node description
        
        read:  # Read operation
          connection: data  # (Required) Connection name from connections section
          path: bronze/sales.csv  # (Required) Path relative to connection base_path
                                  # Result: ./data/bronze/sales.csv
          format: csv  # (Required) Format: 'csv', 'parquet', 'json', 'excel', 'avro'
          
          # Format-specific options (passed to pandas.read_csv or spark.read)
          options:  # (Optional)
            header: 0  # CSV has header row
            dtype:  # Specify column types
              transaction_id: str
              amount: float
            encoding: utf-8
        
        cache: true  # (Optional) Cache result in memory for reuse (default: false)
        
        # Validation (Optional) - Phase 2 feature
        # validation:
        #   not_empty: true
        #   no_nulls:
        #     - transaction_id
      
      # ====================================
      # Node: Transform with SQL
      # ====================================
      - name: clean_sales
        description: "Filter and clean sales data"
        depends_on: [load_raw_sales]  # (Optional) Wait for these nodes first
        
        transform:  # Transform operation
          steps:  # (Required) List of SQL queries or function calls
            # SQL query - references cached DataFrames by node name
            - |
              SELECT
                transaction_id,
                customer_id,
                product_id,
                amount,
                transaction_date
              FROM load_raw_sales
              WHERE amount > 0
                AND transaction_date IS NOT NULL
            
            # You can chain multiple SQL transforms
            # Each operates on the result of the previous
            # - |
            #   SELECT *,
            #     amount * 1.1 as amount_with_tax
            #   FROM clean_sales
      
      # ====================================
      # Node: Custom Python Transform
      # ====================================
      - name: add_metrics
        description: "Add custom business metrics using Context API"
        depends_on: [clean_sales]
        
        transform:
          steps:
            # Call registered Python function with Context API
            # Register with: @transform decorator
            # Example function:
            #   from odibi import transform
            #   @transform
            #   def calculate_revenue_metrics(context, window_days: int, metric_type: str):
            #       df = context.get('clean_sales')  # Get cached DataFrame by name
            #       # Your transformation logic here
            #       return enriched_df
            - function: calculate_revenue_metrics
              params:  # Function parameters
                window_days: 30
                metric_type: revenue
        
        cache: true
      
      # ====================================
      # Node: Write to Parquet
      # ====================================
      - name: save_silver
        description: "Save cleaned data to silver layer"
        depends_on: [add_metrics]
        
        write:  # Write operation
          connection: data  # (Required) Connection name
          path: silver/sales.parquet  # (Required) Path relative to connection
                                      # Result: ./data/silver/sales.parquet
          format: parquet  # (Required) Format: 'csv', 'parquet', 'json', 'excel', 'avro'
          mode: overwrite  # (Required) Write mode: 'overwrite' or 'append'
          
          # Format-specific options
          options:  # (Optional)
            compression: snappy  # Parquet: snappy, gzip, brotli
            # partition_by: ['year', 'month']  # Spark only (Phase 3)

  # ========================================
  # Pipeline 2: Silver to Gold with Delta Lake
  # ========================================
  - pipeline: silver_to_gold
    description: "Aggregate and enrich for analytics with Delta Lake"
    layer: aggregation
    
    nodes:
      - name: load_silver
        read:
          connection: data
          path: silver/sales.parquet
          format: parquet
        cache: true
      
      - name: customer_summary
        depends_on: [load_silver]
        transform:
          steps:
            - |
              SELECT
                customer_id,
                COUNT(*) as transaction_count,
                SUM(amount) as total_spent,
                AVG(amount) as avg_transaction,
                MAX(transaction_date) as last_purchase_date
              FROM load_silver
              GROUP BY customer_id
              HAVING total_spent > 100
      
      - name: save_gold
        description: "Write to Delta Lake for ACID transactions & time travel"
        depends_on: [customer_summary]
        write:
          connection: data
          path: gold/customer_summary.delta  # Delta format (recommended for production)
          format: delta  # Provides ACID transactions, time travel, schema evolution
          mode: append  # Safe append with ACID guarantees
          options:
            mergeSchema: true  # Allow schema evolution (add new columns)
            # partition_by: ['year']  # Optional: partition for large datasets

  # ========================================
  # Pipeline 3: Delta Lake Time Travel Example
  # ========================================
  - pipeline: delta_time_travel
    description: "Read and compare Delta Lake versions"
    layer: analytics
    
    nodes:
      # Read latest version
      - name: read_latest_gold
        read:
          connection: data
          path: gold/customer_summary.delta
          format: delta
        cache: true
      
      # Read specific version (time travel)
      - name: read_version_5
        read:
          connection: data
          path: gold/customer_summary.delta
          format: delta
          options:
            versionAsOf: 5  # Time travel to version 5
        cache: true
      
      # Compare versions
      - name: compare_versions
        depends_on: [read_latest_gold, read_version_5]
        transform:
          steps:
            - |
              SELECT
                'Latest' as version_type,
                COUNT(*) as customer_count,
                SUM(total_spent) as total_revenue
              FROM read_latest_gold
              UNION ALL
              SELECT
                'Version 5' as version_type,
                COUNT(*) as customer_count,
                SUM(total_spent) as total_revenue
              FROM read_version_5
      
      - name: save_comparison
        depends_on: [compare_versions]
        write:
          connection: data
          path: analytics/version_comparison.csv
          format: csv
          mode: overwrite

  # ========================================
  # Pipeline 4: Azure SQL Integration
  # ========================================
  - pipeline: azure_sql_integration
    description: "Load dimension data from Azure SQL"
    layer: integration
    
    nodes:
      # Read from Azure SQL
      - name: load_customer_dimension
        description: "Load customer master data from Azure SQL"
        read:
          connection: azure_db
          format: sql  # Required format field
          table: dim_customer  # SQL table name
          # Alternatively use query:
          # query: "SELECT * FROM dim_customer WHERE active = 1"
        cache: true
      
      # Join with local data
      - name: enrich_with_dimensions
        depends_on: [load_customer_dimension, load_silver]
        transform:
          steps:
            - |
              SELECT
                s.*,
                c.customer_name,
                c.customer_segment,
                c.region
              FROM load_silver s
              LEFT JOIN load_customer_dimension c
                ON s.customer_id = c.customer_id
      
      - name: save_enriched
        depends_on: [enrich_with_dimensions]
        write:
          connection: data
          path: gold/enriched_sales.delta
          format: delta
          mode: append

# ============================================
# SUPPORTED FORMATS (Reference)
# ============================================
# Available Now (Phase 2B+):
# - csv: Comma-separated values
# - parquet: Columnar format (recommended for large datasets)
# - json: JSON format (newline-delimited)
# - excel: Excel spreadsheets (.xlsx)
# - avro: Avro binary format (requires odibi[pandas])
# - delta: Delta Lake tables with ACID & time travel (Phase 2B ✅)
# - sql: SQL database tables via Azure SQL or other connections

# ============================================
# NODE OPERATIONS REFERENCE
# ============================================
# Every node must have at least ONE of:
# - read: Load data from connection
# - transform: Transform data (SQL or function)
# - write: Save data to connection
#
# A node can have ALL THREE:
# - read → transform → write (ETL pattern)
#
# Or combinations:
# - read only (load data, cache for downstream)
# - transform only (operate on depends_on data)
# - write only (save depends_on data)
# - read + transform (load and clean)
# - transform + write (clean and save)

# ============================================
# HOW TO USE THIS TEMPLATE
# ============================================
#
# 1. Copy this file to your project
# 2. Update project metadata (name, description, owner)
# 3. Configure connections (where your data lives)
# 4. Configure story output (where reports go)
# 5. Define pipelines (your transformation logic)
# 6. Remove optional sections you don't need
# 7. Run: python -c "from odibi.pipeline import Pipeline; Pipeline.from_yaml('config.yaml').run()"
#
# For minimal example: examples/example_local.yaml
# For documentation: docs/CONFIGURATION_EXPLAINED.md
# For walkthroughs: walkthroughs/01_local_pipeline_pandas.ipynb
